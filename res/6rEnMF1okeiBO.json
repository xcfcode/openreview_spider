{"notes": [{"ddate": null, "legacy_migration": true, "tmdate": 1393544220000, "tcdate": 1393544220000, "number": 1, "id": "_AcWXVswj-XvN", "invitation": "ICLR.cc/2014/-/submission/conference/reply", "forum": "6rEnMF1okeiBO", "replyto": "XahgXeP_L0ZYU", "signatures": ["Andrew Davis"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "6a1b,\r\n\r\nThanks for the reply -- always good to have constructive feedback.\r\n\r\n> I don't think the revised paper really addresses my main concern, which is that it's extremely difficult to get a speedup in practice from the proposed method. I think you'll have quite a lot of difficulty doing so with cuda-convnet, since GPUs generally do not handle applications with a lot of branching well.\r\n\r\nYou are correct -- for generic matrix multiplication, obtaining a speedup will be difficult. Even for matrix multiplications with highly structured outputs (eg., multiplying an upper-triangular matrix by another upper-triangular matrix will yield another upper-tri matrix, so we don't need to calculate the lower triangular entries), the formulation is difficult and doesn't quite perform at 2x (in the case of tri * tri) even when implemented efficiently.\r\n\r\nThe bulk of the computation for CNNs occur in the first few layers.  When plugging in values for the layer-wise sparsity and low-rank approximations, full-sparsity on the output layers for an Imagenet-sized network hardly impacts the relative change in FLOPs. Therefore, we are focusing mainly on implementing the conditional computation for CNNs at this point.\r\n\r\nWhile it is true that GPUs are not generally efficient when following arbitrary branches for different threads (warp divergence), NVIDIA GPUs can happily follow branches, as long as every thread in the warp follows the branch. We are working on a CUDA kernel that works on 8x8 filters (64 pixels), so when we skip a step in the convolution for a particular image and particular filter, precisely two warps follow the same execution path. There is some overhead in formulating the convolution in this way (we have to use more local memory and we have to do reductions, which wastes threads as we proceed through the reduction tree), but the path ahead looks promising. We plan to extend this idea to 4x4 filters and 12x12 filters, where we can skip pairs of 4x4 filters or 12x12 filters at a time. (ie., two 4x4 filters, or 32 pixels, are calculated together in one warp. If we have to calculate one, then we calculate both. If we do not have to calculate either, skip the computation. There is a similar scheme for 12x12, but we only 'waste' 16 pixels worth of calculation if we have to calcuate the step in the convolution for either filter, because the 'boundary' between the two filters spills over a warp).\r\n\r\n> While I understand that you're trying to see how much your method degrades neural net performance rather than trying to get a new state of the art result, I think it is important to work with state of the art methods and large models. When starting with a small or underperforming baseline, you might obtain a result showing that your method doesn't harm performance, merely because the base performance was poor and easily captured by a simpler model family.\r\n\r\nThis is an excellent point. We look forward to the results when applying this approach to larger networks.\r\n\r\n> The paper by Leonard et al is quite a bit different than what you are doing. You are trying to learn which values are zero after using a standard training procedure.\r\n\r\nNot necessarily -- in this paper, the conditional computation is applied during the entirity of training.\r\n\r\n> Leonard et al are training a net to intentionally *make more of the activations zero*. Their procedure explicitly groups the zeros into contiguous blocks, so that the indexing logic required to ignore the zeros is cheap.\r\n\r\nCuriously, I'm not seeing anything in this reference talking about encouraging the zeros of the hidden activation to be contiguous. Perhaps you have another reference that I am not aware of?\r\n\r\n> Also, the secondary network used to determine which blocks should be computed is significantly smaller than the primary classification network. In your case, the secondary network has the same number of outputs as the primary network has hidden units. Despite all of these advantages over your approach, Leonard et al still didn't demonstrate a speedup, so I think your approach will really be fighting an uphill battle to get one.\r\n\r\n'Experiments are performed with a gater of 400 hidden units and 2000 output units. The main path also has 2000 hidden units. A sparsity constraint is imposed on the 2000 gater output units such that each is non-zero 10% of the time, on average.'  I see that the gater and the hidden units are always of the same size. Are we looking at the same paper?\r\n\r\n> Getting 1.05% or below on MNIST is not hard with dropout. Grad students from both Nando de Freitas and Yoshua Bengio's lab have replicated that result without any trouble.\r\n\r\nBy adding noise to the input layer (setting pixels to zero with p=0.2), I've been able to bring MNIST down to ~1%. Initial results show similar behavior when perturbed with an increasingly harsh bottleneck on the gating layer. I would like to revise the paper with these results, but I am not sure if we are past the deadline for resubmission. \r\n\r\nI've gotten SVHN down to ~7% by adding noise to the input, but I suspect the results will always be very far from state-of-the-art with a permutation invariant network."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Low-Rank Approximations for Conditional Feedforward Computation in Deep Neural Networks", "decision": "submitted, no decision", "abstract": "Scalability properties of deep neural networks raise key research questions, particularly as the problems considered become larger and more challenging. This paper expands on the idea of conditional computation introduced by Bengio, et. al., where the nodes of a deep network are augmented by a set of gating units that determine when a node should be calculated. By factorizing the weight matrix into a low-rank approximation, an estimation of the sign of the pre-nonlinearity activation can be efficiently obtained. For networks using rectified-linear hidden units, this implies that the computation of a hidden unit with an estimated negative pre-nonlinearity can be ommitted altogether, as its value will become zero when nonlinearity is applied. For sparse neural networks, this can result in considerable speed gains. Experimental results using the MNIST and SVHN data sets with a fully-connected deep neural network demonstrate the performance robustness of the proposed scheme with respect to the error introduced by the conditional computation process.", "pdf": "https://arxiv.org/abs/1312.4461", "paperhash": "davis|lowrank_approximations_for_conditional_feedforward_computation_in_deep_neural_networks", "keywords": [], "conflicts": [], "authors": ["Andrew Davis", "Itamar Arel"], "authorids": ["ndrw.dvvs@gmail.com", "itamar@ieee.org"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1393472940000, "tcdate": 1393472940000, "number": 1, "id": "XahgXeP_L0ZYU", "invitation": "ICLR.cc/2014/-/submission/conference/reply", "forum": "6rEnMF1okeiBO", "replyto": "wMWZGo27UK-XQ", "signatures": ["anonymous reviewer 6a1b"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "I don't think the revised paper really addresses my main concern, which is that it's extremely difficult to get a speedup in practice from the proposed method. I think you'll have quite a lot of difficulty doing so with cuda-convnet, since GPUs generally do not handle applications with a lot of branching well.\r\n\r\nWhile I understand that you're trying to see how much your method degrades neural net performance rather than trying to get a new state of the art result, I think it is important to work with state of the art methods and large models. When starting with a small or underperforming baseline, you might obtain a result showing that your method doesn't harm performance, merely because the base performance was poor and easily captured by a simpler model family.\r\n\r\nThe paper by Leonard et al is quite a bit different than what you are doing. You are trying to learn which values are zero after using a standard training procedure. Leonard et al are training a net to intentionally *make more of the activations zero*. Their procedure explicitly groups the zeros into contiguous blocks, so that the indexing logic required to ignore the zeros is cheap. Also, the secondary network used to determine which blocks should be computed is significantly smaller than the primary classification network. In your case, the secondary network has the same number of outputs as the primary network has hidden units. Despite all of these advantages over your approach, Leonard et al still didn't demonstrate a speedup, so I think your approach will really be fighting an uphill battle to get one.\r\n\r\nGetting 1.05% or below on MNIST is not hard with dropout. Grad students from both Nando de Freitas and Yoshua Bengio's lab have replicated that result without any trouble."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Low-Rank Approximations for Conditional Feedforward Computation in Deep Neural Networks", "decision": "submitted, no decision", "abstract": "Scalability properties of deep neural networks raise key research questions, particularly as the problems considered become larger and more challenging. This paper expands on the idea of conditional computation introduced by Bengio, et. al., where the nodes of a deep network are augmented by a set of gating units that determine when a node should be calculated. By factorizing the weight matrix into a low-rank approximation, an estimation of the sign of the pre-nonlinearity activation can be efficiently obtained. For networks using rectified-linear hidden units, this implies that the computation of a hidden unit with an estimated negative pre-nonlinearity can be ommitted altogether, as its value will become zero when nonlinearity is applied. For sparse neural networks, this can result in considerable speed gains. Experimental results using the MNIST and SVHN data sets with a fully-connected deep neural network demonstrate the performance robustness of the proposed scheme with respect to the error introduced by the conditional computation process.", "pdf": "https://arxiv.org/abs/1312.4461", "paperhash": "davis|lowrank_approximations_for_conditional_feedforward_computation_in_deep_neural_networks", "keywords": [], "conflicts": [], "authors": ["Andrew Davis", "Itamar Arel"], "authorids": ["ndrw.dvvs@gmail.com", "itamar@ieee.org"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392081000000, "tcdate": 1392081000000, "number": 3, "id": "5LNqFhzCG2FPM", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "6rEnMF1okeiBO", "replyto": "6rEnMF1okeiBO", "signatures": ["anonymous reviewer b1e7"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Low-Rank Approximations for Conditional Feedforward Computation in Deep Neural Networks", "review": "The authors investigate a proposed method of speeding up computation in a feed-forward ReLU network by predicting the sign of the presynaptic activations with a low-rank predictor obtained by SVD on the weights.\r\n\r\nNovelty: medium\r\nQuality: low\r\n\r\nPros:\r\n- Investigates a problem of substantial interest to the community, that of scaling up neural nets through what Yoshua Bengio has dubbed \u201cconditional computation\u201d\r\n- Experimental procedures are well documented, software cited\r\n\r\nCons:\r\n- misinterprets the goals articulated by Bengio on conditional computation: namely, to explicitly learn which computations to do/which parts of an architecture to activate, rather than simply identify ways of speeding up existing architectures through predictive computation. In that respect it could still be an interesting line of inquiry but not really \u201cconditional computation\u201d related.\r\n- Provides no empirical benchmarking: I strongly suspect an empirical investigation of the proposed speedup would reveal that the cache unfriendliness of the non-uniform memory access would result in a significant slowdown even for large, relatively sparse network.\r\n- The commentary contains several instances of speculation that is not qualified as such (see below)\r\n- The experimental baselines are questionable, uninteresting, and applying the same sort of fully connected architecture to only two tasks, one of which has not been widely studied with fully-connected networks and thus hard to judge. Especially in the first layer it seems like the efficacy of the low rank predictor may depend heavily on the input distribution.\r\n\r\nDetailed comments:\r\n\r\nSec 2.1:\r\n- The speculation that parameter redundancy leads to activation redundancy is very questionable. Consider two filters consisting of oriented, localized edge filters, one appearing exclusively in the upper left quadrant of the receptive field and the other appearing in the lower right, with 3 of 4 quadrants having zero weights in each filter. The two have extremely redundant parameters (only a few bits are needed to describe one given knowledge of the other), but their activations are not redundant in the least.\r\n\r\nSec 2.2:\r\n- Both of the datasets on which you run experiments have had extremely competitive (state of the art, in the case of SVHN) performance documented with maxout activations, where activities are completely non-sparse. The importance of representational sparsity is thus far less clear than you seem to suggest.\r\n\r\nSec 3.1:\r\n- Notational comment: sigma() is typically reserved for the logistic function, or at least a sigmoidal function such as tanh. Its use to represent the ReLU activation is somewhat jarring. \r\n- You can probably assume that a reader of this paper understands how to perform matrix multiplication. The exposition in terms of dot products is unnecessary.\r\n- Regarding considerable speed gains, I find this dubious given the degree to which optimized matrix-matrix multiplication libraries (the BLAS, etc.) leverage the properties of today\u2019s CPU and GPU hardware. In sparse linear algebra applications where sheer memory requirements are not the limiting factor (i.e. making explicitly representing the sparse matrix infeasible), sparsity well in excess of 99% is typically necessary for specialized sparse matrix multiplication routines to beat BLAS on a sparse problem represented densely. While you may be able to claim an asymptotic speedup, it\u2019s unclear whether this means anything in practice for a wide class of problems of interest. Even if things are sparse enough for there to be a savings, it\u2019s unclear whether this would be negated by the necessity of performing the low-rank computation as well.\r\n- It would be interesting and comforting to see some analysis (theoretical or empirical) of the probability of the low rank approximation making a mistake (and each kind of mistake, as mistakenly flagging a filter response as having a positive value results in no actual error being made in the final analysis, as it will be computed and then presumably thresholded to zero, whereas not computing one that should\u2019ve been positive will affect the representation passed up to the next layer). If you assume bounded norms of the weight vectors I\u2019m fairly sure you could say something theoretically as a function of the rank of the approximation.\r\n- I\u2019d also be interested in how the activation estimator\u2019s efficacy differs for the first layer and subsequent layers, given that the latter have a more sparse input distribution, and whether preprocessing helps or not. Similarly, how does error accumulate through this process when you use it at multiple layers?\r\n\r\nSec 3.4:\r\n- multiplying an asymptotic O(...) expression by a constant makes no sense. O() notation is explicitly defined to absorb all multiplicative constants. What you should do instead is either a) make use of a numerical methods textbook pseudocode implementation of SVD to bound the number of theoretical flops, or b) drop the O(...), add a multiplicative constant (which you could bound) and add O(...)\u2019s for the lower-order terms.\r\n\r\nSec 4.1 & 4.2\r\n- Generally, the lack of comparison to the literature for your baselines is troubling. I don\u2019t know what the state of the art is for a permutation-invariant method (such as a fully connected network) on SVHN, but your nets seem critically underresourced. I know that layer sizes for an MNIST network achieving close to 1% typically features layer sizes at least double those of what you report, and it seems like you are underfitting significantly on SVHN. The whole point of conditional computation is to save computation time in very large models, but you restrict your investigation to extremely impoverished architectures.\r\n- Your MNIST results are significantly worse with what is the permutation-invariant state of the art standard nowadays and there\u2019s really no excuse for it, an error of 1.05-1.06% is easily reproducible. At the very least it would frame the error rates in terms of results with which people are familiar.\r\n- The architectures of the activation estimators seem arbitrary and it is not clear how they were chosen, or why a wider variety were not explored.\r\n- In addition to the error rates for various activation estimator architectures, you should make clear the theoretical speedup provided by this architecture, and the measured empirical speedup.\r\n\r\nSection 5:\r\n- The extension to the convolutional setting is not obvious to me at all. Sure, you can write out the convolution as a matrix multiply with a structured sparse matrix but the SVD of that is going to be dense and gigantic.\r\n- There\u2019s no mention of the possibility of learning the low-rank approximation, which seems like the natural thing to do, especially as concerns test-time speed-up rather than train-time speedup. I also consider it critical to compare against the case where there is no full rank weight matrix, just this low rank version being used as the actual weight matrix, to show that maintaining and carefully multiplying by the full rank weight matrix actually is in some respect a necessary ingredient. If you can get away with simply learning factored weight matrices (i.e. introducing bottleneck linear layers) then this scheme is needlessly complicated."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Low-Rank Approximations for Conditional Feedforward Computation in Deep Neural Networks", "decision": "submitted, no decision", "abstract": "Scalability properties of deep neural networks raise key research questions, particularly as the problems considered become larger and more challenging. This paper expands on the idea of conditional computation introduced by Bengio, et. al., where the nodes of a deep network are augmented by a set of gating units that determine when a node should be calculated. By factorizing the weight matrix into a low-rank approximation, an estimation of the sign of the pre-nonlinearity activation can be efficiently obtained. For networks using rectified-linear hidden units, this implies that the computation of a hidden unit with an estimated negative pre-nonlinearity can be ommitted altogether, as its value will become zero when nonlinearity is applied. For sparse neural networks, this can result in considerable speed gains. Experimental results using the MNIST and SVHN data sets with a fully-connected deep neural network demonstrate the performance robustness of the proposed scheme with respect to the error introduced by the conditional computation process.", "pdf": "https://arxiv.org/abs/1312.4461", "paperhash": "davis|lowrank_approximations_for_conditional_feedforward_computation_in_deep_neural_networks", "keywords": [], "conflicts": [], "authors": ["Andrew Davis", "Itamar Arel"], "authorids": ["ndrw.dvvs@gmail.com", "itamar@ieee.org"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1390968540000, "tcdate": 1390968540000, "number": 2, "id": "wMWZGo27UK-XQ", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "6rEnMF1okeiBO", "replyto": "6rEnMF1okeiBO", "signatures": ["Andrew Davis"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "-Demonstrates that the sign approximation method slightly improves test error.\r\nI'm not sure where you're seeing this -- on the test set, the sign approximation always increased the test error (as a function of how poor the approximation was). One should expect the method to make test error slightly worse as a trade-off for faster computation. This approach does not try to improve generalization -- it is forming the basis of a method to trade-off between speed and accuracy.\r\n\r\n-The main purpose of the work...\r\n-It's not obvious...\r\n-If computational cost can in fact be reduced...\r\nThese three points have been addressed in a new revision of the manuscript. There are many parameters that determine the theoretical speedup (size of each layer, how approximately the weight matrix can be calculated, how sparse the activations are), so the answer (that is, the overall speedup) depends on these factors.\r\n\r\n-The baselines that are improved upon are not competitive\r\nThe baselines are not intended to be competitive to state-of-the-art methods.  The emphasis is on investigating how the activation estimation degrades the training of the neural network.\r\n\r\n-It's not clear that the hyper parameters were chosen in a way that makes the comparisons fair\r\nWe've added a description of how the hyperparameters were chosen to the manuscript.\r\n\r\n-I don't think you can necessarily conclude...\r\nInteresting, I did not know this -- I was going off of the introduction in Coates et al's paper.\r\n\r\n-I don't think you can conclude...\r\nI disagree.  If the biases are part of the weight matrix (an okay thing to do), then we observe the following:\r\n\r\na in R^{N \times d}, W in R^{d \times h}, U in R^{d \times k}, V in R^{k \times h}:\r\n\r\nUV = hat{W}\r\n\r\nAssuming:\r\nrank(hat{W}) = k\r\nrank(a) = d\r\n\r\nThen:\r\nrank(ahat{W}) le min(rank(a), rank(hat{W})) = min(d, k)\r\n\r\nBecause k < d (otherwise, we are wasting time with the conditional computation, because the activation estimator would take longer than the regular feed-forward), the rank of the resulting activations can be fairly well represented by a linear projection of lower-rank, indicating redundancy in the same sense that the weights are redundant.  Redundant does not necessarily mean useless -- the extra dimensionality gives us some more 'wiggle-room' for discrimination.  The key observation is that we may be able to 'get away with' a very low-rank representation of 'W' if we are only interested in recovering the sign of 'a'.\r\n\r\n-Simply being able to predict...\r\nAgreed -- the activation estimator is designed to be efficient, but it is certainly possible to be slower overall if a bad choice is made for the rank, or if the activations are not sufficiently sparse.  There are likely smarter and faster ways of doing the conditional computation that have not been investigated.  We've added a section describing the number of FLOPS required for a regular neural net and a neural net augmented by this conditional computation scheme.\r\n\r\n-You also need some mechanism...\r\nI agree.  This part will require the most effort to implement well.  It is hard to beat off-the-shelf BLAS libraries, but the proposed matrix-multiplication would not be a sparse-sparse (ie., memory-bound) operation.  It would be dense-dense, with some additional control structure to allow certain dot-products to be skipped.  This will surely add some overhead, and it is hard to speculate exactly how much.  Using the equations in Section 3.4 (the new section on the FLOPS required), there could be as much as a 5-20x speedup in feed-forward for an Imagenet-size network, depending on sparsity and rank. As such, we feel it is worth investigating this special matrix multiplication.\r\n\r\n-I think you're missing the fundamental idea...\r\nWe disagree.  Bengio, Leonard, and Courville put a preprint on arXiv (http://arxiv.org/abs/1308.3432, citation [3]) using a similar scheme, but differing in execution -- an auxiliary network off to the side that decides which neurons need to be calculated, which is trained by backpropagation (with a sparsity constraint).  This approach is still at least linear in the number of hidden neurons.  Implementing a network that is exponential in the number of features will likely take an incremental effort, and we feel that this approach is a good starting point.\r\n\r\n-I'm not sure sparsity of the representation...\r\nThanks for pointing this out -- indeed, the activations of maxout networks are dense, making the statement in the paper about sparsity too strong.  We only meant to say that sparse representations work well in a lot of cases, so we should take advantage of them.\r\n\r\n-Your presentation is confusing...\r\nThank you for pointing this out, I have fixed this.\r\n\r\n-I don't understand the paragraph on dropout...\r\nI have revised the wording here, apologies for the confusion.  There is no obvious connection to the dropout rate and the eventual sparsity of the network, in the sense of 'given a dropout rate p, the sparsity of the activations of the geometric-mean network will be q'.  It is simply a side-effect that a neural net trained with dropout tends to be more sparse than a network that is trained without.\r\n\r\n-1 is probably a pretty high value for the initial biases.\r\nWe used a suggestion in the Imagenet paper -- set the biases to a high value so the neurons are mostly active for the initial part of training. We had good initial results with it, so we did not explore other values when it came to tuning hyperparameters.\r\n\r\n-Was dropout applied to the input...\r\nWe were not using dropout on the input layer (is this commonly done outside of the context of denoising autoencoders? I'd be interested in seeing a reference for this.)  Thanks for pointing this out, I've clarified this in the new manuscript.\r\n\r\n-Where did these hyper parameters come from?...\r\nI've updated the manuscript to detail the hyperparameter selection (done in the usual way -- split the training set into a training and a validation set, try several sets of hyperparameters, select the set of hyperparameters that yields the best validation set.)  The hyperparameters were chosen using the baseline network and were not used to guide the performance of the predictor network.  You may be misunderstanding the point of this work, which is to investigate the degradation of performance as the activation estimator becomes more approximate -- we are not trying to improve the performance of neural nets with the activation estimator.  The interesting part is how approximate we can make it before performance suffers noticeably.\r\n\r\n-...your experiments are bypassing the most important part of the paper, which is to demonstrate a speedup...\r\nThis is true, as the conditional matrix multiplication has not yet been implemented in an efficient way.  The implementation of the conditional matrix multiplication will certainly have some overhead, but we do not expect it to be as great as the overhead seen in sparse-sparse operations.  There are two reasons for this:\r\n1) We are still ultimately doing a dense-dense matrix multiplication, so we don't need to spend any time creating sparse matrices.\r\n2) We will still be able to take advantage of things like locality, caching, vector operations, (all of which are mostly off the plate with sparse operations) so we expect the operation to still be mostly compute-bound (as opposed to sparse operations which are memory-bound).  We are beginning to work on a CPU as well as a GPU implementation.\r\nA conditional matrix multiplication could also allow us to only calculate the backpropagation through the required (ie., non-zero) neurons, which could lead to similar speed gains in the backpropagation phase.\r\n\r\n-Moreover, as far as I can tell, you don't quantify the cost of frequently running SVD during training.\r\nThe cost of the SVD is fairly insignificant because it is evaluated so infrequently (without impacting the accuracy of the estimator too greatly).  On my machine, evaluating an SVD takes ~0.30s for a 1000x784 matrix.  Each epoch for the SVHN net took ~300s.  We've found some interesting ways to speed up successive SVD calculations with warm start methods (faster factorization given a closer starting point to the final factorization), but we have not experimented with them yet.  Admittedly, the SVD is a pretty heavy operation, and we would like to move away from it, especially considering that there are many properties of the SVD that aren't crucial for this approach to work.\r\n\r\n-...The bigger of the two has about three million parameters and doesn't seem to be able to fit the SVHN dataset all that well...\r\nThe codebase was confined to using a fully-connected net, so it is not surprising that the SVHN results were far from state-of-the-art.  The results are comparable to the stacked auto-encoder results reported in 'Reading Digits in Natural Images with Unsupervised Feature Learning' (Netzer, et al).\r\n\r\n-Are your nets actually large enough...\r\nGood question.  deeplearntoolbox turned out to be a slightly inappropriate codebase (a very slow convnet implementation, no GPU support) for the purposes of this work, and I am currently modifying cuda-convnet to work with conditional computation.  We look forward to applying this method to large convolutional networks trained on more difficult datasets, such as Imagenet.\r\n\r\n-...I'm confident that your baseline control system is not good...\r\nIn Nitish Srivastava's MS thesis, he reports 1.25% for a fully-connected relu network with dropout, and 1.05% for a fully-connected relu network with dropout and a max norm constraint.  It is very difficult to get less than 1% on the permutation-invariant task without resorting to things like elastic deformations.  While the reported ~1.4% in this work is far from these results, I believe the 0.4% difference in performance can be attributed to a less-than-optimal hyperparameter search, or perhaps some subtle difference in implementation, eg., learning rate scheduling, momentum scheduling, weight initialization scheme, etc.  Because the purpose of this work is to investigate the effects of the approximation on the performance of the network, less time was spent on finding hyperparameters that would yield state-of-the-art performance on MNIST."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Low-Rank Approximations for Conditional Feedforward Computation in Deep Neural Networks", "decision": "submitted, no decision", "abstract": "Scalability properties of deep neural networks raise key research questions, particularly as the problems considered become larger and more challenging. This paper expands on the idea of conditional computation introduced by Bengio, et. al., where the nodes of a deep network are augmented by a set of gating units that determine when a node should be calculated. By factorizing the weight matrix into a low-rank approximation, an estimation of the sign of the pre-nonlinearity activation can be efficiently obtained. For networks using rectified-linear hidden units, this implies that the computation of a hidden unit with an estimated negative pre-nonlinearity can be ommitted altogether, as its value will become zero when nonlinearity is applied. For sparse neural networks, this can result in considerable speed gains. Experimental results using the MNIST and SVHN data sets with a fully-connected deep neural network demonstrate the performance robustness of the proposed scheme with respect to the error introduced by the conditional computation process.", "pdf": "https://arxiv.org/abs/1312.4461", "paperhash": "davis|lowrank_approximations_for_conditional_feedforward_computation_in_deep_neural_networks", "keywords": [], "conflicts": [], "authors": ["Andrew Davis", "Itamar Arel"], "authorids": ["ndrw.dvvs@gmail.com", "itamar@ieee.org"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1390272720000, "tcdate": 1390272720000, "number": 1, "id": "_HtzHMbQzfHQd", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "6rEnMF1okeiBO", "replyto": "6rEnMF1okeiBO", "signatures": ["anonymous reviewer 6a1b"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Low-Rank Approximations for Conditional Feedforward Computation in Deep Neural Networks", "review": "Summary of contributions: Proposes a specific means to achieve Bengio's goal of 'conditional computation'. The specific mechanism is to use the sign of approximate activations to predict which activations are zero, so that the full-rank values of rectified linear units don't need to be computed for units that are probably 0. \r\n\r\nNovelty: moderate\r\nQuality: low\r\n\r\nPros:\r\n-Demonstrates that the sign approximation method slightly improves test error.\r\nCons:\r\n-The main purpose of the work is to reduce computational cost, but computational cost is never quantified\r\n-It's not obvious that predicting which units are zero will lead to the ability to reduce computation cost\r\n-If computational cost can in fact be reduced, it's not clear that the cost of the predictor is less than the cost that its use could remove\r\n-The baselines that are improved upon are not competitive\r\n-It's not clear that the hyper parameters were chosen in a way that makes the comparisons fair\r\n-Some of the commentary is misleading or questionable\r\n\r\nDetailed comments:\r\n\r\npg 1\r\nI don't think you can necessarily conclude that the computational requirements to train a large net decreased between the publication of [12] and the publication of [4]. The purpose of [12] was largely to demonstrate how many machines their system could use. At the ICML poster session where [12] was first presented, I heard someone else ask Quoc Le if the number of machines was actually necessary to get the results they had, and he said the high number of machines was not necessary, they just wanted to claim credit for networking that many machines to train a neural net. It's likely that most of the savings in [4] relative to [12] are just due to the fact that most of the computation in [12] was wasted.\r\n\r\npg 2\r\n\r\nSection 2.1\r\n\r\nI don't think you can conclude that redundancy of parameters implies redundancy of activations. For example, two units with the same weight vector have very redundant parameters, but changing just their bias unit can give them very different activations. Presumably nets would not benefit much from increased size if the units were truly redundant.\r\n\r\nSimply being able to predict which units in a net are going to have zero activation doesn't necessarily mean that you will get computational savings. You need the cost of predicting which units are zero to be cheap enough that on average running both the predictor net and the predicted subset of the main net is cheaper than running just the main net. You also need some mechanism for cheaply running subsets of the main net. Doing sparse matrix operations with an arbitrary, dynamically updated sparsity mask is not significantly more efficient than just doing dense operations, to my knowledge.\r\n\r\nI think you're missing the fundamental idea of conditional computation, at least as described in [2] and [3]. Bengio is not advocating figuring out cheaper ways of computing exactly the same function we already compute. He's advocating figuring out how to compute only some of the features that are most relevant to processing a particular input. What you're doing is more of a software engineering optimization where you figure out which features are zero. Bengio is advocating something more ambitious, figuring out which features are unnecessary even if they are nonzero. Another issue with your approach is that you're trying to individually predict which features are nonzero. This means you need to be able to have the predictor make an explicit decision for each of them individually, so your runtime is going to have asymptotic cost which is at least linear in the number of units. Bengio is advocating making predictions about entire groups of units at the same time. If done right, this has the chance of having cost only logarithmic in the number of features in the model, or, to phrase it more excitingly, being able to run a model that has exponentially more features than runtime and memory requirements.\r\n\r\nSection 2.2\r\nI'm not sure sparsity of the representation is the correct explanation for the performance of the rectifier nets in [7]. The sparse rectifier nets in [7] have sparse activations, but they have equally sparse gradients. i.e., the gradient through a unit is zero for an example if and only if the activation is zero for that unit. This paper ( http://arxiv.org/pdf/1302.4389v4.pdf ) shows that similar nets can perform well if they have sparse gradients but non-sparse activations. See especially Fig 2. of that paper. So sparse gradients may be more important than sparse activations.\r\n\r\n\r\npg 3\r\n\r\nYour presentation is confusing because you refer the reader to fig 3.1 before defining S, but S is used in the caption of this figure.\r\n\r\nUsually one does not use italic letters when denoting the 'sgn' function, since italic letters are used for algebraic variables.\r\n\r\npg 4\r\n\r\nSection 3.3\r\n\r\nI don't understand the paragraph on dropout and sparsity. The most problematic sentence is 'During training, the sparsity of the network is likely less than p for each minibatch.' Does 'more sparsity' mean fewer active units? So less sparsity means more active units? If we set p to zero, this implies that every unit is likely active. But that is the default SGD training algorithm, in which we certainly know that not all the units are active. Also, are you counting units zeroed by dropout as contributing to the sparsity? What about units that are zeroed by their input being negative? Why should their be any straightforward and simple relation between the dropout p and the number of units zeroed by their inputs being negative?\r\n\r\n\r\nSection 3.4\r\n\r\n1 is probably a pretty high value for the initial biases. Was dropout applied to the input as well as the hidden layers? If so, p = 0.5 is probably too high for use on the input.\r\n\r\n\r\npg 5\r\n\r\nTable 4.1: Where did these hyper parameters come from? Why should the same hyper parameters be used for all conditions? How can we know that your 'control' curve in Fig 4.1 is a good baseline to improve over? If you hand-tuned your hyper parameters to work well with the S predictor net active, then it's not very surprising the control would perform worse. I'm not saying you intentionally did this, but without a description of your methodology for obtaining the hyperparameters, it seems likely that you implicitly did this while trying to get your new method to work.\r\n\r\nIt seems like your experiments are bypassing the most important part of the paper, which is to demonstrate a speedup. I don't see anything about timing experiments here. I suspect you can't actually get a speedup by this approach due to the difficulty of leveraging a speedup from sparsity if the sparsity pattern is dynamic rather than fixed, also due to the difficulty of leveraging sparsity on GPU if there is not a regular structure to the sparsity pattern. Moreover, as far as I can tell, you don't quantify the cost of frequently running SVD during training.\r\n\r\nAre your nets actually large enough to be interesting? The bigger of the two has about three million parameters and doesn't seem to be able to fit the SVHN dataset all that well. Conditional computation is mostly interesting for its ability to push the frontier of network capacity, so I'm not sure the scenario you've experimented on is really interesting from the perspective of evaluating a conditional computation method.\r\n\r\npg 6\r\n\r\nSimilar criticism for MNIST, except here I'm confident that your baseline control system is not good. With dropout you should be able to get down to < 1% test error."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Low-Rank Approximations for Conditional Feedforward Computation in Deep Neural Networks", "decision": "submitted, no decision", "abstract": "Scalability properties of deep neural networks raise key research questions, particularly as the problems considered become larger and more challenging. This paper expands on the idea of conditional computation introduced by Bengio, et. al., where the nodes of a deep network are augmented by a set of gating units that determine when a node should be calculated. By factorizing the weight matrix into a low-rank approximation, an estimation of the sign of the pre-nonlinearity activation can be efficiently obtained. For networks using rectified-linear hidden units, this implies that the computation of a hidden unit with an estimated negative pre-nonlinearity can be ommitted altogether, as its value will become zero when nonlinearity is applied. For sparse neural networks, this can result in considerable speed gains. Experimental results using the MNIST and SVHN data sets with a fully-connected deep neural network demonstrate the performance robustness of the proposed scheme with respect to the error introduced by the conditional computation process.", "pdf": "https://arxiv.org/abs/1312.4461", "paperhash": "davis|lowrank_approximations_for_conditional_feedforward_computation_in_deep_neural_networks", "keywords": [], "conflicts": [], "authors": ["Andrew Davis", "Itamar Arel"], "authorids": ["ndrw.dvvs@gmail.com", "itamar@ieee.org"]}, "tags": [], "invitation": {}}}, {"replyto": null, "ddate": null, "legacy_migration": true, "tmdate": 1387435020000, "tcdate": 1387435020000, "number": 10, "id": "6rEnMF1okeiBO", "invitation": "ICLR.cc/2014/conference/-/submission", "forum": "6rEnMF1okeiBO", "signatures": ["ndrw.dvvs@gmail.com"], "readers": ["everyone"], "content": {"title": "Low-Rank Approximations for Conditional Feedforward Computation in Deep Neural Networks", "decision": "submitted, no decision", "abstract": "Scalability properties of deep neural networks raise key research questions, particularly as the problems considered become larger and more challenging. This paper expands on the idea of conditional computation introduced by Bengio, et. al., where the nodes of a deep network are augmented by a set of gating units that determine when a node should be calculated. By factorizing the weight matrix into a low-rank approximation, an estimation of the sign of the pre-nonlinearity activation can be efficiently obtained. For networks using rectified-linear hidden units, this implies that the computation of a hidden unit with an estimated negative pre-nonlinearity can be ommitted altogether, as its value will become zero when nonlinearity is applied. For sparse neural networks, this can result in considerable speed gains. Experimental results using the MNIST and SVHN data sets with a fully-connected deep neural network demonstrate the performance robustness of the proposed scheme with respect to the error introduced by the conditional computation process.", "pdf": "https://arxiv.org/abs/1312.4461", "paperhash": "davis|lowrank_approximations_for_conditional_feedforward_computation_in_deep_neural_networks", "keywords": [], "conflicts": [], "authors": ["Andrew Davis", "Itamar Arel"], "authorids": ["ndrw.dvvs@gmail.com", "itamar@ieee.org"]}, "writers": [], "details": {"replyCount": 5, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1369422751717, "tmdate": 1496674357195, "id": "ICLR.cc/2014/conference/-/submission", "writers": ["ICLR.cc/2014"], "signatures": ["OpenReview.net"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": []}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1377198751717, "cdate": 1496674357195}}}], "count": 6}