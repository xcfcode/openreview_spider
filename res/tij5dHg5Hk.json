{"notes": [{"id": "tij5dHg5Hk", "original": "t9HCp4FgS4x", "number": 3680, "cdate": 1601308409484, "ddate": null, "tcdate": 1601308409484, "tmdate": 1614985693006, "tddate": null, "forum": "tij5dHg5Hk", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Run Away From your Teacher: a New Self-Supervised Approach Solving the Puzzle of BYOL", "authorids": ["~Haizhou_Shi1", "~Dongliang_Luo1", "~Siliang_Tang1", "~Jian_Wang14", "~Yueting_Zhuang1"], "authors": ["Haizhou Shi", "Dongliang Luo", "Siliang Tang", "Jian Wang", "Yueting Zhuang"], "keywords": ["representation learning", "self-supervised learning", "contrastive learning", "regularization", "theory"], "abstract": "Recently, a newly proposed self-supervised framework Bootstrap Your Own Latent (BYOL) seriously challenges the necessity of negative samples in contrastive-based learning frameworks. BYOL works like a charm despite the fact that it discards the negative samples completely and there is no measure to prevent collapse in its training objective. In this paper, we suggest understanding BYOL from the view of our newly proposed interpretable self-supervised learning framework, Run Away From your Teacher (RAFT). RAFT optimizes two objectives at the same time: (i) aligning two views of the same data to similar representations and (ii) running away from the model's Mean Teacher (MT, the exponential moving average of the history models) instead of BYOL's running towards it. The second term of RAFT explicitly prevents the representation collapse and thus makes RAFT a more conceptually reliable framework. We provide basic benchmarks of RAFT on CIFAR10 to validate the effectiveness of our method. Furthermore, we prove that BYOL is equivalent to RAFT under certain conditions, providing solid reasoning for BYOL's counter-intuitive success.", "one-sentence_summary": "We propose a new interpretable self-supervised approach RAFT solving the puzzle why BYOL works without collapse.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shi|run_away_from_your_teacher_a_new_selfsupervised_approach_solving_the_puzzle_of_byol", "pdf": "/pdf/db96bdbd92de6e910e733dfa0b89bc0a29b3dcc8.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=8vFBZNn211", "_bibtex": "@misc{\nshi2021run,\ntitle={Run Away From your Teacher: a New Self-Supervised Approach Solving the Puzzle of {\\{}BYOL{\\}}},\nauthor={Haizhou Shi and Dongliang Luo and Siliang Tang and Jian Wang and Yueting Zhuang},\nyear={2021},\nurl={https://openreview.net/forum?id=tij5dHg5Hk}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 12, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "SZWM3Zmn1-S", "original": null, "number": 1, "cdate": 1610040455431, "ddate": null, "tcdate": 1610040455431, "tmdate": 1610474058038, "tddate": null, "forum": "tij5dHg5Hk", "replyto": "tij5dHg5Hk", "invitation": "ICLR.cc/2021/Conference/Paper3680/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "Most of the reviewers and AC found many claims of this submission unsubstantiated. "}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Run Away From your Teacher: a New Self-Supervised Approach Solving the Puzzle of BYOL", "authorids": ["~Haizhou_Shi1", "~Dongliang_Luo1", "~Siliang_Tang1", "~Jian_Wang14", "~Yueting_Zhuang1"], "authors": ["Haizhou Shi", "Dongliang Luo", "Siliang Tang", "Jian Wang", "Yueting Zhuang"], "keywords": ["representation learning", "self-supervised learning", "contrastive learning", "regularization", "theory"], "abstract": "Recently, a newly proposed self-supervised framework Bootstrap Your Own Latent (BYOL) seriously challenges the necessity of negative samples in contrastive-based learning frameworks. BYOL works like a charm despite the fact that it discards the negative samples completely and there is no measure to prevent collapse in its training objective. In this paper, we suggest understanding BYOL from the view of our newly proposed interpretable self-supervised learning framework, Run Away From your Teacher (RAFT). RAFT optimizes two objectives at the same time: (i) aligning two views of the same data to similar representations and (ii) running away from the model's Mean Teacher (MT, the exponential moving average of the history models) instead of BYOL's running towards it. The second term of RAFT explicitly prevents the representation collapse and thus makes RAFT a more conceptually reliable framework. We provide basic benchmarks of RAFT on CIFAR10 to validate the effectiveness of our method. Furthermore, we prove that BYOL is equivalent to RAFT under certain conditions, providing solid reasoning for BYOL's counter-intuitive success.", "one-sentence_summary": "We propose a new interpretable self-supervised approach RAFT solving the puzzle why BYOL works without collapse.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shi|run_away_from_your_teacher_a_new_selfsupervised_approach_solving_the_puzzle_of_byol", "pdf": "/pdf/db96bdbd92de6e910e733dfa0b89bc0a29b3dcc8.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=8vFBZNn211", "_bibtex": "@misc{\nshi2021run,\ntitle={Run Away From your Teacher: a New Self-Supervised Approach Solving the Puzzle of {\\{}BYOL{\\}}},\nauthor={Haizhou Shi and Dongliang Luo and Siliang Tang and Jian Wang and Yueting Zhuang},\nyear={2021},\nurl={https://openreview.net/forum?id=tij5dHg5Hk}\n}"}, "tags": [], "invitation": {"reply": {"forum": "tij5dHg5Hk", "replyto": "tij5dHg5Hk", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040455417, "tmdate": 1610474058023, "id": "ICLR.cc/2021/Conference/Paper3680/-/Decision"}}}, {"id": "lVkhGm1JujR", "original": null, "number": 8, "cdate": 1605978367558, "ddate": null, "tcdate": 1605978367558, "tmdate": 1605978367558, "tddate": null, "forum": "tij5dHg5Hk", "replyto": "p40Znp-nO1_", "invitation": "ICLR.cc/2021/Conference/Paper3680/-/Official_Comment", "content": {"title": "Reply to R2", "comment": "We would like to thank R2 for the valuable questions and advice. We would like to respond to your concerns one by one in this post. \n\n## Concern 1\nThanks for pointing out that our paper\u2019s title oversells the contributions we made in our work. We sincerely apologize for that. While besides the title itself, we are meticulous in the statement we make in the main text of our paper: we avoid  claiming we solve the problem why BYOL works, and we further point out that RAFT is just \u201cconceptually working\u201d, but its success is also dependent on the predictor. As for the reason why our main focus is on the predictor instead of other elements, we would like to give an informal explanation. \n\n**Why we don\u2019t analyze BN in the predictor:** the blog post[1] which motivates the author of [2] states in its main content clearly shows that when the BN is removed from the predictor, BYOL continues to work (refer to Table \u201cPerformance for each variation\u201d). \n\n**Why we don\u2019t analyze BN in general:** firstly according to the appendix of the blog post[1], when the BN is both removed from the predictor and the projector, training for longer epochs would make the model recover. Secondly, the author of [2] also states in the openreview reply that the BN is just a \u201csufficient condition\u201d, which might not be crucial to the success of BYOL. Besides, the original author of BYOL also published another version of BYOL that doesn\u2019t require the batch statistics at all[3], indicating that BN might not be the most essential component. \n\n## Concern 2 \nWe don\u2019t focus on the relative performance between three algorithms we evaluate in the paper (BYOL, BYOL\u2019, and RAFT). However, the only comparison that\u2019s crucial to our conclusion is the comparison between the algorithms and the random baseline. We focus on whether the algorithm works or not, e.g. whether the representation collapse happens. RAFT is a more conceptually working algorithm lies in one analysis and one experimental result:\n\n**Analysis:** In Figure 2b (2a in the latest version), when two samples are mapped to have initial different gradient, the RAFT would separate them in the next couple of iterations. \n\n**Experimental Result:** In Table 1, the RAFT loss remains to be effective regularizing the alignment loss, which indicates that RAFT is more unified learning objective compared to BYOL and BYOL\u2019. \n\n## Concern 3\nDue to the limitation of our computational resources, we don\u2019t train our method as [4] in 1000 epochs. Instead we train them in 300 epochs, which could be told by the trend of the accuracy doesn\u2019t achieve the best performance since BYOL performs better when trained longer. More importantly as we stated in the response to Concern 2, we only care whether the algorithm outperforms the random baseline, since that\u2019s our main focus on explaining how BYOL avoids the representation collapse. \n\n## Concern 4\nThank you for pointing out that evaluating the algorithms by other large-scale datasets would make our work more solid. We are trying to gather more computational resources to evaluate our proposed RAFT and BYOL. However we would like to also point out that based on the experimental results of BYOL paper and our paper, the dataset would not change BYOL from collapse to non-collapse, nor in the other way. Thus in this respect, the dataset is not so crucial to our final conclusion. Again, we thank the reviewer for this advice and we would improve the solidness of our work in the near future. \n\n## Concern 5\nThank you for noticing this phenomenon (which shows that you understand our work from the alignment-uniformity framework, and that\u2019s what really makes us happy)! Unfortunately, this is also unclear to us. Our paper disentangles the analysis of BYOL into two separate parts: \n- When is BYOL approximately equivalent to BYOL\u2019?\n- How does the predictor help produce the representation uniformity? \nWhile these two problems are also challenging, our main contribution is that we provide empirical results supporting the legitimacy of these two parts. We don\u2019t expect that we completely solve them in a single paper, which means we leave the job of analyzing them to the future. \n\n\n[1] Abe Fetterman & Josh Albrecht. \u201cUnderstanding self-supervised and contrastive learning with \"Bootstrap Your Own Latent\" (BYOL).\u201d https://untitled-ai.github.io/understanding-self-supervised-contrastive-learning.html\n[2] Tian, Yuandong, et al. \"Understanding Self-supervised Learning with Dual Deep Networks.\" arXiv preprint arXiv:2010.00578 (2020).\n[3] Pierre H. Richemond et al. \u201cBYOL works even without batch statistics.\u201d arXiv preprint arXiv:2010.10241 \n[4] Ermolov, Aleksandr, et al. \"Whitening for Self-Supervised Representation Learning.\" arXiv preprint arXiv:2007.06346 (2020).\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3680/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3680/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Run Away From your Teacher: a New Self-Supervised Approach Solving the Puzzle of BYOL", "authorids": ["~Haizhou_Shi1", "~Dongliang_Luo1", "~Siliang_Tang1", "~Jian_Wang14", "~Yueting_Zhuang1"], "authors": ["Haizhou Shi", "Dongliang Luo", "Siliang Tang", "Jian Wang", "Yueting Zhuang"], "keywords": ["representation learning", "self-supervised learning", "contrastive learning", "regularization", "theory"], "abstract": "Recently, a newly proposed self-supervised framework Bootstrap Your Own Latent (BYOL) seriously challenges the necessity of negative samples in contrastive-based learning frameworks. BYOL works like a charm despite the fact that it discards the negative samples completely and there is no measure to prevent collapse in its training objective. In this paper, we suggest understanding BYOL from the view of our newly proposed interpretable self-supervised learning framework, Run Away From your Teacher (RAFT). RAFT optimizes two objectives at the same time: (i) aligning two views of the same data to similar representations and (ii) running away from the model's Mean Teacher (MT, the exponential moving average of the history models) instead of BYOL's running towards it. The second term of RAFT explicitly prevents the representation collapse and thus makes RAFT a more conceptually reliable framework. We provide basic benchmarks of RAFT on CIFAR10 to validate the effectiveness of our method. Furthermore, we prove that BYOL is equivalent to RAFT under certain conditions, providing solid reasoning for BYOL's counter-intuitive success.", "one-sentence_summary": "We propose a new interpretable self-supervised approach RAFT solving the puzzle why BYOL works without collapse.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shi|run_away_from_your_teacher_a_new_selfsupervised_approach_solving_the_puzzle_of_byol", "pdf": "/pdf/db96bdbd92de6e910e733dfa0b89bc0a29b3dcc8.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=8vFBZNn211", "_bibtex": "@misc{\nshi2021run,\ntitle={Run Away From your Teacher: a New Self-Supervised Approach Solving the Puzzle of {\\{}BYOL{\\}}},\nauthor={Haizhou Shi and Dongliang Luo and Siliang Tang and Jian Wang and Yueting Zhuang},\nyear={2021},\nurl={https://openreview.net/forum?id=tij5dHg5Hk}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "tij5dHg5Hk", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3680/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3680/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3680/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3680/Authors|ICLR.cc/2021/Conference/Paper3680/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3680/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923834992, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3680/-/Official_Comment"}}}, {"id": "73rFsHqP6Bu", "original": null, "number": 7, "cdate": 1605977670241, "ddate": null, "tcdate": 1605977670241, "tmdate": 1605977670241, "tddate": null, "forum": "tij5dHg5Hk", "replyto": "JGO_OAVdEQU", "invitation": "ICLR.cc/2021/Conference/Paper3680/-/Official_Comment", "content": {"title": "Reply to R3 (part 3/3)", "comment": "### Result (4)\nFirstly, thank you for the advice that adopting other datasets other than CIFAR10 would increase the reliability of our experimental results and we are aware of that. We are currently actively searching for more computational resources to validate the effectiveness of our RAFT algorithm, while we would like to point out that the data distribution (dataset) will not change whether a method collapses or not. If any, as mentioned by the reviewer, the poor quality of CIFAR10 (\u201cfew classes, small images, few discriminative features\u201d) only increases the difficulty for an algorithm to work. Most importantly, being aware of the shortcoming of the dataset, we never claim the superiority of the RAFT in terms of its better capability generating good-quality representations. Instead, the RAFT loss is more unified from the perspective of algorithm designing: we know alignment and uniformity are two terms regularizing each other, and we want to leverage the EMA of the history models, would we choose to fit to it or run away from it? The answer is pretty clear. \n\nSecondly, the same logic applies when we address the concern you raised: \u201cBYOL was not correctly tuned.\u201d The optimizer doesn\u2019t affect the property whether the model collapses or not, which is demonstrated by the BYOL original paper and our experimental results. The cosine decay trick is only crucial in terms of the final performance: with or without it, the model effectively works better than the random baseline. Therefore we don\u2019t consider discussing them as we focus on the essential component making BYOL avoid collapse. \n\n### Shortcut (1)\nWe sincerely apologize if our phrasing causes your misunderstanding on our claim that \"predictor is a dissatisfactory property of BYOL\". When we say \"dissatisfactory\" we mean that the current explanation on BYOL doesn't consider the predictor seriously. By making this argument, we want to emphasize the importance of the predictor and what we did in this work was to at least show the efficacy of it with respect to the fact that the predictor helps establish the equivalence between BYOL' and RAFT. One main contribution of our work is to emphasize that there are some additional unexpected effects brought by this predictor, and the equivalence between BYOL\u2019 and RAFT is one of them. Analyzing the efficacy of the predictor is a must if we want to fully understand how BYOL works. \n\n### Shortcut (2)\nWe acknowledge that the approximate equivalence between BYOL and BYOL\u2019 is supported only by the empirical study. While given the similarity between the two losses and the upper bounding relation, the closeness between the two is somewhat obvious. We would like to explore under what condition this equivalence holds in the near future. \n\n### Shortcut (3)\nIn our paper, our claim is made on the BYOL\u2019, which is composed of two loss terms, but not BYOL itself. Based on the previous work of alignment-uniformity framework, that the alignment is regularized by the uniformity objective, our claim that cross-model term regularizes the alignment term is self-contained. \n\n### Shortcut (4)\nThe RAFT loss is better than BYOL in terms of leveraging the MT because when the predictor is removed, running away from MT remains to be an \u201ceffective regularizer\u201d for the alignment loss, thus is a more unified method compared to BYOL and BYOL\u2019.\n\n## About Writing\nAgain, we are extremely sorry if our writing disturbs you. Our intention of writing this paper was to put all the materials in the main text. Unfortunately, due to the limitation of length, we have to put some of the empirical evidence and theoretical proof to the appendix. We will upload another version of our work to arxiv that properly addresses our writing problem. Besides, we only mention the word \u201cprojector\u201d in the appendix, never in the conclusion. We believe the reviewer\u2019s accusation of our discussing unpublished results is unintentional and we fully understand it. \n\nTo summarize, we are grateful that R3 provides so many important reviews and questions. However, we sincerely request a full re-evaluation of our work after our clarification on the misunderstanding. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3680/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3680/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Run Away From your Teacher: a New Self-Supervised Approach Solving the Puzzle of BYOL", "authorids": ["~Haizhou_Shi1", "~Dongliang_Luo1", "~Siliang_Tang1", "~Jian_Wang14", "~Yueting_Zhuang1"], "authors": ["Haizhou Shi", "Dongliang Luo", "Siliang Tang", "Jian Wang", "Yueting Zhuang"], "keywords": ["representation learning", "self-supervised learning", "contrastive learning", "regularization", "theory"], "abstract": "Recently, a newly proposed self-supervised framework Bootstrap Your Own Latent (BYOL) seriously challenges the necessity of negative samples in contrastive-based learning frameworks. BYOL works like a charm despite the fact that it discards the negative samples completely and there is no measure to prevent collapse in its training objective. In this paper, we suggest understanding BYOL from the view of our newly proposed interpretable self-supervised learning framework, Run Away From your Teacher (RAFT). RAFT optimizes two objectives at the same time: (i) aligning two views of the same data to similar representations and (ii) running away from the model's Mean Teacher (MT, the exponential moving average of the history models) instead of BYOL's running towards it. The second term of RAFT explicitly prevents the representation collapse and thus makes RAFT a more conceptually reliable framework. We provide basic benchmarks of RAFT on CIFAR10 to validate the effectiveness of our method. Furthermore, we prove that BYOL is equivalent to RAFT under certain conditions, providing solid reasoning for BYOL's counter-intuitive success.", "one-sentence_summary": "We propose a new interpretable self-supervised approach RAFT solving the puzzle why BYOL works without collapse.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shi|run_away_from_your_teacher_a_new_selfsupervised_approach_solving_the_puzzle_of_byol", "pdf": "/pdf/db96bdbd92de6e910e733dfa0b89bc0a29b3dcc8.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=8vFBZNn211", "_bibtex": "@misc{\nshi2021run,\ntitle={Run Away From your Teacher: a New Self-Supervised Approach Solving the Puzzle of {\\{}BYOL{\\}}},\nauthor={Haizhou Shi and Dongliang Luo and Siliang Tang and Jian Wang and Yueting Zhuang},\nyear={2021},\nurl={https://openreview.net/forum?id=tij5dHg5Hk}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "tij5dHg5Hk", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3680/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3680/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3680/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3680/Authors|ICLR.cc/2021/Conference/Paper3680/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3680/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923834992, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3680/-/Official_Comment"}}}, {"id": "xeNO-cW0aAP", "original": null, "number": 6, "cdate": 1605977185917, "ddate": null, "tcdate": 1605977185917, "tmdate": 1605977185917, "tddate": null, "forum": "tij5dHg5Hk", "replyto": "JGO_OAVdEQU", "invitation": "ICLR.cc/2021/Conference/Paper3680/-/Official_Comment", "content": {"title": "Reply to R3 (part 2/3)", "comment": "## Response to the Valuable Advice & Other Comments\nWe would like to address the concerns raised by R3 in the \u201cResults\u201d and \u201cShortcuts\u201d section. As for the \u201cWriting\u201d, we apologize if our phrasing somehow disturbs you. We will try to mild our excitement of finding that optimizing two opposite losses would yield the same effect and shift to a more formal language. Below we list the summarization and the response, please correct us if we misunderstand your advice.  \n\n### Result (1)\n**Review.** The reviewer summarizes the BYOL\u2019 upper bound loss as the distance between the \u201cprojection and the projector\u201d. The reviewer thinks that BYOL itself doesn\u2019t minimize a loss due to the stop gradient, which the reviewer thinks is supported by the non-stationary distribution of the target network and the non-convergence observed in BYOL. Therefore, the reviewer concludes that analyzing and altering the loss would be wrong, which is supported by our visualization results in appendix F.1 and offers us a better direction of analyzing BYOL: gradient.\n\n**Reply.** Firstly, the reviewer wrongly summarizes the BYOL\u2019 upper bound loss. In fact, the BYOL\u2019 loss is composed of two objectives: (i) attracting the representations of the two views of the same data after the predictor and (ii) repealing the online and the MT under the same data distribution. \n\nSecondly, it\u2019s hard to understand the point of the reviewer\u2019s claim that \u201cBYOL doesn\u2019t minimize a loss\u201d. What\u2019s more, the reviewer\u2019s theory \u201csince the target network is non-stationary, BYOL doesn\u2019t minimize a loss\u201d is just stating a conditional phenomenon observed by the researcher, but is a completely wrong causal inference: if we remove the predictor from BYOL, the target network remains \u201cnon-stationary\u201d, but BYOL indeed minimizes a loss: the loss of BYOL quickly goes to zero and follows the representation collapse (refer to our result in appendix). Our way of analyzing BYOL starts from observing the two quantifiable metrics that have been shown crucial to the contrastive-based methods: alignment and uniformity. Though BYOL does not explicitly optimize them, we find that these two metrics are indeed optimized during training by estimating them. The whole point of analyzing BYOL lies in how we can relate its training objective to the alignment-uniformity framework, and approximately equating BYOL and BYOL\u2019 is empirically supported, but not theoretically. We would like to discuss under what condition this approximation holds in the future, while in this paper our main focus is to provide an overall understanding framework for BYOL. \n\nThirdly, we would like to point out, negating the claim that BYOL is approximately equivalent to BYOL\u2019 by the qualitative results presented in F.2 is itself a shortcut: retraining the neural network on the same dataset with different random seed would yield the close performance while probably different qualitative results. The reason why we present the qualitative results in the appendix is that we would like to show the apparent difference between the collapsed methods and the working one. \n\nAt last, we agree that analyzing BYOL from the perspective of the gradient would be a good direction, while our approximating and redesigning the loss function would consequently cause effects on the gradient. There is no contradiction between these two methods. We would like to investigate the problem with your advice in the near future. \n\n### Result (2)\nSince the representation space is a hypersphere, the concentration and separation of the data samples are mainly influenced by the tangential component of the gradient. This condition is trivial and easy to satisfy. Suppose the unit vector $z$ is the representation produced by the online network and the unit vector $\\bar{z}$ is the representation produced by the MT. The original loss is: \n\n\\begin{align}\n\\mathcal L = \\big|\\big| z - \\bar{z} \\big|\\big|_2^2\n\\end{align}\n\nAfter applying the condition using simple mathematical techniques, the loss is changed to:\n\n\\begin{align}\n\\mathcal L = \\frac{1}{\\langle z, \\bar{z}\\rangle} \\big|\\big| z\\langle z,\\bar{z}\\rangle - \\bar{z} \\big|\\big|_2^2\n\\end{align}\n\nWhere the inner-product $\\langle z, \\bar{z} \\rangle$ is a scalar and doesn\u2019t generate any gradient. Our additional experiments show that this condition also doesn\u2019t affect whether the algorithm would collapse. Please refer to our latest version of the paper.\n\n### Result (3)\nOur claim \u201cthe predictor is essential to the collapse prevention of BYOL\u201d is based on the observation that when the predictor is removed, the collapse happens. Other factors are also important to the final quality of the representation distribution, while they do not essentially affect whether the algorithm would collapse, which is also supported by the original BYOL paper Table.5b[1]. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3680/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3680/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Run Away From your Teacher: a New Self-Supervised Approach Solving the Puzzle of BYOL", "authorids": ["~Haizhou_Shi1", "~Dongliang_Luo1", "~Siliang_Tang1", "~Jian_Wang14", "~Yueting_Zhuang1"], "authors": ["Haizhou Shi", "Dongliang Luo", "Siliang Tang", "Jian Wang", "Yueting Zhuang"], "keywords": ["representation learning", "self-supervised learning", "contrastive learning", "regularization", "theory"], "abstract": "Recently, a newly proposed self-supervised framework Bootstrap Your Own Latent (BYOL) seriously challenges the necessity of negative samples in contrastive-based learning frameworks. BYOL works like a charm despite the fact that it discards the negative samples completely and there is no measure to prevent collapse in its training objective. In this paper, we suggest understanding BYOL from the view of our newly proposed interpretable self-supervised learning framework, Run Away From your Teacher (RAFT). RAFT optimizes two objectives at the same time: (i) aligning two views of the same data to similar representations and (ii) running away from the model's Mean Teacher (MT, the exponential moving average of the history models) instead of BYOL's running towards it. The second term of RAFT explicitly prevents the representation collapse and thus makes RAFT a more conceptually reliable framework. We provide basic benchmarks of RAFT on CIFAR10 to validate the effectiveness of our method. Furthermore, we prove that BYOL is equivalent to RAFT under certain conditions, providing solid reasoning for BYOL's counter-intuitive success.", "one-sentence_summary": "We propose a new interpretable self-supervised approach RAFT solving the puzzle why BYOL works without collapse.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shi|run_away_from_your_teacher_a_new_selfsupervised_approach_solving_the_puzzle_of_byol", "pdf": "/pdf/db96bdbd92de6e910e733dfa0b89bc0a29b3dcc8.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=8vFBZNn211", "_bibtex": "@misc{\nshi2021run,\ntitle={Run Away From your Teacher: a New Self-Supervised Approach Solving the Puzzle of {\\{}BYOL{\\}}},\nauthor={Haizhou Shi and Dongliang Luo and Siliang Tang and Jian Wang and Yueting Zhuang},\nyear={2021},\nurl={https://openreview.net/forum?id=tij5dHg5Hk}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "tij5dHg5Hk", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3680/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3680/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3680/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3680/Authors|ICLR.cc/2021/Conference/Paper3680/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3680/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923834992, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3680/-/Official_Comment"}}}, {"id": "-2IiV-3N9s3", "original": null, "number": 5, "cdate": 1605975192246, "ddate": null, "tcdate": 1605975192246, "tmdate": 1605975219772, "tddate": null, "forum": "tij5dHg5Hk", "replyto": "JGO_OAVdEQU", "invitation": "ICLR.cc/2021/Conference/Paper3680/-/Official_Comment", "content": {"title": "Reply to R3 (part 1/3)", "comment": "Dear R3, we appreciate the patience of you reading our paper and giving such detailed feedback. There is much helpful advice and many constructive ideas in your review. **However, we feel sorry that your judgement of our work may be conditioned on the misunderstanding of the evaluation metric \u201clinear evaluation protocol\u201d in the self-supervised learning field (and other reviewers seem not having the similar concern).** We would like to address your concerns in our following reply. \n\n## Linear Evaluation Protocol and Random Baseline \nWe are somewhat concerned about the fact that the reviewer doesn\u2019t understand the widely used  evaluation metric in self-supervised learning, which is reflected by his/her misunderstanding of \u201crandom baseline\u201d. In the \u201cWriting\u201d part, the reviewer writes, \n> In section 3, random is ill-defined. In CIFAR10, random should be 10%, I assume that you refer to random projection. Please clarify.\n\nHere we would like to clarify why the \u201crandom baseline\u201d is not 10%.\n\nMost of work in the field of self-supervised learning adopts the evaluation metric called \u201clinear evaluation protocol\u201d to estimate the quality of the representation distribution. Normally, after training under the pretext task, we would yield an encoder network (or feature extractor). To evaluate how well this encoder network is, we fix the weights of it, and then add a linear layer on top of the encoder to train a classifier on the labeled dataset. The point of the linear evaluation protocol is to see whether the data of the same class can be mapped to the representation space so that they could be easily identified. Therefore if we randomly initialize a network and evaluate it under the linear evaluation protocol, we would normally yield better classification accuracy than the RANDOM CLASSIFIER (which has 10% of accuracy on CIFAR10), due to the natural pixel-level intra-class similarity. In our paper, we clearly stated the setting at the beginning of Section 3: \n> The performance of BYOL original model, whose predictor $q_w$ is a two-layer MLP with batch normalization, evaluated on the linear evaluation protocol (Kolesnikov et al., 2019; Kornblith et al., 2019; Chen et al., 2020a; He et al., 2020; Grill et al., 2020) reaches 68.08 \u00b1 0.84%. When the predictor is removed, the performance degenerates to 20.92 \u00b1 1.29%, which is even lower than the random baseline\u2019s 42.74 \u00b1 0.41%.\n\nSome may argue that the misunderstanding of the evaluation metric is caused by our poor writing, while in the section 3 of the original paper of BYOL[1], the author\u2019s description is of the same style of ours, which can\u2019t be simpler and clearer:\n> We apply this procedure by predicting a fixed randomly initialized network achieves 18.8% top-1 accuracy (Table 5a) on the linear evaluation protocol on ImageNet, whereas the randomly initialized network only achieves 1.4% by itself.\n\nAccording to the reviewer, since the ImageNet is of 1000-class classification task, then the random baseline should have 0.1% of accuracy instead of 1.4%, while in reality it's not the case. \n\nThe basic understanding of the evaluation metric is fundamental to fair evaluation. In our case, misunderstanding the linear evaluation protocol would lead to misunderstanding the concept of the representation collapse, which is more crucial to rating our contributions. And this consequential misunderstanding is also reflected in the reviewer No.3\u2019s response. \nIn the \u201cResults\u201d part, the reviewer writes, \n> Table D.3 shows that RAFT/BYOL' does not collapse without predictors when $\\beta$ is high. Albeit providing low accuracy, a non-collapse is quite surprising. Unfortunately, the authors leave it for future work. \n\nIn fact, in our paper, we introduce the alignment-uniformity framework[2] to readers to understand the concept of representation collapse. The representation collapse means that most of the data are mapped to the same meaningless point, which can be reflected by the metric of uniformity. When the predictor is removed, BYOL, BYOL\u2019 and RAFT are all collapsed since their uniformity loss is much higher than the random baseline: -0.14, -0.10, and -0.006 respectively, while the random baseline even has -0.51 of uniformity. \n\nWe believe that the basic understanding of the linear evaluation protocol and the representation collapse is crucial to the objectiveness of the review. And therefore we sincerely hope that the reviewer could re-evaluate our work after reading our reply.\n\n[1] Jean-Bastien Grill, Florian Strub, Florent Altche \u0301, Corentin Tallec, Pierre H Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own latent: A new approach to self-supervised learning. arXiv preprint arXiv:2006.07733, 2020.\n\n[2] Tongzhou Wang and Phillip Isola. Understanding contrastive representation learning through alignment and uniformity on the hypersphere. arXiv preprint arXiv:2005.10242, 2020. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3680/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3680/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Run Away From your Teacher: a New Self-Supervised Approach Solving the Puzzle of BYOL", "authorids": ["~Haizhou_Shi1", "~Dongliang_Luo1", "~Siliang_Tang1", "~Jian_Wang14", "~Yueting_Zhuang1"], "authors": ["Haizhou Shi", "Dongliang Luo", "Siliang Tang", "Jian Wang", "Yueting Zhuang"], "keywords": ["representation learning", "self-supervised learning", "contrastive learning", "regularization", "theory"], "abstract": "Recently, a newly proposed self-supervised framework Bootstrap Your Own Latent (BYOL) seriously challenges the necessity of negative samples in contrastive-based learning frameworks. BYOL works like a charm despite the fact that it discards the negative samples completely and there is no measure to prevent collapse in its training objective. In this paper, we suggest understanding BYOL from the view of our newly proposed interpretable self-supervised learning framework, Run Away From your Teacher (RAFT). RAFT optimizes two objectives at the same time: (i) aligning two views of the same data to similar representations and (ii) running away from the model's Mean Teacher (MT, the exponential moving average of the history models) instead of BYOL's running towards it. The second term of RAFT explicitly prevents the representation collapse and thus makes RAFT a more conceptually reliable framework. We provide basic benchmarks of RAFT on CIFAR10 to validate the effectiveness of our method. Furthermore, we prove that BYOL is equivalent to RAFT under certain conditions, providing solid reasoning for BYOL's counter-intuitive success.", "one-sentence_summary": "We propose a new interpretable self-supervised approach RAFT solving the puzzle why BYOL works without collapse.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shi|run_away_from_your_teacher_a_new_selfsupervised_approach_solving_the_puzzle_of_byol", "pdf": "/pdf/db96bdbd92de6e910e733dfa0b89bc0a29b3dcc8.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=8vFBZNn211", "_bibtex": "@misc{\nshi2021run,\ntitle={Run Away From your Teacher: a New Self-Supervised Approach Solving the Puzzle of {\\{}BYOL{\\}}},\nauthor={Haizhou Shi and Dongliang Luo and Siliang Tang and Jian Wang and Yueting Zhuang},\nyear={2021},\nurl={https://openreview.net/forum?id=tij5dHg5Hk}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "tij5dHg5Hk", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3680/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3680/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3680/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3680/Authors|ICLR.cc/2021/Conference/Paper3680/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3680/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923834992, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3680/-/Official_Comment"}}}, {"id": "8DkYJ2f1tcR", "original": null, "number": 4, "cdate": 1605973840238, "ddate": null, "tcdate": 1605973840238, "tmdate": 1605973840238, "tddate": null, "forum": "tij5dHg5Hk", "replyto": "Hbfpvs6x3b9", "invitation": "ICLR.cc/2021/Conference/Paper3680/-/Official_Comment", "content": {"title": "Reply to R4 (part 2/2)", "comment": "## Question 2\n\nWe sincerely apologize if our phrasing causes your misunderstanding on our claim that \"predictor is a dissatisfactory property of BYOL\". When we say \"dissatisfactory\" we mean that the current explanation on BYOL doesn't consider the predictor seriously. By making this argument, we want to emphasize the importance of the predictor and what we did in this work was to at least show the efficacy of it with respect to the fact that the predictor helps establish the equivalence between BYOL' and RAFT.\n\nWe agree that RAFT doesn\u2019t completely solve the problem of collapse when the predictor is removed. Our claim that RAFT is a more unified objective (refer to the Figure.2(b) in the latest version of our paper) and is thus more \u201cessential\u201d is based on the previous work of alignment-uniformity framework[2], which demonstrates that the alignment loss and the uniformity loss are two competing factors regularizing each other. Evaluate the role of RAFT and BYOL\u2019 from the perspective of algorithm designing: suppose you want to use another term to constrain the alignment loss which incorporates the Mean Teacher, would you choose to fit to it or run away from it? RAFT loss remains to be an effective regularizer when the predictor is removed, while BYOL\u2019 fails to do so, which implies that RAFT is more favorable. \n\nYes, this conclusion still has potential to be improved by enabling the framework to work when the predictor is removed, while demanding our single paper to solve all the problems would be unfair, let alone under the 8-page limitation. Our further analysis on the efficacy of the predictor focuses on the equivalence between BYOL\u2019 and RAFT, which makes two crucial contributions: \nThe importance of the predictor lies in at least equating the two opposite training objectives. And we emphasize the efficacy of the predictor needs to be further studied. \n\nThere are multiple factors entangled in BYOL. Our contribution provides a novel view to investigate it. Under our framework, the direction of analyzing BYOL is much clearer than before: two separate questions wait to be answered in the future: \n- Under what condition BYOL is equivalent to BYOL\u2019? \n- How does the predictor help optimizing the uniformity of the representation in RAFT?\n\nWe hope our explanation on the RAFT loss and our framework of understanding the working mechanism behind BYOL would provide a new direction of leveraging the MT in the future. We would also like to thank R4 for the two valuable comments, which we think would be helpful to our future study. \n\n[2] Tongzhou Wang and Phillip Isola. Understanding contrastive representation learning through alignment and uniformity on the hypersphere. arXiv preprint arXiv:2005.10242, 2020."}, "signatures": ["ICLR.cc/2021/Conference/Paper3680/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3680/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Run Away From your Teacher: a New Self-Supervised Approach Solving the Puzzle of BYOL", "authorids": ["~Haizhou_Shi1", "~Dongliang_Luo1", "~Siliang_Tang1", "~Jian_Wang14", "~Yueting_Zhuang1"], "authors": ["Haizhou Shi", "Dongliang Luo", "Siliang Tang", "Jian Wang", "Yueting Zhuang"], "keywords": ["representation learning", "self-supervised learning", "contrastive learning", "regularization", "theory"], "abstract": "Recently, a newly proposed self-supervised framework Bootstrap Your Own Latent (BYOL) seriously challenges the necessity of negative samples in contrastive-based learning frameworks. BYOL works like a charm despite the fact that it discards the negative samples completely and there is no measure to prevent collapse in its training objective. In this paper, we suggest understanding BYOL from the view of our newly proposed interpretable self-supervised learning framework, Run Away From your Teacher (RAFT). RAFT optimizes two objectives at the same time: (i) aligning two views of the same data to similar representations and (ii) running away from the model's Mean Teacher (MT, the exponential moving average of the history models) instead of BYOL's running towards it. The second term of RAFT explicitly prevents the representation collapse and thus makes RAFT a more conceptually reliable framework. We provide basic benchmarks of RAFT on CIFAR10 to validate the effectiveness of our method. Furthermore, we prove that BYOL is equivalent to RAFT under certain conditions, providing solid reasoning for BYOL's counter-intuitive success.", "one-sentence_summary": "We propose a new interpretable self-supervised approach RAFT solving the puzzle why BYOL works without collapse.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shi|run_away_from_your_teacher_a_new_selfsupervised_approach_solving_the_puzzle_of_byol", "pdf": "/pdf/db96bdbd92de6e910e733dfa0b89bc0a29b3dcc8.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=8vFBZNn211", "_bibtex": "@misc{\nshi2021run,\ntitle={Run Away From your Teacher: a New Self-Supervised Approach Solving the Puzzle of {\\{}BYOL{\\}}},\nauthor={Haizhou Shi and Dongliang Luo and Siliang Tang and Jian Wang and Yueting Zhuang},\nyear={2021},\nurl={https://openreview.net/forum?id=tij5dHg5Hk}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "tij5dHg5Hk", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3680/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3680/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3680/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3680/Authors|ICLR.cc/2021/Conference/Paper3680/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3680/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923834992, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3680/-/Official_Comment"}}}, {"id": "Q2qedEYua-Q", "original": null, "number": 3, "cdate": 1605972402621, "ddate": null, "tcdate": 1605972402621, "tmdate": 1605972402621, "tddate": null, "forum": "tij5dHg5Hk", "replyto": "Hbfpvs6x3b9", "invitation": "ICLR.cc/2021/Conference/Paper3680/-/Official_Comment", "content": {"title": "Reply to R4 (part 1/2)", "comment": "Thank you for the valuable advice. It\u2019s sad that you only noticed the weaknesses of our paper and ignored all the contributions we\u2019ve made. We would like to address your major concerns through this reply and recapitulate our contribution. \n\n## Question 1\n\n**Question.** The reviewer thinks we are unaware of the difference between contrasting two samples (\u201ccross-sample\u201d) and contrasting two functions (our \u201ccross-model\u201d loss). Then the reviewer argues that contrasting two functions is not capable of collapse prevention by giving a corner case where the weight of the matrix is initialized to zero. The point of this \u201czero-initialized\u201d example is to emphasize that special care is required when dealing with the cross-model term. \n\n**Reply.** Firstly, the example given is baseless and overly critical. As known by every practitioner in deep learning, neural networks require a reasonable initialization scheme, and zero-initialization often completely disables a network. The reviewer\u2019s reasoning is applicable to attacking any form of losses, even BYOL itself: if there is an intermediate layer whose weight is zero in the BYOL, then the loss would be a zero constant and nothing would be learned during training. \n\nWe understand the reviewer\u2019s concern and we are prudent with our conclusions. There are some achievable conditions required when we claim that maximizing the cross-model term could prevent collapse, and the randomness of the initial representation distribution is one of them. What\u2019s more, we shall emphasize that maximizing the cross-model loss is not unconditionally equivalent to contrasting two samples in the representation space, and we avoid claiming it in the paper. We argue that only when the EMA is considered in the target network, the cross-model loss is able to contrast samples. In section 4.2, we leverage the conclusion in the Mean Teacher[1], which is also mentioned in R3, to bridge the gap between the sample averaging and model averaging: \n> There has been a lot of work demonstrating that weight averaging is roughly equal to sample averaging[1], thus if two samples\u2019 representations are close to each other at the beginning and their initial updating directions are opposite, then RAFT consistently separates them in the representation space.\n\n[1] Antti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. In Advances in neural information processing systems, pp. 1195\u20131204, 2017.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3680/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3680/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Run Away From your Teacher: a New Self-Supervised Approach Solving the Puzzle of BYOL", "authorids": ["~Haizhou_Shi1", "~Dongliang_Luo1", "~Siliang_Tang1", "~Jian_Wang14", "~Yueting_Zhuang1"], "authors": ["Haizhou Shi", "Dongliang Luo", "Siliang Tang", "Jian Wang", "Yueting Zhuang"], "keywords": ["representation learning", "self-supervised learning", "contrastive learning", "regularization", "theory"], "abstract": "Recently, a newly proposed self-supervised framework Bootstrap Your Own Latent (BYOL) seriously challenges the necessity of negative samples in contrastive-based learning frameworks. BYOL works like a charm despite the fact that it discards the negative samples completely and there is no measure to prevent collapse in its training objective. In this paper, we suggest understanding BYOL from the view of our newly proposed interpretable self-supervised learning framework, Run Away From your Teacher (RAFT). RAFT optimizes two objectives at the same time: (i) aligning two views of the same data to similar representations and (ii) running away from the model's Mean Teacher (MT, the exponential moving average of the history models) instead of BYOL's running towards it. The second term of RAFT explicitly prevents the representation collapse and thus makes RAFT a more conceptually reliable framework. We provide basic benchmarks of RAFT on CIFAR10 to validate the effectiveness of our method. Furthermore, we prove that BYOL is equivalent to RAFT under certain conditions, providing solid reasoning for BYOL's counter-intuitive success.", "one-sentence_summary": "We propose a new interpretable self-supervised approach RAFT solving the puzzle why BYOL works without collapse.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shi|run_away_from_your_teacher_a_new_selfsupervised_approach_solving_the_puzzle_of_byol", "pdf": "/pdf/db96bdbd92de6e910e733dfa0b89bc0a29b3dcc8.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=8vFBZNn211", "_bibtex": "@misc{\nshi2021run,\ntitle={Run Away From your Teacher: a New Self-Supervised Approach Solving the Puzzle of {\\{}BYOL{\\}}},\nauthor={Haizhou Shi and Dongliang Luo and Siliang Tang and Jian Wang and Yueting Zhuang},\nyear={2021},\nurl={https://openreview.net/forum?id=tij5dHg5Hk}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "tij5dHg5Hk", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3680/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3680/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3680/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3680/Authors|ICLR.cc/2021/Conference/Paper3680/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3680/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923834992, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3680/-/Official_Comment"}}}, {"id": "M5nKi6k4eZ", "original": null, "number": 2, "cdate": 1605971352989, "ddate": null, "tcdate": 1605971352989, "tmdate": 1605971352989, "tddate": null, "forum": "tij5dHg5Hk", "replyto": "bLQo-ZBk1eq", "invitation": "ICLR.cc/2021/Conference/Paper3680/-/Official_Comment", "content": {"title": "Reply to R1", "comment": "Dear R1, \n\nThank you for the kind and constructive comments. We are also aware of your concern whether RAFT is comparable to the SOTA methods when applied to the large-scale datasets such as ImageNet. The main point of our paper, however, focuses on why BYOL doesn\u2019t collapse. Larger datasets evaluate the effectiveness of the algorithm, while using smaller one is sufficient to demonstrate whether the algorithm would collapse or not: if our algorithm performs better than the random baseline and close to the BYOL, then we are confident to claim it\u2019s a non-collapsing framework. On the other hand, if the algorithm fails (by fail, we mean worse than the random baseline) on CIFAR10, then even if it works on ImageNet, we would still regard it as flawed since it heavily depends on the data distribution. As for evaluating RAFT on ImageNet, we are trying to gather more computational resources to validate our proposed method, although we wouldn\u2019t view RAFT\u2019s effectiveness as the all-important contribution of this work.\n\nBeing aware of the limitation brought by the dataset, we avoid claiming that RAFT is better than BYOL in terms of its performance on the linear evaluation protocol. The value of our work lies in the attempt of subsuming BYOL into the already-verified alignment-uniformity framework, and jumping out of the current understanding frameworks originally provided by BYOL under mild conditions, including Teacher-Student framework, DQN\u2019s Online-Target framework and Mean Teacher in semi-supervised learning. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3680/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3680/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Run Away From your Teacher: a New Self-Supervised Approach Solving the Puzzle of BYOL", "authorids": ["~Haizhou_Shi1", "~Dongliang_Luo1", "~Siliang_Tang1", "~Jian_Wang14", "~Yueting_Zhuang1"], "authors": ["Haizhou Shi", "Dongliang Luo", "Siliang Tang", "Jian Wang", "Yueting Zhuang"], "keywords": ["representation learning", "self-supervised learning", "contrastive learning", "regularization", "theory"], "abstract": "Recently, a newly proposed self-supervised framework Bootstrap Your Own Latent (BYOL) seriously challenges the necessity of negative samples in contrastive-based learning frameworks. BYOL works like a charm despite the fact that it discards the negative samples completely and there is no measure to prevent collapse in its training objective. In this paper, we suggest understanding BYOL from the view of our newly proposed interpretable self-supervised learning framework, Run Away From your Teacher (RAFT). RAFT optimizes two objectives at the same time: (i) aligning two views of the same data to similar representations and (ii) running away from the model's Mean Teacher (MT, the exponential moving average of the history models) instead of BYOL's running towards it. The second term of RAFT explicitly prevents the representation collapse and thus makes RAFT a more conceptually reliable framework. We provide basic benchmarks of RAFT on CIFAR10 to validate the effectiveness of our method. Furthermore, we prove that BYOL is equivalent to RAFT under certain conditions, providing solid reasoning for BYOL's counter-intuitive success.", "one-sentence_summary": "We propose a new interpretable self-supervised approach RAFT solving the puzzle why BYOL works without collapse.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shi|run_away_from_your_teacher_a_new_selfsupervised_approach_solving_the_puzzle_of_byol", "pdf": "/pdf/db96bdbd92de6e910e733dfa0b89bc0a29b3dcc8.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=8vFBZNn211", "_bibtex": "@misc{\nshi2021run,\ntitle={Run Away From your Teacher: a New Self-Supervised Approach Solving the Puzzle of {\\{}BYOL{\\}}},\nauthor={Haizhou Shi and Dongliang Luo and Siliang Tang and Jian Wang and Yueting Zhuang},\nyear={2021},\nurl={https://openreview.net/forum?id=tij5dHg5Hk}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "tij5dHg5Hk", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3680/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3680/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3680/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3680/Authors|ICLR.cc/2021/Conference/Paper3680/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3680/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923834992, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3680/-/Official_Comment"}}}, {"id": "p40Znp-nO1_", "original": null, "number": 1, "cdate": 1603456785303, "ddate": null, "tcdate": 1603456785303, "tmdate": 1605023957021, "tddate": null, "forum": "tij5dHg5Hk", "replyto": "tij5dHg5Hk", "invitation": "ICLR.cc/2021/Conference/Paper3680/-/Official_Review", "content": {"title": "Review2", "review": "This paper mainly proposes an objective that incorporates the target network in BYOL in a opposite way that encourages the prediction of the online network to be far away from the target network.\n\nConcerns:\n\n1.  It is overly claimed that \"we unravel the puzzle of how BYOL avoids representation collapse\". For example, the authors fail to capture some intrinsic properties of BYOL, such as the role of prediction head (MLP + BN).  The theorem 5.1 can only deal with the prediction head that is linear. The authors could refer to the [1] for more insights.\n\n2. The experimental results do not support the claim that \"RAFT is a conceptually non-collapsing algorithm\". In table 1, for results equipped with q_{w}-MLP, RAPT with better acc (71.31) in fact does not have smaller uniform loss. Instead, its alignment loss is smaller (which brings the acc improvement). So, the RAFT does not actually always enlarge the uniformity as claimed.\n \n3. The implementation of BYOL in this paper regarding Cifar10 is not convincing. [2] also use the resnet18 as the encoder and it achieves the accuracy with 91+ in Cifar10. However, the reproduced result in this paper is only around 70. \n\n4. BYOL proves its own effectiveness in ImageNet. To make fair comparisons, the authors shall conduct experiments in the same dataset. Otherwise, the claim regarding to the BYOL might not be solid.\n\n5. It is acceptable that running away from the mean teacher increases the difficulty of alignment. But it is unclear to the reviewer why it can produce global uniformity in the representation space?\n\n[1] Tian, Yuandong, et al. \"Understanding Self-supervised Learning with Dual Deep Networks.\" arXiv preprint arXiv:2010.00578 (2020).\n\n[2] Ermolov, Aleksandr, et al. \"Whitening for Self-Supervised Representation Learning.\" arXiv preprint arXiv:2007.06346 (2020).", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3680/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3680/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Run Away From your Teacher: a New Self-Supervised Approach Solving the Puzzle of BYOL", "authorids": ["~Haizhou_Shi1", "~Dongliang_Luo1", "~Siliang_Tang1", "~Jian_Wang14", "~Yueting_Zhuang1"], "authors": ["Haizhou Shi", "Dongliang Luo", "Siliang Tang", "Jian Wang", "Yueting Zhuang"], "keywords": ["representation learning", "self-supervised learning", "contrastive learning", "regularization", "theory"], "abstract": "Recently, a newly proposed self-supervised framework Bootstrap Your Own Latent (BYOL) seriously challenges the necessity of negative samples in contrastive-based learning frameworks. BYOL works like a charm despite the fact that it discards the negative samples completely and there is no measure to prevent collapse in its training objective. In this paper, we suggest understanding BYOL from the view of our newly proposed interpretable self-supervised learning framework, Run Away From your Teacher (RAFT). RAFT optimizes two objectives at the same time: (i) aligning two views of the same data to similar representations and (ii) running away from the model's Mean Teacher (MT, the exponential moving average of the history models) instead of BYOL's running towards it. The second term of RAFT explicitly prevents the representation collapse and thus makes RAFT a more conceptually reliable framework. We provide basic benchmarks of RAFT on CIFAR10 to validate the effectiveness of our method. Furthermore, we prove that BYOL is equivalent to RAFT under certain conditions, providing solid reasoning for BYOL's counter-intuitive success.", "one-sentence_summary": "We propose a new interpretable self-supervised approach RAFT solving the puzzle why BYOL works without collapse.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shi|run_away_from_your_teacher_a_new_selfsupervised_approach_solving_the_puzzle_of_byol", "pdf": "/pdf/db96bdbd92de6e910e733dfa0b89bc0a29b3dcc8.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=8vFBZNn211", "_bibtex": "@misc{\nshi2021run,\ntitle={Run Away From your Teacher: a New Self-Supervised Approach Solving the Puzzle of {\\{}BYOL{\\}}},\nauthor={Haizhou Shi and Dongliang Luo and Siliang Tang and Jian Wang and Yueting Zhuang},\nyear={2021},\nurl={https://openreview.net/forum?id=tij5dHg5Hk}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "tij5dHg5Hk", "replyto": "tij5dHg5Hk", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3680/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538071576, "tmdate": 1606915788203, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3680/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3680/-/Official_Review"}}}, {"id": "Hbfpvs6x3b9", "original": null, "number": 3, "cdate": 1603848648332, "ddate": null, "tcdate": 1603848648332, "tmdate": 1605023956961, "tddate": null, "forum": "tij5dHg5Hk", "replyto": "tij5dHg5Hk", "invitation": "ICLR.cc/2021/Conference/Paper3680/-/Official_Review", "content": {"title": "Studying an interesting problem, however the main claim is erroneous", "review": "**Summary**: the paper aims to explain the success of BYOL, a recently proposed contrastive method that mysteriously avoids the trivial constant solution without requiring negative samples. The paper proposes a new loss named RAFT. Compared to BYOL, RAFT is more general since it subsumes a variation of BYOL as its special case, and contains a cross-model term to be maximized which regularizes the alignment loss and encourages the online encoder to \"run away\" from the mean teacher.\n\nThe paper claims this cross-model term encourages disparity, which could help demystify why BYOL does not collapse to a trivial solution. However, the cross-model term itself cannot prevent outputs from collapsing, as explained below.\n\n**Question 1**: my main concern is the effectiveness of the cross-model loss: I disagree that the cross-model loss prevents collapsing representations. I think the authors may be confusing contrasting two samples (\"cross-sample\") and contrasting two functions (\"cross-model\"):\n- The cross-model loss is essentially the L2 distance between two functions, which is the average squared error between two model outputs on the *same sample.*\n- The common contrastive loss, which contrasts outputs from the same model on *different samples*.\n\nFor example, suppose MT is a constant function at the $t_{th}$ iteration (i.e. the function outputs some constant $c$ for all input), then the online encoder could be updated to be another constant as far away from $c$ as possible, i.e. the cross-model loss is maximized, however we still have the sample collapsing issue. As a side note, a constant function also achieves a perfect alignment loss.\nMore concretely, consider $f(x) = Wx +b$ where $W$ is initialized to be the all-0 matrix, i.e. $f(x) = b$ is a constant function. Then for all future updates, learning $f$ only updates $b$ but not $W$ (since there's no gradient on $W$), and therefore $f$ will remain a constant function, i.e. it always collapses the points. One may argue that it is wrong to choose $W = 0$, but the point is, the success of BYOL needs more careful analysis of the optimization process, which cannot be addressed by the cross-model loss term itself.\n\n**Question 2**: section 3 phrases the need of a predictor as a disadvantage of BYOL, however RAFT also requires a predictor head to achieve good classification performance. Studying the effect of the predictor is an interesting direction and will make the paper much stronger, as the authors also point out in the conclusion.\n\n**Other comments**:\n- Table 3: why are there no results for BYOL'-MLP? Comparing RAFT-NP to BYOL'-NP, there doesn't seem to be a clear edge of RAFT, both in terms of the uniformity loss and the accuracy.\nIt would also be better to highlight the key results in the table. The current table is quite dense; adding more highlights and comments will have the reader understand what to take away from these results.\n- Paper organization: a lot of material is deferred to the appendix, which makes the paper a bit hard to follow since the reader needs to jump back and forth. It would be better if results in the appendix are better summarized in the main text.\n- The term BYOL' first appears at the end of the first paragraph on page 2 without a definition.\n- Minor typo: there's an extra left parentheses in front of the second $f$ in equation (5); an extra comma after \"distribution\" in the first paragraph of section 3.", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3680/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3680/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Run Away From your Teacher: a New Self-Supervised Approach Solving the Puzzle of BYOL", "authorids": ["~Haizhou_Shi1", "~Dongliang_Luo1", "~Siliang_Tang1", "~Jian_Wang14", "~Yueting_Zhuang1"], "authors": ["Haizhou Shi", "Dongliang Luo", "Siliang Tang", "Jian Wang", "Yueting Zhuang"], "keywords": ["representation learning", "self-supervised learning", "contrastive learning", "regularization", "theory"], "abstract": "Recently, a newly proposed self-supervised framework Bootstrap Your Own Latent (BYOL) seriously challenges the necessity of negative samples in contrastive-based learning frameworks. BYOL works like a charm despite the fact that it discards the negative samples completely and there is no measure to prevent collapse in its training objective. In this paper, we suggest understanding BYOL from the view of our newly proposed interpretable self-supervised learning framework, Run Away From your Teacher (RAFT). RAFT optimizes two objectives at the same time: (i) aligning two views of the same data to similar representations and (ii) running away from the model's Mean Teacher (MT, the exponential moving average of the history models) instead of BYOL's running towards it. The second term of RAFT explicitly prevents the representation collapse and thus makes RAFT a more conceptually reliable framework. We provide basic benchmarks of RAFT on CIFAR10 to validate the effectiveness of our method. Furthermore, we prove that BYOL is equivalent to RAFT under certain conditions, providing solid reasoning for BYOL's counter-intuitive success.", "one-sentence_summary": "We propose a new interpretable self-supervised approach RAFT solving the puzzle why BYOL works without collapse.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shi|run_away_from_your_teacher_a_new_selfsupervised_approach_solving_the_puzzle_of_byol", "pdf": "/pdf/db96bdbd92de6e910e733dfa0b89bc0a29b3dcc8.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=8vFBZNn211", "_bibtex": "@misc{\nshi2021run,\ntitle={Run Away From your Teacher: a New Self-Supervised Approach Solving the Puzzle of {\\{}BYOL{\\}}},\nauthor={Haizhou Shi and Dongliang Luo and Siliang Tang and Jian Wang and Yueting Zhuang},\nyear={2021},\nurl={https://openreview.net/forum?id=tij5dHg5Hk}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "tij5dHg5Hk", "replyto": "tij5dHg5Hk", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3680/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538071576, "tmdate": 1606915788203, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3680/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3680/-/Official_Review"}}}, {"id": "bLQo-ZBk1eq", "original": null, "number": 4, "cdate": 1604067707962, "ddate": null, "tcdate": 1604067707962, "tmdate": 1605023956894, "tddate": null, "forum": "tij5dHg5Hk", "replyto": "tij5dHg5Hk", "invitation": "ICLR.cc/2021/Conference/Paper3680/-/Official_Review", "content": {"title": "Official Blind Review", "review": "*Summary*\nThe paper provides a new perspective on the BYOL self-supervised learning method. First, the paper introduces an upper-bound objective, BYOL', that is easier to analyze than BYOL because it is composed of two well understood losses: an alignment loss and cross-model loss. Further, it shows empirically that optimizing BYOL' is similar to optimizing BYOL. Second, the paper introduces the RAFT method which maximizes the alignment loss instead of minimizing it. The paper proves that under some assumptions, such as a linear predictor function, optimizing BYOL' is equivalent to RAFT. Based on this analysis, the paper explains why the predictor function is essential for BYOL and why it is hard to achieve convergence.\n\n*Quality*\nI really like the analysis of the paper. The paper provides a mix of theoretical and empirical argument for understanding BYOL, and introduces a new method called RAFT. The main drawback of the paper is that it limits the empirical analysis to a single and much simpler experimental setup using CIFAR10 and resnet18. I believe that since BYOL's significance is an empirical one and is mainly established on Imagenet, any empirical analysis of BYOL in other simpler settings is quite limited.\n\n*Clarity*\nThe authors have done a very good job in writing this paper. The logic, presentation and results are quite clear to understand.\n\n*Originality*\nI find the paper quite interesting and original in its analysis. I especially like the analysing BYOL through the BYOL' upper-bound.\n\n*Significance*\nI think the results of the paper could have been quite more significant if applied on other experimental setups. While I understand working with SOTA models can be computationally expensive, the main argument of this line of work is empirical and it is hard to be convincing without more extensive empirical results.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3680/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3680/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Run Away From your Teacher: a New Self-Supervised Approach Solving the Puzzle of BYOL", "authorids": ["~Haizhou_Shi1", "~Dongliang_Luo1", "~Siliang_Tang1", "~Jian_Wang14", "~Yueting_Zhuang1"], "authors": ["Haizhou Shi", "Dongliang Luo", "Siliang Tang", "Jian Wang", "Yueting Zhuang"], "keywords": ["representation learning", "self-supervised learning", "contrastive learning", "regularization", "theory"], "abstract": "Recently, a newly proposed self-supervised framework Bootstrap Your Own Latent (BYOL) seriously challenges the necessity of negative samples in contrastive-based learning frameworks. BYOL works like a charm despite the fact that it discards the negative samples completely and there is no measure to prevent collapse in its training objective. In this paper, we suggest understanding BYOL from the view of our newly proposed interpretable self-supervised learning framework, Run Away From your Teacher (RAFT). RAFT optimizes two objectives at the same time: (i) aligning two views of the same data to similar representations and (ii) running away from the model's Mean Teacher (MT, the exponential moving average of the history models) instead of BYOL's running towards it. The second term of RAFT explicitly prevents the representation collapse and thus makes RAFT a more conceptually reliable framework. We provide basic benchmarks of RAFT on CIFAR10 to validate the effectiveness of our method. Furthermore, we prove that BYOL is equivalent to RAFT under certain conditions, providing solid reasoning for BYOL's counter-intuitive success.", "one-sentence_summary": "We propose a new interpretable self-supervised approach RAFT solving the puzzle why BYOL works without collapse.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shi|run_away_from_your_teacher_a_new_selfsupervised_approach_solving_the_puzzle_of_byol", "pdf": "/pdf/db96bdbd92de6e910e733dfa0b89bc0a29b3dcc8.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=8vFBZNn211", "_bibtex": "@misc{\nshi2021run,\ntitle={Run Away From your Teacher: a New Self-Supervised Approach Solving the Puzzle of {\\{}BYOL{\\}}},\nauthor={Haizhou Shi and Dongliang Luo and Siliang Tang and Jian Wang and Yueting Zhuang},\nyear={2021},\nurl={https://openreview.net/forum?id=tij5dHg5Hk}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "tij5dHg5Hk", "replyto": "tij5dHg5Hk", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3680/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538071576, "tmdate": 1606915788203, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3680/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3680/-/Official_Review"}}}, {"id": "JGO_OAVdEQU", "original": null, "number": 2, "cdate": 1603755792742, "ddate": null, "tcdate": 1603755792742, "tmdate": 1605023956815, "tddate": null, "forum": "tij5dHg5Hk", "replyto": "tij5dHg5Hk", "invitation": "ICLR.cc/2021/Conference/Paper3680/-/Official_Review", "content": {"title": "RAFT", "review": "This paper analyses the recently proposed Bootstrap Your Own Latent (BYOL) algorithm for self-supervised learning and image representation. \nThe authors first derive an alternative training procedure called BYOL' by computing an upper bound of the BYOL objective function.\nAfter diverse analyses, the authors then introduce Run Away From Your Teacher (RAFT), where RAFT is another BYOL variant that resembles contrastive method by having an attractive and repealing term in the training objective. According to the authors, this decomposition allows for a better understanding of the training dynamics.\n\nFinally, the authors made the following transitivity reasoning:\n - BYOL and BYOL' are almost equivalent\n - RAFT and BYOL' are shown to be equivalent under some assumptions.\nThus, conclusions that are drawn from analyzing RAFT should still hold while analyzing BYOL. They thus link the interest of BYOL's predictor and the EMA through the RAFT loss decomposition. \n\nI have multiple strong concerns regarding this paper. These concerns are both on the paper results, shortcuts in the analysis, and the writing style. \n\n\nResults:\n--------------\n\n - In section 4, the authors introduce BYOL' as a variant of BYOL. To do so, they derive an upper bound on the BYOL loss, i.e. the L2 distance between the projection and the projector, and they try to minimize it. However, this approach disregards that BYOL does not minimize a loss (due to the stop gradient). In other words, the BYOL objective keeps evolving during training; the target distribution is non-stationary.  As mentioned in the BYOL paper: \"Similar to GANs, where there is no loss that is jointly minimized w.r.t. both the discriminator and generator parameters; there is therefore no a priori reason why BYOL\u2019s parameters would\nconverge to a minimum of L_BYOL given the online and target parameters\". Minimizing an upper-bound is at best insufficient, at worst a non-sense. The sentences, \"minimizing L_{BYOL'}  would yield similar performance as minimizing L_{BYOL}\" and \"we conclude that optimizing L_{BYOL'} is almost equivalent to L_{BYOL}\" are unfortunately wrong.  This is somewhat highlighted different qualitative results in Appendix F.1.b != F.1.d.\nA better approach would be to ensure that the *gradients* go in a similar direction (so the training dynamics are similar rather than the objective function). However, even such a demonstration could be insufficient due to compounding factors in the training dynamics. \n - The 1-1 mapping between BYOL' and RAFT rely on three hypotheses. While (i) and (ii) are reasonable, hypothesis (iii) is quite strong, and more importantly, neither elaborated nor discussed. In other words, I am unable to validate/invalidate the interest of the theoretical results. Would it be possible to measure the normal gradient empirically? To bound it? \n - In section 3, i would recommend the author to mention that multiple components were also in the BYOL paper; especially when writing \"therefore, we conclude the predictor is essential to the collapse prevention of BYOL.\"\n - Although I acknowledge that self-supervised learning requires heavy computational requirement, and few teams may run experiments on ImageNet. Yet, I would recommend the authors to not use CIFAR10 as the dataset has multiple known issues (few classes, small images, few discriminative features). Other variants such at STL or ImageNete can be trained on a single GPU over a day, and are less prone to misinterpretation in the results. Besides, I want to point out that BYOL was not correctly tuned: the experiments are based on a different optimizer (Adam vs LARS) and no cosine decay were used for the EMA, while these two components seem to be critical, as mentioned in BYOL and arxiv:2010.1024. \n\nOverall, I have a serious concern about the paper's core contributions. However, there are still some good elements in the paper that I think are under-exploited:\n - RAFT is itself an original, new and interesting algorithm. The potential link to BYOL is indeed an interesting lead, but in its current state, I would make it a discussion more than a key contribution.\n - Table D.3 shows that RAFT/BYOL' does not collapse without predictors when \\beta is high. Albeit providing low accuracy, a non-collapse is quite surprising. Unfortunately, the authors leave it for future work\n\n\nShortcuts:\n--------------\nI was surprised by multiple shortcuts in the reasoning process or undiscussed conclusions:\n - The authors mention that the predictor is a dissatisfactory property of BYOL. Could they elaborate? This is actual the key component of the method (if not the only one!), and such pro/cons could be detailed in light of other methods. \n - In section 4.1, the authors mention that: similar accuracies and losses are sufficient somewhat confirm that BYOL and BYOL' are similar. Two completely different methods may have the same errors while being radically different... \n - In Section 4.2, the authors mention that \"Based on the form of BYOL, we conclude that MT is used to regularize the alignment loss\". However, there is no experiments to try to contradict/validate this claim. Differently, the EMA may ease the optimization process or it may have different properties. Even if I understand the logic behind this statement, I regret that the authors do not try to confort it. \n- In section 4.2, the authors mention that there exist multiple works (while only citing one...) demonstrating that EMA is \"roughly\" equivalent to sample averaging and may encourage diversity. While this is sometimes true in specific settings (cf. markov game and fictitious play), this is also known to ease optimization (cf. target network in DQN). Stating that RAFT is better than BYOL because it better leverage the EMA target is tricky without proper analysis.\n- Albeit understandable, the transitivity between BYOL and RAFT is difficult to defend due to multiple approximations and hypothesis. Therefore, it is of paramount importance that the approximations and hypothesis are validated, which is not sufficiently done in the paper. \n\n\nWriting: \n--------------\n - Although papers' writing quality remain subjective, I tend to expect a formal language. I kind of feel ill-at-ease when reading sentences including \"BYOL works like a charm\", \"disclosing the mistery\", \"to go harsher\", \"bizarre phenomon\". Other sentences also expresses judgement such as \"inconsistent behavior\", \"dissatisfactory property of BYOL\" or \"has admirable property\" without proper argumentation. \n - It is non-trivial to follow the different version of the algorithms... which are defined in the appendix. Please consider renaming BYOL'. \n - A related work section would have been useful to put in perspective BYOL that are theoretically motivated e.g. AMDIM, InfoMin, other self-supervised learning methods without negative example, e.g. DeepCluster, SwAV. Section 2 is more about the background, not related work.\n - there are a few confusions in the notation, \\alpha \\beta have different meaning across equations (Eq 7 vs 8)\n - In section 3, random is ill-defined. In Cifar10, random should be 10%, I assume that you refer to random projection. Please clarify.  \n - Figure 1 is clear, and I recommend to keep it as it is.\n - From my perspective, the mathematical explanation in Section 5 is quite obfuscated, and I would recommend a full rewriting.\n - Please avoid unnecessary taxonomy, e.g. uniformity optimizer, effective regulizers and others.\n - In conclusion, you mentioned some results about the projector. However, you never detail them in the paper. Please, do not discuss unpublished results.\n\nOverall, I had difficulties following the paper: I keep alternating between the appendix, previous sections, and the text. Again, the phrasing makes me ill-at-ease.\n\n\nConclusion:\n--------------------\nI have some serious concerns about the core results of the paper. Importantly, Theorem 4.1 follows a misinterpretation of the BYOL training dynamics. From my perspective, there are too many unjustified claims, and I cannot recommend paper acceptance. However, there is some good idea in the paper, and I strongly encourage the authors to study RAFT independently of BYOL in the future.", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper3680/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3680/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Run Away From your Teacher: a New Self-Supervised Approach Solving the Puzzle of BYOL", "authorids": ["~Haizhou_Shi1", "~Dongliang_Luo1", "~Siliang_Tang1", "~Jian_Wang14", "~Yueting_Zhuang1"], "authors": ["Haizhou Shi", "Dongliang Luo", "Siliang Tang", "Jian Wang", "Yueting Zhuang"], "keywords": ["representation learning", "self-supervised learning", "contrastive learning", "regularization", "theory"], "abstract": "Recently, a newly proposed self-supervised framework Bootstrap Your Own Latent (BYOL) seriously challenges the necessity of negative samples in contrastive-based learning frameworks. BYOL works like a charm despite the fact that it discards the negative samples completely and there is no measure to prevent collapse in its training objective. In this paper, we suggest understanding BYOL from the view of our newly proposed interpretable self-supervised learning framework, Run Away From your Teacher (RAFT). RAFT optimizes two objectives at the same time: (i) aligning two views of the same data to similar representations and (ii) running away from the model's Mean Teacher (MT, the exponential moving average of the history models) instead of BYOL's running towards it. The second term of RAFT explicitly prevents the representation collapse and thus makes RAFT a more conceptually reliable framework. We provide basic benchmarks of RAFT on CIFAR10 to validate the effectiveness of our method. Furthermore, we prove that BYOL is equivalent to RAFT under certain conditions, providing solid reasoning for BYOL's counter-intuitive success.", "one-sentence_summary": "We propose a new interpretable self-supervised approach RAFT solving the puzzle why BYOL works without collapse.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shi|run_away_from_your_teacher_a_new_selfsupervised_approach_solving_the_puzzle_of_byol", "pdf": "/pdf/db96bdbd92de6e910e733dfa0b89bc0a29b3dcc8.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=8vFBZNn211", "_bibtex": "@misc{\nshi2021run,\ntitle={Run Away From your Teacher: a New Self-Supervised Approach Solving the Puzzle of {\\{}BYOL{\\}}},\nauthor={Haizhou Shi and Dongliang Luo and Siliang Tang and Jian Wang and Yueting Zhuang},\nyear={2021},\nurl={https://openreview.net/forum?id=tij5dHg5Hk}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "tij5dHg5Hk", "replyto": "tij5dHg5Hk", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3680/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538071576, "tmdate": 1606915788203, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3680/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3680/-/Official_Review"}}}], "count": 13}