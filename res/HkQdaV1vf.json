{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1528124431695, "tcdate": 1518452075008, "number": 146, "cdate": 1518452075008, "id": "HkQdaV1vf", "invitation": "ICLR.cc/2018/Workshop/-/Submission", "forum": "HkQdaV1vf", "signatures": ["~Jiajin_Li1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop"], "content": {"title": "Policy Optimization with Second-Order Advantage Information", "abstract": "Policy optimization on high-dimensional action spaces exhibits its difficulty caused by the high variance of the policy gradient estimators. We present the action subspace dependent gradient (ASDG) estimator which incorporates the Rao-Blackwell theorem (RB) and Control Variates (CV) into a unified framework to reduce the variance. To invoke RB, the algorithm learns the underlying factorization structure among the action space based on the second-order gradient of the advantage function with respect to the action. Empirical studies demonstrate the performance improvement on high-dimensional synthetic settings and OpenAI Gym's MuJoCo continuous control tasks. ", "paperhash": "li|policy_optimization_with_secondorder_advantage_information", "keywords": ["Policy gradient", "Variance Reduction", "Control Variates", "Rao-Blackwellization"], "_bibtex": "@misc{\n  li2018policy,\n  title={Policy Optimization with Second-Order Advantage Information},\n  author={Jiajin Li and Baoxiang Wang},\n  year={2018},\n  url={https://openreview.net/forum?id=HkQdaV1vf}\n}", "authorids": ["jjli@cse.cuhk.edu.hk", "bxwang@cse.cuhk.edu.hk"], "authors": ["Jiajin Li", "Baoxiang Wang"], "TL;DR": "A novel policy gradient estimator incorporating both Rao-Blackwell theorem and Control Variates into a unified framework. ", "pdf": "/pdf/64c7b14b72177cba05764cdb2dd52270f1270821.pdf"}, "nonreaders": [], "details": {"replyCount": 5, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1518472800000, "tmdate": 1518474081690, "id": "ICLR.cc/2018/Workshop/-/Submission", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Workshop"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2018/Workshop", "description": "Your authorized identity to be associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 9, "value-regex": "upload", "description": "Upload a PDF file that ends with .pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 8, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names. Please provide real names; identities will be anonymized."}, "keywords": {"order": 6, "values-regex": "(^$)|[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of keywords."}, "TL;DR": {"required": false, "order": 7, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,500}"}, "authorids": {"required": true, "order": 3, "values-regex": "([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,},){0,}([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,})", "description": "Comma separated list of author email addresses, lowercased, in the same order as above. For authors with existing OpenReview accounts, please make sure that the provided email address(es) match those listed in the author's profile. Please provide real emails; identities will be anonymized."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1526248800000, "cdate": 1518474081690}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582798643, "tcdate": 1520624553467, "number": 1, "cdate": 1520624553467, "id": "SyZ2QwltM", "invitation": "ICLR.cc/2018/Workshop/-/Paper146/Official_Review", "forum": "HkQdaV1vf", "replyto": "HkQdaV1vf", "signatures": ["ICLR.cc/2018/Workshop/Paper146/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper146/AnonReviewer2"], "content": {"title": "A good exploratory work on variance reduction in policy gradient, but lacks some empirical validations", "rating": "6: Marginally above acceptance threshold", "review": "The paper proposes using row-shifting approximate block diagonalization of advantage estimate to identity uncorrelated action subspaces to apply the methods in Wu et. al. (2018) over subspaces. It shows that it leads to some improvements in policy gradient performances.\n\nThe method proposed in the paper is closest to Wu et. al. (2018), except it identifies axis-aligned subspaces using quadratic advantage approximations. The method is a good addition to a list of control variates/baseline methods for policy gradient, and I recommend for acceptance. Q-Prop [Gu et. al., 2017] should also be added as a reference. \n\nThe author comment discusses Tucker et. al. (2018) results and suggests it\u2019s mainly on control variates. This isn\u2019t perfectly accurate, since the main discussion in their paper is adding extra single step action dependency in baseline (where control variate subsumes Wu et. al. 2018, except practical performance differences in function approximations) does not reduce variance significantly enough in some domains beyond learning a better baseline. Empirical estimations of the variance reductions are expected as follow-up.\n\nQuestions:\n- Parameterization of c(s, a) and c_k(s,a_k) could be better clarified. Isn\u2019t block diagonalization/row shifting different per state sample s? If so, it seems unclear how c is parameterized. \n\nPros:\n- Clear description of the method\n\nCons:\n- Performance improvements aren\u2019t significant. In Figure 2, good halfcheetah results are enough, since they are notoriously high variance\n- Empirical validation of variance reduction is required to complete the paper  \n", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Policy Optimization with Second-Order Advantage Information", "abstract": "Policy optimization on high-dimensional action spaces exhibits its difficulty caused by the high variance of the policy gradient estimators. We present the action subspace dependent gradient (ASDG) estimator which incorporates the Rao-Blackwell theorem (RB) and Control Variates (CV) into a unified framework to reduce the variance. To invoke RB, the algorithm learns the underlying factorization structure among the action space based on the second-order gradient of the advantage function with respect to the action. Empirical studies demonstrate the performance improvement on high-dimensional synthetic settings and OpenAI Gym's MuJoCo continuous control tasks. ", "paperhash": "li|policy_optimization_with_secondorder_advantage_information", "keywords": ["Policy gradient", "Variance Reduction", "Control Variates", "Rao-Blackwellization"], "_bibtex": "@misc{\n  li2018policy,\n  title={Policy Optimization with Second-Order Advantage Information},\n  author={Jiajin Li and Baoxiang Wang},\n  year={2018},\n  url={https://openreview.net/forum?id=HkQdaV1vf}\n}", "authorids": ["jjli@cse.cuhk.edu.hk", "bxwang@cse.cuhk.edu.hk"], "authors": ["Jiajin Li", "Baoxiang Wang"], "TL;DR": "A novel policy gradient estimator incorporating both Rao-Blackwell theorem and Control Variates into a unified framework. ", "pdf": "/pdf/64c7b14b72177cba05764cdb2dd52270f1270821.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582798413, "id": "ICLR.cc/2018/Workshop/-/Paper146/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper146/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper146/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper146/AnonReviewer1", "ICLR.cc/2018/Workshop/Paper146/AnonReviewer3"], "reply": {"forum": "HkQdaV1vf", "replyto": "HkQdaV1vf", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper146/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper146/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582798413}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582637977, "tcdate": 1520810192556, "number": 2, "cdate": 1520810192556, "id": "BydCOEQYz", "invitation": "ICLR.cc/2018/Workshop/-/Paper146/Official_Review", "forum": "HkQdaV1vf", "replyto": "HkQdaV1vf", "signatures": ["ICLR.cc/2018/Workshop/Paper146/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper146/AnonReviewer1"], "content": {"title": "Interesting contribution, not sure about experimental evidence", "rating": "6: Marginally above acceptance threshold", "review": "Summary:\nThe goal of the paper is to reduce the variance of policy gradients methods when the action space is high dimensional.\nTo do so, they combine the idea of having action-dependent baselines (Generalized Action Dependent Baseline, Hao et al. 2018) with the idea of utilising independence between action dimensions (or groups thereof, i.e. factors, Wu et al. 2018) to have a separate baseline function for each factor. \nThey propose a method to learn an underlying factorisation structure, with the number of independent factors being a hyperparameter.\n\nNovelty (6/10):\nMost of the ideas in this paper have been previously published, in particular, both the RB and CV step.\nThey state that Wu et al. assumed a fully factorized policy, however in their paper, Appendix E, Wu et al. 2018 also discuss more general factorizations of the action.\nWhat is new in this context is to automatically learn the factorization, given the number of independent factors. They do so by estimating the Hessian and applying the minimum cut algorithm. \n\nClarity (6/10):\nThe paper builds upon a great variety of previous work, which makes it difficult to understand it on its own without consulting some of the referenced papers as there is not enough space for a sufficiently detailed background section.\nOverall, the paper explains the method sufficiently well, but could be structured better: For example, the usage of the minimum-cut algorithm to compute the partition based on the Hessian is only mentioned in the introduction but not outlined or even mentioned in the methods section.\nI did not understand the sentence \"ASDG's trade-off results in the combination of both the merits of its extreme cases\" which was used to explain the superior performance in the experiment section on the synthetic task.\n\nSignificance (8/10):\nResearch around the question of how to reduce the variance of policy gradient algorithms is highly active at the moment. This paper fits nicely into this.\n\nQuality (5/10):\n- (+) Code is given\n- (-) No proofs are given\n- (-) There are inconsistencies between their reproduced results for ADFB (Wu et al.), which they use as baseline, and the results in the original paper: The results roughly agree for Ant, but are wastly better for HalfCheetah and Hopper in the original paper than they are reported here (~4000 vs ~1000) and (~3500 vs ~1500).\n- Why is the performance of ADFB in the synthetic task decreasing over time (for experiments c) and d) )?\n\nPros:\n- Highly relevant topic\n- Interesting contribution on how to find a good factorisation of the policy that can be used for variance reduction\n\nCons:\n- Combination of a lot of different ideas makes it difficult to distil the effect of their original contribution. For example, if one sees the learned factorization as their main original contribution, then I'm not sure why the CV step (Hao et al.) is necessary?\n- Inconsistencies in baseline performance (see 'Quality' section). \n- Lacking proofs and detailed description of their method (likely due to their combination of many different ideas)\n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Policy Optimization with Second-Order Advantage Information", "abstract": "Policy optimization on high-dimensional action spaces exhibits its difficulty caused by the high variance of the policy gradient estimators. We present the action subspace dependent gradient (ASDG) estimator which incorporates the Rao-Blackwell theorem (RB) and Control Variates (CV) into a unified framework to reduce the variance. To invoke RB, the algorithm learns the underlying factorization structure among the action space based on the second-order gradient of the advantage function with respect to the action. Empirical studies demonstrate the performance improvement on high-dimensional synthetic settings and OpenAI Gym's MuJoCo continuous control tasks. ", "paperhash": "li|policy_optimization_with_secondorder_advantage_information", "keywords": ["Policy gradient", "Variance Reduction", "Control Variates", "Rao-Blackwellization"], "_bibtex": "@misc{\n  li2018policy,\n  title={Policy Optimization with Second-Order Advantage Information},\n  author={Jiajin Li and Baoxiang Wang},\n  year={2018},\n  url={https://openreview.net/forum?id=HkQdaV1vf}\n}", "authorids": ["jjli@cse.cuhk.edu.hk", "bxwang@cse.cuhk.edu.hk"], "authors": ["Jiajin Li", "Baoxiang Wang"], "TL;DR": "A novel policy gradient estimator incorporating both Rao-Blackwell theorem and Control Variates into a unified framework. ", "pdf": "/pdf/64c7b14b72177cba05764cdb2dd52270f1270821.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582798413, "id": "ICLR.cc/2018/Workshop/-/Paper146/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper146/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper146/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper146/AnonReviewer1", "ICLR.cc/2018/Workshop/Paper146/AnonReviewer3"], "reply": {"forum": "HkQdaV1vf", "replyto": "HkQdaV1vf", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper146/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper146/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582798413}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582618236, "tcdate": 1520867399602, "number": 3, "cdate": 1520867399602, "id": "SJgLdGNtz", "invitation": "ICLR.cc/2018/Workshop/-/Paper146/Official_Review", "forum": "HkQdaV1vf", "replyto": "HkQdaV1vf", "signatures": ["ICLR.cc/2018/Workshop/Paper146/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper146/AnonReviewer3"], "content": {"title": "Review", "rating": "7: Good paper, accept", "review": "This paper essentially combines two ideas - the first is reparametrized action-conditional baselines, the second is to make block-diagonal assumption on the advantage function (instead of fully diagonal). While neither of the ideas appears fundamentally novel (and we recover as a special cases both ideas- ADFB and GADB), the combination is still worthwhile to investigate. Writing and notation in general could be improved (The 'computing the partition' section in particular is hard to parse). Experiments demonstrate moderate improvements over the baselines (ADFB/GADB).\n\nMinor: \n- 'Otherwise, if we assume that Hessian is diagonal... equals to...\" : is this accurate? This results does not appear in Wu et al. afaik, and writing a baseline as a sum of baseline feels like a mistake (all are effectively conditional expectations of the same quantity, so the sum of some shouldn't equal another). Or did I misread the notation?\n", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Policy Optimization with Second-Order Advantage Information", "abstract": "Policy optimization on high-dimensional action spaces exhibits its difficulty caused by the high variance of the policy gradient estimators. We present the action subspace dependent gradient (ASDG) estimator which incorporates the Rao-Blackwell theorem (RB) and Control Variates (CV) into a unified framework to reduce the variance. To invoke RB, the algorithm learns the underlying factorization structure among the action space based on the second-order gradient of the advantage function with respect to the action. Empirical studies demonstrate the performance improvement on high-dimensional synthetic settings and OpenAI Gym's MuJoCo continuous control tasks. ", "paperhash": "li|policy_optimization_with_secondorder_advantage_information", "keywords": ["Policy gradient", "Variance Reduction", "Control Variates", "Rao-Blackwellization"], "_bibtex": "@misc{\n  li2018policy,\n  title={Policy Optimization with Second-Order Advantage Information},\n  author={Jiajin Li and Baoxiang Wang},\n  year={2018},\n  url={https://openreview.net/forum?id=HkQdaV1vf}\n}", "authorids": ["jjli@cse.cuhk.edu.hk", "bxwang@cse.cuhk.edu.hk"], "authors": ["Jiajin Li", "Baoxiang Wang"], "TL;DR": "A novel policy gradient estimator incorporating both Rao-Blackwell theorem and Control Variates into a unified framework. ", "pdf": "/pdf/64c7b14b72177cba05764cdb2dd52270f1270821.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582798413, "id": "ICLR.cc/2018/Workshop/-/Paper146/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper146/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper146/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper146/AnonReviewer1", "ICLR.cc/2018/Workshop/Paper146/AnonReviewer3"], "reply": {"forum": "HkQdaV1vf", "replyto": "HkQdaV1vf", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper146/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper146/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582798413}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521573564885, "tcdate": 1521573564885, "number": 96, "cdate": 1521573564546, "id": "BJra0ARYM", "invitation": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "forum": "HkQdaV1vf", "replyto": "HkQdaV1vf", "signatures": ["ICLR.cc/2018/Workshop/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Program_Chairs"], "content": {"decision": "Accept", "title": "ICLR 2018 Workshop Acceptance Decision", "comment": "Congratulations, your paper was accepted to the ICLR workshop."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Policy Optimization with Second-Order Advantage Information", "abstract": "Policy optimization on high-dimensional action spaces exhibits its difficulty caused by the high variance of the policy gradient estimators. We present the action subspace dependent gradient (ASDG) estimator which incorporates the Rao-Blackwell theorem (RB) and Control Variates (CV) into a unified framework to reduce the variance. To invoke RB, the algorithm learns the underlying factorization structure among the action space based on the second-order gradient of the advantage function with respect to the action. Empirical studies demonstrate the performance improvement on high-dimensional synthetic settings and OpenAI Gym's MuJoCo continuous control tasks. ", "paperhash": "li|policy_optimization_with_secondorder_advantage_information", "keywords": ["Policy gradient", "Variance Reduction", "Control Variates", "Rao-Blackwellization"], "_bibtex": "@misc{\n  li2018policy,\n  title={Policy Optimization with Second-Order Advantage Information},\n  author={Jiajin Li and Baoxiang Wang},\n  year={2018},\n  url={https://openreview.net/forum?id=HkQdaV1vf}\n}", "authorids": ["jjli@cse.cuhk.edu.hk", "bxwang@cse.cuhk.edu.hk"], "authors": ["Jiajin Li", "Baoxiang Wang"], "TL;DR": "A novel policy gradient estimator incorporating both Rao-Blackwell theorem and Control Variates into a unified framework. ", "pdf": "/pdf/64c7b14b72177cba05764cdb2dd52270f1270821.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518629844880, "id": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Program_Chairs"], "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Workshop/-/Submission", "writers": {"values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Workshop/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Workshop Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept", "Reject"]}}}, "nonreaders": [], "noninvitees": [], "cdate": 1518629844880}}}, {"tddate": null, "ddate": null, "tmdate": 1520523468319, "tcdate": 1520523468319, "number": 1, "cdate": 1520523468319, "id": "H14RuC0df", "invitation": "ICLR.cc/2018/Workshop/-/Paper146/Public_Comment", "forum": "HkQdaV1vf", "replyto": "HkQdaV1vf", "signatures": ["~Baoxiang_Wang1"], "readers": ["everyone"], "writers": ["~Baoxiang_Wang1"], "content": {"title": "Regarding the recent study \"The Mirage of Action-Dependent Baselines in Reinforcement Learning\"", "comment": "A recent paper is discussing the Mirage [1] of the \"action-depend baselines\". We'd like to note that the paper is discussing the ineffectiveness of action-dependent control variate (CV) [2,3]. Adding the action-dependent Rao-Blackwellization (RB) on top of CV [4 and our paper] is still a very promising improvement. Also, our algorithm and theory are straightforwardly extended to both action-dependent or independent (only state-dependent) CV. We also find in our experiment that we have similar performance using either action-dependent CV (baseline) or just state-dependent baseline (A2C).\n\n[1] Tucker, George, et al. \"The Mirage of Action-Dependent Baselines in Reinforcement Learning.\" arXiv preprint arXiv:1802.10031 (2018).\n[2] Liu, Hao, et al. \"Action-dependent Control Variates for Policy Optimization via Stein Identity.\"  ICLR 2018.\n[3] Grathwohl, Will, et al. \"Backpropagation through the Void: Optimizing control variates for black-box gradient estimation.\"  ICLR 2018.\n[4] Wu, Cathy, et al. \"Variance Reduction for Policy Gradient with Action-Dependent Factorized Baselines.\" ICLR 2018."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Policy Optimization with Second-Order Advantage Information", "abstract": "Policy optimization on high-dimensional action spaces exhibits its difficulty caused by the high variance of the policy gradient estimators. We present the action subspace dependent gradient (ASDG) estimator which incorporates the Rao-Blackwell theorem (RB) and Control Variates (CV) into a unified framework to reduce the variance. To invoke RB, the algorithm learns the underlying factorization structure among the action space based on the second-order gradient of the advantage function with respect to the action. Empirical studies demonstrate the performance improvement on high-dimensional synthetic settings and OpenAI Gym's MuJoCo continuous control tasks. ", "paperhash": "li|policy_optimization_with_secondorder_advantage_information", "keywords": ["Policy gradient", "Variance Reduction", "Control Variates", "Rao-Blackwellization"], "_bibtex": "@misc{\n  li2018policy,\n  title={Policy Optimization with Second-Order Advantage Information},\n  author={Jiajin Li and Baoxiang Wang},\n  year={2018},\n  url={https://openreview.net/forum?id=HkQdaV1vf}\n}", "authorids": ["jjli@cse.cuhk.edu.hk", "bxwang@cse.cuhk.edu.hk"], "authors": ["Jiajin Li", "Baoxiang Wang"], "TL;DR": "A novel policy gradient estimator incorporating both Rao-Blackwell theorem and Control Variates into a unified framework. ", "pdf": "/pdf/64c7b14b72177cba05764cdb2dd52270f1270821.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518712625950, "id": "ICLR.cc/2018/Workshop/-/Paper146/Public_Comment", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper146/Reviewers"], "reply": {"replyto": null, "forum": "HkQdaV1vf", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1518712625950}}}], "count": 6}