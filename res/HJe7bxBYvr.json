{"notes": [{"id": "HJe7bxBYvr", "original": "BJg7kelYwB", "number": 2130, "cdate": 1569439739133, "ddate": null, "tcdate": 1569439739133, "tmdate": 1577168275058, "tddate": null, "forum": "HJe7bxBYvr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Avoiding Negative Side-Effects and Promoting Safe Exploration with Imaginative Planning", "authors": ["Dhruv Ramani", "Benjamin Eysenbach"], "authorids": ["dhruvramani98@gmail.com", "beysenba@cs.cmu.edu"], "keywords": ["Reinforcement Learning", "AI-Safety", "Model-Based Reinforcement Learning", "Safe-Exploration"], "abstract": "  With the recent proliferation of the usage of reinforcement learning (RL) agents for solving real-world tasks, safety emerges as a necessary ingredient for their successful application. In this paper, we focus on ensuring the safety of the agent while making sure that the agent does not cause any unnecessary disruptions to its environment. The current approaches to this problem, such as manually constraining the agent or adding a safety penalty to the reward function, can introduce bad incentives. In complex domains, these approaches are simply intractable, as they require knowing apriori all the possible unsafe scenarios an agent could encounter. We propose a model-based approach to safety that allows the agent to look into the future and be aware of the future consequences of its actions. We learn the transition dynamics of the environment and generate a directed graph called the imaginative module. This graph encapsulates all possible trajectories that can be followed by the agent, allowing the agent to efficiently traverse through the imagined environment without ever taking any action in reality. A baseline state, which can either represent a safe or an unsafe state (based on whichever is easier to define) is taken as a human input, and the imaginative module is used to predict whether the current actions of the agent can cause it to end up in dangerous states in the future. Our imaginative module can be seen as a ``plug-and-play'' approach to ensuring safety, as it is compatible with any existing RL algorithm and any task with discrete action space. Our method induces the agent to act safely while learning to solve the task. We experimentally validate our proposal on two gridworld environments and a self-driving car simulator, demonstrating that our approach to safety visits unsafe states significantly less frequently than a baseline.", "pdf": "/pdf/bdba2804d40b580b341591526ce4971f8f5611dc.pdf", "paperhash": "ramani|avoiding_negative_sideeffects_and_promoting_safe_exploration_with_imaginative_planning", "original_pdf": "/attachment/bdba2804d40b580b341591526ce4971f8f5611dc.pdf", "_bibtex": "@misc{\nramani2020avoiding,\ntitle={Avoiding Negative Side-Effects and Promoting Safe Exploration with Imaginative Planning},\nauthor={Dhruv Ramani and Benjamin Eysenbach},\nyear={2020},\nurl={https://openreview.net/forum?id=HJe7bxBYvr}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "p4aynpOYq-", "original": null, "number": 1, "cdate": 1576798741363, "ddate": null, "tcdate": 1576798741363, "tmdate": 1576800894867, "tddate": null, "forum": "HJe7bxBYvr", "replyto": "HJe7bxBYvr", "invitation": "ICLR.cc/2020/Conference/Paper2130/-/Decision", "content": {"decision": "Reject", "comment": "This paper tackles the problem of safe exploration in RL. The proposed approach uses an imaginative module to construct a connectivity graph between all states using forward predictions. The idea then consists in using this graph to plan a trajectory which avoids states labelled as \"unsafe\".\n\nSeveral concerns were raised and the authors did not provide any rebuttal. A major point is that the assumption that the approach has access to what are unsafe states, which is either unreasonable in practice or makes the problem much simpler. Another major point is the uniform data collection about every state-action pairs. This can be really unsafe and defeats the purpose of safe exploration following this phase. These questions may be due to a miscomprehension, indicating that the paper should be clarified, as demanded by reviewers. Finally, the experiments would benefit from additional details in order to be correctly understood.\n\nAll reviewers agree that this paper should be rejected. Hence, I recommend reject.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Avoiding Negative Side-Effects and Promoting Safe Exploration with Imaginative Planning", "authors": ["Dhruv Ramani", "Benjamin Eysenbach"], "authorids": ["dhruvramani98@gmail.com", "beysenba@cs.cmu.edu"], "keywords": ["Reinforcement Learning", "AI-Safety", "Model-Based Reinforcement Learning", "Safe-Exploration"], "abstract": "  With the recent proliferation of the usage of reinforcement learning (RL) agents for solving real-world tasks, safety emerges as a necessary ingredient for their successful application. In this paper, we focus on ensuring the safety of the agent while making sure that the agent does not cause any unnecessary disruptions to its environment. The current approaches to this problem, such as manually constraining the agent or adding a safety penalty to the reward function, can introduce bad incentives. In complex domains, these approaches are simply intractable, as they require knowing apriori all the possible unsafe scenarios an agent could encounter. We propose a model-based approach to safety that allows the agent to look into the future and be aware of the future consequences of its actions. We learn the transition dynamics of the environment and generate a directed graph called the imaginative module. This graph encapsulates all possible trajectories that can be followed by the agent, allowing the agent to efficiently traverse through the imagined environment without ever taking any action in reality. A baseline state, which can either represent a safe or an unsafe state (based on whichever is easier to define) is taken as a human input, and the imaginative module is used to predict whether the current actions of the agent can cause it to end up in dangerous states in the future. Our imaginative module can be seen as a ``plug-and-play'' approach to ensuring safety, as it is compatible with any existing RL algorithm and any task with discrete action space. Our method induces the agent to act safely while learning to solve the task. We experimentally validate our proposal on two gridworld environments and a self-driving car simulator, demonstrating that our approach to safety visits unsafe states significantly less frequently than a baseline.", "pdf": "/pdf/bdba2804d40b580b341591526ce4971f8f5611dc.pdf", "paperhash": "ramani|avoiding_negative_sideeffects_and_promoting_safe_exploration_with_imaginative_planning", "original_pdf": "/attachment/bdba2804d40b580b341591526ce4971f8f5611dc.pdf", "_bibtex": "@misc{\nramani2020avoiding,\ntitle={Avoiding Negative Side-Effects and Promoting Safe Exploration with Imaginative Planning},\nauthor={Dhruv Ramani and Benjamin Eysenbach},\nyear={2020},\nurl={https://openreview.net/forum?id=HJe7bxBYvr}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "HJe7bxBYvr", "replyto": "HJe7bxBYvr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795725487, "tmdate": 1576800277393, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2130/-/Decision"}}}, {"id": "HkgkkpsmtB", "original": null, "number": 1, "cdate": 1571171543138, "ddate": null, "tcdate": 1571171543138, "tmdate": 1573756900156, "tddate": null, "forum": "HJe7bxBYvr", "replyto": "HJe7bxBYvr", "invitation": "ICLR.cc/2020/Conference/Paper2130/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #1", "review": "Thie paper proposes using an \"imagination\" module to provide safe exploration during RL learning. The imagination module is used to perform forward predictions, constructing a graph between possible states. If any action would lead to a \"base state\" that is an unsafe state that action will not be executed and another \"safe\" action is selected from the policy.\n\nI have a number of comments and questions after reading the paper:\n- How do you get the forward model to be usably accurate? You do say that the model is a CNN model and is shared to learn the reward function as well. In the paper it says your method will lead to the agent never reaching an unsafe state, do you train the network in some way to make sure it does not make an inaccurate prediction around the unsafe state?\n- There is a lot of repetitive content in the paper that can be discarded to condense down the paper and make it more readable.\n- It would be nice to see more tasks or at least one that was more realistic... The tasks used in the paper appear to be common ones but they still feel rather artificial. Also, it seems in the paper there are only learning curves for 2 of the 3 tasks.\n- In the related work you say \"However, these methods are difficult to quantify and depend a lot on the diversity of settings in which they are performed.\" Can you expand on this? Why do they depend on these issues so much? The motivation for your method stems from these methods not being good enough, so further detail on this facet is important.\n- If you are using environments with discrete actions and performing prediction I am not sure if that can be called imaginative. Rather it should be called sampling. The forward model does not even appear to be stochastic.\n- The description of the imagination module training is not very clear. Is it trained on 5000 tuples or is the network trained for 5000 updates? There needs to be much more detail on this process.\n\nOverall the method seems interesting but does not appear to be a significant improvement.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper2130/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2130/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Avoiding Negative Side-Effects and Promoting Safe Exploration with Imaginative Planning", "authors": ["Dhruv Ramani", "Benjamin Eysenbach"], "authorids": ["dhruvramani98@gmail.com", "beysenba@cs.cmu.edu"], "keywords": ["Reinforcement Learning", "AI-Safety", "Model-Based Reinforcement Learning", "Safe-Exploration"], "abstract": "  With the recent proliferation of the usage of reinforcement learning (RL) agents for solving real-world tasks, safety emerges as a necessary ingredient for their successful application. In this paper, we focus on ensuring the safety of the agent while making sure that the agent does not cause any unnecessary disruptions to its environment. The current approaches to this problem, such as manually constraining the agent or adding a safety penalty to the reward function, can introduce bad incentives. In complex domains, these approaches are simply intractable, as they require knowing apriori all the possible unsafe scenarios an agent could encounter. We propose a model-based approach to safety that allows the agent to look into the future and be aware of the future consequences of its actions. We learn the transition dynamics of the environment and generate a directed graph called the imaginative module. This graph encapsulates all possible trajectories that can be followed by the agent, allowing the agent to efficiently traverse through the imagined environment without ever taking any action in reality. A baseline state, which can either represent a safe or an unsafe state (based on whichever is easier to define) is taken as a human input, and the imaginative module is used to predict whether the current actions of the agent can cause it to end up in dangerous states in the future. Our imaginative module can be seen as a ``plug-and-play'' approach to ensuring safety, as it is compatible with any existing RL algorithm and any task with discrete action space. Our method induces the agent to act safely while learning to solve the task. We experimentally validate our proposal on two gridworld environments and a self-driving car simulator, demonstrating that our approach to safety visits unsafe states significantly less frequently than a baseline.", "pdf": "/pdf/bdba2804d40b580b341591526ce4971f8f5611dc.pdf", "paperhash": "ramani|avoiding_negative_sideeffects_and_promoting_safe_exploration_with_imaginative_planning", "original_pdf": "/attachment/bdba2804d40b580b341591526ce4971f8f5611dc.pdf", "_bibtex": "@misc{\nramani2020avoiding,\ntitle={Avoiding Negative Side-Effects and Promoting Safe Exploration with Imaginative Planning},\nauthor={Dhruv Ramani and Benjamin Eysenbach},\nyear={2020},\nurl={https://openreview.net/forum?id=HJe7bxBYvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJe7bxBYvr", "replyto": "HJe7bxBYvr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2130/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2130/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575338019688, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2130/Reviewers"], "noninvitees": [], "tcdate": 1570237727277, "tmdate": 1575338019702, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2130/-/Official_Review"}}}, {"id": "HkxTiwWTYB", "original": null, "number": 2, "cdate": 1571784612930, "ddate": null, "tcdate": 1571784612930, "tmdate": 1572972379215, "tddate": null, "forum": "HJe7bxBYvr", "replyto": "HJe7bxBYvr", "invitation": "ICLR.cc/2020/Conference/Paper2130/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper presents a model-based approach to safety in RL, where the agent uses a transition model to plan ahead to avoid actions that can lead it to unsafe states. They call the planning component an imaginative module. The agent takes the baseline state as input - that can be used to define either a safe or unsafe state, that is used in the planning component. The authors claim that using these two techniques they can tackle both the safe exploration (not violating safety constraints during learning) and irreversible side-effects (unintended irreversible behavior due to poorly designed reward-function).  They validate their approach on two grid world environments and self-driving car simulators.\n \nThis paper should be rejected because of the assumptions it makes goes against the very task they are trying to solve. In the sense, the task is trivial given the assumptions they have. \n\n1) The inconsistent assumption regarding the access to trajectories to learn a model. \nThe authors start with the assumption that the agent does not have access to the model (Sec 3) , and they explicitly learn the model. However, in the very next section (Sec 4), the authors assume that they can deploy a number of agents that interact with the environment randomly and collect that data to learn a complete transition model. Note that this assumption is wrong because:\nIf the random data agents are \u201csafe\u201d, i.e., don\u2019t violate any safety constraint or cause any harmful behavior in the environment, then it is equivalent to assuming the agent having access to all the data to learn the model. This is a very big assumption that essentially says the agent has access to the model, which defeats the purpose of the safe-exploration problem. \nIf the random agents are \u201cunsafe\u201d, i.e., they can violate the safety constraint, then it goes against the very claim made about their method being able to respect the constraints throughout the learning process. \n\n2) The assumption about the baseline state(s).  \nThis is also a pretty big assumption to have, that is not acknowledged in the paper. If the agent already has the set of all the states it needs to avoid (or the set of states that are safe), then along with the assumption regarding access to the model, solving reversibility is significantly easier task then the general safe exploration problem [1, 2]\n\n3) The results reported in Figure 4 are not statistically significant. The experiments are only run over 3 random seeds [3] \n\n4) Can you give a few more details about the assumptions? In terms of how realistic they are or how essential they are to the method.\n\n\nThings to improve the paper that did not impact the score:\n- The negative side-effects problem that this work address is only based on reversibility criteria. \nClaim about learning the dynamics model is sample efficient is unsupported. \n\n\n\nReferences: \n[1] Berkenkamp, Felix, et al. \"Safe model-based reinforcement learning with stability guarantees.\" Advances in neural information processing systems. 2017.\n\n[2] Dalal, Gal, et al. \"Safe exploration in continuous action spaces.\" arXiv preprint arXiv:1801.08757 (2018).\n\n[3] Henderson, Peter, et al. \"Deep reinforcement learning that matters.\" Thirty-Second AAAI Conference on Artificial Intelligence. 2018.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2130/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2130/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Avoiding Negative Side-Effects and Promoting Safe Exploration with Imaginative Planning", "authors": ["Dhruv Ramani", "Benjamin Eysenbach"], "authorids": ["dhruvramani98@gmail.com", "beysenba@cs.cmu.edu"], "keywords": ["Reinforcement Learning", "AI-Safety", "Model-Based Reinforcement Learning", "Safe-Exploration"], "abstract": "  With the recent proliferation of the usage of reinforcement learning (RL) agents for solving real-world tasks, safety emerges as a necessary ingredient for their successful application. In this paper, we focus on ensuring the safety of the agent while making sure that the agent does not cause any unnecessary disruptions to its environment. The current approaches to this problem, such as manually constraining the agent or adding a safety penalty to the reward function, can introduce bad incentives. In complex domains, these approaches are simply intractable, as they require knowing apriori all the possible unsafe scenarios an agent could encounter. We propose a model-based approach to safety that allows the agent to look into the future and be aware of the future consequences of its actions. We learn the transition dynamics of the environment and generate a directed graph called the imaginative module. This graph encapsulates all possible trajectories that can be followed by the agent, allowing the agent to efficiently traverse through the imagined environment without ever taking any action in reality. A baseline state, which can either represent a safe or an unsafe state (based on whichever is easier to define) is taken as a human input, and the imaginative module is used to predict whether the current actions of the agent can cause it to end up in dangerous states in the future. Our imaginative module can be seen as a ``plug-and-play'' approach to ensuring safety, as it is compatible with any existing RL algorithm and any task with discrete action space. Our method induces the agent to act safely while learning to solve the task. We experimentally validate our proposal on two gridworld environments and a self-driving car simulator, demonstrating that our approach to safety visits unsafe states significantly less frequently than a baseline.", "pdf": "/pdf/bdba2804d40b580b341591526ce4971f8f5611dc.pdf", "paperhash": "ramani|avoiding_negative_sideeffects_and_promoting_safe_exploration_with_imaginative_planning", "original_pdf": "/attachment/bdba2804d40b580b341591526ce4971f8f5611dc.pdf", "_bibtex": "@misc{\nramani2020avoiding,\ntitle={Avoiding Negative Side-Effects and Promoting Safe Exploration with Imaginative Planning},\nauthor={Dhruv Ramani and Benjamin Eysenbach},\nyear={2020},\nurl={https://openreview.net/forum?id=HJe7bxBYvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJe7bxBYvr", "replyto": "HJe7bxBYvr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2130/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2130/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575338019688, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2130/Reviewers"], "noninvitees": [], "tcdate": 1570237727277, "tmdate": 1575338019702, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2130/-/Official_Review"}}}, {"id": "rygzKGNTtH", "original": null, "number": 3, "cdate": 1571795578469, "ddate": null, "tcdate": 1571795578469, "tmdate": 1572972379168, "tddate": null, "forum": "HJe7bxBYvr", "replyto": "HJe7bxBYvr", "invitation": "ICLR.cc/2020/Conference/Paper2130/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "N/A", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "This paper proposes to use learned transition models to do two separate things: (i) avoid unsafe states and (ii) allow an alternative channel for task reward specification. The idea is to create a comprehensive connectivity graph of the states in the environment. Once done, an agent can avoid unsafe states by avoiding states that are unconnected to a specified safe state. A practitioner might also specify safe/unsafe states as an additional source of information about the reward.\n\nThis paper suffers from poor and loose writing, incomplete specification of its experiments, unrealistic assumptions during evaluation (Sec 5.3 \"we create the graph using rollouts from the actual environment\" to avoid errors from learning a transition model). \n\nThe paper does not address basic concerns with its approach: how is the model to be learned at all, if it is to be comprehensive in the way that is necessary for the connectivity graph (which this paper calls an \"imaginative module\")? The authors say this is done through multiple agents performing random actions in the environment, in which case, isn't this extremely unsafe training time by the paper's own definition of safe exploration? \n\nFurther, creating a complete connectivity graph is unrealistic even for fully known transition models in most reasonably complex settings, such as, say, Go or Chess. \n\nIf the transition model is fully known as in the car racing setting, why not directly use that to plan and solve the game?\n\nExperiments show fewer \"unsafe\" states for the paper's approach compared to a method that has no way to know that those states are unsafe. How is this a reasonable validation, especially when the transition model is fully known? Also, this is an insufficient metric by itself as it says nothing about whether the method actually performed well at the task."}, "signatures": ["ICLR.cc/2020/Conference/Paper2130/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2130/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Avoiding Negative Side-Effects and Promoting Safe Exploration with Imaginative Planning", "authors": ["Dhruv Ramani", "Benjamin Eysenbach"], "authorids": ["dhruvramani98@gmail.com", "beysenba@cs.cmu.edu"], "keywords": ["Reinforcement Learning", "AI-Safety", "Model-Based Reinforcement Learning", "Safe-Exploration"], "abstract": "  With the recent proliferation of the usage of reinforcement learning (RL) agents for solving real-world tasks, safety emerges as a necessary ingredient for their successful application. In this paper, we focus on ensuring the safety of the agent while making sure that the agent does not cause any unnecessary disruptions to its environment. The current approaches to this problem, such as manually constraining the agent or adding a safety penalty to the reward function, can introduce bad incentives. In complex domains, these approaches are simply intractable, as they require knowing apriori all the possible unsafe scenarios an agent could encounter. We propose a model-based approach to safety that allows the agent to look into the future and be aware of the future consequences of its actions. We learn the transition dynamics of the environment and generate a directed graph called the imaginative module. This graph encapsulates all possible trajectories that can be followed by the agent, allowing the agent to efficiently traverse through the imagined environment without ever taking any action in reality. A baseline state, which can either represent a safe or an unsafe state (based on whichever is easier to define) is taken as a human input, and the imaginative module is used to predict whether the current actions of the agent can cause it to end up in dangerous states in the future. Our imaginative module can be seen as a ``plug-and-play'' approach to ensuring safety, as it is compatible with any existing RL algorithm and any task with discrete action space. Our method induces the agent to act safely while learning to solve the task. We experimentally validate our proposal on two gridworld environments and a self-driving car simulator, demonstrating that our approach to safety visits unsafe states significantly less frequently than a baseline.", "pdf": "/pdf/bdba2804d40b580b341591526ce4971f8f5611dc.pdf", "paperhash": "ramani|avoiding_negative_sideeffects_and_promoting_safe_exploration_with_imaginative_planning", "original_pdf": "/attachment/bdba2804d40b580b341591526ce4971f8f5611dc.pdf", "_bibtex": "@misc{\nramani2020avoiding,\ntitle={Avoiding Negative Side-Effects and Promoting Safe Exploration with Imaginative Planning},\nauthor={Dhruv Ramani and Benjamin Eysenbach},\nyear={2020},\nurl={https://openreview.net/forum?id=HJe7bxBYvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJe7bxBYvr", "replyto": "HJe7bxBYvr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2130/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2130/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575338019688, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2130/Reviewers"], "noninvitees": [], "tcdate": 1570237727277, "tmdate": 1575338019702, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2130/-/Official_Review"}}}], "count": 5}