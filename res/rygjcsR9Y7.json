{"notes": [{"id": "rklleT7c24", "original": null, "number": 10, "cdate": 1557966055930, "ddate": null, "tcdate": 1557966055930, "tmdate": 1557966055930, "tddate": null, "forum": "rygjcsR9Y7", "replyto": "S1xeNtXq3E", "invitation": "ICLR.cc/2019/Conference/-/Paper562/Official_Comment", "content": {"title": "Rebuttal not public", "comment": "The authors chose to make their rebuttals privately to the reviewers, which is something that OpenReview supports."}, "signatures": ["ICLR.cc/2019/Conference/Paper562/Area_Chair1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper562/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper562/Area_Chair1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SOM-VAE: Interpretable Discrete Representation Learning on Time Series", "abstract": "High-dimensional time series are common in many domains. Since human cognition is not optimized to work well in high-dimensional spaces, these areas could benefit from interpretable low-dimensional representations. However, most representation learning algorithms for time series data are difficult to interpret. This is due to non-intuitive mappings from data features to salient properties of the representation and non-smoothness over time.\nTo address this problem, we propose a new representation learning framework building on ideas from interpretable discrete dimensionality reduction and deep generative modeling. This framework allows us to learn discrete representations of time series, which give rise to smooth and interpretable embeddings with superior clustering performance. We introduce a new way to overcome the non-differentiability in discrete representation learning and present a gradient-based version of the traditional self-organizing map algorithm that is more performant than the original. Furthermore, to allow for a probabilistic interpretation of our method, we integrate a Markov model in the representation space.\nThis model uncovers the temporal transition structure, improves clustering performance even further and provides additional explanatory insights as well as a natural representation of uncertainty.\nWe evaluate our model in terms of clustering performance and interpretability on static (Fashion-)MNIST data, a time series of linearly interpolated (Fashion-)MNIST images, a chaotic Lorenz attractor system with two macro states, as well as on a challenging real world medical time series application on the eICU data set. Our learned representations compare favorably with competitor methods and facilitate downstream tasks on the real world data.", "keywords": ["deep learning", "self-organizing map", "variational autoencoder", "representation learning", "time series", "machine learning", "interpretability"], "authorids": ["fortuin@inf.ethz.ch", "mhueser@inf.ethz.ch", "locatelf@inf.ethz.ch", "heiko.strathmann@gmail.com", "raetsch@inf.ethz.ch"], "authors": ["Vincent Fortuin", "Matthias H\u00fcser", "Francesco Locatello", "Heiko Strathmann", "Gunnar R\u00e4tsch"], "TL;DR": "We present a method to learn interpretable representations on time series using ideas from variational autoencoders, self-organizing maps and probabilistic models.", "pdf": "/pdf/15c8cbb113daa27eb888054e58b057bcfeb1a203.pdf", "paperhash": "fortuin|somvae_interpretable_discrete_representation_learning_on_time_series", "_bibtex": "@inproceedings{\nfortuin2018deep,\ntitle={Deep Self-Organization: Interpretable Discrete Representation Learning on Time Series},\nauthor={Vincent Fortuin and Matthias H\u00fcser and Francesco Locatello and Heiko Strathmann and Gunnar R\u00e4tsch},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rygjcsR9Y7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper562/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621620859, "tddate": null, "super": null, "final": null, "reply": {"forum": "rygjcsR9Y7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper562/Authors", "ICLR.cc/2019/Conference/Paper562/Reviewers", "ICLR.cc/2019/Conference/Paper562/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper562/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper562/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper562/Authors|ICLR.cc/2019/Conference/Paper562/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper562/Reviewers", "ICLR.cc/2019/Conference/Paper562/Authors", "ICLR.cc/2019/Conference/Paper562/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621620859}}}, {"id": "S1xeNtXq3E", "original": null, "number": 1, "cdate": 1557965096269, "ddate": null, "tcdate": 1557965096269, "tmdate": 1557965096269, "tddate": null, "forum": "rygjcsR9Y7", "replyto": "rygjcsR9Y7", "invitation": "ICLR.cc/2019/Conference/-/Paper562/Public_Comment", "content": {"comment": "I am wondering why I don't see any rebuttal in this paper. From the meta-review, it seems that those comment did exist. So is it deleted? It very significantly reduces the comprehension of the paper.", "title": "All the rebutal deleted or there is no rebutal at all?"}, "signatures": ["~Chun-Hao_Chang1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Chun-Hao_Chang1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SOM-VAE: Interpretable Discrete Representation Learning on Time Series", "abstract": "High-dimensional time series are common in many domains. Since human cognition is not optimized to work well in high-dimensional spaces, these areas could benefit from interpretable low-dimensional representations. However, most representation learning algorithms for time series data are difficult to interpret. This is due to non-intuitive mappings from data features to salient properties of the representation and non-smoothness over time.\nTo address this problem, we propose a new representation learning framework building on ideas from interpretable discrete dimensionality reduction and deep generative modeling. This framework allows us to learn discrete representations of time series, which give rise to smooth and interpretable embeddings with superior clustering performance. We introduce a new way to overcome the non-differentiability in discrete representation learning and present a gradient-based version of the traditional self-organizing map algorithm that is more performant than the original. Furthermore, to allow for a probabilistic interpretation of our method, we integrate a Markov model in the representation space.\nThis model uncovers the temporal transition structure, improves clustering performance even further and provides additional explanatory insights as well as a natural representation of uncertainty.\nWe evaluate our model in terms of clustering performance and interpretability on static (Fashion-)MNIST data, a time series of linearly interpolated (Fashion-)MNIST images, a chaotic Lorenz attractor system with two macro states, as well as on a challenging real world medical time series application on the eICU data set. Our learned representations compare favorably with competitor methods and facilitate downstream tasks on the real world data.", "keywords": ["deep learning", "self-organizing map", "variational autoencoder", "representation learning", "time series", "machine learning", "interpretability"], "authorids": ["fortuin@inf.ethz.ch", "mhueser@inf.ethz.ch", "locatelf@inf.ethz.ch", "heiko.strathmann@gmail.com", "raetsch@inf.ethz.ch"], "authors": ["Vincent Fortuin", "Matthias H\u00fcser", "Francesco Locatello", "Heiko Strathmann", "Gunnar R\u00e4tsch"], "TL;DR": "We present a method to learn interpretable representations on time series using ideas from variational autoencoders, self-organizing maps and probabilistic models.", "pdf": "/pdf/15c8cbb113daa27eb888054e58b057bcfeb1a203.pdf", "paperhash": "fortuin|somvae_interpretable_discrete_representation_learning_on_time_series", "_bibtex": "@inproceedings{\nfortuin2018deep,\ntitle={Deep Self-Organization: Interpretable Discrete Representation Learning on Time Series},\nauthor={Vincent Fortuin and Matthias H\u00fcser and Francesco Locatello and Heiko Strathmann and Gunnar R\u00e4tsch},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rygjcsR9Y7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper562/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311807314, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "rygjcsR9Y7", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper562/Authors", "ICLR.cc/2019/Conference/Paper562/Reviewers", "ICLR.cc/2019/Conference/Paper562/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper562/Authors", "ICLR.cc/2019/Conference/Paper562/Reviewers", "ICLR.cc/2019/Conference/Paper562/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311807314}}}, {"id": "rygjcsR9Y7", "original": "HJeFJOcqF7", "number": 562, "cdate": 1538087826744, "ddate": null, "tcdate": 1538087826744, "tmdate": 1546612154333, "tddate": null, "forum": "rygjcsR9Y7", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "SOM-VAE: Interpretable Discrete Representation Learning on Time Series", "abstract": "High-dimensional time series are common in many domains. Since human cognition is not optimized to work well in high-dimensional spaces, these areas could benefit from interpretable low-dimensional representations. However, most representation learning algorithms for time series data are difficult to interpret. This is due to non-intuitive mappings from data features to salient properties of the representation and non-smoothness over time.\nTo address this problem, we propose a new representation learning framework building on ideas from interpretable discrete dimensionality reduction and deep generative modeling. This framework allows us to learn discrete representations of time series, which give rise to smooth and interpretable embeddings with superior clustering performance. We introduce a new way to overcome the non-differentiability in discrete representation learning and present a gradient-based version of the traditional self-organizing map algorithm that is more performant than the original. Furthermore, to allow for a probabilistic interpretation of our method, we integrate a Markov model in the representation space.\nThis model uncovers the temporal transition structure, improves clustering performance even further and provides additional explanatory insights as well as a natural representation of uncertainty.\nWe evaluate our model in terms of clustering performance and interpretability on static (Fashion-)MNIST data, a time series of linearly interpolated (Fashion-)MNIST images, a chaotic Lorenz attractor system with two macro states, as well as on a challenging real world medical time series application on the eICU data set. Our learned representations compare favorably with competitor methods and facilitate downstream tasks on the real world data.", "keywords": ["deep learning", "self-organizing map", "variational autoencoder", "representation learning", "time series", "machine learning", "interpretability"], "authorids": ["fortuin@inf.ethz.ch", "mhueser@inf.ethz.ch", "locatelf@inf.ethz.ch", "heiko.strathmann@gmail.com", "raetsch@inf.ethz.ch"], "authors": ["Vincent Fortuin", "Matthias H\u00fcser", "Francesco Locatello", "Heiko Strathmann", "Gunnar R\u00e4tsch"], "TL;DR": "We present a method to learn interpretable representations on time series using ideas from variational autoencoders, self-organizing maps and probabilistic models.", "pdf": "/pdf/15c8cbb113daa27eb888054e58b057bcfeb1a203.pdf", "paperhash": "fortuin|somvae_interpretable_discrete_representation_learning_on_time_series", "_bibtex": "@inproceedings{\nfortuin2018deep,\ntitle={Deep Self-Organization: Interpretable Discrete Representation Learning on Time Series},\nauthor={Vincent Fortuin and Matthias H\u00fcser and Francesco Locatello and Heiko Strathmann and Gunnar R\u00e4tsch},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rygjcsR9Y7},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "BygrPrxHe4", "original": null, "number": 1, "cdate": 1545041245197, "ddate": null, "tcdate": 1545041245197, "tmdate": 1545354483852, "tddate": null, "forum": "rygjcsR9Y7", "replyto": "rygjcsR9Y7", "invitation": "ICLR.cc/2019/Conference/-/Paper562/Meta_Review", "content": {"metareview": "This paper combines probabilistic models, VAEs, and self-organizing maps to learn interpretable representations on time series. The proposed contributions are a novel and interesting combination of existing ideas, in particular, the extension to time-series data by modeling the cluster dynamics. The empirical results show improved unsupervised clustering performance, on both synthetic and real datasets, compared to a number of baselines. The resulting 2D embedding also provides an interpretable visualization.\n\nThe reviewers and the AC identified a number of potential weaknesses in the presentation in the original submission: (1) there was insufficient background on SOMs, leaving the readers unable to comprehend the contributions, (2) some of the details about the experiments were missing, such as how the baselines were constructed, (3) additional experiments were needed in regards to the hyper-parameters, such as number of clusters and the weighting in the loss, and (4) Figure 4d required a description of the results.\n\nThe revision and the comments by the authors addressed most of these comments, and the reviewers felt that their concerns had been alleviated.\n\nThus, the reviewers felt the paper should be accepted.", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Accept (Poster)", "title": "Solid contributions, strong results, well-written"}, "signatures": ["ICLR.cc/2019/Conference/Paper562/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper562/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SOM-VAE: Interpretable Discrete Representation Learning on Time Series", "abstract": "High-dimensional time series are common in many domains. Since human cognition is not optimized to work well in high-dimensional spaces, these areas could benefit from interpretable low-dimensional representations. However, most representation learning algorithms for time series data are difficult to interpret. This is due to non-intuitive mappings from data features to salient properties of the representation and non-smoothness over time.\nTo address this problem, we propose a new representation learning framework building on ideas from interpretable discrete dimensionality reduction and deep generative modeling. This framework allows us to learn discrete representations of time series, which give rise to smooth and interpretable embeddings with superior clustering performance. We introduce a new way to overcome the non-differentiability in discrete representation learning and present a gradient-based version of the traditional self-organizing map algorithm that is more performant than the original. Furthermore, to allow for a probabilistic interpretation of our method, we integrate a Markov model in the representation space.\nThis model uncovers the temporal transition structure, improves clustering performance even further and provides additional explanatory insights as well as a natural representation of uncertainty.\nWe evaluate our model in terms of clustering performance and interpretability on static (Fashion-)MNIST data, a time series of linearly interpolated (Fashion-)MNIST images, a chaotic Lorenz attractor system with two macro states, as well as on a challenging real world medical time series application on the eICU data set. Our learned representations compare favorably with competitor methods and facilitate downstream tasks on the real world data.", "keywords": ["deep learning", "self-organizing map", "variational autoencoder", "representation learning", "time series", "machine learning", "interpretability"], "authorids": ["fortuin@inf.ethz.ch", "mhueser@inf.ethz.ch", "locatelf@inf.ethz.ch", "heiko.strathmann@gmail.com", "raetsch@inf.ethz.ch"], "authors": ["Vincent Fortuin", "Matthias H\u00fcser", "Francesco Locatello", "Heiko Strathmann", "Gunnar R\u00e4tsch"], "TL;DR": "We present a method to learn interpretable representations on time series using ideas from variational autoencoders, self-organizing maps and probabilistic models.", "pdf": "/pdf/15c8cbb113daa27eb888054e58b057bcfeb1a203.pdf", "paperhash": "fortuin|somvae_interpretable_discrete_representation_learning_on_time_series", "_bibtex": "@inproceedings{\nfortuin2018deep,\ntitle={Deep Self-Organization: Interpretable Discrete Representation Learning on Time Series},\nauthor={Vincent Fortuin and Matthias H\u00fcser and Francesco Locatello and Heiko Strathmann and Gunnar R\u00e4tsch},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rygjcsR9Y7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper562/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353172328, "tddate": null, "super": null, "final": null, "reply": {"forum": "rygjcsR9Y7", "replyto": "rygjcsR9Y7", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper562/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper562/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper562/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353172328}}}, {"id": "H1gXD7Qa2m", "original": null, "number": 2, "cdate": 1541383002875, "ddate": null, "tcdate": 1541383002875, "tmdate": 1543700966121, "tddate": null, "forum": "rygjcsR9Y7", "replyto": "rygjcsR9Y7", "invitation": "ICLR.cc/2019/Conference/-/Paper562/Official_Review", "content": {"title": "A representation learning method for time series data", "review": "This paper proposes a deep learning method for representation learning in time series data. The goal is to learn a discrete two-dimensional representation of the time series data in an interpretable manner. The model is constructed on the basis of self-organizing maps (SOM) and involves reconstruction error in the training. In order to address the non-differentiability in the discrete representation assignment, the authors propose to include an extra reconstruction loss term w.r.t. the discrete representation. The authors conduct experiments on both static and time series data and validate that the method perform better than related methods in terms of clustering results as well as interpretability.\n\nThis paper deals with an interesting problem as learning an interpretable representation in time series data is important in areas such as health care and business. However, I am afraid the presentation of this paper is a bit difficult to follow. Some concerns/questions as below:\n\n1) As the paper is based on SOM, some illustration of this method would be helpful for readers to understand the idea and learn the major contribution;\n\n2) The authors use NMI and purity to evaluate the clustering performance. I was curious why not use the clustering accuracy as well?\n\n3) Some more explanation on Fig. 4(d) would be helpful.", "rating": "6: Marginally above acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2019/Conference/Paper562/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "SOM-VAE: Interpretable Discrete Representation Learning on Time Series", "abstract": "High-dimensional time series are common in many domains. Since human cognition is not optimized to work well in high-dimensional spaces, these areas could benefit from interpretable low-dimensional representations. However, most representation learning algorithms for time series data are difficult to interpret. This is due to non-intuitive mappings from data features to salient properties of the representation and non-smoothness over time.\nTo address this problem, we propose a new representation learning framework building on ideas from interpretable discrete dimensionality reduction and deep generative modeling. This framework allows us to learn discrete representations of time series, which give rise to smooth and interpretable embeddings with superior clustering performance. We introduce a new way to overcome the non-differentiability in discrete representation learning and present a gradient-based version of the traditional self-organizing map algorithm that is more performant than the original. Furthermore, to allow for a probabilistic interpretation of our method, we integrate a Markov model in the representation space.\nThis model uncovers the temporal transition structure, improves clustering performance even further and provides additional explanatory insights as well as a natural representation of uncertainty.\nWe evaluate our model in terms of clustering performance and interpretability on static (Fashion-)MNIST data, a time series of linearly interpolated (Fashion-)MNIST images, a chaotic Lorenz attractor system with two macro states, as well as on a challenging real world medical time series application on the eICU data set. Our learned representations compare favorably with competitor methods and facilitate downstream tasks on the real world data.", "keywords": ["deep learning", "self-organizing map", "variational autoencoder", "representation learning", "time series", "machine learning", "interpretability"], "authorids": ["fortuin@inf.ethz.ch", "mhueser@inf.ethz.ch", "locatelf@inf.ethz.ch", "heiko.strathmann@gmail.com", "raetsch@inf.ethz.ch"], "authors": ["Vincent Fortuin", "Matthias H\u00fcser", "Francesco Locatello", "Heiko Strathmann", "Gunnar R\u00e4tsch"], "TL;DR": "We present a method to learn interpretable representations on time series using ideas from variational autoencoders, self-organizing maps and probabilistic models.", "pdf": "/pdf/15c8cbb113daa27eb888054e58b057bcfeb1a203.pdf", "paperhash": "fortuin|somvae_interpretable_discrete_representation_learning_on_time_series", "_bibtex": "@inproceedings{\nfortuin2018deep,\ntitle={Deep Self-Organization: Interpretable Discrete Representation Learning on Time Series},\nauthor={Vincent Fortuin and Matthias H\u00fcser and Francesco Locatello and Heiko Strathmann and Gunnar R\u00e4tsch},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rygjcsR9Y7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper562/Official_Review", "cdate": 1542234433070, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "rygjcsR9Y7", "replyto": "rygjcsR9Y7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper562/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335752523, "tmdate": 1552335752523, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper562/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "rkgYwm-Q0Q", "original": null, "number": 5, "cdate": 1542816608685, "ddate": null, "tcdate": 1542816608685, "tmdate": 1542816608685, "tddate": null, "forum": "rygjcsR9Y7", "replyto": "BygvWeDu6X", "invitation": "ICLR.cc/2019/Conference/-/Paper562/Official_Comment", "content": {"title": "The revised version addressed my concerns", "comment": "I would like to thank the authors for adding more details and clarifications in the appendix. The revised version addressed my concerns. I have no further questions."}, "signatures": ["ICLR.cc/2019/Conference/Paper562/AnonReviewer3"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper562/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper562/AnonReviewer3", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SOM-VAE: Interpretable Discrete Representation Learning on Time Series", "abstract": "High-dimensional time series are common in many domains. Since human cognition is not optimized to work well in high-dimensional spaces, these areas could benefit from interpretable low-dimensional representations. However, most representation learning algorithms for time series data are difficult to interpret. This is due to non-intuitive mappings from data features to salient properties of the representation and non-smoothness over time.\nTo address this problem, we propose a new representation learning framework building on ideas from interpretable discrete dimensionality reduction and deep generative modeling. This framework allows us to learn discrete representations of time series, which give rise to smooth and interpretable embeddings with superior clustering performance. We introduce a new way to overcome the non-differentiability in discrete representation learning and present a gradient-based version of the traditional self-organizing map algorithm that is more performant than the original. Furthermore, to allow for a probabilistic interpretation of our method, we integrate a Markov model in the representation space.\nThis model uncovers the temporal transition structure, improves clustering performance even further and provides additional explanatory insights as well as a natural representation of uncertainty.\nWe evaluate our model in terms of clustering performance and interpretability on static (Fashion-)MNIST data, a time series of linearly interpolated (Fashion-)MNIST images, a chaotic Lorenz attractor system with two macro states, as well as on a challenging real world medical time series application on the eICU data set. Our learned representations compare favorably with competitor methods and facilitate downstream tasks on the real world data.", "keywords": ["deep learning", "self-organizing map", "variational autoencoder", "representation learning", "time series", "machine learning", "interpretability"], "authorids": ["fortuin@inf.ethz.ch", "mhueser@inf.ethz.ch", "locatelf@inf.ethz.ch", "heiko.strathmann@gmail.com", "raetsch@inf.ethz.ch"], "authors": ["Vincent Fortuin", "Matthias H\u00fcser", "Francesco Locatello", "Heiko Strathmann", "Gunnar R\u00e4tsch"], "TL;DR": "We present a method to learn interpretable representations on time series using ideas from variational autoencoders, self-organizing maps and probabilistic models.", "pdf": "/pdf/15c8cbb113daa27eb888054e58b057bcfeb1a203.pdf", "paperhash": "fortuin|somvae_interpretable_discrete_representation_learning_on_time_series", "_bibtex": "@inproceedings{\nfortuin2018deep,\ntitle={Deep Self-Organization: Interpretable Discrete Representation Learning on Time Series},\nauthor={Vincent Fortuin and Matthias H\u00fcser and Francesco Locatello and Heiko Strathmann and Gunnar R\u00e4tsch},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rygjcsR9Y7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper562/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621620859, "tddate": null, "super": null, "final": null, "reply": {"forum": "rygjcsR9Y7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper562/Authors", "ICLR.cc/2019/Conference/Paper562/Reviewers", "ICLR.cc/2019/Conference/Paper562/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper562/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper562/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper562/Authors|ICLR.cc/2019/Conference/Paper562/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper562/Reviewers", "ICLR.cc/2019/Conference/Paper562/Authors", "ICLR.cc/2019/Conference/Paper562/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621620859}}}, {"id": "HkgyBz103m", "original": null, "number": 3, "cdate": 1541431862527, "ddate": null, "tcdate": 1541431862527, "tmdate": 1541533888072, "tddate": null, "forum": "rygjcsR9Y7", "replyto": "rygjcsR9Y7", "invitation": "ICLR.cc/2019/Conference/-/Paper562/Official_Review", "content": {"title": "Very nice research", "review": "This paper proposes a novel clustering technique that combines the self-organising map (SOM) (Kohonen, 1998) ideas with the differentiable quantized clustering ideas of VQ-VAE (van den Oord et al, 2017). The resulting algorithm is able to achieve better unsupervised clustering than either technique on its own. It also beats the k-means clustering approach. The authors also suggest augmenting their setup with a model of cluster transition dynamics for time-series data, which seems to improve the clustering further, as well as providing an interpretable 2D visualisation of the system's dynamics.\n\nThis approach addresses an important problem of easy interpretable visualisation of complex dynamics of a multi-dimensional system. This solution can have immediate wide spread real life applications, for example in fields like medicine or finance. The paper is very well written and the model clearly outperforms its baselines. The authors also include very nice evaluation of the importance of the different parts of the model for the final performance.\n\nThis is one of the best papers I have reviewed in a while. The only question I have is in terms of the medical data. The map learnt by SOM-VAE-prob presented in Fig. 4 appears to have 2 clusters with 'less healthy' patients (near the top left and top right edges). It would be good to have an analysis of what differences there are between these two clusters, and whether they are recovered consistently. \n\n\n", "rating": "9: Top 15% of accepted papers, strong accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper562/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SOM-VAE: Interpretable Discrete Representation Learning on Time Series", "abstract": "High-dimensional time series are common in many domains. Since human cognition is not optimized to work well in high-dimensional spaces, these areas could benefit from interpretable low-dimensional representations. However, most representation learning algorithms for time series data are difficult to interpret. This is due to non-intuitive mappings from data features to salient properties of the representation and non-smoothness over time.\nTo address this problem, we propose a new representation learning framework building on ideas from interpretable discrete dimensionality reduction and deep generative modeling. This framework allows us to learn discrete representations of time series, which give rise to smooth and interpretable embeddings with superior clustering performance. We introduce a new way to overcome the non-differentiability in discrete representation learning and present a gradient-based version of the traditional self-organizing map algorithm that is more performant than the original. Furthermore, to allow for a probabilistic interpretation of our method, we integrate a Markov model in the representation space.\nThis model uncovers the temporal transition structure, improves clustering performance even further and provides additional explanatory insights as well as a natural representation of uncertainty.\nWe evaluate our model in terms of clustering performance and interpretability on static (Fashion-)MNIST data, a time series of linearly interpolated (Fashion-)MNIST images, a chaotic Lorenz attractor system with two macro states, as well as on a challenging real world medical time series application on the eICU data set. Our learned representations compare favorably with competitor methods and facilitate downstream tasks on the real world data.", "keywords": ["deep learning", "self-organizing map", "variational autoencoder", "representation learning", "time series", "machine learning", "interpretability"], "authorids": ["fortuin@inf.ethz.ch", "mhueser@inf.ethz.ch", "locatelf@inf.ethz.ch", "heiko.strathmann@gmail.com", "raetsch@inf.ethz.ch"], "authors": ["Vincent Fortuin", "Matthias H\u00fcser", "Francesco Locatello", "Heiko Strathmann", "Gunnar R\u00e4tsch"], "TL;DR": "We present a method to learn interpretable representations on time series using ideas from variational autoencoders, self-organizing maps and probabilistic models.", "pdf": "/pdf/15c8cbb113daa27eb888054e58b057bcfeb1a203.pdf", "paperhash": "fortuin|somvae_interpretable_discrete_representation_learning_on_time_series", "_bibtex": "@inproceedings{\nfortuin2018deep,\ntitle={Deep Self-Organization: Interpretable Discrete Representation Learning on Time Series},\nauthor={Vincent Fortuin and Matthias H\u00fcser and Francesco Locatello and Heiko Strathmann and Gunnar R\u00e4tsch},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rygjcsR9Y7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper562/Official_Review", "cdate": 1542234433070, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "rygjcsR9Y7", "replyto": "rygjcsR9Y7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper562/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335752523, "tmdate": 1552335752523, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper562/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "SJeYiFul3X", "original": null, "number": 1, "cdate": 1540553120609, "ddate": null, "tcdate": 1540553120609, "tmdate": 1541533887657, "tddate": null, "forum": "rygjcsR9Y7", "replyto": "rygjcsR9Y7", "invitation": "ICLR.cc/2019/Conference/-/Paper562/Official_Review", "content": {"title": "Interesting work, methodological aspects and implementation details to be clarified. Improving comparison with state of the art", "review": "This work addresses the problem of learning latent embeddings of high-dimensional time series data. The paper emphasises the need of interpretable representations accounting for the correlated nature of temporal data. To this scope, the study proposes to cluster the data in a latent space estimated through an auto-encoder. The clustering is obtained by leveraging on the idea of self-organising maps (SOM). Within this setting, the data is mapped into a 2D lattice where each coordinate point represents the center of an inner cluster. \nThis construction motivates the formulation of the auto encoder through the definition of several cost terms promoting reconstruction, clustering, and consistency across latent mappings. \nThis definition of the problem allows an heuristic for circumventing the non-differentiability of the discrete mapping. The enhance consistency over time, the model is further equipped with an additional cost term enforcing transition smoothness across data points and latent embeddings. \n\nThe experiments are carried out with respect to synthetic 2D time-series, chaotic time-series from dynamical systems, and clinical data. In each case the proposed method shows promising results with respect to the proposed benchmark. \n\nThe study presents some interesting methodological and technical ideas. On the other hand the manuscript presentation is quite convoluted, at the expense of a lacks of clarity in the details about the implementation of the methodology. Moreover, motivated by practical aspects, the model optimisation relies on computational strategies not completely supported from the theoretical point of view (such as the zeroing of the gradient in backpropagation, or the approximation of the clustering function to overcome non-differentiability). The impact of these modeling choices would deserve more investigation and discussion. \n\nDetailed comments:\n\n- As also stated by the authors, the use of a 2D latent representation is completely arbitrary. It may be true that a 2D embedding provides a simple visualisation, however interpretability can be obtained also with much richer representations in a number of different ways (e.g. sparsity, parametric representations, \u2026). Therefore the feeling is that the proposed structure may be quite ad-hoc, and one may wonder whether the algorithm would still generalise to more complex latent representations.\n- Related to the previous comment, the number of latent points seems to be crucial to the performance of the method. However this aspect is not discussed in detail, while it would be beneficial to provide experiment about the sensitivity and accuracy with respect to the choice if this parameters.\n- The method relies on several cost terms plugged together. While each of them takes care of specific consistency aspects of the model, their mutual relation and balance may be very critical. This is governed by a series of trade-off parameters whose effect is not discussed  nor explored throughout the study. I guess that the optimisation stability may be also quite sensitive to this trade-off, and it would be important to provide more details about this aspect. \n- Surprisingly, k-means seems to perform quite well in spite of its simplicity. Also, there is no mention about initialisation and choice of the parameter \u201ck\u201d. The authors may want to better discuss the performance of this algorithm, especially compared to its much lower modeling complexity with respect to the proposed method. \n- Still related to the comparison with respect to the state-of-art, interpretability in time series analysis can be achieved with much lesser assumptions and parameters by using standard approaches such as independent component analysis. I would expect this sort of comparison, especially in case of long-term data such as the one provided in the Lorenz system. \n- Clustering of short-term time series, such as the clinical ones, is a challenging task. The feeling is that a highly parametrised model, such as the proposed one,  may still not be superior with respect to classical methods, such as the mixture of linear regressions. This sort of comparison would be quite informative to appreciate the real value of the proposed methodology.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper562/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SOM-VAE: Interpretable Discrete Representation Learning on Time Series", "abstract": "High-dimensional time series are common in many domains. Since human cognition is not optimized to work well in high-dimensional spaces, these areas could benefit from interpretable low-dimensional representations. However, most representation learning algorithms for time series data are difficult to interpret. This is due to non-intuitive mappings from data features to salient properties of the representation and non-smoothness over time.\nTo address this problem, we propose a new representation learning framework building on ideas from interpretable discrete dimensionality reduction and deep generative modeling. This framework allows us to learn discrete representations of time series, which give rise to smooth and interpretable embeddings with superior clustering performance. We introduce a new way to overcome the non-differentiability in discrete representation learning and present a gradient-based version of the traditional self-organizing map algorithm that is more performant than the original. Furthermore, to allow for a probabilistic interpretation of our method, we integrate a Markov model in the representation space.\nThis model uncovers the temporal transition structure, improves clustering performance even further and provides additional explanatory insights as well as a natural representation of uncertainty.\nWe evaluate our model in terms of clustering performance and interpretability on static (Fashion-)MNIST data, a time series of linearly interpolated (Fashion-)MNIST images, a chaotic Lorenz attractor system with two macro states, as well as on a challenging real world medical time series application on the eICU data set. Our learned representations compare favorably with competitor methods and facilitate downstream tasks on the real world data.", "keywords": ["deep learning", "self-organizing map", "variational autoencoder", "representation learning", "time series", "machine learning", "interpretability"], "authorids": ["fortuin@inf.ethz.ch", "mhueser@inf.ethz.ch", "locatelf@inf.ethz.ch", "heiko.strathmann@gmail.com", "raetsch@inf.ethz.ch"], "authors": ["Vincent Fortuin", "Matthias H\u00fcser", "Francesco Locatello", "Heiko Strathmann", "Gunnar R\u00e4tsch"], "TL;DR": "We present a method to learn interpretable representations on time series using ideas from variational autoencoders, self-organizing maps and probabilistic models.", "pdf": "/pdf/15c8cbb113daa27eb888054e58b057bcfeb1a203.pdf", "paperhash": "fortuin|somvae_interpretable_discrete_representation_learning_on_time_series", "_bibtex": "@inproceedings{\nfortuin2018deep,\ntitle={Deep Self-Organization: Interpretable Discrete Representation Learning on Time Series},\nauthor={Vincent Fortuin and Matthias H\u00fcser and Francesco Locatello and Heiko Strathmann and Gunnar R\u00e4tsch},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rygjcsR9Y7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper562/Official_Review", "cdate": 1542234433070, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "rygjcsR9Y7", "replyto": "rygjcsR9Y7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper562/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335752523, "tmdate": 1552335752523, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper562/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 8}