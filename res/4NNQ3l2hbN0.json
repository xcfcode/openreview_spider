{"notes": [{"id": "4NNQ3l2hbN0", "original": "YKbpAr3h3aG", "number": 2270, "cdate": 1601308250172, "ddate": null, "tcdate": 1601308250172, "tmdate": 1614985717805, "tddate": null, "forum": "4NNQ3l2hbN0", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Search Data Structure Learning", "authorids": ["~Mathieu_Duchesneau1", "~Hansenclever_Bassani1", "tappa@iro.umontreal.ca"], "authors": ["Mathieu Duchesneau", "Hansenclever Bassani", "Alain Tapp"], "keywords": ["Machine Learning", "Search Data Structure", "Information Retrieval", "Binary Embeddings"], "abstract": "In our modern world, an enormous amount of data surrounds us, and we are rarely interested in more than a handful of data points at once. It is like searching for needles in a haystack, and in many cases, there is no better algorithm than a random search, which might not be viable. Previously proposed algorithms for efficient database access are made for particular applications such as finding the min/max, finding all points within a range or finding the k-nearest neighbours. Consequently, there is a lack of versatility concerning what we can search when it comes to a gigantic database. In this work, we propose Search Data Structure Learning (SDSL), a generalization of the standard Search Data Structure (SDS) in which the machine has to learn how to search in the database. To evaluate approaches in this field, we propose a novel metric called Sequential Search Work Ratio (SSWR), a natural way of measuring a search's efficiency and quality. Finally, we inaugurate the field with the Efficient Learnable Binary Access (ELBA), a family of models for Search Data Structure Learning. It requires a means to train two parametric functions and a search data structure for binary codes. For the training, we developed a novel loss function, the F-beta Loss. For the SDS, we describe the Multi-Bernoulli Search (MBS), a novel approach for probabilistic binary codes. Finally, we exhibit the F-beta Loss and the MBS synergy by experimentally showing that it is at least twice as better than using the alternative loss functions of MIHash and HashNet and twenty times better than with another SDS based on the Hamming radius.", "one-sentence_summary": "We describe a new field of Machine Learning call Search Data Structure Learning for which we develop a novel model that outperforms related approaches. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "duchesneau|search_data_structure_learning", "pdf": "/pdf/cae817858469c3c965a641f5e4c22ae8e1872666.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=05EYWpjGPx", "_bibtex": "@misc{\nduchesneau2021search,\ntitle={Search Data Structure Learning},\nauthor={Mathieu Duchesneau and Hansenclever Bassani and Alain Tapp},\nyear={2021},\nurl={https://openreview.net/forum?id=4NNQ3l2hbN0}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "xtRKCUKF3kT", "original": null, "number": 1, "cdate": 1610040424394, "ddate": null, "tcdate": 1610040424394, "tmdate": 1610474023518, "tddate": null, "forum": "4NNQ3l2hbN0", "replyto": "4NNQ3l2hbN0", "invitation": "ICLR.cc/2021/Conference/Paper2270/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "This paper addresses an interesting problem and all reviewers agree.  Most reviewers found the paper difficult to understand and it was hard to see the novel contributions.    The paper will need a significant revision before publication."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Search Data Structure Learning", "authorids": ["~Mathieu_Duchesneau1", "~Hansenclever_Bassani1", "tappa@iro.umontreal.ca"], "authors": ["Mathieu Duchesneau", "Hansenclever Bassani", "Alain Tapp"], "keywords": ["Machine Learning", "Search Data Structure", "Information Retrieval", "Binary Embeddings"], "abstract": "In our modern world, an enormous amount of data surrounds us, and we are rarely interested in more than a handful of data points at once. It is like searching for needles in a haystack, and in many cases, there is no better algorithm than a random search, which might not be viable. Previously proposed algorithms for efficient database access are made for particular applications such as finding the min/max, finding all points within a range or finding the k-nearest neighbours. Consequently, there is a lack of versatility concerning what we can search when it comes to a gigantic database. In this work, we propose Search Data Structure Learning (SDSL), a generalization of the standard Search Data Structure (SDS) in which the machine has to learn how to search in the database. To evaluate approaches in this field, we propose a novel metric called Sequential Search Work Ratio (SSWR), a natural way of measuring a search's efficiency and quality. Finally, we inaugurate the field with the Efficient Learnable Binary Access (ELBA), a family of models for Search Data Structure Learning. It requires a means to train two parametric functions and a search data structure for binary codes. For the training, we developed a novel loss function, the F-beta Loss. For the SDS, we describe the Multi-Bernoulli Search (MBS), a novel approach for probabilistic binary codes. Finally, we exhibit the F-beta Loss and the MBS synergy by experimentally showing that it is at least twice as better than using the alternative loss functions of MIHash and HashNet and twenty times better than with another SDS based on the Hamming radius.", "one-sentence_summary": "We describe a new field of Machine Learning call Search Data Structure Learning for which we develop a novel model that outperforms related approaches. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "duchesneau|search_data_structure_learning", "pdf": "/pdf/cae817858469c3c965a641f5e4c22ae8e1872666.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=05EYWpjGPx", "_bibtex": "@misc{\nduchesneau2021search,\ntitle={Search Data Structure Learning},\nauthor={Mathieu Duchesneau and Hansenclever Bassani and Alain Tapp},\nyear={2021},\nurl={https://openreview.net/forum?id=4NNQ3l2hbN0}\n}"}, "tags": [], "invitation": {"reply": {"forum": "4NNQ3l2hbN0", "replyto": "4NNQ3l2hbN0", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040424378, "tmdate": 1610474023502, "id": "ICLR.cc/2021/Conference/Paper2270/-/Decision"}}}, {"id": "_NaBhYj9qy", "original": null, "number": 2, "cdate": 1603888559679, "ddate": null, "tcdate": 1603888559679, "tmdate": 1606966804969, "tddate": null, "forum": "4NNQ3l2hbN0", "replyto": "4NNQ3l2hbN0", "invitation": "ICLR.cc/2021/Conference/Paper2270/-/Official_Review", "content": {"title": "This paper proposes a general search framework called Search Data Structure Learning. A learning based method is designed to learn how to search in the database. In order to train the whole process, a novel loss function is designed and differentiable parameterized networks are adopted. The authors also design a novel metric to evaluate the performance. Experiments on NoisyMnist are conducted to demonstrate the effectiveness of the proposed method.", "review": "The paper is generally well written, and the authors target on an important and challenging problem of learning how to search. Search is a complex problem, and people designed various data structures to handle different kinds of search problems. The authors generalize these searching data structures by search data structure learning. The idea is novel and very interesting.\n\nThen, the authors design SSWR metric evaluate the efficiency and quality of the search process. And the authors design ELBA(Efficient Learning Binary Access) by proposing a novel loss function and neural networks to create a discrete binary code(s) for both the queries and documents. And the authors conduct several experiments on NoisyMnist dataset to compare with several methods to show the advantage of the proposed framework.\n\nMNIST dataset is a simple and small dataset, so more challenging datasets (such as SIFT-1M and GIST-1M) used in the search community are suggested to make the paper more convincing.\n\n=======================\n\nAfter reading all the review comments and rebuttals, I would like to change my score to 4. \nThe paper is interesting, but more detailed analysis and experiments are needed to make the work more clear and convincing.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2270/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2270/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Search Data Structure Learning", "authorids": ["~Mathieu_Duchesneau1", "~Hansenclever_Bassani1", "tappa@iro.umontreal.ca"], "authors": ["Mathieu Duchesneau", "Hansenclever Bassani", "Alain Tapp"], "keywords": ["Machine Learning", "Search Data Structure", "Information Retrieval", "Binary Embeddings"], "abstract": "In our modern world, an enormous amount of data surrounds us, and we are rarely interested in more than a handful of data points at once. It is like searching for needles in a haystack, and in many cases, there is no better algorithm than a random search, which might not be viable. Previously proposed algorithms for efficient database access are made for particular applications such as finding the min/max, finding all points within a range or finding the k-nearest neighbours. Consequently, there is a lack of versatility concerning what we can search when it comes to a gigantic database. In this work, we propose Search Data Structure Learning (SDSL), a generalization of the standard Search Data Structure (SDS) in which the machine has to learn how to search in the database. To evaluate approaches in this field, we propose a novel metric called Sequential Search Work Ratio (SSWR), a natural way of measuring a search's efficiency and quality. Finally, we inaugurate the field with the Efficient Learnable Binary Access (ELBA), a family of models for Search Data Structure Learning. It requires a means to train two parametric functions and a search data structure for binary codes. For the training, we developed a novel loss function, the F-beta Loss. For the SDS, we describe the Multi-Bernoulli Search (MBS), a novel approach for probabilistic binary codes. Finally, we exhibit the F-beta Loss and the MBS synergy by experimentally showing that it is at least twice as better than using the alternative loss functions of MIHash and HashNet and twenty times better than with another SDS based on the Hamming radius.", "one-sentence_summary": "We describe a new field of Machine Learning call Search Data Structure Learning for which we develop a novel model that outperforms related approaches. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "duchesneau|search_data_structure_learning", "pdf": "/pdf/cae817858469c3c965a641f5e4c22ae8e1872666.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=05EYWpjGPx", "_bibtex": "@misc{\nduchesneau2021search,\ntitle={Search Data Structure Learning},\nauthor={Mathieu Duchesneau and Hansenclever Bassani and Alain Tapp},\nyear={2021},\nurl={https://openreview.net/forum?id=4NNQ3l2hbN0}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "4NNQ3l2hbN0", "replyto": "4NNQ3l2hbN0", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2270/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538100146, "tmdate": 1606915779319, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2270/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2270/-/Official_Review"}}}, {"id": "sdI5n-45XLQ", "original": null, "number": 4, "cdate": 1604002056372, "ddate": null, "tcdate": 1604002056372, "tmdate": 1606791317616, "tddate": null, "forum": "4NNQ3l2hbN0", "replyto": "4NNQ3l2hbN0", "invitation": "ICLR.cc/2021/Conference/Paper2270/-/Official_Review", "content": {"title": "The paper needs to justify the new metric.", "review": "In this paper, the authors proposed Search Data Structure Learning (SDSL), which they claim to be a generalization of the standard Search Data Structure. They also present a new metric called Sequential Search Work Ratio (SSWR) to evaluate the quality and efficiency of the search. They introduced a new loss called F-beta Loss, showing their algorithm is better than two previous results, MIHash (Cakir et al. 2017) and HashNet (Cao et al. 2017).\n\nI appreciate the key message the paper is trying to convey: we need the formal definition or mutual agreement on the problem as well as the correct evaluation metrics to push forward a research area. However, I have several major concerns about the contribution of this paper. \n\n1. I do not see a formal definition of SDSL. Definition 3.1 is just a definition of matching and non-matching relations. \n2. What is new in the metric defined in Definition 3.4? The denominator is constant for all search methods. C(.,.,.) is the cost of re-ranking, and w0 is the cost of searching (filtering the candidate). \n3. There is no theoretical or empirical comparison/evaluation of the proposed metric. The calculations in Appendix A are very standard calculations. It is unclear about the innovation of this metric from a theory perspective. In experiments, the authors directly use SSWR, and there is no justification on why this is the right metric.\n\nBefore proposing the new loss, etc., I suggest the authors could go back and justify the metric's effectiveness. Otherwise, it is hard to conclude if this paper has made any progress.\n\n=========================================\n\nThanks for the rebuttal and revision. My first concern has been addressed. However, I still found the proposal lack empirical or theoretical proof, so I am not convinced the contribution is principle enough. I decide to keep my original score.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2270/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2270/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Search Data Structure Learning", "authorids": ["~Mathieu_Duchesneau1", "~Hansenclever_Bassani1", "tappa@iro.umontreal.ca"], "authors": ["Mathieu Duchesneau", "Hansenclever Bassani", "Alain Tapp"], "keywords": ["Machine Learning", "Search Data Structure", "Information Retrieval", "Binary Embeddings"], "abstract": "In our modern world, an enormous amount of data surrounds us, and we are rarely interested in more than a handful of data points at once. It is like searching for needles in a haystack, and in many cases, there is no better algorithm than a random search, which might not be viable. Previously proposed algorithms for efficient database access are made for particular applications such as finding the min/max, finding all points within a range or finding the k-nearest neighbours. Consequently, there is a lack of versatility concerning what we can search when it comes to a gigantic database. In this work, we propose Search Data Structure Learning (SDSL), a generalization of the standard Search Data Structure (SDS) in which the machine has to learn how to search in the database. To evaluate approaches in this field, we propose a novel metric called Sequential Search Work Ratio (SSWR), a natural way of measuring a search's efficiency and quality. Finally, we inaugurate the field with the Efficient Learnable Binary Access (ELBA), a family of models for Search Data Structure Learning. It requires a means to train two parametric functions and a search data structure for binary codes. For the training, we developed a novel loss function, the F-beta Loss. For the SDS, we describe the Multi-Bernoulli Search (MBS), a novel approach for probabilistic binary codes. Finally, we exhibit the F-beta Loss and the MBS synergy by experimentally showing that it is at least twice as better than using the alternative loss functions of MIHash and HashNet and twenty times better than with another SDS based on the Hamming radius.", "one-sentence_summary": "We describe a new field of Machine Learning call Search Data Structure Learning for which we develop a novel model that outperforms related approaches. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "duchesneau|search_data_structure_learning", "pdf": "/pdf/cae817858469c3c965a641f5e4c22ae8e1872666.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=05EYWpjGPx", "_bibtex": "@misc{\nduchesneau2021search,\ntitle={Search Data Structure Learning},\nauthor={Mathieu Duchesneau and Hansenclever Bassani and Alain Tapp},\nyear={2021},\nurl={https://openreview.net/forum?id=4NNQ3l2hbN0}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "4NNQ3l2hbN0", "replyto": "4NNQ3l2hbN0", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2270/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538100146, "tmdate": 1606915779319, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2270/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2270/-/Official_Review"}}}, {"id": "1KJvIAicNpw", "original": null, "number": 5, "cdate": 1605817549596, "ddate": null, "tcdate": 1605817549596, "tmdate": 1605977273020, "tddate": null, "forum": "4NNQ3l2hbN0", "replyto": "cRftJDEmlT", "invitation": "ICLR.cc/2021/Conference/Paper2270/-/Official_Comment", "content": {"title": "Re: A new evaluation metric and a new loss for search via binary codes", "comment": "1) The SSWR (and SWR) is not designed only for binary codes and hash-based SDS. While it is possible to compare a graph-based SDS with another graph-based SDS, comparing a graph-based SDS with a hash-based SDS is tricky. In practice comparing time is possibly the ideal measure of cost. However, it is not adequate for theoretical analysis since the results depend on the implementation and the hardware used. Consequently, it is impossible to compare hash-based SDS with tree/graph-based SDS without making dubious assumptions (e.g. x hashes = y nodes lookups) that would be hard to justify and probably cause prejudice to one type of SDS. \n2) The SSWR never evaluates the order the sets of candidates are generated using the relevance oracle cost because, indeed, the sets are not given randomly. The SSWR uses the relevance oracle cost on the generator's last set to quantify a random search cost in this set with the oracle.\n3) We propose to add the following discussion of other metrics before the introduction of the SSWR.\na) The mAP does not consider the work done to perform the ranking. An SDS could compare the query to every document in the database and have a good mAP. In SDSL, we want to monitor the quality as well as the efficiency of retrieval.\nb) In our opinion, the Recall Query per second (RQPS) is not suitable for theoretical analysis since it depends on the implementation and hardware used. However, it would be easy to generalize the RQPS by changing what quantifies the work (something other than the number of seconds). However, the most crucial contribution of SSWR w.r.t. the RQPS is that it allows the SDS to generate an arbitrary number of candidates. This is because the RQPS has a parameter (k) that limits the number of candidates it generates. This parameter prevents a model from generating all documents in the database as its candidates, which would give 100% recall without any computation. Ultimately, the parameter k is a fix to the flaw that the RQPS does not consider the precision. Producing the database is not viable to have a good SSWR, and the SSWR achieves this without limiting the number of candidates that can be generated.\nc) The SSWR is the speedup over brutal search when we answer the following questions. What happens when the searches are not exact? How to deal with candidates of varying sizes? How to take into account searches that do not find enough relevant documents?\nd) The SSWR is probably correlated with the RQPS, but it is possible to (adversarially) design models where they are not. For example, by randomly generating the first k candidates. \n4) We were not particularly interested in ANN as we aim to show that it is possible to work in a generalized framework. For cross-modal retrieval, while the SDSL framework supports it. Hashing is inadequate, in general. This is because hashing implies some generalized form of transitivity in the relation, i.e. q1 ~ d1, q2 ~ d1, q2 ~ d2 \u21d2 q1 ~ d2, which does not hold for most cross-modal retrieval tasks. Cross-modal retrieval is part of our field of interest but would probably require something else than a hashing based SDS.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2270/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2270/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Search Data Structure Learning", "authorids": ["~Mathieu_Duchesneau1", "~Hansenclever_Bassani1", "tappa@iro.umontreal.ca"], "authors": ["Mathieu Duchesneau", "Hansenclever Bassani", "Alain Tapp"], "keywords": ["Machine Learning", "Search Data Structure", "Information Retrieval", "Binary Embeddings"], "abstract": "In our modern world, an enormous amount of data surrounds us, and we are rarely interested in more than a handful of data points at once. It is like searching for needles in a haystack, and in many cases, there is no better algorithm than a random search, which might not be viable. Previously proposed algorithms for efficient database access are made for particular applications such as finding the min/max, finding all points within a range or finding the k-nearest neighbours. Consequently, there is a lack of versatility concerning what we can search when it comes to a gigantic database. In this work, we propose Search Data Structure Learning (SDSL), a generalization of the standard Search Data Structure (SDS) in which the machine has to learn how to search in the database. To evaluate approaches in this field, we propose a novel metric called Sequential Search Work Ratio (SSWR), a natural way of measuring a search's efficiency and quality. Finally, we inaugurate the field with the Efficient Learnable Binary Access (ELBA), a family of models for Search Data Structure Learning. It requires a means to train two parametric functions and a search data structure for binary codes. For the training, we developed a novel loss function, the F-beta Loss. For the SDS, we describe the Multi-Bernoulli Search (MBS), a novel approach for probabilistic binary codes. Finally, we exhibit the F-beta Loss and the MBS synergy by experimentally showing that it is at least twice as better than using the alternative loss functions of MIHash and HashNet and twenty times better than with another SDS based on the Hamming radius.", "one-sentence_summary": "We describe a new field of Machine Learning call Search Data Structure Learning for which we develop a novel model that outperforms related approaches. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "duchesneau|search_data_structure_learning", "pdf": "/pdf/cae817858469c3c965a641f5e4c22ae8e1872666.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=05EYWpjGPx", "_bibtex": "@misc{\nduchesneau2021search,\ntitle={Search Data Structure Learning},\nauthor={Mathieu Duchesneau and Hansenclever Bassani and Alain Tapp},\nyear={2021},\nurl={https://openreview.net/forum?id=4NNQ3l2hbN0}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "4NNQ3l2hbN0", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2270/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2270/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2270/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2270/Authors|ICLR.cc/2021/Conference/Paper2270/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2270/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923850333, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2270/-/Official_Comment"}}}, {"id": "7HBe9LdJcpR", "original": null, "number": 4, "cdate": 1605817385625, "ddate": null, "tcdate": 1605817385625, "tmdate": 1605817385625, "tddate": null, "forum": "4NNQ3l2hbN0", "replyto": "sdI5n-45XLQ", "invitation": "ICLR.cc/2021/Conference/Paper2270/-/Official_Comment", "content": {"title": "Re: The paper needs to justify the new metric.", "comment": "1) Definition 3.1 does not define the SDSL framework. It formalizes the dichotomy between relative and absolute relations. The definition of SDSL is the minimization at the end of section 3. To clarify, we propose to add a formal definition above the last paragraph of section 3, which will be modified to support the definition.\n2)\na) The denominator is constant since it is the cost of using the oracle without the SDS. For comparing models, it is possible only to use the numerator. However, it is tough to interpret the SDS quality only by looking at the numerator. The SSWR is a ratio with a straightforward interpretation. E.g. a SSWR of 0.5 indicates that the SDS is twice as performant as the oracle alone. Furthermore, an SSWR below 1 indicates that the SDS is useful.\nb) The relevance oracle cost is not the cost of re-ranking. It considers how many relevant documents are present in a set and how many are required and compute the expected number of calls to the oracle needed to find enough relevant documents by doing a random search. We can interpret the |H| term in the SSWR as a re-ranking cost of the previous candidates' sets.\n3) The SSWR compares the scenario where we use an oracle and an SDS with the scenario where we use an oracle only in a way that considers both the retrieval's quality and cost simultaneously.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2270/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2270/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Search Data Structure Learning", "authorids": ["~Mathieu_Duchesneau1", "~Hansenclever_Bassani1", "tappa@iro.umontreal.ca"], "authors": ["Mathieu Duchesneau", "Hansenclever Bassani", "Alain Tapp"], "keywords": ["Machine Learning", "Search Data Structure", "Information Retrieval", "Binary Embeddings"], "abstract": "In our modern world, an enormous amount of data surrounds us, and we are rarely interested in more than a handful of data points at once. It is like searching for needles in a haystack, and in many cases, there is no better algorithm than a random search, which might not be viable. Previously proposed algorithms for efficient database access are made for particular applications such as finding the min/max, finding all points within a range or finding the k-nearest neighbours. Consequently, there is a lack of versatility concerning what we can search when it comes to a gigantic database. In this work, we propose Search Data Structure Learning (SDSL), a generalization of the standard Search Data Structure (SDS) in which the machine has to learn how to search in the database. To evaluate approaches in this field, we propose a novel metric called Sequential Search Work Ratio (SSWR), a natural way of measuring a search's efficiency and quality. Finally, we inaugurate the field with the Efficient Learnable Binary Access (ELBA), a family of models for Search Data Structure Learning. It requires a means to train two parametric functions and a search data structure for binary codes. For the training, we developed a novel loss function, the F-beta Loss. For the SDS, we describe the Multi-Bernoulli Search (MBS), a novel approach for probabilistic binary codes. Finally, we exhibit the F-beta Loss and the MBS synergy by experimentally showing that it is at least twice as better than using the alternative loss functions of MIHash and HashNet and twenty times better than with another SDS based on the Hamming radius.", "one-sentence_summary": "We describe a new field of Machine Learning call Search Data Structure Learning for which we develop a novel model that outperforms related approaches. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "duchesneau|search_data_structure_learning", "pdf": "/pdf/cae817858469c3c965a641f5e4c22ae8e1872666.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=05EYWpjGPx", "_bibtex": "@misc{\nduchesneau2021search,\ntitle={Search Data Structure Learning},\nauthor={Mathieu Duchesneau and Hansenclever Bassani and Alain Tapp},\nyear={2021},\nurl={https://openreview.net/forum?id=4NNQ3l2hbN0}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "4NNQ3l2hbN0", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2270/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2270/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2270/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2270/Authors|ICLR.cc/2021/Conference/Paper2270/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2270/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923850333, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2270/-/Official_Comment"}}}, {"id": "7csm5drILL", "original": null, "number": 3, "cdate": 1605817125334, "ddate": null, "tcdate": 1605817125334, "tmdate": 1605817125334, "tddate": null, "forum": "4NNQ3l2hbN0", "replyto": "CrhwJMm0wJy", "invitation": "ICLR.cc/2021/Conference/Paper2270/-/Official_Comment", "content": {"title": "Re: Interesting line of research; lacks sufficient clarity and positioning with respect to relevant existing literature (part 2)", "comment": "To further clarify our framework:\n1) We propose to add: \"This is to allow the general case where the relation is relative\" after describing $R$ as a set of $R_D$. \n2) In this work, we provide an algorithm that is limited to the absolute case. However, the formalism that we introduced can describe a more general case that was never tackled before from the efficiency perspective. \n3) We propose to:\na) replace the first sentence of the paragraph preceding definition 3.5 with \"It is not rare that an SDS can be slightly modified to produce a sequence of sets candidates.\".\nb) add \"The reason why the SSWR uses the relevance oracle cost over the last sets of candidates only is because the generator did not found enough relevant documents before generating the last sets of candidates. Consequently, an exhaustive search with the oracle in the previous candidates sets was required before asking the generator to yield more candidates. The SSWR account for this exhaustive search with the |H| term. Finally, the sets of candidates are intended to be mutually exclusive because this will reduce the relevance oracle cost computed over the last sets of candidates and give a better SSWR. However, it is not mandatory.\" after the definition of the SSWR.\n4) We propose to add: \"Where we minimize the expectation over all databases to ensure the quality of the generator even if the database changes. By letting the distribution over D to be deterministic, we fall into the case with a static database.\" after defining the objective.\n5) We propose to add: \"It is possible to generate candidates (gen(q)) with MBS by yielding candidates each time a search in a back-end structure is made for the equivalent search(q)\" in the paragraph explaining insertion and searching in MBS. We would also add generation with halting in the example given in the appendix.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2270/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2270/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Search Data Structure Learning", "authorids": ["~Mathieu_Duchesneau1", "~Hansenclever_Bassani1", "tappa@iro.umontreal.ca"], "authors": ["Mathieu Duchesneau", "Hansenclever Bassani", "Alain Tapp"], "keywords": ["Machine Learning", "Search Data Structure", "Information Retrieval", "Binary Embeddings"], "abstract": "In our modern world, an enormous amount of data surrounds us, and we are rarely interested in more than a handful of data points at once. It is like searching for needles in a haystack, and in many cases, there is no better algorithm than a random search, which might not be viable. Previously proposed algorithms for efficient database access are made for particular applications such as finding the min/max, finding all points within a range or finding the k-nearest neighbours. Consequently, there is a lack of versatility concerning what we can search when it comes to a gigantic database. In this work, we propose Search Data Structure Learning (SDSL), a generalization of the standard Search Data Structure (SDS) in which the machine has to learn how to search in the database. To evaluate approaches in this field, we propose a novel metric called Sequential Search Work Ratio (SSWR), a natural way of measuring a search's efficiency and quality. Finally, we inaugurate the field with the Efficient Learnable Binary Access (ELBA), a family of models for Search Data Structure Learning. It requires a means to train two parametric functions and a search data structure for binary codes. For the training, we developed a novel loss function, the F-beta Loss. For the SDS, we describe the Multi-Bernoulli Search (MBS), a novel approach for probabilistic binary codes. Finally, we exhibit the F-beta Loss and the MBS synergy by experimentally showing that it is at least twice as better than using the alternative loss functions of MIHash and HashNet and twenty times better than with another SDS based on the Hamming radius.", "one-sentence_summary": "We describe a new field of Machine Learning call Search Data Structure Learning for which we develop a novel model that outperforms related approaches. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "duchesneau|search_data_structure_learning", "pdf": "/pdf/cae817858469c3c965a641f5e4c22ae8e1872666.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=05EYWpjGPx", "_bibtex": "@misc{\nduchesneau2021search,\ntitle={Search Data Structure Learning},\nauthor={Mathieu Duchesneau and Hansenclever Bassani and Alain Tapp},\nyear={2021},\nurl={https://openreview.net/forum?id=4NNQ3l2hbN0}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "4NNQ3l2hbN0", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2270/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2270/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2270/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2270/Authors|ICLR.cc/2021/Conference/Paper2270/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2270/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923850333, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2270/-/Official_Comment"}}}, {"id": "CrhwJMm0wJy", "original": null, "number": 2, "cdate": 1605816805675, "ddate": null, "tcdate": 1605816805675, "tmdate": 1605816805675, "tddate": null, "forum": "4NNQ3l2hbN0", "replyto": "w2qzaLJ1EOD", "invitation": "ICLR.cc/2021/Conference/Paper2270/-/Official_Comment", "content": {"title": "Re: Interesting line of research; lacks sufficient clarity and positioning with respect to relevant existing literature", "comment": "Thanks a lot for the review. While reading it, we realized that we did not adequately explain and motivate SDSL's two fundamental aspects.\n* Dynamic databases\n* Relative relations\n\nFirst, in dynamic databases, we have to deal with insertion and deletion. The following extreme but tangible example of dynamic databases highly inspired us: a company wants to design a search engine trained once and ready for distribution to multiple clients without further training on each client's database. Hopefully, this showcases why we believe it is essential to have a framework that considers dynamic databases. \n\nSecondly, by relative relations, we mean the case where the relevance of a document w.r.t. a query depends on the other documents in the database. The most studied relative relations is probably KNN. It is relative since it is impossible to know if a document is in the k-nearest neighbour of a query without knowing the other documents. In contrast, radius search is an example of an absolute relation because it is possible to know if a document is relevant to a query only by looking at the query-document pair. However, we did not introduce relative relations only for KNN. Another rather exciting example is multiple supporting facts\n\"A harder task is to answer questions where two supporting statements have to be chained to answer the question [...] where to answer the question \"Where is the football?\" one has to combine information from two sentences \"John is in the playground\" and \"John picked up the football\". \"[1]\nbut for a vast amount of facts.\n\nWe propose to add, in the introduction, similar paragraphs as the ones above to motivate and clarify the two critical points of the SDSL framework.\n\nTo put in contrast with the Learning to Search [A] framework. It is possible to update the framework to deal with dynamic databases by taking an expectation over the databases in the retrieval quality Q(t) and computational cost C(t). However, it is not clear how to deal with relative relations because the selection function T(q, x) is a \"matching function\" that does not exist for relative tasks. Generalizing the selection function by allowing it to consider the whole database (i.e. with T(q, X)) does not work because T(q, X) could use the ranking function s(x, q) on every document and nothing would penalize such exhaustive strategies since the computational cost is the number of candidates. Nevertheless, this is not the main issue. As with the framework proposed in [B], the computational cost does not consider the retrieval cost but only the size of the candidates set (divided by the number of documents in the database for [B]). Those frameworks fail to quantify the work needed to retrieve the candidates. For example, [A] relies on timing in the evaluation of their model. The SSWR is a unique quantity that quantifies both the cost of retrieval and the candidates' quality simultaneously.\n\nIn the related work section, we propose to add a paragraph similar to the one above to position ourselves and clarify our framework.\n\nIn the second part of this article, we propose a hashing based SDS that works only for absolute relations. While this is not tackling the most general case, we proposed the first instance (to the best of our knowledge) of an efficient hashing structure called Hashing Multi-Bernoulli Search (HMBS).\n\n\"Specifically, the goal of learning to hash is to embed data into compact binary codes so that the hamming distance between two codes reflects their original similarity. In order to perform efficient hamming distance search using the embedded representation, an additional efficient algorithmic structure is still needed. (How to come up with such an efficient algorithm is an issue usually ignored by learning to hash algorithms.)\" [A]\n\nWe propose to add a concrete example with small values of n, M, and T in the appendix to solidify the description of the HMBS data structure. This is the kind of algorithm that is easier to explain with an example.\n\nSince we designed the proposed loss function for absolute relations, we only need to train with query-document pairs, i.e. we do not need the relevance to be relative to a database. This simplifies the minimization over query-database pairs to query-document pairs.\n\nThose modifications hopefully strengthen the article by making it more straightforward and better positioned within the current literature.\n\n[1] Weston, J., Bordes, A., Chopra, S., Rush, A. M., van Merri\u00ebnboer, B., Joulin, A., & Mikolov, T. (2015). Towards ai-complete question answering: A set of prerequisite toy tasks. arXiv preprint arXiv:1502.05698.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2270/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2270/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Search Data Structure Learning", "authorids": ["~Mathieu_Duchesneau1", "~Hansenclever_Bassani1", "tappa@iro.umontreal.ca"], "authors": ["Mathieu Duchesneau", "Hansenclever Bassani", "Alain Tapp"], "keywords": ["Machine Learning", "Search Data Structure", "Information Retrieval", "Binary Embeddings"], "abstract": "In our modern world, an enormous amount of data surrounds us, and we are rarely interested in more than a handful of data points at once. It is like searching for needles in a haystack, and in many cases, there is no better algorithm than a random search, which might not be viable. Previously proposed algorithms for efficient database access are made for particular applications such as finding the min/max, finding all points within a range or finding the k-nearest neighbours. Consequently, there is a lack of versatility concerning what we can search when it comes to a gigantic database. In this work, we propose Search Data Structure Learning (SDSL), a generalization of the standard Search Data Structure (SDS) in which the machine has to learn how to search in the database. To evaluate approaches in this field, we propose a novel metric called Sequential Search Work Ratio (SSWR), a natural way of measuring a search's efficiency and quality. Finally, we inaugurate the field with the Efficient Learnable Binary Access (ELBA), a family of models for Search Data Structure Learning. It requires a means to train two parametric functions and a search data structure for binary codes. For the training, we developed a novel loss function, the F-beta Loss. For the SDS, we describe the Multi-Bernoulli Search (MBS), a novel approach for probabilistic binary codes. Finally, we exhibit the F-beta Loss and the MBS synergy by experimentally showing that it is at least twice as better than using the alternative loss functions of MIHash and HashNet and twenty times better than with another SDS based on the Hamming radius.", "one-sentence_summary": "We describe a new field of Machine Learning call Search Data Structure Learning for which we develop a novel model that outperforms related approaches. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "duchesneau|search_data_structure_learning", "pdf": "/pdf/cae817858469c3c965a641f5e4c22ae8e1872666.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=05EYWpjGPx", "_bibtex": "@misc{\nduchesneau2021search,\ntitle={Search Data Structure Learning},\nauthor={Mathieu Duchesneau and Hansenclever Bassani and Alain Tapp},\nyear={2021},\nurl={https://openreview.net/forum?id=4NNQ3l2hbN0}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "4NNQ3l2hbN0", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2270/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2270/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2270/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2270/Authors|ICLR.cc/2021/Conference/Paper2270/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2270/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923850333, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2270/-/Official_Comment"}}}, {"id": "w2qzaLJ1EOD", "original": null, "number": 1, "cdate": 1603828743433, "ddate": null, "tcdate": 1603828743433, "tmdate": 1605024250063, "tddate": null, "forum": "4NNQ3l2hbN0", "replyto": "4NNQ3l2hbN0", "invitation": "ICLR.cc/2021/Conference/Paper2270/-/Official_Review", "content": {"title": "Interesting line of research; lacks sufficient clarity and positioning with respect to relevant existing literature", "review": "This paper proposes a new objective to learn representations that allow for efficient and accurate search with the Multi-Bernoulli Search data structure. The motivation for such a scheme is strong and the preliminary empirical results demonstrate the utility of using the proposed representation learning objective and the data structure. \n\n\nHowever, I am currently leaning towards a reject because of two main reasons: The first being the clarity of the presentation in the paper that makes it hard to identify (i) novel contributions, (ii) key algorithmic and technical details -- the main paper is supposed to be somewhat self-sufficient; with the current version, it was significantly hard for me to follow through the presentation even when repeatedly referring to the supplement. The second reason is the lack of positioning of the proposed scheme/objective/data structure to a long line of research on the use of machine learning (with novel metrics/objectives and data structures) for search, including the learning of space partitioning trees [A, B, C], locality sensitive hashes [E] and representations [D] -- it is quite possible that the proposed scheme is solving a different and/or more general problem but I believe it would be useful to connect this \"Search Data Structure Learning\" to the existing work on \"Learning to Search\".\n\n\nBeyond the aforementioned high level comments, please find the following specific comments/questions that should be addressed:\n\n- The set of relations $\\mathcal{R}$ and the per-database $\\mathcal{R}_D$ could use further motivation as to how they relate to search and/or relate to the task of learning search data structures. \n- It is not clear why the distinction between \"absolute\" and \"relative\" is necessary in the context of the problem being targeted in this paper.\n- The SSWR definition is unclear to me and can use more exposition. It is not clear that $\\delta_t, t = 1, \\ldots, T$ is a sequence of sets. Also, it is not explained why the oracle costs are only incurred for the final $\\delta_T$ and not all intermediate ones (unless $\\delta_{t-1} \\subseteq \\delta_t$) or to the complete $\\cup_{i=1}^T \\delta_t$.\n- At the end of section 3, after presentation of SSWR, it is not clear why we are minimizing for a search sequence generator $\\mathcal{G}$ that is aggregated over database-query pairs $(D, q)$ -- wouldn't we learn a data structure per database (as is done for data structures used for nearest-neighbor search)? Can this be clarified, and if it is deliberate, please motivate the need and advantage of learning across databases.\n- The algorithm description is very hard to follow -- there are M back-end data structures and also the learned representation is used to generate M probable back-ends to insert in and T probable back-end to search from. It is not clear how the search involves $T$ passes over the $M$ back-ends. It would be good to clarify in the main paper that the \"outcomes\" are keys for each backend and all $T$ keys are tried in all $M$ back-ends.\n- How are we leveraging multiple databases as shown in the previous loss function?\n- The halting mechanism requires better presentation and clarification. It makes intuitive sense. But the presentation of the training algorithm (at least in the main paper) seems insufficient to provide enough context about the specifics of the halting mechanism.\n\n\n\n[A] Li, Z., Ning, H., Cao, L., Zhang, T., Gong, Y., & Huang, T. S. (2011). Learning to search efficiently in high dimensions. In Advances in Neural Information Processing Systems (pp. 1710-1718).\n[B] Cayton, L., & Dasgupta, S. (2008). A learning framework for nearest neighbor search. In Advances in Neural Information Processing Systems (pp. 233-240).\n[C] Dong, Yihe, et al. \"Learning Space Partitions for Nearest Neighbor Search.\" ICLR 2020.\n[D] Sablayrolles, A., Douze, M., Schmid, C., & Jegou, H. (2018, September). Spreading vectors for similarity search. In International Conference on Learning Representations.\n[E] Wang, J., Liu, W., Kumar, S., & Chang, S. F. (2015). Learning to hash for indexing big data -- A survey. Proceedings of the IEEE, 104(1), 34-57.\n", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2270/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2270/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Search Data Structure Learning", "authorids": ["~Mathieu_Duchesneau1", "~Hansenclever_Bassani1", "tappa@iro.umontreal.ca"], "authors": ["Mathieu Duchesneau", "Hansenclever Bassani", "Alain Tapp"], "keywords": ["Machine Learning", "Search Data Structure", "Information Retrieval", "Binary Embeddings"], "abstract": "In our modern world, an enormous amount of data surrounds us, and we are rarely interested in more than a handful of data points at once. It is like searching for needles in a haystack, and in many cases, there is no better algorithm than a random search, which might not be viable. Previously proposed algorithms for efficient database access are made for particular applications such as finding the min/max, finding all points within a range or finding the k-nearest neighbours. Consequently, there is a lack of versatility concerning what we can search when it comes to a gigantic database. In this work, we propose Search Data Structure Learning (SDSL), a generalization of the standard Search Data Structure (SDS) in which the machine has to learn how to search in the database. To evaluate approaches in this field, we propose a novel metric called Sequential Search Work Ratio (SSWR), a natural way of measuring a search's efficiency and quality. Finally, we inaugurate the field with the Efficient Learnable Binary Access (ELBA), a family of models for Search Data Structure Learning. It requires a means to train two parametric functions and a search data structure for binary codes. For the training, we developed a novel loss function, the F-beta Loss. For the SDS, we describe the Multi-Bernoulli Search (MBS), a novel approach for probabilistic binary codes. Finally, we exhibit the F-beta Loss and the MBS synergy by experimentally showing that it is at least twice as better than using the alternative loss functions of MIHash and HashNet and twenty times better than with another SDS based on the Hamming radius.", "one-sentence_summary": "We describe a new field of Machine Learning call Search Data Structure Learning for which we develop a novel model that outperforms related approaches. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "duchesneau|search_data_structure_learning", "pdf": "/pdf/cae817858469c3c965a641f5e4c22ae8e1872666.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=05EYWpjGPx", "_bibtex": "@misc{\nduchesneau2021search,\ntitle={Search Data Structure Learning},\nauthor={Mathieu Duchesneau and Hansenclever Bassani and Alain Tapp},\nyear={2021},\nurl={https://openreview.net/forum?id=4NNQ3l2hbN0}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "4NNQ3l2hbN0", "replyto": "4NNQ3l2hbN0", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2270/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538100146, "tmdate": 1606915779319, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2270/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2270/-/Official_Review"}}}, {"id": "cRftJDEmlT", "original": null, "number": 3, "cdate": 1603932698502, "ddate": null, "tcdate": 1603932698502, "tmdate": 1605024249935, "tddate": null, "forum": "4NNQ3l2hbN0", "replyto": "4NNQ3l2hbN0", "invitation": "ICLR.cc/2021/Conference/Paper2270/-/Official_Review", "content": {"title": "A new evaluation metric and a new loss for search via binary codes", "review": "\nThis article presents a new evaluation metric that improves upon hamming distance of two. Meanwhile, a new F-beta loss is proposed with advantages over other learn to hash methods in this evaluation metric. \n\n\n\nStrength:\nA new formulation on both evaluation metric and a new loss function\n\n\nWeakness:\n1. Claim: In the introduction of the paper, different data structures such as graph-based Efanna, HNSW, and ONNG. However, in the following sections of the paper, the author does not discuss the possibility of SWR pr SSWR on these methods. Is SSWR only designed for binary codes based information retrieval?\n2. Definition of cost: In Appendix A, the cost is estimated by how many calls in expectation\nare needed to find those k elements if we sample from S uniformly without replacement. Is there any justification for why we assume the k elements by uniform sampling? In hashing based retrieval, the sampling is based on a distribution with PDE monotonic to the distance [1]. In this case, is the cost estimated correctly?\n3. Comparison with other evaluation metrics: Is there any discussion of SSWR with evaluation metrics other than p@2. For instance, mAP, queries per second(QPS), and speedups over brutal force search. Is higher QPS or higher speedups over brutal force equivalent to higher SSWR?\n4. Experiments: This paper only presents results in image search settings.  Is there more comparison of f-beta loss in different search settings? For instance, (1) ANN search with l2 or cosine similarity. (2) Maximum Inner Product Search (3) Cross-modal retrieval such as text-image search.\n\n[1]Mutual Information Estimation using LSH Sampling https://www.semanticscholar.org/paper/Mutual-Information-Estimation-using-LSH-Sampling-Spring-Shrivastava/7241d1e839fdb69f7f0cc70220ad055e8900946c\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2270/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2270/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Search Data Structure Learning", "authorids": ["~Mathieu_Duchesneau1", "~Hansenclever_Bassani1", "tappa@iro.umontreal.ca"], "authors": ["Mathieu Duchesneau", "Hansenclever Bassani", "Alain Tapp"], "keywords": ["Machine Learning", "Search Data Structure", "Information Retrieval", "Binary Embeddings"], "abstract": "In our modern world, an enormous amount of data surrounds us, and we are rarely interested in more than a handful of data points at once. It is like searching for needles in a haystack, and in many cases, there is no better algorithm than a random search, which might not be viable. Previously proposed algorithms for efficient database access are made for particular applications such as finding the min/max, finding all points within a range or finding the k-nearest neighbours. Consequently, there is a lack of versatility concerning what we can search when it comes to a gigantic database. In this work, we propose Search Data Structure Learning (SDSL), a generalization of the standard Search Data Structure (SDS) in which the machine has to learn how to search in the database. To evaluate approaches in this field, we propose a novel metric called Sequential Search Work Ratio (SSWR), a natural way of measuring a search's efficiency and quality. Finally, we inaugurate the field with the Efficient Learnable Binary Access (ELBA), a family of models for Search Data Structure Learning. It requires a means to train two parametric functions and a search data structure for binary codes. For the training, we developed a novel loss function, the F-beta Loss. For the SDS, we describe the Multi-Bernoulli Search (MBS), a novel approach for probabilistic binary codes. Finally, we exhibit the F-beta Loss and the MBS synergy by experimentally showing that it is at least twice as better than using the alternative loss functions of MIHash and HashNet and twenty times better than with another SDS based on the Hamming radius.", "one-sentence_summary": "We describe a new field of Machine Learning call Search Data Structure Learning for which we develop a novel model that outperforms related approaches. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "duchesneau|search_data_structure_learning", "pdf": "/pdf/cae817858469c3c965a641f5e4c22ae8e1872666.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=05EYWpjGPx", "_bibtex": "@misc{\nduchesneau2021search,\ntitle={Search Data Structure Learning},\nauthor={Mathieu Duchesneau and Hansenclever Bassani and Alain Tapp},\nyear={2021},\nurl={https://openreview.net/forum?id=4NNQ3l2hbN0}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "4NNQ3l2hbN0", "replyto": "4NNQ3l2hbN0", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2270/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538100146, "tmdate": 1606915779319, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2270/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2270/-/Official_Review"}}}], "count": 10}