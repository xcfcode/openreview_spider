{"notes": [{"replyto": null, "ddate": null, "tmdate": 1486675361517, "tcdate": 1478283051797, "number": 277, "id": "SyVVJ85lg", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "SyVVJ85lg", "signatures": ["~Hang_Qi1"], "readers": ["everyone"], "content": {"title": "Paleo: A Performance Model for Deep Neural Networks", "abstract": "Although various scalable deep learning software packages have been proposed, it remains unclear how to best leverage parallel and distributed computing infrastructure to accelerate their training and deployment. Moreover, the effectiveness of existing parallel and distributed systems varies widely based on the neural network architecture and dataset under consideration.  In order to efficiently explore the space of scalable deep learning systems and quickly diagnose their effectiveness for a given problem instance, we introduce an analytical performance model called Paleo. Our key observation is that a neural network architecture carries with it a declarative specification of the computational requirements associated with its training and evaluation. By extracting these requirements from a given architecture and mapping them to a specific point within the design space of software, hardware and communication strategies, Paleo can efficiently and accurately model the expected scalability and performance of a putative deep learning system.  We show that Paleo is robust to the choice of network architecture, hardware, software, communication schemes, and parallelization strategies. We further demonstrate its ability to accurately model various recently published scalability results for CNNs such as NiN, Inception and AlexNet.", "pdf": "/pdf/c30790984e960f282cd1534f65f5f26444ec9a19.pdf", "TL;DR": "Paleo: An analytical performance model for exploring the space of scalable deep learning systems and quickly diagnosing their effectiveness for a given problem instance.", "paperhash": "qi|paleo_a_performance_model_for_deep_neural_networks", "keywords": ["Deep learning"], "conflicts": ["cs.ucla.edu", "cs.berkeley.edu"], "authors": ["Hang Qi", "Evan R. Sparks", "Ameet Talwalkar"], "authorids": ["hangqi@cs.ucla.edu", "sparks@cs.berkeley.edu", "ameet@cs.ucla.edu"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 12, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396475520, "tcdate": 1486396475520, "number": 1, "id": "H1QShzU_x", "invitation": "ICLR.cc/2017/conference/-/paper277/acceptance", "forum": "SyVVJ85lg", "replyto": "SyVVJ85lg", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"title": "ICLR committee final decision", "comment": "The reviewers were consistent in their praise of the paper. They asked for newer architectures, e.g. ResNet, DenseNet. The authors released an update with a Caffe converter which provides access to a wide range of CNNs and residual networks (ResNet-50 and DenseNet examples are provided). This seems like an incredibly useful tool and very glad it is open source. Paper is a clear accept.", "decision": "Accept (Poster)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Paleo: A Performance Model for Deep Neural Networks", "abstract": "Although various scalable deep learning software packages have been proposed, it remains unclear how to best leverage parallel and distributed computing infrastructure to accelerate their training and deployment. Moreover, the effectiveness of existing parallel and distributed systems varies widely based on the neural network architecture and dataset under consideration.  In order to efficiently explore the space of scalable deep learning systems and quickly diagnose their effectiveness for a given problem instance, we introduce an analytical performance model called Paleo. Our key observation is that a neural network architecture carries with it a declarative specification of the computational requirements associated with its training and evaluation. By extracting these requirements from a given architecture and mapping them to a specific point within the design space of software, hardware and communication strategies, Paleo can efficiently and accurately model the expected scalability and performance of a putative deep learning system.  We show that Paleo is robust to the choice of network architecture, hardware, software, communication schemes, and parallelization strategies. We further demonstrate its ability to accurately model various recently published scalability results for CNNs such as NiN, Inception and AlexNet.", "pdf": "/pdf/c30790984e960f282cd1534f65f5f26444ec9a19.pdf", "TL;DR": "Paleo: An analytical performance model for exploring the space of scalable deep learning systems and quickly diagnosing their effectiveness for a given problem instance.", "paperhash": "qi|paleo_a_performance_model_for_deep_neural_networks", "keywords": ["Deep learning"], "conflicts": ["cs.ucla.edu", "cs.berkeley.edu"], "authors": ["Hang Qi", "Evan R. Sparks", "Ameet Talwalkar"], "authorids": ["hangqi@cs.ucla.edu", "sparks@cs.berkeley.edu", "ameet@cs.ucla.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396476036, "id": "ICLR.cc/2017/conference/-/paper277/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "SyVVJ85lg", "replyto": "SyVVJ85lg", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396476036}}}, {"tddate": null, "tmdate": 1485136483295, "tcdate": 1481719344729, "number": 2, "id": "S1Y403RQe", "invitation": "ICLR.cc/2017/conference/-/paper277/official/review", "forum": "SyVVJ85lg", "replyto": "SyVVJ85lg", "signatures": ["ICLR.cc/2017/conference/paper277/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper277/AnonReviewer2"], "content": {"title": "Final review: Sound paper but a very simple model, few experiments at start but more added.", "rating": "6: Marginally above acceptance threshold", "review": "In PALEO the authors propose a simple model of execution of deep neural networks. It turns out that even this simple model allows to quite accurately predict the computation time for image recognition networks both in single-machine and distributed settings.\n\nThe ability to predict network running time is very useful, and the paper shows that even a simple model does it reasonably, which is a strength. But the tests are only performed on a few networks of very similar type (AlexNet, Inception, NiN) and only in a few settings. Much broader experiments, including a variety of models (RNNs, fully connected, adversarial, etc.) in a variety of settings (different batch sizes, layer sizes, node placement on devices, etc.) would probably reveal weaknesses of the proposed very simplified model. This is why this reviewer considers this paper borderline -- it's a first step, but a very basic one and without sufficiently large experimental underpinning.\n\nMore experiments were added, so I'm updating my score.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Paleo: A Performance Model for Deep Neural Networks", "abstract": "Although various scalable deep learning software packages have been proposed, it remains unclear how to best leverage parallel and distributed computing infrastructure to accelerate their training and deployment. Moreover, the effectiveness of existing parallel and distributed systems varies widely based on the neural network architecture and dataset under consideration.  In order to efficiently explore the space of scalable deep learning systems and quickly diagnose their effectiveness for a given problem instance, we introduce an analytical performance model called Paleo. Our key observation is that a neural network architecture carries with it a declarative specification of the computational requirements associated with its training and evaluation. By extracting these requirements from a given architecture and mapping them to a specific point within the design space of software, hardware and communication strategies, Paleo can efficiently and accurately model the expected scalability and performance of a putative deep learning system.  We show that Paleo is robust to the choice of network architecture, hardware, software, communication schemes, and parallelization strategies. We further demonstrate its ability to accurately model various recently published scalability results for CNNs such as NiN, Inception and AlexNet.", "pdf": "/pdf/c30790984e960f282cd1534f65f5f26444ec9a19.pdf", "TL;DR": "Paleo: An analytical performance model for exploring the space of scalable deep learning systems and quickly diagnosing their effectiveness for a given problem instance.", "paperhash": "qi|paleo_a_performance_model_for_deep_neural_networks", "keywords": ["Deep learning"], "conflicts": ["cs.ucla.edu", "cs.berkeley.edu"], "authors": ["Hang Qi", "Evan R. Sparks", "Ameet Talwalkar"], "authorids": ["hangqi@cs.ucla.edu", "sparks@cs.berkeley.edu", "ameet@cs.ucla.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512641107, "id": "ICLR.cc/2017/conference/-/paper277/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper277/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper277/AnonReviewer3", "ICLR.cc/2017/conference/paper277/AnonReviewer2", "ICLR.cc/2017/conference/paper277/AnonReviewer1"], "reply": {"forum": "SyVVJ85lg", "replyto": "SyVVJ85lg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper277/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper277/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512641107}}}, {"tddate": null, "tmdate": 1484186968200, "tcdate": 1484186968200, "number": 5, "id": "BJgvHw4Ug", "invitation": "ICLR.cc/2017/conference/-/paper277/public/comment", "forum": "SyVVJ85lg", "replyto": "SyVVJ85lg", "signatures": ["~Hang_Qi1"], "readers": ["everyone"], "writers": ["~Hang_Qi1"], "content": {"title": "Rebuttal", "comment": "We thank all the reviewers for reading and commenting on the paper! \n\nAnonReviewer1: \u201cIt would be interesting to see some newer network architectures with skip connections such as ResNet, and DenseNet\u201d\n\nResponse: \nWe released a converter to port Caffe model specs to Paleo, and thus Paleo now supports a wide range of CNNs and residual networks.  Our GitHub repository provides several examples, including ResNet-50 and DenseNet via the Caffe converter. Details can be found in our open source repository (https://github.com/TalwalkarLab/paleo) and online UI (https://talwalkarlab.github.io/paleo/).\n\nAnonReviewer2: \n\u201c...tests are only performed on a few networks of very similar type...\u201d\n\u201cMuch broader experiments, including a variety of models (RNNs, fully connected, adversarial, etc.) in a variety of settings (different batch sizes, layer sizes, node placement on devices, etc.) would probably reveal weaknesses of the proposed very simplified model.\u201d\n\nResponse: \nWe agree with the reviewers and it is our goal to make Paleo support a wide variety of models. Based on the observation that neural networks are assembled with a set of commonly reused operators, we take a bottom-up approach to estimating scaling properties of neural networks by analyzing these operators. Since the vast majority of scalability efforts have thus far focused on convolutional neural networks, we initially focused on CNNs. However, we are actively working on generalizing our work and have already extended Paleo to support GAN models, e.g., see our additional experimental result for a GAN model in Section 4.3.2 of the revised version of our paper (1/11/2017). Paleo can naturally extend to other types of models (e.g. RNNs) following the same modeling principles, and we will continue working on such extensions in further releases of Paleo. \n\nIn terms of the variety of settings, although the NiN, AlexNet, Inception are all CNNs, they includes very different layer sizes and configurations. In the case studies we presented in the paper we already consider a variety of batch sizes. In our third case study with a hybrid model, we evaluate model parallelism and node placement on up to eight devices. Remarkably, our simple model can make accurate predictions for all of these experimental setups, and we view the simplicity (and associated transparency / interpretability) of our model to be a key virtue of Paleo. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Paleo: A Performance Model for Deep Neural Networks", "abstract": "Although various scalable deep learning software packages have been proposed, it remains unclear how to best leverage parallel and distributed computing infrastructure to accelerate their training and deployment. Moreover, the effectiveness of existing parallel and distributed systems varies widely based on the neural network architecture and dataset under consideration.  In order to efficiently explore the space of scalable deep learning systems and quickly diagnose their effectiveness for a given problem instance, we introduce an analytical performance model called Paleo. Our key observation is that a neural network architecture carries with it a declarative specification of the computational requirements associated with its training and evaluation. By extracting these requirements from a given architecture and mapping them to a specific point within the design space of software, hardware and communication strategies, Paleo can efficiently and accurately model the expected scalability and performance of a putative deep learning system.  We show that Paleo is robust to the choice of network architecture, hardware, software, communication schemes, and parallelization strategies. We further demonstrate its ability to accurately model various recently published scalability results for CNNs such as NiN, Inception and AlexNet.", "pdf": "/pdf/c30790984e960f282cd1534f65f5f26444ec9a19.pdf", "TL;DR": "Paleo: An analytical performance model for exploring the space of scalable deep learning systems and quickly diagnosing their effectiveness for a given problem instance.", "paperhash": "qi|paleo_a_performance_model_for_deep_neural_networks", "keywords": ["Deep learning"], "conflicts": ["cs.ucla.edu", "cs.berkeley.edu"], "authors": ["Hang Qi", "Evan R. Sparks", "Ameet Talwalkar"], "authorids": ["hangqi@cs.ucla.edu", "sparks@cs.berkeley.edu", "ameet@cs.ucla.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287644775, "id": "ICLR.cc/2017/conference/-/paper277/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SyVVJ85lg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper277/reviewers", "ICLR.cc/2017/conference/paper277/areachairs"], "cdate": 1485287644775}}}, {"tddate": null, "tmdate": 1481871177760, "tcdate": 1481871177760, "number": 3, "id": "H1GUJz-Ne", "invitation": "ICLR.cc/2017/conference/-/paper277/official/review", "forum": "SyVVJ85lg", "replyto": "SyVVJ85lg", "signatures": ["ICLR.cc/2017/conference/paper277/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper277/AnonReviewer1"], "content": {"title": "", "rating": "7: Good paper, accept", "review": "This paper introduces an analytical performance model to estimate the training and evaluation time of a given network for different software, hardware and communication strategies. \nThe paper is very clear.  The authors included many freedoms in the variables while calculating the run-time of a network such as the number of workers, bandwidth, platform, and parallelization strategy. Their results are consistent with the reported results from literature.\nFurthermore, their code is open-source and the live demo is looking good. \nThe authors mentioned in their comment that they will allow users to upload customized networks and model splits in the coming releases of the interface, then the tool can become very useful.\nIt would be interesting to see some newer network architectures with skip connections such as ResNet, and DenseNet.\n\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Paleo: A Performance Model for Deep Neural Networks", "abstract": "Although various scalable deep learning software packages have been proposed, it remains unclear how to best leverage parallel and distributed computing infrastructure to accelerate their training and deployment. Moreover, the effectiveness of existing parallel and distributed systems varies widely based on the neural network architecture and dataset under consideration.  In order to efficiently explore the space of scalable deep learning systems and quickly diagnose their effectiveness for a given problem instance, we introduce an analytical performance model called Paleo. Our key observation is that a neural network architecture carries with it a declarative specification of the computational requirements associated with its training and evaluation. By extracting these requirements from a given architecture and mapping them to a specific point within the design space of software, hardware and communication strategies, Paleo can efficiently and accurately model the expected scalability and performance of a putative deep learning system.  We show that Paleo is robust to the choice of network architecture, hardware, software, communication schemes, and parallelization strategies. We further demonstrate its ability to accurately model various recently published scalability results for CNNs such as NiN, Inception and AlexNet.", "pdf": "/pdf/c30790984e960f282cd1534f65f5f26444ec9a19.pdf", "TL;DR": "Paleo: An analytical performance model for exploring the space of scalable deep learning systems and quickly diagnosing their effectiveness for a given problem instance.", "paperhash": "qi|paleo_a_performance_model_for_deep_neural_networks", "keywords": ["Deep learning"], "conflicts": ["cs.ucla.edu", "cs.berkeley.edu"], "authors": ["Hang Qi", "Evan R. Sparks", "Ameet Talwalkar"], "authorids": ["hangqi@cs.ucla.edu", "sparks@cs.berkeley.edu", "ameet@cs.ucla.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512641107, "id": "ICLR.cc/2017/conference/-/paper277/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper277/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper277/AnonReviewer3", "ICLR.cc/2017/conference/paper277/AnonReviewer2", "ICLR.cc/2017/conference/paper277/AnonReviewer1"], "reply": {"forum": "SyVVJ85lg", "replyto": "SyVVJ85lg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper277/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper277/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512641107}}}, {"tddate": null, "tmdate": 1481719481048, "tcdate": 1481719481042, "number": 3, "id": "Hy-p02CQe", "invitation": "ICLR.cc/2017/conference/-/paper277/pre-review/question", "forum": "SyVVJ85lg", "replyto": "SyVVJ85lg", "signatures": ["ICLR.cc/2017/conference/paper277/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper277/AnonReviewer2"], "content": {"title": "No question", "question": "I found the paper clear enough, had no questions, but want this task to go away."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Paleo: A Performance Model for Deep Neural Networks", "abstract": "Although various scalable deep learning software packages have been proposed, it remains unclear how to best leverage parallel and distributed computing infrastructure to accelerate their training and deployment. Moreover, the effectiveness of existing parallel and distributed systems varies widely based on the neural network architecture and dataset under consideration.  In order to efficiently explore the space of scalable deep learning systems and quickly diagnose their effectiveness for a given problem instance, we introduce an analytical performance model called Paleo. Our key observation is that a neural network architecture carries with it a declarative specification of the computational requirements associated with its training and evaluation. By extracting these requirements from a given architecture and mapping them to a specific point within the design space of software, hardware and communication strategies, Paleo can efficiently and accurately model the expected scalability and performance of a putative deep learning system.  We show that Paleo is robust to the choice of network architecture, hardware, software, communication schemes, and parallelization strategies. We further demonstrate its ability to accurately model various recently published scalability results for CNNs such as NiN, Inception and AlexNet.", "pdf": "/pdf/c30790984e960f282cd1534f65f5f26444ec9a19.pdf", "TL;DR": "Paleo: An analytical performance model for exploring the space of scalable deep learning systems and quickly diagnosing their effectiveness for a given problem instance.", "paperhash": "qi|paleo_a_performance_model_for_deep_neural_networks", "keywords": ["Deep learning"], "conflicts": ["cs.ucla.edu", "cs.berkeley.edu"], "authors": ["Hang Qi", "Evan R. Sparks", "Ameet Talwalkar"], "authorids": ["hangqi@cs.ucla.edu", "sparks@cs.berkeley.edu", "ameet@cs.ucla.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481719481613, "id": "ICLR.cc/2017/conference/-/paper277/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper277/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper277/AnonReviewer3", "ICLR.cc/2017/conference/paper277/AnonReviewer1", "ICLR.cc/2017/conference/paper277/AnonReviewer2"], "reply": {"forum": "SyVVJ85lg", "replyto": "SyVVJ85lg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper277/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper277/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481719481613}}}, {"tddate": null, "tmdate": 1481443746860, "tcdate": 1481443746851, "number": 4, "id": "SkiotF5Xg", "invitation": "ICLR.cc/2017/conference/-/paper277/public/comment", "forum": "SyVVJ85lg", "replyto": "SyzvzN7Qx", "signatures": ["~Hang_Qi1"], "readers": ["everyone"], "writers": ["~Hang_Qi1"], "content": {"title": "Live demo is online now", "comment": "As a follow-up to the previous response, we have release a live demo at https://talwalkarlab.github.io/paleo/.\nNotably, we added a cost estimation feature that predicts dollar costs on AWS instances.\n\nThe current interface provides a predefined set of configurations and works with data parallelism. We will allow users to upload customized networks and model splits in the coming releases of the interface; although core implementations are already include in the open-sourced repository."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Paleo: A Performance Model for Deep Neural Networks", "abstract": "Although various scalable deep learning software packages have been proposed, it remains unclear how to best leverage parallel and distributed computing infrastructure to accelerate their training and deployment. Moreover, the effectiveness of existing parallel and distributed systems varies widely based on the neural network architecture and dataset under consideration.  In order to efficiently explore the space of scalable deep learning systems and quickly diagnose their effectiveness for a given problem instance, we introduce an analytical performance model called Paleo. Our key observation is that a neural network architecture carries with it a declarative specification of the computational requirements associated with its training and evaluation. By extracting these requirements from a given architecture and mapping them to a specific point within the design space of software, hardware and communication strategies, Paleo can efficiently and accurately model the expected scalability and performance of a putative deep learning system.  We show that Paleo is robust to the choice of network architecture, hardware, software, communication schemes, and parallelization strategies. We further demonstrate its ability to accurately model various recently published scalability results for CNNs such as NiN, Inception and AlexNet.", "pdf": "/pdf/c30790984e960f282cd1534f65f5f26444ec9a19.pdf", "TL;DR": "Paleo: An analytical performance model for exploring the space of scalable deep learning systems and quickly diagnosing their effectiveness for a given problem instance.", "paperhash": "qi|paleo_a_performance_model_for_deep_neural_networks", "keywords": ["Deep learning"], "conflicts": ["cs.ucla.edu", "cs.berkeley.edu"], "authors": ["Hang Qi", "Evan R. Sparks", "Ameet Talwalkar"], "authorids": ["hangqi@cs.ucla.edu", "sparks@cs.berkeley.edu", "ameet@cs.ucla.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287644775, "id": "ICLR.cc/2017/conference/-/paper277/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SyVVJ85lg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper277/reviewers", "ICLR.cc/2017/conference/paper277/areachairs"], "cdate": 1485287644775}}}, {"tddate": null, "tmdate": 1481045863835, "tcdate": 1480962649822, "number": 1, "id": "SyzvzN7Qx", "invitation": "ICLR.cc/2017/conference/-/paper277/official/review", "forum": "SyVVJ85lg", "replyto": "SyVVJ85lg", "signatures": ["ICLR.cc/2017/conference/paper277/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper277/AnonReviewer3"], "content": {"title": "Technically sound. Only useful under the assumption that the code is released.", "rating": "6: Marginally above acceptance threshold", "review": "This paper is technically sound. It highlights well the strengths and weaknesses of the proposed simplified model.\n\nIn terms of impact, its novelty is limited, in the sense that the authors did seemingly the right thing and obtained the expected outcomes. The idea of modeling deep learning computation is not in itself particularly novel. As a companion paper to an open source release of the model, it would meet my bar of acceptance in the same vein as a paper describing a novel dataset, which might not provide groundbreaking insights, yet be generally useful to the community.\n\nIn the absence of released code, even if the authors promise to release it soon, I am more ambivalent, since that's where all the value lies. It would also be a different story if the authors had been able to use this framework to make novel architectural decisions that improved training scalability in some way, and incorporated such new insights in the paper.\n\nUPDATED: code is now available. Revised review accordingly.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Paleo: A Performance Model for Deep Neural Networks", "abstract": "Although various scalable deep learning software packages have been proposed, it remains unclear how to best leverage parallel and distributed computing infrastructure to accelerate their training and deployment. Moreover, the effectiveness of existing parallel and distributed systems varies widely based on the neural network architecture and dataset under consideration.  In order to efficiently explore the space of scalable deep learning systems and quickly diagnose their effectiveness for a given problem instance, we introduce an analytical performance model called Paleo. Our key observation is that a neural network architecture carries with it a declarative specification of the computational requirements associated with its training and evaluation. By extracting these requirements from a given architecture and mapping them to a specific point within the design space of software, hardware and communication strategies, Paleo can efficiently and accurately model the expected scalability and performance of a putative deep learning system.  We show that Paleo is robust to the choice of network architecture, hardware, software, communication schemes, and parallelization strategies. We further demonstrate its ability to accurately model various recently published scalability results for CNNs such as NiN, Inception and AlexNet.", "pdf": "/pdf/c30790984e960f282cd1534f65f5f26444ec9a19.pdf", "TL;DR": "Paleo: An analytical performance model for exploring the space of scalable deep learning systems and quickly diagnosing their effectiveness for a given problem instance.", "paperhash": "qi|paleo_a_performance_model_for_deep_neural_networks", "keywords": ["Deep learning"], "conflicts": ["cs.ucla.edu", "cs.berkeley.edu"], "authors": ["Hang Qi", "Evan R. Sparks", "Ameet Talwalkar"], "authorids": ["hangqi@cs.ucla.edu", "sparks@cs.berkeley.edu", "ameet@cs.ucla.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512641107, "id": "ICLR.cc/2017/conference/-/paper277/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper277/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper277/AnonReviewer3", "ICLR.cc/2017/conference/paper277/AnonReviewer2", "ICLR.cc/2017/conference/paper277/AnonReviewer1"], "reply": {"forum": "SyVVJ85lg", "replyto": "SyVVJ85lg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper277/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper277/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512641107}}}, {"tddate": null, "tmdate": 1481011335515, "tcdate": 1481011335510, "number": 3, "id": "BJJqle4Ql", "invitation": "ICLR.cc/2017/conference/-/paper277/public/comment", "forum": "SyVVJ85lg", "replyto": "SyzvzN7Qx", "signatures": ["~Hang_Qi1"], "readers": ["everyone"], "writers": ["~Hang_Qi1"], "content": {"title": "Open-source release", "comment": "Thank you for the comment. As an update, we have pushed our initial open-source release at https://github.com/TalwalkarLab/paleo.\n\nIn addition, we are also working on a web interface based on this code. We will post the link here when it becomes available by the end of this week."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Paleo: A Performance Model for Deep Neural Networks", "abstract": "Although various scalable deep learning software packages have been proposed, it remains unclear how to best leverage parallel and distributed computing infrastructure to accelerate their training and deployment. Moreover, the effectiveness of existing parallel and distributed systems varies widely based on the neural network architecture and dataset under consideration.  In order to efficiently explore the space of scalable deep learning systems and quickly diagnose their effectiveness for a given problem instance, we introduce an analytical performance model called Paleo. Our key observation is that a neural network architecture carries with it a declarative specification of the computational requirements associated with its training and evaluation. By extracting these requirements from a given architecture and mapping them to a specific point within the design space of software, hardware and communication strategies, Paleo can efficiently and accurately model the expected scalability and performance of a putative deep learning system.  We show that Paleo is robust to the choice of network architecture, hardware, software, communication schemes, and parallelization strategies. We further demonstrate its ability to accurately model various recently published scalability results for CNNs such as NiN, Inception and AlexNet.", "pdf": "/pdf/c30790984e960f282cd1534f65f5f26444ec9a19.pdf", "TL;DR": "Paleo: An analytical performance model for exploring the space of scalable deep learning systems and quickly diagnosing their effectiveness for a given problem instance.", "paperhash": "qi|paleo_a_performance_model_for_deep_neural_networks", "keywords": ["Deep learning"], "conflicts": ["cs.ucla.edu", "cs.berkeley.edu"], "authors": ["Hang Qi", "Evan R. Sparks", "Ameet Talwalkar"], "authorids": ["hangqi@cs.ucla.edu", "sparks@cs.berkeley.edu", "ameet@cs.ucla.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287644775, "id": "ICLR.cc/2017/conference/-/paper277/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SyVVJ85lg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper277/reviewers", "ICLR.cc/2017/conference/paper277/areachairs"], "cdate": 1485287644775}}}, {"tddate": null, "tmdate": 1480822713935, "tcdate": 1480822713926, "number": 2, "id": "HJMp1G-Qx", "invitation": "ICLR.cc/2017/conference/-/paper277/public/comment", "forum": "SyVVJ85lg", "replyto": "Syhu4rRfx", "signatures": ["~Hang_Qi1"], "readers": ["everyone"], "writers": ["~Hang_Qi1"], "content": {"title": "Clarifications", "comment": "Thanks for the comments! Here are the clarifications:\n\n(1) The definition of \u201cone step time\u201d is the total time of forward propagation, backward propagation, and parameter update for one mini-batch on one worker.  We will add a sentence in the text to clarify this in our next revision.\n\n(2) We did not include the reported times in Table 2 for various reasons: for case studies 1 and 3 the original publications do not mention these times explicitly; for case study 2 no run time information is provided whatsoever; and since case study 4 is a hypothetical setup, there is no number to report.\n\nHowever, for case studies 1 and 3, we can in fact derived this number approximately from the available information in the papers, and the numbers are as follows: \n- Case 1:  Paleo: 1918.33 ms*; FireCaffe: 2274.59 ms.\n- Case 3:  Paleo: 402 ms; OneWeirdTrick: 418.35 ms.\n\n* We just noticed a typo in Paleo\u2019s one step time estimate for Case 1 our v2 draft (11/15). We fixed a minor bug in our Case 1 results between v1 and v2 (as seen Table 3), but we didn\u2019t update our \u2018one step time\u2019 number in Table 2. We apologize for this oversight and it will be fixed shortly in our next revision."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Paleo: A Performance Model for Deep Neural Networks", "abstract": "Although various scalable deep learning software packages have been proposed, it remains unclear how to best leverage parallel and distributed computing infrastructure to accelerate their training and deployment. Moreover, the effectiveness of existing parallel and distributed systems varies widely based on the neural network architecture and dataset under consideration.  In order to efficiently explore the space of scalable deep learning systems and quickly diagnose their effectiveness for a given problem instance, we introduce an analytical performance model called Paleo. Our key observation is that a neural network architecture carries with it a declarative specification of the computational requirements associated with its training and evaluation. By extracting these requirements from a given architecture and mapping them to a specific point within the design space of software, hardware and communication strategies, Paleo can efficiently and accurately model the expected scalability and performance of a putative deep learning system.  We show that Paleo is robust to the choice of network architecture, hardware, software, communication schemes, and parallelization strategies. We further demonstrate its ability to accurately model various recently published scalability results for CNNs such as NiN, Inception and AlexNet.", "pdf": "/pdf/c30790984e960f282cd1534f65f5f26444ec9a19.pdf", "TL;DR": "Paleo: An analytical performance model for exploring the space of scalable deep learning systems and quickly diagnosing their effectiveness for a given problem instance.", "paperhash": "qi|paleo_a_performance_model_for_deep_neural_networks", "keywords": ["Deep learning"], "conflicts": ["cs.ucla.edu", "cs.berkeley.edu"], "authors": ["Hang Qi", "Evan R. Sparks", "Ameet Talwalkar"], "authorids": ["hangqi@cs.ucla.edu", "sparks@cs.berkeley.edu", "ameet@cs.ucla.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287644775, "id": "ICLR.cc/2017/conference/-/paper277/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SyVVJ85lg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper277/reviewers", "ICLR.cc/2017/conference/paper277/areachairs"], "cdate": 1485287644775}}}, {"tddate": null, "tmdate": 1480639603669, "tcdate": 1480639603665, "number": 2, "id": "Syhu4rRfx", "invitation": "ICLR.cc/2017/conference/-/paper277/pre-review/question", "forum": "SyVVJ85lg", "replyto": "SyVVJ85lg", "signatures": ["ICLR.cc/2017/conference/paper277/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper277/AnonReviewer1"], "content": {"title": "Clarification - Table 2", "question": "Can you explain what is \"one step time\" in Table 2? Also again in Table 2, there are estimates by PALEO, can you also add the reported results from the publications for these estimations? "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Paleo: A Performance Model for Deep Neural Networks", "abstract": "Although various scalable deep learning software packages have been proposed, it remains unclear how to best leverage parallel and distributed computing infrastructure to accelerate their training and deployment. Moreover, the effectiveness of existing parallel and distributed systems varies widely based on the neural network architecture and dataset under consideration.  In order to efficiently explore the space of scalable deep learning systems and quickly diagnose their effectiveness for a given problem instance, we introduce an analytical performance model called Paleo. Our key observation is that a neural network architecture carries with it a declarative specification of the computational requirements associated with its training and evaluation. By extracting these requirements from a given architecture and mapping them to a specific point within the design space of software, hardware and communication strategies, Paleo can efficiently and accurately model the expected scalability and performance of a putative deep learning system.  We show that Paleo is robust to the choice of network architecture, hardware, software, communication schemes, and parallelization strategies. We further demonstrate its ability to accurately model various recently published scalability results for CNNs such as NiN, Inception and AlexNet.", "pdf": "/pdf/c30790984e960f282cd1534f65f5f26444ec9a19.pdf", "TL;DR": "Paleo: An analytical performance model for exploring the space of scalable deep learning systems and quickly diagnosing their effectiveness for a given problem instance.", "paperhash": "qi|paleo_a_performance_model_for_deep_neural_networks", "keywords": ["Deep learning"], "conflicts": ["cs.ucla.edu", "cs.berkeley.edu"], "authors": ["Hang Qi", "Evan R. Sparks", "Ameet Talwalkar"], "authorids": ["hangqi@cs.ucla.edu", "sparks@cs.berkeley.edu", "ameet@cs.ucla.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481719481613, "id": "ICLR.cc/2017/conference/-/paper277/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper277/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper277/AnonReviewer3", "ICLR.cc/2017/conference/paper277/AnonReviewer1", "ICLR.cc/2017/conference/paper277/AnonReviewer2"], "reply": {"forum": "SyVVJ85lg", "replyto": "SyVVJ85lg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper277/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper277/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481719481613}}}, {"tddate": null, "tmdate": 1480366806753, "tcdate": 1480366759114, "number": 1, "id": "BJyhqG5Ge", "invitation": "ICLR.cc/2017/conference/-/paper277/public/comment", "forum": "SyVVJ85lg", "replyto": "S1hB5J9Gg", "signatures": ["~Hang_Qi1"], "readers": ["everyone"], "writers": ["~Hang_Qi1"], "content": {"title": "Yes", "comment": "Thanks for the comment! Yes, we do have plans to make it publicly available soon. And indeed as one of applications/future directions, it can be used by software frameworks for informed model splitting and device placement. Although for this paper we emphasize our model is robust to the choice of network architecture, hardware, software, communication schemes, and parallelization strategies."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Paleo: A Performance Model for Deep Neural Networks", "abstract": "Although various scalable deep learning software packages have been proposed, it remains unclear how to best leverage parallel and distributed computing infrastructure to accelerate their training and deployment. Moreover, the effectiveness of existing parallel and distributed systems varies widely based on the neural network architecture and dataset under consideration.  In order to efficiently explore the space of scalable deep learning systems and quickly diagnose their effectiveness for a given problem instance, we introduce an analytical performance model called Paleo. Our key observation is that a neural network architecture carries with it a declarative specification of the computational requirements associated with its training and evaluation. By extracting these requirements from a given architecture and mapping them to a specific point within the design space of software, hardware and communication strategies, Paleo can efficiently and accurately model the expected scalability and performance of a putative deep learning system.  We show that Paleo is robust to the choice of network architecture, hardware, software, communication schemes, and parallelization strategies. We further demonstrate its ability to accurately model various recently published scalability results for CNNs such as NiN, Inception and AlexNet.", "pdf": "/pdf/c30790984e960f282cd1534f65f5f26444ec9a19.pdf", "TL;DR": "Paleo: An analytical performance model for exploring the space of scalable deep learning systems and quickly diagnosing their effectiveness for a given problem instance.", "paperhash": "qi|paleo_a_performance_model_for_deep_neural_networks", "keywords": ["Deep learning"], "conflicts": ["cs.ucla.edu", "cs.berkeley.edu"], "authors": ["Hang Qi", "Evan R. Sparks", "Ameet Talwalkar"], "authorids": ["hangqi@cs.ucla.edu", "sparks@cs.berkeley.edu", "ameet@cs.ucla.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287644775, "id": "ICLR.cc/2017/conference/-/paper277/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SyVVJ85lg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper277/reviewers", "ICLR.cc/2017/conference/paper277/areachairs"], "cdate": 1485287644775}}}, {"tddate": null, "tmdate": 1480354371854, "tcdate": 1480354371850, "number": 1, "id": "S1hB5J9Gg", "invitation": "ICLR.cc/2017/conference/-/paper277/pre-review/question", "forum": "SyVVJ85lg", "replyto": "SyVVJ85lg", "signatures": ["ICLR.cc/2017/conference/paper277/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper277/AnonReviewer3"], "content": {"title": "Any plans to open-source the model?", "question": "Or better: incorporate it into TensorFlow so that the framework is able to make better decisions."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Paleo: A Performance Model for Deep Neural Networks", "abstract": "Although various scalable deep learning software packages have been proposed, it remains unclear how to best leverage parallel and distributed computing infrastructure to accelerate their training and deployment. Moreover, the effectiveness of existing parallel and distributed systems varies widely based on the neural network architecture and dataset under consideration.  In order to efficiently explore the space of scalable deep learning systems and quickly diagnose their effectiveness for a given problem instance, we introduce an analytical performance model called Paleo. Our key observation is that a neural network architecture carries with it a declarative specification of the computational requirements associated with its training and evaluation. By extracting these requirements from a given architecture and mapping them to a specific point within the design space of software, hardware and communication strategies, Paleo can efficiently and accurately model the expected scalability and performance of a putative deep learning system.  We show that Paleo is robust to the choice of network architecture, hardware, software, communication schemes, and parallelization strategies. We further demonstrate its ability to accurately model various recently published scalability results for CNNs such as NiN, Inception and AlexNet.", "pdf": "/pdf/c30790984e960f282cd1534f65f5f26444ec9a19.pdf", "TL;DR": "Paleo: An analytical performance model for exploring the space of scalable deep learning systems and quickly diagnosing their effectiveness for a given problem instance.", "paperhash": "qi|paleo_a_performance_model_for_deep_neural_networks", "keywords": ["Deep learning"], "conflicts": ["cs.ucla.edu", "cs.berkeley.edu"], "authors": ["Hang Qi", "Evan R. Sparks", "Ameet Talwalkar"], "authorids": ["hangqi@cs.ucla.edu", "sparks@cs.berkeley.edu", "ameet@cs.ucla.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481719481613, "id": "ICLR.cc/2017/conference/-/paper277/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper277/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper277/AnonReviewer3", "ICLR.cc/2017/conference/paper277/AnonReviewer1", "ICLR.cc/2017/conference/paper277/AnonReviewer2"], "reply": {"forum": "SyVVJ85lg", "replyto": "SyVVJ85lg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper277/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper277/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481719481613}}}], "count": 13}