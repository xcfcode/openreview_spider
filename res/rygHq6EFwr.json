{"notes": [{"id": "rygHq6EFwr", "original": "BJg_W36vwr", "number": 705, "cdate": 1569439116823, "ddate": null, "tcdate": 1569439116823, "tmdate": 1577168254345, "tddate": null, "forum": "rygHq6EFwr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "GResNet: Graph Residual Network for Reviving Deep GNNs from Suspended Animation", "authors": ["Jiawei Zhang", "Lin Meng"], "authorids": ["jiawei@ifmlab.org", "lin@ifmlab.org"], "keywords": ["Graph Neural Networks", "Node Classification", "Representation Learning"], "TL;DR": "Identifying suspended animation problem with GNNs, propose a new model to resolve the problem with graph residual learning.", "abstract": "The existing graph neural networks (GNNs) based on the spectral graph convolutional operator have been criticized for its performance degradation, which is especially common for the models with deep architectures. In this paper, we further identify the suspended animation problem with the existing GNNs. Such a problem happens when the model depth reaches the suspended animation limit, and the model will not respond to the training data any more and become not learnable. Analysis about the causes of the suspended animation problem with existing GNNs will be provided in this paper, whereas several other peripheral factors that will impact the problem will be reported as well. To resolve the problem, we introduce the GRESNET (Graph Residual Network) framework in this paper, which creates extensively connected highways to involve nodes\u2019 raw features or intermediate representations throughout the graph for all the model layers. Different from the other learning settings, the extensive connections in the graph data will render the existing simple residual learning methods fail to work. We prove the effectiveness of the introduced new graph residual terms from the norm preservation perspective, which will help avoid dramatic changes to the node\u2019s representations between sequential layers. Detailed studies about the GRESNET framework for many existing GNNs, including GCN, GAT and LOOPYNET, will be reported in the paper with extensive empirical experiments on real-world benchmark datasets.", "pdf": "/pdf/eb260039993ec90976aaef54278ffb7f24011343.pdf", "code": "https://github.com/anonymous-sourcecode/GResNet", "paperhash": "zhang|gresnet_graph_residual_network_for_reviving_deep_gnns_from_suspended_animation", "original_pdf": "/attachment/e1556ba32a7089915ca0fa343161848eee1329c2.pdf", "_bibtex": "@misc{\nzhang2020gresnet,\ntitle={{\\{}GR{\\}}esNet: Graph Residual Network for Reviving Deep {\\{}GNN{\\}}s from Suspended Animation},\nauthor={Jiawei Zhang and Lin Meng},\nyear={2020},\nurl={https://openreview.net/forum?id=rygHq6EFwr}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 11, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "OY4uYTwyvg", "original": null, "number": 1, "cdate": 1576798703804, "ddate": null, "tcdate": 1576798703804, "tmdate": 1576800932249, "tddate": null, "forum": "rygHq6EFwr", "replyto": "rygHq6EFwr", "invitation": "ICLR.cc/2020/Conference/Paper705/-/Decision", "content": {"decision": "Reject", "comment": "This paper studies the \u201csuspended animation limit\u201d of various graph neural networks (GNNs) and provides some theoretical analysis to explain its cause. To overcome the limitation, the authors propose Graph Residual Network (GRESNET) framework to involve nodes\u2019 raw features or intermediate representations throughout the graph for all the model layers. The main concern of the reviewers is: the assumption made for theoretical analysis that the fully connected layer is identical mapping is too stringent. The paper does not gather sufficient support from the reviewers to merit acceptance, even after author response and reviewer discussion.  I thus recommend reject.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "GResNet: Graph Residual Network for Reviving Deep GNNs from Suspended Animation", "authors": ["Jiawei Zhang", "Lin Meng"], "authorids": ["jiawei@ifmlab.org", "lin@ifmlab.org"], "keywords": ["Graph Neural Networks", "Node Classification", "Representation Learning"], "TL;DR": "Identifying suspended animation problem with GNNs, propose a new model to resolve the problem with graph residual learning.", "abstract": "The existing graph neural networks (GNNs) based on the spectral graph convolutional operator have been criticized for its performance degradation, which is especially common for the models with deep architectures. In this paper, we further identify the suspended animation problem with the existing GNNs. Such a problem happens when the model depth reaches the suspended animation limit, and the model will not respond to the training data any more and become not learnable. Analysis about the causes of the suspended animation problem with existing GNNs will be provided in this paper, whereas several other peripheral factors that will impact the problem will be reported as well. To resolve the problem, we introduce the GRESNET (Graph Residual Network) framework in this paper, which creates extensively connected highways to involve nodes\u2019 raw features or intermediate representations throughout the graph for all the model layers. Different from the other learning settings, the extensive connections in the graph data will render the existing simple residual learning methods fail to work. We prove the effectiveness of the introduced new graph residual terms from the norm preservation perspective, which will help avoid dramatic changes to the node\u2019s representations between sequential layers. Detailed studies about the GRESNET framework for many existing GNNs, including GCN, GAT and LOOPYNET, will be reported in the paper with extensive empirical experiments on real-world benchmark datasets.", "pdf": "/pdf/eb260039993ec90976aaef54278ffb7f24011343.pdf", "code": "https://github.com/anonymous-sourcecode/GResNet", "paperhash": "zhang|gresnet_graph_residual_network_for_reviving_deep_gnns_from_suspended_animation", "original_pdf": "/attachment/e1556ba32a7089915ca0fa343161848eee1329c2.pdf", "_bibtex": "@misc{\nzhang2020gresnet,\ntitle={{\\{}GR{\\}}esNet: Graph Residual Network for Reviving Deep {\\{}GNN{\\}}s from Suspended Animation},\nauthor={Jiawei Zhang and Lin Meng},\nyear={2020},\nurl={https://openreview.net/forum?id=rygHq6EFwr}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "rygHq6EFwr", "replyto": "rygHq6EFwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795704063, "tmdate": 1576800251571, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper705/-/Decision"}}}, {"id": "B1evBVMx5r", "original": null, "number": 3, "cdate": 1571984446982, "ddate": null, "tcdate": 1571984446982, "tmdate": 1573966270372, "tddate": null, "forum": "rygHq6EFwr", "replyto": "rygHq6EFwr", "invitation": "ICLR.cc/2020/Conference/Paper705/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #2", "review": "The paper studies the causes of the empirically poor performance in deep structures that plagues existing GNNs, and identify the suspended animation problem as the main issue. In analogy to the Residual CNN network, a residual graph network is proposed to address such issue. Moreover, the underlying Markov chain property such as stationary distribution is theoretically analyzed, the so-called suspended animation limit is defined and its upper and lower bounds are established. Empirical experiments are relatived short and less sufficient, with comparisons on there datasets: Cora, Citeseer, and Pubmed. It would be more convincing to present its performance on a more diverse range of datasets.  Note the results on Citeseer is inferior to existing method. It is helpful to clearly explain why  this could be the case.\n\nPost rebuttal edition: After reading the reviews and the authors' reply, several questions such as the major concerns over this oversimplified linear assumptions surface out, as discussed in length by other reviewers. Meanwhile, I still believe there are useful merits of this paper. Thus I adjust my current rating to weak accept.\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper705/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper705/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "GResNet: Graph Residual Network for Reviving Deep GNNs from Suspended Animation", "authors": ["Jiawei Zhang", "Lin Meng"], "authorids": ["jiawei@ifmlab.org", "lin@ifmlab.org"], "keywords": ["Graph Neural Networks", "Node Classification", "Representation Learning"], "TL;DR": "Identifying suspended animation problem with GNNs, propose a new model to resolve the problem with graph residual learning.", "abstract": "The existing graph neural networks (GNNs) based on the spectral graph convolutional operator have been criticized for its performance degradation, which is especially common for the models with deep architectures. In this paper, we further identify the suspended animation problem with the existing GNNs. Such a problem happens when the model depth reaches the suspended animation limit, and the model will not respond to the training data any more and become not learnable. Analysis about the causes of the suspended animation problem with existing GNNs will be provided in this paper, whereas several other peripheral factors that will impact the problem will be reported as well. To resolve the problem, we introduce the GRESNET (Graph Residual Network) framework in this paper, which creates extensively connected highways to involve nodes\u2019 raw features or intermediate representations throughout the graph for all the model layers. Different from the other learning settings, the extensive connections in the graph data will render the existing simple residual learning methods fail to work. We prove the effectiveness of the introduced new graph residual terms from the norm preservation perspective, which will help avoid dramatic changes to the node\u2019s representations between sequential layers. Detailed studies about the GRESNET framework for many existing GNNs, including GCN, GAT and LOOPYNET, will be reported in the paper with extensive empirical experiments on real-world benchmark datasets.", "pdf": "/pdf/eb260039993ec90976aaef54278ffb7f24011343.pdf", "code": "https://github.com/anonymous-sourcecode/GResNet", "paperhash": "zhang|gresnet_graph_residual_network_for_reviving_deep_gnns_from_suspended_animation", "original_pdf": "/attachment/e1556ba32a7089915ca0fa343161848eee1329c2.pdf", "_bibtex": "@misc{\nzhang2020gresnet,\ntitle={{\\{}GR{\\}}esNet: Graph Residual Network for Reviving Deep {\\{}GNN{\\}}s from Suspended Animation},\nauthor={Jiawei Zhang and Lin Meng},\nyear={2020},\nurl={https://openreview.net/forum?id=rygHq6EFwr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rygHq6EFwr", "replyto": "rygHq6EFwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper705/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper705/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575704699532, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper705/Reviewers"], "noninvitees": [], "tcdate": 1570237748280, "tmdate": 1575704699548, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper705/-/Official_Review"}}}, {"id": "r1lMWMi2oH", "original": null, "number": 8, "cdate": 1573855738232, "ddate": null, "tcdate": 1573855738232, "tmdate": 1573855971266, "tddate": null, "forum": "rygHq6EFwr", "replyto": "rygHq6EFwr", "invitation": "ICLR.cc/2020/Conference/Paper705/-/Official_Comment", "content": {"title": "To All Reviewers (changes in the PDF submission)", "comment": "Hi All,\n\nWe have worked no the rebuttal for this submission in the past week, we also appreciate your constructive suggestions and comments. Hope our response can resolve your concerns about this paper. Some of the proposed concerns are hard (or impossible) to resolve actually, like analysis with the non-linear layers. It is not only hard for us in this paper, but also a very challenging task for the whole community to study the model performance bounds with such Multiple Non-linear layers. Really hope the reviewers can understand that.\n\nTogether with the responses posted below, we also updated the PDF submission by updating/adding/deleting some contents in the paper. The above PDF submission is slightly longer than 10 pages now and the reference is also longer than 2 pages. We will shrink the paper to the required page limit in the camera-ready version instead.\n\n\nThe main changes to the PDF submission include:\n\n(1) New clarification to some concepts (e.g., power-law) are added. New reference papers are added.\n\n(2) New experimental results added.\n\n(2-1) In the new Section 8.1 and Table 4 in the appendix, we provide the results of GResNet on two other large-sized graph datasets, PPI and Reddit. These two datasets are not the common choices for graph neural network evaluation actually. Therefore, many entries of the baseline methods are not provided in Table 4. Also GAT is too slow, and not runnable on the large sized graph dataset (it is not our problem), so we cannot get the result of GResNet(GAT) out neither in one week. We will complement the Table 4 in the camera-ready version of this paper instead.\n\n(2-2) In the new Section 8.2 (Table 5, and Figure 4-8) in the appendix, we provide the experimental results of the models with even deeper architectures, e.g., 10, 20, 30, 40, 50. According to the results, some of the residual terms (e.g., naive, graph-naive) will fail to work as the model goes deeper, but raw and graph raw residual terms can still work very well.\n\n(2-3) In Section 9 (Table 6-7) in the appendix, we add the analysis and results about several other factors, including training set size and the raw feature encoding methods. Just for the reviewer's information.\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper705/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper705/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "GResNet: Graph Residual Network for Reviving Deep GNNs from Suspended Animation", "authors": ["Jiawei Zhang", "Lin Meng"], "authorids": ["jiawei@ifmlab.org", "lin@ifmlab.org"], "keywords": ["Graph Neural Networks", "Node Classification", "Representation Learning"], "TL;DR": "Identifying suspended animation problem with GNNs, propose a new model to resolve the problem with graph residual learning.", "abstract": "The existing graph neural networks (GNNs) based on the spectral graph convolutional operator have been criticized for its performance degradation, which is especially common for the models with deep architectures. In this paper, we further identify the suspended animation problem with the existing GNNs. Such a problem happens when the model depth reaches the suspended animation limit, and the model will not respond to the training data any more and become not learnable. Analysis about the causes of the suspended animation problem with existing GNNs will be provided in this paper, whereas several other peripheral factors that will impact the problem will be reported as well. To resolve the problem, we introduce the GRESNET (Graph Residual Network) framework in this paper, which creates extensively connected highways to involve nodes\u2019 raw features or intermediate representations throughout the graph for all the model layers. Different from the other learning settings, the extensive connections in the graph data will render the existing simple residual learning methods fail to work. We prove the effectiveness of the introduced new graph residual terms from the norm preservation perspective, which will help avoid dramatic changes to the node\u2019s representations between sequential layers. Detailed studies about the GRESNET framework for many existing GNNs, including GCN, GAT and LOOPYNET, will be reported in the paper with extensive empirical experiments on real-world benchmark datasets.", "pdf": "/pdf/eb260039993ec90976aaef54278ffb7f24011343.pdf", "code": "https://github.com/anonymous-sourcecode/GResNet", "paperhash": "zhang|gresnet_graph_residual_network_for_reviving_deep_gnns_from_suspended_animation", "original_pdf": "/attachment/e1556ba32a7089915ca0fa343161848eee1329c2.pdf", "_bibtex": "@misc{\nzhang2020gresnet,\ntitle={{\\{}GR{\\}}esNet: Graph Residual Network for Reviving Deep {\\{}GNN{\\}}s from Suspended Animation},\nauthor={Jiawei Zhang and Lin Meng},\nyear={2020},\nurl={https://openreview.net/forum?id=rygHq6EFwr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rygHq6EFwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper705/Authors", "ICLR.cc/2020/Conference/Paper705/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper705/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper705/Reviewers", "ICLR.cc/2020/Conference/Paper705/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper705/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper705/Authors|ICLR.cc/2020/Conference/Paper705/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504167470, "tmdate": 1576860545592, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper705/Authors", "ICLR.cc/2020/Conference/Paper705/Reviewers", "ICLR.cc/2020/Conference/Paper705/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper705/-/Official_Comment"}}}, {"id": "BylYv9r2sS", "original": null, "number": 6, "cdate": 1573833312772, "ddate": null, "tcdate": 1573833312772, "tmdate": 1573854784635, "tddate": null, "forum": "rygHq6EFwr", "replyto": "S1lx7puZtB", "invitation": "ICLR.cc/2020/Conference/Paper705/-/Official_Comment", "content": {"title": "Response to Review #3 (Part 2, continued ...)", "comment": "\n\n******************\nQuestion 3: Justify the adding of residual links from the perspective of Theorem 1.\n\nResponse: We clarify that the Corollary 1 is the justification for the na\u00efve residual term actually, we can also drive similar bound terms for the raw residual terms based on Corollary 1 and [9]. For the remaining two residual terms, the derivation and proof will be very similar as well. We don't plan to study the residual term analysis in this way, since it will make the analysis sections too redundant and also fragmented, and also introduce multiple Corollaries with duplicated contents/words.\n\nIf the reviewer can further take a look at Theorem 3, we clarify that Theorem 3 used in the paper is more appropriate for illustrating the effectiveness of the proposed residual terms due to:\n(1) except the assumption of the mapping properties, it has no other assumptions on the model.\n(2) it works for all these residual terms, and we don\u2019t need to case by case analysis.\n(3) The gap term $\\delta = c \\cdot \\frac{\\log 2K}{K}$ illustrates that deeper model with our proposed residual terms tend to lead to smaller gaps instead.\n\nIf the reviewer really think the case by case study on the residual terms is better, we will also add it to the appendix as another way to illustrate the effectiveness of the residual terms. However, we still have the concerns, since it will make the analysis section looks long-winded and also fragmented, and it is also duplicated with the current Theorem 3.\n\nHope the reviewer can understand our concern.\n\n******************\n\nQuestion 4: The authors study the GNNs up to 7 layers at most. \n\nResponse: We clarify that we focus on illustrating the suspended animation problem with the existing GNNs, and we also intend to propose a feasible solution to resolve the problem. For GCN, its suspended animation limit is 5, so we only show the experiment results of the model with depth up to 7 in the original submission.\n\nActually, we have studied GCN to 50 layers. We have added the extra experimental results on GCN with very deep layers in Figure 4-8 in the appendix (Sec 8.2 on page 16-17 of the updated PDF submission of the paper), just for the reviewer\u2019s information. According to the results, we can observe that GCN, GResNet(naive) and GResNet(graph-naive) will all fail to work as the model depth increases to 20 and more. However, GResNet(raw) and GResNet(graph-raw) can still work very well for very deep architectures.\n\n******************\n\nQuestion 5: Impact of the other factors together with the residual terms on the performance of the models.\n\nResponse: We clarify that we have the analysis results available on these factors actually. However, since the results are not very closely related to the analysis of suspended animation problem , so we didn\u2019t put them in the paper.\n\nSince the reviewer point this out, we also add the results in the paper appendix to illustrate the impact of the other factors together with the residual terms on the models. The results are available in Section 9 in the appendix (on page 18) of the updated PDF submission above.\n\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper705/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper705/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "GResNet: Graph Residual Network for Reviving Deep GNNs from Suspended Animation", "authors": ["Jiawei Zhang", "Lin Meng"], "authorids": ["jiawei@ifmlab.org", "lin@ifmlab.org"], "keywords": ["Graph Neural Networks", "Node Classification", "Representation Learning"], "TL;DR": "Identifying suspended animation problem with GNNs, propose a new model to resolve the problem with graph residual learning.", "abstract": "The existing graph neural networks (GNNs) based on the spectral graph convolutional operator have been criticized for its performance degradation, which is especially common for the models with deep architectures. In this paper, we further identify the suspended animation problem with the existing GNNs. Such a problem happens when the model depth reaches the suspended animation limit, and the model will not respond to the training data any more and become not learnable. Analysis about the causes of the suspended animation problem with existing GNNs will be provided in this paper, whereas several other peripheral factors that will impact the problem will be reported as well. To resolve the problem, we introduce the GRESNET (Graph Residual Network) framework in this paper, which creates extensively connected highways to involve nodes\u2019 raw features or intermediate representations throughout the graph for all the model layers. Different from the other learning settings, the extensive connections in the graph data will render the existing simple residual learning methods fail to work. We prove the effectiveness of the introduced new graph residual terms from the norm preservation perspective, which will help avoid dramatic changes to the node\u2019s representations between sequential layers. Detailed studies about the GRESNET framework for many existing GNNs, including GCN, GAT and LOOPYNET, will be reported in the paper with extensive empirical experiments on real-world benchmark datasets.", "pdf": "/pdf/eb260039993ec90976aaef54278ffb7f24011343.pdf", "code": "https://github.com/anonymous-sourcecode/GResNet", "paperhash": "zhang|gresnet_graph_residual_network_for_reviving_deep_gnns_from_suspended_animation", "original_pdf": "/attachment/e1556ba32a7089915ca0fa343161848eee1329c2.pdf", "_bibtex": "@misc{\nzhang2020gresnet,\ntitle={{\\{}GR{\\}}esNet: Graph Residual Network for Reviving Deep {\\{}GNN{\\}}s from Suspended Animation},\nauthor={Jiawei Zhang and Lin Meng},\nyear={2020},\nurl={https://openreview.net/forum?id=rygHq6EFwr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rygHq6EFwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper705/Authors", "ICLR.cc/2020/Conference/Paper705/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper705/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper705/Reviewers", "ICLR.cc/2020/Conference/Paper705/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper705/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper705/Authors|ICLR.cc/2020/Conference/Paper705/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504167470, "tmdate": 1576860545592, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper705/Authors", "ICLR.cc/2020/Conference/Paper705/Reviewers", "ICLR.cc/2020/Conference/Paper705/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper705/-/Official_Comment"}}}, {"id": "Hygru6r3jH", "original": null, "number": 7, "cdate": 1573834093184, "ddate": null, "tcdate": 1573834093184, "tmdate": 1573854250985, "tddate": null, "forum": "rygHq6EFwr", "replyto": "S1lx7puZtB", "invitation": "ICLR.cc/2020/Conference/Paper705/-/Official_Comment", "content": {"title": "Response to Review #3 (Part 1)", "comment": "\n\nFirst of all, we would like to appreciate the reviewer for the constructive suggestions. There is a space limit for our response textual input, so we will split our response into three parts. Here is the part 1, and part 2 and part 3 are posted below. The mentioned references are at the very bottom of the response.\n\n******************\nQuestion 1: Assumption on the identical assumption for the FC layer.\n\nResponse: Firstly, we agree with the reviewer that we may need to involve the non-linear layers into the model performance analysis if we can, since the non-linear layers tend to affect the suspended animation problem as well. The impacts come from multiple perspectives, one of the impact is caused by the gradient vanishing/exploding problem in learning the model with multiple non-linear layers. Also the non-linear layers will project the node representations to random states other than the stationary representations. \n\nHowever, we also hope the reviewer can understand the reason that we make that assumption. As we all know, analysis with the multiple non-linear layers is a difficult task, the community still have great challenges in interpreting and explaining the learning process of deep learning models by this context so far. Therefore, we propose to simplify the analysis settings based on the assumption mentioned in the reviewer's comments. We would like to clarify that our assumption also makes sense actually in certain perspectives illustrated as follows.\n\n(1) Non-linear layer will not revive the suspended animated models. We clarify that if a model suffers from the suspended animation problem, the non-linear layer will not change the suspended limitation problem. As clarified in the second paragraph of Sec 3.2 on page 4, the parameter W is actually shared for all the nodes in the GCN model. For any two nodes which obtain the identical representations (i.e., suffering from the suspended animation happens as stated in Theorem 1), their representations obtained via the non-linear layer will still be the same (i.e., the projected representations will still have the suspended animation problem).\n\n(2) Analysis feasibility. Involving the non-linear layers in the model analysis will make the graph neural network model performance and the animation limit bound introduced in Sec 4 extremely hard (or even impossible) to study. It will still due to the black-box property of the deep models with multiple non-linear projections. \n\n(3) Markov-Chain layer is our main focus. We also have experimental studies on node classification with the same dataset by using MLP as the model, which will not suffer from the suspended animation problem at all. Meanwhile, compared with MLP, the main differences between GCN and MLP is due to the Markov-Chain layers involved in the model. Therefore, in this paper, we propose to be focused on studying the impact of the Markov-Chain layer on the model learning performance. \n\n\n******************\n\nQuestion 2: Na\u00efve residual links work pretty good on the several datasets.\n\nResponse: We clarify that to check if na\u00efve residual links can work or not, we may need to check Figure3(a) and Figure3(b). From the plots, we observe that for the GResNet(GCN,na\u00efve) with less than 4 layers, we also acknowledge that its learning performance is not bad. However, from Figure 1, we observe that GCN without the na\u00efve residual term can already perform very good with no greater than 5 layers. In other words, the good performance for GResNet(GCN,na\u00efve) with no greater than 5 layers is not due to the na\u00efve residual terms, since the base model is not bad for these cases.\n\nHowever, we it comes to the GResNet(GCN,na\u00efve) model with 5,6,7 layers. From Figure3(a) and Figure3(b), we cannot really say that \u201cna\u00efve residual links work pretty good on several datasets\u201d to resolve the suspended animation problem, since they don't really work well actually. If we further check Figure 3(e)-3(h), the curve differences between GResNet(GCN,na\u00efve) vs GResNet(GCN,graph-raw) is very clear. I think the reviewer can also agree on this.\n\nTo further help resolve the reviewer's concern on this point, we also add new experimental results on the GCN and GResNet models with deeper architectures on the Cora dataset, where the model depth include 2, 10, 20, 30, 40, and 50. The results are reported in Section 8.2 on pages 16-17 in the appendix of the updated PDF submitted above.\n\nAccording to the new Table 5 and Figures 4-8, we can observe that the naive, graph-naive residual terms will fail to work as the model depth further increases to 10, 20, and even more. However, raw and graph-raw residual terms can still work very well for the models even with 50 layers. Hope the new results will help resolve your question.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper705/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper705/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "GResNet: Graph Residual Network for Reviving Deep GNNs from Suspended Animation", "authors": ["Jiawei Zhang", "Lin Meng"], "authorids": ["jiawei@ifmlab.org", "lin@ifmlab.org"], "keywords": ["Graph Neural Networks", "Node Classification", "Representation Learning"], "TL;DR": "Identifying suspended animation problem with GNNs, propose a new model to resolve the problem with graph residual learning.", "abstract": "The existing graph neural networks (GNNs) based on the spectral graph convolutional operator have been criticized for its performance degradation, which is especially common for the models with deep architectures. In this paper, we further identify the suspended animation problem with the existing GNNs. Such a problem happens when the model depth reaches the suspended animation limit, and the model will not respond to the training data any more and become not learnable. Analysis about the causes of the suspended animation problem with existing GNNs will be provided in this paper, whereas several other peripheral factors that will impact the problem will be reported as well. To resolve the problem, we introduce the GRESNET (Graph Residual Network) framework in this paper, which creates extensively connected highways to involve nodes\u2019 raw features or intermediate representations throughout the graph for all the model layers. Different from the other learning settings, the extensive connections in the graph data will render the existing simple residual learning methods fail to work. We prove the effectiveness of the introduced new graph residual terms from the norm preservation perspective, which will help avoid dramatic changes to the node\u2019s representations between sequential layers. Detailed studies about the GRESNET framework for many existing GNNs, including GCN, GAT and LOOPYNET, will be reported in the paper with extensive empirical experiments on real-world benchmark datasets.", "pdf": "/pdf/eb260039993ec90976aaef54278ffb7f24011343.pdf", "code": "https://github.com/anonymous-sourcecode/GResNet", "paperhash": "zhang|gresnet_graph_residual_network_for_reviving_deep_gnns_from_suspended_animation", "original_pdf": "/attachment/e1556ba32a7089915ca0fa343161848eee1329c2.pdf", "_bibtex": "@misc{\nzhang2020gresnet,\ntitle={{\\{}GR{\\}}esNet: Graph Residual Network for Reviving Deep {\\{}GNN{\\}}s from Suspended Animation},\nauthor={Jiawei Zhang and Lin Meng},\nyear={2020},\nurl={https://openreview.net/forum?id=rygHq6EFwr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rygHq6EFwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper705/Authors", "ICLR.cc/2020/Conference/Paper705/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper705/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper705/Reviewers", "ICLR.cc/2020/Conference/Paper705/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper705/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper705/Authors|ICLR.cc/2020/Conference/Paper705/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504167470, "tmdate": 1576860545592, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper705/Authors", "ICLR.cc/2020/Conference/Paper705/Reviewers", "ICLR.cc/2020/Conference/Paper705/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper705/-/Official_Comment"}}}, {"id": "SylLxFsojr", "original": null, "number": 4, "cdate": 1573791982187, "ddate": null, "tcdate": 1573791982187, "tmdate": 1573853946385, "tddate": null, "forum": "rygHq6EFwr", "replyto": "HygOfsbjtH", "invitation": "ICLR.cc/2020/Conference/Paper705/-/Official_Comment", "content": {"title": "Response to Review #1 (Part 1)", "comment": "First of all, we would like to appreciate the reviewer for the constructive suggestions. There is a space limit for our response textual input, so we will split our response into two parts. Here is the part 1, and part 2 will be posted as follows.\n\n******************\nQuestion 1: Concerns with the assumption in Section 3 and 4.\n\nResponse: Firstly, we agree with the reviewer that we may need to involve the non-linear layers into the model performance analysis if we can, since the non-linear layers tend to affect the suspended animation problem as well. The impacts come from multiple perspectives, one of the impact is caused by the gradient vanishing/exploding problem in learning the model with multiple non-linear layers. Also the non-linear layers will project the node representations to random states other than the stationary representations. \n\nHowever, we also hope the reviewer can understand the reason that we make that assumption. As we all know, analysis with the multiple non-linear layers is a difficult task, the community still have great challenges in interpreting and explaining the learning process of deep learning models by this context so far. Therefore, we propose to simplify the analysis settings based on the assumption mentioned in the reviewer's comments. We would like to clarify that our assumption also makes sense actually in certain perspectives illustrated as follows.\n\n(1) Non-linear layer will not revive the suspended animated models. We clarify that if a model suffers from the suspended animation problem, the non-linear layer will not change the suspended limitation problem. As clarified in the second paragraph of Sec 3.2 on page 4, the parameter W is actually shared for all the nodes in the GCN model. For any two nodes which obtain the identical representations (i.e., suffering from the suspended animation happens as stated in Theorem 1), their representations obtained via the non-linear layer will still be the same (i.e., the projected representations will still have the suspended animation problem).\n\n(2) Analysis feasibility. Involving the non-linear layers in the model analysis will make the graph neural network model performance and the animation limit bound introduced in Sec 4 extremely hard (or even impossible) to study. It will still due to the black-box property of the deep models with multiple non-linear projections. \n\n(3) Markov-Chain layer is our main focus. We also have experimental studies on node classification with the same dataset by using MLP as the model, which will not suffer from the suspended animation problem at all. Meanwhile, compared with MLP, the main differences between GCN and MLP is due to the Markov-Chain layers involved in the model. Therefore, in this paper, we propose to be focused on studying the impact of the Markov-Chain layer on the model learning performance. \n\n******************\n\nQuestion 2: Originality and existing related works.\n\nResponse: We clarify that the contributions of this paper involve four main parts: (1) identify a problem with existing graph neural network models; (2) study the causes and explain the problem; and (3) propose a tentative solution to resolve the problem; and (4) analyze the feasibility of the proposed solution.\n\nWe just check the referred two papers mentioned by the reviewer, and we agree that the methods proposed in these two papers are very close to the solution proposed in our paper. We clarify that the main contributions of this paper lie in all these four parts mentioned above actually, not just the proposed solution (i.e., part (3) on the tentative solution).\n\nWe also appreciate the reviewer for pointing out the relevant literatures [1][2], and we have cited them properly in the updated PDF uploaded above (these two papers are mentioned on page 8, the paragraph below Figure 3 in Sec 5.2). \n\n[1] X. Wang and A. Gupta. Videos as Space-Time Region Graphs. ECCV 2018.\n[2] N. Wang, Y. Zhang, Z. Li , Y. Fu, W, Liu, Y. Jiang. Pixel2Mesh: Generating 3D Mesh Models from Single RGB Images. ECCV 2018."}, "signatures": ["ICLR.cc/2020/Conference/Paper705/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper705/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "GResNet: Graph Residual Network for Reviving Deep GNNs from Suspended Animation", "authors": ["Jiawei Zhang", "Lin Meng"], "authorids": ["jiawei@ifmlab.org", "lin@ifmlab.org"], "keywords": ["Graph Neural Networks", "Node Classification", "Representation Learning"], "TL;DR": "Identifying suspended animation problem with GNNs, propose a new model to resolve the problem with graph residual learning.", "abstract": "The existing graph neural networks (GNNs) based on the spectral graph convolutional operator have been criticized for its performance degradation, which is especially common for the models with deep architectures. In this paper, we further identify the suspended animation problem with the existing GNNs. Such a problem happens when the model depth reaches the suspended animation limit, and the model will not respond to the training data any more and become not learnable. Analysis about the causes of the suspended animation problem with existing GNNs will be provided in this paper, whereas several other peripheral factors that will impact the problem will be reported as well. To resolve the problem, we introduce the GRESNET (Graph Residual Network) framework in this paper, which creates extensively connected highways to involve nodes\u2019 raw features or intermediate representations throughout the graph for all the model layers. Different from the other learning settings, the extensive connections in the graph data will render the existing simple residual learning methods fail to work. We prove the effectiveness of the introduced new graph residual terms from the norm preservation perspective, which will help avoid dramatic changes to the node\u2019s representations between sequential layers. Detailed studies about the GRESNET framework for many existing GNNs, including GCN, GAT and LOOPYNET, will be reported in the paper with extensive empirical experiments on real-world benchmark datasets.", "pdf": "/pdf/eb260039993ec90976aaef54278ffb7f24011343.pdf", "code": "https://github.com/anonymous-sourcecode/GResNet", "paperhash": "zhang|gresnet_graph_residual_network_for_reviving_deep_gnns_from_suspended_animation", "original_pdf": "/attachment/e1556ba32a7089915ca0fa343161848eee1329c2.pdf", "_bibtex": "@misc{\nzhang2020gresnet,\ntitle={{\\{}GR{\\}}esNet: Graph Residual Network for Reviving Deep {\\{}GNN{\\}}s from Suspended Animation},\nauthor={Jiawei Zhang and Lin Meng},\nyear={2020},\nurl={https://openreview.net/forum?id=rygHq6EFwr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rygHq6EFwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper705/Authors", "ICLR.cc/2020/Conference/Paper705/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper705/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper705/Reviewers", "ICLR.cc/2020/Conference/Paper705/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper705/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper705/Authors|ICLR.cc/2020/Conference/Paper705/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504167470, "tmdate": 1576860545592, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper705/Authors", "ICLR.cc/2020/Conference/Paper705/Reviewers", "ICLR.cc/2020/Conference/Paper705/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper705/-/Official_Comment"}}}, {"id": "BJevIIooiS", "original": null, "number": 2, "cdate": 1573791311329, "ddate": null, "tcdate": 1573791311329, "tmdate": 1573853823003, "tddate": null, "forum": "rygHq6EFwr", "replyto": "B1evBVMx5r", "invitation": "ICLR.cc/2020/Conference/Paper705/-/Official_Comment", "content": {"title": "Response to Reviewer #2", "comment": "First of all, we would like to appreciate the reviewer for the support and the constructive suggestions.\n\n******************\nQuestion 1: Empirical experiments on more diverse datasets other than Cora, Citeseer and Pubmed. \n\nWe clarify that Cora, Citeseer and Pubmed are the top three commonly used benchmark datasets in graph neural network studies, e.g., [1][2][3][4][5][6][7][8] etc. For comparison fairness with the existing works, we only show the experimental results on these three datasets in this paper. \n\nWe do have also witnessed several other datasets used in the recent graph neural network papers, but majority of them are still the variants of Cora, Citeseer and Pubmed (by graph sampling or including more labeled data instances). The remaining ones are either private or rarely used by the existing papers. The reviewers can also refer to the webpage for more information about the major datasets studied by the community on graph neural network studies.\n\nhttps://paperswithcode.com/task/node-classification \n\nDuring the rebuttal period, we tried to get the experimental results of GResNet on the PPI and Reddit datasets as well, which are two other public benchmark dataset used in some graph neural network works [2][8]. The experimental results are reported in the updated version of the submitted PDF paper. The reviewer can refer to Table 4 in the appendix (on page 15) of the updated PDF submitted above for more information about the experimental results. \n\nWe need to mention that GAT used in GResNet is very slow on large datasets (it is not our problem but the problem with the GAT base model), we cannot get the results of GResNet(GAT) out on Reddit during the rebuttal period. Also since PPI and Reddit are not common used in other graph neural network papers, many of the entries in Table 4 are not provided. We will add these results in the camera-ready version of this paper later.\n\nWe will seek for more public graph benchmark datasets as suggested by the reviewer. It may take longer time than the rebuttal time allows, so we plan to add the comparison experiments in the camera-ready version of this paper as well.\n\n******************\n\nQuestion 2: Inferior experimental results on the Citeseer compared with the existing works.\n\nWe clarify that the GResNet model proposed in this paper differs a lot from the latest graph neural network models, i.e., APPNP, GOCN as reported in Table 3. Instead of introducing new and complex model architectures (e.g., APPNP) or complicated optimization approaches (e.g., GOCN) merely for learning performance improvement, we aim to introduce a general framework, which can work for any base models to revive them from the suspended animation problem instead. So, the objective differences may lead to slightly different experimental performance of the models.\n\nAs pointed out by the reviewer, the learning performance of our model GResNet(GCN) and GResNet(LoopyNet) is slightly inferior to APPNP and GOCN. Besides the main objective differences mentioned above for these different works, it can also be caused by: \n(1) weak base models GCN and LoopyNet used in these two methods, since we observe that GResNet(GAT) can achieve the best performance among all the comparison methods (also better than APPNP and GOCN); \n(2) slightly different learning settings, APPNP allows the model to involve more labeled training data for model learning, which may lead to slightly higher scores. However, we strictly follow the conventional learning setting (labeled data ratio) for our GResNet methods. \n\nThe experimental results provided in Table 3 is just to provide the latest research results obtained by the current papers just for the reviewers' and readers' information. So, the slightly inferior performance of GResNet than APPNP doesn't necessarily indicate that GResNet is not good. The main objective of this paper is still focused on studying the suspended animation problem with deep graph neural network models, not to compare the learning scores.\n\n******************\n\nHope our response has resolved your concerns. If there is any proposed question about this paper not resolved in our response, welcome to let us know and we are happy to discuss more with you.\n\n\nReferences used in the above response:\n[1] GCN: Semi-Supervised Classification with Graph Convolutional Networks\n[2] GAT: Graph Attention Networks\n[3] DGI: Deep Graph Infomax\n[4] GraphStar: Graph Star Net for Generalized Multi-Task Learning\n[5] APPNP: Predict then Propagate: Graph Neural Networks meet Personalized PageRank\n[6] GOCN: Graph Optimized Convolutional Networks\n[7] GraphNAS: GraphNAS: Graph Neural Architecture Search with Reinforcement Learning\n[8] FastGCN: Fast Learning with Graph Convolutional Networks via Importance Sampling\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper705/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper705/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "GResNet: Graph Residual Network for Reviving Deep GNNs from Suspended Animation", "authors": ["Jiawei Zhang", "Lin Meng"], "authorids": ["jiawei@ifmlab.org", "lin@ifmlab.org"], "keywords": ["Graph Neural Networks", "Node Classification", "Representation Learning"], "TL;DR": "Identifying suspended animation problem with GNNs, propose a new model to resolve the problem with graph residual learning.", "abstract": "The existing graph neural networks (GNNs) based on the spectral graph convolutional operator have been criticized for its performance degradation, which is especially common for the models with deep architectures. In this paper, we further identify the suspended animation problem with the existing GNNs. Such a problem happens when the model depth reaches the suspended animation limit, and the model will not respond to the training data any more and become not learnable. Analysis about the causes of the suspended animation problem with existing GNNs will be provided in this paper, whereas several other peripheral factors that will impact the problem will be reported as well. To resolve the problem, we introduce the GRESNET (Graph Residual Network) framework in this paper, which creates extensively connected highways to involve nodes\u2019 raw features or intermediate representations throughout the graph for all the model layers. Different from the other learning settings, the extensive connections in the graph data will render the existing simple residual learning methods fail to work. We prove the effectiveness of the introduced new graph residual terms from the norm preservation perspective, which will help avoid dramatic changes to the node\u2019s representations between sequential layers. Detailed studies about the GRESNET framework for many existing GNNs, including GCN, GAT and LOOPYNET, will be reported in the paper with extensive empirical experiments on real-world benchmark datasets.", "pdf": "/pdf/eb260039993ec90976aaef54278ffb7f24011343.pdf", "code": "https://github.com/anonymous-sourcecode/GResNet", "paperhash": "zhang|gresnet_graph_residual_network_for_reviving_deep_gnns_from_suspended_animation", "original_pdf": "/attachment/e1556ba32a7089915ca0fa343161848eee1329c2.pdf", "_bibtex": "@misc{\nzhang2020gresnet,\ntitle={{\\{}GR{\\}}esNet: Graph Residual Network for Reviving Deep {\\{}GNN{\\}}s from Suspended Animation},\nauthor={Jiawei Zhang and Lin Meng},\nyear={2020},\nurl={https://openreview.net/forum?id=rygHq6EFwr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rygHq6EFwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper705/Authors", "ICLR.cc/2020/Conference/Paper705/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper705/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper705/Reviewers", "ICLR.cc/2020/Conference/Paper705/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper705/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper705/Authors|ICLR.cc/2020/Conference/Paper705/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504167470, "tmdate": 1576860545592, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper705/Authors", "ICLR.cc/2020/Conference/Paper705/Reviewers", "ICLR.cc/2020/Conference/Paper705/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper705/-/Official_Comment"}}}, {"id": "r1gJduBniS", "original": null, "number": 5, "cdate": 1573832807305, "ddate": null, "tcdate": 1573832807305, "tmdate": 1573845728238, "tddate": null, "forum": "rygHq6EFwr", "replyto": "S1lx7puZtB", "invitation": "ICLR.cc/2020/Conference/Paper705/-/Official_Comment", "content": {"title": "Response to Review #3 (Part 3, continued ...)", "comment": "\n******************\nQuestion 6: Experiments on larger graph datasets.\n\nResponse: We clarify that Cora, Citeseer and Pubmed are the top three commonly used benchmark datasets in graph neural network studies, e.g., [1][2][3][4][5][6][7][8] etc. For comparison fairness with the existing works, we only show the experimental results on these three datasets in this paper. \n\nWe do have also witnessed several other datasets used in the recent graph neural network papers, but majority of them are still the variants of Cora, Citeseer and Pubmed (by graph sampling or including more labeled data instances). The remaining ones are either private or rarely used by the existing papers. The reviewers can also refer to the webpage for more information about the major datasets studied by the community on graph neural network studies.\n\nhttps://paperswithcode.com/task/node-classification \n\nDuring the rebuttal period, as suggested by the reviewer, we also tried to get the experimental results of GResNet on the PPI and Reddit datasets as well, which are other public benchmark dataset used in graph neural network works [2][8]. The experimental results are reported in the updated version of the submitted PDF paper. The reviewer can refer to updated Table 4 in the appendix of the paper (on page 15) more information about the experimental results. \n\nWe need to mention that GAT used in GResNet is very slow on large datasets (it is not our problem but the problem with the GAT base model), we cannot get the results of GResNet(GAT) out on Reddit during the rebuttal period. Since PPI and Reddit are not common used in other graph neural network papers, so many of the entries in Table 4 are not provided. We will add these results in the camera-ready version of this paper later.\n\nWe will seek for more public graph benchmark datasets as suggested by the reviewer. It may take longer time than the rebuttal time allows, so we plan to add the comparison experiments in the camera-ready version of this paper instead.\n\n******************\n\nHope our response has resolved your concerns. If there is any proposed question about this paper not resolved in our response, welcome to let us know and we are happy to discuss more with you.\n\n\nReferences used in the above response:\n[1] GCN: Semi-Supervised Classification with Graph Convolutional Networks\n[2] GAT: Graph Attention Networks\n[3] DGI: Deep Graph Infomax\n[4] GraphStar: Graph Star Net for Generalized Multi-Task Learning\n[5] APPNP: Predict then Propagate: Graph Neural Networks meet Personalized PageRank\n[6] GOCN: Graph Optimized Convolutional Networks\n[7] GraphNAS: GraphNAS: Graph Neural Architecture Search with Reinforcement Learning\n[8] FastGCN: Fast Learning with Graph Convolutional Networks via Importance Sampling\n[9] Improving Random Walk Estimation Accuracy with Uniform Restart.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper705/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper705/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "GResNet: Graph Residual Network for Reviving Deep GNNs from Suspended Animation", "authors": ["Jiawei Zhang", "Lin Meng"], "authorids": ["jiawei@ifmlab.org", "lin@ifmlab.org"], "keywords": ["Graph Neural Networks", "Node Classification", "Representation Learning"], "TL;DR": "Identifying suspended animation problem with GNNs, propose a new model to resolve the problem with graph residual learning.", "abstract": "The existing graph neural networks (GNNs) based on the spectral graph convolutional operator have been criticized for its performance degradation, which is especially common for the models with deep architectures. In this paper, we further identify the suspended animation problem with the existing GNNs. Such a problem happens when the model depth reaches the suspended animation limit, and the model will not respond to the training data any more and become not learnable. Analysis about the causes of the suspended animation problem with existing GNNs will be provided in this paper, whereas several other peripheral factors that will impact the problem will be reported as well. To resolve the problem, we introduce the GRESNET (Graph Residual Network) framework in this paper, which creates extensively connected highways to involve nodes\u2019 raw features or intermediate representations throughout the graph for all the model layers. Different from the other learning settings, the extensive connections in the graph data will render the existing simple residual learning methods fail to work. We prove the effectiveness of the introduced new graph residual terms from the norm preservation perspective, which will help avoid dramatic changes to the node\u2019s representations between sequential layers. Detailed studies about the GRESNET framework for many existing GNNs, including GCN, GAT and LOOPYNET, will be reported in the paper with extensive empirical experiments on real-world benchmark datasets.", "pdf": "/pdf/eb260039993ec90976aaef54278ffb7f24011343.pdf", "code": "https://github.com/anonymous-sourcecode/GResNet", "paperhash": "zhang|gresnet_graph_residual_network_for_reviving_deep_gnns_from_suspended_animation", "original_pdf": "/attachment/e1556ba32a7089915ca0fa343161848eee1329c2.pdf", "_bibtex": "@misc{\nzhang2020gresnet,\ntitle={{\\{}GR{\\}}esNet: Graph Residual Network for Reviving Deep {\\{}GNN{\\}}s from Suspended Animation},\nauthor={Jiawei Zhang and Lin Meng},\nyear={2020},\nurl={https://openreview.net/forum?id=rygHq6EFwr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rygHq6EFwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper705/Authors", "ICLR.cc/2020/Conference/Paper705/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper705/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper705/Reviewers", "ICLR.cc/2020/Conference/Paper705/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper705/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper705/Authors|ICLR.cc/2020/Conference/Paper705/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504167470, "tmdate": 1576860545592, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper705/Authors", "ICLR.cc/2020/Conference/Paper705/Reviewers", "ICLR.cc/2020/Conference/Paper705/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper705/-/Official_Comment"}}}, {"id": "rylppdijiB", "original": null, "number": 3, "cdate": 1573791941428, "ddate": null, "tcdate": 1573791941428, "tmdate": 1573832532082, "tddate": null, "forum": "rygHq6EFwr", "replyto": "HygOfsbjtH", "invitation": "ICLR.cc/2020/Conference/Paper705/-/Official_Comment", "content": {"title": "Response to Review #1 (Part 2, Continued...)", "comment": "\n\n******************\nQuestion 3: \u201cmajority of the nodes are of very small degrees\u201d is not justified and only apply to the internet topology in Faloutsos et al. (1999)\n\nResponse: We clarify that node degree \u201cpower-law distribution\u201d is well-known concept in graph studies. It depicts the observation that \u201cmajority of the nodes in a graph will have a small degree, and a small amount of the nodes can have a large degree\u201d. If the reviewer has the Faloutsos et al. (1999) paper available, it is suggested to refer to the Figure 5 and Figure 6 in the paper. These two plots are the representative plots on node degree power-law distribution. It is a log-log plot on node degree (the x axis) and the node number (the y axis). From the plot we can observe that majority of the nodes have a degree less than 10 actually.\n\nSimilar observation has been reported on the bibliographic network data as well in [3] (Cora, Citeseer and Pubmed datasets used in this paper are all bibliographic networks actually). The reviewer can refer to Figure 6 in [3] for more information on the related bibliographic networks. We have also cited this related paper in our updated paper submitted above, and also added necessary words to make this concept clearer in Sec 4.2.\n\n[3] M. E. J. Newman. The structure and function of complex networks\n\n******************\nQuestion 4: I feel the \u201cRaw Feature Coding\u201d and the \u201cNetwork Degree Distribution\u201d are sort of repetitive, and the eq. (11) is eq. (12) at the stationary point.\n\nResponse: We clarify that \u201cRaw Feature Coding\u201d and \u201cNetwork Degree Distribution\u201d are not repetitive and they are totally different factors. Their analyses are also very different actually.\n\nWe want to clarify that Equ (11) used for the \u201cNetwork Degree Distribution\u201d is at the stationary point. However, Equ (12) for \u201cRaw Feature Coding\u201d is based on the raw representation in Equ (3) and Equ (4), which doesn\u2019t require the representations to be at the stationary point.\n\nAs suggested by the reviewer, we have also revised and updated the presentations for these two factors in the updated version of this paper just to avoid unnecessary confusions for the readers. The changes are added just after Equ(12) in the updated PDF of this paper.\n\n******************\n\nHope our response has resolved your concerns. If there is any proposed question about this paper not resolved in our response, welcome to let us know and we are happy to discuss more with you.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper705/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper705/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "GResNet: Graph Residual Network for Reviving Deep GNNs from Suspended Animation", "authors": ["Jiawei Zhang", "Lin Meng"], "authorids": ["jiawei@ifmlab.org", "lin@ifmlab.org"], "keywords": ["Graph Neural Networks", "Node Classification", "Representation Learning"], "TL;DR": "Identifying suspended animation problem with GNNs, propose a new model to resolve the problem with graph residual learning.", "abstract": "The existing graph neural networks (GNNs) based on the spectral graph convolutional operator have been criticized for its performance degradation, which is especially common for the models with deep architectures. In this paper, we further identify the suspended animation problem with the existing GNNs. Such a problem happens when the model depth reaches the suspended animation limit, and the model will not respond to the training data any more and become not learnable. Analysis about the causes of the suspended animation problem with existing GNNs will be provided in this paper, whereas several other peripheral factors that will impact the problem will be reported as well. To resolve the problem, we introduce the GRESNET (Graph Residual Network) framework in this paper, which creates extensively connected highways to involve nodes\u2019 raw features or intermediate representations throughout the graph for all the model layers. Different from the other learning settings, the extensive connections in the graph data will render the existing simple residual learning methods fail to work. We prove the effectiveness of the introduced new graph residual terms from the norm preservation perspective, which will help avoid dramatic changes to the node\u2019s representations between sequential layers. Detailed studies about the GRESNET framework for many existing GNNs, including GCN, GAT and LOOPYNET, will be reported in the paper with extensive empirical experiments on real-world benchmark datasets.", "pdf": "/pdf/eb260039993ec90976aaef54278ffb7f24011343.pdf", "code": "https://github.com/anonymous-sourcecode/GResNet", "paperhash": "zhang|gresnet_graph_residual_network_for_reviving_deep_gnns_from_suspended_animation", "original_pdf": "/attachment/e1556ba32a7089915ca0fa343161848eee1329c2.pdf", "_bibtex": "@misc{\nzhang2020gresnet,\ntitle={{\\{}GR{\\}}esNet: Graph Residual Network for Reviving Deep {\\{}GNN{\\}}s from Suspended Animation},\nauthor={Jiawei Zhang and Lin Meng},\nyear={2020},\nurl={https://openreview.net/forum?id=rygHq6EFwr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rygHq6EFwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper705/Authors", "ICLR.cc/2020/Conference/Paper705/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper705/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper705/Reviewers", "ICLR.cc/2020/Conference/Paper705/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper705/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper705/Authors|ICLR.cc/2020/Conference/Paper705/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504167470, "tmdate": 1576860545592, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper705/Authors", "ICLR.cc/2020/Conference/Paper705/Reviewers", "ICLR.cc/2020/Conference/Paper705/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper705/-/Official_Comment"}}}, {"id": "S1lx7puZtB", "original": null, "number": 1, "cdate": 1571028247799, "ddate": null, "tcdate": 1571028247799, "tmdate": 1572972562575, "tddate": null, "forum": "rygHq6EFwr", "replyto": "rygHq6EFwr", "invitation": "ICLR.cc/2020/Conference/Paper705/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "In this paper, the authors study the problem of adding residual connection to GNN for node classification. The authors first identify the problem referred to as \u201cSuspended Animation\u201d in GNN when the depth increases. Then the authors provide both empirical and theoretical characterization for the behavior. To handle this issue, the authors study several different ways to add residual connections in GNN including the na\u00efve method proposed in Kipf et al. The authors carry out extensive experiments on three datasets on different residual connections for the node classification task.\n\nStrength\n1. The authors identify and study an interesting and important issue in GNN as the \u201cSuspended Animation\u201d issue.\n2. The authors provide both empirical and theoretical analysis for the \u201cSuspended Animation\u201d behavior. Moreover, the authors provide theoretical justification for the added residual connection in GNN as gradient norm bound.\n3. The authors carry out extensive experiments on different variants of residual links. Morover, the authors provide code online for reproducibility.\n\nWeakness:\n1. In the theoretical analysis, the assumption that the FC layer is identical mapping is too simplistic. The analysis differs from the actual model especially when the residual links are considered in equation (8), where we have a sum of FC layer output and residual connection. Actually, the empirical results show that na\u00efve residual links work pretty good on several datasets. It goes against the analysis in Corollary 1.\n2. Though the authors provide bound on the norm of gradient under residual links, it would be better if authors could justify the adding of residual link from the perspective of Theorem 1.\n3. The authors study the depth of GNN up to 7 layers at most. It would be interesting to see how the model performs under really deep networks.\n4. The authors mention several factors to affect GNN in section 4.2. It would be interesting to see how these factors like training data set and feature coding interacts with different ways of adding residual connections.\n5. It is very informative that the authors compare their methods on the widely used three datasets. It would be better if the authors could carry out experiment on larger graphs to verify the empirical observations. For example, do we need to have deeper networks for larger graphs?\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper705/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper705/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "GResNet: Graph Residual Network for Reviving Deep GNNs from Suspended Animation", "authors": ["Jiawei Zhang", "Lin Meng"], "authorids": ["jiawei@ifmlab.org", "lin@ifmlab.org"], "keywords": ["Graph Neural Networks", "Node Classification", "Representation Learning"], "TL;DR": "Identifying suspended animation problem with GNNs, propose a new model to resolve the problem with graph residual learning.", "abstract": "The existing graph neural networks (GNNs) based on the spectral graph convolutional operator have been criticized for its performance degradation, which is especially common for the models with deep architectures. In this paper, we further identify the suspended animation problem with the existing GNNs. Such a problem happens when the model depth reaches the suspended animation limit, and the model will not respond to the training data any more and become not learnable. Analysis about the causes of the suspended animation problem with existing GNNs will be provided in this paper, whereas several other peripheral factors that will impact the problem will be reported as well. To resolve the problem, we introduce the GRESNET (Graph Residual Network) framework in this paper, which creates extensively connected highways to involve nodes\u2019 raw features or intermediate representations throughout the graph for all the model layers. Different from the other learning settings, the extensive connections in the graph data will render the existing simple residual learning methods fail to work. We prove the effectiveness of the introduced new graph residual terms from the norm preservation perspective, which will help avoid dramatic changes to the node\u2019s representations between sequential layers. Detailed studies about the GRESNET framework for many existing GNNs, including GCN, GAT and LOOPYNET, will be reported in the paper with extensive empirical experiments on real-world benchmark datasets.", "pdf": "/pdf/eb260039993ec90976aaef54278ffb7f24011343.pdf", "code": "https://github.com/anonymous-sourcecode/GResNet", "paperhash": "zhang|gresnet_graph_residual_network_for_reviving_deep_gnns_from_suspended_animation", "original_pdf": "/attachment/e1556ba32a7089915ca0fa343161848eee1329c2.pdf", "_bibtex": "@misc{\nzhang2020gresnet,\ntitle={{\\{}GR{\\}}esNet: Graph Residual Network for Reviving Deep {\\{}GNN{\\}}s from Suspended Animation},\nauthor={Jiawei Zhang and Lin Meng},\nyear={2020},\nurl={https://openreview.net/forum?id=rygHq6EFwr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rygHq6EFwr", "replyto": "rygHq6EFwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper705/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper705/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575704699532, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper705/Reviewers"], "noninvitees": [], "tcdate": 1570237748280, "tmdate": 1575704699548, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper705/-/Official_Review"}}}, {"id": "HygOfsbjtH", "original": null, "number": 2, "cdate": 1571654415911, "ddate": null, "tcdate": 1571654415911, "tmdate": 1572972562540, "tddate": null, "forum": "rygHq6EFwr", "replyto": "rygHq6EFwr", "invitation": "ICLR.cc/2020/Conference/Paper705/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "Summary:\n\nThis paper studies the \u201csuspended animation limit\u201d of various GNNs \u2013 an important one for how to train a good Graph network. The authors provide sufficient analysis by simplify GNNs as a series of 1-step Markov chains (which is my concern as stated in the section on main issues), while pointing out the limitations quantitatively as in the Theorem 2. Under the assumption, the authors propose several new forms of ResNets for GCNs, which can successfully overcome the limitation.\n\nOverall, the motivation of this work is clear and meaninfgful. The proposed residual architecture is effective, and the presentation is clear and easy to understand. \n\nHowever, my main concerns are on the initial assumptions for analyzing the suspension of GNNs. See the following comments.\n\nThis paper is generally well written and easy to understand. The organization of each part is well-balanced.\n\nOriginality:\n\nTo the best of my knowledge, numerous methods (i.e., targeting on applications) address this problem by augmenting A [1] or X [2] with similarity of feature representation learned from other sources. However, this paper specifically analyzes the problem in a principle way. I consider this work is generally novel.\n\n[1] X. Wang and A. Gupta. Videos as Space-Time Region Graphs. ECCV 2018.\n[2] N. Wang, Y. Zhang, Z. Li , Y. Fu, W, Liu, Y. Jiang. Pixel2Mesh: Generating 3D Mesh Models from Single RGB Images. ECCV 2018.\n\nSignificance:\n\nThe significance lies mostly in motivation and the proposed GResNet.\n\nMotivation: This paper studies the phenomenal that GNNs tends to not respond to the input data when certain depth of a network is reached, which the authors called as suspended animation limit. Studying problem is fundamental and important, and also unique since different with CNNs where data are independent, the data instance within GNNs are highly correlated.\n\nGResNet: Given the differences, and within the Theorem 2 where the residual formulation of CNNs does not apply to GNNs, the paper also proposes several new formulations, i.e., in figure 2, where the suspension is avoidable and the performance under the same experimental settings is obviously boosted. \n\nMain issues: \nMy major concern to this work lies in the assumption used throughout section 3 and 4. At the beginning of Section 3.2, the authors assume that W is identity. Since X is assumed to be column-wise normalized, the nonlinearity is removable. However, this is not true in real cases: W is actually learnable and not bounded. When W is learned to be negative, Relu layer is not removable, and the behavior of the network will be completely different with what the paper depicted. Indeed, GNNs contain stacked linear+nonlinear functions, which cannot be simplified as a linear Markov chain. It is analogy to CNNs, which is not possibly be simplified as a group of average poolings.\n\nMinor issues:\n1. I agree that under the assumption, eq. (11) shows that the differences between the learned representations are not discriminative, however the claim \u201cmajority of the nodes are of very small degrees\u201d is not justified and only apply to the internet topology in Faloutsos et al. (1999).\n\n2. I feel the \u201cRaw Feature Coding\u201d and the \u201cNetwork Degree Distribution\u201d are sort of repetitive, and the eq. (11) is eq. (12) at the stationary point.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper705/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper705/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "GResNet: Graph Residual Network for Reviving Deep GNNs from Suspended Animation", "authors": ["Jiawei Zhang", "Lin Meng"], "authorids": ["jiawei@ifmlab.org", "lin@ifmlab.org"], "keywords": ["Graph Neural Networks", "Node Classification", "Representation Learning"], "TL;DR": "Identifying suspended animation problem with GNNs, propose a new model to resolve the problem with graph residual learning.", "abstract": "The existing graph neural networks (GNNs) based on the spectral graph convolutional operator have been criticized for its performance degradation, which is especially common for the models with deep architectures. In this paper, we further identify the suspended animation problem with the existing GNNs. Such a problem happens when the model depth reaches the suspended animation limit, and the model will not respond to the training data any more and become not learnable. Analysis about the causes of the suspended animation problem with existing GNNs will be provided in this paper, whereas several other peripheral factors that will impact the problem will be reported as well. To resolve the problem, we introduce the GRESNET (Graph Residual Network) framework in this paper, which creates extensively connected highways to involve nodes\u2019 raw features or intermediate representations throughout the graph for all the model layers. Different from the other learning settings, the extensive connections in the graph data will render the existing simple residual learning methods fail to work. We prove the effectiveness of the introduced new graph residual terms from the norm preservation perspective, which will help avoid dramatic changes to the node\u2019s representations between sequential layers. Detailed studies about the GRESNET framework for many existing GNNs, including GCN, GAT and LOOPYNET, will be reported in the paper with extensive empirical experiments on real-world benchmark datasets.", "pdf": "/pdf/eb260039993ec90976aaef54278ffb7f24011343.pdf", "code": "https://github.com/anonymous-sourcecode/GResNet", "paperhash": "zhang|gresnet_graph_residual_network_for_reviving_deep_gnns_from_suspended_animation", "original_pdf": "/attachment/e1556ba32a7089915ca0fa343161848eee1329c2.pdf", "_bibtex": "@misc{\nzhang2020gresnet,\ntitle={{\\{}GR{\\}}esNet: Graph Residual Network for Reviving Deep {\\{}GNN{\\}}s from Suspended Animation},\nauthor={Jiawei Zhang and Lin Meng},\nyear={2020},\nurl={https://openreview.net/forum?id=rygHq6EFwr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rygHq6EFwr", "replyto": "rygHq6EFwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper705/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper705/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575704699532, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper705/Reviewers"], "noninvitees": [], "tcdate": 1570237748280, "tmdate": 1575704699548, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper705/-/Official_Review"}}}], "count": 12}