{"notes": [{"id": "S1xqRTNtDr", "original": "HkgHOCZdDB", "number": 866, "cdate": 1569439186028, "ddate": null, "tcdate": 1569439186028, "tmdate": 1577168256510, "tddate": null, "forum": "S1xqRTNtDr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Learning a Behavioral Repertoire from Demonstrations", "authors": ["Niels Justesen", "Miguel Gonz\u00e1lez Duque", "Daniel Cabarcas Jaramillo", "Jean-Baptiste Mouret", "Sebastian Risi"], "authorids": ["noju@itu.edu", "migonzalez@unal.edu.co", "dcarbarc@unal.edu.co", "jean-baptiste.mouret@inria.fr", "sebr@itu.dk"], "keywords": ["Behavioral Repertoires", "Imitation Learning", "Deep Learning", "Adaptation", "StarCraft 2"], "TL;DR": "BRIL allows a single neural network to learn a repertoire of behaviors from a set of demonstrations that can be precisely modulated.", "abstract": "Imitation Learning (IL) is a machine learning approach to learn a policy from a set of demonstrations. IL can be useful to kick-start learning before applying reinforcement learning (RL) but it can also be useful on its own, e.g. to learn to imitate human players in video games. However, a major limitation of current IL approaches is that they learn only a single ``\"average\" policy based on a dataset that possibly contains demonstrations of numerous different types of behaviors. In this paper, we present a new approach called Behavioral Repertoire Imitation Learning (BRIL) that instead learns a repertoire of behaviors from a set of demonstrations by augmenting the state-action pairs with behavioral descriptions. The outcome of this approach is a single neural network policy conditioned on a behavior description that can be precisely modulated. We apply this approach to train a policy on 7,777 human demonstrations for the build-order planning task in StarCraft II. Dimensionality reduction techniques are applied to construct a low-dimensional behavioral space from the high-dimensional army unit composition of each demonstration. The results demonstrate that the learned policy can be effectively manipulated to express distinct behaviors. Additionally, by applying the UCB1 algorithm, the policy can adapt its behavior -in-between games- to reach a performance beyond that of the traditional IL baseline approach.", "pdf": "/pdf/0ef3206a531452869fde3dda234d09a4517fcc22.pdf", "paperhash": "justesen|learning_a_behavioral_repertoire_from_demonstrations", "original_pdf": "/attachment/0ef3206a531452869fde3dda234d09a4517fcc22.pdf", "_bibtex": "@misc{\njustesen2020learning,\ntitle={Learning a Behavioral Repertoire from Demonstrations},\nauthor={Niels Justesen and Miguel Gonz{\\'a}lez Duque and Daniel Cabarcas Jaramillo and Jean-Baptiste Mouret and Sebastian Risi},\nyear={2020},\nurl={https://openreview.net/forum?id=S1xqRTNtDr}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "LtGgnFsDTH", "original": null, "number": 1, "cdate": 1576798708187, "ddate": null, "tcdate": 1576798708187, "tmdate": 1576800928172, "tddate": null, "forum": "S1xqRTNtDr", "replyto": "S1xqRTNtDr", "invitation": "ICLR.cc/2020/Conference/Paper866/-/Decision", "content": {"decision": "Reject", "comment": "This paper proposes a way to lean context-dependent policies from demonstrations, where the context represents behavior labels obtained by annotating demonstrations with differences in behavior across dimensions and the reduced in 2 dimensions. Results are conducted in the domain of StarCraft. The main concerns from the reviewers related to the paper\u2019s novelty (as pointed by R2) and experiments (particularly the lack of comparison with other methods and the evaluation of only 4 out of the 62 behaviour clusters, as pointed by R3). As such, I cannot recommend acceptance, as current results do not provide strong empirical evidence about the superiority of the method against other alternatives.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning a Behavioral Repertoire from Demonstrations", "authors": ["Niels Justesen", "Miguel Gonz\u00e1lez Duque", "Daniel Cabarcas Jaramillo", "Jean-Baptiste Mouret", "Sebastian Risi"], "authorids": ["noju@itu.edu", "migonzalez@unal.edu.co", "dcarbarc@unal.edu.co", "jean-baptiste.mouret@inria.fr", "sebr@itu.dk"], "keywords": ["Behavioral Repertoires", "Imitation Learning", "Deep Learning", "Adaptation", "StarCraft 2"], "TL;DR": "BRIL allows a single neural network to learn a repertoire of behaviors from a set of demonstrations that can be precisely modulated.", "abstract": "Imitation Learning (IL) is a machine learning approach to learn a policy from a set of demonstrations. IL can be useful to kick-start learning before applying reinforcement learning (RL) but it can also be useful on its own, e.g. to learn to imitate human players in video games. However, a major limitation of current IL approaches is that they learn only a single ``\"average\" policy based on a dataset that possibly contains demonstrations of numerous different types of behaviors. In this paper, we present a new approach called Behavioral Repertoire Imitation Learning (BRIL) that instead learns a repertoire of behaviors from a set of demonstrations by augmenting the state-action pairs with behavioral descriptions. The outcome of this approach is a single neural network policy conditioned on a behavior description that can be precisely modulated. We apply this approach to train a policy on 7,777 human demonstrations for the build-order planning task in StarCraft II. Dimensionality reduction techniques are applied to construct a low-dimensional behavioral space from the high-dimensional army unit composition of each demonstration. The results demonstrate that the learned policy can be effectively manipulated to express distinct behaviors. Additionally, by applying the UCB1 algorithm, the policy can adapt its behavior -in-between games- to reach a performance beyond that of the traditional IL baseline approach.", "pdf": "/pdf/0ef3206a531452869fde3dda234d09a4517fcc22.pdf", "paperhash": "justesen|learning_a_behavioral_repertoire_from_demonstrations", "original_pdf": "/attachment/0ef3206a531452869fde3dda234d09a4517fcc22.pdf", "_bibtex": "@misc{\njustesen2020learning,\ntitle={Learning a Behavioral Repertoire from Demonstrations},\nauthor={Niels Justesen and Miguel Gonz{\\'a}lez Duque and Daniel Cabarcas Jaramillo and Jean-Baptiste Mouret and Sebastian Risi},\nyear={2020},\nurl={https://openreview.net/forum?id=S1xqRTNtDr}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "S1xqRTNtDr", "replyto": "S1xqRTNtDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795728288, "tmdate": 1576800280672, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper866/-/Decision"}}}, {"id": "H1eOuKlwKB", "original": null, "number": 1, "cdate": 1571387760333, "ddate": null, "tcdate": 1571387760333, "tmdate": 1572972542532, "tddate": null, "forum": "S1xqRTNtDr", "replyto": "S1xqRTNtDr", "invitation": "ICLR.cc/2020/Conference/Paper866/-/Official_Review", "content": {"rating": "1: Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "Summary: \nThe paper proposes behavioral repertoire imitation learning (BRIL) which aims to learn a collection of policy from diverse demonstrations. BRIL learns such a collection by learning a context-dependent policy, where the context variable represents behavior of each demonstration. To obtain a context variable, BRIL rely on user\u2019s knowledge, where the user manually defines a feature space that describes behavior. This feature space is then reduced by using a dimensionality reduction method such as t-SNE. Lastly, the policy is learned by supervised learning (behavior cloning) with a state-context input variable and an action output variable. The method is experimentally evaluated on the StarCraft environment. The results show that BRIL performs better than two baselines: behavior cloning on diverse demonstrations and behavior cloning on clustered demonstrations. \n\nScore: \nThe weaknesses of the paper are novelty, clarity, and evaluation. Please see the detailed comments below. I vote for rejection. \n\nComments:\n- Novelty of the proposed idea. \nThe major issue of the paper is the lack of novelty. The idea of learning a context-dependent policy in BRIL closely resembles that of existing multi-modal IL methods (Wang et al., 2017, Li et al., 2017). The main difference is that, BRIL relies on a manually defined context variable (behavioral feature space). In contrast, the existing methods aim to learn the context variable from demonstrations. BRIL is too simple when compared to the existing methods. Moreover, using a manually specified feature space does not go well with the main principle of deep learning, which is to learn informative feature spaces from data end-to-end. I think that ICLR is not a suitable venue for this paper. \n\n- Clarity of the proposed method.\nThe second issue of the paper is clarity. Specifically, two important steps of BRIL is policy learning by supervised learning (behavior cloning) and dimensionality reduction by t-SNE. However, explanations of these two steps are vague and incomplete. For example, in Section 2.1, the paper describes IL as supervised learning, but does not mention the issue of covariate shift, which is well-known when treating IL as supervised learning (Ross et al., 2011). Also, it is incorrect to state that an IL agent cannot interact with the environment during training, since many IL methods such as GAIL require interactions with the environment during training. Meanwhile, in Section 2.4, it is unclear how probability distributions in t-SNE reflect similarity between data points. \n\n[1] St\u00e9phane Ross, Geoffrey Gordon, and Drew Bagnell. A reduction of imitation learning and structured prediction to no-regret online learning. AISTATS, 2011.\n\n- Evaluation of the proposed method is too narrow.\nThe paper lacks important baseline methods in the experiment. Specifically, the paper does not compare BRIL against multi-modal IL methods (Wang et al., 2017, Li et al., 2017) which also learn a context-dependent policy. Moreover, BRIL is evaluated only on the StarCraft environment with only one kind of manually specified feature. This raises a question of generality and sensitivity against the choice of feature of BRIL. To improve the paper, I suggest the authors to compared the proposed method against multi-modal IL methods on different environment, and evaluate BRIL with different choices of the behavioral features.\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper866/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper866/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning a Behavioral Repertoire from Demonstrations", "authors": ["Niels Justesen", "Miguel Gonz\u00e1lez Duque", "Daniel Cabarcas Jaramillo", "Jean-Baptiste Mouret", "Sebastian Risi"], "authorids": ["noju@itu.edu", "migonzalez@unal.edu.co", "dcarbarc@unal.edu.co", "jean-baptiste.mouret@inria.fr", "sebr@itu.dk"], "keywords": ["Behavioral Repertoires", "Imitation Learning", "Deep Learning", "Adaptation", "StarCraft 2"], "TL;DR": "BRIL allows a single neural network to learn a repertoire of behaviors from a set of demonstrations that can be precisely modulated.", "abstract": "Imitation Learning (IL) is a machine learning approach to learn a policy from a set of demonstrations. IL can be useful to kick-start learning before applying reinforcement learning (RL) but it can also be useful on its own, e.g. to learn to imitate human players in video games. However, a major limitation of current IL approaches is that they learn only a single ``\"average\" policy based on a dataset that possibly contains demonstrations of numerous different types of behaviors. In this paper, we present a new approach called Behavioral Repertoire Imitation Learning (BRIL) that instead learns a repertoire of behaviors from a set of demonstrations by augmenting the state-action pairs with behavioral descriptions. The outcome of this approach is a single neural network policy conditioned on a behavior description that can be precisely modulated. We apply this approach to train a policy on 7,777 human demonstrations for the build-order planning task in StarCraft II. Dimensionality reduction techniques are applied to construct a low-dimensional behavioral space from the high-dimensional army unit composition of each demonstration. The results demonstrate that the learned policy can be effectively manipulated to express distinct behaviors. Additionally, by applying the UCB1 algorithm, the policy can adapt its behavior -in-between games- to reach a performance beyond that of the traditional IL baseline approach.", "pdf": "/pdf/0ef3206a531452869fde3dda234d09a4517fcc22.pdf", "paperhash": "justesen|learning_a_behavioral_repertoire_from_demonstrations", "original_pdf": "/attachment/0ef3206a531452869fde3dda234d09a4517fcc22.pdf", "_bibtex": "@misc{\njustesen2020learning,\ntitle={Learning a Behavioral Repertoire from Demonstrations},\nauthor={Niels Justesen and Miguel Gonz{\\'a}lez Duque and Daniel Cabarcas Jaramillo and Jean-Baptiste Mouret and Sebastian Risi},\nyear={2020},\nurl={https://openreview.net/forum?id=S1xqRTNtDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "S1xqRTNtDr", "replyto": "S1xqRTNtDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper866/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper866/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575821683281, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper866/Reviewers"], "noninvitees": [], "tcdate": 1570237745843, "tmdate": 1575821683297, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper866/-/Official_Review"}}}, {"id": "BJeFAQ3Ttr", "original": null, "number": 2, "cdate": 1571828689352, "ddate": null, "tcdate": 1571828689352, "tmdate": 1572972542496, "tddate": null, "forum": "S1xqRTNtDr", "replyto": "S1xqRTNtDr", "invitation": "ICLR.cc/2020/Conference/Paper866/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This work examines the problem of using training a policy which can emulate a variety of different strategies based on a set of demonstrations representing this space of strategies.  The proposed method, BRIL, computes a feature vector for each demonstration, and then employs a dimensionality reduction technique to map the demonstrations to a latent space of strategies.  BRIL then preforms behavioral cloning on these demonstrations, with the reduced representation of the current strategy as an additional input to the policy model.  Empirical evaluation of BRIL is conducted in StarCraft II, where the agent is tasked with scheduling the construction of different units (other aspects of play are controlled by built-in AI).  Results show that when conditioned on good strategies, the BRIL model is superior to a base imitation learning model trained without strategy information.\n\nThe best way to view this work is as a method for learning goal conditioned polices from demonstrations.  While the raw data does not distinguish between different strategies, a postprocessing phase generates task description vectors with specific semantics (the ratios of different unit types built) which are given as input to the BRIL model in addition to the current state.  Therefore, rather than identifying the latent space of strategies present in the demonstration data, BIRL learns a policy which is parameterized by an externally defined space of target behaviors.  The dimensionality reduction step in the strategy space does recover some latent structure, but this is still with respect to a task-specific space of strategy annotations, rather than the demonstrated behaviors themselves.  \n\nWhile this is not an issue with BRIL itself, it should be made clear in the paper that BRIL is learning policies conditioned on explicit strategy descriptions, so that the work can be properly positioned in the literature.  Neither the theoretical discussion nor the empirical results compare BRIL against existing work on learning goal-conditioned policies.\n\nThe empirical results comparing BRIL against a base IL agent are interesting, however, in that BRIL, when conditioned on the best strategy (building mostly marines), actually outperforms IL trained solely on episodes which followed this strategy.  This suggest the possibility that BRIL was able to transfer knowledge from episodes demonstrating other strategies to make up for the limited information available in the demonstrations of the target behavior.\n\nIn section 4.4 the paper briefly discusses the idea of evaluating different strategies executed by the BRIL model, and selecting those that perform best, as a means of learning strong policies for the StarCraft task.  The use of human demonstrations to improve the performance of learning algorithms on difficult control tasks is a widely studied problem, and optimization.  This idea is not explored in any great detail however, and no comparisons with existing methods combining IL with RL are conducted, so the value of BRIL in that context is unclear.  I would, however, recommend this as direction for future work with BRIL."}, "signatures": ["ICLR.cc/2020/Conference/Paper866/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper866/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning a Behavioral Repertoire from Demonstrations", "authors": ["Niels Justesen", "Miguel Gonz\u00e1lez Duque", "Daniel Cabarcas Jaramillo", "Jean-Baptiste Mouret", "Sebastian Risi"], "authorids": ["noju@itu.edu", "migonzalez@unal.edu.co", "dcarbarc@unal.edu.co", "jean-baptiste.mouret@inria.fr", "sebr@itu.dk"], "keywords": ["Behavioral Repertoires", "Imitation Learning", "Deep Learning", "Adaptation", "StarCraft 2"], "TL;DR": "BRIL allows a single neural network to learn a repertoire of behaviors from a set of demonstrations that can be precisely modulated.", "abstract": "Imitation Learning (IL) is a machine learning approach to learn a policy from a set of demonstrations. IL can be useful to kick-start learning before applying reinforcement learning (RL) but it can also be useful on its own, e.g. to learn to imitate human players in video games. However, a major limitation of current IL approaches is that they learn only a single ``\"average\" policy based on a dataset that possibly contains demonstrations of numerous different types of behaviors. In this paper, we present a new approach called Behavioral Repertoire Imitation Learning (BRIL) that instead learns a repertoire of behaviors from a set of demonstrations by augmenting the state-action pairs with behavioral descriptions. The outcome of this approach is a single neural network policy conditioned on a behavior description that can be precisely modulated. We apply this approach to train a policy on 7,777 human demonstrations for the build-order planning task in StarCraft II. Dimensionality reduction techniques are applied to construct a low-dimensional behavioral space from the high-dimensional army unit composition of each demonstration. The results demonstrate that the learned policy can be effectively manipulated to express distinct behaviors. Additionally, by applying the UCB1 algorithm, the policy can adapt its behavior -in-between games- to reach a performance beyond that of the traditional IL baseline approach.", "pdf": "/pdf/0ef3206a531452869fde3dda234d09a4517fcc22.pdf", "paperhash": "justesen|learning_a_behavioral_repertoire_from_demonstrations", "original_pdf": "/attachment/0ef3206a531452869fde3dda234d09a4517fcc22.pdf", "_bibtex": "@misc{\njustesen2020learning,\ntitle={Learning a Behavioral Repertoire from Demonstrations},\nauthor={Niels Justesen and Miguel Gonz{\\'a}lez Duque and Daniel Cabarcas Jaramillo and Jean-Baptiste Mouret and Sebastian Risi},\nyear={2020},\nurl={https://openreview.net/forum?id=S1xqRTNtDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "S1xqRTNtDr", "replyto": "S1xqRTNtDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper866/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper866/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575821683281, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper866/Reviewers"], "noninvitees": [], "tcdate": 1570237745843, "tmdate": 1575821683297, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper866/-/Official_Review"}}}, {"id": "rJg75VLZ9B", "original": null, "number": 3, "cdate": 1572066442691, "ddate": null, "tcdate": 1572066442691, "tmdate": 1572972542453, "tddate": null, "forum": "S1xqRTNtDr", "replyto": "S1xqRTNtDr", "invitation": "ICLR.cc/2020/Conference/Paper866/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper presents Behavioral Repertoire Imitation Learning (BRIL) which is a way to learn a policy via imitation learning that can be modulated with different behavior inputs that adjust the policy's behavior.\u00a0 Demonstrations used in training are labeled with differences in behavior across dimensions (which are then reduced to two dimensions using t-SNE), and then these behavior labels are provided as additional input when training a NN from demonstrations using behavior cloning.\u00a0 Experimental results are shown for learning a BRIL policy from 7000+ demonstrations of humans playing StarCraft, and are compared to that of learning a single behavior cloned policy trained on all demonstrations as well as behavior cloned policies trained on subsets of demonstrations clustered by their behavior. \n\nOne of the authors' claims is that BRIL is able to learn a policy that can have a wide variety of behaviors based on the behavior input given as input to the policy which is backed by their results.  They also claim that the BRIL model can be tuned to have higher performance than a policy learned by traditional IL which is shown by their results, although I think that may be somewhat of a function of the environment and set of demonstrations.\n\nAnother claim by the authors in section 4.3 is that behavior can be successfully controlled, and they say \"for both BRIL and IL on behavioral clusters, the average expressed behavior is closest to the cluster centroid that it was modulated to behave as, among the four clusters we selected.\"  I'm not sure this statement is true as IL (C30) is closer to C11 than C30 in table 1.\n\nThe biggest weakness in this paper -- and barrier to acceptance -- is in the really small sample size of the results where only 4 cluster behaviors out of 62 are evaluated.\u00a0 I would like to see information about the aggregate performance and relative behaviors of BRIL policies compared to the policies trained on the clustered subset of demonstrations across all clusters (or certainly a lot more than just 4 which is less than 10%!) in order to have a better evaluation of the BRIL approach.  The claims of BRIL doing better than clustered IL and being close to a behavior cluster centroid are not that convincing with so few sampled data points.  \n\nIn section 4.2 how do you define or quantify \"the most meaningful data separation\" when doing a grid search on parameters for clustering?\n\nOutside of making things easier to visualize, I'm not sure why it is necessary to reduce the dimensionality of the behavior input for BRIL. Reducing the dimensionality also reduces the set of different policy behaviors that can be expressed.  Additionally, if one might want to adjust a behavior such as having more or less of a certain unit type in StarCraft, the reduced behavior space makes it harder and less intuitive to do so. \n\nShowing that UCB can end up selecting the best BRIL policy (out of 4) and thereby resulting in a better cumulative win record than the traditional IL baseline is kind of trivial and doesn't add much to the paper.\u00a0 Using a Bayesian optimizer as suggested for future work to select the best behavior inputs for the BRIL policy would be more interesting and give more credence for BRIL being useful in discovering better policies."}, "signatures": ["ICLR.cc/2020/Conference/Paper866/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper866/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning a Behavioral Repertoire from Demonstrations", "authors": ["Niels Justesen", "Miguel Gonz\u00e1lez Duque", "Daniel Cabarcas Jaramillo", "Jean-Baptiste Mouret", "Sebastian Risi"], "authorids": ["noju@itu.edu", "migonzalez@unal.edu.co", "dcarbarc@unal.edu.co", "jean-baptiste.mouret@inria.fr", "sebr@itu.dk"], "keywords": ["Behavioral Repertoires", "Imitation Learning", "Deep Learning", "Adaptation", "StarCraft 2"], "TL;DR": "BRIL allows a single neural network to learn a repertoire of behaviors from a set of demonstrations that can be precisely modulated.", "abstract": "Imitation Learning (IL) is a machine learning approach to learn a policy from a set of demonstrations. IL can be useful to kick-start learning before applying reinforcement learning (RL) but it can also be useful on its own, e.g. to learn to imitate human players in video games. However, a major limitation of current IL approaches is that they learn only a single ``\"average\" policy based on a dataset that possibly contains demonstrations of numerous different types of behaviors. In this paper, we present a new approach called Behavioral Repertoire Imitation Learning (BRIL) that instead learns a repertoire of behaviors from a set of demonstrations by augmenting the state-action pairs with behavioral descriptions. The outcome of this approach is a single neural network policy conditioned on a behavior description that can be precisely modulated. We apply this approach to train a policy on 7,777 human demonstrations for the build-order planning task in StarCraft II. Dimensionality reduction techniques are applied to construct a low-dimensional behavioral space from the high-dimensional army unit composition of each demonstration. The results demonstrate that the learned policy can be effectively manipulated to express distinct behaviors. Additionally, by applying the UCB1 algorithm, the policy can adapt its behavior -in-between games- to reach a performance beyond that of the traditional IL baseline approach.", "pdf": "/pdf/0ef3206a531452869fde3dda234d09a4517fcc22.pdf", "paperhash": "justesen|learning_a_behavioral_repertoire_from_demonstrations", "original_pdf": "/attachment/0ef3206a531452869fde3dda234d09a4517fcc22.pdf", "_bibtex": "@misc{\njustesen2020learning,\ntitle={Learning a Behavioral Repertoire from Demonstrations},\nauthor={Niels Justesen and Miguel Gonz{\\'a}lez Duque and Daniel Cabarcas Jaramillo and Jean-Baptiste Mouret and Sebastian Risi},\nyear={2020},\nurl={https://openreview.net/forum?id=S1xqRTNtDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "S1xqRTNtDr", "replyto": "S1xqRTNtDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper866/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper866/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575821683281, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper866/Reviewers"], "noninvitees": [], "tcdate": 1570237745843, "tmdate": 1575821683297, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper866/-/Official_Review"}}}], "count": 5}