{"notes": [{"tddate": null, "replyto": null, "ddate": null, "cdate": null, "tmdate": 1487264843186, "tcdate": 1477541588899, "number": 8, "id": "rkpACe1lx", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "rkpACe1lx", "signatures": ["~David_Ha1"], "readers": ["everyone"], "content": {"title": "HyperNetworks", "abstract": "This work explores hypernetworks: an approach of using one network, also known as a hypernetwork, to generate the weights for another network.  We apply hypernetworks to generate adaptive weights for recurrent networks. In this case, hypernetworks can be viewed as a relaxed form of weight-sharing across layers. In our implementation, hypernetworks are are trained jointly with the main network in an end-to-end fashion.  Our main result is that hypernetworks can generate non-shared weights for LSTM and achieve state-of-the-art results on a variety of sequence modelling tasks including character-level language modelling, handwriting generation and neural machine translation, challenging the weight-sharing paradigm for recurrent networks.", "pdf": "/pdf/bbbae35015016cd02efae22e4caad0aba8e8a2fd.pdf", "TL;DR": "We train a small RNN to generate weights for a larger RNN, and train the system end-to-end.  We obtain state-of-the-art results on a variety of sequence modelling tasks.", "paperhash": "ha|hypernetworks", "author_emails": "hadavid@google.com,adai@google.com,qvl@google.com", "conflicts": ["google.com"], "keywords": ["Natural language processing", "Deep learning", "Supervised Learning"], "authors": ["David Ha", "Andrew M. Dai", "Quoc V. Le"], "authorids": ["hadavid@google.com", "adai@google.com", "qvl@google.com"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 14, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396307396, "tcdate": 1486396307396, "number": 1, "id": "B1sqjzIdg", "invitation": "ICLR.cc/2017/conference/-/paper8/acceptance", "forum": "rkpACe1lx", "replyto": "rkpACe1lx", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"title": "ICLR committee final decision", "comment": "The paper contains an interesting idea, and after the revision of 3rd Jan, the presentation is clear enough as well. (Although I find it now contains an odd repetition where related work is presented first in section 2, and then later in section 3.2).", "decision": "Accept (Poster)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "HyperNetworks", "abstract": "This work explores hypernetworks: an approach of using one network, also known as a hypernetwork, to generate the weights for another network.  We apply hypernetworks to generate adaptive weights for recurrent networks. In this case, hypernetworks can be viewed as a relaxed form of weight-sharing across layers. In our implementation, hypernetworks are are trained jointly with the main network in an end-to-end fashion.  Our main result is that hypernetworks can generate non-shared weights for LSTM and achieve state-of-the-art results on a variety of sequence modelling tasks including character-level language modelling, handwriting generation and neural machine translation, challenging the weight-sharing paradigm for recurrent networks.", "pdf": "/pdf/bbbae35015016cd02efae22e4caad0aba8e8a2fd.pdf", "TL;DR": "We train a small RNN to generate weights for a larger RNN, and train the system end-to-end.  We obtain state-of-the-art results on a variety of sequence modelling tasks.", "paperhash": "ha|hypernetworks", "author_emails": "hadavid@google.com,adai@google.com,qvl@google.com", "conflicts": ["google.com"], "keywords": ["Natural language processing", "Deep learning", "Supervised Learning"], "authors": ["David Ha", "Andrew M. Dai", "Quoc V. Le"], "authorids": ["hadavid@google.com", "adai@google.com", "qvl@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396307948, "id": "ICLR.cc/2017/conference/-/paper8/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "rkpACe1lx", "replyto": "rkpACe1lx", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396307948}}}, {"tddate": null, "tmdate": 1485554994141, "tcdate": 1485554994141, "number": 7, "id": "B1cNrHKPl", "invitation": "ICLR.cc/2017/conference/-/paper8/public/comment", "forum": "rkpACe1lx", "replyto": "B1_ZyAV4g", "signatures": ["~David_Ha1"], "readers": ["everyone"], "writers": ["~David_Ha1"], "content": {"title": "Response to AnonReviewer4 score revision.", "comment": "Hi, AnonReviewer4,\n\nThanks for revising your rating from 6 -> 8, based off the revised paper we have submitted on Jan 2, 2017.\n\nThe review process have helped us strengthen the presentation of our results in the revision of the paper.\n\nRegards,\n\nDavid\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "HyperNetworks", "abstract": "This work explores hypernetworks: an approach of using one network, also known as a hypernetwork, to generate the weights for another network.  We apply hypernetworks to generate adaptive weights for recurrent networks. In this case, hypernetworks can be viewed as a relaxed form of weight-sharing across layers. In our implementation, hypernetworks are are trained jointly with the main network in an end-to-end fashion.  Our main result is that hypernetworks can generate non-shared weights for LSTM and achieve state-of-the-art results on a variety of sequence modelling tasks including character-level language modelling, handwriting generation and neural machine translation, challenging the weight-sharing paradigm for recurrent networks.", "pdf": "/pdf/bbbae35015016cd02efae22e4caad0aba8e8a2fd.pdf", "TL;DR": "We train a small RNN to generate weights for a larger RNN, and train the system end-to-end.  We obtain state-of-the-art results on a variety of sequence modelling tasks.", "paperhash": "ha|hypernetworks", "author_emails": "hadavid@google.com,adai@google.com,qvl@google.com", "conflicts": ["google.com"], "keywords": ["Natural language processing", "Deep learning", "Supervised Learning"], "authors": ["David Ha", "Andrew M. Dai", "Quoc V. Le"], "authorids": ["hadavid@google.com", "adai@google.com", "qvl@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287764136, "id": "ICLR.cc/2017/conference/-/paper8/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rkpACe1lx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper8/reviewers", "ICLR.cc/2017/conference/paper8/areachairs"], "cdate": 1485287764136}}}, {"tddate": null, "tmdate": 1485514624488, "tcdate": 1482116864432, "number": 3, "id": "B1_ZyAV4g", "invitation": "ICLR.cc/2017/conference/-/paper8/official/review", "forum": "rkpACe1lx", "replyto": "rkpACe1lx", "signatures": ["ICLR.cc/2017/conference/paper8/AnonReviewer4"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper8/AnonReviewer4"], "content": {"title": "Very interesting work, especially the hyper recurrent networks LM results.", "rating": "8: Top 50% of accepted papers, clear accept", "review": "Although the trainable parameters might be reduced significantly, unfortunately the training and recognition speech cannot be reduced in this way.\nUnfortunately, as the results show, the authors could not get better results with less parameters.\nHowever, the proposed structure with even more number of parameters shows significant gain e.g. in LM.\n\nThe paper should be reorganized, and shortened. It is sometimes difficult to follow and sometimes inconsistent.\nE.g.: the weights of the feedforward network depend only on an embedding vector (see also my previous comments on linear bottlenecks), whereas in recurrent network the generated weights also depend on the input observation or its hidden representation.\n\nCould the authors provide the num. of trainable parameters for Table 6?\n\nProbably presenting less results could also improve the readability.\nOnly marginal accept due to the writing style.\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "HyperNetworks", "abstract": "This work explores hypernetworks: an approach of using one network, also known as a hypernetwork, to generate the weights for another network.  We apply hypernetworks to generate adaptive weights for recurrent networks. In this case, hypernetworks can be viewed as a relaxed form of weight-sharing across layers. In our implementation, hypernetworks are are trained jointly with the main network in an end-to-end fashion.  Our main result is that hypernetworks can generate non-shared weights for LSTM and achieve state-of-the-art results on a variety of sequence modelling tasks including character-level language modelling, handwriting generation and neural machine translation, challenging the weight-sharing paradigm for recurrent networks.", "pdf": "/pdf/bbbae35015016cd02efae22e4caad0aba8e8a2fd.pdf", "TL;DR": "We train a small RNN to generate weights for a larger RNN, and train the system end-to-end.  We obtain state-of-the-art results on a variety of sequence modelling tasks.", "paperhash": "ha|hypernetworks", "author_emails": "hadavid@google.com,adai@google.com,qvl@google.com", "conflicts": ["google.com"], "keywords": ["Natural language processing", "Deep learning", "Supervised Learning"], "authors": ["David Ha", "Andrew M. Dai", "Quoc V. Le"], "authorids": ["hadavid@google.com", "adai@google.com", "qvl@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512725530, "id": "ICLR.cc/2017/conference/-/paper8/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper8/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper8/AnonReviewer3", "ICLR.cc/2017/conference/paper8/AnonReviewer2", "ICLR.cc/2017/conference/paper8/AnonReviewer4"], "reply": {"forum": "rkpACe1lx", "replyto": "rkpACe1lx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper8/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper8/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512725530}}}, {"tddate": null, "tmdate": 1483430851199, "tcdate": 1483429114411, "number": 5, "id": "SyMbS0uSg", "invitation": "ICLR.cc/2017/conference/-/paper8/public/comment", "forum": "rkpACe1lx", "replyto": "B1_ZyAV4g", "signatures": ["~David_Ha1"], "readers": ["everyone"], "writers": ["~David_Ha1"], "content": {"title": "Response to AnonReviewer4, Revised paper.", "comment": "Hi, AnonReviewer4,\n\nThanks for your review.  We agree with your points about the writing style of the paper.  Based on your feedback, and also the feedback from the other reviewers, we have significantly rewritten the paper, shortened it and presented only the RNN part of the paper, so that it is more focused, self-contained and consistent. We\u2019re happy to change the title of the paper to more RNN-focused, if that\u2019s something the reviewer feels the right thing to do.  The parameter count is also included in the machine translation section as requested. Please take a look and let us know your thoughts!"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "HyperNetworks", "abstract": "This work explores hypernetworks: an approach of using one network, also known as a hypernetwork, to generate the weights for another network.  We apply hypernetworks to generate adaptive weights for recurrent networks. In this case, hypernetworks can be viewed as a relaxed form of weight-sharing across layers. In our implementation, hypernetworks are are trained jointly with the main network in an end-to-end fashion.  Our main result is that hypernetworks can generate non-shared weights for LSTM and achieve state-of-the-art results on a variety of sequence modelling tasks including character-level language modelling, handwriting generation and neural machine translation, challenging the weight-sharing paradigm for recurrent networks.", "pdf": "/pdf/bbbae35015016cd02efae22e4caad0aba8e8a2fd.pdf", "TL;DR": "We train a small RNN to generate weights for a larger RNN, and train the system end-to-end.  We obtain state-of-the-art results on a variety of sequence modelling tasks.", "paperhash": "ha|hypernetworks", "author_emails": "hadavid@google.com,adai@google.com,qvl@google.com", "conflicts": ["google.com"], "keywords": ["Natural language processing", "Deep learning", "Supervised Learning"], "authors": ["David Ha", "Andrew M. Dai", "Quoc V. Le"], "authorids": ["hadavid@google.com", "adai@google.com", "qvl@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287764136, "id": "ICLR.cc/2017/conference/-/paper8/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rkpACe1lx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper8/reviewers", "ICLR.cc/2017/conference/paper8/areachairs"], "cdate": 1485287764136}}}, {"tddate": null, "tmdate": 1483430277302, "tcdate": 1483430277302, "number": 6, "id": "H1atK0dSg", "invitation": "ICLR.cc/2017/conference/-/paper8/public/comment", "forum": "rkpACe1lx", "replyto": "r15r5JDre", "signatures": ["~David_Ha1"], "readers": ["everyone"], "writers": ["~David_Ha1"], "content": {"title": "Response", "comment": "Thanks for the review!\n\nWe have revised the paper to focus on the RNN methodology and results, to keep it inline with the conference format as the reviewers suggested, and have thus taken out the CNN section.  When we were starting out with this work, we originally started with CNNs.  But as you have pointed out, when we enforced weight-sharing across layers of a deep convnet, we found it difficult to match existing baseline results.  The insight from CNNs lead us to explore applying the opposite methodology, and to allow weight-sharing for RNNs instead, to see if we can improve baseline RNNs.\n\nWe hope this type of methodology can be applied to other types of RNNs going forward, and serve as a useful way to train RNNs to have context-dependent parameters.  We have released the code on github, and refactored it recently, to encourage others to use this technique for their own projects.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "HyperNetworks", "abstract": "This work explores hypernetworks: an approach of using one network, also known as a hypernetwork, to generate the weights for another network.  We apply hypernetworks to generate adaptive weights for recurrent networks. In this case, hypernetworks can be viewed as a relaxed form of weight-sharing across layers. In our implementation, hypernetworks are are trained jointly with the main network in an end-to-end fashion.  Our main result is that hypernetworks can generate non-shared weights for LSTM and achieve state-of-the-art results on a variety of sequence modelling tasks including character-level language modelling, handwriting generation and neural machine translation, challenging the weight-sharing paradigm for recurrent networks.", "pdf": "/pdf/bbbae35015016cd02efae22e4caad0aba8e8a2fd.pdf", "TL;DR": "We train a small RNN to generate weights for a larger RNN, and train the system end-to-end.  We obtain state-of-the-art results on a variety of sequence modelling tasks.", "paperhash": "ha|hypernetworks", "author_emails": "hadavid@google.com,adai@google.com,qvl@google.com", "conflicts": ["google.com"], "keywords": ["Natural language processing", "Deep learning", "Supervised Learning"], "authors": ["David Ha", "Andrew M. Dai", "Quoc V. Le"], "authorids": ["hadavid@google.com", "adai@google.com", "qvl@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287764136, "id": "ICLR.cc/2017/conference/-/paper8/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rkpACe1lx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper8/reviewers", "ICLR.cc/2017/conference/paper8/areachairs"], "cdate": 1485287764136}}}, {"tddate": null, "tmdate": 1483428985256, "tcdate": 1483428985256, "number": 4, "id": "Hkbt4R_rl", "invitation": "ICLR.cc/2017/conference/-/paper8/public/comment", "forum": "rkpACe1lx", "replyto": "HkwR93bEg", "signatures": ["~David_Ha1"], "readers": ["everyone"], "writers": ["~David_Ha1"], "content": {"title": "Response to AnonReviewer2, Revised paper.", "comment": "Hi, AnonReviewer2,\n\nThanks for the review!  We agree that the style of the paper suffers from a lack of focus.  We have rewritten the paper significantly to focus on the RNN part. We\u2019re happy to change the title of the paper to more RNN-focused, if that\u2019s something the reviewer feels the right thing to do.  Please take a look and let us know your feedback.\n\nAs for the point you mentioned about how HyperLSTM\u2019s improvements over LSTM come mainly from increasing the number of parameters, we tried to demonstrate in the Character PTB experiment, and also in the Handwriting Generation experiment, that the HyperLSTM can outperform conventional LSTMs with a lower parameter count.  For example, in the Char PTB experiment, we found that HyperLSTM with 1000 units can outperform an LSTM with 1250 units, and for the similarly in the handwriting generation experiment, the HyperLSTM with 900 hidden units outperform significantly the vanilla LSTM with 1000 units.  We hope that with the rewritten version that focuses on the RNN, it is easier to follow and digest the experimental results.\n\nFor your question regarding the softmax layer size:  For the MT experiment, the model does use a considerably large softmax layer size (32K for the GNMT architecture), although less than the 100K you mentioned.  We did not find it challenging for the HyperLSTM Cell to generate weights for the main LSTM.  The HyperLSTM Cell only had to generate a weight scaling vector of size 1000 (number of units of the main LSTM), which is not directly related to the size of the softmax layer.  We have expanded the MT section in the revised version of the paper, to try to emphasize the applicability of HyperLSTM to large-scale architectures such as GNMT.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "HyperNetworks", "abstract": "This work explores hypernetworks: an approach of using one network, also known as a hypernetwork, to generate the weights for another network.  We apply hypernetworks to generate adaptive weights for recurrent networks. In this case, hypernetworks can be viewed as a relaxed form of weight-sharing across layers. In our implementation, hypernetworks are are trained jointly with the main network in an end-to-end fashion.  Our main result is that hypernetworks can generate non-shared weights for LSTM and achieve state-of-the-art results on a variety of sequence modelling tasks including character-level language modelling, handwriting generation and neural machine translation, challenging the weight-sharing paradigm for recurrent networks.", "pdf": "/pdf/bbbae35015016cd02efae22e4caad0aba8e8a2fd.pdf", "TL;DR": "We train a small RNN to generate weights for a larger RNN, and train the system end-to-end.  We obtain state-of-the-art results on a variety of sequence modelling tasks.", "paperhash": "ha|hypernetworks", "author_emails": "hadavid@google.com,adai@google.com,qvl@google.com", "conflicts": ["google.com"], "keywords": ["Natural language processing", "Deep learning", "Supervised Learning"], "authors": ["David Ha", "Andrew M. Dai", "Quoc V. Le"], "authorids": ["hadavid@google.com", "adai@google.com", "qvl@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287764136, "id": "ICLR.cc/2017/conference/-/paper8/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rkpACe1lx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper8/reviewers", "ICLR.cc/2017/conference/paper8/areachairs"], "cdate": 1485287764136}}}, {"tddate": null, "tmdate": 1483428777810, "tcdate": 1483428777810, "number": 3, "id": "ryGnXAuBx", "invitation": "ICLR.cc/2017/conference/-/paper8/public/comment", "forum": "rkpACe1lx", "replyto": "SJVe59xVx", "signatures": ["~David_Ha1"], "readers": ["everyone"], "writers": ["~David_Ha1"], "content": {"title": "Response to AnonReviewer3, Revised paper.", "comment": "Hi, AnonReviewer3,\n\nThanks for the review.  I appreciate the effort and detail you have put into writing the review and personally found it educational.  We took your suggestion to refocus the paper on the RNN section and have rewritten it with that in mind, leaving out the CNN section, and expanded on the Machine Translation section which was a bit lacking as you mentioned.\n\nWe have also followed your advice and added a related approaches section to describe the relation and differences with multiplicative RNNs and the other related works with the HyperRNN, in addition to adding the Second Order RNN work to the related works section.  In addition, we have improved style of the writing to make it more self contained and less dependent on the appendices, as you have outlined in the review.  Would appreciate it greatly if you can help us review the revised version of our paper, and provide us feedback on the improvements."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "HyperNetworks", "abstract": "This work explores hypernetworks: an approach of using one network, also known as a hypernetwork, to generate the weights for another network.  We apply hypernetworks to generate adaptive weights for recurrent networks. In this case, hypernetworks can be viewed as a relaxed form of weight-sharing across layers. In our implementation, hypernetworks are are trained jointly with the main network in an end-to-end fashion.  Our main result is that hypernetworks can generate non-shared weights for LSTM and achieve state-of-the-art results on a variety of sequence modelling tasks including character-level language modelling, handwriting generation and neural machine translation, challenging the weight-sharing paradigm for recurrent networks.", "pdf": "/pdf/bbbae35015016cd02efae22e4caad0aba8e8a2fd.pdf", "TL;DR": "We train a small RNN to generate weights for a larger RNN, and train the system end-to-end.  We obtain state-of-the-art results on a variety of sequence modelling tasks.", "paperhash": "ha|hypernetworks", "author_emails": "hadavid@google.com,adai@google.com,qvl@google.com", "conflicts": ["google.com"], "keywords": ["Natural language processing", "Deep learning", "Supervised Learning"], "authors": ["David Ha", "Andrew M. Dai", "Quoc V. Le"], "authorids": ["hadavid@google.com", "adai@google.com", "qvl@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287764136, "id": "ICLR.cc/2017/conference/-/paper8/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rkpACe1lx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper8/reviewers", "ICLR.cc/2017/conference/paper8/areachairs"], "cdate": 1485287764136}}}, {"tddate": null, "tmdate": 1483303489675, "tcdate": 1483303489675, "number": 1, "id": "r15r5JDre", "invitation": "ICLR.cc/2017/conference/-/paper8/public/review", "forum": "rkpACe1lx", "replyto": "rkpACe1lx", "signatures": ["~Alex_Lamb1"], "readers": ["everyone"], "writers": ["~Alex_Lamb1"], "content": {"title": "Important Contribution in a Well Studied Area", "rating": "9: Top 15% of accepted papers, strong accept", "review": "A well known limitation in deep neural networks is that the same parameters are typically used for all examples, even though different examples have very different characteristics.  For example, recognizing animals will likely require different features than categorizing flowers.  Using different parameters for different types of examples has the potential to greatly reduce underfitting.  This can be seen in recent results with generative models, where image quality is much better for less diverse datasets.  However, it is difficult to use different parameters for different examples because we typically train using minibatches, which relies on using the same parameters for all examples in a minibatch (i.e. doing matrix multiplies in a fully-connected network).  \n\nThe hypernetworks paper cleverly proposes to get around this problem by adapting different \"parameters\" for different time steps in recurrent networks and different.  The basic insight is that a minibatch will always include many different examples from the same time step or spatial position, so there is no computational issue involved with using different \"parameters\".  In this paper, the \"parameters\" are modified for different positions based on the output from a hypernetwork which conditions on the time step.  Hypothetically, this hypernetwork could also condition on other features that are shared by all sequences in the minibatch.  \n\nI expect this method to become standard for training RNNs, especially where the length of the sequences is the same during the training and testing phases.  Penn Treebank is a highly competitive baseline, so the SOTA result reported here is impressive.  The experiments on convolutional networks are less experimentally impressive.  I suspect that the authors were aiming to achieve state of the art results here but settled with achieving a reduction in the number of parameters.  It might even be worthwhile to consider a synthetic experiment where two completely different types of image are appended (i.e. birds on the left and flowers on the right) and show that the hypernetwork helps in this situation.  It may be the case that for convnets, the cases where hypernetworks help are very specific.  \n\nFor RNNs, it seems to be the case that explicitly changing the nature of the computation depending on the position in the sequence greatly improves generalization.  While a usual RNN could learn to store a counter (indicating the position in the sequence), the hypernetwork could be a more efficient way to add capacity.  \n\nApplications to time series forecasting and modeling could be an interesting area for future work.  ", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "HyperNetworks", "abstract": "This work explores hypernetworks: an approach of using one network, also known as a hypernetwork, to generate the weights for another network.  We apply hypernetworks to generate adaptive weights for recurrent networks. In this case, hypernetworks can be viewed as a relaxed form of weight-sharing across layers. In our implementation, hypernetworks are are trained jointly with the main network in an end-to-end fashion.  Our main result is that hypernetworks can generate non-shared weights for LSTM and achieve state-of-the-art results on a variety of sequence modelling tasks including character-level language modelling, handwriting generation and neural machine translation, challenging the weight-sharing paradigm for recurrent networks.", "pdf": "/pdf/bbbae35015016cd02efae22e4caad0aba8e8a2fd.pdf", "TL;DR": "We train a small RNN to generate weights for a larger RNN, and train the system end-to-end.  We obtain state-of-the-art results on a variety of sequence modelling tasks.", "paperhash": "ha|hypernetworks", "author_emails": "hadavid@google.com,adai@google.com,qvl@google.com", "conflicts": ["google.com"], "keywords": ["Natural language processing", "Deep learning", "Supervised Learning"], "authors": ["David Ha", "Andrew M. Dai", "Quoc V. Le"], "authorids": ["hadavid@google.com", "adai@google.com", "qvl@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1483303490287, "id": "ICLR.cc/2017/conference/-/paper8/public/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rkpACe1lx", "replyto": "rkpACe1lx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "noninvitees": ["hadavid@google.com", "adai@google.com", "qvl@google.com", "ICLR.cc/2017/conference/paper8/reviewers", "ICLR.cc/2017/conference/paper8/areachairs", "~Alex_Lamb1"], "cdate": 1483303490287}}}, {"tddate": null, "tmdate": 1481915086842, "tcdate": 1481915086842, "number": 2, "id": "HkwR93bEg", "invitation": "ICLR.cc/2017/conference/-/paper8/official/review", "forum": "rkpACe1lx", "replyto": "rkpACe1lx", "signatures": ["ICLR.cc/2017/conference/paper8/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper8/AnonReviewer2"], "content": {"title": "A novel neural network training approach, but the presentation could be more clear", "rating": "7: Good paper, accept", "review": "This paper proposes an interesting new method for training neural networks, i.e., a hypernetwork is used to generate the model parameters of the main network. The authors demonstrated that the total number of model parameters could be smaller while achieving competitive results on the image classification task. In particular, the hyperLSTM with non-shared weights can achieve excellent results compared to conventional LSTM and its variants on a couple of LM talks, which is very inspiring.    \n\n--pros\n\nThis work demonstrates that it is possible to generate the neural network model parameters using another network that can achieve competitive results by a few relative large scale experiments. The idea itself is very inspiring, and the experiments are very solid.\n\n--cons\n\nThe paper would be much stronger if it was more focused. In particular, it is unclear what is the key advantage of this hypernetwork approach. It is argued that in the paper that can achieve competitive results using smaller number of trainable model parameters. However, in the running time, the computational complexity is the same as the standard main network for static networks, such as ConvNet, and the computational cost is even larger for dynamic networks such as LSTMs. The improvements of hyperLSTMs over conventional LSTM and its variants seem mainly come from increasing the number of model parameters.\n\n--minor question,\n\n The ConvNet and LSTM used in the experiments do not have a large softmax layer. For most of the word-level tasks for either LM or MT, the softmax layer could be more than 100K. Is it going to be challenging for the hyperNetwork generate large number of weights for that case, and is it going to slowing the training down significantly?\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "HyperNetworks", "abstract": "This work explores hypernetworks: an approach of using one network, also known as a hypernetwork, to generate the weights for another network.  We apply hypernetworks to generate adaptive weights for recurrent networks. In this case, hypernetworks can be viewed as a relaxed form of weight-sharing across layers. In our implementation, hypernetworks are are trained jointly with the main network in an end-to-end fashion.  Our main result is that hypernetworks can generate non-shared weights for LSTM and achieve state-of-the-art results on a variety of sequence modelling tasks including character-level language modelling, handwriting generation and neural machine translation, challenging the weight-sharing paradigm for recurrent networks.", "pdf": "/pdf/bbbae35015016cd02efae22e4caad0aba8e8a2fd.pdf", "TL;DR": "We train a small RNN to generate weights for a larger RNN, and train the system end-to-end.  We obtain state-of-the-art results on a variety of sequence modelling tasks.", "paperhash": "ha|hypernetworks", "author_emails": "hadavid@google.com,adai@google.com,qvl@google.com", "conflicts": ["google.com"], "keywords": ["Natural language processing", "Deep learning", "Supervised Learning"], "authors": ["David Ha", "Andrew M. Dai", "Quoc V. Le"], "authorids": ["hadavid@google.com", "adai@google.com", "qvl@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512725530, "id": "ICLR.cc/2017/conference/-/paper8/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper8/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper8/AnonReviewer3", "ICLR.cc/2017/conference/paper8/AnonReviewer2", "ICLR.cc/2017/conference/paper8/AnonReviewer4"], "reply": {"forum": "rkpACe1lx", "replyto": "rkpACe1lx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper8/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper8/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512725530}}}, {"tddate": null, "tmdate": 1481841131695, "tcdate": 1481841131688, "number": 1, "id": "SJVe59xVx", "invitation": "ICLR.cc/2017/conference/-/paper8/official/review", "forum": "rkpACe1lx", "replyto": "rkpACe1lx", "signatures": ["ICLR.cc/2017/conference/paper8/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper8/AnonReviewer3"], "content": {"title": "I would suggest to make the paper shorter and clearer possibly leaving the CNN results for latter publication. The relation with multiplicative/order 2 networks and eventual differences need to be explained.", "rating": "6: Marginally above acceptance threshold", "review": "*** Paper Summary ***\n\nThe paper proposes to a new neural network architecture. The layer weights of a classical network are computed as a function of a latent representation associated with the layer. Two instances are presented (i) a CNN where each layer weight is computed from a lower dimensional layer embedding vector; (ii) an RNN where each layer weight is computed from a secondary RNN state.\n\n*** Review Summary ***\n\nPros: \n- I like the idea of bringing multiplicative RNNs and their predecessors back into the spotlight. \n- LM and MT results are excellent.\n\nCons:  \n- The paper could be better written. It is too long for the conference format and need refocussing. \n- On related work, the relation with multiplicative RNN and their generic tensor product predecessor (Order 2 networks, wrt C. Lee Giles definition) should be mentioned in the related work section and the differences with earlier research need to be explained and motivated (by the way it is better to say that something is revisiting an old idea or training it at modern scale/on modern tasks than ommitting it).\n- on focus, it is not clear if your goal is to achieve better performance or more compact networks. In the RNN section you lean toward the former, in the CNN section you seem to lean toward the latter.\n\nI would suggest to make the paper shorter and clearer possibly leaving the CNN results for latter publication. The relation with multiplicative/order 2 networks and eventual differences need to be explained.\n\n*** Detailed Review ***\n\nMultiplicative networks are an extremely powerfull architecture and bringing them back into the spotlight is excellent. This paper has excellent results but suffer poor presentation, lack of a clear focus. It spends time on details and ommit important points. In its current form, it is much too long to long and his not self contained without the appendices.\n\nSpending more time on multiplicative RNNs, order 2 networks at the begining of the paper would be excellent. This will let you highlight the difference between this paper and earlier work. It would also be necessary to spend a little time on why multiplicative RNN were less used than gated RNN: it seems that the optimization problem their training involve is tricker and it would be helpful to explain whether you had a harder time tweaking optimization parameters or whether you needed longer training sessions compared to LSTMs, regular CNN. On name, I am not sure that \"hypernetwork\" help the reader understand better what the proposed architecture compared to multiplicative interactions.\n\nIn section 3.2, you seem to imply that there are different settings of hypernetworks that allow to vary from an RNN to a CNN, this is not clear to me, maybe you could show how this would work on a simple temporal problem with equations. \n\nThe work on CNN and RNN are rather disconnected to me: for CNN, you seem to be interested in a low rank structure of the weights, showing that similar performance can be achieved with less weights. It is not clear to me why to pursue that goal. Do you expect speedups? less memory for embedded applications? In that case you should compare with alternative strategies, e.g. model compression (Caruana et al 2006, aka Dark Knowledge, Hinton et al 2014) or hashed networks (Chen et al 2015). \n\nFor RNN, you seem to target better perplexity/BLEU and model compactness is not a priority. Instead of making the weights have a simpler structure, you make them richer, i.e. dependent over time. It seems in that case models might be bigger and take longer to train. You might want to comment on training time, inference time, memory requirement in that case, as you highlight it might be an important goal in the CNN section. Overall, I am not sure it helps to have this mixed message. I would rather see the paper fit in the conference format with the RNN results alone and a clearer explanation and defers the publications of the CNN results when a proper comparison with memory concerned methods is performed.\n\nSome of the discussions are not clear to me, I am not sure what message the reader should get from Figure 2 or from the discussion on saturation statistics (p10, Figure 5). Similarly, I am not sure if Figure 4 is showing anything: everything should change more drastically at word boundaries even in a regular LSTM (states, gates units should look very different before/after a space); without such a comparison it is hard to see if this is unique to your network.\n\nThe results on handwriting generation are harder to compare for me. Log-loss are hard to understand, I have no sense whether the difference between models is significant (what would be the variance in this metric under boostrap sampling of the training set?). I am not sold either on qualitative metric were human can assess quality but human cannot evaluate if the network is repeating the training set. Did you thing at precision/recall metric for ink, possibly with some spatial tolerance ? (e.g. evaluation of segmentation tasks in vision).\n\nThe MT experiments are insufficiently discussed in the main text.\n\nOverall, I would suggest to make the paper shorter and clearer possibly leaving the CNN results for latter publication. You need to properly discuss the relation to multiplicative/order 2 networks and highlight the differences. Unclear discussion can be eliminated to make the experimental setup and the results presentation clearer in the main text.\n\n*** References ***\n\nM.W. Goudreau, C.L. Giles, S.T. Chakradhar, D. Chen, \"First-Order Vs. Second-Order Single Layer Recurrent Neural Networks,\"IEEE Trans. on Neural Networks, 5 (3), p. 511, 1994.\n\nCristian Bucila, Rich Caruana, and Alexandru Niculescu-Mizil, \"Model Compression,\" The Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD-2006), August 2006, pp. 535-541.\n\nDark knowledge, G Hinton, O Vinyals, J Dean 2014\n\nW. Chen, J. Wilson, S. Tyree, K. Weinberger and Y. Chen, Compressing Neural Networks with the Hashing Trick, Proc. International Conference on Machine Learning (ICML-15)", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "HyperNetworks", "abstract": "This work explores hypernetworks: an approach of using one network, also known as a hypernetwork, to generate the weights for another network.  We apply hypernetworks to generate adaptive weights for recurrent networks. In this case, hypernetworks can be viewed as a relaxed form of weight-sharing across layers. In our implementation, hypernetworks are are trained jointly with the main network in an end-to-end fashion.  Our main result is that hypernetworks can generate non-shared weights for LSTM and achieve state-of-the-art results on a variety of sequence modelling tasks including character-level language modelling, handwriting generation and neural machine translation, challenging the weight-sharing paradigm for recurrent networks.", "pdf": "/pdf/bbbae35015016cd02efae22e4caad0aba8e8a2fd.pdf", "TL;DR": "We train a small RNN to generate weights for a larger RNN, and train the system end-to-end.  We obtain state-of-the-art results on a variety of sequence modelling tasks.", "paperhash": "ha|hypernetworks", "author_emails": "hadavid@google.com,adai@google.com,qvl@google.com", "conflicts": ["google.com"], "keywords": ["Natural language processing", "Deep learning", "Supervised Learning"], "authors": ["David Ha", "Andrew M. Dai", "Quoc V. Le"], "authorids": ["hadavid@google.com", "adai@google.com", "qvl@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512725530, "id": "ICLR.cc/2017/conference/-/paper8/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper8/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper8/AnonReviewer3", "ICLR.cc/2017/conference/paper8/AnonReviewer2", "ICLR.cc/2017/conference/paper8/AnonReviewer4"], "reply": {"forum": "rkpACe1lx", "replyto": "rkpACe1lx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper8/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper8/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512725530}}}, {"tddate": null, "tmdate": 1481114156196, "tcdate": 1481114156189, "number": 2, "id": "BJEVMKSQg", "invitation": "ICLR.cc/2017/conference/-/paper8/public/comment", "forum": "rkpACe1lx", "replyto": "S1BAH31Xg", "signatures": ["~David_Ha1"], "readers": ["everyone"], "writers": ["~David_Ha1"], "content": {"title": "re: questions", "comment": "Hi, thanks for the useful pointers.  For the RNN case, we have made an ongoing effort to try to improve the weight augmentation approach with low-rank matrix approximation, specifically using the HyperRNN to generate an additive (or multiplicative) low-rank 'fudge' to existing weight matrix of the main RNN.  I found that the current row-multiplicative method in eq 7-8 still offers the best performance while keeping computational performance relatively efficient, compared to low-rank factorized versions.  It may be possible to combine both approaches to squeeze out more performance at the expense of computational time / additional memory.\n\nFor section 3.1, your description is an elegant way to formulate the approach, by having the embedding vector z which depends on both j and i.\n\nWe have not tried to beat the SOTA results on CIFAR-10, as currently I found it more challenging to train much larger versions of a hypernetwork for CNN.  I'm actually trying out an approach of using hypernetworks for model distillation (https://arxiv.org/abs/1503.02531), which may help cope with some of the challenges.  For example, taking a pre-trained ensemble of ResNets, and recording the logits produced for the training set, and then training the hypernetwork version of the same ResNet to learn the logits.  I think learning soft-labels will help with both the training stability and performance.  This is still an ongoing effort for the future work."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "HyperNetworks", "abstract": "This work explores hypernetworks: an approach of using one network, also known as a hypernetwork, to generate the weights for another network.  We apply hypernetworks to generate adaptive weights for recurrent networks. In this case, hypernetworks can be viewed as a relaxed form of weight-sharing across layers. In our implementation, hypernetworks are are trained jointly with the main network in an end-to-end fashion.  Our main result is that hypernetworks can generate non-shared weights for LSTM and achieve state-of-the-art results on a variety of sequence modelling tasks including character-level language modelling, handwriting generation and neural machine translation, challenging the weight-sharing paradigm for recurrent networks.", "pdf": "/pdf/bbbae35015016cd02efae22e4caad0aba8e8a2fd.pdf", "TL;DR": "We train a small RNN to generate weights for a larger RNN, and train the system end-to-end.  We obtain state-of-the-art results on a variety of sequence modelling tasks.", "paperhash": "ha|hypernetworks", "author_emails": "hadavid@google.com,adai@google.com,qvl@google.com", "conflicts": ["google.com"], "keywords": ["Natural language processing", "Deep learning", "Supervised Learning"], "authors": ["David Ha", "Andrew M. Dai", "Quoc V. Le"], "authorids": ["hadavid@google.com", "adai@google.com", "qvl@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287764136, "id": "ICLR.cc/2017/conference/-/paper8/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rkpACe1lx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper8/reviewers", "ICLR.cc/2017/conference/paper8/areachairs"], "cdate": 1485287764136}}}, {"tddate": null, "tmdate": 1480734156876, "tcdate": 1480734156873, "number": 2, "id": "S1BAH31Xg", "invitation": "ICLR.cc/2017/conference/-/paper8/pre-review/question", "forum": "rkpACe1lx", "replyto": "rkpACe1lx", "signatures": ["ICLR.cc/2017/conference/paper8/AnonReviewer4"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper8/AnonReviewer4"], "content": {"title": "questions", "question": "To reduce the num. of parameters: in a linear element, low-rank factorization is often used, aka. linear bottleneck, often with mean or batch normalization.\nHave the authors tried such technique? E.g. low-rank factorized representation of K^j.\nFor RNNs, Gated Recurrent Convolutional Units have also been tried in the literature.\n\nIn section 3.1:\n  the authors argue against the one-layer hyper network. However using embedding vector z which depends on j (depth) and i (num of input, 1..N_{in}) would be also an option.\n  In eq 2, W_i*z^j + b_i := z_i,j would exactly result in this, and would not necessarily increase the num. of parameters -> no need for two layers.\n\nCould the authors outperform the best result (or achieve the same) in Table 2 with less parameters, e.g. <=3.77%, <13.3M ?\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "HyperNetworks", "abstract": "This work explores hypernetworks: an approach of using one network, also known as a hypernetwork, to generate the weights for another network.  We apply hypernetworks to generate adaptive weights for recurrent networks. In this case, hypernetworks can be viewed as a relaxed form of weight-sharing across layers. In our implementation, hypernetworks are are trained jointly with the main network in an end-to-end fashion.  Our main result is that hypernetworks can generate non-shared weights for LSTM and achieve state-of-the-art results on a variety of sequence modelling tasks including character-level language modelling, handwriting generation and neural machine translation, challenging the weight-sharing paradigm for recurrent networks.", "pdf": "/pdf/bbbae35015016cd02efae22e4caad0aba8e8a2fd.pdf", "TL;DR": "We train a small RNN to generate weights for a larger RNN, and train the system end-to-end.  We obtain state-of-the-art results on a variety of sequence modelling tasks.", "paperhash": "ha|hypernetworks", "author_emails": "hadavid@google.com,adai@google.com,qvl@google.com", "conflicts": ["google.com"], "keywords": ["Natural language processing", "Deep learning", "Supervised Learning"], "authors": ["David Ha", "Andrew M. Dai", "Quoc V. Le"], "authorids": ["hadavid@google.com", "adai@google.com", "qvl@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959509175, "id": "ICLR.cc/2017/conference/-/paper8/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper8/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper8/AnonReviewer2", "ICLR.cc/2017/conference/paper8/AnonReviewer4"], "reply": {"forum": "rkpACe1lx", "replyto": "rkpACe1lx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper8/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper8/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959509175}}}, {"tddate": null, "tmdate": 1480638508476, "tcdate": 1480638508469, "number": 1, "id": "HyE4lBCMg", "invitation": "ICLR.cc/2017/conference/-/paper8/public/comment", "forum": "rkpACe1lx", "replyto": "B1hyYPsfl", "signatures": ["~David_Ha1"], "readers": ["everyone"], "writers": ["~David_Ha1"], "content": {"title": "re: layer embedding and run-time complexity", "comment": "For the static hypernetworks described in Section 3, and in eq(1), the embedding vectors of each layer (z^j) are also learned parameters, and they are learned end-to-end during training of the image classification task along with the rest of the model.  So as you described, we could instead choose to represent each z^j by a one-hot, say determined by the layer index, and map z^j to the embedding space with an embedding matrix and learn that matrix end-to-end during training.  This would be equivalent to learning the embedding vectors directly.\n\nFor the run-time complexity, in the CNN case described in section 3, the extra run-time required will be in the order of the feeding the set of embedding vectors (z^j) through the hypernetwork to produce the set of kernels K^j.  During inference, the set of kernels K^j can be computed just once by the hypernetwork, and stored to be used as many times as required by the main network, so in that sense the run-time complexity is largely dominated by the main network as you mentioned.  During training, however, the hypernetwork will still be utilized at each forward and backward pass of every mini-batch, but in practice, we notice that the training time per mini-batch is ~ only 10% slower than the normal convnet case for MNIST and CIFAR10 experiments, as most of the complexity still resides in the main network.\n\nIn the RNN case, you are correct that the run-time complexity is larger than conventional RNNs, as the hypernetwork is operated at each timestep.\n\nPlease let us know if you have any more questions or concerns, thanks."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "HyperNetworks", "abstract": "This work explores hypernetworks: an approach of using one network, also known as a hypernetwork, to generate the weights for another network.  We apply hypernetworks to generate adaptive weights for recurrent networks. In this case, hypernetworks can be viewed as a relaxed form of weight-sharing across layers. In our implementation, hypernetworks are are trained jointly with the main network in an end-to-end fashion.  Our main result is that hypernetworks can generate non-shared weights for LSTM and achieve state-of-the-art results on a variety of sequence modelling tasks including character-level language modelling, handwriting generation and neural machine translation, challenging the weight-sharing paradigm for recurrent networks.", "pdf": "/pdf/bbbae35015016cd02efae22e4caad0aba8e8a2fd.pdf", "TL;DR": "We train a small RNN to generate weights for a larger RNN, and train the system end-to-end.  We obtain state-of-the-art results on a variety of sequence modelling tasks.", "paperhash": "ha|hypernetworks", "author_emails": "hadavid@google.com,adai@google.com,qvl@google.com", "conflicts": ["google.com"], "keywords": ["Natural language processing", "Deep learning", "Supervised Learning"], "authors": ["David Ha", "Andrew M. Dai", "Quoc V. Le"], "authorids": ["hadavid@google.com", "adai@google.com", "qvl@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287764136, "id": "ICLR.cc/2017/conference/-/paper8/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rkpACe1lx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper8/reviewers", "ICLR.cc/2017/conference/paper8/areachairs"], "cdate": 1485287764136}}}, {"tddate": null, "tmdate": 1480452324551, "tcdate": 1480452324547, "number": 1, "id": "B1hyYPsfl", "invitation": "ICLR.cc/2017/conference/-/paper8/pre-review/question", "forum": "rkpACe1lx", "replyto": "rkpACe1lx", "signatures": ["ICLR.cc/2017/conference/paper8/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper8/AnonReviewer2"], "content": {"title": "layer embedding and run-time complexity", "question": "Can you explain how do you obtain the layer embedding vector like z^j in eq(1)? Do you map the layer index j represented by a one-hot vector to z^j like word index to a word vector in language modeling?\n\nIs It correct that the run-time complexity of deep CNNs generated by the hyperNetwork is the same as the conventional main network, as the hypernetwork will be discarded, while for RNNs with hypernetworks, the run-time complexity is larger than conventional RNNs, as the hypernetwork should also operate for each time step?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "HyperNetworks", "abstract": "This work explores hypernetworks: an approach of using one network, also known as a hypernetwork, to generate the weights for another network.  We apply hypernetworks to generate adaptive weights for recurrent networks. In this case, hypernetworks can be viewed as a relaxed form of weight-sharing across layers. In our implementation, hypernetworks are are trained jointly with the main network in an end-to-end fashion.  Our main result is that hypernetworks can generate non-shared weights for LSTM and achieve state-of-the-art results on a variety of sequence modelling tasks including character-level language modelling, handwriting generation and neural machine translation, challenging the weight-sharing paradigm for recurrent networks.", "pdf": "/pdf/bbbae35015016cd02efae22e4caad0aba8e8a2fd.pdf", "TL;DR": "We train a small RNN to generate weights for a larger RNN, and train the system end-to-end.  We obtain state-of-the-art results on a variety of sequence modelling tasks.", "paperhash": "ha|hypernetworks", "author_emails": "hadavid@google.com,adai@google.com,qvl@google.com", "conflicts": ["google.com"], "keywords": ["Natural language processing", "Deep learning", "Supervised Learning"], "authors": ["David Ha", "Andrew M. Dai", "Quoc V. Le"], "authorids": ["hadavid@google.com", "adai@google.com", "qvl@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959509175, "id": "ICLR.cc/2017/conference/-/paper8/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper8/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper8/AnonReviewer2", "ICLR.cc/2017/conference/paper8/AnonReviewer4"], "reply": {"forum": "rkpACe1lx", "replyto": "rkpACe1lx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper8/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper8/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959509175}}}], "count": 15}