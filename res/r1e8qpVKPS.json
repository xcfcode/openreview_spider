{"notes": [{"id": "r1e8qpVKPS", "original": "Bkx89TTDwH", "number": 708, "cdate": 1569439118093, "ddate": null, "tcdate": 1569439118093, "tmdate": 1577168282239, "tddate": null, "forum": "r1e8qpVKPS", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["takagi@mns.k.u-tokyo.ac.jp", "nagano@mns.k.u-tokyo.ac.jp", "yoshida@mns.k.u-tokyo.ac.jp", "okada@edu.k.u-tokyo.ac.jp"], "title": "Role of two learning rates in convergence of model-agnostic meta-learning", "authors": ["Shiro Takagi", "Yoshihiro Nagano", "Yuki Yoshida", "Masato Okada"], "pdf": "/pdf/cdbfdee484753ccb694c6522f860f50c7c0b5838.pdf", "TL;DR": "We analyzed the role of two learning rates in model-agnostic meta-learning in convergence.", "abstract": "Model-agnostic meta-learning (MAML) is known as a powerful meta-learning method. However, MAML is notorious for being hard to train because of the existence of two learning rates. Therefore, in this paper, we derive the conditions that inner learning rate $\\alpha$ and meta-learning rate $\\beta$ must satisfy for MAML to converge to minima with some simplifications. We find that the upper bound of $\\beta$ depends on $ \\alpha$, in contrast to the case of using the normal gradient descent method. Moreover, we show that the threshold of $\\beta$ increases as $\\alpha$ approaches its own upper bound. This result is verified by experiments on various few-shot tasks and architectures; specifically, we perform sinusoid regression and classification of Omniglot and MiniImagenet datasets with a multilayer perceptron and a convolutional neural network. Based on this outcome, we present a guideline for determining the learning rates: first, search for the largest possible $\\alpha$; next, tune $\\beta$ based on the chosen value of $\\alpha$.", "code": "https://drive.google.com/file/d/1Seej9xI03F7_2wh4deDTBk_4aFyb2otb/view?usp=sharing", "keywords": ["meta-learning", "convergence"], "paperhash": "takagi|role_of_two_learning_rates_in_convergence_of_modelagnostic_metalearning", "original_pdf": "/attachment/0f5dd5362e2c727d95bd09005ddc337162c4eabb.pdf", "_bibtex": "@misc{\ntakagi2020role,\ntitle={Role of two learning rates in convergence of model-agnostic meta-learning},\nauthor={Shiro Takagi and Yoshihiro Nagano and Yuki Yoshida and Masato Okada},\nyear={2020},\nurl={https://openreview.net/forum?id=r1e8qpVKPS}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 12, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "z4j4TXaTk", "original": null, "number": 1, "cdate": 1576798703869, "ddate": null, "tcdate": 1576798703869, "tmdate": 1576800932191, "tddate": null, "forum": "r1e8qpVKPS", "replyto": "r1e8qpVKPS", "invitation": "ICLR.cc/2020/Conference/Paper708/-/Decision", "content": {"decision": "Reject", "comment": "This paper theoretically and empirically studies the inner and outer learning rate of the MAML algorithm and their role in convergence. While the paper presents some interesting ideas and add to our theoretical understanding of meta-learning algorithms, the reviewers raised concerns about the relevance of the theory. Further the empirical study is somewhat preliminary and doesn't compare to prior works that also try to stabilize the MAML algorithm, further bringing into question its usefulness. As such, the current form of the paper doesn't meet the bar for ICLR.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["takagi@mns.k.u-tokyo.ac.jp", "nagano@mns.k.u-tokyo.ac.jp", "yoshida@mns.k.u-tokyo.ac.jp", "okada@edu.k.u-tokyo.ac.jp"], "title": "Role of two learning rates in convergence of model-agnostic meta-learning", "authors": ["Shiro Takagi", "Yoshihiro Nagano", "Yuki Yoshida", "Masato Okada"], "pdf": "/pdf/cdbfdee484753ccb694c6522f860f50c7c0b5838.pdf", "TL;DR": "We analyzed the role of two learning rates in model-agnostic meta-learning in convergence.", "abstract": "Model-agnostic meta-learning (MAML) is known as a powerful meta-learning method. However, MAML is notorious for being hard to train because of the existence of two learning rates. Therefore, in this paper, we derive the conditions that inner learning rate $\\alpha$ and meta-learning rate $\\beta$ must satisfy for MAML to converge to minima with some simplifications. We find that the upper bound of $\\beta$ depends on $ \\alpha$, in contrast to the case of using the normal gradient descent method. Moreover, we show that the threshold of $\\beta$ increases as $\\alpha$ approaches its own upper bound. This result is verified by experiments on various few-shot tasks and architectures; specifically, we perform sinusoid regression and classification of Omniglot and MiniImagenet datasets with a multilayer perceptron and a convolutional neural network. Based on this outcome, we present a guideline for determining the learning rates: first, search for the largest possible $\\alpha$; next, tune $\\beta$ based on the chosen value of $\\alpha$.", "code": "https://drive.google.com/file/d/1Seej9xI03F7_2wh4deDTBk_4aFyb2otb/view?usp=sharing", "keywords": ["meta-learning", "convergence"], "paperhash": "takagi|role_of_two_learning_rates_in_convergence_of_modelagnostic_metalearning", "original_pdf": "/attachment/0f5dd5362e2c727d95bd09005ddc337162c4eabb.pdf", "_bibtex": "@misc{\ntakagi2020role,\ntitle={Role of two learning rates in convergence of model-agnostic meta-learning},\nauthor={Shiro Takagi and Yoshihiro Nagano and Yuki Yoshida and Masato Okada},\nyear={2020},\nurl={https://openreview.net/forum?id=r1e8qpVKPS}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "r1e8qpVKPS", "replyto": "r1e8qpVKPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795711725, "tmdate": 1576800260977, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper708/-/Decision"}}}, {"id": "Bkx4feTiiS", "original": null, "number": 8, "cdate": 1573797900118, "ddate": null, "tcdate": 1573797900118, "tmdate": 1573797900118, "tddate": null, "forum": "r1e8qpVKPS", "replyto": "HylESCaqjr", "invitation": "ICLR.cc/2020/Conference/Paper708/-/Official_Comment", "content": {"title": "Thank your for your response", "comment": "Dear Reviewer 2,\n\nThank you for your polite and quick response. We thought we had derived the necessary and sufficient condition for the single-task case and a sufficient condition for the multi-task case. However, as you appropriately point out, (eigenvalues of Hessian) \u2265 0 is a necessary condition and  (eigenvalues of Hessian) > 0 is a sufficient condition. Hence, what we said in the manuscript did not explain what we actually did correctly. That was our oversight. We also acknowledge that our expression was still confusing. Therefore, we re-updated the manuscript to make it clear that we derived a sufficient condition for both the single-task case and multi-task case. Accordingly, we corrected all expressions to be consistent with this modification, i.e. condition \u2192 sufficient condition. We uploaded the revised manuscript. We appreciate again your sincere contribution to the constructive discussion."}, "signatures": ["ICLR.cc/2020/Conference/Paper708/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper708/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["takagi@mns.k.u-tokyo.ac.jp", "nagano@mns.k.u-tokyo.ac.jp", "yoshida@mns.k.u-tokyo.ac.jp", "okada@edu.k.u-tokyo.ac.jp"], "title": "Role of two learning rates in convergence of model-agnostic meta-learning", "authors": ["Shiro Takagi", "Yoshihiro Nagano", "Yuki Yoshida", "Masato Okada"], "pdf": "/pdf/cdbfdee484753ccb694c6522f860f50c7c0b5838.pdf", "TL;DR": "We analyzed the role of two learning rates in model-agnostic meta-learning in convergence.", "abstract": "Model-agnostic meta-learning (MAML) is known as a powerful meta-learning method. However, MAML is notorious for being hard to train because of the existence of two learning rates. Therefore, in this paper, we derive the conditions that inner learning rate $\\alpha$ and meta-learning rate $\\beta$ must satisfy for MAML to converge to minima with some simplifications. We find that the upper bound of $\\beta$ depends on $ \\alpha$, in contrast to the case of using the normal gradient descent method. Moreover, we show that the threshold of $\\beta$ increases as $\\alpha$ approaches its own upper bound. This result is verified by experiments on various few-shot tasks and architectures; specifically, we perform sinusoid regression and classification of Omniglot and MiniImagenet datasets with a multilayer perceptron and a convolutional neural network. Based on this outcome, we present a guideline for determining the learning rates: first, search for the largest possible $\\alpha$; next, tune $\\beta$ based on the chosen value of $\\alpha$.", "code": "https://drive.google.com/file/d/1Seej9xI03F7_2wh4deDTBk_4aFyb2otb/view?usp=sharing", "keywords": ["meta-learning", "convergence"], "paperhash": "takagi|role_of_two_learning_rates_in_convergence_of_modelagnostic_metalearning", "original_pdf": "/attachment/0f5dd5362e2c727d95bd09005ddc337162c4eabb.pdf", "_bibtex": "@misc{\ntakagi2020role,\ntitle={Role of two learning rates in convergence of model-agnostic meta-learning},\nauthor={Shiro Takagi and Yoshihiro Nagano and Yuki Yoshida and Masato Okada},\nyear={2020},\nurl={https://openreview.net/forum?id=r1e8qpVKPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1e8qpVKPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper708/Authors", "ICLR.cc/2020/Conference/Paper708/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper708/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper708/Reviewers", "ICLR.cc/2020/Conference/Paper708/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper708/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper708/Authors|ICLR.cc/2020/Conference/Paper708/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504167423, "tmdate": 1576860534571, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper708/Authors", "ICLR.cc/2020/Conference/Paper708/Reviewers", "ICLR.cc/2020/Conference/Paper708/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper708/-/Official_Comment"}}}, {"id": "HylESCaqjr", "original": null, "number": 7, "cdate": 1573735996130, "ddate": null, "tcdate": 1573735996130, "tmdate": 1573735996130, "tddate": null, "forum": "r1e8qpVKPS", "replyto": "rklWoeK8jS", "invitation": "ICLR.cc/2020/Conference/Paper708/-/Official_Comment", "content": {"title": "Response Acknowledged", "comment": "Dear Authors,\n\nThanks for the detailed response. I have also checked the updated manuscript.\n\nI still have some questions regarding the updates:\n\n- In our updated statement \"we derived a condition for a simplified MAML to locally converge to local minima from any point in the vicinity of the local minimum,\" is the condition necessary or sufficient? Judging from the response, I think the authors meant sufficient condition. However, the main text still uses expressions such as \"the condition that $\\alpha$ should satisfy for $\\theta^*$ to be a local minimum...\" and \"the condition that $\\beta$ must satisfy is as follows...\" which mean necessary conditions.\n\nAlso, note that, (eigenvalues of Hessian) \u2265 0 is a *necessary* condition for local minima, whereas (eigenvalues of Hessian) > 0 is a *sufficient* condition. This means that in Section 3.1.1, eq (9), the authors are still writing a *necessary* condition for local minima, while in Section 3.1.2, the authors seem to derive a *sufficient* condition for convergence. I'm still confused.\n\n- To clarify: by your response to the multi-task case, do you mean that your condition is now a sufficient condition for local convergence to a local minimum at its vicinity?"}, "signatures": ["ICLR.cc/2020/Conference/Paper708/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper708/AnonReviewer2", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["takagi@mns.k.u-tokyo.ac.jp", "nagano@mns.k.u-tokyo.ac.jp", "yoshida@mns.k.u-tokyo.ac.jp", "okada@edu.k.u-tokyo.ac.jp"], "title": "Role of two learning rates in convergence of model-agnostic meta-learning", "authors": ["Shiro Takagi", "Yoshihiro Nagano", "Yuki Yoshida", "Masato Okada"], "pdf": "/pdf/cdbfdee484753ccb694c6522f860f50c7c0b5838.pdf", "TL;DR": "We analyzed the role of two learning rates in model-agnostic meta-learning in convergence.", "abstract": "Model-agnostic meta-learning (MAML) is known as a powerful meta-learning method. However, MAML is notorious for being hard to train because of the existence of two learning rates. Therefore, in this paper, we derive the conditions that inner learning rate $\\alpha$ and meta-learning rate $\\beta$ must satisfy for MAML to converge to minima with some simplifications. We find that the upper bound of $\\beta$ depends on $ \\alpha$, in contrast to the case of using the normal gradient descent method. Moreover, we show that the threshold of $\\beta$ increases as $\\alpha$ approaches its own upper bound. This result is verified by experiments on various few-shot tasks and architectures; specifically, we perform sinusoid regression and classification of Omniglot and MiniImagenet datasets with a multilayer perceptron and a convolutional neural network. Based on this outcome, we present a guideline for determining the learning rates: first, search for the largest possible $\\alpha$; next, tune $\\beta$ based on the chosen value of $\\alpha$.", "code": "https://drive.google.com/file/d/1Seej9xI03F7_2wh4deDTBk_4aFyb2otb/view?usp=sharing", "keywords": ["meta-learning", "convergence"], "paperhash": "takagi|role_of_two_learning_rates_in_convergence_of_modelagnostic_metalearning", "original_pdf": "/attachment/0f5dd5362e2c727d95bd09005ddc337162c4eabb.pdf", "_bibtex": "@misc{\ntakagi2020role,\ntitle={Role of two learning rates in convergence of model-agnostic meta-learning},\nauthor={Shiro Takagi and Yoshihiro Nagano and Yuki Yoshida and Masato Okada},\nyear={2020},\nurl={https://openreview.net/forum?id=r1e8qpVKPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1e8qpVKPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper708/Authors", "ICLR.cc/2020/Conference/Paper708/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper708/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper708/Reviewers", "ICLR.cc/2020/Conference/Paper708/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper708/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper708/Authors|ICLR.cc/2020/Conference/Paper708/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504167423, "tmdate": 1576860534571, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper708/Authors", "ICLR.cc/2020/Conference/Paper708/Reviewers", "ICLR.cc/2020/Conference/Paper708/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper708/-/Official_Comment"}}}, {"id": "Skl4ix9UjH", "original": null, "number": 6, "cdate": 1573458075905, "ddate": null, "tcdate": 1573458075905, "tmdate": 1573458075905, "tddate": null, "forum": "r1e8qpVKPS", "replyto": "r1e8qpVKPS", "invitation": "ICLR.cc/2020/Conference/Paper708/-/Official_Comment", "content": {"title": "Summary of the overall revision", "comment": "Dear reviewers,\n\nThank you for your detailed review and constructive feedback. We uploaded the revised manuscript. Although we responded to each reviewer, we would like to explain a summary of the revision below.\n\n- Considering the reviewer 2\u2019s feedback, we changed the expression \u201cthe necessary condition for MAML to converge to minima\u201d to \u201cthe condition for a simplified MAML to locally converge to local minima from any point in the vicinity of the local minima\u201d because this is more suitable expression to explain what we did. Hence, we altered all expressions to expressions consistent with this modification, i.e. convergence \u2014> local convergence. \n\n- We added the detail derivation of Eq. (12) in the main text. \n\n- We explicitly write in Appendix C that the calculation of Tg is done for the training loss of the sinusoid regression and the experimental setting is the same as 5.2 except that the total number of iterations is 50000 and learning rates \\alpha and \\beta are fixed to be 1e-2 and 1e-3 respectively.\n\n- We altered all minor mistakes and misleading expressions pointed out, i.e. we indicate that \\nabla_\\theta L_\\tau (\\theta) is an *estimate* of the true gradient. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper708/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper708/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["takagi@mns.k.u-tokyo.ac.jp", "nagano@mns.k.u-tokyo.ac.jp", "yoshida@mns.k.u-tokyo.ac.jp", "okada@edu.k.u-tokyo.ac.jp"], "title": "Role of two learning rates in convergence of model-agnostic meta-learning", "authors": ["Shiro Takagi", "Yoshihiro Nagano", "Yuki Yoshida", "Masato Okada"], "pdf": "/pdf/cdbfdee484753ccb694c6522f860f50c7c0b5838.pdf", "TL;DR": "We analyzed the role of two learning rates in model-agnostic meta-learning in convergence.", "abstract": "Model-agnostic meta-learning (MAML) is known as a powerful meta-learning method. However, MAML is notorious for being hard to train because of the existence of two learning rates. Therefore, in this paper, we derive the conditions that inner learning rate $\\alpha$ and meta-learning rate $\\beta$ must satisfy for MAML to converge to minima with some simplifications. We find that the upper bound of $\\beta$ depends on $ \\alpha$, in contrast to the case of using the normal gradient descent method. Moreover, we show that the threshold of $\\beta$ increases as $\\alpha$ approaches its own upper bound. This result is verified by experiments on various few-shot tasks and architectures; specifically, we perform sinusoid regression and classification of Omniglot and MiniImagenet datasets with a multilayer perceptron and a convolutional neural network. Based on this outcome, we present a guideline for determining the learning rates: first, search for the largest possible $\\alpha$; next, tune $\\beta$ based on the chosen value of $\\alpha$.", "code": "https://drive.google.com/file/d/1Seej9xI03F7_2wh4deDTBk_4aFyb2otb/view?usp=sharing", "keywords": ["meta-learning", "convergence"], "paperhash": "takagi|role_of_two_learning_rates_in_convergence_of_modelagnostic_metalearning", "original_pdf": "/attachment/0f5dd5362e2c727d95bd09005ddc337162c4eabb.pdf", "_bibtex": "@misc{\ntakagi2020role,\ntitle={Role of two learning rates in convergence of model-agnostic meta-learning},\nauthor={Shiro Takagi and Yoshihiro Nagano and Yuki Yoshida and Masato Okada},\nyear={2020},\nurl={https://openreview.net/forum?id=r1e8qpVKPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1e8qpVKPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper708/Authors", "ICLR.cc/2020/Conference/Paper708/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper708/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper708/Reviewers", "ICLR.cc/2020/Conference/Paper708/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper708/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper708/Authors|ICLR.cc/2020/Conference/Paper708/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504167423, "tmdate": 1576860534571, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper708/Authors", "ICLR.cc/2020/Conference/Paper708/Reviewers", "ICLR.cc/2020/Conference/Paper708/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper708/-/Official_Comment"}}}, {"id": "rklkKZYLir", "original": null, "number": 5, "cdate": 1573454198545, "ddate": null, "tcdate": 1573454198545, "tmdate": 1573454198545, "tddate": null, "forum": "r1e8qpVKPS", "replyto": "SJloDQiSFB", "invitation": "ICLR.cc/2020/Conference/Paper708/-/Official_Comment", "content": {"title": "Response 2", "comment": "> The baselines should be compared, to support the effectiveness of this proposed algorithm. For example, Behl et al. (2019) automatically tuning the learning rates during training definitely needs to be compared, since both aim to stabilize the training of MAML.\n\nThank you for your feedback. Admitting that both works concern about the stability of MAML training, we think that our work is a different type of study from Behl et al. (2019). Although contribution of Behl et al. (2019) was proposing a specific algorithm to stabilize MAML training, our study focuses on investigating the mechanism of MAML training and revealing when MAML training fails, and our main argument is independent of a specific algorithm. Also, we think that the study of Behl et al. (2019) and our work are hard to be compared. A guideline we suggested \u201cIdentifying the large \\alpha first and then tuning \\beta\u201d is a guideline to search the learning rates for stable training as long as  you use fixed scalar learning rates. Learning rates identified following the guideline are maximum possible fixed learning rates. Therefore, if we compare the training of the learning rates with that of Behl et al. (2019)\u2019s algorithms, it will be just a comparison between the performance of fixed learning rates and that of auto-tuned learning rates and not a comparison between our guideline and Behl et al. (2019)\u2019s algorithms. We appreciate your constructive feedback."}, "signatures": ["ICLR.cc/2020/Conference/Paper708/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper708/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["takagi@mns.k.u-tokyo.ac.jp", "nagano@mns.k.u-tokyo.ac.jp", "yoshida@mns.k.u-tokyo.ac.jp", "okada@edu.k.u-tokyo.ac.jp"], "title": "Role of two learning rates in convergence of model-agnostic meta-learning", "authors": ["Shiro Takagi", "Yoshihiro Nagano", "Yuki Yoshida", "Masato Okada"], "pdf": "/pdf/cdbfdee484753ccb694c6522f860f50c7c0b5838.pdf", "TL;DR": "We analyzed the role of two learning rates in model-agnostic meta-learning in convergence.", "abstract": "Model-agnostic meta-learning (MAML) is known as a powerful meta-learning method. However, MAML is notorious for being hard to train because of the existence of two learning rates. Therefore, in this paper, we derive the conditions that inner learning rate $\\alpha$ and meta-learning rate $\\beta$ must satisfy for MAML to converge to minima with some simplifications. We find that the upper bound of $\\beta$ depends on $ \\alpha$, in contrast to the case of using the normal gradient descent method. Moreover, we show that the threshold of $\\beta$ increases as $\\alpha$ approaches its own upper bound. This result is verified by experiments on various few-shot tasks and architectures; specifically, we perform sinusoid regression and classification of Omniglot and MiniImagenet datasets with a multilayer perceptron and a convolutional neural network. Based on this outcome, we present a guideline for determining the learning rates: first, search for the largest possible $\\alpha$; next, tune $\\beta$ based on the chosen value of $\\alpha$.", "code": "https://drive.google.com/file/d/1Seej9xI03F7_2wh4deDTBk_4aFyb2otb/view?usp=sharing", "keywords": ["meta-learning", "convergence"], "paperhash": "takagi|role_of_two_learning_rates_in_convergence_of_modelagnostic_metalearning", "original_pdf": "/attachment/0f5dd5362e2c727d95bd09005ddc337162c4eabb.pdf", "_bibtex": "@misc{\ntakagi2020role,\ntitle={Role of two learning rates in convergence of model-agnostic meta-learning},\nauthor={Shiro Takagi and Yoshihiro Nagano and Yuki Yoshida and Masato Okada},\nyear={2020},\nurl={https://openreview.net/forum?id=r1e8qpVKPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1e8qpVKPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper708/Authors", "ICLR.cc/2020/Conference/Paper708/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper708/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper708/Reviewers", "ICLR.cc/2020/Conference/Paper708/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper708/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper708/Authors|ICLR.cc/2020/Conference/Paper708/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504167423, "tmdate": 1576860534571, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper708/Authors", "ICLR.cc/2020/Conference/Paper708/Reviewers", "ICLR.cc/2020/Conference/Paper708/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper708/-/Official_Comment"}}}, {"id": "rJeiD-FUiB", "original": null, "number": 4, "cdate": 1573454179024, "ddate": null, "tcdate": 1573454179024, "tmdate": 1573454179024, "tddate": null, "forum": "r1e8qpVKPS", "replyto": "SJloDQiSFB", "invitation": "ICLR.cc/2020/Conference/Paper708/-/Official_Comment", "content": {"title": "Response 1", "comment": "Thank you for your thoughtful review and comments. Our response to your reviews are as follows:\n\n\n> In Section 3.1.1, the authors ignored Tg based on an observation that Tg is small. However, even in the Appendix, the authors did not explicate the experimental setting where they reach such a conclusion. Will Tg be always small then, in any task and any dataset?\n\nWe specified that the experimental setting is the same as that of Section 5.2 except that particular learning rates are used and the total number of iterations is larger in the Appendix of the new manuscript. We confirmed that at least Tg is small under that setting where we conducted the sinusoid regression. Although we did not check the Omniglot and MiniImagenet classification, we believe that the presumption is valid enough for predicting the experimental result since the result of our theory is applicable even for these datasets and tasks.\n\n\n> Practically, we often train MAML in 5 or 10 steps instead of only one step. Will the conclusion apply to such setups? Or will the model itself diverge during the inner loop as more steps are taken, provided with a larger value of \\alpha?\n\nAlthough we do not investigate the multi-step cases, we think that our conclusion has an implication for these cases as well. Ignoring higher-order terms and assuming that our assumptions hold, the regularization term of the simplified meta-objective will be from the squared norm of the gradient to the inner product of gradient of the loss at step 0 and sum of the gradient at each step, as we mentioned in Appendix A.2 : \\tilde L(\\theta)  = L(\\theta) - \\alpha / 2 g^T g \u2014> \\tilde L(\\theta) = L(\\theta) - \\alpha / 2 \\sum_i g_k^T g_i. Intuitively, if all gradients are orthogonal each other, a multi-steps case is not that different from a one-step case. On the contrary, if all of them are in the same direction, the gradient norm will be multiplied by the number of steps, making the largest possible \\alpha will be divided by the number of steps. In the real setting, cosine similarity of gradients will be between 0 and 1, the maximum possible \\alpha is expected to be a little bit smaller.\n\n\n> Practically, we merely use vanilla SGD. The figure in the Appendix shows that the main conclusion actually does not apply to Adam, which disempower the practicability of the theoretical guideline.\n\nThank you for your feedback. We acknowledge that it would have been better if our conclusion applied to Adam as well for practicability. Because the effective learning rate of Adam is defined parameter-wise and changes during training, it was not suitable to predict the experimental result based on the conclusion from the theory about a fixed scalar learning rate. One of our contributions is suggesting to regard MAML as optimization with the negative gradient penalty and this formulation itself, of course, can be used for analyzing the adaptive optimizer too, though the procedure of the analysis will not be the same as what we did in this study. Comparing the result with ours may reveal why we seem to get a different result between gradient descent and Adam. Even though our result may not be immediately applicable to Adam, we think that our theory still has the value as the first step to analyze the case including optimizer like Adam.\n\n\n> How can we search the \u201clargest possible\u201d inner loop learning rate? Should we still use grid-search or heuristic search? In that case, what is the implication/shortcut that this proposed guideline brings?\n\nThe key takeaway of our work is that you do not have to find the \u201coptimal pair of \\alpha and \\beta\u201d, which is unknown, to tune the learning rates. Rather, it is enough to \u201cidentify the largest possible \\alpha\u201d for MAML to converge. Since you just need to find a particular \\alpha, you can find the maximum possible \\alpha with the binary search algorithm which runs only in logarithmic time even in the worst case, for example. This does not take that much time because you do not have to wait until convergence to find the largest possible \\alpha.\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper708/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper708/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["takagi@mns.k.u-tokyo.ac.jp", "nagano@mns.k.u-tokyo.ac.jp", "yoshida@mns.k.u-tokyo.ac.jp", "okada@edu.k.u-tokyo.ac.jp"], "title": "Role of two learning rates in convergence of model-agnostic meta-learning", "authors": ["Shiro Takagi", "Yoshihiro Nagano", "Yuki Yoshida", "Masato Okada"], "pdf": "/pdf/cdbfdee484753ccb694c6522f860f50c7c0b5838.pdf", "TL;DR": "We analyzed the role of two learning rates in model-agnostic meta-learning in convergence.", "abstract": "Model-agnostic meta-learning (MAML) is known as a powerful meta-learning method. However, MAML is notorious for being hard to train because of the existence of two learning rates. Therefore, in this paper, we derive the conditions that inner learning rate $\\alpha$ and meta-learning rate $\\beta$ must satisfy for MAML to converge to minima with some simplifications. We find that the upper bound of $\\beta$ depends on $ \\alpha$, in contrast to the case of using the normal gradient descent method. Moreover, we show that the threshold of $\\beta$ increases as $\\alpha$ approaches its own upper bound. This result is verified by experiments on various few-shot tasks and architectures; specifically, we perform sinusoid regression and classification of Omniglot and MiniImagenet datasets with a multilayer perceptron and a convolutional neural network. Based on this outcome, we present a guideline for determining the learning rates: first, search for the largest possible $\\alpha$; next, tune $\\beta$ based on the chosen value of $\\alpha$.", "code": "https://drive.google.com/file/d/1Seej9xI03F7_2wh4deDTBk_4aFyb2otb/view?usp=sharing", "keywords": ["meta-learning", "convergence"], "paperhash": "takagi|role_of_two_learning_rates_in_convergence_of_modelagnostic_metalearning", "original_pdf": "/attachment/0f5dd5362e2c727d95bd09005ddc337162c4eabb.pdf", "_bibtex": "@misc{\ntakagi2020role,\ntitle={Role of two learning rates in convergence of model-agnostic meta-learning},\nauthor={Shiro Takagi and Yoshihiro Nagano and Yuki Yoshida and Masato Okada},\nyear={2020},\nurl={https://openreview.net/forum?id=r1e8qpVKPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1e8qpVKPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper708/Authors", "ICLR.cc/2020/Conference/Paper708/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper708/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper708/Reviewers", "ICLR.cc/2020/Conference/Paper708/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper708/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper708/Authors|ICLR.cc/2020/Conference/Paper708/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504167423, "tmdate": 1576860534571, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper708/Authors", "ICLR.cc/2020/Conference/Paper708/Reviewers", "ICLR.cc/2020/Conference/Paper708/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper708/-/Official_Comment"}}}, {"id": "SJeuGWtLjS", "original": null, "number": 3, "cdate": 1573454095647, "ddate": null, "tcdate": 1573454095647, "tmdate": 1573454095647, "tddate": null, "forum": "r1e8qpVKPS", "replyto": "rJx4PXPnFr", "invitation": "ICLR.cc/2020/Conference/Paper708/-/Official_Comment", "content": {"title": "Response", "comment": "Dear Reviewer 3,\n\nThank you for your review. we would like to address your concern below.\n\n\n> It doesn\u2019t help that the criteria relies on the spectrum of the Hessian evaluated at a critical point, which of course is not known before convergence in non quadratic setting. The paper would be way more convincing if the authors could use their findings to describe some more precise heuristics to tune the learning rates, and conduct a proper comparison of its performance with other methods.\n\nThank you for your comment. We do not think that it is essential to know the specific value of the spectrum of the Hessian. Contribution of our work is not only identifying the specific value of the upper bounds but rather revealing the relationship between the upper bounds of the inner learning rate and meta-learning rate. Thanks to the relationship, we can show the guideline suggesting that we should identify the largest possible inner learning rate."}, "signatures": ["ICLR.cc/2020/Conference/Paper708/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper708/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["takagi@mns.k.u-tokyo.ac.jp", "nagano@mns.k.u-tokyo.ac.jp", "yoshida@mns.k.u-tokyo.ac.jp", "okada@edu.k.u-tokyo.ac.jp"], "title": "Role of two learning rates in convergence of model-agnostic meta-learning", "authors": ["Shiro Takagi", "Yoshihiro Nagano", "Yuki Yoshida", "Masato Okada"], "pdf": "/pdf/cdbfdee484753ccb694c6522f860f50c7c0b5838.pdf", "TL;DR": "We analyzed the role of two learning rates in model-agnostic meta-learning in convergence.", "abstract": "Model-agnostic meta-learning (MAML) is known as a powerful meta-learning method. However, MAML is notorious for being hard to train because of the existence of two learning rates. Therefore, in this paper, we derive the conditions that inner learning rate $\\alpha$ and meta-learning rate $\\beta$ must satisfy for MAML to converge to minima with some simplifications. We find that the upper bound of $\\beta$ depends on $ \\alpha$, in contrast to the case of using the normal gradient descent method. Moreover, we show that the threshold of $\\beta$ increases as $\\alpha$ approaches its own upper bound. This result is verified by experiments on various few-shot tasks and architectures; specifically, we perform sinusoid regression and classification of Omniglot and MiniImagenet datasets with a multilayer perceptron and a convolutional neural network. Based on this outcome, we present a guideline for determining the learning rates: first, search for the largest possible $\\alpha$; next, tune $\\beta$ based on the chosen value of $\\alpha$.", "code": "https://drive.google.com/file/d/1Seej9xI03F7_2wh4deDTBk_4aFyb2otb/view?usp=sharing", "keywords": ["meta-learning", "convergence"], "paperhash": "takagi|role_of_two_learning_rates_in_convergence_of_modelagnostic_metalearning", "original_pdf": "/attachment/0f5dd5362e2c727d95bd09005ddc337162c4eabb.pdf", "_bibtex": "@misc{\ntakagi2020role,\ntitle={Role of two learning rates in convergence of model-agnostic meta-learning},\nauthor={Shiro Takagi and Yoshihiro Nagano and Yuki Yoshida and Masato Okada},\nyear={2020},\nurl={https://openreview.net/forum?id=r1e8qpVKPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1e8qpVKPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper708/Authors", "ICLR.cc/2020/Conference/Paper708/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper708/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper708/Reviewers", "ICLR.cc/2020/Conference/Paper708/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper708/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper708/Authors|ICLR.cc/2020/Conference/Paper708/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504167423, "tmdate": 1576860534571, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper708/Authors", "ICLR.cc/2020/Conference/Paper708/Reviewers", "ICLR.cc/2020/Conference/Paper708/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper708/-/Official_Comment"}}}, {"id": "H1gkAeY8jH", "original": null, "number": 2, "cdate": 1573454022595, "ddate": null, "tcdate": 1573454022595, "tmdate": 1573454022595, "tddate": null, "forum": "r1e8qpVKPS", "replyto": "BygN9L1ycS", "invitation": "ICLR.cc/2020/Conference/Paper708/-/Official_Comment", "content": {"title": "Response 2", "comment": "> Even if (12) is true, there are pathological cases lying in a set of measure zero, where (12) converges even when (13) is not satisfied. For simplicity, consider v(t+1) = G v(t), where G is a real symmetric matrix. Assume all but one eigenvalue \\lambda_1 are greater than 1 and |\\lambda_1| < 1. Let v_1 be the corresponding eigenvalue for \\lambda_1. Then, if v(0) = v_1, the sequence v(t) converges to zero. This means that the \"necessary condition\" derived in (16) may not actually be a necessary condition for convergence.\n\nWe admit that we were not able to derive the \u201cnecessary condition\u201d in the original paper. We changed the expression from just \u201cconvergence\u201d to \u201clocal convergence from any point in the vicinity of local minimum\u201d to eliminate the pathological case that you showed.\n\n\n> In the update rule (1) and (2), it'd be better to mention that \\nabla_\\theta L_\\tau (\\theta) is an *estimate* of the true gradient using the training data and test data, respectively. In their current status, the gradients in (1) and (2) will read as the full gradients of the population where the training/test data points are sampled from.\n\nWe articulated in the new manuscript that \\nabla_\\theta L_\\tau (\\theta) is an estimate of the true gradient as you appropriately advised.\n\n\n> Section 2.1 only introduces the case where the update (1) is done only once in the inner loop. However, later Section 2.2 and Section 3, the paper says \"only one step is taken for update...\". Without the prior knowledge that there are versions in which multiple updates (1) are done, the readers can easily get confused.\n\nWe specified in the new manuscript that MAML allows multiple steps in Section 2.1.\n\n\n> The word \"minima\" used throughout should better be corrected to \"local minima,\" as mere minima may be understood as \"global minima.\"\n\nWe changed all \u201dminima\u201d to \u201dlocal minima\u201d\uff0e\n\n\n> In the abstract, there is a phrase \"in contrast to the case of using the normal gradient descent method.\" Which setting do you mean by the \"normal gradient descent\"?\n\nWe meant the normal gradient descent as the training when \\alpha =0. We admit it was confusing expression and deleted the phrase.\n\n\n> After equations (3) and (4), the paper says \"The above is known as the first-order approximation...\", but as far as I'm concerned, the first-order approximation version of MAML is (3) without the hessian term, not (4). This statement can potentially be misleading.\n\n\"The above is known as the first-order approximation...\" was misleading and could give expression as if we did the well-known first-order approximation. We articulated that the result with our approximation can be regarded as the result of well-known first-order approximation only when all of our assumptions hold.\n\n\n> Column vectors / row vectors are mixed up. In (3), the partial derivatives are row vectors, but in (4) g_\\tau(\\theta)'s are column vectors. However, right above (3), g_\\tau(\\theta) is a row vector this time.\n> Did (5) come from the approximation (4)? If so, as \\tilde L(\\theta) is defined to be L(\\theta'), the two sides of (5) are \"approximately equal\" not \"equal.\"\n> In Eq (19): P_\\tau (\\theta - \\theta^*) -> (\\theta-\\theta^*).\n\nFollowing your advice, we corrected all of them.\n\n\n> At the end of page 6, what do you mean by \"not a sum but a mean\"? Isn't sum of n things equal to the mean, except for a factor of 1/n?\n\nWhether the loss is sum or average affects the value of the apparent learning rate. If we use the summation loss, the apparent learning rate seems to be n-times larger than when the averaged loss is used. In our research, we would like to discuss how large the maximum learning rate can be. Though we write the formulation of MAML in Section 2 following the formulation in the original MAML paper, we would like to make learning rates independent of the number of tasks and the sample size and hence we experimented with the average loss."}, "signatures": ["ICLR.cc/2020/Conference/Paper708/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper708/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["takagi@mns.k.u-tokyo.ac.jp", "nagano@mns.k.u-tokyo.ac.jp", "yoshida@mns.k.u-tokyo.ac.jp", "okada@edu.k.u-tokyo.ac.jp"], "title": "Role of two learning rates in convergence of model-agnostic meta-learning", "authors": ["Shiro Takagi", "Yoshihiro Nagano", "Yuki Yoshida", "Masato Okada"], "pdf": "/pdf/cdbfdee484753ccb694c6522f860f50c7c0b5838.pdf", "TL;DR": "We analyzed the role of two learning rates in model-agnostic meta-learning in convergence.", "abstract": "Model-agnostic meta-learning (MAML) is known as a powerful meta-learning method. However, MAML is notorious for being hard to train because of the existence of two learning rates. Therefore, in this paper, we derive the conditions that inner learning rate $\\alpha$ and meta-learning rate $\\beta$ must satisfy for MAML to converge to minima with some simplifications. We find that the upper bound of $\\beta$ depends on $ \\alpha$, in contrast to the case of using the normal gradient descent method. Moreover, we show that the threshold of $\\beta$ increases as $\\alpha$ approaches its own upper bound. This result is verified by experiments on various few-shot tasks and architectures; specifically, we perform sinusoid regression and classification of Omniglot and MiniImagenet datasets with a multilayer perceptron and a convolutional neural network. Based on this outcome, we present a guideline for determining the learning rates: first, search for the largest possible $\\alpha$; next, tune $\\beta$ based on the chosen value of $\\alpha$.", "code": "https://drive.google.com/file/d/1Seej9xI03F7_2wh4deDTBk_4aFyb2otb/view?usp=sharing", "keywords": ["meta-learning", "convergence"], "paperhash": "takagi|role_of_two_learning_rates_in_convergence_of_modelagnostic_metalearning", "original_pdf": "/attachment/0f5dd5362e2c727d95bd09005ddc337162c4eabb.pdf", "_bibtex": "@misc{\ntakagi2020role,\ntitle={Role of two learning rates in convergence of model-agnostic meta-learning},\nauthor={Shiro Takagi and Yoshihiro Nagano and Yuki Yoshida and Masato Okada},\nyear={2020},\nurl={https://openreview.net/forum?id=r1e8qpVKPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1e8qpVKPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper708/Authors", "ICLR.cc/2020/Conference/Paper708/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper708/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper708/Reviewers", "ICLR.cc/2020/Conference/Paper708/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper708/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper708/Authors|ICLR.cc/2020/Conference/Paper708/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504167423, "tmdate": 1576860534571, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper708/Authors", "ICLR.cc/2020/Conference/Paper708/Reviewers", "ICLR.cc/2020/Conference/Paper708/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper708/-/Official_Comment"}}}, {"id": "rklWoeK8jS", "original": null, "number": 1, "cdate": 1573453976813, "ddate": null, "tcdate": 1573453976813, "tmdate": 1573453976813, "tddate": null, "forum": "r1e8qpVKPS", "replyto": "BygN9L1ycS", "invitation": "ICLR.cc/2020/Conference/Paper708/-/Official_Comment", "content": {"title": "Response 1", "comment": "Dear Reviewer 2,\n\nThank you for your careful review and constructive feedback. We admit that it was inaccurate and misleading to say that \u201cwe derived the necessary condition for MAML to converge to minima\u201d. Considering your feedback, we changed the expression to \u201cwe derived a condition for a simplified MAML to locally converge to local minima from any point in the vicinity of the local minimum\u201d since it is more accurate to explain what we did. We also modified our manuscript in consideration of your other comments. We appreciate your insightful feedback. Though we modified our theory, we think that the implication from our theory itself is still useful and is not reduced by the modifications. We would like to respond to your comment one by one.\n\n\n> The paper assumes that training set is equal to test set and these sets do not change over iterations, which is essentially equivalent to assuming full gradient access; this is different from the stochastic setting of MAML.\n\nAs you point out, we do not consider the stochasticity of the data sampling from the same task, though task during training can be different from that for fine-tuning, or course. We discuss the averaged behavior of a simplified MAML throughout the training in the manuscript. Admitting that stochasticity of the data from the same task matters and it would be better to consider it to derive the condition for a more realistic case, looking at the average case still often tells us important implications as our theory predict the result of the experiment regardless of such a simplification. We believe that our research is a good first step to analyze the relationship between the inner learning rate \\alpha and the meta-learning rate \\beta further. As to the variability between tasks, we will respond later.\n\n\n> The \"necessary condition\" is derived in the case where there is only one task, and there is no discussion on generalizing to some other tasks. This means that the setting is too simplified so that it is not even a meta-learning setup.\n\nAs we mentioned, it was inaccurate to say that we derived the necessary condition for MAML to converge to minima and we changed the expression. We admit that the one-task setting is too simple to call MAML. As you comment, it is not any more multi-task learning. Yet, this simplification allows us to find the relationship between the inner learning rate and meta-learning rate and in the sense, this simplification is useful for researchers to acquire an intuitive understanding of how MAML works.\n\n\n> In the extension to multiple tasks, the paper derives \"sufficient conditions\" to a set of \"necessary conditions\" for convergence of MAML to local minima. This means that, if we consider the sets of events A = {convergence of MAML to local min}, B = {necessary conditions for A}, and C = {sufficient conditions for B}, A is a subset of B and C is a subset of B and nothing can be specified between A and C. In the extreme case, A and C may be even disjoint subsets of B. Thus, at least in theory, the conditions for multiple tasks do not tell us anything about convergence.\n\nWe admit that in the extreme case, A and C may be disjoint. Considering your point out, we altered A = {convergence of MAML to local min} to A\u2019 = {local convergence of a simplified MAML to local min from any point in the vicinity of local minimum}. Therefore, the \u201cnecessary condition\u201d we originally derived is the necessary and sufficient condition and the condition of multi-task setting is now the sufficient condition of A\u2019. Also, since we can talk about multi-task setting thanks to this modification, we are able to consider the variability between different tasks. We think that this grasps the essence of MAML as the conclusion of our theory is consistent with the experimental result. Thank you for your feedback\uff0e\n\n\n> The analysis relies on a number of \"A is approximately equal to B\" arguments without careful handling of errors, e.g., (3) and (4), and Tg = 0.\n\nAs we mentioned before, the \u201ccondition of MAML\u201d was misleading. It is more accurate to call our condition \u201ccondition of a simplified MAML\u201d in consideration of the approximation you pointed out as well. We think that errors from the approximation are negligible at least to investigate the relationship between \\alpha and \\beta since our theory can explain well the result of experiments with the benchmark dataset.\n\n\n> In Section 3.1.2 and eq (12), the paper reparametrizes \\theta to v and analyzes updates on v. How is (12) obtained from the original update rule of \\theta? In fact, the paper analyzes steepest GD but doesn't provide the explicit update rule; for example, is it steepest GD with respect to which norm? It'd be helpful to have the details in the main text.\n\nThank you for your feedback. we added the details in the main text and corrected a mistake too: P(\\theta - \\theta*) \u2014> P^T(\\theta - \\theta*)."}, "signatures": ["ICLR.cc/2020/Conference/Paper708/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper708/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["takagi@mns.k.u-tokyo.ac.jp", "nagano@mns.k.u-tokyo.ac.jp", "yoshida@mns.k.u-tokyo.ac.jp", "okada@edu.k.u-tokyo.ac.jp"], "title": "Role of two learning rates in convergence of model-agnostic meta-learning", "authors": ["Shiro Takagi", "Yoshihiro Nagano", "Yuki Yoshida", "Masato Okada"], "pdf": "/pdf/cdbfdee484753ccb694c6522f860f50c7c0b5838.pdf", "TL;DR": "We analyzed the role of two learning rates in model-agnostic meta-learning in convergence.", "abstract": "Model-agnostic meta-learning (MAML) is known as a powerful meta-learning method. However, MAML is notorious for being hard to train because of the existence of two learning rates. Therefore, in this paper, we derive the conditions that inner learning rate $\\alpha$ and meta-learning rate $\\beta$ must satisfy for MAML to converge to minima with some simplifications. We find that the upper bound of $\\beta$ depends on $ \\alpha$, in contrast to the case of using the normal gradient descent method. Moreover, we show that the threshold of $\\beta$ increases as $\\alpha$ approaches its own upper bound. This result is verified by experiments on various few-shot tasks and architectures; specifically, we perform sinusoid regression and classification of Omniglot and MiniImagenet datasets with a multilayer perceptron and a convolutional neural network. Based on this outcome, we present a guideline for determining the learning rates: first, search for the largest possible $\\alpha$; next, tune $\\beta$ based on the chosen value of $\\alpha$.", "code": "https://drive.google.com/file/d/1Seej9xI03F7_2wh4deDTBk_4aFyb2otb/view?usp=sharing", "keywords": ["meta-learning", "convergence"], "paperhash": "takagi|role_of_two_learning_rates_in_convergence_of_modelagnostic_metalearning", "original_pdf": "/attachment/0f5dd5362e2c727d95bd09005ddc337162c4eabb.pdf", "_bibtex": "@misc{\ntakagi2020role,\ntitle={Role of two learning rates in convergence of model-agnostic meta-learning},\nauthor={Shiro Takagi and Yoshihiro Nagano and Yuki Yoshida and Masato Okada},\nyear={2020},\nurl={https://openreview.net/forum?id=r1e8qpVKPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1e8qpVKPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper708/Authors", "ICLR.cc/2020/Conference/Paper708/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper708/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper708/Reviewers", "ICLR.cc/2020/Conference/Paper708/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper708/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper708/Authors|ICLR.cc/2020/Conference/Paper708/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504167423, "tmdate": 1576860534571, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper708/Authors", "ICLR.cc/2020/Conference/Paper708/Reviewers", "ICLR.cc/2020/Conference/Paper708/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper708/-/Official_Comment"}}}, {"id": "SJloDQiSFB", "original": null, "number": 1, "cdate": 1571300195209, "ddate": null, "tcdate": 1571300195209, "tmdate": 1572972562196, "tddate": null, "forum": "r1e8qpVKPS", "replyto": "r1e8qpVKPS", "invitation": "ICLR.cc/2020/Conference/Paper708/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "In this paper, the authors tackled one important research problem in MAML, i.e., the optimization instability, by investigating the two learning rates. Though I appreciate the theoretic contribution this work makes, I am not sure with the practical significance of it.  Below please find my detailed comments. \n\nPros:\n-\tIn this work, the authors focused on an important problem \u2013 training MAML is kind of unstable and tricky, so that developing guidelines that stabilize MAML or its first-order approximations is of significance. \n-\tThis work theoretically discusses the relationship between the inner loop learning rate and the outer one, under a set of assumptions. \n-\tThe paper is well written and easy to follow.\n\nCons:\n-\tSome of the simplifications for proving are empirical, so that the proof itself is not that rigorous. \n  o\tFor example, In Section 3.1.1, the authors ignored Tg based on an observation that Tg is small. However, even in the Appendix, the authors did not explicate the experimental setting where they reach such a conclusion. Will Tg be always small then, in any task and any dataset?\n-\tThe take-away of this work is not clear, in other words, it makes small contribution to the practical training of MAML. \n  o\tPractically, we often train MAML in 5 or 10 steps instead of only one step. Will the conclusion apply to such setups? Or will the model itself diverge during the inner loop as more steps are taken, provided with a larger value of \\alpha?\n  o\tPractically, we merely use vanilla SGD. The figure in the Appendix shows that the main conclusion actually does not apply to Adam, which disempower the practicability of the theoretical guideline. \n  o\tHow can we search the \u201clargest possible\u201d inner loop learning rate? Should we still use grid-search or heuristic search? In that case, what is the implication/shortcut that this proposed guideline brings? We still reach a stable training process by tuning the hyperparameters in a brute-force fashion.  \n-\tThe baselines should be compared, to support the effectiveness of this proposed algorithm. For example, Behl et al. (2019) automatically tuning the learning rates during training definitely needs to be compared, since both aim to stabilize the training of MAML."}, "signatures": ["ICLR.cc/2020/Conference/Paper708/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper708/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["takagi@mns.k.u-tokyo.ac.jp", "nagano@mns.k.u-tokyo.ac.jp", "yoshida@mns.k.u-tokyo.ac.jp", "okada@edu.k.u-tokyo.ac.jp"], "title": "Role of two learning rates in convergence of model-agnostic meta-learning", "authors": ["Shiro Takagi", "Yoshihiro Nagano", "Yuki Yoshida", "Masato Okada"], "pdf": "/pdf/cdbfdee484753ccb694c6522f860f50c7c0b5838.pdf", "TL;DR": "We analyzed the role of two learning rates in model-agnostic meta-learning in convergence.", "abstract": "Model-agnostic meta-learning (MAML) is known as a powerful meta-learning method. However, MAML is notorious for being hard to train because of the existence of two learning rates. Therefore, in this paper, we derive the conditions that inner learning rate $\\alpha$ and meta-learning rate $\\beta$ must satisfy for MAML to converge to minima with some simplifications. We find that the upper bound of $\\beta$ depends on $ \\alpha$, in contrast to the case of using the normal gradient descent method. Moreover, we show that the threshold of $\\beta$ increases as $\\alpha$ approaches its own upper bound. This result is verified by experiments on various few-shot tasks and architectures; specifically, we perform sinusoid regression and classification of Omniglot and MiniImagenet datasets with a multilayer perceptron and a convolutional neural network. Based on this outcome, we present a guideline for determining the learning rates: first, search for the largest possible $\\alpha$; next, tune $\\beta$ based on the chosen value of $\\alpha$.", "code": "https://drive.google.com/file/d/1Seej9xI03F7_2wh4deDTBk_4aFyb2otb/view?usp=sharing", "keywords": ["meta-learning", "convergence"], "paperhash": "takagi|role_of_two_learning_rates_in_convergence_of_modelagnostic_metalearning", "original_pdf": "/attachment/0f5dd5362e2c727d95bd09005ddc337162c4eabb.pdf", "_bibtex": "@misc{\ntakagi2020role,\ntitle={Role of two learning rates in convergence of model-agnostic meta-learning},\nauthor={Shiro Takagi and Yoshihiro Nagano and Yuki Yoshida and Masato Okada},\nyear={2020},\nurl={https://openreview.net/forum?id=r1e8qpVKPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "r1e8qpVKPS", "replyto": "r1e8qpVKPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper708/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper708/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574962776505, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper708/Reviewers"], "noninvitees": [], "tcdate": 1570237748238, "tmdate": 1574962776518, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper708/-/Official_Review"}}}, {"id": "rJx4PXPnFr", "original": null, "number": 2, "cdate": 1571742555905, "ddate": null, "tcdate": 1571742555905, "tmdate": 1572972562161, "tddate": null, "forum": "r1e8qpVKPS", "replyto": "r1e8qpVKPS", "invitation": "ICLR.cc/2020/Conference/Paper708/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I do not know much about this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "The authors study a method to help tuning the two learning rates used in the MAML training algorithm. First, they derive a necessary condition for the convergence of the gradient descent in the single task setting. The condition relies on the eigenvalues of the Hessian of the task loss. This condition is reminiscent of convergence criteria for gradient descent on quadratic objectives, and the authors make the interesting observation that the criteria for the exterior learning rate beta depends on the interior learning rate alpha. \nHowever, in the setting of interest that is the multitask learning, the trick used to analyse the eigenvalues doesn\u2019t work anymore. To circumvent this issue, the authors provides a sufficient condition for the multitask equivalent of the necessary condition to hold.\n\nThe derivations are correct (few typos, see nitpick below) and the paper is nicely presented. I\u2019m not a MAML expert so I\u2019ll let the other reviewers judge how this paper compares to the current literature. However, I think that the empirical work can be pushed further. The experiments on Omniglot and MiniImagenet are coherent with the theory, but I am not completely sure of their impact. Indeed, the learning rate domain on which MAML converge seems coherent with what the authors are predicting, but it doesn\u2019t allow us to choose the learning rates before training the model. It doesn\u2019t help that the criteria relies on the spectrum of the Hessian evaluated at a critical point, which of course is not known before convergence in non quadratic setting.\nThe paper would be way more convincing if the authors could use their findings to describe some more precise heuristics to tune the learning rates, and conduct a proper comparison of its performance with other methods.\n\nAs a result, I think the paper is exploring an interesting direction, but that the empirical work might be too preliminary for publication.\n\nNitpick:\n\n- Notations in 2.2 are a bit confused.The gradient of L wrt theta is denoted first with nabla, then with partial derivatives. Nabla is then used to denote the Jacobian of theta\u2019 wrt theta. There is a small mistake in the 4th line of 2.2. Chain rule for gradient of L wrt theta\u2019 does not give (d L / d theta) times gradient but instead give gradient times (d L / d theta)^T. The error is fixed when passing from (3) to (4) so it doesn\u2019t impact the paper.\n- Also, a transpose is missing in the taylor expansion on page 3.\n- \"A multilayer perceptron with two hidden units of size 40\" -> layers?\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper708/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper708/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["takagi@mns.k.u-tokyo.ac.jp", "nagano@mns.k.u-tokyo.ac.jp", "yoshida@mns.k.u-tokyo.ac.jp", "okada@edu.k.u-tokyo.ac.jp"], "title": "Role of two learning rates in convergence of model-agnostic meta-learning", "authors": ["Shiro Takagi", "Yoshihiro Nagano", "Yuki Yoshida", "Masato Okada"], "pdf": "/pdf/cdbfdee484753ccb694c6522f860f50c7c0b5838.pdf", "TL;DR": "We analyzed the role of two learning rates in model-agnostic meta-learning in convergence.", "abstract": "Model-agnostic meta-learning (MAML) is known as a powerful meta-learning method. However, MAML is notorious for being hard to train because of the existence of two learning rates. Therefore, in this paper, we derive the conditions that inner learning rate $\\alpha$ and meta-learning rate $\\beta$ must satisfy for MAML to converge to minima with some simplifications. We find that the upper bound of $\\beta$ depends on $ \\alpha$, in contrast to the case of using the normal gradient descent method. Moreover, we show that the threshold of $\\beta$ increases as $\\alpha$ approaches its own upper bound. This result is verified by experiments on various few-shot tasks and architectures; specifically, we perform sinusoid regression and classification of Omniglot and MiniImagenet datasets with a multilayer perceptron and a convolutional neural network. Based on this outcome, we present a guideline for determining the learning rates: first, search for the largest possible $\\alpha$; next, tune $\\beta$ based on the chosen value of $\\alpha$.", "code": "https://drive.google.com/file/d/1Seej9xI03F7_2wh4deDTBk_4aFyb2otb/view?usp=sharing", "keywords": ["meta-learning", "convergence"], "paperhash": "takagi|role_of_two_learning_rates_in_convergence_of_modelagnostic_metalearning", "original_pdf": "/attachment/0f5dd5362e2c727d95bd09005ddc337162c4eabb.pdf", "_bibtex": "@misc{\ntakagi2020role,\ntitle={Role of two learning rates in convergence of model-agnostic meta-learning},\nauthor={Shiro Takagi and Yoshihiro Nagano and Yuki Yoshida and Masato Okada},\nyear={2020},\nurl={https://openreview.net/forum?id=r1e8qpVKPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "r1e8qpVKPS", "replyto": "r1e8qpVKPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper708/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper708/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574962776505, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper708/Reviewers"], "noninvitees": [], "tcdate": 1570237748238, "tmdate": 1574962776518, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper708/-/Official_Review"}}}, {"id": "BygN9L1ycS", "original": null, "number": 3, "cdate": 1571907212079, "ddate": null, "tcdate": 1571907212079, "tmdate": 1572972562126, "tddate": null, "forum": "r1e8qpVKPS", "replyto": "r1e8qpVKPS", "invitation": "ICLR.cc/2020/Conference/Paper708/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper studies the two learning rates \\alpha and \\beta used in Model-Agnostic Meta-Learning (MAML) algorithm by [Finn et al., 2017]. MAML is known to be difficult to train, and part of the reason why is the need to tune the two learning rates. Under simplifications, the paper derives some necessary conditions on \\alpha and \\beta for the MAML iterates to converge to local minima, and then verifies the theory by experiments on synthetic and real-world data.\n\nOverall, I think this paper should be rejected. In my opinion, this paper analyzes a simplified setting which is far from the original MAML setting and yet derives conditions that are not very meaningful or useful. Although it is interesting to see some match between theory and experiments (especially Fig 4), the theory part seems to have room for improvement, and I will detail the reasons in the following.\n\nI believe that the analysis is done on an overly simplified setting and easily breaks without such simplifications.\n- The paper assumes that training set is equal to test set and these sets do not change over iterations, which is essentially equivalent to assuming full gradient access; this is different from the stochastic setting of MAML. \n- The \"necessary condition\" is derived in the case where there is only one task, and there is no discussion on generalizing to some other tasks. This means that the setting is too simplified so that it is not even a meta-learning setup.\n- In the extension to multiple tasks, the paper derives \"sufficient conditions\" to a set of \"necessary conditions\" for convergence of MAML to local minima. This means that, if we consider the sets of events A = {convergence of MAML to local min}, B = {necessary conditions for A}, and C = {sufficient conditions for B}, A is a subset of B and C is a subset of B and nothing can be specified between A and C. In the extreme case, A and C may be even disjoint subsets of B. Thus, at least in theory, the conditions for multiple tasks do not tell us anything about convergence.\n- The analysis relies on a number of \"A is approximately equal to B\" arguments without careful handling of errors, e.g., (3) and (4), and Tg = 0.\n\nI also have concerns about the correctness of the analysis in the single task case.\n- In Section 3.1.2 and eq (12), the paper reparametrizes \\theta to v and analyzes updates on v. How is (12) obtained from the original update rule of \\theta? In fact, the paper analyzes steepest GD but doesn't provide the explicit update rule; for example, is it steepest GD with respect to which norm? It'd be helpful to have the details in the main text.\n- Even if (12) is true, there are pathological cases lying in a set of measure zero, where (12) converges even when (13) is not satisfied. For simplicity, consider v(t+1) = G v(t), where G is a real symmetric matrix. Assume all but one eigenvalue \\lambda_1 are greater than 1 and |\\lambda_1| < 1. Let v_1 be the corresponding eigenvalue for \\lambda_1. Then, if v(0) = v_1, the sequence v(t) converges to zero. This means that the \"necessary condition\" derived in (16) may not actually be a necessary condition for convergence.\n\nThere are also some points in the main text that doesn't describe the setting clearly; if the reader has no prior knowledge of MAML algorithm, they may get confused.\n- In the update rule (1) and (2), it'd be better to mention that \\nabla_\\theta L_\\tau (\\theta) is an *estimate* of the true gradient using the training data and test data, respectively. In their current status, the gradients in (1) and (2) will read as the full gradients of the population where the training/test data points are sampled from.\n- Section 2.1 only introduces the case where the update (1) is done only once in the inner loop. However, later Section 2.2 and Section 3, the paper says \"only one step is taken for update...\". Without the prior knowledge that there are versions in which multiple updates (1) are done, the readers can easily get confused.\n\nMinor comments\n- The word \"minima\" used throughout should better be corrected to \"local minima,\" as mere minima may be understood as \"global minima.\"\n- In the abstract, there is a phrase \"in contrast to the case of using the normal gradient descent method.\" Which setting do you mean by the \"normal gradient descent\"?\n- After equations (3) and (4), the paper says \"The above is known as the first-order approximation...\", but as far as I'm concerned, the first-order approximation version of MAML is (3) without the hessian term, not (4). This statement can potentially be misleading.\n- Column vectors / row vectors are mixed up. In (3), the partial derivatives are row vectors, but in (4) g_\\tau(\\theta)'s are column vectors. However, right above (3), g_\\tau(\\theta) is a row vector this time.\n- Did (5) come from the approximation (4)? If so, as \\tilde L(\\theta) is defined to be L(\\theta'), the two sides of (5) are \"approximately equal\" not \"equal.\"\n- In Eq (19): P_\\tau (\\theta - \\theta^*) -> (\\theta-\\theta^*).\n- At the end of page 6, what do you mean by \"not a sum but a mean\"? Isn't sum of n things equal to the mean, except for a factor of 1/n?"}, "signatures": ["ICLR.cc/2020/Conference/Paper708/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper708/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["takagi@mns.k.u-tokyo.ac.jp", "nagano@mns.k.u-tokyo.ac.jp", "yoshida@mns.k.u-tokyo.ac.jp", "okada@edu.k.u-tokyo.ac.jp"], "title": "Role of two learning rates in convergence of model-agnostic meta-learning", "authors": ["Shiro Takagi", "Yoshihiro Nagano", "Yuki Yoshida", "Masato Okada"], "pdf": "/pdf/cdbfdee484753ccb694c6522f860f50c7c0b5838.pdf", "TL;DR": "We analyzed the role of two learning rates in model-agnostic meta-learning in convergence.", "abstract": "Model-agnostic meta-learning (MAML) is known as a powerful meta-learning method. However, MAML is notorious for being hard to train because of the existence of two learning rates. Therefore, in this paper, we derive the conditions that inner learning rate $\\alpha$ and meta-learning rate $\\beta$ must satisfy for MAML to converge to minima with some simplifications. We find that the upper bound of $\\beta$ depends on $ \\alpha$, in contrast to the case of using the normal gradient descent method. Moreover, we show that the threshold of $\\beta$ increases as $\\alpha$ approaches its own upper bound. This result is verified by experiments on various few-shot tasks and architectures; specifically, we perform sinusoid regression and classification of Omniglot and MiniImagenet datasets with a multilayer perceptron and a convolutional neural network. Based on this outcome, we present a guideline for determining the learning rates: first, search for the largest possible $\\alpha$; next, tune $\\beta$ based on the chosen value of $\\alpha$.", "code": "https://drive.google.com/file/d/1Seej9xI03F7_2wh4deDTBk_4aFyb2otb/view?usp=sharing", "keywords": ["meta-learning", "convergence"], "paperhash": "takagi|role_of_two_learning_rates_in_convergence_of_modelagnostic_metalearning", "original_pdf": "/attachment/0f5dd5362e2c727d95bd09005ddc337162c4eabb.pdf", "_bibtex": "@misc{\ntakagi2020role,\ntitle={Role of two learning rates in convergence of model-agnostic meta-learning},\nauthor={Shiro Takagi and Yoshihiro Nagano and Yuki Yoshida and Masato Okada},\nyear={2020},\nurl={https://openreview.net/forum?id=r1e8qpVKPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "r1e8qpVKPS", "replyto": "r1e8qpVKPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper708/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper708/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574962776505, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper708/Reviewers"], "noninvitees": [], "tcdate": 1570237748238, "tmdate": 1574962776518, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper708/-/Official_Review"}}}], "count": 13}