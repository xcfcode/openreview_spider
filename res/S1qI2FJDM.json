{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1528124444213, "tcdate": 1518472274229, "number": 316, "cdate": 1518472274229, "id": "S1qI2FJDM", "invitation": "ICLR.cc/2018/Workshop/-/Submission", "forum": "S1qI2FJDM", "signatures": ["~Siyue_Wang1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop"], "content": {"title": "Defending DNN Adversarial Attacks with Pruning and Logits Augmentation", "abstract": "Deep neural networks (DNNs) have been shown to be powerful models and perform extremely well on many complicated artificial intelligent tasks. However, recent research found that these powerful models are vulnerable to adversarial attacks, i.e., intentionally added imperceptible perturbations to DNN inputs can easily mislead the DNNs with extremely high confidence. In this work, we enhance the robustness of DNNs under adversarial attacks by using pruning method and logits augmentation, therefore, we achieve both higher defense against adversarial examples and more compressed DNN models. We have observed defense against adversarial attacks under the white box attack assumption. Our defense mechanisms work even better under the black box attack assumption.\n", "paperhash": "ye|defending_dnn_adversarial_attacks_with_pruning_and_logits_augmentation", "keywords": ["Adversarial Attacks", "Neural Network Security", "Weight Pruning", "Logits Augmentation"], "_bibtex": "@misc{\n  ye2018defending,\n  title={Defending DNN Adversarial Attacks with Pruning and Logits Augmentation},\n  author={Shaokai Ye and Siyue Wang and Xiao Wang and Bo Yuan and Wujie Wen and Xue Lin},\n  year={2018},\n  url={https://openreview.net/forum?id=S1qI2FJDM}\n}", "authorids": ["sy106@syr.edu", "wang.siy@husky.neu.edu", "kxw@bu.edu", "byuan@ccny.cuny.edu", "wwen@fiu.edu", "xue.lin@northeastern.edu"], "authors": ["Shaokai Ye", "Siyue Wang", "Xiao Wang", "Bo Yuan", "Wujie Wen", "Xue Lin"], "TL;DR": "Defending DNN Adversarial Attacks with Pruning and Logits Augmentation", "pdf": "/pdf/1d0856cbe8336f83e68bd7bc08f90b976bc5b622.pdf"}, "nonreaders": [], "details": {"replyCount": 3, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1518472800000, "tmdate": 1518474081690, "id": "ICLR.cc/2018/Workshop/-/Submission", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Workshop"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2018/Workshop", "description": "Your authorized identity to be associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 9, "value-regex": "upload", "description": "Upload a PDF file that ends with .pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 8, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names. Please provide real names; identities will be anonymized."}, "keywords": {"order": 6, "values-regex": "(^$)|[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of keywords."}, "TL;DR": {"required": false, "order": 7, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,500}"}, "authorids": {"required": true, "order": 3, "values-regex": "([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,},){0,}([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,})", "description": "Comma separated list of author email addresses, lowercased, in the same order as above. For authors with existing OpenReview accounts, please make sure that the provided email address(es) match those listed in the author's profile. Please provide real emails; identities will be anonymized."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1526248800000, "cdate": 1518474081690}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582932521, "tcdate": 1520314216352, "number": 1, "cdate": 1520314216352, "id": "SyedPisdz", "invitation": "ICLR.cc/2018/Workshop/-/Paper316/Official_Review", "forum": "S1qI2FJDM", "replyto": "S1qI2FJDM", "signatures": ["ICLR.cc/2018/Workshop/Paper316/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper316/AnonReviewer1"], "content": {"title": "review", "rating": "4: Ok but not good enough - rejection", "review": "The paper presents two approaches to makes models robust to adversarial attacks.\nThe first one consider pruning model weights similarly to what was proposed in Han et al (2015). The second one consider multiplying the weights of the last layer by a large constant (they call this \"logits augmentation\"). I don't understand how is this supposed to help. There's no mathematical explanation of what is happening when we do so.\nOverall, results on adversarial examples seem better, but no results are given on clean examples (a sentence in the paper suggest results are not worse than the baseline, but a table would help).\nOverall, two ideas are proposed but no explanation are given to why they should work, and how they compare to other known defenses.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Defending DNN Adversarial Attacks with Pruning and Logits Augmentation", "abstract": "Deep neural networks (DNNs) have been shown to be powerful models and perform extremely well on many complicated artificial intelligent tasks. However, recent research found that these powerful models are vulnerable to adversarial attacks, i.e., intentionally added imperceptible perturbations to DNN inputs can easily mislead the DNNs with extremely high confidence. In this work, we enhance the robustness of DNNs under adversarial attacks by using pruning method and logits augmentation, therefore, we achieve both higher defense against adversarial examples and more compressed DNN models. We have observed defense against adversarial attacks under the white box attack assumption. Our defense mechanisms work even better under the black box attack assumption.\n", "paperhash": "ye|defending_dnn_adversarial_attacks_with_pruning_and_logits_augmentation", "keywords": ["Adversarial Attacks", "Neural Network Security", "Weight Pruning", "Logits Augmentation"], "_bibtex": "@misc{\n  ye2018defending,\n  title={Defending DNN Adversarial Attacks with Pruning and Logits Augmentation},\n  author={Shaokai Ye and Siyue Wang and Xiao Wang and Bo Yuan and Wujie Wen and Xue Lin},\n  year={2018},\n  url={https://openreview.net/forum?id=S1qI2FJDM}\n}", "authorids": ["sy106@syr.edu", "wang.siy@husky.neu.edu", "kxw@bu.edu", "byuan@ccny.cuny.edu", "wwen@fiu.edu", "xue.lin@northeastern.edu"], "authors": ["Shaokai Ye", "Siyue Wang", "Xiao Wang", "Bo Yuan", "Wujie Wen", "Xue Lin"], "TL;DR": "Defending DNN Adversarial Attacks with Pruning and Logits Augmentation", "pdf": "/pdf/1d0856cbe8336f83e68bd7bc08f90b976bc5b622.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582932326, "id": "ICLR.cc/2018/Workshop/-/Paper316/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper316/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper316/AnonReviewer1", "ICLR.cc/2018/Workshop/Paper316/AnonReviewer3"], "reply": {"forum": "S1qI2FJDM", "replyto": "S1qI2FJDM", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper316/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper316/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582932326}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582818845, "tcdate": 1520613041417, "number": 2, "cdate": 1520613041417, "id": "SkF3LEeYG", "invitation": "ICLR.cc/2018/Workshop/-/Paper316/Official_Review", "forum": "S1qI2FJDM", "replyto": "S1qI2FJDM", "signatures": ["ICLR.cc/2018/Workshop/Paper316/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper316/AnonReviewer3"], "content": {"title": "Pruning makes neural networks more robust", "rating": "6: Marginally above acceptance threshold", "review": "This paper presents two somewhat new ideas for defending against adversarial input attacks on image classification neural networks models.  They compress the model using the pruning technique from Han et. al. 2015, and they modify the weights in the last fully connected layer by a linear multiplier.  They show that the first idea on its own, can defend well against targeted FGSM and BIM on both MNIST and CIFAR10.  The combination of the two methods defends well against untargeted FGSM.  While neither of their techniques performs well against the attack from Carlini and Wagner (CW).  \n\nThe paper has some grammatical problems, but is relatively easy to read and understand.\n\nPros\n\t- Work on a topic which is very hot right now\n\t- Two new defense mechanisms that can be added to (and possibly combined with) the existing tool chest of defenses\n\t- Strong performance against some adversaries\n\nCons:\n\t- neither idea is super novel.  The pruning idea is somewhat similar to defensive distillation and the logits augmentation is very similar to gradient inhibition from Liu 2018\n\t- they do not compare against existing defense mechanisms.  As the effectiveness of defenses depends on lot on detailed choices in the attack it's hard to know for sure how other defenses would perform in their setup, making it difficult to see how their technique fares\n\nAll in all, I think it's interesting that network pruning alone performs so well as a defense, and I think others in the community will find this result interesting as well, so I while  I think the paper is not super strong, I do believe it's above the bar for acceptance.\n", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Defending DNN Adversarial Attacks with Pruning and Logits Augmentation", "abstract": "Deep neural networks (DNNs) have been shown to be powerful models and perform extremely well on many complicated artificial intelligent tasks. However, recent research found that these powerful models are vulnerable to adversarial attacks, i.e., intentionally added imperceptible perturbations to DNN inputs can easily mislead the DNNs with extremely high confidence. In this work, we enhance the robustness of DNNs under adversarial attacks by using pruning method and logits augmentation, therefore, we achieve both higher defense against adversarial examples and more compressed DNN models. We have observed defense against adversarial attacks under the white box attack assumption. Our defense mechanisms work even better under the black box attack assumption.\n", "paperhash": "ye|defending_dnn_adversarial_attacks_with_pruning_and_logits_augmentation", "keywords": ["Adversarial Attacks", "Neural Network Security", "Weight Pruning", "Logits Augmentation"], "_bibtex": "@misc{\n  ye2018defending,\n  title={Defending DNN Adversarial Attacks with Pruning and Logits Augmentation},\n  author={Shaokai Ye and Siyue Wang and Xiao Wang and Bo Yuan and Wujie Wen and Xue Lin},\n  year={2018},\n  url={https://openreview.net/forum?id=S1qI2FJDM}\n}", "authorids": ["sy106@syr.edu", "wang.siy@husky.neu.edu", "kxw@bu.edu", "byuan@ccny.cuny.edu", "wwen@fiu.edu", "xue.lin@northeastern.edu"], "authors": ["Shaokai Ye", "Siyue Wang", "Xiao Wang", "Bo Yuan", "Wujie Wen", "Xue Lin"], "TL;DR": "Defending DNN Adversarial Attacks with Pruning and Logits Augmentation", "pdf": "/pdf/1d0856cbe8336f83e68bd7bc08f90b976bc5b622.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582932326, "id": "ICLR.cc/2018/Workshop/-/Paper316/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper316/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper316/AnonReviewer1", "ICLR.cc/2018/Workshop/Paper316/AnonReviewer3"], "reply": {"forum": "S1qI2FJDM", "replyto": "S1qI2FJDM", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper316/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper316/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582932326}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521573588344, "tcdate": 1521573588344, "number": 194, "cdate": 1521573588006, "id": "SJ3A0CRFf", "invitation": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "forum": "S1qI2FJDM", "replyto": "S1qI2FJDM", "signatures": ["ICLR.cc/2018/Workshop/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Program_Chairs"], "content": {"decision": "Reject", "title": "ICLR 2018 Workshop Acceptance Decision", "comment": "Based on the reviews, this paper has not been accepted for presentation at the ICLR workshop. However, the conversation and updates can continue to appear here on OpenReview."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Defending DNN Adversarial Attacks with Pruning and Logits Augmentation", "abstract": "Deep neural networks (DNNs) have been shown to be powerful models and perform extremely well on many complicated artificial intelligent tasks. However, recent research found that these powerful models are vulnerable to adversarial attacks, i.e., intentionally added imperceptible perturbations to DNN inputs can easily mislead the DNNs with extremely high confidence. In this work, we enhance the robustness of DNNs under adversarial attacks by using pruning method and logits augmentation, therefore, we achieve both higher defense against adversarial examples and more compressed DNN models. We have observed defense against adversarial attacks under the white box attack assumption. Our defense mechanisms work even better under the black box attack assumption.\n", "paperhash": "ye|defending_dnn_adversarial_attacks_with_pruning_and_logits_augmentation", "keywords": ["Adversarial Attacks", "Neural Network Security", "Weight Pruning", "Logits Augmentation"], "_bibtex": "@misc{\n  ye2018defending,\n  title={Defending DNN Adversarial Attacks with Pruning and Logits Augmentation},\n  author={Shaokai Ye and Siyue Wang and Xiao Wang and Bo Yuan and Wujie Wen and Xue Lin},\n  year={2018},\n  url={https://openreview.net/forum?id=S1qI2FJDM}\n}", "authorids": ["sy106@syr.edu", "wang.siy@husky.neu.edu", "kxw@bu.edu", "byuan@ccny.cuny.edu", "wwen@fiu.edu", "xue.lin@northeastern.edu"], "authors": ["Shaokai Ye", "Siyue Wang", "Xiao Wang", "Bo Yuan", "Wujie Wen", "Xue Lin"], "TL;DR": "Defending DNN Adversarial Attacks with Pruning and Logits Augmentation", "pdf": "/pdf/1d0856cbe8336f83e68bd7bc08f90b976bc5b622.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518629844880, "id": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Program_Chairs"], "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Workshop/-/Submission", "writers": {"values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Workshop/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Workshop Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept", "Reject"]}}}, "nonreaders": [], "noninvitees": [], "cdate": 1518629844880}}}], "count": 4}