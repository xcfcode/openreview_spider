{"notes": [{"id": "k2Om84I9JuX", "original": "cIH2zUeSQFn", "number": 543, "cdate": 1601308066966, "ddate": null, "tcdate": 1601308066966, "tmdate": 1614985692552, "tddate": null, "forum": "k2Om84I9JuX", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Descending through a Crowded Valley \u2014 Benchmarking Deep Learning Optimizers", "authorids": ["~Robin_Marc_Schmidt1", "~Frank_Schneider1", "~Philipp_Hennig1"], "authors": ["Robin Marc Schmidt", "Frank Schneider", "Philipp Hennig"], "keywords": ["Deep learning", "optimizers", "benchmark"], "abstract": "Choosing the optimizer is considered to be among the most crucial design decisions in deep learning, and it is not an easy one. The growing literature now lists hundreds of optimization methods. In the absence of clear theoretical guidance and conclusive empirical evidence, the decision is often made based on anecdotes. In this work, we aim to replace these anecdotes, if not with a conclusive ranking, then at least with evidence-backed heuristics. To do so, we perform an extensive, standardized benchmark of more than a dozen particularly popular deep learning optimizers while giving a concise overview of the wide range of possible choices. Analyzing almost $35,000$ individual runs, we contribute the following three points: (i) Optimizer performance varies greatly across tasks. (ii) We observe that evaluating multiple optimizers with default parameters works approximately as well as tuning the hyperparameters of a single, fixed optimizer. (iii) While we can not discern an optimization method clearly dominating across all tested tasks, we identify a significantly reduced subset of specific algorithms and parameter choices that generally lead to competitive results in our experiments. This subset includes popular favorites and some lesser-known contenders. We have open-sourced all our experimental results, making them directly available as challenging and well-tuned baselines. This allows for more meaningful comparisons when evaluating novel optimization methods without requiring any further computational efforts.", "one-sentence_summary": "A large-scale deep learning optimizer benchmark with open-sourced results for more meaningful optimizer comparisons.", "pdf": "/pdf/c1dd51b5d487496f707a9957cdd518288dd529e4.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "schmidt|descending_through_a_crowded_valley_benchmarking_deep_learning_optimizers", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=NKAcbIbIMn", "_bibtex": "@misc{\nschmidt2021descending,\ntitle={Descending through a Crowded Valley {\\textemdash} Benchmarking Deep Learning Optimizers},\nauthor={Robin Marc Schmidt and Frank Schneider and Philipp Hennig},\nyear={2021},\nurl={https://openreview.net/forum?id=k2Om84I9JuX}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 19, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "53l9rxmWRfr", "original": null, "number": 1, "cdate": 1610040455923, "ddate": null, "tcdate": 1610040455923, "tmdate": 1610474058576, "tddate": null, "forum": "k2Om84I9JuX", "replyto": "k2Om84I9JuX", "invitation": "ICLR.cc/2021/Conference/Paper543/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "Contributions of this type are very important for the community. There is a great deal of confusion among practitioners about how to pick optimizers. Perhaps worse, there is confusion among optimization researchers about how to demonstrate the effectiveness of their novel algorithms on deep learning tasks. I applaud this paper as one of the best attempts to make sense of this confusion.\n\nUnfortunately, I am recommending that it is rejected. This was an extremely difficult decision. This paper was very thoroughly discussed by reviewers, both with the authors and after the feedback phase. I agree with R4 that this paper is exemplary in terms of its breadth of optimizer choices. I also agree with R3 that this paper's choices regarding hyperparameter search spaces and seed fixing significantly diminish the contribution of the paper at hand. The key issue that persuaded my decision centered on whether the paper's evidence supported its conclusions.\n\nThe two key conclusions that I want to highlight are:\n\n1. *evaluating multiple optimizers with default parameters works approximately as well as tuning the hyperparameters of a single, fixed optimizer*\n\n2. *different optimizers exhibit a surprisingly similar performance distribution compared to a single method that is re-tuned or simply re-run with different random seeds*\n\nThese conclusions can only be supported if optimizers are well-tuned. Based on R3's remarks and a quick reading of the paper, I am concerned that the use of fixed search spaces means that these optimizers cannot be considered well-tuned. This concern splits into two sub-concerns.\n\n1. I appreciate the author's desire to encode \"no prior knowledge about well-working hyperparameter values\". Unfortunately, I don't think this is realistic or possible. The learning rate range used in this paper did not include 1e100 for good reasons, all of which depend on the prior knowledge of our community. This isn't just a glib concern, the apparently neutral search spaces may bias the conclusions towards well-known methods whose hyperparameters are well-understood.\n\n2. I am also skeptical of the choice to use the same range for hyperparameters with \"similar naming\". The reason is that these hyperparameters *may have been misnamed by the inventors* and may, in fact, play very different roles in the dynamics of optimization.\n\nTop-line conclusions have a way of becoming memes in our community. Therefore, it is critical that conclusions, as stated, are actually supported by the experimental design and the empirical evidence. Unfortunately, I am not confident that this is is the case for the paper at hand.\n\nIt is clear that this paper represents a heroic effort by the authors. I am aware of the challenges involved in getting this type of paper published and of the urgent need for them. I hope that the authors address the concerns that I expressed and the concerns of the reviewers in a future submission."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Descending through a Crowded Valley \u2014 Benchmarking Deep Learning Optimizers", "authorids": ["~Robin_Marc_Schmidt1", "~Frank_Schneider1", "~Philipp_Hennig1"], "authors": ["Robin Marc Schmidt", "Frank Schneider", "Philipp Hennig"], "keywords": ["Deep learning", "optimizers", "benchmark"], "abstract": "Choosing the optimizer is considered to be among the most crucial design decisions in deep learning, and it is not an easy one. The growing literature now lists hundreds of optimization methods. In the absence of clear theoretical guidance and conclusive empirical evidence, the decision is often made based on anecdotes. In this work, we aim to replace these anecdotes, if not with a conclusive ranking, then at least with evidence-backed heuristics. To do so, we perform an extensive, standardized benchmark of more than a dozen particularly popular deep learning optimizers while giving a concise overview of the wide range of possible choices. Analyzing almost $35,000$ individual runs, we contribute the following three points: (i) Optimizer performance varies greatly across tasks. (ii) We observe that evaluating multiple optimizers with default parameters works approximately as well as tuning the hyperparameters of a single, fixed optimizer. (iii) While we can not discern an optimization method clearly dominating across all tested tasks, we identify a significantly reduced subset of specific algorithms and parameter choices that generally lead to competitive results in our experiments. This subset includes popular favorites and some lesser-known contenders. We have open-sourced all our experimental results, making them directly available as challenging and well-tuned baselines. This allows for more meaningful comparisons when evaluating novel optimization methods without requiring any further computational efforts.", "one-sentence_summary": "A large-scale deep learning optimizer benchmark with open-sourced results for more meaningful optimizer comparisons.", "pdf": "/pdf/c1dd51b5d487496f707a9957cdd518288dd529e4.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "schmidt|descending_through_a_crowded_valley_benchmarking_deep_learning_optimizers", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=NKAcbIbIMn", "_bibtex": "@misc{\nschmidt2021descending,\ntitle={Descending through a Crowded Valley {\\textemdash} Benchmarking Deep Learning Optimizers},\nauthor={Robin Marc Schmidt and Frank Schneider and Philipp Hennig},\nyear={2021},\nurl={https://openreview.net/forum?id=k2Om84I9JuX}\n}"}, "tags": [], "invitation": {"reply": {"forum": "k2Om84I9JuX", "replyto": "k2Om84I9JuX", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040455910, "tmdate": 1610474058561, "id": "ICLR.cc/2021/Conference/Paper543/-/Decision"}}}, {"id": "uwB9TctXzfq", "original": null, "number": 2, "cdate": 1603834704081, "ddate": null, "tcdate": 1603834704081, "tmdate": 1606796734191, "tddate": null, "forum": "k2Om84I9JuX", "replyto": "k2Om84I9JuX", "invitation": "ICLR.cc/2021/Conference/Paper543/-/Official_Review", "content": {"title": "Review of \"Descending through a Crowded Valley \u2014 Benchmarking Deep Learning Optimizers\"", "review": "Summary:\nThis paper benchmarks popular optimizers for training neural networks. The experiments consider all possible combinations of 3 different tuning budgets, and 4 different fixed learning rate schedules on 8 deep learning workloads for 14 optimizers. The paper highlights two main observations: 1) there is no clear dominating optimizer, and 2) selecting from a pool of optimizers with their default parameters is often as good as tuning a fixed optimizer.\nThe main contribution of this work is the open-sourced experiment results for a multitude of cases which can serve as a baseline for future research in optimizers for deep learning.\n\nStrengths:\n- The biggest strength of this work is that all the details regarding the benchmarking protocol are presented and justified. In addition to this, the caveats of the protocol are stated explicitly. The explicit and transparent nature of this work can help practitioners make more informed decisions, and prevent them from misinterpreting the results to something more than what is presented.\n- To my knowledge, this is the first paper to compare a large number of optimizers, selected based on popularity in the research community.\n- The problems considered are of varying difficulty, and includes tasks other than image classification. \n- Different levels of tuning budgets are considered, with the smallest budget being 1 trial (evaluating the default setting), and the largest being 50 trials.\n\nWeaknesses\n- The current tuning procedure is unstable. Tuning with the seed fixed is like optimizing for the specific seed. Furthermore, the hyperparameters that produce the best performance for a specific seed tends to be more unstable (which the authors agree to in appendix C); evaluating such an unstable setting on different seeds unnecessarily penalizes the optimizer. What makes more sense to me is to tune with the same number of trials, with each trial having a different seed, and using bootstrapping to compute the statistics (mean, standard deviation, etc). What we want to see with the tuning experiments is how well the optimizer can do (as an upper bound), and how stable it is. With the current approach, it\u2019s hard to observe the best performance, at which point, I\u2019m not sure how meaningful the error measurements are. All in all, I think it\u2019s more meaningful to show how varying the best optimizer performance can be when tuning with a different set of seeds (since everyone uses different seeds), than to show the variance of a specific set of hyperparameters that is most likely unstable, on many seeds.\n- I believe the experiments lack results for the \u201cwell-tuned\u201d case. The optimizers all use a fixed hyperparameter search range for all problems, which can\u2019t be competitive over different tasks of varying difficulty. I understand that this study assumes the model practitioner to be someone who doesn\u2019t have prior knowledge about the optimizer, let alone the search ranges. However, I think it\u2019s reasonable to believe that a practitioner would try to verify the search range by testing some hyperparameter values before committing 25 or 50 trials to the search range. Likewise, it would be useful to see results with a more calibrated search space per test problem. This can be done with the 50 trial budget by, for example, using 25 on a wide search space, and the other 25 on a more refined search space. At the very least, it would be useful to see the performance vs hyperparameter value plotted for the existing experiments to see whether the ranges could have been trivially improved (for example, if the performance tends to increase/decrease with the learning rate, but the best performance lied on the boundary of the search space, the range could have been shifted). This sort of tuning procedure is not unknown in the community. See [1, 2, 3].\n\nCurrently, I think the weaknesses outweigh the strengths of the paper. It is my understanding that optimizer comparisons should be done between reasonably good versions of the optimizers, and I think better versions of the optimizers could have been presented with a different methodology, given the same computational budget.\n \n\n\n[1] Wilson, Ashia C., et al. \"The marginal value of adaptive gradient methods in machine learning.\" Advances in neural information processing systems. 2017.\n\n[2] Shallue, Christopher J., et al. \"Measuring the effects of data parallelism on neural network training.\" arXiv preprint arXiv:1811.03600 (2018).\n\n[3] Choi, Dami, et al. \"On empirical comparisons of optimizers for deep learning.\" arXiv preprint arXiv:1910.05446 (2019).\n\n\nUpdate:\n\nI have read over the changes made by the authors, and also the other reviewer\u2019s responses. I am maintaining my score, because I don\u2019t think the current version of the paper is enough of a contribution to get accepted. As mentioned in my responses below, I would be happy to accept a future version of the paper that addresses my comments above. ", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper543/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper543/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Descending through a Crowded Valley \u2014 Benchmarking Deep Learning Optimizers", "authorids": ["~Robin_Marc_Schmidt1", "~Frank_Schneider1", "~Philipp_Hennig1"], "authors": ["Robin Marc Schmidt", "Frank Schneider", "Philipp Hennig"], "keywords": ["Deep learning", "optimizers", "benchmark"], "abstract": "Choosing the optimizer is considered to be among the most crucial design decisions in deep learning, and it is not an easy one. The growing literature now lists hundreds of optimization methods. In the absence of clear theoretical guidance and conclusive empirical evidence, the decision is often made based on anecdotes. In this work, we aim to replace these anecdotes, if not with a conclusive ranking, then at least with evidence-backed heuristics. To do so, we perform an extensive, standardized benchmark of more than a dozen particularly popular deep learning optimizers while giving a concise overview of the wide range of possible choices. Analyzing almost $35,000$ individual runs, we contribute the following three points: (i) Optimizer performance varies greatly across tasks. (ii) We observe that evaluating multiple optimizers with default parameters works approximately as well as tuning the hyperparameters of a single, fixed optimizer. (iii) While we can not discern an optimization method clearly dominating across all tested tasks, we identify a significantly reduced subset of specific algorithms and parameter choices that generally lead to competitive results in our experiments. This subset includes popular favorites and some lesser-known contenders. We have open-sourced all our experimental results, making them directly available as challenging and well-tuned baselines. This allows for more meaningful comparisons when evaluating novel optimization methods without requiring any further computational efforts.", "one-sentence_summary": "A large-scale deep learning optimizer benchmark with open-sourced results for more meaningful optimizer comparisons.", "pdf": "/pdf/c1dd51b5d487496f707a9957cdd518288dd529e4.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "schmidt|descending_through_a_crowded_valley_benchmarking_deep_learning_optimizers", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=NKAcbIbIMn", "_bibtex": "@misc{\nschmidt2021descending,\ntitle={Descending through a Crowded Valley {\\textemdash} Benchmarking Deep Learning Optimizers},\nauthor={Robin Marc Schmidt and Frank Schneider and Philipp Hennig},\nyear={2021},\nurl={https://openreview.net/forum?id=k2Om84I9JuX}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "k2Om84I9JuX", "replyto": "k2Om84I9JuX", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper543/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538140781, "tmdate": 1606915788334, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper543/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper543/-/Official_Review"}}}, {"id": "4c3EviB6hxx", "original": null, "number": 3, "cdate": 1603847811333, "ddate": null, "tcdate": 1603847811333, "tmdate": 1606740992881, "tddate": null, "forum": "k2Om84I9JuX", "replyto": "k2Om84I9JuX", "invitation": "ICLR.cc/2021/Conference/Paper543/-/Official_Review", "content": {"title": "Official Review", "review": "### Summary\nThe authors of this paper conducted a thorough evaluation of deep learning optimizers across different compute budgets and learning rate schedules. They provide detailed analysis of the results. The design decisions are well-reasoned and explained throughout the paper.\n\n### Comments\n* As the authors note, there is certainly value in understanding the practical tradeoffs between optimizers: \"for most algorithms, the only formal empirical evaluation is offered by the original work introducing the method\"\n* The writing is clear and easy to follow.\n* Many of the findings are useful in the context of the DeepOBS dataset. For instance, Figure 3 highlights the diminishing returns of increasing the budget when tuning hyperparameters.\n* Open-sourcing the data is great and beneficial for the community.\n\n* The benchmark would benefit from a larger scale dataset(s). I'm not in favor of solely adopting DeepOBS, as past papers have shown systematic differences in evaluation at different scales [1][2]. Investigating whether there are systematic differences in optimization on larger problems e.g. machine translation or ImageNet would be valuable. \n* As Reviewer 4 mentions, the momentum parameter should be tuned as 1 - \\rho.\n\n### Recommendation / Justification\nI vote that this paper is below the acceptance threshold. There are many things to like about the approach taken is this paper, as highlighted above. However, the lack of larger scales datasets lessens the significance of the conclusions.\n\nI'd increase my score if concerns about the datasets used were addressed. I understand it is challenging to do so during the rebuttal period, but I strongly believe that larger scale datasets would strengthen the work significantly.\n\n### Minor feedback \n* A tabular form of Figure 4 would improve clarity.\n* I think it is worth acknowledging techniques for averaging the weights of neural networks, as these can have a substantial impact on final performance (Polyak averaging, exponential moving average, Stochastic Weight Averaging).\n* I believe it is also worthwhile to benchmark a second-order optimizer. While the compute per step is more expensive, the comparison could be made fair by using the same compute budget for each optimizer.\n* I am surprised by the choice of \\alpha when tuning the lookahead optimizer. My suspicion is that tuning the momentum and learning rate is more fruitful than trying low values of \\alpha.\n\n\n[1] Frankle, Jonathan, Gintare Karolina Dziugaite, Daniel M. Roy, and Michael Carbin. \"Stabilizing the lottery ticket hypothesis.\" arXiv preprint arXiv:1903.01611 (2019).\n\n[2] Gale, Trevor, Erich Elsen, and Sara Hooker. \"The state of sparsity in deep neural networks.\" arXiv preprint arXiv:1902.09574 (2019).\n\n\nEdit: After the rebuttal period, I maintain my original rating but am increasing the confidence of my evaluation from 4 to 5. I thank the authors for their hard work and engaging in discussion. I agree with Reviewer 3 that tuning with a fixed seed and the lack of search space refinement is a major weakness. The lack of a larger dataset further limit the applicability of the results. As such, I do not believe the paper in its current form should be accepted to ICLR. ", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper543/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper543/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Descending through a Crowded Valley \u2014 Benchmarking Deep Learning Optimizers", "authorids": ["~Robin_Marc_Schmidt1", "~Frank_Schneider1", "~Philipp_Hennig1"], "authors": ["Robin Marc Schmidt", "Frank Schneider", "Philipp Hennig"], "keywords": ["Deep learning", "optimizers", "benchmark"], "abstract": "Choosing the optimizer is considered to be among the most crucial design decisions in deep learning, and it is not an easy one. The growing literature now lists hundreds of optimization methods. In the absence of clear theoretical guidance and conclusive empirical evidence, the decision is often made based on anecdotes. In this work, we aim to replace these anecdotes, if not with a conclusive ranking, then at least with evidence-backed heuristics. To do so, we perform an extensive, standardized benchmark of more than a dozen particularly popular deep learning optimizers while giving a concise overview of the wide range of possible choices. Analyzing almost $35,000$ individual runs, we contribute the following three points: (i) Optimizer performance varies greatly across tasks. (ii) We observe that evaluating multiple optimizers with default parameters works approximately as well as tuning the hyperparameters of a single, fixed optimizer. (iii) While we can not discern an optimization method clearly dominating across all tested tasks, we identify a significantly reduced subset of specific algorithms and parameter choices that generally lead to competitive results in our experiments. This subset includes popular favorites and some lesser-known contenders. We have open-sourced all our experimental results, making them directly available as challenging and well-tuned baselines. This allows for more meaningful comparisons when evaluating novel optimization methods without requiring any further computational efforts.", "one-sentence_summary": "A large-scale deep learning optimizer benchmark with open-sourced results for more meaningful optimizer comparisons.", "pdf": "/pdf/c1dd51b5d487496f707a9957cdd518288dd529e4.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "schmidt|descending_through_a_crowded_valley_benchmarking_deep_learning_optimizers", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=NKAcbIbIMn", "_bibtex": "@misc{\nschmidt2021descending,\ntitle={Descending through a Crowded Valley {\\textemdash} Benchmarking Deep Learning Optimizers},\nauthor={Robin Marc Schmidt and Frank Schneider and Philipp Hennig},\nyear={2021},\nurl={https://openreview.net/forum?id=k2Om84I9JuX}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "k2Om84I9JuX", "replyto": "k2Om84I9JuX", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper543/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538140781, "tmdate": 1606915788334, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper543/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper543/-/Official_Review"}}}, {"id": "AxesDY_IXwf", "original": null, "number": 20, "cdate": 1606254808795, "ddate": null, "tcdate": 1606254808795, "tmdate": 1606254834555, "tddate": null, "forum": "k2Om84I9JuX", "replyto": "lXdnkvD5p2t", "invitation": "ICLR.cc/2021/Conference/Paper543/-/Official_Comment", "content": {"title": "The paper has a narrow scope and does not address some of the concerns that the authors claim", "comment": "All papers involve making choices, which comes with drawbacks and caveats. It\u2019s also the job of the reviewers to judge whether the choices are justified and adequate enough to be published. In my opinion, the choices made in this paper are not adequate and justified enough.\n\nCurrently, the paper presents itself with a very narrow and limited scope. The experiments are only applicable to those who tune over a single seed, those who only have enough resources to use default parameter values, and those who tune over the arbitrary fixed search range decided by the authors and some other non-referenced group of people. I personally don\u2019t think a paper that targets a very specific group of people, who even the authors think are inexperienced, is enough to get accepted to a conference.\n\nThe authors seem to claim that their experiments are more than what they are; for instance, in their earlier reply, they mentioned that they \u201ctook care to not only care about performance but also ease-of-use of the optimizer\u201d. I disagree with both points. Regarding the performance aspect, I mentioned in my previous reply that I could come up with a method that takes the same amount of compute, relatively simple, but produces better results by using separate search spaces. Regarding \u201cease-of-use\u201d, I don\u2019t see how the current protocol reveals anything about the ease of use of an optimizer. Using the example that the authors brought up, how would we know of the existence of optimizer A or B if we don\u2019t try other ranges? It seems like the only way to show such a property (which I agree would be very useful) is to refine the search spaces and observe that a consistent search space was chosen for one optimizer. Since we\u2019re in the topic of examples, I can come up with another example to defend the refinement procedure: it could be the case that one optimizer outperforms all other optimizers, but only with parameter values that are outside the range of that chosen, but still something reasonable, so that a simple two-stage refinement would have revealed it. I think finding (or attempting to find) such examples (or the lack of) is the kind of contribution that I would be happy to accept as a paper. \n\nLastly, I want to mention that I\u2019m mainly criticizing the fixed search space approach, and not advocating for a specific tuning procedure. I brought up the 25-25 split two-stage procedure, because it seemed simple, and similar to what previous researches have done in the past. It could be 10-40, 20-30, or whatever that seems justifiable, as long as it is consistently done over all methods. The authors seem to justify not doing a refinement procedure by saying that it \u201crequire(s) additional human decisions and can thus introduce bias\u201d, and it is a \u201cdebated\u201d approach. Bias and debated-ness are all imprecise terms; we can\u2019t possibly measure and compare the amount of bias introduced by two different methods. Therefore, I can only conclude that the appeal to using a fixed search space is to cater to a specific group of people, and also to make it \u201ceasier\u201d to compare. I don\u2019t think this is enough of a justification to not do search space refinements,  which should yield better performance in general, and could help answer questions like which optimizer is more \u201ceasy to tune\u201d. \n\nAll in all, I never expected a paper to satisfy everyone. But I also cannot accept a paper with a narrow scope, which could have easily been avoided. I would be happy to accept a future version of the paper that addresses the comments I made in my original review. "}, "signatures": ["ICLR.cc/2021/Conference/Paper543/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper543/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Descending through a Crowded Valley \u2014 Benchmarking Deep Learning Optimizers", "authorids": ["~Robin_Marc_Schmidt1", "~Frank_Schneider1", "~Philipp_Hennig1"], "authors": ["Robin Marc Schmidt", "Frank Schneider", "Philipp Hennig"], "keywords": ["Deep learning", "optimizers", "benchmark"], "abstract": "Choosing the optimizer is considered to be among the most crucial design decisions in deep learning, and it is not an easy one. The growing literature now lists hundreds of optimization methods. In the absence of clear theoretical guidance and conclusive empirical evidence, the decision is often made based on anecdotes. In this work, we aim to replace these anecdotes, if not with a conclusive ranking, then at least with evidence-backed heuristics. To do so, we perform an extensive, standardized benchmark of more than a dozen particularly popular deep learning optimizers while giving a concise overview of the wide range of possible choices. Analyzing almost $35,000$ individual runs, we contribute the following three points: (i) Optimizer performance varies greatly across tasks. (ii) We observe that evaluating multiple optimizers with default parameters works approximately as well as tuning the hyperparameters of a single, fixed optimizer. (iii) While we can not discern an optimization method clearly dominating across all tested tasks, we identify a significantly reduced subset of specific algorithms and parameter choices that generally lead to competitive results in our experiments. This subset includes popular favorites and some lesser-known contenders. We have open-sourced all our experimental results, making them directly available as challenging and well-tuned baselines. This allows for more meaningful comparisons when evaluating novel optimization methods without requiring any further computational efforts.", "one-sentence_summary": "A large-scale deep learning optimizer benchmark with open-sourced results for more meaningful optimizer comparisons.", "pdf": "/pdf/c1dd51b5d487496f707a9957cdd518288dd529e4.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "schmidt|descending_through_a_crowded_valley_benchmarking_deep_learning_optimizers", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=NKAcbIbIMn", "_bibtex": "@misc{\nschmidt2021descending,\ntitle={Descending through a Crowded Valley {\\textemdash} Benchmarking Deep Learning Optimizers},\nauthor={Robin Marc Schmidt and Frank Schneider and Philipp Hennig},\nyear={2021},\nurl={https://openreview.net/forum?id=k2Om84I9JuX}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "k2Om84I9JuX", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper543/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper543/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper543/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper543/Authors|ICLR.cc/2021/Conference/Paper543/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper543/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923869837, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper543/-/Official_Comment"}}}, {"id": "lXdnkvD5p2t", "original": null, "number": 18, "cdate": 1606224671136, "ddate": null, "tcdate": 1606224671136, "tmdate": 1606224671136, "tddate": null, "forum": "k2Om84I9JuX", "replyto": "EMBy9LUYYAf", "invitation": "ICLR.cc/2021/Conference/Paper543/-/Official_Comment", "content": {"title": "Reply", "comment": "We are sorry to hear that our response does not seem to have satisfied your concerns.\nIf it helps, we are of course happy to move Figure 11 from the appendix to the main paper.\n\nHowever, your main concern seems to be our use of universal search spaces instead of adapting them individually. You agree that \u201cno one method is perfect, and they each have pros and cons\u201d and we are open about possible drawbacks of our (unavoidable) choices. We could state them more extensively in the paper if this would address your concerns.\n\nAllow us to defend our tuning method with one example. Let\u2019s assume two optimizers A and B which both result in the same performance if tuned well. Optimizer A can always be tuned in the interval [1e-3,1] and Optimizer B\u2019s interval depends on the problem, it might be [1e-6,1e-3] or [1e-1,1e2]. Wouldn\u2019t you agree that Optimizer A is easier to use? If we use 25 runs to find the interval and 25 runs to tune it, both optimizers would look the same (given our admittedly simplified assumptions).\nAgain, this is not to say that our method is the only valid choice! But yours isn\u2019t either: In the same way that you suggested a 25/25 split (of tuning, adjusting the search space, and tuning again), another reviewer might prefer a 40/10 (better coverage) or a 10/40 (better tuning) split. Or a 12/13 split for the smaller budget. Or criticize this method as increasing the variance of the results and introducing yet another arbitrary choice. For example, the paper by Choi et al. (2019) that you reference, uses the problem-dependent search space you suggested, and their choice has also been debated.\n\nWe accept that any benchmark will always invite criticism. But if the bar is to satisfy everyone, then these papers become virtually impossible. We agree with Reviewer 4 that perhaps the approach you suggest \u201cwould best be studied in a separate work on search space selection\u201d that could provide another data point for benchmark optimizers additional to our work."}, "signatures": ["ICLR.cc/2021/Conference/Paper543/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper543/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Descending through a Crowded Valley \u2014 Benchmarking Deep Learning Optimizers", "authorids": ["~Robin_Marc_Schmidt1", "~Frank_Schneider1", "~Philipp_Hennig1"], "authors": ["Robin Marc Schmidt", "Frank Schneider", "Philipp Hennig"], "keywords": ["Deep learning", "optimizers", "benchmark"], "abstract": "Choosing the optimizer is considered to be among the most crucial design decisions in deep learning, and it is not an easy one. The growing literature now lists hundreds of optimization methods. In the absence of clear theoretical guidance and conclusive empirical evidence, the decision is often made based on anecdotes. In this work, we aim to replace these anecdotes, if not with a conclusive ranking, then at least with evidence-backed heuristics. To do so, we perform an extensive, standardized benchmark of more than a dozen particularly popular deep learning optimizers while giving a concise overview of the wide range of possible choices. Analyzing almost $35,000$ individual runs, we contribute the following three points: (i) Optimizer performance varies greatly across tasks. (ii) We observe that evaluating multiple optimizers with default parameters works approximately as well as tuning the hyperparameters of a single, fixed optimizer. (iii) While we can not discern an optimization method clearly dominating across all tested tasks, we identify a significantly reduced subset of specific algorithms and parameter choices that generally lead to competitive results in our experiments. This subset includes popular favorites and some lesser-known contenders. We have open-sourced all our experimental results, making them directly available as challenging and well-tuned baselines. This allows for more meaningful comparisons when evaluating novel optimization methods without requiring any further computational efforts.", "one-sentence_summary": "A large-scale deep learning optimizer benchmark with open-sourced results for more meaningful optimizer comparisons.", "pdf": "/pdf/c1dd51b5d487496f707a9957cdd518288dd529e4.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "schmidt|descending_through_a_crowded_valley_benchmarking_deep_learning_optimizers", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=NKAcbIbIMn", "_bibtex": "@misc{\nschmidt2021descending,\ntitle={Descending through a Crowded Valley {\\textemdash} Benchmarking Deep Learning Optimizers},\nauthor={Robin Marc Schmidt and Frank Schneider and Philipp Hennig},\nyear={2021},\nurl={https://openreview.net/forum?id=k2Om84I9JuX}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "k2Om84I9JuX", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper543/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper543/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper543/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper543/Authors|ICLR.cc/2021/Conference/Paper543/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper543/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923869837, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper543/-/Official_Comment"}}}, {"id": "tSOF5So-CyR", "original": null, "number": 17, "cdate": 1606213625426, "ddate": null, "tcdate": 1606213625426, "tmdate": 1606213625426, "tddate": null, "forum": "k2Om84I9JuX", "replyto": "uMuv40CgbXP", "invitation": "ICLR.cc/2021/Conference/Paper543/-/Official_Comment", "content": {"title": "Neutral sentiment", "comment": "I have read the other reviews and the author's responses and maintain my (weak) recommendation for acceptance. \n\nIn my opinion, this paper does not set a gold standard for optimizer benchmarking. The experimental design decisions and baselines are reasonable in some situations, yet inapplicable in others.\nI do, however, have the same criticism towards previously published work in this area. Given the importance of good optimizer benchmarking and the long way we have to go, I think that the contribution of this work is valuable to the community and it should be shared."}, "signatures": ["ICLR.cc/2021/Conference/Paper543/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper543/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Descending through a Crowded Valley \u2014 Benchmarking Deep Learning Optimizers", "authorids": ["~Robin_Marc_Schmidt1", "~Frank_Schneider1", "~Philipp_Hennig1"], "authors": ["Robin Marc Schmidt", "Frank Schneider", "Philipp Hennig"], "keywords": ["Deep learning", "optimizers", "benchmark"], "abstract": "Choosing the optimizer is considered to be among the most crucial design decisions in deep learning, and it is not an easy one. The growing literature now lists hundreds of optimization methods. In the absence of clear theoretical guidance and conclusive empirical evidence, the decision is often made based on anecdotes. In this work, we aim to replace these anecdotes, if not with a conclusive ranking, then at least with evidence-backed heuristics. To do so, we perform an extensive, standardized benchmark of more than a dozen particularly popular deep learning optimizers while giving a concise overview of the wide range of possible choices. Analyzing almost $35,000$ individual runs, we contribute the following three points: (i) Optimizer performance varies greatly across tasks. (ii) We observe that evaluating multiple optimizers with default parameters works approximately as well as tuning the hyperparameters of a single, fixed optimizer. (iii) While we can not discern an optimization method clearly dominating across all tested tasks, we identify a significantly reduced subset of specific algorithms and parameter choices that generally lead to competitive results in our experiments. This subset includes popular favorites and some lesser-known contenders. We have open-sourced all our experimental results, making them directly available as challenging and well-tuned baselines. This allows for more meaningful comparisons when evaluating novel optimization methods without requiring any further computational efforts.", "one-sentence_summary": "A large-scale deep learning optimizer benchmark with open-sourced results for more meaningful optimizer comparisons.", "pdf": "/pdf/c1dd51b5d487496f707a9957cdd518288dd529e4.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "schmidt|descending_through_a_crowded_valley_benchmarking_deep_learning_optimizers", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=NKAcbIbIMn", "_bibtex": "@misc{\nschmidt2021descending,\ntitle={Descending through a Crowded Valley {\\textemdash} Benchmarking Deep Learning Optimizers},\nauthor={Robin Marc Schmidt and Frank Schneider and Philipp Hennig},\nyear={2021},\nurl={https://openreview.net/forum?id=k2Om84I9JuX}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "k2Om84I9JuX", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper543/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper543/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper543/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper543/Authors|ICLR.cc/2021/Conference/Paper543/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper543/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923869837, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper543/-/Official_Comment"}}}, {"id": "zSUepUS-64W", "original": null, "number": 16, "cdate": 1606206181996, "ddate": null, "tcdate": 1606206181996, "tmdate": 1606206181996, "tddate": null, "forum": "k2Om84I9JuX", "replyto": "BweQ-71Ryp5", "invitation": "ICLR.cc/2021/Conference/Paper543/-/Official_Comment", "content": {"title": "It's impossible to tease apart the optimizer and search space selection process", "comment": "It\u2019s impossible to talk about empirical performance of optimizers without discussing the search space, because optimizer performance is very heavily dependent on the search space. Therefore, it doesn't make sense to tease apart the optimizer and the search space selection process. This is exactly why comparing optimizers empirically is so difficult-- there seems to be no real answer as to what is fair. But this doesn\u2019t mean that we should be complacent about choosing suboptimal ranges. We know that a given optimizer\u2019s optimal hyperparameter values change based on the architecture and the dataset. I think a simple two-step procedure that is consistent among all optimizers is good enough for me, and is already so much better than sticking to a fixed range. As I mentioned in my response to the authors, this method doesn\u2019t introduce any more bias than what already exists with the chosen fixed ranges. It\u2019s just not acceptable for there to be a possibility where different conclusions could have been made with a slightly more involved setup that doesn\u2019t take any more computation."}, "signatures": ["ICLR.cc/2021/Conference/Paper543/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper543/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Descending through a Crowded Valley \u2014 Benchmarking Deep Learning Optimizers", "authorids": ["~Robin_Marc_Schmidt1", "~Frank_Schneider1", "~Philipp_Hennig1"], "authors": ["Robin Marc Schmidt", "Frank Schneider", "Philipp Hennig"], "keywords": ["Deep learning", "optimizers", "benchmark"], "abstract": "Choosing the optimizer is considered to be among the most crucial design decisions in deep learning, and it is not an easy one. The growing literature now lists hundreds of optimization methods. In the absence of clear theoretical guidance and conclusive empirical evidence, the decision is often made based on anecdotes. In this work, we aim to replace these anecdotes, if not with a conclusive ranking, then at least with evidence-backed heuristics. To do so, we perform an extensive, standardized benchmark of more than a dozen particularly popular deep learning optimizers while giving a concise overview of the wide range of possible choices. Analyzing almost $35,000$ individual runs, we contribute the following three points: (i) Optimizer performance varies greatly across tasks. (ii) We observe that evaluating multiple optimizers with default parameters works approximately as well as tuning the hyperparameters of a single, fixed optimizer. (iii) While we can not discern an optimization method clearly dominating across all tested tasks, we identify a significantly reduced subset of specific algorithms and parameter choices that generally lead to competitive results in our experiments. This subset includes popular favorites and some lesser-known contenders. We have open-sourced all our experimental results, making them directly available as challenging and well-tuned baselines. This allows for more meaningful comparisons when evaluating novel optimization methods without requiring any further computational efforts.", "one-sentence_summary": "A large-scale deep learning optimizer benchmark with open-sourced results for more meaningful optimizer comparisons.", "pdf": "/pdf/c1dd51b5d487496f707a9957cdd518288dd529e4.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "schmidt|descending_through_a_crowded_valley_benchmarking_deep_learning_optimizers", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=NKAcbIbIMn", "_bibtex": "@misc{\nschmidt2021descending,\ntitle={Descending through a Crowded Valley {\\textemdash} Benchmarking Deep Learning Optimizers},\nauthor={Robin Marc Schmidt and Frank Schneider and Philipp Hennig},\nyear={2021},\nurl={https://openreview.net/forum?id=k2Om84I9JuX}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "k2Om84I9JuX", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper543/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper543/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper543/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper543/Authors|ICLR.cc/2021/Conference/Paper543/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper543/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923869837, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper543/-/Official_Comment"}}}, {"id": "EMBy9LUYYAf", "original": null, "number": 15, "cdate": 1606204402698, "ddate": null, "tcdate": 1606204402698, "tmdate": 1606204402698, "tddate": null, "forum": "k2Om84I9JuX", "replyto": "nS7tOsHRq0", "invitation": "ICLR.cc/2021/Conference/Paper543/-/Official_Comment", "content": {"title": "I'm not convinced by the response", "comment": "I appreciate the authors\u2019 effort to account for different use cases of optimizers, and to model the \u201caverage practitioner\u201d. However, following the experimental protocol of a fictitious group of people is not only imprecise, but also propagates and justifies bad practice, like tuning over a single seed, or using the same search ranges across all problems. I think this paper would be much more useful if it provided practical guidelines that ultimately result in better empirical results, rather than follow the authors\u2019 idea of a model practitioner. \n\nGoing back to my review, I think it\u2019s clear that doing bootstrapping to estimate the metrics, will give better results than tuning on a single seed and evaluating on a different set of seeds. Not only this, but tuning with varying seeds doesn\u2019t cost more computationally, and the resulting measurements are robust to the choice of seed that any given person would choose. The existence of Figure 11 doesn\u2019t satisfy me, since the main body still contains results using the original tuning strategy. Furthermore, the mean of maxes (over different seeds) and the error measurements aren\u2019t addressed by Figure 11, which includes only the max over a single seed. \n\nOn a similar note, using a more tailored search space per problem should yield better performance than using a fixed global search space. The topic of hyperparameter search spaces is tricky but important to discuss, because optimizer performance is heavily dependent on the search space. Regarding the authors\u2019 response, I think it\u2019s impossible to take out the human factor in deciding the search spaces, unless we have an infinite amount of resources. In fact, the search spaces used in the paper were arbitrarily set by the authors, or chosen by a group of people through many empirical studies. Is the worry that by a human further refining the search ranges, that they will unknowingly bias certain methods over others? Isn\u2019t this already the case, with more known optimizers getting a better range, since they were used many times in the past and therefore, has a more refined search space, compared to the more recent ones whose search ranges haven\u2019t been explored? I feel like refining the search space per problem is a way to address this implicit bias, and make the comparisons more fair. I agree with the authors that no one method is perfect, and they each have pros and cons. However, I would opt for a method that yields better results, and practically useful information, like the specific search ranges that worked well for a given optimizer and dataset pair. The \u201ctwo-stage\u201d method that I proposed in my review, doesn\u2019t add any more computation, and is simple compared to bayesian optimization.\n\nIn summary, this paper simulates the \u201ccareful practitioner who does not have \u2026 a broad range of personal experiences,\u201d which resulted in unnecessary penalization of optimizers, which could have been avoided with no additional computational effort. I\u2019m not convinced by the author\u2019s response, and additionally, agree with other reviewers\u2019 concern about the limitation of the chosen workloads. Therefore, I am lowering my score, and recommend rejection."}, "signatures": ["ICLR.cc/2021/Conference/Paper543/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper543/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Descending through a Crowded Valley \u2014 Benchmarking Deep Learning Optimizers", "authorids": ["~Robin_Marc_Schmidt1", "~Frank_Schneider1", "~Philipp_Hennig1"], "authors": ["Robin Marc Schmidt", "Frank Schneider", "Philipp Hennig"], "keywords": ["Deep learning", "optimizers", "benchmark"], "abstract": "Choosing the optimizer is considered to be among the most crucial design decisions in deep learning, and it is not an easy one. The growing literature now lists hundreds of optimization methods. In the absence of clear theoretical guidance and conclusive empirical evidence, the decision is often made based on anecdotes. In this work, we aim to replace these anecdotes, if not with a conclusive ranking, then at least with evidence-backed heuristics. To do so, we perform an extensive, standardized benchmark of more than a dozen particularly popular deep learning optimizers while giving a concise overview of the wide range of possible choices. Analyzing almost $35,000$ individual runs, we contribute the following three points: (i) Optimizer performance varies greatly across tasks. (ii) We observe that evaluating multiple optimizers with default parameters works approximately as well as tuning the hyperparameters of a single, fixed optimizer. (iii) While we can not discern an optimization method clearly dominating across all tested tasks, we identify a significantly reduced subset of specific algorithms and parameter choices that generally lead to competitive results in our experiments. This subset includes popular favorites and some lesser-known contenders. We have open-sourced all our experimental results, making them directly available as challenging and well-tuned baselines. This allows for more meaningful comparisons when evaluating novel optimization methods without requiring any further computational efforts.", "one-sentence_summary": "A large-scale deep learning optimizer benchmark with open-sourced results for more meaningful optimizer comparisons.", "pdf": "/pdf/c1dd51b5d487496f707a9957cdd518288dd529e4.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "schmidt|descending_through_a_crowded_valley_benchmarking_deep_learning_optimizers", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=NKAcbIbIMn", "_bibtex": "@misc{\nschmidt2021descending,\ntitle={Descending through a Crowded Valley {\\textemdash} Benchmarking Deep Learning Optimizers},\nauthor={Robin Marc Schmidt and Frank Schneider and Philipp Hennig},\nyear={2021},\nurl={https://openreview.net/forum?id=k2Om84I9JuX}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "k2Om84I9JuX", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper543/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper543/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper543/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper543/Authors|ICLR.cc/2021/Conference/Paper543/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper543/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923869837, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper543/-/Official_Comment"}}}, {"id": "WG5IBU04WgD", "original": null, "number": 1, "cdate": 1603750321375, "ddate": null, "tcdate": 1603750321375, "tmdate": 1605977337979, "tddate": null, "forum": "k2Om84I9JuX", "replyto": "k2Om84I9JuX", "invitation": "ICLR.cc/2021/Conference/Paper543/-/Official_Review", "content": {"title": "Very well done with one experimental concern", "review": "Overview:\nOverall I believe this paper is extremely well written and organized. The introduction and limitations are very useful groundings of optimization research, and I hope that the community reads this paper and internalizes its message of making more meaningful research in optimization (instead of yet-another-Adam-variant!).\n\nIn a previous comment I brought up a serious concern about the tuning ranges of momentum-like parameters, which I believe could bias the results towards optimizers that were tuned with ranges whose lower end was 0.5 (Adam, AMSBound, AMSGrad, AdaBound, LA(RAdam), NAdam, RAdam). Besides this concern, I overall would normally argue for a very strong acceptance, but until this is corrected I am unsure I can recommend accepting. I look forward to discussing correcting this with the authors however, and am willing to dramatically raise my score!\n\n**NOTE: updated score after seeing author replies and updated draft. I believe that this work is exemplary in terms of being careful about baseline construction, something that is unfortunately too often overlooked in our field. Additionally, it rigorously highlights another important point that I believe many often overlook, that \"there are now enough optimizers\"; community effort should be diverted from introducing small variations around Adam and instead invest focus on more meaningful improvements in scaling machine learning optimization. I do still believe that ImageNet and a larger transformer experiment would be extremely valuable to add to a later version, and hope the authors can eventually secure the computing power to add these.**\n\nPros:\n-Table 2 is extremely useful and I think should be additionally put on a GitHub where it can be updated with links to papers.\n-The authors clearly put a lot of careful thought into how to study and present these results, taking into account numerous caveats that almost all other papers totally ignore, such as the limitations of their study, the importance of tuning ranges, learning rate schedules, etc.\n\nConcerns:\n-I am very understanding and sympathetic to the issue of compute constraints, but I do believe that the study would be even more useful if a ResNet-50/Imagenet and a Transformer pipeline were used. The authors discuss that GANs and RL are not included, and those seem like very different types of optimization to me so I am more understanding of not including them. If the authors are academics then I have seen researchers have success in the past with getting grants from cloud providers, namely the lottery ticket hypothesis line of work and the TF Research Cloud (I found out about it through this paper https://openreview.net/forum?id=S1gSj0NKvB), which can easily be used to heavily tune ImageNet and Transformer runs. Outside of that, according to this benchmark https://dawn.cs.stanford.edu/benchmark/ImageNet/train.html, the cost of ImageNet runs has come down considerably in recent years, to $10-20 per run.\n-The experimental results could partially be explained by the No Free Lunch theorem, and the authors could at least reference this in the section.\n-What regularization, if any, was used for these problems, and was that also tuned? One could argue that, while optimization and regularization are in theory orthogonal to each other in what they try to accomplish (train vs test performance), they are both part of the update rule whose hyperparameters are being tuned. Also I believe it is important to be careful and note if coupled or decoupled weight decay/L2 is being used is important; I assume it is coupled because the DeepOBS code that is referenced uses L2 regularization, which means that optimizers that use preconditioning (Adam-like optimizers) could be impacted by this differently than those that do not (SGD/Momentum).\n\nWriting:\n-In the \u201cTuning method\u201d paragraph in section 2.3, \u201cIn case there is no prior knowledge provided in the op.cit. we chose\u201d seems like a syntax error.\n\nPrior work:\nThe authors do a very thorough literature search, and properly reference and discuss similar prior studies.\n\nAdditional feedback, comments, suggestions for improvement and questions for the authors:\n-Awesome job providing per-step values for results, it would be further useful to have code that could easily plot them side-by-side so that future researchers would be further encouraged to include them in their figures.\n-May be worth noting that, in addition to the optimizer hyperparameters, one could also tune the batch normalization momentum/epsilon, for additional performance gains.\n-Figure 3 seems very useful, however I believe it would be much better presented as a series of box plots. A nice recent example of this is Figure 2 in https://arxiv.org/abs/1906.02530. This could also be done for Figure 4 where each nested box is an individual optimizer, and it shows the distribution over runs for each optimizer for each problem.\n-The trapez schedule always seems to be the best, and I wonder if this is due to only one of the learning rate ramp up, which has been shown to be beneficial to stabilize training (although it is unclear if this is required), or the learning rate becoming quite small at the end, which has been shown to be necessary so that the optimizer can better learn the noisy directions of the objective. It would be useful for future work to consider each of these learning rate schedules (ramp-up, ramp-down) separately, although that requires even more compute.\n", "rating": "9: Top 15% of accepted papers, strong accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper543/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper543/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Descending through a Crowded Valley \u2014 Benchmarking Deep Learning Optimizers", "authorids": ["~Robin_Marc_Schmidt1", "~Frank_Schneider1", "~Philipp_Hennig1"], "authors": ["Robin Marc Schmidt", "Frank Schneider", "Philipp Hennig"], "keywords": ["Deep learning", "optimizers", "benchmark"], "abstract": "Choosing the optimizer is considered to be among the most crucial design decisions in deep learning, and it is not an easy one. The growing literature now lists hundreds of optimization methods. In the absence of clear theoretical guidance and conclusive empirical evidence, the decision is often made based on anecdotes. In this work, we aim to replace these anecdotes, if not with a conclusive ranking, then at least with evidence-backed heuristics. To do so, we perform an extensive, standardized benchmark of more than a dozen particularly popular deep learning optimizers while giving a concise overview of the wide range of possible choices. Analyzing almost $35,000$ individual runs, we contribute the following three points: (i) Optimizer performance varies greatly across tasks. (ii) We observe that evaluating multiple optimizers with default parameters works approximately as well as tuning the hyperparameters of a single, fixed optimizer. (iii) While we can not discern an optimization method clearly dominating across all tested tasks, we identify a significantly reduced subset of specific algorithms and parameter choices that generally lead to competitive results in our experiments. This subset includes popular favorites and some lesser-known contenders. We have open-sourced all our experimental results, making them directly available as challenging and well-tuned baselines. This allows for more meaningful comparisons when evaluating novel optimization methods without requiring any further computational efforts.", "one-sentence_summary": "A large-scale deep learning optimizer benchmark with open-sourced results for more meaningful optimizer comparisons.", "pdf": "/pdf/c1dd51b5d487496f707a9957cdd518288dd529e4.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "schmidt|descending_through_a_crowded_valley_benchmarking_deep_learning_optimizers", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=NKAcbIbIMn", "_bibtex": "@misc{\nschmidt2021descending,\ntitle={Descending through a Crowded Valley {\\textemdash} Benchmarking Deep Learning Optimizers},\nauthor={Robin Marc Schmidt and Frank Schneider and Philipp Hennig},\nyear={2021},\nurl={https://openreview.net/forum?id=k2Om84I9JuX}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "k2Om84I9JuX", "replyto": "k2Om84I9JuX", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper543/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538140781, "tmdate": 1606915788334, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper543/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper543/-/Official_Review"}}}, {"id": "uMuv40CgbXP", "original": null, "number": 14, "cdate": 1605977170458, "ddate": null, "tcdate": 1605977170458, "tmdate": 1605977170458, "tddate": null, "forum": "k2Om84I9JuX", "replyto": "mU1SESwFqEB", "invitation": "ICLR.cc/2021/Conference/Paper543/-/Official_Comment", "content": {"title": "Updated score", "comment": "Thank you very much for the heroic effort! I have updated my score accordingly.\n\nI believe that this work is exemplary in terms of being careful about baseline construction, something that is unfortunately too often overlooked in our field. Additionally, it rigorously highlights another important point that I believe many often overlook, that \"there are now enough optimizers\"; community effort should be diverted from introducing small variations around Adam and instead invest focus on more meaningful improvements in scaling machine learning optimization."}, "signatures": ["ICLR.cc/2021/Conference/Paper543/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper543/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Descending through a Crowded Valley \u2014 Benchmarking Deep Learning Optimizers", "authorids": ["~Robin_Marc_Schmidt1", "~Frank_Schneider1", "~Philipp_Hennig1"], "authors": ["Robin Marc Schmidt", "Frank Schneider", "Philipp Hennig"], "keywords": ["Deep learning", "optimizers", "benchmark"], "abstract": "Choosing the optimizer is considered to be among the most crucial design decisions in deep learning, and it is not an easy one. The growing literature now lists hundreds of optimization methods. In the absence of clear theoretical guidance and conclusive empirical evidence, the decision is often made based on anecdotes. In this work, we aim to replace these anecdotes, if not with a conclusive ranking, then at least with evidence-backed heuristics. To do so, we perform an extensive, standardized benchmark of more than a dozen particularly popular deep learning optimizers while giving a concise overview of the wide range of possible choices. Analyzing almost $35,000$ individual runs, we contribute the following three points: (i) Optimizer performance varies greatly across tasks. (ii) We observe that evaluating multiple optimizers with default parameters works approximately as well as tuning the hyperparameters of a single, fixed optimizer. (iii) While we can not discern an optimization method clearly dominating across all tested tasks, we identify a significantly reduced subset of specific algorithms and parameter choices that generally lead to competitive results in our experiments. This subset includes popular favorites and some lesser-known contenders. We have open-sourced all our experimental results, making them directly available as challenging and well-tuned baselines. This allows for more meaningful comparisons when evaluating novel optimization methods without requiring any further computational efforts.", "one-sentence_summary": "A large-scale deep learning optimizer benchmark with open-sourced results for more meaningful optimizer comparisons.", "pdf": "/pdf/c1dd51b5d487496f707a9957cdd518288dd529e4.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "schmidt|descending_through_a_crowded_valley_benchmarking_deep_learning_optimizers", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=NKAcbIbIMn", "_bibtex": "@misc{\nschmidt2021descending,\ntitle={Descending through a Crowded Valley {\\textemdash} Benchmarking Deep Learning Optimizers},\nauthor={Robin Marc Schmidt and Frank Schneider and Philipp Hennig},\nyear={2021},\nurl={https://openreview.net/forum?id=k2Om84I9JuX}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "k2Om84I9JuX", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper543/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper543/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper543/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper543/Authors|ICLR.cc/2021/Conference/Paper543/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper543/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923869837, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper543/-/Official_Comment"}}}, {"id": "mU1SESwFqEB", "original": null, "number": 13, "cdate": 1605963777164, "ddate": null, "tcdate": 1605963777164, "tmdate": 1605963777164, "tddate": null, "forum": "k2Om84I9JuX", "replyto": "k2Om84I9JuX", "invitation": "ICLR.cc/2021/Conference/Paper543/-/Official_Comment", "content": {"title": "Updated Version", "comment": "Dear Reviewers,\n\nwe want to thank you again for your constructive feedback. We have just submitted an improved version of our paper, incorporating your suggestions.\n\n* Most importantly, we re-ran a significant part of our benchmark, using the suggested momentum search space for all schedules and both tuning budgets (this is more than we initially thought we would be able to do in the rebuttal timeframe). Although this changed a few details, the overall statements of the paper remain unchanged.\n* We have updated all results and plots in the paper as well as in the anonymous repository.\n* We have also addressed Stochastic Weight Averaging, L2-regularization, and larger data sets in an added paragraph of the Limitations [SECTION 4], along with the more minor feedback such as our description of the work by Sivaprasad et al. [SECTION 1.1], the use of op. cit [SECTION 2.3], No Free Lunch Theorem [SECTION 1], etc.\n* We added the extensive list of optimizers to our repository Readme with links to the respective papers and provided an additional tabular version of Figure 4 in Appendix H.\n\nTo address your requests we committed significant resources over the past days. You all seem to agree that this is important, valuable, and (most importantly) beneficial work to be shared with the community. A paper like this inevitably requires choices that can be criticized one way or another. We hope, though, that our efforts to address your requests have alleviated your concerns. Thanks again for your time and your comments!"}, "signatures": ["ICLR.cc/2021/Conference/Paper543/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper543/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Descending through a Crowded Valley \u2014 Benchmarking Deep Learning Optimizers", "authorids": ["~Robin_Marc_Schmidt1", "~Frank_Schneider1", "~Philipp_Hennig1"], "authors": ["Robin Marc Schmidt", "Frank Schneider", "Philipp Hennig"], "keywords": ["Deep learning", "optimizers", "benchmark"], "abstract": "Choosing the optimizer is considered to be among the most crucial design decisions in deep learning, and it is not an easy one. The growing literature now lists hundreds of optimization methods. In the absence of clear theoretical guidance and conclusive empirical evidence, the decision is often made based on anecdotes. In this work, we aim to replace these anecdotes, if not with a conclusive ranking, then at least with evidence-backed heuristics. To do so, we perform an extensive, standardized benchmark of more than a dozen particularly popular deep learning optimizers while giving a concise overview of the wide range of possible choices. Analyzing almost $35,000$ individual runs, we contribute the following three points: (i) Optimizer performance varies greatly across tasks. (ii) We observe that evaluating multiple optimizers with default parameters works approximately as well as tuning the hyperparameters of a single, fixed optimizer. (iii) While we can not discern an optimization method clearly dominating across all tested tasks, we identify a significantly reduced subset of specific algorithms and parameter choices that generally lead to competitive results in our experiments. This subset includes popular favorites and some lesser-known contenders. We have open-sourced all our experimental results, making them directly available as challenging and well-tuned baselines. This allows for more meaningful comparisons when evaluating novel optimization methods without requiring any further computational efforts.", "one-sentence_summary": "A large-scale deep learning optimizer benchmark with open-sourced results for more meaningful optimizer comparisons.", "pdf": "/pdf/c1dd51b5d487496f707a9957cdd518288dd529e4.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "schmidt|descending_through_a_crowded_valley_benchmarking_deep_learning_optimizers", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=NKAcbIbIMn", "_bibtex": "@misc{\nschmidt2021descending,\ntitle={Descending through a Crowded Valley {\\textemdash} Benchmarking Deep Learning Optimizers},\nauthor={Robin Marc Schmidt and Frank Schneider and Philipp Hennig},\nyear={2021},\nurl={https://openreview.net/forum?id=k2Om84I9JuX}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "k2Om84I9JuX", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper543/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper543/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper543/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper543/Authors|ICLR.cc/2021/Conference/Paper543/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper543/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923869837, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper543/-/Official_Comment"}}}, {"id": "6LvY2aN-l_C", "original": null, "number": 12, "cdate": 1605929461936, "ddate": null, "tcdate": 1605929461936, "tmdate": 1605929461936, "tddate": null, "forum": "k2Om84I9JuX", "replyto": "4bWpeD_6ltQ", "invitation": "ICLR.cc/2021/Conference/Paper543/-/Official_Comment", "content": {"title": "Response Thoughts", "comment": "Thank you for your response. I have read through the other reviews and responses. \n\nI generally maintain my original thoughts about large datasets and agree with the comments made by reviewer 4 (I am also sympathetic of compute/time constraints). I think that omitting GANs and RL is fine as we'd expect different patterns of optimization in those domains. Including larger datasets is important for novelty/significance. I looked through Sivaprasad et al. (2020) again, and it seems like the current set of experiments extend their work primarily via the inclusion of more optimizers and learning rate schedules. They do draw the conclusion that Adam is the most practical optimizer in terms of ease to tune/performance, which is different from the conclusion in this paper.\n\nOverall, I think the changes and experiments you are conducting now definitely do strengthen the paper and make it more valuable to the community."}, "signatures": ["ICLR.cc/2021/Conference/Paper543/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper543/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Descending through a Crowded Valley \u2014 Benchmarking Deep Learning Optimizers", "authorids": ["~Robin_Marc_Schmidt1", "~Frank_Schneider1", "~Philipp_Hennig1"], "authors": ["Robin Marc Schmidt", "Frank Schneider", "Philipp Hennig"], "keywords": ["Deep learning", "optimizers", "benchmark"], "abstract": "Choosing the optimizer is considered to be among the most crucial design decisions in deep learning, and it is not an easy one. The growing literature now lists hundreds of optimization methods. In the absence of clear theoretical guidance and conclusive empirical evidence, the decision is often made based on anecdotes. In this work, we aim to replace these anecdotes, if not with a conclusive ranking, then at least with evidence-backed heuristics. To do so, we perform an extensive, standardized benchmark of more than a dozen particularly popular deep learning optimizers while giving a concise overview of the wide range of possible choices. Analyzing almost $35,000$ individual runs, we contribute the following three points: (i) Optimizer performance varies greatly across tasks. (ii) We observe that evaluating multiple optimizers with default parameters works approximately as well as tuning the hyperparameters of a single, fixed optimizer. (iii) While we can not discern an optimization method clearly dominating across all tested tasks, we identify a significantly reduced subset of specific algorithms and parameter choices that generally lead to competitive results in our experiments. This subset includes popular favorites and some lesser-known contenders. We have open-sourced all our experimental results, making them directly available as challenging and well-tuned baselines. This allows for more meaningful comparisons when evaluating novel optimization methods without requiring any further computational efforts.", "one-sentence_summary": "A large-scale deep learning optimizer benchmark with open-sourced results for more meaningful optimizer comparisons.", "pdf": "/pdf/c1dd51b5d487496f707a9957cdd518288dd529e4.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "schmidt|descending_through_a_crowded_valley_benchmarking_deep_learning_optimizers", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=NKAcbIbIMn", "_bibtex": "@misc{\nschmidt2021descending,\ntitle={Descending through a Crowded Valley {\\textemdash} Benchmarking Deep Learning Optimizers},\nauthor={Robin Marc Schmidt and Frank Schneider and Philipp Hennig},\nyear={2021},\nurl={https://openreview.net/forum?id=k2Om84I9JuX}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "k2Om84I9JuX", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper543/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper543/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper543/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper543/Authors|ICLR.cc/2021/Conference/Paper543/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper543/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923869837, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper543/-/Official_Comment"}}}, {"id": "BweQ-71Ryp5", "original": null, "number": 11, "cdate": 1605476029317, "ddate": null, "tcdate": 1605476029317, "tmdate": 1605476029317, "tddate": null, "forum": "k2Om84I9JuX", "replyto": "nS7tOsHRq0", "invitation": "ICLR.cc/2021/Conference/Paper543/-/Official_Comment", "content": {"title": "Search space selection procedure", "comment": "I have seen that many times in practice, machine learning users tune over a fixed set of hyperparameter points independent of dataset, and only sometimes increase the ranges. However, for problems where significant investment must be made, search spaces are refined through trial and error, at the high cost of compute. So I agree with Reviewer 3 that it is valid and not unheard of to do a search space refinement, however I also agree with the authors here in using a fixed search space for all problems. If using problem-dependent or hand-refined search spaces, it becomes difficult to tease apart where the performance boosts are coming from: the optimizer or the search space selection process? For example, in your 25 exploratory then 25 refinement trial scenario, would one see significant improvements in optimizer performance if 10 trials were used to explore and then 40 to refine (or vice versa)?\n\nIn general, I believe that as long as authors are upfront about their search space selection process, one can argue for either approach, but perhaps the refinement approach would best be studied in a separate work on search space selection. In the current draft, the authors are explicit in their discussion of these nuances and highlight the possible drawbacks of their choice, so I do not believe they should be penalized for this."}, "signatures": ["ICLR.cc/2021/Conference/Paper543/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper543/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Descending through a Crowded Valley \u2014 Benchmarking Deep Learning Optimizers", "authorids": ["~Robin_Marc_Schmidt1", "~Frank_Schneider1", "~Philipp_Hennig1"], "authors": ["Robin Marc Schmidt", "Frank Schneider", "Philipp Hennig"], "keywords": ["Deep learning", "optimizers", "benchmark"], "abstract": "Choosing the optimizer is considered to be among the most crucial design decisions in deep learning, and it is not an easy one. The growing literature now lists hundreds of optimization methods. In the absence of clear theoretical guidance and conclusive empirical evidence, the decision is often made based on anecdotes. In this work, we aim to replace these anecdotes, if not with a conclusive ranking, then at least with evidence-backed heuristics. To do so, we perform an extensive, standardized benchmark of more than a dozen particularly popular deep learning optimizers while giving a concise overview of the wide range of possible choices. Analyzing almost $35,000$ individual runs, we contribute the following three points: (i) Optimizer performance varies greatly across tasks. (ii) We observe that evaluating multiple optimizers with default parameters works approximately as well as tuning the hyperparameters of a single, fixed optimizer. (iii) While we can not discern an optimization method clearly dominating across all tested tasks, we identify a significantly reduced subset of specific algorithms and parameter choices that generally lead to competitive results in our experiments. This subset includes popular favorites and some lesser-known contenders. We have open-sourced all our experimental results, making them directly available as challenging and well-tuned baselines. This allows for more meaningful comparisons when evaluating novel optimization methods without requiring any further computational efforts.", "one-sentence_summary": "A large-scale deep learning optimizer benchmark with open-sourced results for more meaningful optimizer comparisons.", "pdf": "/pdf/c1dd51b5d487496f707a9957cdd518288dd529e4.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "schmidt|descending_through_a_crowded_valley_benchmarking_deep_learning_optimizers", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=NKAcbIbIMn", "_bibtex": "@misc{\nschmidt2021descending,\ntitle={Descending through a Crowded Valley {\\textemdash} Benchmarking Deep Learning Optimizers},\nauthor={Robin Marc Schmidt and Frank Schneider and Philipp Hennig},\nyear={2021},\nurl={https://openreview.net/forum?id=k2Om84I9JuX}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "k2Om84I9JuX", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper543/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper543/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper543/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper543/Authors|ICLR.cc/2021/Conference/Paper543/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper543/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923869837, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper543/-/Official_Comment"}}}, {"id": "kTBmKuI7PWS", "original": null, "number": 10, "cdate": 1605294568791, "ddate": null, "tcdate": 1605294568791, "tmdate": 1605294568791, "tddate": null, "forum": "k2Om84I9JuX", "replyto": "ZDB6mDT4qDr", "invitation": "ICLR.cc/2021/Conference/Paper543/-/Official_Comment", "content": {"title": "Tuning ranges", "comment": "I believe that investing would be worthwhile!\n\nOne nit about the tuning ranges, for (Adam, AMSBound, AMSGrad, AdaBound, LA(RAdam), NAdam, RAdam) the momentum-like parameter is tuned on LU(0.5, 0.999), which would be the equivalent of tuning $1 - \\rho$ on LU(1e-4, 0.5), but you're tuning on LU(1e-4, 1.0). In practice this likely won't matter, but I would highlight it in the text."}, "signatures": ["ICLR.cc/2021/Conference/Paper543/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper543/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Descending through a Crowded Valley \u2014 Benchmarking Deep Learning Optimizers", "authorids": ["~Robin_Marc_Schmidt1", "~Frank_Schneider1", "~Philipp_Hennig1"], "authors": ["Robin Marc Schmidt", "Frank Schneider", "Philipp Hennig"], "keywords": ["Deep learning", "optimizers", "benchmark"], "abstract": "Choosing the optimizer is considered to be among the most crucial design decisions in deep learning, and it is not an easy one. The growing literature now lists hundreds of optimization methods. In the absence of clear theoretical guidance and conclusive empirical evidence, the decision is often made based on anecdotes. In this work, we aim to replace these anecdotes, if not with a conclusive ranking, then at least with evidence-backed heuristics. To do so, we perform an extensive, standardized benchmark of more than a dozen particularly popular deep learning optimizers while giving a concise overview of the wide range of possible choices. Analyzing almost $35,000$ individual runs, we contribute the following three points: (i) Optimizer performance varies greatly across tasks. (ii) We observe that evaluating multiple optimizers with default parameters works approximately as well as tuning the hyperparameters of a single, fixed optimizer. (iii) While we can not discern an optimization method clearly dominating across all tested tasks, we identify a significantly reduced subset of specific algorithms and parameter choices that generally lead to competitive results in our experiments. This subset includes popular favorites and some lesser-known contenders. We have open-sourced all our experimental results, making them directly available as challenging and well-tuned baselines. This allows for more meaningful comparisons when evaluating novel optimization methods without requiring any further computational efforts.", "one-sentence_summary": "A large-scale deep learning optimizer benchmark with open-sourced results for more meaningful optimizer comparisons.", "pdf": "/pdf/c1dd51b5d487496f707a9957cdd518288dd529e4.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "schmidt|descending_through_a_crowded_valley_benchmarking_deep_learning_optimizers", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=NKAcbIbIMn", "_bibtex": "@misc{\nschmidt2021descending,\ntitle={Descending through a Crowded Valley {\\textemdash} Benchmarking Deep Learning Optimizers},\nauthor={Robin Marc Schmidt and Frank Schneider and Philipp Hennig},\nyear={2021},\nurl={https://openreview.net/forum?id=k2Om84I9JuX}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "k2Om84I9JuX", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper543/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper543/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper543/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper543/Authors|ICLR.cc/2021/Conference/Paper543/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper543/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923869837, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper543/-/Official_Comment"}}}, {"id": "ZDB6mDT4qDr", "original": null, "number": 9, "cdate": 1605173837502, "ddate": null, "tcdate": 1605173837502, "tmdate": 1605173837502, "tddate": null, "forum": "k2Om84I9JuX", "replyto": "WG5IBU04WgD", "invitation": "ICLR.cc/2021/Conference/Paper543/-/Official_Comment", "content": {"title": "Response to the Comments of Reviewer 4", "comment": "We want to thank you for the very constructive feedback.\n\nAs we already wrote below, we agree with your point about the momentum tuning ranges. This is an absolutely valid concern that we will address. We are currently trying to get our hands on as much hardware as possible to update the corresponding results. From our current estimate, it will be a nail-biter whether we can finish every single re-run by the end of this feedback session.\nThus, it would be very valuable to us if you could give us direct feedback on whether our current plans will seem satisfactory to you, or whether you want us to do things differently. Here is what we plan to do:\n\n- We will change the tuning distribution of Adadelta, Lookahead Momentum, Momentum, NAG, and RMSProp. Instead of tuning $\\rho$ with $LU(1e-3, 1)$, we will now tune $1-\\rho$ with $LU(1e-4,1)$ as you suggested.\n- We will start with all runs affecting the results in Figures 2 and 4 in the main text.\n- After that, we will iteratively update all remaining Figures and the Appendix when the corresponding results arrive.\n\nWe will post updates on OpenReview as soon as they arrive and can show the update figures in our anonymous git repo or in the updated pdf.\n\nThanks again for your willingness to debate with us. We\u2019re investing significant resources in trying to address your concerns and want to make sure this effort is worthwhile.\n\nWe also greatly appreciate the more minor suggestions. We have commented above, in the reply to Reviewer 1, on the use of larger data sets, but putting Table 2 on Github with links, mentioning the No Free Lunch Theorem, changing \"op.cit.\" to something more comprehensible, and commenting on the used regularization are great suggestions that we will implement immediately."}, "signatures": ["ICLR.cc/2021/Conference/Paper543/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper543/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Descending through a Crowded Valley \u2014 Benchmarking Deep Learning Optimizers", "authorids": ["~Robin_Marc_Schmidt1", "~Frank_Schneider1", "~Philipp_Hennig1"], "authors": ["Robin Marc Schmidt", "Frank Schneider", "Philipp Hennig"], "keywords": ["Deep learning", "optimizers", "benchmark"], "abstract": "Choosing the optimizer is considered to be among the most crucial design decisions in deep learning, and it is not an easy one. The growing literature now lists hundreds of optimization methods. In the absence of clear theoretical guidance and conclusive empirical evidence, the decision is often made based on anecdotes. In this work, we aim to replace these anecdotes, if not with a conclusive ranking, then at least with evidence-backed heuristics. To do so, we perform an extensive, standardized benchmark of more than a dozen particularly popular deep learning optimizers while giving a concise overview of the wide range of possible choices. Analyzing almost $35,000$ individual runs, we contribute the following three points: (i) Optimizer performance varies greatly across tasks. (ii) We observe that evaluating multiple optimizers with default parameters works approximately as well as tuning the hyperparameters of a single, fixed optimizer. (iii) While we can not discern an optimization method clearly dominating across all tested tasks, we identify a significantly reduced subset of specific algorithms and parameter choices that generally lead to competitive results in our experiments. This subset includes popular favorites and some lesser-known contenders. We have open-sourced all our experimental results, making them directly available as challenging and well-tuned baselines. This allows for more meaningful comparisons when evaluating novel optimization methods without requiring any further computational efforts.", "one-sentence_summary": "A large-scale deep learning optimizer benchmark with open-sourced results for more meaningful optimizer comparisons.", "pdf": "/pdf/c1dd51b5d487496f707a9957cdd518288dd529e4.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "schmidt|descending_through_a_crowded_valley_benchmarking_deep_learning_optimizers", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=NKAcbIbIMn", "_bibtex": "@misc{\nschmidt2021descending,\ntitle={Descending through a Crowded Valley {\\textemdash} Benchmarking Deep Learning Optimizers},\nauthor={Robin Marc Schmidt and Frank Schneider and Philipp Hennig},\nyear={2021},\nurl={https://openreview.net/forum?id=k2Om84I9JuX}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "k2Om84I9JuX", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper543/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper543/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper543/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper543/Authors|ICLR.cc/2021/Conference/Paper543/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper543/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923869837, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper543/-/Official_Comment"}}}, {"id": "nS7tOsHRq0", "original": null, "number": 8, "cdate": 1605171320389, "ddate": null, "tcdate": 1605171320389, "tmdate": 1605171320389, "tddate": null, "forum": "k2Om84I9JuX", "replyto": "uwB9TctXzfq", "invitation": "ICLR.cc/2021/Conference/Paper543/-/Official_Comment", "content": {"title": "Response to the Comments of Reviewer 3", "comment": "Dear Reviewer 3,\n\nwe want to thank you for your time reviewing our paper and providing specific feedback.\n\nIt is great to hear that you found our benchmark well presented and justified in all its details. We also took great care to highlight its limitations and we are thus happy to read that you listed this as a strength of this work.\n\nRegarding the stability of our tuning procedure, we tried to model our process on an average practitioner. We expect them to do some kind of hyperparameter search (random search in our case) using a single run per setting (i.e. using a single seed). Once they found the best-working configuration, however, they might train the network multiple times, possibly using operations that would effectively change the random seed (e.g. using different data sampling, slightly changing the architecture, etc.). Our benchmark also offers the ability to assess how stable the optimizer would behave in this setting.\nNote, however, that it is still possible to assess \"how well the optimizer can do (as an upper bound)\" using just the seed that has been used for tuning. For example, Figure 11 shows the results of only the tuning seed and everyone is welcome to use our open-source data to delve deeper.\n\nWhen designing our benchmark we took care to not only care about performance but also about ease-of-use of the optimizer. If one can re-use the same search space for different problems, it makes this optimizer obviously easier to use. One can just run the hyperparameter search on the weekend, and get the results by Monday irrespective of the problem. This should be reflected in the benchmark, at least somehow.\nWe did consider other hyperparameter tuning methods, such as the \"two-stage random search\" you are proposing or Bayesian methods. However, we decided against these more elaborate schemes mainly because they require additional human decisions and can thus introduce bias.\nNote, that, for example, the paper by Choi et al. (2019) has been criticized for its use of problem-dependent search spaces on OpenReview and by Sivaprasad et al. (2020). This is not to say that our method is superior, but that each method has its pros and cons.\n\nWe want to also invite you to engage in the discussion we have below with Reviewer 4, in which we are trying to find a constructive way to address as many requests made by you and the other reviewers as possible within the rebuttal phase."}, "signatures": ["ICLR.cc/2021/Conference/Paper543/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper543/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Descending through a Crowded Valley \u2014 Benchmarking Deep Learning Optimizers", "authorids": ["~Robin_Marc_Schmidt1", "~Frank_Schneider1", "~Philipp_Hennig1"], "authors": ["Robin Marc Schmidt", "Frank Schneider", "Philipp Hennig"], "keywords": ["Deep learning", "optimizers", "benchmark"], "abstract": "Choosing the optimizer is considered to be among the most crucial design decisions in deep learning, and it is not an easy one. The growing literature now lists hundreds of optimization methods. In the absence of clear theoretical guidance and conclusive empirical evidence, the decision is often made based on anecdotes. In this work, we aim to replace these anecdotes, if not with a conclusive ranking, then at least with evidence-backed heuristics. To do so, we perform an extensive, standardized benchmark of more than a dozen particularly popular deep learning optimizers while giving a concise overview of the wide range of possible choices. Analyzing almost $35,000$ individual runs, we contribute the following three points: (i) Optimizer performance varies greatly across tasks. (ii) We observe that evaluating multiple optimizers with default parameters works approximately as well as tuning the hyperparameters of a single, fixed optimizer. (iii) While we can not discern an optimization method clearly dominating across all tested tasks, we identify a significantly reduced subset of specific algorithms and parameter choices that generally lead to competitive results in our experiments. This subset includes popular favorites and some lesser-known contenders. We have open-sourced all our experimental results, making them directly available as challenging and well-tuned baselines. This allows for more meaningful comparisons when evaluating novel optimization methods without requiring any further computational efforts.", "one-sentence_summary": "A large-scale deep learning optimizer benchmark with open-sourced results for more meaningful optimizer comparisons.", "pdf": "/pdf/c1dd51b5d487496f707a9957cdd518288dd529e4.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "schmidt|descending_through_a_crowded_valley_benchmarking_deep_learning_optimizers", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=NKAcbIbIMn", "_bibtex": "@misc{\nschmidt2021descending,\ntitle={Descending through a Crowded Valley {\\textemdash} Benchmarking Deep Learning Optimizers},\nauthor={Robin Marc Schmidt and Frank Schneider and Philipp Hennig},\nyear={2021},\nurl={https://openreview.net/forum?id=k2Om84I9JuX}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "k2Om84I9JuX", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper543/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper543/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper543/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper543/Authors|ICLR.cc/2021/Conference/Paper543/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper543/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923869837, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper543/-/Official_Comment"}}}, {"id": "8i_oW1Bbddm", "original": null, "number": 6, "cdate": 1605170928254, "ddate": null, "tcdate": 1605170928254, "tmdate": 1605171235896, "tddate": null, "forum": "k2Om84I9JuX", "replyto": "TTKV3qe6ok7", "invitation": "ICLR.cc/2021/Conference/Paper543/-/Official_Comment", "content": {"title": "Response to the Comments of Reviewer 2", "comment": "Dear Reviewer 2,\nthank you for your positive review of our work. We are happy that you agree with us that the paper offers important results to be shared with the community.\n\nWe agree with your description of the work by Sivaprasad et al. (2020). We did not mean to characterize their work as a comparison of hyperparameter tuning methods, but we realize that our phrasing can be understood like this. We will re-formulate this part.\n\nContribution (ii) is indeed based on Figure 2 and Figures 9 to 12 in the Appendix. Looking at a tuned optimizer, i.e. a single column, there is almost always a red (or at least white) cell in this column, indicating that there is a better-performing optimizer with default parameters.\nWhether this is a preferable strategy, however, depends on multiple factors, such as what the specific practitioner would consider \"similar performance\" or how much performance they are willing to trade-off for cheaper computation.\nIn Section 3, we describe that Adam (and its variants) as well as AdaBound (and AMSBound) are optimizers that work well without tuning. Taking the better of them can often provide competitive results, even compared to tuned optimizers. If additional budget is available, adding a tuned version of Adam (or its variants) seems to be a good strategy. These conclusions are based mainly on Figure 2 and Figure 4, as well as the corresponding Figures in the appendix.\n\nFigure 3 is indeed not invariant to re-shifting and re-scaling. As such, it is best not read quantitatively, but qualitatively. What this figure shows is that tuning helps, but with diminishing returns, and also that there is a lot of underlying noise. Both statements also hold after re-scaling the used metric.\n\nWe want to also invite you to engage in the discussion we have below with Reviewer 4, in which we are trying to find a constructive way to address as many requests made by you and the other reviewers as possible within the rebuttal phase."}, "signatures": ["ICLR.cc/2021/Conference/Paper543/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper543/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Descending through a Crowded Valley \u2014 Benchmarking Deep Learning Optimizers", "authorids": ["~Robin_Marc_Schmidt1", "~Frank_Schneider1", "~Philipp_Hennig1"], "authors": ["Robin Marc Schmidt", "Frank Schneider", "Philipp Hennig"], "keywords": ["Deep learning", "optimizers", "benchmark"], "abstract": "Choosing the optimizer is considered to be among the most crucial design decisions in deep learning, and it is not an easy one. The growing literature now lists hundreds of optimization methods. In the absence of clear theoretical guidance and conclusive empirical evidence, the decision is often made based on anecdotes. In this work, we aim to replace these anecdotes, if not with a conclusive ranking, then at least with evidence-backed heuristics. To do so, we perform an extensive, standardized benchmark of more than a dozen particularly popular deep learning optimizers while giving a concise overview of the wide range of possible choices. Analyzing almost $35,000$ individual runs, we contribute the following three points: (i) Optimizer performance varies greatly across tasks. (ii) We observe that evaluating multiple optimizers with default parameters works approximately as well as tuning the hyperparameters of a single, fixed optimizer. (iii) While we can not discern an optimization method clearly dominating across all tested tasks, we identify a significantly reduced subset of specific algorithms and parameter choices that generally lead to competitive results in our experiments. This subset includes popular favorites and some lesser-known contenders. We have open-sourced all our experimental results, making them directly available as challenging and well-tuned baselines. This allows for more meaningful comparisons when evaluating novel optimization methods without requiring any further computational efforts.", "one-sentence_summary": "A large-scale deep learning optimizer benchmark with open-sourced results for more meaningful optimizer comparisons.", "pdf": "/pdf/c1dd51b5d487496f707a9957cdd518288dd529e4.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "schmidt|descending_through_a_crowded_valley_benchmarking_deep_learning_optimizers", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=NKAcbIbIMn", "_bibtex": "@misc{\nschmidt2021descending,\ntitle={Descending through a Crowded Valley {\\textemdash} Benchmarking Deep Learning Optimizers},\nauthor={Robin Marc Schmidt and Frank Schneider and Philipp Hennig},\nyear={2021},\nurl={https://openreview.net/forum?id=k2Om84I9JuX}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "k2Om84I9JuX", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper543/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper543/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper543/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper543/Authors|ICLR.cc/2021/Conference/Paper543/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper543/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923869837, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper543/-/Official_Comment"}}}, {"id": "4bWpeD_6ltQ", "original": null, "number": 7, "cdate": 1605171145063, "ddate": null, "tcdate": 1605171145063, "tmdate": 1605171221028, "tddate": null, "forum": "k2Om84I9JuX", "replyto": "4c3EviB6hxx", "invitation": "ICLR.cc/2021/Conference/Paper543/-/Official_Comment", "content": {"title": "Response to the Comments of Reviewer 1", "comment": "Dear Reviewer 1,\n\nthank you for providing a detailed review of our paper. We greatly appreciate that you called our work \"a thorough evaluation of deep learning optimizers\".\n\nWe appreciate your comment about larger data sets. The size of the training problems, however, is another aspect where a benchmark has to find compromises. Training on ImageNet is significantly more costly in time and resources than on the architectures we used. So, given finite resources, opting for these large problems would have required us to reduce statistical fidelity or the number of optimizers to compare. In our opinion, our setup strikes a better balance. Of course one can always argue that one dimension or another in this balance should have been weighted differently, but we argue that such personal desiderata should not preclude the publication of our work. We agree that evaluating large-scale problems is an interesting avenue for further research in this area, similar to benchmarking optimizers on GANs or RL. We are happy to include a statement like this into our Limitation section, acknowledging that our benchmark is more applicable to small and medium-scale problems.\nIt is also debatable whether large-scale problems are actually \u201ctypical\u201d for the bulk of practitioners in real-world settings. Outside of computer vision, medium-size and even smallish datasets are not uncommon.\n\nWe will address your minor feedback as much as possible. Adding a second-order optimizer would be interesting for future work. As you mentioned, it would require keeping the runtime of runs of different optimizer constant. This is tricky, even if exclusively using identical hardware, as the runtime can be affected by many factors. Providing the data of Figure 4 as a table or acknowledging techniques for averaging weights is certainly possible.\n\nWe want to also invite you to engage in the discussion we have below with Reviewer 4, in which we are trying to find a constructive way to address as many requests made by you and the other reviewers as possible within the rebuttal phase."}, "signatures": ["ICLR.cc/2021/Conference/Paper543/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper543/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Descending through a Crowded Valley \u2014 Benchmarking Deep Learning Optimizers", "authorids": ["~Robin_Marc_Schmidt1", "~Frank_Schneider1", "~Philipp_Hennig1"], "authors": ["Robin Marc Schmidt", "Frank Schneider", "Philipp Hennig"], "keywords": ["Deep learning", "optimizers", "benchmark"], "abstract": "Choosing the optimizer is considered to be among the most crucial design decisions in deep learning, and it is not an easy one. The growing literature now lists hundreds of optimization methods. In the absence of clear theoretical guidance and conclusive empirical evidence, the decision is often made based on anecdotes. In this work, we aim to replace these anecdotes, if not with a conclusive ranking, then at least with evidence-backed heuristics. To do so, we perform an extensive, standardized benchmark of more than a dozen particularly popular deep learning optimizers while giving a concise overview of the wide range of possible choices. Analyzing almost $35,000$ individual runs, we contribute the following three points: (i) Optimizer performance varies greatly across tasks. (ii) We observe that evaluating multiple optimizers with default parameters works approximately as well as tuning the hyperparameters of a single, fixed optimizer. (iii) While we can not discern an optimization method clearly dominating across all tested tasks, we identify a significantly reduced subset of specific algorithms and parameter choices that generally lead to competitive results in our experiments. This subset includes popular favorites and some lesser-known contenders. We have open-sourced all our experimental results, making them directly available as challenging and well-tuned baselines. This allows for more meaningful comparisons when evaluating novel optimization methods without requiring any further computational efforts.", "one-sentence_summary": "A large-scale deep learning optimizer benchmark with open-sourced results for more meaningful optimizer comparisons.", "pdf": "/pdf/c1dd51b5d487496f707a9957cdd518288dd529e4.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "schmidt|descending_through_a_crowded_valley_benchmarking_deep_learning_optimizers", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=NKAcbIbIMn", "_bibtex": "@misc{\nschmidt2021descending,\ntitle={Descending through a Crowded Valley {\\textemdash} Benchmarking Deep Learning Optimizers},\nauthor={Robin Marc Schmidt and Frank Schneider and Philipp Hennig},\nyear={2021},\nurl={https://openreview.net/forum?id=k2Om84I9JuX}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "k2Om84I9JuX", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper543/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper543/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper543/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper543/Authors|ICLR.cc/2021/Conference/Paper543/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper543/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923869837, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper543/-/Official_Comment"}}}, {"id": "TTKV3qe6ok7", "original": null, "number": 4, "cdate": 1603881454748, "ddate": null, "tcdate": 1603881454748, "tmdate": 1605024664261, "tddate": null, "forum": "k2Om84I9JuX", "replyto": "k2Om84I9JuX", "invitation": "ICLR.cc/2021/Conference/Paper543/-/Official_Review", "content": {"title": "Important, well executed, experiments with unfortunately no clear-cut outcome", "review": "This paper presents an extensive independent benchmark of 14 popular optimizers on a variety of deep learning tasks from DeepOBS (Schneider et al. 2019). They compare them at three different tuning budgets and with 4 learning rate schedules. The authors are realistic about their setup. They acknowledge that different people might have different desires for such a benchmark, and they are clear about the choices they made to keep the experiments feasible. While there is no clear-cut answer that tells practitioners which optimizer to use in what scenario and how to tune it, these experiments are valuable and I believe it is important that these results are shared with the community.\n\nThe quality of the presentation and the writing is good. \n\nIn terms of novelty, the authors model the target audience slightly differently from previous work (Schneider et al. 2019, Choi et al. 2019, Sivaprasad et al. 2020). I am not convinced that this approach is better per se than others, but a different angle and a different set of optimizers is a valuable contribution to the community. I believe that the description of (Sivaprasad et al. 2020) in Section 1.1. is not entirely accurate. They do not compare hyperparameter tuning methods, but rather benchmark optimizers similarly to this work at a continuum of hyperparameter tuning budgets (all with random search).\n\nFinally, let me share two concerns:\n\n1. The intro mentions three contributions: (i) performance varies greatly, (ii) trying different optimizers works as well as tuning a single one, (iii) they identify a significantly reduced subset of algorithms and parameter choices that perform well across experiments. Points (ii) and (iii) are interesting, but (ii) is formulated quite imprecisely and it is hard to see on which results this is based. I inspected Figures 9 to 12 in the Appendix and conclude \"this might be true, but it is hard to see\". I believe a quantative statement would be more useful/meaningful. Similarly, for point (iii) it is not clear from which results this is concluded, and what the high-performing subset is. Such a list would be valuable to many practitioners and should be clearly stated in the main text.\n\n2. Figure 3 shows relative improvement across tasks. Any such measurements of 'improvement' are dependent on re-shifting or re-scaling of the loss, and are not necessarily meaningful when aggregated into a plot like this. Consider accuracy: measuring relative improvement (accuracy 1 / accuracy 2) would yield drastically different numbers than (error 1 / error 2 = (1-acc1) / (1-acc2)).", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper543/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper543/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Descending through a Crowded Valley \u2014 Benchmarking Deep Learning Optimizers", "authorids": ["~Robin_Marc_Schmidt1", "~Frank_Schneider1", "~Philipp_Hennig1"], "authors": ["Robin Marc Schmidt", "Frank Schneider", "Philipp Hennig"], "keywords": ["Deep learning", "optimizers", "benchmark"], "abstract": "Choosing the optimizer is considered to be among the most crucial design decisions in deep learning, and it is not an easy one. The growing literature now lists hundreds of optimization methods. In the absence of clear theoretical guidance and conclusive empirical evidence, the decision is often made based on anecdotes. In this work, we aim to replace these anecdotes, if not with a conclusive ranking, then at least with evidence-backed heuristics. To do so, we perform an extensive, standardized benchmark of more than a dozen particularly popular deep learning optimizers while giving a concise overview of the wide range of possible choices. Analyzing almost $35,000$ individual runs, we contribute the following three points: (i) Optimizer performance varies greatly across tasks. (ii) We observe that evaluating multiple optimizers with default parameters works approximately as well as tuning the hyperparameters of a single, fixed optimizer. (iii) While we can not discern an optimization method clearly dominating across all tested tasks, we identify a significantly reduced subset of specific algorithms and parameter choices that generally lead to competitive results in our experiments. This subset includes popular favorites and some lesser-known contenders. We have open-sourced all our experimental results, making them directly available as challenging and well-tuned baselines. This allows for more meaningful comparisons when evaluating novel optimization methods without requiring any further computational efforts.", "one-sentence_summary": "A large-scale deep learning optimizer benchmark with open-sourced results for more meaningful optimizer comparisons.", "pdf": "/pdf/c1dd51b5d487496f707a9957cdd518288dd529e4.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "schmidt|descending_through_a_crowded_valley_benchmarking_deep_learning_optimizers", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=NKAcbIbIMn", "_bibtex": "@misc{\nschmidt2021descending,\ntitle={Descending through a Crowded Valley {\\textemdash} Benchmarking Deep Learning Optimizers},\nauthor={Robin Marc Schmidt and Frank Schneider and Philipp Hennig},\nyear={2021},\nurl={https://openreview.net/forum?id=k2Om84I9JuX}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "k2Om84I9JuX", "replyto": "k2Om84I9JuX", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper543/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538140781, "tmdate": 1606915788334, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper543/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper543/-/Official_Review"}}}], "count": 20}