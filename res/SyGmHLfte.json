{"notes": [{"tddate": null, "ddate": null, "cdate": null, "original": null, "tmdate": 1490028562245, "tcdate": 1490028562245, "number": 1, "id": "B19z_Kaig", "invitation": "ICLR.cc/2017/workshop/-/paper33/acceptance", "forum": "SyGmHLfte", "replyto": "SyGmHLfte", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Infinite Dimensional Word Embeddings", "abstract": "We describe a method for learning word embeddings with data-dependent dimensionality.  Our Infinite Skip-Gram (iSG) and Infinite Continuous Bag-of-Words (iCBOW) are nonparametric analogs of Mikolov et al.'s (2013) well-known 'word2vec' models.  Vectors are made infinite dimensional by employing techniques used by Cote & Larochelle (2016) to define a RBM with an infinite number of hidden units.  We show qualitatively and quantitatively that the iSG and iCBOW are competitive with their fixed-dimension counterparts while having the ability to infer the appropriate capacity of each word representation.", "pdf": "/pdf/90e7c6f656a67800d772e23cfc50c9fc075d10f2.pdf", "TL;DR": "Word embeddings with stochastic dimensionality.", "paperhash": "nalisnick|infinite_dimensional_word_embeddings", "keywords": ["Natural language processing", "Unsupervised Learning"], "conflicts": ["uci.edu", "princeton.edu"], "authors": ["Eric Nalisnick", "Sachin Ravi"], "authorids": ["enalisni@uci.edu", "sachinr@cs.princeton.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1490028562826, "id": "ICLR.cc/2017/workshop/-/paper33/acceptance", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "SyGmHLfte", "replyto": "SyGmHLfte", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept", "Reject"]}}}, "nonreaders": [], "cdate": 1490028562826}}}, {"tddate": null, "tmdate": 1489461787146, "tcdate": 1489461787146, "number": 2, "id": "ry77zkHox", "invitation": "ICLR.cc/2017/workshop/-/paper33/official/review", "forum": "SyGmHLfte", "replyto": "SyGmHLfte", "signatures": ["ICLR.cc/2017/workshop/paper33/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper33/AnonReviewer2"], "content": {"title": "Interesting idea, but not exactly infinite-dimensional", "rating": "4: Ok but not good enough - rejection", "review": "This paper proposes a method for learning word embeddings with data-dependent dimensionality. Different words have different numbers of non-zero dimensions, which allows for more or less information to be stored in a given word embedding. The authors argue that this is important to reduce various kinds of over-fitting, which seems to make sense, at least intuitively.\n\nI like the general motivation and idea, but I don't entirely understand how the setup achieves the desired goal. If I understand correctly, the dot product between two embeddings has non-zero contributions for the first l_min components of the two embeddings, where l_min is the smaller of the two vector dimensionalities. This means that *both* vectors must be high-dimensional in order for the increased capacity of a one of the vectors to manifest. This property seems at odds with the information-storage argument given in the paper, which seems to imply that the dimensionality of a *single* embedding is all that is important. \n\nI also think that the title and much of the wording in the paper is misleading insofar as they suggest that the embeddings may be infinite-dimensional. Restricting \"the model to grow only one dimension at a time\" necessarily enforces that the embeddings remain finite-dimensional (given that there are a finite-number of time steps), so it seems like the entire analysis should be able to be recast in a finite-dimensional setting without loss of generality. In doing so, I think this would eliminate the need to consider how to account for divergences in the infinite-dimensional setting, and largely reduce the novelty of the proposed algorithm.\n\nI don't find the empirical evidence particularly convincing of the usefulness of the proposed approach.  All-in-all, while the idea is quite interesting, I think this paper lacks the theoretical justification and empirical basis to merit its acceptance as a workshop paper. ", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Infinite Dimensional Word Embeddings", "abstract": "We describe a method for learning word embeddings with data-dependent dimensionality.  Our Infinite Skip-Gram (iSG) and Infinite Continuous Bag-of-Words (iCBOW) are nonparametric analogs of Mikolov et al.'s (2013) well-known 'word2vec' models.  Vectors are made infinite dimensional by employing techniques used by Cote & Larochelle (2016) to define a RBM with an infinite number of hidden units.  We show qualitatively and quantitatively that the iSG and iCBOW are competitive with their fixed-dimension counterparts while having the ability to infer the appropriate capacity of each word representation.", "pdf": "/pdf/90e7c6f656a67800d772e23cfc50c9fc075d10f2.pdf", "TL;DR": "Word embeddings with stochastic dimensionality.", "paperhash": "nalisnick|infinite_dimensional_word_embeddings", "keywords": ["Natural language processing", "Unsupervised Learning"], "conflicts": ["uci.edu", "princeton.edu"], "authors": ["Eric Nalisnick", "Sachin Ravi"], "authorids": ["enalisni@uci.edu", "sachinr@cs.princeton.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489461787931, "id": "ICLR.cc/2017/workshop/-/paper33/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper33/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper33/AnonReviewer1", "ICLR.cc/2017/workshop/paper33/AnonReviewer2"], "reply": {"forum": "SyGmHLfte", "replyto": "SyGmHLfte", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper33/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper33/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489461787931}}}, {"tddate": null, "tmdate": 1489322566312, "tcdate": 1489322566312, "number": 1, "id": "ByRrzTMil", "invitation": "ICLR.cc/2017/workshop/-/paper33/official/review", "forum": "SyGmHLfte", "replyto": "SyGmHLfte", "signatures": ["ICLR.cc/2017/workshop/paper33/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper33/AnonReviewer1"], "content": {"title": "review", "rating": "7: Good paper, accept", "review": "This paper proposes a method to learn infinite dimensional word embeddings, based on the infinite RMB method of Cote and Larochelle (2016). \nThe paper applies the technique to learn infinite dimension Continuous Bag of Words and Skip Gram models, and show results on word similarity datasets.\n\nI think this is not a trivial extension of the original infinite RMB method.\nWhile the quantitative results are not very convincing (much worse than 200D CBOW and SG), it is a reasonably good workshop paper that offers interesting insights on how a word embedding model uses its dimensions to represent words with multiple senses.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Infinite Dimensional Word Embeddings", "abstract": "We describe a method for learning word embeddings with data-dependent dimensionality.  Our Infinite Skip-Gram (iSG) and Infinite Continuous Bag-of-Words (iCBOW) are nonparametric analogs of Mikolov et al.'s (2013) well-known 'word2vec' models.  Vectors are made infinite dimensional by employing techniques used by Cote & Larochelle (2016) to define a RBM with an infinite number of hidden units.  We show qualitatively and quantitatively that the iSG and iCBOW are competitive with their fixed-dimension counterparts while having the ability to infer the appropriate capacity of each word representation.", "pdf": "/pdf/90e7c6f656a67800d772e23cfc50c9fc075d10f2.pdf", "TL;DR": "Word embeddings with stochastic dimensionality.", "paperhash": "nalisnick|infinite_dimensional_word_embeddings", "keywords": ["Natural language processing", "Unsupervised Learning"], "conflicts": ["uci.edu", "princeton.edu"], "authors": ["Eric Nalisnick", "Sachin Ravi"], "authorids": ["enalisni@uci.edu", "sachinr@cs.princeton.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489461787931, "id": "ICLR.cc/2017/workshop/-/paper33/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper33/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper33/AnonReviewer1", "ICLR.cc/2017/workshop/paper33/AnonReviewer2"], "reply": {"forum": "SyGmHLfte", "replyto": "SyGmHLfte", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper33/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper33/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489461787931}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1487197466490, "tcdate": 1487197466490, "number": 33, "id": "SyGmHLfte", "invitation": "ICLR.cc/2017/workshop/-/submission", "forum": "SyGmHLfte", "signatures": ["~Eric_Nalisnick1"], "readers": ["everyone"], "content": {"title": "Infinite Dimensional Word Embeddings", "abstract": "We describe a method for learning word embeddings with data-dependent dimensionality.  Our Infinite Skip-Gram (iSG) and Infinite Continuous Bag-of-Words (iCBOW) are nonparametric analogs of Mikolov et al.'s (2013) well-known 'word2vec' models.  Vectors are made infinite dimensional by employing techniques used by Cote & Larochelle (2016) to define a RBM with an infinite number of hidden units.  We show qualitatively and quantitatively that the iSG and iCBOW are competitive with their fixed-dimension counterparts while having the ability to infer the appropriate capacity of each word representation.", "pdf": "/pdf/90e7c6f656a67800d772e23cfc50c9fc075d10f2.pdf", "TL;DR": "Word embeddings with stochastic dimensionality.", "paperhash": "nalisnick|infinite_dimensional_word_embeddings", "keywords": ["Natural language processing", "Unsupervised Learning"], "conflicts": ["uci.edu", "princeton.edu"], "authors": ["Eric Nalisnick", "Sachin Ravi"], "authorids": ["enalisni@uci.edu", "sachinr@cs.princeton.edu"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 3, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1487690420000, "tmdate": 1484242559574, "id": "ICLR.cc/2017/workshop/-/submission", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1495466420000, "cdate": 1484242559574}}}], "count": 4}