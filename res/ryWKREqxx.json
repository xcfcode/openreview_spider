{"notes": [{"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396446005, "tcdate": 1486396446005, "number": 1, "id": "BJ8QhG8_g", "invitation": "ICLR.cc/2017/conference/-/paper227/acceptance", "forum": "ryWKREqxx", "replyto": "ryWKREqxx", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision", "comment": "The reviewers mostly agree that this paper offers valuable insights about a family of problems in automatic \"reading\" of text, as well as current solutions. The paper seems to fail to generate excitement because it doesn't really point the way forward. I disagree with some reviewers about the work's suitability for ICLR (as opposed to an ACL venue) since ML researchers are also thinking about these tasks now. The consensus is that the paper will have greater impact (wherever it is published) with a clearer message."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Emergent Predication Structure in Vector Representations of Neural Readers", "abstract": "Reading comprehension is a question answering task where the answer is to be found in a given passage about entities and events not mentioned in general knowledge sources. A significant number of neural architectures for this task (neural readers) have recently been developed and evaluated on large cloze-style datasets. We present experiments supporting the emergence of \u201cpredication structure\u201d in the hidden state vectors of a class of neural readers including the Attentive Reader and Stanford Reader. We posits that the hidden state vectors can be viewed as (a representation of) a concatenation [P, c] of a \u201cpredicate vector\u201d P and a \u201cconstant symbol vector\u201d c and that the hidden state represents the atomic formula P(c). This predication structure plays a conceptual role in relating \u201caggregation readers\u201d such as the Attentive Reader and the Stanford Reader to \u201cexplicit reference readers\u201d such as the Attention-Sum Reader, the Gated-Attention Reader and the Attention-over-Attention Reader. In an independent contribution, we show that the addition of linguistics features to the input to existing neural readers significantly boosts performance yielding the best results to date on the Who-did-What dataset.", "pdf": "/pdf/f63096e07757781f0e672bf98fdf9d45af5502d0.pdf", "TL;DR": "Provide some novel insights on reading comprehension models and boost the performance of those models", "paperhash": "wang|emergent_predication_structure_in_vector_representations_of_neural_readers", "conflicts": ["ttic.edu"], "keywords": ["Natural language processing", "Deep learning", "Applications"], "authors": ["Hai Wang", "Takeshi Onishi", "Kevin Gimpel", "David McAllester"], "authorids": ["haiwang@ttic.edu", "tonishi@ttic.edu", "kgimpel@ttic.edu", "mcallester@ttic.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396446560, "id": "ICLR.cc/2017/conference/-/paper227/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "ryWKREqxx", "replyto": "ryWKREqxx", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396446560}}}, {"tddate": null, "tmdate": 1483981943475, "tcdate": 1483981943475, "number": 6, "id": "BkkKErZUl", "invitation": "ICLR.cc/2017/conference/-/paper227/public/comment", "forum": "ryWKREqxx", "replyto": "Skn0jlpXg", "signatures": ["~Hai_Wang1"], "readers": ["everyone"], "writers": ["~Hai_Wang1"], "content": {"title": "Rebuttal to Reviewer2", "comment": "First of all, thanks for this valuable suggestion. Below are our answers for the question.  \n\nReviewer: However, I found the organization and the overall message of this paper quite confusing. First of all, it feels that the authors want to explain the above behavior with some definition of the \u201cstructures\u201d. However, I am not sure that how successful the attempt is. For me, it is still not clear what the structures are. This makes reading section 4 a bit frustrating. \n\nResponse: Our claim is that in the aggregation reader the hidden state vector can be decomposed into two parts --- a semantics-free entity pointer and a property being stated of that pointer.  See our earlier comments on predication structure.\n\nReviewer: I am also not sure what is the take home message of this paper. Does it mean that the entity marking should be used in the MR models? Should we design models that can also model the entity reference at the same time? What are the roles of the linguistic features here? Should we use linguistic structure to overcome the reference issue?\n\n\nResponse: \nQ: Does it mean that the entity marking should be used in the MR models?\nQ: Should we design models that can also model the entity reference at the same time?\n\nEntity pointers contain reference information.  In the case of multiple choice reading comprehension questions the pointers indicate which answer choice is being referred to.  All high performing reading comprehension systems rely on reference annotation of some sort.  Anonymization provides reference annotation because it replaces entities in both the answer choices and the passage with entity identifiers making the coreference apparent.  But if we eventually want to move to doing the coreference within the deep model then entity pointers seem essential. It seems important to note that aggregation readers are already constructing entity pointers.\n\nQ: What are the roles of the linguistic features here? Should we use linguistic structure to overcome the reference issue?\n\nOur linguistic features provide some simple information that seems orthogonal to the issue of entity pointers and predication structure. The fact that we observe consistent improvement by adding these features suggests that the readers are not being learned to capture this information otherwise. We find improvements from features with both an aggregation reader and an explicit reference reader (see Table 3). Like the above point about modeling coreference better in the deep models themselves, these findings suggest that another future research direction is to seek to capture simple beneficial linguistics features in the readers themselves. We will make this more clear in the paper. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Emergent Predication Structure in Vector Representations of Neural Readers", "abstract": "Reading comprehension is a question answering task where the answer is to be found in a given passage about entities and events not mentioned in general knowledge sources. A significant number of neural architectures for this task (neural readers) have recently been developed and evaluated on large cloze-style datasets. We present experiments supporting the emergence of \u201cpredication structure\u201d in the hidden state vectors of a class of neural readers including the Attentive Reader and Stanford Reader. We posits that the hidden state vectors can be viewed as (a representation of) a concatenation [P, c] of a \u201cpredicate vector\u201d P and a \u201cconstant symbol vector\u201d c and that the hidden state represents the atomic formula P(c). This predication structure plays a conceptual role in relating \u201caggregation readers\u201d such as the Attentive Reader and the Stanford Reader to \u201cexplicit reference readers\u201d such as the Attention-Sum Reader, the Gated-Attention Reader and the Attention-over-Attention Reader. In an independent contribution, we show that the addition of linguistics features to the input to existing neural readers significantly boosts performance yielding the best results to date on the Who-did-What dataset.", "pdf": "/pdf/f63096e07757781f0e672bf98fdf9d45af5502d0.pdf", "TL;DR": "Provide some novel insights on reading comprehension models and boost the performance of those models", "paperhash": "wang|emergent_predication_structure_in_vector_representations_of_neural_readers", "conflicts": ["ttic.edu"], "keywords": ["Natural language processing", "Deep learning", "Applications"], "authors": ["Hai Wang", "Takeshi Onishi", "Kevin Gimpel", "David McAllester"], "authorids": ["haiwang@ttic.edu", "tonishi@ttic.edu", "kgimpel@ttic.edu", "mcallester@ttic.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287675641, "id": "ICLR.cc/2017/conference/-/paper227/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "ryWKREqxx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper227/reviewers", "ICLR.cc/2017/conference/paper227/areachairs"], "cdate": 1485287675641}}}, {"tddate": null, "tmdate": 1482263181049, "tcdate": 1482263181049, "number": 3, "id": "HkB55bwVx", "invitation": "ICLR.cc/2017/conference/-/paper227/official/review", "forum": "ryWKREqxx", "replyto": "ryWKREqxx", "signatures": ["ICLR.cc/2017/conference/paper227/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper227/AnonReviewer1"], "content": {"title": "Simple predicate structure and data set", "rating": "6: Marginally above acceptance threshold", "review": "The paper aims to consolidate some recent literature in simple types of \"reading comprehension\" tasks involving matching questions to answers to be found in a passage, and then to explore the types of structure learned by these models and propose modifications. These reading comprehension datasets such as CNN/Daily Mail are on the simpler side because they do not generally involve chains of reasoning over multiple pieces of supporting evidence as can be found in datasets like MCTest. Many models have been proposed for this task, and the paper breaks down these models into \"aggregation readers\" and \"explicit reference readers.\" The authors show that the aggregation readers organize their hidden states into a predicate structure which allows them to mimic the explicit reference readers. The authors then experiment with adding linguistic features, including reference features, to the existing models to improve performance.\n\nI appreciate the re-naming and re-writing of the paper to make it more clear that the aggregation readers are specifically learning a predicate structure, as well as the inclusion of results about dimensionality of the symbol space. Further, I think the effort to organize and categorize several different reading comprehension models into broader classes is useful, as the field has been producing many such models and the landscape is unclear. \n\nThe concerns with this paper are that the predicate structure demonstrated is fairly simple, and it is not clear that it provides insight towards the development of better models in the future, since the \"explicit reference readers\" need not learn it, and the CNN/Daily Mail dataset has very little headroom left as demonstrated by Chen et al. 2016. The desire for \"dramatic improvements in performance\" mentioned in the discussion section probably cannot be achieved on these datasets. More complex datasets would probably involve multi-hop inference which this paper does not discuss. Further, the message of the paper is a bit scattered and hard to parse, and could benefit from a bit more focus.\n\nI think that with the explosion of various competing neural network models for NLP tasks, contributions like this one which attempt to organize and analyze the landscape are valuable, but that this paper might be better suited for an NLP conference or journal such as TACL.\n", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Emergent Predication Structure in Vector Representations of Neural Readers", "abstract": "Reading comprehension is a question answering task where the answer is to be found in a given passage about entities and events not mentioned in general knowledge sources. A significant number of neural architectures for this task (neural readers) have recently been developed and evaluated on large cloze-style datasets. We present experiments supporting the emergence of \u201cpredication structure\u201d in the hidden state vectors of a class of neural readers including the Attentive Reader and Stanford Reader. We posits that the hidden state vectors can be viewed as (a representation of) a concatenation [P, c] of a \u201cpredicate vector\u201d P and a \u201cconstant symbol vector\u201d c and that the hidden state represents the atomic formula P(c). This predication structure plays a conceptual role in relating \u201caggregation readers\u201d such as the Attentive Reader and the Stanford Reader to \u201cexplicit reference readers\u201d such as the Attention-Sum Reader, the Gated-Attention Reader and the Attention-over-Attention Reader. In an independent contribution, we show that the addition of linguistics features to the input to existing neural readers significantly boosts performance yielding the best results to date on the Who-did-What dataset.", "pdf": "/pdf/f63096e07757781f0e672bf98fdf9d45af5502d0.pdf", "TL;DR": "Provide some novel insights on reading comprehension models and boost the performance of those models", "paperhash": "wang|emergent_predication_structure_in_vector_representations_of_neural_readers", "conflicts": ["ttic.edu"], "keywords": ["Natural language processing", "Deep learning", "Applications"], "authors": ["Hai Wang", "Takeshi Onishi", "Kevin Gimpel", "David McAllester"], "authorids": ["haiwang@ttic.edu", "tonishi@ttic.edu", "kgimpel@ttic.edu", "mcallester@ttic.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512656307, "id": "ICLR.cc/2017/conference/-/paper227/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper227/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper227/AnonReviewer2", "ICLR.cc/2017/conference/paper227/AnonReviewer3", "ICLR.cc/2017/conference/paper227/AnonReviewer1"], "reply": {"forum": "ryWKREqxx", "replyto": "ryWKREqxx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper227/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper227/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512656307}}}, {"tddate": null, "tmdate": 1482016731214, "tcdate": 1482016613517, "number": 2, "id": "ByaPPS7Vl", "invitation": "ICLR.cc/2017/conference/-/paper227/official/review", "forum": "ryWKREqxx", "replyto": "ryWKREqxx", "signatures": ["ICLR.cc/2017/conference/paper227/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper227/AnonReviewer3"], "content": {"title": "", "rating": "6: Marginally above acceptance threshold", "review": "This paper aims to provide an insightful and analytic survey over the recent literature on reading comprehension with the distinct goal of investigating whether logical structure (or predication, as the authors rephrased in their response) arises in many of the recent models. I really like the spirit of the paper and appreciate the efforts to organize rather chaotic recent literature into two unified themes: \"aggregation readers\" and \"explicit reference models\u201d. Overall the quality of writing is great and section 3 was especially nice to read. I\u2019m also happy with the proposed rewording from \"logical structure\" to \u201cpredication\", and the clarification by the authors was detailed and helpful.\n\nI think I still have slight mixed feelings about the contribution of the work. First, I wonder whether the choice of the dataset was ideal in the first place to accomplish the desired goal of the paper. There have been concerns about CNN/DailyMail dataset (Chen et al. ACL\u201916) and it is not clear to me whether the dataset supports investigation on logical structure of interesting kinds. Maybe it is bound to be rather about lack of logical structure.\n\nSecond, I wish the discussion on predication sheds more practical insights into dataset design or model design to better tackle reading comprehension challenges. In that sense, it may have been more helpful if the authors could make more precise analysis on different types of reading comprehension challenges, what types of logical structure are lacking in various existing models and datasets, and point to specific directions where the community needs to focus more.\n\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Emergent Predication Structure in Vector Representations of Neural Readers", "abstract": "Reading comprehension is a question answering task where the answer is to be found in a given passage about entities and events not mentioned in general knowledge sources. A significant number of neural architectures for this task (neural readers) have recently been developed and evaluated on large cloze-style datasets. We present experiments supporting the emergence of \u201cpredication structure\u201d in the hidden state vectors of a class of neural readers including the Attentive Reader and Stanford Reader. We posits that the hidden state vectors can be viewed as (a representation of) a concatenation [P, c] of a \u201cpredicate vector\u201d P and a \u201cconstant symbol vector\u201d c and that the hidden state represents the atomic formula P(c). This predication structure plays a conceptual role in relating \u201caggregation readers\u201d such as the Attentive Reader and the Stanford Reader to \u201cexplicit reference readers\u201d such as the Attention-Sum Reader, the Gated-Attention Reader and the Attention-over-Attention Reader. In an independent contribution, we show that the addition of linguistics features to the input to existing neural readers significantly boosts performance yielding the best results to date on the Who-did-What dataset.", "pdf": "/pdf/f63096e07757781f0e672bf98fdf9d45af5502d0.pdf", "TL;DR": "Provide some novel insights on reading comprehension models and boost the performance of those models", "paperhash": "wang|emergent_predication_structure_in_vector_representations_of_neural_readers", "conflicts": ["ttic.edu"], "keywords": ["Natural language processing", "Deep learning", "Applications"], "authors": ["Hai Wang", "Takeshi Onishi", "Kevin Gimpel", "David McAllester"], "authorids": ["haiwang@ttic.edu", "tonishi@ttic.edu", "kgimpel@ttic.edu", "mcallester@ttic.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512656307, "id": "ICLR.cc/2017/conference/-/paper227/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper227/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper227/AnonReviewer2", "ICLR.cc/2017/conference/paper227/AnonReviewer3", "ICLR.cc/2017/conference/paper227/AnonReviewer1"], "reply": {"forum": "ryWKREqxx", "replyto": "ryWKREqxx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper227/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper227/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512656307}}}, {"tddate": null, "tmdate": 1481732186597, "tcdate": 1481732186590, "number": 5, "id": "BJ7DegkVl", "invitation": "ICLR.cc/2017/conference/-/paper227/public/comment", "forum": "ryWKREqxx", "replyto": "ryWKREqxx", "signatures": ["~Hai_Wang1"], "readers": ["everyone"], "writers": ["~Hai_Wang1"], "content": {"title": "Update the papers", "comment": "Hi, Dear reviewers,\n     \n   Thanks for the valuable suggestions on our paper and sorry to make you confused about the notations, we have updated the paper to make it more clear. Please see the latest version."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Emergent Predication Structure in Vector Representations of Neural Readers", "abstract": "Reading comprehension is a question answering task where the answer is to be found in a given passage about entities and events not mentioned in general knowledge sources. A significant number of neural architectures for this task (neural readers) have recently been developed and evaluated on large cloze-style datasets. We present experiments supporting the emergence of \u201cpredication structure\u201d in the hidden state vectors of a class of neural readers including the Attentive Reader and Stanford Reader. We posits that the hidden state vectors can be viewed as (a representation of) a concatenation [P, c] of a \u201cpredicate vector\u201d P and a \u201cconstant symbol vector\u201d c and that the hidden state represents the atomic formula P(c). This predication structure plays a conceptual role in relating \u201caggregation readers\u201d such as the Attentive Reader and the Stanford Reader to \u201cexplicit reference readers\u201d such as the Attention-Sum Reader, the Gated-Attention Reader and the Attention-over-Attention Reader. In an independent contribution, we show that the addition of linguistics features to the input to existing neural readers significantly boosts performance yielding the best results to date on the Who-did-What dataset.", "pdf": "/pdf/f63096e07757781f0e672bf98fdf9d45af5502d0.pdf", "TL;DR": "Provide some novel insights on reading comprehension models and boost the performance of those models", "paperhash": "wang|emergent_predication_structure_in_vector_representations_of_neural_readers", "conflicts": ["ttic.edu"], "keywords": ["Natural language processing", "Deep learning", "Applications"], "authors": ["Hai Wang", "Takeshi Onishi", "Kevin Gimpel", "David McAllester"], "authorids": ["haiwang@ttic.edu", "tonishi@ttic.edu", "kgimpel@ttic.edu", "mcallester@ttic.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287675641, "id": "ICLR.cc/2017/conference/-/paper227/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "ryWKREqxx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper227/reviewers", "ICLR.cc/2017/conference/paper227/areachairs"], "cdate": 1485287675641}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1481731984461, "tcdate": 1478278777121, "number": 227, "id": "ryWKREqxx", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "ryWKREqxx", "signatures": ["~Hai_Wang1"], "readers": ["everyone"], "content": {"title": "Emergent Predication Structure in Vector Representations of Neural Readers", "abstract": "Reading comprehension is a question answering task where the answer is to be found in a given passage about entities and events not mentioned in general knowledge sources. A significant number of neural architectures for this task (neural readers) have recently been developed and evaluated on large cloze-style datasets. We present experiments supporting the emergence of \u201cpredication structure\u201d in the hidden state vectors of a class of neural readers including the Attentive Reader and Stanford Reader. We posits that the hidden state vectors can be viewed as (a representation of) a concatenation [P, c] of a \u201cpredicate vector\u201d P and a \u201cconstant symbol vector\u201d c and that the hidden state represents the atomic formula P(c). This predication structure plays a conceptual role in relating \u201caggregation readers\u201d such as the Attentive Reader and the Stanford Reader to \u201cexplicit reference readers\u201d such as the Attention-Sum Reader, the Gated-Attention Reader and the Attention-over-Attention Reader. In an independent contribution, we show that the addition of linguistics features to the input to existing neural readers significantly boosts performance yielding the best results to date on the Who-did-What dataset.", "pdf": "/pdf/f63096e07757781f0e672bf98fdf9d45af5502d0.pdf", "TL;DR": "Provide some novel insights on reading comprehension models and boost the performance of those models", "paperhash": "wang|emergent_predication_structure_in_vector_representations_of_neural_readers", "conflicts": ["ttic.edu"], "keywords": ["Natural language processing", "Deep learning", "Applications"], "authors": ["Hai Wang", "Takeshi Onishi", "Kevin Gimpel", "David McAllester"], "authorids": ["haiwang@ttic.edu", "tonishi@ttic.edu", "kgimpel@ttic.edu", "mcallester@ttic.edu"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 12, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "tmdate": 1481718470477, "tcdate": 1481045857535, "number": 1, "id": "ryKDwuE7e", "invitation": "ICLR.cc/2017/conference/-/paper227/public/comment", "forum": "ryWKREqxx", "replyto": "Hy6jgHx7x", "signatures": ["~David_McAllester1"], "readers": ["everyone"], "writers": ["~David_McAllester1"], "content": {"title": "replies to reviewer 1", "comment": "Reviewer 1, we greatly appreciate your careful analysis of our paper.\n\n\nQuestion: In what sense is this a logical structure? A main finding of the paper seems to be the section after equation 11 where it is shown that the aggregation readers effectively learn to mimic the explicit reference readers. This is an emergent structure, but what sort of logic does it encode? Is the term \"logic\" in reference to the symbolic nature of the learned anonymized entity embeddings? \n\n\nReply: We are arguing for the emergence of \u201cpredication\u201d.  We will change the title to \u201cEmergent Predication Structure \u2026\u201d.  By predication we mean that the hidden state h_t of the passage at position (or word) t can be viewed as a pair (Phi_t, c_t) where Phi_t is a property (or statement or predicate) being stated of a particular constant symbol c_t at position t in the passage.  A logician might write this as h_t = Phi_t[c_t].  We present evidence that the hidden state vector h_t can be \u201cparsed\u201d into its parts Phi_t and c_t and that these parts can be interpreted as a predicate and a constant symbol.\n\n\nA question has the form Psi[x] --- a question asks for a candidate answer c such that the passage implies Psi[c].  At a simplified level we are looking for a place t in the passage such that the statement Phi_t[x] being expressed at position t implies Psi[x] (for any x).  In this case the answer is the constant symbol c_t being \u201ctalked about\u201d at position t.\n\n\nTo allow for a combination of evidence from different statements in the passage the answer selection might be\n\n\n       (1) argmax_c   sum_t    <e(Phi_t),e(Psi)>  <e(c_t),e(c)>.\n\n\nThe first inner product is interpreted as measuring the extent to which Phi_t[x] implies Psi[x] for any x.  The second inner product should be near zero if c_t is different from c and near a large constant if c_t = c.\n\n\nSelection rule (1) is not built into aggregation readers. In an aggregation reader the training objective has the form\n\n\n      (2)  argmax_c  sum_t   <h_t,q>  <h_t,e(c)>.\n\n\nHere q is the vector representation of the question.  Equations (1) and (2) agree if q is interpreted as e(Psi) and h_t is a direct sum of e(Phi_t) and e(c_t).  We claim that this structure of q and h_t emerges from training on (2).  This claim is supported in table 1 and figure 1.\n\n\nSelection rules (1) and (2) also address a question of reviewer 3 --- the support for an answer can indeed combine several statements in the passage.\n\n\nWe will revise the paper shortly to make the intended structure clearer.\n\nQuestion: (11) and (12) imply a H = S (+) E direct sum structure for the representations? This is because the entity identifiers, being semantics-free, must be permutable (approximately) without changing the model scores, correct? Do the authors have a sense of how big the E subspace must be to handle a given number of entity vectors? Is it equal to the maximum number of entities in a document?\n\n\nReply: Yes, our intuition started from the the fact that the entity identifiers can be permuted and therefore must be semantics free.  Even for non-anonymized data such as Who-Did-What, annotating the passage with which places in the passage refer to which answers improves performance.  We believe that even in this case an \u201canswer pointer\u201d is coded in the hidden state and equations (11) and (12) appear to hold.  This is supported by the performance of the one-hot pointer reader in table 2.\n\n\nIn the case of CNN there are about 500 entities but the entity embedding is only 100 dimensional.  We did some experiments with \u201ctwo-sparse\u201d entity id embeddings which were designed and not trained.  These did reasonably well.  These codings give 2d(d-1) distinct embeddings in dimension d (there are d choose 2 sparsity patterns and 4 choices of the sign of the non-zero values). We looked at the literature on packing vectors on the surface of a sphere where we found the suggestion of two-sparse vectors.  This generalizes to k-sparse vectors but we have no experiments for k > 2. We will add some discussion of this in the next revision.\n\nQuestion: Regarding equation 11, it is unclear why the inner-product of h_{t} and the answer embedding would be a fixed positive constant 'c' - especially since h_{t} is the embedding obtained by running a bi-LSTM/GRU encoder on the paragraph. i.e. for all t in R(a,p), the h_t's will encode everything it has seen before. However it is intuitive that since all t's in R(a,p) refers to positions of answers in the document, the inner product would be high (but not necessarily fixed).\n\n\n\n\nReply: We select the entity identifier with the largest inner product with the weighted hidden state vector.   If one of the entity identifier embeddings had larger norm than the others then the selection process would be biased toward that entity identifier.  But the system should be invariant to permuting the identifiers.  The caption of figure 1 gives the mean and variance of the square norm of the entity embeddings and indeed the variance (17) is small compared to the mean (169).  We will say something about this in the revised version.\n\nQuestion: Also in equation 11, it is also very counter-intuitive that for all t not in R(a,p) the inner product of h_{t} and the answer embedding would be zero. For example, consider the embedding of h_{t+1}. Since this word is just after the answer word, will it have a very low inner product? It would be nice to add this number after row 2 in table 1.\n\n\nReply: We will add this to table 1.\n\n\nIt should be noted that a precise word position for each candidate answer is given to the system. In the case of CNN it is the position of the anonymized entity identifier.  In other cases it is given by explicit reference annotation.  The attention heatmaps show that the attention is highly focused on the precise word of the given candidate answer positions.  So the equivalence of aggregation readers and explicit reference readers does not require that (11) holds at non-answer positions adjacent to answer positions.  But if the attention is highly focused on the precise answer positions there is no reason to remember answer embeddings --- the LSTM (or GRU) could immediately forget them to leave room in the hidden state for more useful information. But we will do the measurement.\n\nQuestion: It is hard to follow equation 12. A new variable h' has been introduced just before which has not been used in the equation which makes it even more confusing. I think the paper would benefit a lot if section 4 is re-visited and written more clearly because I think it is the most important section of the paper (especially explanation/intuition behind equation 12 and 13).\n\n\nReply: The introduction of h\u2019 in the discussion is simply to formalize the invariance to permutation of the entity identifiers.  We may drop this and just say \u201cbecause the system should be invariant to entity identifier permutation \u2026\u201d.  We will definitely rework section 4.\n\nQuestion: Equation 13 suggests the existence of a sum-structure between two orthogonal subspaces S (subspace of statement vectors) and E(subspace of entity pointers). Do the experiment in section 5 explicitly capture this? It is clear to me that the one-hot and general pointer encoding are ways to inject coreference information in the model but does it explain the additive nature of the embeddings (since the two vectors are concatenated).\n\n\nReply:  In general concatenation guarantees sum-structure --- [x,y] can be uniquely decomposed as [x,0] + [0,y] where the addends are orthogonal.  It would be simpler to work just with concatenation.  However, the learning system need not make the orthogonal subspaces S and E axis-aligned.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Emergent Predication Structure in Vector Representations of Neural Readers", "abstract": "Reading comprehension is a question answering task where the answer is to be found in a given passage about entities and events not mentioned in general knowledge sources. A significant number of neural architectures for this task (neural readers) have recently been developed and evaluated on large cloze-style datasets. We present experiments supporting the emergence of \u201cpredication structure\u201d in the hidden state vectors of a class of neural readers including the Attentive Reader and Stanford Reader. We posits that the hidden state vectors can be viewed as (a representation of) a concatenation [P, c] of a \u201cpredicate vector\u201d P and a \u201cconstant symbol vector\u201d c and that the hidden state represents the atomic formula P(c). This predication structure plays a conceptual role in relating \u201caggregation readers\u201d such as the Attentive Reader and the Stanford Reader to \u201cexplicit reference readers\u201d such as the Attention-Sum Reader, the Gated-Attention Reader and the Attention-over-Attention Reader. In an independent contribution, we show that the addition of linguistics features to the input to existing neural readers significantly boosts performance yielding the best results to date on the Who-did-What dataset.", "pdf": "/pdf/f63096e07757781f0e672bf98fdf9d45af5502d0.pdf", "TL;DR": "Provide some novel insights on reading comprehension models and boost the performance of those models", "paperhash": "wang|emergent_predication_structure_in_vector_representations_of_neural_readers", "conflicts": ["ttic.edu"], "keywords": ["Natural language processing", "Deep learning", "Applications"], "authors": ["Hai Wang", "Takeshi Onishi", "Kevin Gimpel", "David McAllester"], "authorids": ["haiwang@ttic.edu", "tonishi@ttic.edu", "kgimpel@ttic.edu", "mcallester@ttic.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287675641, "id": "ICLR.cc/2017/conference/-/paper227/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "ryWKREqxx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper227/reviewers", "ICLR.cc/2017/conference/paper227/areachairs"], "cdate": 1485287675641}}}, {"tddate": null, "tmdate": 1481604051671, "tcdate": 1481604051663, "number": 1, "id": "Skn0jlpXg", "invitation": "ICLR.cc/2017/conference/-/paper227/official/review", "forum": "ryWKREqxx", "replyto": "ryWKREqxx", "signatures": ["ICLR.cc/2017/conference/paper227/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper227/AnonReviewer2"], "content": {"title": "Review", "rating": "5: Marginally below acceptance threshold", "review": "The paper proposed to analyze several recently developed machine readers and found that some machine readers could potentially take advantages of the entity marker (given that the same marker points out to the same entity). I usually like analysis papers, but I found the argument proposed in this paper not very clear.\n\nI like the experiments on the Stanford reader, which shows that the entity marker in fact helps the Stanford reader on WDW. I found that results rather interesting.\n\nHowever, I found the organization and the overall message of this paper quite confusing. First of all, it feels that the authors want to explain the above behavior with some definition of the \u201cstructures\u201d. However, I am not sure that how successful the attempt is. For me, it is still not clear what the structures are. This makes reading section 4 a bit frustrating. \n\nI am also not sure what is the take home message of this paper. Does it mean that the entity marking should be used in the MR models? Should we design models that can also model the entity reference at the same time? What are the roles of the linguistic features here? Should we use linguistic structure to overcome the reference issue?\n\nOverall, I feel that the analysis is interesting, but I feel that the paper can benefit from having a more focused argument.\n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Emergent Predication Structure in Vector Representations of Neural Readers", "abstract": "Reading comprehension is a question answering task where the answer is to be found in a given passage about entities and events not mentioned in general knowledge sources. A significant number of neural architectures for this task (neural readers) have recently been developed and evaluated on large cloze-style datasets. We present experiments supporting the emergence of \u201cpredication structure\u201d in the hidden state vectors of a class of neural readers including the Attentive Reader and Stanford Reader. We posits that the hidden state vectors can be viewed as (a representation of) a concatenation [P, c] of a \u201cpredicate vector\u201d P and a \u201cconstant symbol vector\u201d c and that the hidden state represents the atomic formula P(c). This predication structure plays a conceptual role in relating \u201caggregation readers\u201d such as the Attentive Reader and the Stanford Reader to \u201cexplicit reference readers\u201d such as the Attention-Sum Reader, the Gated-Attention Reader and the Attention-over-Attention Reader. In an independent contribution, we show that the addition of linguistics features to the input to existing neural readers significantly boosts performance yielding the best results to date on the Who-did-What dataset.", "pdf": "/pdf/f63096e07757781f0e672bf98fdf9d45af5502d0.pdf", "TL;DR": "Provide some novel insights on reading comprehension models and boost the performance of those models", "paperhash": "wang|emergent_predication_structure_in_vector_representations_of_neural_readers", "conflicts": ["ttic.edu"], "keywords": ["Natural language processing", "Deep learning", "Applications"], "authors": ["Hai Wang", "Takeshi Onishi", "Kevin Gimpel", "David McAllester"], "authorids": ["haiwang@ttic.edu", "tonishi@ttic.edu", "kgimpel@ttic.edu", "mcallester@ttic.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512656307, "id": "ICLR.cc/2017/conference/-/paper227/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper227/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper227/AnonReviewer2", "ICLR.cc/2017/conference/paper227/AnonReviewer3", "ICLR.cc/2017/conference/paper227/AnonReviewer1"], "reply": {"forum": "ryWKREqxx", "replyto": "ryWKREqxx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper227/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper227/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512656307}}}, {"tddate": null, "tmdate": 1481072410773, "tcdate": 1481072410764, "number": 4, "id": "B17mJ1Sml", "invitation": "ICLR.cc/2017/conference/-/paper227/public/comment", "forum": "ryWKREqxx", "replyto": "SkNuJqRzx", "signatures": ["~David_McAllester1"], "readers": ["everyone"], "writers": ["~David_McAllester1"], "content": {"title": "reply to reviewer 3", "comment": "All reviewers asked for clarification of section 4.  Please see the detailed response to the first question of reviewer 1.  This response also addresses integration of information from different parts of the passage. We will rework section 4.  \n\n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Emergent Predication Structure in Vector Representations of Neural Readers", "abstract": "Reading comprehension is a question answering task where the answer is to be found in a given passage about entities and events not mentioned in general knowledge sources. A significant number of neural architectures for this task (neural readers) have recently been developed and evaluated on large cloze-style datasets. We present experiments supporting the emergence of \u201cpredication structure\u201d in the hidden state vectors of a class of neural readers including the Attentive Reader and Stanford Reader. We posits that the hidden state vectors can be viewed as (a representation of) a concatenation [P, c] of a \u201cpredicate vector\u201d P and a \u201cconstant symbol vector\u201d c and that the hidden state represents the atomic formula P(c). This predication structure plays a conceptual role in relating \u201caggregation readers\u201d such as the Attentive Reader and the Stanford Reader to \u201cexplicit reference readers\u201d such as the Attention-Sum Reader, the Gated-Attention Reader and the Attention-over-Attention Reader. In an independent contribution, we show that the addition of linguistics features to the input to existing neural readers significantly boosts performance yielding the best results to date on the Who-did-What dataset.", "pdf": "/pdf/f63096e07757781f0e672bf98fdf9d45af5502d0.pdf", "TL;DR": "Provide some novel insights on reading comprehension models and boost the performance of those models", "paperhash": "wang|emergent_predication_structure_in_vector_representations_of_neural_readers", "conflicts": ["ttic.edu"], "keywords": ["Natural language processing", "Deep learning", "Applications"], "authors": ["Hai Wang", "Takeshi Onishi", "Kevin Gimpel", "David McAllester"], "authorids": ["haiwang@ttic.edu", "tonishi@ttic.edu", "kgimpel@ttic.edu", "mcallester@ttic.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287675641, "id": "ICLR.cc/2017/conference/-/paper227/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "ryWKREqxx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper227/reviewers", "ICLR.cc/2017/conference/paper227/areachairs"], "cdate": 1485287675641}}}, {"tddate": null, "tmdate": 1481070568203, "tcdate": 1481070568197, "number": 3, "id": "BkeeO0N7g", "invitation": "ICLR.cc/2017/conference/-/paper227/public/comment", "forum": "ryWKREqxx", "replyto": "S138stCzl", "signatures": ["~David_McAllester1"], "readers": ["everyone"], "writers": ["~David_McAllester1"], "content": {"title": "reply to reviewer 2", "comment": "Question: 1.\tIn table 2, what gain did you get by using NER and POS features? How much gain can be obtained from just the first three features?\n\n\nReply:\nNER and POS features are only used for the CBT experiments. For the anonymized WDW experiments, only the first three features were used. We will update the paper to make it more clear.\n\nQuestion: 2.\tI found table 1 and figure 1 are very confusing to me. Could the authors explain to me what kind of logical structure is being found by the network? \n\n\nReply: You are not the only one. :) Please see the detailed explanation in the answer to the first question of reviewer 1.  We will rework section 4 along the lines of the response to reviewer 1.\n\n\n\n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Emergent Predication Structure in Vector Representations of Neural Readers", "abstract": "Reading comprehension is a question answering task where the answer is to be found in a given passage about entities and events not mentioned in general knowledge sources. A significant number of neural architectures for this task (neural readers) have recently been developed and evaluated on large cloze-style datasets. We present experiments supporting the emergence of \u201cpredication structure\u201d in the hidden state vectors of a class of neural readers including the Attentive Reader and Stanford Reader. We posits that the hidden state vectors can be viewed as (a representation of) a concatenation [P, c] of a \u201cpredicate vector\u201d P and a \u201cconstant symbol vector\u201d c and that the hidden state represents the atomic formula P(c). This predication structure plays a conceptual role in relating \u201caggregation readers\u201d such as the Attentive Reader and the Stanford Reader to \u201cexplicit reference readers\u201d such as the Attention-Sum Reader, the Gated-Attention Reader and the Attention-over-Attention Reader. In an independent contribution, we show that the addition of linguistics features to the input to existing neural readers significantly boosts performance yielding the best results to date on the Who-did-What dataset.", "pdf": "/pdf/f63096e07757781f0e672bf98fdf9d45af5502d0.pdf", "TL;DR": "Provide some novel insights on reading comprehension models and boost the performance of those models", "paperhash": "wang|emergent_predication_structure_in_vector_representations_of_neural_readers", "conflicts": ["ttic.edu"], "keywords": ["Natural language processing", "Deep learning", "Applications"], "authors": ["Hai Wang", "Takeshi Onishi", "Kevin Gimpel", "David McAllester"], "authorids": ["haiwang@ttic.edu", "tonishi@ttic.edu", "kgimpel@ttic.edu", "mcallester@ttic.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287675641, "id": "ICLR.cc/2017/conference/-/paper227/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "ryWKREqxx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper227/reviewers", "ICLR.cc/2017/conference/paper227/areachairs"], "cdate": 1485287675641}}}, {"tddate": null, "tmdate": 1480770642140, "tcdate": 1480769700733, "number": 3, "id": "Hy6jgHx7x", "invitation": "ICLR.cc/2017/conference/-/paper227/pre-review/question", "forum": "ryWKREqxx", "replyto": "ryWKREqxx", "signatures": ["ICLR.cc/2017/conference/paper227/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper227/AnonReviewer1"], "content": {"title": "Questions", "question": "In what sense is this a logical structure? A main finding of the paper seems to be the section after equation 11 where it is shown that the aggregation readers effectively learn to mimic the explicit reference readers. This is an emergent structure, but what sort of logic does it encode? Is the term \"logic\" in reference to the symbolic nature of the learned anonymized entity embeddings? \n\n(11) and (12) imply a H = S (+) E direct sum structure for the representations? This is because the entity identifiers, being semantics-free, must be permutable (approximately) without changing the model scores, correct? Do the authors have a sense of how big the E subspace must be to handle a given number of entity vectors? Is it equal to the maximum number of entities in a document?\n\nRegarding equation 11, it is unclear why the inner-product of h_{t} and the answer embedding would be a fixed positive constant 'c' - especially since h_{t} is the embedding obtained by running a bi-LSTM/GRU encoder on the paragraph. i.e. for all t in R(a,p), the h_t's will encode everything it has seen before. However it is intuitive that since all t's in R(a,p) refers to positions of answers in the document, the inner product would be high (but not necessarily fixed).\n\nAlso in equation 11, it is also very counter-intuitive that for all t not in R(a,p) the inner product of h_{t} and the answer embedding would be zero. For example, consider the embedding of h_{t+1}. Since this word is just after the answer word, will it have a very low inner product? It would be nice to add this number after row 2 in table 1.\n\nIt is hard to follow equation 12. A new variable h' has been introduced just before which has not been used in the equation which makes it even more confusing. I think the paper would benefit a lot if section 4 is re-visited and written more clearly because I think it is the most important section of the paper (especially explanation/intuition behind equation 12 and 13).\n\nEquation 13 suggests the existence of a sum-structure between two orthogonal subspaces S (subspace of statement vectors) and E(subspace of entity pointers). Do the experiment in section 5 explicitly capture this? It is clear to me that the one-hot and general pointer encoding are ways to inject coreference information in the model but does it explain the additive nature of the embeddings (since the two vectors are concatenated).\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Emergent Predication Structure in Vector Representations of Neural Readers", "abstract": "Reading comprehension is a question answering task where the answer is to be found in a given passage about entities and events not mentioned in general knowledge sources. A significant number of neural architectures for this task (neural readers) have recently been developed and evaluated on large cloze-style datasets. We present experiments supporting the emergence of \u201cpredication structure\u201d in the hidden state vectors of a class of neural readers including the Attentive Reader and Stanford Reader. We posits that the hidden state vectors can be viewed as (a representation of) a concatenation [P, c] of a \u201cpredicate vector\u201d P and a \u201cconstant symbol vector\u201d c and that the hidden state represents the atomic formula P(c). This predication structure plays a conceptual role in relating \u201caggregation readers\u201d such as the Attentive Reader and the Stanford Reader to \u201cexplicit reference readers\u201d such as the Attention-Sum Reader, the Gated-Attention Reader and the Attention-over-Attention Reader. In an independent contribution, we show that the addition of linguistics features to the input to existing neural readers significantly boosts performance yielding the best results to date on the Who-did-What dataset.", "pdf": "/pdf/f63096e07757781f0e672bf98fdf9d45af5502d0.pdf", "TL;DR": "Provide some novel insights on reading comprehension models and boost the performance of those models", "paperhash": "wang|emergent_predication_structure_in_vector_representations_of_neural_readers", "conflicts": ["ttic.edu"], "keywords": ["Natural language processing", "Deep learning", "Applications"], "authors": ["Hai Wang", "Takeshi Onishi", "Kevin Gimpel", "David McAllester"], "authorids": ["haiwang@ttic.edu", "tonishi@ttic.edu", "kgimpel@ttic.edu", "mcallester@ttic.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959394376, "id": "ICLR.cc/2017/conference/-/paper227/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper227/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper227/AnonReviewer2", "ICLR.cc/2017/conference/paper227/AnonReviewer3", "ICLR.cc/2017/conference/paper227/AnonReviewer1"], "reply": {"forum": "ryWKREqxx", "replyto": "ryWKREqxx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper227/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper227/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959394376}}}, {"tddate": null, "tmdate": 1480658795573, "tcdate": 1480658795568, "number": 2, "id": "SkNuJqRzx", "invitation": "ICLR.cc/2017/conference/-/paper227/pre-review/question", "forum": "ryWKREqxx", "replyto": "ryWKREqxx", "signatures": ["ICLR.cc/2017/conference/paper227/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper227/AnonReviewer3"], "content": {"title": "logical structure", "question": "In section 4, the authors describe how logical structure is emergent in a variety of neural readers that were proposed for machine comprehension tasks like CNN/DailyMail. Is it correct to interpret that the logical structure summarized in equation 11 and Table 1 boils down to semantic similarity between the (attended) text snippet and the question? Or am I missing something here?\n\n(I think I was rather expecting to see conjunction, disjunction, if-then etc for logical structure...)\n\nAlso, can any of the neural readers discussed in the paper handle inference or reasoning that requires synthesizing information taken from more than one snippets from the given text?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Emergent Predication Structure in Vector Representations of Neural Readers", "abstract": "Reading comprehension is a question answering task where the answer is to be found in a given passage about entities and events not mentioned in general knowledge sources. A significant number of neural architectures for this task (neural readers) have recently been developed and evaluated on large cloze-style datasets. We present experiments supporting the emergence of \u201cpredication structure\u201d in the hidden state vectors of a class of neural readers including the Attentive Reader and Stanford Reader. We posits that the hidden state vectors can be viewed as (a representation of) a concatenation [P, c] of a \u201cpredicate vector\u201d P and a \u201cconstant symbol vector\u201d c and that the hidden state represents the atomic formula P(c). This predication structure plays a conceptual role in relating \u201caggregation readers\u201d such as the Attentive Reader and the Stanford Reader to \u201cexplicit reference readers\u201d such as the Attention-Sum Reader, the Gated-Attention Reader and the Attention-over-Attention Reader. In an independent contribution, we show that the addition of linguistics features to the input to existing neural readers significantly boosts performance yielding the best results to date on the Who-did-What dataset.", "pdf": "/pdf/f63096e07757781f0e672bf98fdf9d45af5502d0.pdf", "TL;DR": "Provide some novel insights on reading comprehension models and boost the performance of those models", "paperhash": "wang|emergent_predication_structure_in_vector_representations_of_neural_readers", "conflicts": ["ttic.edu"], "keywords": ["Natural language processing", "Deep learning", "Applications"], "authors": ["Hai Wang", "Takeshi Onishi", "Kevin Gimpel", "David McAllester"], "authorids": ["haiwang@ttic.edu", "tonishi@ttic.edu", "kgimpel@ttic.edu", "mcallester@ttic.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959394376, "id": "ICLR.cc/2017/conference/-/paper227/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper227/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper227/AnonReviewer2", "ICLR.cc/2017/conference/paper227/AnonReviewer3", "ICLR.cc/2017/conference/paper227/AnonReviewer1"], "reply": {"forum": "ryWKREqxx", "replyto": "ryWKREqxx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper227/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper227/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959394376}}}, {"tddate": null, "tmdate": 1480657747809, "tcdate": 1480657747805, "number": 1, "id": "S138stCzl", "invitation": "ICLR.cc/2017/conference/-/paper227/pre-review/question", "forum": "ryWKREqxx", "replyto": "ryWKREqxx", "signatures": ["ICLR.cc/2017/conference/paper227/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper227/AnonReviewer2"], "content": {"title": "Pre-review Questions", "question": "1.\tIn table 2, what gain did you get by using NER and POS features? How much gain can be obtained from just the first three features?\n2.\tI found table 1 and figure 1 are very confusing to me. Could the authors explain to me what kind of logical structure is being found by the network?  \n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Emergent Predication Structure in Vector Representations of Neural Readers", "abstract": "Reading comprehension is a question answering task where the answer is to be found in a given passage about entities and events not mentioned in general knowledge sources. A significant number of neural architectures for this task (neural readers) have recently been developed and evaluated on large cloze-style datasets. We present experiments supporting the emergence of \u201cpredication structure\u201d in the hidden state vectors of a class of neural readers including the Attentive Reader and Stanford Reader. We posits that the hidden state vectors can be viewed as (a representation of) a concatenation [P, c] of a \u201cpredicate vector\u201d P and a \u201cconstant symbol vector\u201d c and that the hidden state represents the atomic formula P(c). This predication structure plays a conceptual role in relating \u201caggregation readers\u201d such as the Attentive Reader and the Stanford Reader to \u201cexplicit reference readers\u201d such as the Attention-Sum Reader, the Gated-Attention Reader and the Attention-over-Attention Reader. In an independent contribution, we show that the addition of linguistics features to the input to existing neural readers significantly boosts performance yielding the best results to date on the Who-did-What dataset.", "pdf": "/pdf/f63096e07757781f0e672bf98fdf9d45af5502d0.pdf", "TL;DR": "Provide some novel insights on reading comprehension models and boost the performance of those models", "paperhash": "wang|emergent_predication_structure_in_vector_representations_of_neural_readers", "conflicts": ["ttic.edu"], "keywords": ["Natural language processing", "Deep learning", "Applications"], "authors": ["Hai Wang", "Takeshi Onishi", "Kevin Gimpel", "David McAllester"], "authorids": ["haiwang@ttic.edu", "tonishi@ttic.edu", "kgimpel@ttic.edu", "mcallester@ttic.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959394376, "id": "ICLR.cc/2017/conference/-/paper227/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper227/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper227/AnonReviewer2", "ICLR.cc/2017/conference/paper227/AnonReviewer3", "ICLR.cc/2017/conference/paper227/AnonReviewer1"], "reply": {"forum": "ryWKREqxx", "replyto": "ryWKREqxx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper227/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper227/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959394376}}}], "count": 13}