{"notes": [{"id": "54-QTuqSLyn", "original": "sSU_n6YVdl7", "number": 3091, "cdate": 1601308342966, "ddate": null, "tcdate": 1601308342966, "tmdate": 1614985645125, "tddate": null, "forum": "54-QTuqSLyn", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Mitigating Mode Collapse by Sidestepping Catastrophic Forgetting", "authorids": ["~Karttikeya_Mangalam1", "~Rohin_Garg1", "~Jathushan_Rajasegaran1", "~Taesung_Park2"], "authors": ["Karttikeya Mangalam", "Rohin Garg", "Jathushan Rajasegaran", "Taesung Park"], "keywords": ["mode collapse", "catastrophic forgetting", "multi-adversarial training"], "abstract": "Generative Adversarial Networks (GANs) are a class of generative models used for various applications, but they have been known to suffer from the mode collapse problem, in which some modes of the target distribution are ignored by the generator.  Investigative study using a new data generation procedure indicates that the mode collapse of the generator is driven by the discriminator\u2019s inability to maintain classification accuracy on previously seen samples, a phenomenon called Catastrophic Forgetting in continual learning. Motivated by this observation, we introduce a novel training procedure that dynamically spawns additional dis-criminators to remember previous modes of generation. On several datasets, we show that our training scheme can be plugged-in to existing GAN frameworks to mitigate mode collapse and improve standard metrics for GAN evaluation.", "one-sentence_summary": "We propose a relationship between catastrophic forgetting in discriminator and mode collapse in generator and propose a dynamic multi adversarial training (DMAT) solution to tackle this issue in GAN training. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "mangalam|mitigating_mode_collapse_by_sidestepping_catastrophic_forgetting", "supplementary_material": "/attachment/5484b45d4b97c083392dcecc03c001481da78392.zip", "pdf": "/pdf/4746b3f2f5791e4968c1653041b1af5415919b23.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=69rMuKm8sN", "_bibtex": "@misc{\nmangalam2021mitigating,\ntitle={Mitigating Mode Collapse by Sidestepping Catastrophic Forgetting},\nauthor={Karttikeya Mangalam and Rohin Garg and Jathushan Rajasegaran and Taesung Park},\nyear={2021},\nurl={https://openreview.net/forum?id=54-QTuqSLyn}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 6, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "j1J74VQ36xc", "original": null, "number": 1, "cdate": 1610040517108, "ddate": null, "tcdate": 1610040517108, "tmdate": 1610474125370, "tddate": null, "forum": "54-QTuqSLyn", "replyto": "54-QTuqSLyn", "invitation": "ICLR.cc/2021/Conference/Paper3091/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "The authors propose an approach to mitigate mode collapse phenomena in GANs. Motivated by the intuition that mode collapse stems from catastrophic forgetting of the discriminator, the authors propose a solution inspired by recent research in continual learning and dynamically add new discriminators during training. The authors empirically demonstrate that combining the proposed scheme with existing GANs leads to improvements in terms of Inception Score and FID. \n\nThis paper is trying to address a significant problem for the generative modeling community. The reviewers appreciated the clarity of writing, the empirical results, and the idea of using normalising flows for an elegant visualisation. However, the reviewers have pointed out several major issues which were not adequately addressed by the authors. The first one is the clear failure to position the work with respect to related work. In fact, the main idea related to catastrophic forgetting was already established in [1,2] and subsequent works. Secondly, the improvements over the baseline come at a significant computational overhead which is extremely challenging and impractical. Finally, given the trend that large-scale models achieve significantly better results in practice, the proposed approach is not only impractical, but potentially extremely wasteful. Given very limited novelty, failure to position the work, and impracticality of the proposed solution, I will recommend rejection.\n\n[1] https://arxiv.org/abs/1810.11598\n\n[2] https://arxiv.org/abs/1911.06997"}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Mitigating Mode Collapse by Sidestepping Catastrophic Forgetting", "authorids": ["~Karttikeya_Mangalam1", "~Rohin_Garg1", "~Jathushan_Rajasegaran1", "~Taesung_Park2"], "authors": ["Karttikeya Mangalam", "Rohin Garg", "Jathushan Rajasegaran", "Taesung Park"], "keywords": ["mode collapse", "catastrophic forgetting", "multi-adversarial training"], "abstract": "Generative Adversarial Networks (GANs) are a class of generative models used for various applications, but they have been known to suffer from the mode collapse problem, in which some modes of the target distribution are ignored by the generator.  Investigative study using a new data generation procedure indicates that the mode collapse of the generator is driven by the discriminator\u2019s inability to maintain classification accuracy on previously seen samples, a phenomenon called Catastrophic Forgetting in continual learning. Motivated by this observation, we introduce a novel training procedure that dynamically spawns additional dis-criminators to remember previous modes of generation. On several datasets, we show that our training scheme can be plugged-in to existing GAN frameworks to mitigate mode collapse and improve standard metrics for GAN evaluation.", "one-sentence_summary": "We propose a relationship between catastrophic forgetting in discriminator and mode collapse in generator and propose a dynamic multi adversarial training (DMAT) solution to tackle this issue in GAN training. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "mangalam|mitigating_mode_collapse_by_sidestepping_catastrophic_forgetting", "supplementary_material": "/attachment/5484b45d4b97c083392dcecc03c001481da78392.zip", "pdf": "/pdf/4746b3f2f5791e4968c1653041b1af5415919b23.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=69rMuKm8sN", "_bibtex": "@misc{\nmangalam2021mitigating,\ntitle={Mitigating Mode Collapse by Sidestepping Catastrophic Forgetting},\nauthor={Karttikeya Mangalam and Rohin Garg and Jathushan Rajasegaran and Taesung Park},\nyear={2021},\nurl={https://openreview.net/forum?id=54-QTuqSLyn}\n}"}, "tags": [], "invitation": {"reply": {"forum": "54-QTuqSLyn", "replyto": "54-QTuqSLyn", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040517095, "tmdate": 1610474125355, "id": "ICLR.cc/2021/Conference/Paper3091/-/Decision"}}}, {"id": "_DUpWBth_Gc", "original": null, "number": 3, "cdate": 1603848713983, "ddate": null, "tcdate": 1603848713983, "tmdate": 1606798885326, "tddate": null, "forum": "54-QTuqSLyn", "replyto": "54-QTuqSLyn", "invitation": "ICLR.cc/2021/Conference/Paper3091/-/Official_Review", "content": {"title": "Not bad results, but a somewhat unprincipled retread of a previously explored idea.", "review": "This paper addresses the well-known phenomenon of model collapse in generative adversarial networks (GANs). In particular, this paper identifies catastrophic forgetting of the discriminator as a potential source of mode collapse and proposes a multi-discriminator framework called DMAT as a solution. DMAT takes inspiration from expansion-based continual learning approaches, which add network capacity (in this case entire discriminators) to preserve past knowledge.\n\nPros:\n1.\tThere are several excellent visualizations of generator oscillation behavior (Figure 1, Figure 4 <- I like the animations) that underpins DMAT\u2019s design.\n2.\tBecause DMAT is primarily based on adding discriminators to the model, it can be fairly simply combined with many GAN approaches. This is demonstrated for example in Table 3.\n3.\tDiscriminators are dynamically added as needed, allowing the network to scale indefinitely to new modes. This also saves some computation at the beginning, when a large number of discriminators may not be necessary.\n4.\tA fairly wide range of experiments were run, and adding DMAT appears to lead to stronger results (higher Inception Score, lower Frechet Inception Distance) across a both an assortment of models and datasets.\n5.\tThe quality of the writing is generally good.\n\nCons:\n1.\tI have concerns over the novelty of this work. The primary motivation of the paper (GAN mode collapse being due to catastrophic forgetting, leading to generator oscillations) is a known phenomenon and has been thoroughly explored before in [1,2]. In particular, [2] also takes a continual learning-inspired approach (regularization, as opposed to expansion) to propose an easy-to-add method, and visuals very similar to this submission\u2019s Figure 4 can be found in both [1] and [2]. Given the strong degree of conceptual overlap, these works probably deserve more mention than a small reference to [1] in the Related Works. This paper presents the \u201cGAN catastrophic forgetting\u201d hypothesis in the Introduction as if it is the authors\u2019 novel contribution. Additionally, multi-discriminator networks have also been well-explored in the past, with several examples mentioned in the Related Works. There are only limited comparisons with these methods in the Experiments, but Table 2 for example does demonstrate that D2GAN (which has 2 discriminators) also manages to cover all modes.\n2.\tThe core methodology of DMAT is adding discriminators to capture new modes. While conceptually simple, this involves adding entire new networks, which can be expensive computationally. How many discriminators does DMAT typically add? In the continual learning literature, expansion of an entire network per task (equivalent to \u201cper mode,\u201d here) is considered very bad, and is the scenario that expansion strategies (some of which have sublinear parameter growth) specifically try to avoid.\n\n3.\tDiscriminator assignment to samples and spawning/initialization is rather ad hoc, relying primarily on heuristics and intuition. A Dirichlet Process Mixture Model/Mixture-of-Experts approach such as in [3] (also a continual learning set-up, albeit for more traditional supervised classification) would be much more principled. Additionally, my understanding is that new discriminators are randomly initialized, which may be slow and inefficient.\n4.\tI don\u2019t find the \u201cfair comparison\u201d in Appendix C to be particularly fair, as it\u2019s unclear if the discriminator can effectively use the capacity of a wider network, and if it could, it\u2019d likely disrupt the delicate minimax game between the discriminator and the generator; too strong of either a generator or discriminator severely disrupts GAN learning.\n5.\tI don\u2019t fully understand the benefits of the proposed data generation process in Section 3.1. Settings like mixture of 8/25 2D Gaussians are fairly commonplace; being 2D means the discriminator decisions are easy to visualize, and the number of classes can be trivially increased/decreased by adding/removing classes. Using normalizing flows don\u2019t make these synthetic distributions any more realistic either.\n\nMiscellaneous:\n1.\tThe title is a little too vague, and in my opinion \u201csidestepping\u201d is somewhat inaccurate: despite some successful results on simple datasets, DMAT\u2019s design doesn\u2019t necessarily avoid catastrophic forgetting entirely. Given the plethora of continual learning strategies, the title should be more specific to DMAT\u2019s strategy (namely model expansion by growing the number of discriminators).\n2.\tAlg 3 is a little confusing. The way it\u2019s currently presented, it appears at first glance that a new discriminator is spawned on every iteration after $T_t$. It would be better if either the spawning logic from Alg 2 appeared in Alg 3, or the call to DSPAWN (Alg2) in Alg 3 made it clearer that a new discriminator wasn\u2019t created on every iteration. Another thing worth noting: DSPAWN sounds very similar to \u201cdespawn,\u201d which implies removing discriminators, not adding them. \n3.\tBoth BigGAN and Side-tuning appear twice in the references.\n4.\tThe gap between the Figure 3 caption and the main body text is a little too narrow.\n\nDecision: \nThe experimental results presented in this paper are promising, but adding entire discriminator networks is computationally expensive, and I find DMAT\u2019s formulation to be a little too unprincipled. Furthermore, while this specific combination may not have appeared in previous literature, discriminator catastrophic forgetting as a cause for mode collapse and multi-discriminator GANs have both been thoroughly explored before, leaving this submission with little novelty of its own. As a result, I recommend rejection.\n\n\n[1] Hoang Thanh-Tung and Truyen Tran. On catastrophic forgetting and mode collapse in gans. arXiv preprint arXiv:1807.04015, 2020.\n\n[2] Kevin J Liang, Chunyuan Li, Guoyin Wang, Lawrence Carin. Generative Adversarial Network Training is a Continual Learning Problem. arXiv preprint arXiv:1811.11083, 2018\n\n[3] Soochan Lee, Junsoo Ha, Dongsu Zhang, Gunhee Kim. A Neural Dirichlet Process Mixture Model for Task-Free Continual Learning. ICLR 2020.\n\n\n=========\nPost-rebuttal\n=========\n\nI thank the authors for their thorough response, which is well argued. Overall, yes, I agree with the authors that there are some differences between their work and prior work. I do not claim that there is zero novelty here. My concern is whether there is enough for this venue, given the high degree of similarity. As the authors do acknowledge that the hypothesis of \"catastrophic forgetting leads to GAN oscillations\" was introduced in other work, then the primary contribution here is replacing one continual learning method for preventing catastrophic forgetting in GAN discriminators with another, and despite the rebuttal, I still find the proposed solution to be rather ad hoc. I keep my score.", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper3091/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3091/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Mitigating Mode Collapse by Sidestepping Catastrophic Forgetting", "authorids": ["~Karttikeya_Mangalam1", "~Rohin_Garg1", "~Jathushan_Rajasegaran1", "~Taesung_Park2"], "authors": ["Karttikeya Mangalam", "Rohin Garg", "Jathushan Rajasegaran", "Taesung Park"], "keywords": ["mode collapse", "catastrophic forgetting", "multi-adversarial training"], "abstract": "Generative Adversarial Networks (GANs) are a class of generative models used for various applications, but they have been known to suffer from the mode collapse problem, in which some modes of the target distribution are ignored by the generator.  Investigative study using a new data generation procedure indicates that the mode collapse of the generator is driven by the discriminator\u2019s inability to maintain classification accuracy on previously seen samples, a phenomenon called Catastrophic Forgetting in continual learning. Motivated by this observation, we introduce a novel training procedure that dynamically spawns additional dis-criminators to remember previous modes of generation. On several datasets, we show that our training scheme can be plugged-in to existing GAN frameworks to mitigate mode collapse and improve standard metrics for GAN evaluation.", "one-sentence_summary": "We propose a relationship between catastrophic forgetting in discriminator and mode collapse in generator and propose a dynamic multi adversarial training (DMAT) solution to tackle this issue in GAN training. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "mangalam|mitigating_mode_collapse_by_sidestepping_catastrophic_forgetting", "supplementary_material": "/attachment/5484b45d4b97c083392dcecc03c001481da78392.zip", "pdf": "/pdf/4746b3f2f5791e4968c1653041b1af5415919b23.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=69rMuKm8sN", "_bibtex": "@misc{\nmangalam2021mitigating,\ntitle={Mitigating Mode Collapse by Sidestepping Catastrophic Forgetting},\nauthor={Karttikeya Mangalam and Rohin Garg and Jathushan Rajasegaran and Taesung Park},\nyear={2021},\nurl={https://openreview.net/forum?id=54-QTuqSLyn}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "54-QTuqSLyn", "replyto": "54-QTuqSLyn", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3091/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538082534, "tmdate": 1606915804912, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3091/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3091/-/Official_Review"}}}, {"id": "bVhebp-YmiB", "original": null, "number": 1, "cdate": 1605474406588, "ddate": null, "tcdate": 1605474406588, "tmdate": 1605474406588, "tddate": null, "forum": "54-QTuqSLyn", "replyto": "54-QTuqSLyn", "invitation": "ICLR.cc/2021/Conference/Paper3091/-/Public_Comment", "content": {"title": "Related works", "comment": " Hi,\n\nThis is a really interesting idea to add additional discriminators dynamically (I once thought about it too). \nI believe [1,2] are relevant items for your \"multiple discriminators\" related work section.\n\n[1][On-Line Adaptative Curriculum Learning for GANs](https://ojs.aaai.org//index.php/AAAI/article/view/4224)\n[2][Multi-objective training of Generative Adversarial Networks with multiple discriminators](http://proceedings.mlr.press/v97/albuquerque19a.html)"}, "signatures": ["~Thang_Doan1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "~Thang_Doan1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Mitigating Mode Collapse by Sidestepping Catastrophic Forgetting", "authorids": ["~Karttikeya_Mangalam1", "~Rohin_Garg1", "~Jathushan_Rajasegaran1", "~Taesung_Park2"], "authors": ["Karttikeya Mangalam", "Rohin Garg", "Jathushan Rajasegaran", "Taesung Park"], "keywords": ["mode collapse", "catastrophic forgetting", "multi-adversarial training"], "abstract": "Generative Adversarial Networks (GANs) are a class of generative models used for various applications, but they have been known to suffer from the mode collapse problem, in which some modes of the target distribution are ignored by the generator.  Investigative study using a new data generation procedure indicates that the mode collapse of the generator is driven by the discriminator\u2019s inability to maintain classification accuracy on previously seen samples, a phenomenon called Catastrophic Forgetting in continual learning. Motivated by this observation, we introduce a novel training procedure that dynamically spawns additional dis-criminators to remember previous modes of generation. On several datasets, we show that our training scheme can be plugged-in to existing GAN frameworks to mitigate mode collapse and improve standard metrics for GAN evaluation.", "one-sentence_summary": "We propose a relationship between catastrophic forgetting in discriminator and mode collapse in generator and propose a dynamic multi adversarial training (DMAT) solution to tackle this issue in GAN training. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "mangalam|mitigating_mode_collapse_by_sidestepping_catastrophic_forgetting", "supplementary_material": "/attachment/5484b45d4b97c083392dcecc03c001481da78392.zip", "pdf": "/pdf/4746b3f2f5791e4968c1653041b1af5415919b23.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=69rMuKm8sN", "_bibtex": "@misc{\nmangalam2021mitigating,\ntitle={Mitigating Mode Collapse by Sidestepping Catastrophic Forgetting},\nauthor={Karttikeya Mangalam and Rohin Garg and Jathushan Rajasegaran and Taesung Park},\nyear={2021},\nurl={https://openreview.net/forum?id=54-QTuqSLyn}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "54-QTuqSLyn", "readers": {"description": "User groups that will be able to read this comment.", "values": ["everyone"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed."}}, "expdate": 1605630600000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3091/Authors", "ICLR.cc/2021/Conference/Paper3091/Reviewers", "ICLR.cc/2021/Conference/Paper3091/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1605024957569, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3091/-/Public_Comment"}}}, {"id": "XXrmp5xdFhn", "original": null, "number": 1, "cdate": 1603507735217, "ddate": null, "tcdate": 1603507735217, "tmdate": 1605024070615, "tddate": null, "forum": "54-QTuqSLyn", "replyto": "54-QTuqSLyn", "invitation": "ICLR.cc/2021/Conference/Paper3091/-/Official_Review", "content": {"title": "The idea of the dynamic discriminator to address the mode collapse is new, but some missing related works and some experiments and discussion need to be justified", "review": "-- Summary --\n\nThis paper studies specifically the mode collapse problem of GAN. It hypothesizes that mode collapse in training GAN is closely related to catastrophic forgetting, and proposes a method to improve the catastrophic forgetting issue to mitigate the mode collapse. The proposed method uses multiple dynamically-spawned discriminators, in which additional discriminators are spawned to remember a few exemplars of data modes. It enables the model to avoid the oscillation in the discriminators' predictions, and handle well different parts of the data. The experiments demonstrate this multiple-spawned-discriminators scheme can be plugged to improves into various baseline GAN models on benchmark datasets (Stacked MNIST, CIFAR-10, CUB500 (qualitative only)). The paper also proposed a synthetic data generation procedure that can generate high-dimensional synthetic data that can be visualized at low-dimension for studying the mode collapse. \n\nOverall, this paper is well-written and clear. Although the idea of multiple adversarial has been discussed in many existing works (some related works are missing in the paper, see below), this paper is different at posing a new scheme and specifically use it to address catastrophic forgetting. The experiments and study are sufficient to demonstrate the contributions.\n\nHowever, the hypothesis of a relation between mode collapse and catastrophic forgetting, which was discussed in [c] (missing also), is not new. The algorithms are not too clear (notations, explanations) that need to be improved in the updated version. Additionally, some claims in the paper need to be justified.\n\n[a] Multi-objective training of Generative Adversarial Networks with multiple discriminators.\n\n[b] Generative Multi-Adversarial Networks.\n\n[c] Self-Supervised GANs via Auxiliary Rotation Loss.\n\n-- Strength -- \n\nS1 \u2013 The proposed algorithms of dynamic-spawned discriminators to address the catastrophic forgetting is new to me.\n\nS2 \u2013 The experiments show significant improvements over baseline models.\n\nS3 \u2013 The synthetic data generation procedure looks useful.\n\n-- Weakness --\n\nW1 \u2013 Some parts of the algorithms are not clear.\n\nW2 \u2013 Quite missing to discuss some important works in the paper.\n\n-- Details --\n\n*Algorithms* -\u00a0It would be a lot helpful and clearer if the authors explain the notations of Algorithms and relating them in the text discussions.\n\nD1 \u2013 Algorithm 1 is a bit confusing with superscripts and subscripts of notations. Not sure superscripts or subscripts are the indices of samples or dimensions? According to the Algorithm, $x_{2D}^0$ is not used after being declared before the loop? And what is $N$ in Algorithm 1?\n\nD2 \u2013 What are $[.]^-$ and $[.]^+$ of the loss function in Algorithm 3? Are the authors training GAN models with hinge losses but not discussed in the paper?\u00a0\n\nD3 \u2013 The random weights m used once to compute weighted means over the discriminators do not looks make sense to me. This random may have much affect to update the generator but their random values are chosen not from any prior pieces of information. I found some discussion about this in the section of \u201cGenerator Training\u201d, yet can the author provide the ablation study to show the importance of using these random weights?\n\n*Toy dataset* \n\nD4 \u2013 In the study of Effect of Data Complexity on Mode Collapse (Table 1): The authors test the mode collapse by \"more than a quarter of the data modes are dropped\". How is \"a quarter of the data modes\" measured? Is there any threshold to decide a mode is dropped or not? And What is z for Level 3 of Table 1?\n\n*Stacked MNIST experiment*\n\nD5 \u2013 In the Stacked MNIST experiment, it's surprised that the original DCGAN can achieve quite a good mode collapse average if following the standard setup of Unrolled GAN (Metz et al., 2016). What is the architecture used for Stacked MNIST experiments?\n\n*Forgetting-collapse interplay experiments*\n\nD6 \u2013 In the proposed method, there are multiple discriminators. Are all features extracted from all discriminators chosen for the quality evaluation in forgetting-collapse interplay experiments?\n\nD7 \u2013 The paper claimed: \"we show ..., and is the *most effective* in preventing mode collapse\". Although I agree that improving the catastrophic forgetting would improve the mode collapse, however, I would argue that there are likely other reasons causing mode collapse. For example, [d] shows the mode collapse still can happen in [c] due to the loss function of the classification. (Thanh-Tung et al., 2020) suggests the gradient penalty can address quite well the mode collapse. Can the authors compare the proposed method without gradient penalty compared to the same GAN models with only gradient penalty? Moreover, recent works [e,f,g] simply apply data augmentation for GAN training that significantly improves its performance. Therefore, I believe that data augmentation can also be helpful to address the mode collapse specifically in the case of limited data. \n\n[d] Self-supervised GAN: Analysis and Improvement with Multi-class Minimax Game.\n\n[e] Differentiable Augmentation for Data-Efficient GAN Training.\n\n[f] On Data Augmentation for GAN Training.\n\n[g] Training Generative Adversarial Networks with Limited Data.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3091/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3091/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Mitigating Mode Collapse by Sidestepping Catastrophic Forgetting", "authorids": ["~Karttikeya_Mangalam1", "~Rohin_Garg1", "~Jathushan_Rajasegaran1", "~Taesung_Park2"], "authors": ["Karttikeya Mangalam", "Rohin Garg", "Jathushan Rajasegaran", "Taesung Park"], "keywords": ["mode collapse", "catastrophic forgetting", "multi-adversarial training"], "abstract": "Generative Adversarial Networks (GANs) are a class of generative models used for various applications, but they have been known to suffer from the mode collapse problem, in which some modes of the target distribution are ignored by the generator.  Investigative study using a new data generation procedure indicates that the mode collapse of the generator is driven by the discriminator\u2019s inability to maintain classification accuracy on previously seen samples, a phenomenon called Catastrophic Forgetting in continual learning. Motivated by this observation, we introduce a novel training procedure that dynamically spawns additional dis-criminators to remember previous modes of generation. On several datasets, we show that our training scheme can be plugged-in to existing GAN frameworks to mitigate mode collapse and improve standard metrics for GAN evaluation.", "one-sentence_summary": "We propose a relationship between catastrophic forgetting in discriminator and mode collapse in generator and propose a dynamic multi adversarial training (DMAT) solution to tackle this issue in GAN training. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "mangalam|mitigating_mode_collapse_by_sidestepping_catastrophic_forgetting", "supplementary_material": "/attachment/5484b45d4b97c083392dcecc03c001481da78392.zip", "pdf": "/pdf/4746b3f2f5791e4968c1653041b1af5415919b23.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=69rMuKm8sN", "_bibtex": "@misc{\nmangalam2021mitigating,\ntitle={Mitigating Mode Collapse by Sidestepping Catastrophic Forgetting},\nauthor={Karttikeya Mangalam and Rohin Garg and Jathushan Rajasegaran and Taesung Park},\nyear={2021},\nurl={https://openreview.net/forum?id=54-QTuqSLyn}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "54-QTuqSLyn", "replyto": "54-QTuqSLyn", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3091/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538082534, "tmdate": 1606915804912, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3091/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3091/-/Official_Review"}}}, {"id": "qYgWer5-JcD", "original": null, "number": 2, "cdate": 1603669614954, "ddate": null, "tcdate": 1603669614954, "tmdate": 1605024070553, "tddate": null, "forum": "54-QTuqSLyn", "replyto": "54-QTuqSLyn", "invitation": "ICLR.cc/2021/Conference/Paper3091/-/Official_Review", "content": {"title": "Review of 'Mitigating code collapse by sidestepping catastrophic forgetting'", "review": "The contributions of this paper revolve around the simple but interesting idea that mode collapse and missing modes in GANs are due (at least in part) by catastrophic forgetting in the discriminator, as the discriminator (and the generator) see a non-stationary training distributions due to their interaction. The authors provide toy experiments clearly illustrating this phenomenon and then propose an algorithm (DMAT) based on multiple discriminators taking charge for different parts of the input space (different modes). They show on several synthetic datasets with varying complexity that DMAT helps where other approaches fail, although DMAT (and all the other methods) still fail on the harder cases. They also show numerical improvements in standard GAN metrics against other methods (I don't know enough to be sure if these are SOTA, though). They also have nice experiments suggesting a strong link between mode collapse and catastrophic forgetting, thus also reinforcing the main claim.\n\nHence I see several good contributions around what seems to be a good idea. I only had a few minor comments, otherwise.\n\n* there is generally a trade-off in generative models between missing modes and spurious modes; it would be good to make sure that we don't gain on one side by losing on the other.\n\n* the algorithm is actually complicated and contains a lot of bells and whistles whose justification is not always clear (hence the statement in 4.1 that other methods are 'more complicated' seems unfair)\n\n* the D2GAN performs as well as DMAT in table 2 but does not get the 'bold' treatment, this should be fixed.\n\n* I do not find the difference between with and without DMAT in figure 3 compelling, so conclusion about this should also be toned down.\n\n* several clarity issues:\n  - alpha_t in Alg. 2 is not specified in the algorithm (although it is in the text)\n  - the justification for alpha_t is not clear (and in particular the specifics of how it changes with t)\n  - the justification for the experimental setup associated with figure 2 is not clear enough; why not keep the capacity of the classifier fixed (and not necessarily linear)\u02c6? \n  - 'with a maximum of one real sample per mode': how would you know, on real data, since the modes are not known a priori? hence this thinking only works for toy data where the modes are known.\n  - English needs proofreading in many places\n", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3091/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3091/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Mitigating Mode Collapse by Sidestepping Catastrophic Forgetting", "authorids": ["~Karttikeya_Mangalam1", "~Rohin_Garg1", "~Jathushan_Rajasegaran1", "~Taesung_Park2"], "authors": ["Karttikeya Mangalam", "Rohin Garg", "Jathushan Rajasegaran", "Taesung Park"], "keywords": ["mode collapse", "catastrophic forgetting", "multi-adversarial training"], "abstract": "Generative Adversarial Networks (GANs) are a class of generative models used for various applications, but they have been known to suffer from the mode collapse problem, in which some modes of the target distribution are ignored by the generator.  Investigative study using a new data generation procedure indicates that the mode collapse of the generator is driven by the discriminator\u2019s inability to maintain classification accuracy on previously seen samples, a phenomenon called Catastrophic Forgetting in continual learning. Motivated by this observation, we introduce a novel training procedure that dynamically spawns additional dis-criminators to remember previous modes of generation. On several datasets, we show that our training scheme can be plugged-in to existing GAN frameworks to mitigate mode collapse and improve standard metrics for GAN evaluation.", "one-sentence_summary": "We propose a relationship between catastrophic forgetting in discriminator and mode collapse in generator and propose a dynamic multi adversarial training (DMAT) solution to tackle this issue in GAN training. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "mangalam|mitigating_mode_collapse_by_sidestepping_catastrophic_forgetting", "supplementary_material": "/attachment/5484b45d4b97c083392dcecc03c001481da78392.zip", "pdf": "/pdf/4746b3f2f5791e4968c1653041b1af5415919b23.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=69rMuKm8sN", "_bibtex": "@misc{\nmangalam2021mitigating,\ntitle={Mitigating Mode Collapse by Sidestepping Catastrophic Forgetting},\nauthor={Karttikeya Mangalam and Rohin Garg and Jathushan Rajasegaran and Taesung Park},\nyear={2021},\nurl={https://openreview.net/forum?id=54-QTuqSLyn}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "54-QTuqSLyn", "replyto": "54-QTuqSLyn", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3091/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538082534, "tmdate": 1606915804912, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3091/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3091/-/Official_Review"}}}, {"id": "dVebc34o6ZC", "original": null, "number": 4, "cdate": 1604586229809, "ddate": null, "tcdate": 1604586229809, "tmdate": 1605024070423, "tddate": null, "forum": "54-QTuqSLyn", "replyto": "54-QTuqSLyn", "invitation": "ICLR.cc/2021/Conference/Paper3091/-/Official_Review", "content": {"title": "An interesting empirical study of mode collapse, evidence for a useful heuristic approach, but claims remain unsubstantiated. ", "review": "The paper proposes and approach to mitigate mode collapse in arbitrary GANs. This is relies on the assumption that mode collapse relates to catastrophic forgetting in the GAN discriminator. The authors design a synthetic experiments based on a normalizing flow generator, allowing to quantify mode collapse. They propose a heuristic to avoid mode collapse of the discriminators by adding additional ones during training. The results show improvements with respect to baseline regarding mode covering and FID.\n\nStrengths:\n- the synthetic experiment to quantify mode collapse looks elegant,\n- the authors report systemic improvement in FID over baselines.\n\nWeaknesses:\n- the authors fail to cite and compare their approach to another approach based on accumulating networks to improve GAN performance (\"AdaGAN: Boosting Generative Models\", Tolstikhin et al. 2017). There are also extra references therein.\n- The relation between mode collapse and catastrophic forgetting in not clearly supported. This can be seen in Fig. 2, were despite the authors statements, there is no clear covariation between forgetting and collapse.\n- While the systematic improvement of mode covering provided by the method is not surprising, the systematic FID improvement are weakened by the absence of detailed (e.g. supplemental) material to reproduce the results and make sure this result is solid.\n- The authors make a link between mode collapse and oscillations, although the later is now a well studied and general phenomenon with solid theoretical results, that can be dealt with by modifying the optimization algorithm (Daskalakis et al, Training GANs with optimism). \n\nOverall, this seems and interesting research direction, but evidence for the main claim of the paper is lacking. In the current state I tend to recommend rejection.\n\n\nPerhaps the authors could find investigate more in depth going through all their experiments, whether their is a clear causal link between mode collapse and catastrophic forgetting. This is likely tricky, in the sense that mode collapse will likely lead to a loss of discriminator performance. Also the authors could compare their approach with alternative solutions, that do not rely on this collapse-forgetting link, such as the two papers cited above.\n\n\n", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3091/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3091/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Mitigating Mode Collapse by Sidestepping Catastrophic Forgetting", "authorids": ["~Karttikeya_Mangalam1", "~Rohin_Garg1", "~Jathushan_Rajasegaran1", "~Taesung_Park2"], "authors": ["Karttikeya Mangalam", "Rohin Garg", "Jathushan Rajasegaran", "Taesung Park"], "keywords": ["mode collapse", "catastrophic forgetting", "multi-adversarial training"], "abstract": "Generative Adversarial Networks (GANs) are a class of generative models used for various applications, but they have been known to suffer from the mode collapse problem, in which some modes of the target distribution are ignored by the generator.  Investigative study using a new data generation procedure indicates that the mode collapse of the generator is driven by the discriminator\u2019s inability to maintain classification accuracy on previously seen samples, a phenomenon called Catastrophic Forgetting in continual learning. Motivated by this observation, we introduce a novel training procedure that dynamically spawns additional dis-criminators to remember previous modes of generation. On several datasets, we show that our training scheme can be plugged-in to existing GAN frameworks to mitigate mode collapse and improve standard metrics for GAN evaluation.", "one-sentence_summary": "We propose a relationship between catastrophic forgetting in discriminator and mode collapse in generator and propose a dynamic multi adversarial training (DMAT) solution to tackle this issue in GAN training. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "mangalam|mitigating_mode_collapse_by_sidestepping_catastrophic_forgetting", "supplementary_material": "/attachment/5484b45d4b97c083392dcecc03c001481da78392.zip", "pdf": "/pdf/4746b3f2f5791e4968c1653041b1af5415919b23.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=69rMuKm8sN", "_bibtex": "@misc{\nmangalam2021mitigating,\ntitle={Mitigating Mode Collapse by Sidestepping Catastrophic Forgetting},\nauthor={Karttikeya Mangalam and Rohin Garg and Jathushan Rajasegaran and Taesung Park},\nyear={2021},\nurl={https://openreview.net/forum?id=54-QTuqSLyn}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "54-QTuqSLyn", "replyto": "54-QTuqSLyn", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3091/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538082534, "tmdate": 1606915804912, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3091/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3091/-/Official_Review"}}}], "count": 7}