{"notes": [{"id": "bFnn6lPn3Sp", "original": "KvkaRZGmvQ_", "number": 327, "cdate": 1601308044167, "ddate": null, "tcdate": 1601308044167, "tmdate": 1614985622157, "tddate": null, "forum": "bFnn6lPn3Sp", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "A Benchmark for Voice-Face Cross-Modal Matching and  Retrieval", "authorids": ["~Chuyuan_Xiong1", "~Deyuan_Zhang1", "~Tao_Liu1", "~Xiaoyong_Du1", "~Jiankun_Tian1", "~Songyan_Xue1"], "authors": ["Chuyuan Xiong", "Deyuan Zhang", "Tao Liu", "Xiaoyong Du", "Jiankun Tian", "Songyan Xue"], "keywords": ["Cross-Modal Learning", "Voice-Face Matching", "Voice-Face Retrieval"], "abstract": "Cross-modal associations between a person's voice and face can be learned algorithmically, and this is a useful functionality in many audio and visual applications. The problem can be defined as two tasks: voice-face matching and retrieval. Recently, this topic has attracted much research attention, but it is still in its early stages of development, and evaluation protocols and test schemes need to be more standardized. Performance metrics for different subtasks are also scarce, and a benchmark for this problem needs to be established. In this paper, a baseline evaluation framework is proposed for voice-face matching and retrieval tasks. Test confidence is analyzed, and a confidence interval for estimated accuracy is proposed. Various state-of-the-art performances with high test confidence are achieved on a series of subtasks using the baseline method  (called TriNet) included in this framework. The source code will be published along with the paper. The results of this study can provide a basis for future research on voice-face cross-modal learning.", "pdf": "/pdf/ce8c756f6b1ad07c5135032cfdf32f1166604520.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xiong|a_benchmark_for_voiceface_crossmodal_matching_and_retrieval", "one-sentence_summary": "A baseline framework  are proposed for voice-face cross-modal matching and retrieval tasks.", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=ZoJTA9WZan", "_bibtex": "@misc{\nxiong2021a,\ntitle={A Benchmark for Voice-Face Cross-Modal Matching and  Retrieval},\nauthor={Chuyuan Xiong and Deyuan Zhang and Tao Liu and Xiaoyong Du and Jiankun Tian and Songyan Xue},\nyear={2021},\nurl={https://openreview.net/forum?id=bFnn6lPn3Sp}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "t04_C3A726X", "original": null, "number": 1, "cdate": 1610040537497, "ddate": null, "tcdate": 1610040537497, "tmdate": 1610474147570, "tddate": null, "forum": "bFnn6lPn3Sp", "replyto": "bFnn6lPn3Sp", "invitation": "ICLR.cc/2021/Conference/Paper327/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "The reviewers pointed out several opportunities for improvements and concurred that the paper needs significant work before it is ready for publication.  The authors did not provide a rebuttal. We hope the review process was useful to the authors. "}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Benchmark for Voice-Face Cross-Modal Matching and  Retrieval", "authorids": ["~Chuyuan_Xiong1", "~Deyuan_Zhang1", "~Tao_Liu1", "~Xiaoyong_Du1", "~Jiankun_Tian1", "~Songyan_Xue1"], "authors": ["Chuyuan Xiong", "Deyuan Zhang", "Tao Liu", "Xiaoyong Du", "Jiankun Tian", "Songyan Xue"], "keywords": ["Cross-Modal Learning", "Voice-Face Matching", "Voice-Face Retrieval"], "abstract": "Cross-modal associations between a person's voice and face can be learned algorithmically, and this is a useful functionality in many audio and visual applications. The problem can be defined as two tasks: voice-face matching and retrieval. Recently, this topic has attracted much research attention, but it is still in its early stages of development, and evaluation protocols and test schemes need to be more standardized. Performance metrics for different subtasks are also scarce, and a benchmark for this problem needs to be established. In this paper, a baseline evaluation framework is proposed for voice-face matching and retrieval tasks. Test confidence is analyzed, and a confidence interval for estimated accuracy is proposed. Various state-of-the-art performances with high test confidence are achieved on a series of subtasks using the baseline method  (called TriNet) included in this framework. The source code will be published along with the paper. The results of this study can provide a basis for future research on voice-face cross-modal learning.", "pdf": "/pdf/ce8c756f6b1ad07c5135032cfdf32f1166604520.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xiong|a_benchmark_for_voiceface_crossmodal_matching_and_retrieval", "one-sentence_summary": "A baseline framework  are proposed for voice-face cross-modal matching and retrieval tasks.", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=ZoJTA9WZan", "_bibtex": "@misc{\nxiong2021a,\ntitle={A Benchmark for Voice-Face Cross-Modal Matching and  Retrieval},\nauthor={Chuyuan Xiong and Deyuan Zhang and Tao Liu and Xiaoyong Du and Jiankun Tian and Songyan Xue},\nyear={2021},\nurl={https://openreview.net/forum?id=bFnn6lPn3Sp}\n}"}, "tags": [], "invitation": {"reply": {"forum": "bFnn6lPn3Sp", "replyto": "bFnn6lPn3Sp", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040537482, "tmdate": 1610474147555, "id": "ICLR.cc/2021/Conference/Paper327/-/Decision"}}}, {"id": "c6T5skOdMhE", "original": null, "number": 1, "cdate": 1603748000360, "ddate": null, "tcdate": 1603748000360, "tmdate": 1605024713139, "tddate": null, "forum": "bFnn6lPn3Sp", "replyto": "bFnn6lPn3Sp", "invitation": "ICLR.cc/2021/Conference/Paper327/-/Official_Review", "content": {"title": "The main contributions (training on more data) and L2 normalising the embeddings of a network are not sufficient for acceptance to ICLR. ", "review": "The goal of this paper is to learn cross-modal associations between a person\u2019s face and a voice. The authors use a standard three stream network trained with a triplet loss, and evaluate on the VoxCeleb-VGGFace datasets.   \n\nStrengths: \n- The authors train on VoxCeleb-VGGFace2, and evaluate on VoxCeleb1, which is a larger set of identities, and perform a t-test to show statistical significance. \n\n- The ablation study showing that performance saturates with more training data is interesting. \n\n\nWeaknesses: \n- There is limited novelty in the method. The authors use the triplet loss which has been widely used for this problem before (Kim et al. 2018, Cheng et al. 20- https://dl.acm.org/doi/pdf/10.1145/3394171.3413710). The authors also claim that anchoring the voice subnetwork with frozen weights is a novel contribution but don\u2019t discuss that this teacher-student style model was already tried in the Learnable PINs paper (where the face subnetwork is frozen). L2 normalisation of embeddings has also been used widely with the triplet loss, and has also been used in Learnable PINs (and hence been applied to this problem as well). \n\n- Table 2 is not a fair comparison. The authors have compared the performance of different models on completely different test sets, with a different number of speakers! To assess the performance of TriNet, it must be compared to SOTA methods on the same test set of 189 speakers, or if the authors prefer - on all of the 1,251 speakers in VoxCeleb1. In this case the other SOTA methods must be re-evaluated on this new test set.  \n\n\n", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper327/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper327/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Benchmark for Voice-Face Cross-Modal Matching and  Retrieval", "authorids": ["~Chuyuan_Xiong1", "~Deyuan_Zhang1", "~Tao_Liu1", "~Xiaoyong_Du1", "~Jiankun_Tian1", "~Songyan_Xue1"], "authors": ["Chuyuan Xiong", "Deyuan Zhang", "Tao Liu", "Xiaoyong Du", "Jiankun Tian", "Songyan Xue"], "keywords": ["Cross-Modal Learning", "Voice-Face Matching", "Voice-Face Retrieval"], "abstract": "Cross-modal associations between a person's voice and face can be learned algorithmically, and this is a useful functionality in many audio and visual applications. The problem can be defined as two tasks: voice-face matching and retrieval. Recently, this topic has attracted much research attention, but it is still in its early stages of development, and evaluation protocols and test schemes need to be more standardized. Performance metrics for different subtasks are also scarce, and a benchmark for this problem needs to be established. In this paper, a baseline evaluation framework is proposed for voice-face matching and retrieval tasks. Test confidence is analyzed, and a confidence interval for estimated accuracy is proposed. Various state-of-the-art performances with high test confidence are achieved on a series of subtasks using the baseline method  (called TriNet) included in this framework. The source code will be published along with the paper. The results of this study can provide a basis for future research on voice-face cross-modal learning.", "pdf": "/pdf/ce8c756f6b1ad07c5135032cfdf32f1166604520.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xiong|a_benchmark_for_voiceface_crossmodal_matching_and_retrieval", "one-sentence_summary": "A baseline framework  are proposed for voice-face cross-modal matching and retrieval tasks.", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=ZoJTA9WZan", "_bibtex": "@misc{\nxiong2021a,\ntitle={A Benchmark for Voice-Face Cross-Modal Matching and  Retrieval},\nauthor={Chuyuan Xiong and Deyuan Zhang and Tao Liu and Xiaoyong Du and Jiankun Tian and Songyan Xue},\nyear={2021},\nurl={https://openreview.net/forum?id=bFnn6lPn3Sp}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "bFnn6lPn3Sp", "replyto": "bFnn6lPn3Sp", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper327/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538145540, "tmdate": 1606915810446, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper327/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper327/-/Official_Review"}}}, {"id": "9d1QRjRWsTv", "original": null, "number": 2, "cdate": 1603773701974, "ddate": null, "tcdate": 1603773701974, "tmdate": 1605024713070, "tddate": null, "forum": "bFnn6lPn3Sp", "replyto": "bFnn6lPn3Sp", "invitation": "ICLR.cc/2021/Conference/Paper327/-/Official_Review", "content": {"title": "Review for A BENCHMARK FOR VOICE-FACE CROSS-MODAL MATCHING AND RETRIEVAL", "review": "This work focuses on the problem of cross-modal matching and retrieval for face and voice modalities. The paper suggests a new benchmark for the evaluation of both matching and retrieval tasks. It also proposes a confidence margin computation to verify the statistical significance of the results. The results reported on Vox (voice) and VGG (face) dataset using the suggested benchmark are encouraging. I have summarized my comments below which will help in improving the quality of this manuscript:  \n\n1. I believe the technical contribution of this paper is very limited for ICLR. This work builds upon the previous works in the area of cross-modal retrieval. The contribution is very incremental and most of the conclusions are very well known. The multi-stream N/W architecture is very similar to the previous works and triplet loss has been used extensively for face and voice tasks in the past. The suggested protocol just uses a different combination of training and evaluation set from Vox and VGG sets.\n\n2. In section 3.2, the authors mentioned \"So the results of all related works that used VoxVGG-1 for training and testing are unreliable.\" I feel this is a very strong statement and may not always be true. First, the Vox dataset is collected from YouTube videos and represents a wide variety of conditions and results on the careful partition (no overlapping identities and conditions) of training and test may be very helpful. Second, in cases where evaluation conditions or use cases are known and similar to the VoxVGG-1 this may not hold true. \n\n3. In speaker recognition, data augmentation plays a huge role in learning a noise-robust representation. It is not clear if the authors applied data augmentation to the Vox2 training set. These works highlight the importance of data augmentation:\nSnyder, D., Garcia-Romero, D., Sell, G., Povey, D. and Khudanpur, S., 2018, April. X-vectors: Robust DNN embeddings for speaker recognition. In 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) (pp. 5329-5333). IEEE.\nZeinali, H., Wang, S., Silnova, A., Mat\u011bjka, P. and Plchot, O., 2019. But system description to voxceleb speaker recognition challenge 2019. arXiv preprint arXiv:1910.12592.\n\n4. Did the authors use any voice activity detection system to remove silence in the audio? \n\n5. In section 5.4, the authors claim \"Therefore, voice anchored embedding learning outperforms face-anchored embedding learning.\" It would be better to clarify these results in Table 5.\n\n6. The current state-of-the-art in speaker recognition is TDNN based x-vector. I would request authors to add a reference to the following paper in the introduction:\nSnyder, D., Garcia-Romero, D., Sell, G., Povey, D. and Khudanpur, S., 2018, April. X-vectors: Robust DNN embeddings for speaker recognition. In 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) (pp. 5329-5333). IEEE.", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper327/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper327/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Benchmark for Voice-Face Cross-Modal Matching and  Retrieval", "authorids": ["~Chuyuan_Xiong1", "~Deyuan_Zhang1", "~Tao_Liu1", "~Xiaoyong_Du1", "~Jiankun_Tian1", "~Songyan_Xue1"], "authors": ["Chuyuan Xiong", "Deyuan Zhang", "Tao Liu", "Xiaoyong Du", "Jiankun Tian", "Songyan Xue"], "keywords": ["Cross-Modal Learning", "Voice-Face Matching", "Voice-Face Retrieval"], "abstract": "Cross-modal associations between a person's voice and face can be learned algorithmically, and this is a useful functionality in many audio and visual applications. The problem can be defined as two tasks: voice-face matching and retrieval. Recently, this topic has attracted much research attention, but it is still in its early stages of development, and evaluation protocols and test schemes need to be more standardized. Performance metrics for different subtasks are also scarce, and a benchmark for this problem needs to be established. In this paper, a baseline evaluation framework is proposed for voice-face matching and retrieval tasks. Test confidence is analyzed, and a confidence interval for estimated accuracy is proposed. Various state-of-the-art performances with high test confidence are achieved on a series of subtasks using the baseline method  (called TriNet) included in this framework. The source code will be published along with the paper. The results of this study can provide a basis for future research on voice-face cross-modal learning.", "pdf": "/pdf/ce8c756f6b1ad07c5135032cfdf32f1166604520.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xiong|a_benchmark_for_voiceface_crossmodal_matching_and_retrieval", "one-sentence_summary": "A baseline framework  are proposed for voice-face cross-modal matching and retrieval tasks.", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=ZoJTA9WZan", "_bibtex": "@misc{\nxiong2021a,\ntitle={A Benchmark for Voice-Face Cross-Modal Matching and  Retrieval},\nauthor={Chuyuan Xiong and Deyuan Zhang and Tao Liu and Xiaoyong Du and Jiankun Tian and Songyan Xue},\nyear={2021},\nurl={https://openreview.net/forum?id=bFnn6lPn3Sp}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "bFnn6lPn3Sp", "replyto": "bFnn6lPn3Sp", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper327/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538145540, "tmdate": 1606915810446, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper327/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper327/-/Official_Review"}}}, {"id": "0kbe8eNg2nf", "original": null, "number": 3, "cdate": 1603864684024, "ddate": null, "tcdate": 1603864684024, "tmdate": 1605024713002, "tddate": null, "forum": "bFnn6lPn3Sp", "replyto": "bFnn6lPn3Sp", "invitation": "ICLR.cc/2021/Conference/Paper327/-/Official_Review", "content": {"title": "technical contribution is limited", "review": "Summary:\nThis paper aims to propose a benchmark for voce-face matching and retrieval problem. As shown by the test confidence analysis, the model is suggested to be evaluated on a large dataset or multiple datasets to avoid the large deviation in the accuracy. A baseline method TriNet and joint matching & retrieval are proposed. Improved results are reported in the experiment section.\n\nMy biggest concern is the unclear contribution of this paper.\nTest confidence is proposed, but has not been used to measure any results reported in this paper. It seems a bit disconnected. The only conclusion from test confidence analysis is to test model on multiple dataset, which is obvious.\n\nThe TriNet uses L2 normalized triplet loss, which is also not new and can be found in many previous work, e.g. [1]. Simply applying this normalized triplet loss to cross-modal matching is not a significant contribution.\n[1] Schroff, Florian, Dmitry Kalenichenko, and James Philbin. \"Facenet: A unified embedding for face recognition and clustering.\" Proceedings of the IEEE conference on computer vision and pattern recognition. 2015.\n\nAlso, the results in Table 2 are not fair comparison. As the paper points out, the training and testing data of TriNet are different from other methods, which means the results are not comparable. So I couldn't see any insight from the results. \n\n\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper327/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper327/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Benchmark for Voice-Face Cross-Modal Matching and  Retrieval", "authorids": ["~Chuyuan_Xiong1", "~Deyuan_Zhang1", "~Tao_Liu1", "~Xiaoyong_Du1", "~Jiankun_Tian1", "~Songyan_Xue1"], "authors": ["Chuyuan Xiong", "Deyuan Zhang", "Tao Liu", "Xiaoyong Du", "Jiankun Tian", "Songyan Xue"], "keywords": ["Cross-Modal Learning", "Voice-Face Matching", "Voice-Face Retrieval"], "abstract": "Cross-modal associations between a person's voice and face can be learned algorithmically, and this is a useful functionality in many audio and visual applications. The problem can be defined as two tasks: voice-face matching and retrieval. Recently, this topic has attracted much research attention, but it is still in its early stages of development, and evaluation protocols and test schemes need to be more standardized. Performance metrics for different subtasks are also scarce, and a benchmark for this problem needs to be established. In this paper, a baseline evaluation framework is proposed for voice-face matching and retrieval tasks. Test confidence is analyzed, and a confidence interval for estimated accuracy is proposed. Various state-of-the-art performances with high test confidence are achieved on a series of subtasks using the baseline method  (called TriNet) included in this framework. The source code will be published along with the paper. The results of this study can provide a basis for future research on voice-face cross-modal learning.", "pdf": "/pdf/ce8c756f6b1ad07c5135032cfdf32f1166604520.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xiong|a_benchmark_for_voiceface_crossmodal_matching_and_retrieval", "one-sentence_summary": "A baseline framework  are proposed for voice-face cross-modal matching and retrieval tasks.", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=ZoJTA9WZan", "_bibtex": "@misc{\nxiong2021a,\ntitle={A Benchmark for Voice-Face Cross-Modal Matching and  Retrieval},\nauthor={Chuyuan Xiong and Deyuan Zhang and Tao Liu and Xiaoyong Du and Jiankun Tian and Songyan Xue},\nyear={2021},\nurl={https://openreview.net/forum?id=bFnn6lPn3Sp}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "bFnn6lPn3Sp", "replyto": "bFnn6lPn3Sp", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper327/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538145540, "tmdate": 1606915810446, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper327/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper327/-/Official_Review"}}}], "count": 5}