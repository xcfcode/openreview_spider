{"notes": [{"id": "E3Ys6a1NTGT", "original": "hkR5Wk70oYl", "number": 1484, "cdate": 1601308165180, "ddate": null, "tcdate": 1601308165180, "tmdate": 1616085676297, "tddate": null, "forum": "E3Ys6a1NTGT", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "The Importance of Pessimism in Fixed-Dataset Policy Optimization", "authorids": ["~Jacob_Buckman2", "cgel@openai.com", "~Marc_G_Bellemare1"], "authors": ["Jacob Buckman", "Carles Gelada", "Marc G Bellemare"], "keywords": ["deep learning", "reinforcement learning", "offline reinforcement learning"], "abstract": "We study worst-case guarantees on the expected return of fixed-dataset policy optimization algorithms. Our core contribution is a unified conceptual and mathematical framework for the study of algorithms in this regime. This analysis reveals that for naive approaches, the possibility of erroneous value overestimation leads to a difficult-to-satisfy requirement: in order to guarantee that we select a policy which is near-optimal, we may need the dataset to be informative of the value of every policy. To avoid this, algorithms can follow the pessimism principle, which states that we should choose the policy which acts optimally in the worst possible world. We show why pessimistic algorithms can achieve good performance even when the dataset is not informative of every policy, and derive families of algorithms which follow this principle. These theoretical findings are validated by experiments on a tabular gridworld, and deep learning experiments on four MinAtar environments.", "one-sentence_summary": "A unified conceptual and mathematical framework for fixed-dataset policy optimization algorithms, revealing the importance of uncertainty and pessimism.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "buckman|the_importance_of_pessimism_in_fixeddataset_policy_optimization", "pdf": "/pdf/e3dfbddf4c9078c3d334b54122b25fd90aba6c9b.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nbuckman2021the,\ntitle={The Importance of Pessimism in Fixed-Dataset Policy Optimization},\nauthor={Jacob Buckman and Carles Gelada and Marc G Bellemare},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=E3Ys6a1NTGT}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "UHQU_ux_qBc", "original": null, "number": 1, "cdate": 1610040493361, "ddate": null, "tcdate": 1610040493361, "tmdate": 1610474099465, "tddate": null, "forum": "E3Ys6a1NTGT", "replyto": "E3Ys6a1NTGT", "invitation": "ICLR.cc/2021/Conference/Paper1484/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Poster)", "comment": "The reviewers agree in their positive evaluation of the paper. A weakness of the paper pointed out by several reviewers was its presentation, which has hovewer improved. Thus, I'm glad to recommend acceptance."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Importance of Pessimism in Fixed-Dataset Policy Optimization", "authorids": ["~Jacob_Buckman2", "cgel@openai.com", "~Marc_G_Bellemare1"], "authors": ["Jacob Buckman", "Carles Gelada", "Marc G Bellemare"], "keywords": ["deep learning", "reinforcement learning", "offline reinforcement learning"], "abstract": "We study worst-case guarantees on the expected return of fixed-dataset policy optimization algorithms. Our core contribution is a unified conceptual and mathematical framework for the study of algorithms in this regime. This analysis reveals that for naive approaches, the possibility of erroneous value overestimation leads to a difficult-to-satisfy requirement: in order to guarantee that we select a policy which is near-optimal, we may need the dataset to be informative of the value of every policy. To avoid this, algorithms can follow the pessimism principle, which states that we should choose the policy which acts optimally in the worst possible world. We show why pessimistic algorithms can achieve good performance even when the dataset is not informative of every policy, and derive families of algorithms which follow this principle. These theoretical findings are validated by experiments on a tabular gridworld, and deep learning experiments on four MinAtar environments.", "one-sentence_summary": "A unified conceptual and mathematical framework for fixed-dataset policy optimization algorithms, revealing the importance of uncertainty and pessimism.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "buckman|the_importance_of_pessimism_in_fixeddataset_policy_optimization", "pdf": "/pdf/e3dfbddf4c9078c3d334b54122b25fd90aba6c9b.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nbuckman2021the,\ntitle={The Importance of Pessimism in Fixed-Dataset Policy Optimization},\nauthor={Jacob Buckman and Carles Gelada and Marc G Bellemare},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=E3Ys6a1NTGT}\n}"}, "tags": [], "invitation": {"reply": {"forum": "E3Ys6a1NTGT", "replyto": "E3Ys6a1NTGT", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040493346, "tmdate": 1610474099449, "id": "ICLR.cc/2021/Conference/Paper1484/-/Decision"}}}, {"id": "y1onO5W1NGG", "original": null, "number": 2, "cdate": 1603763993990, "ddate": null, "tcdate": 1603763993990, "tmdate": 1606360397490, "tddate": null, "forum": "E3Ys6a1NTGT", "replyto": "E3Ys6a1NTGT", "invitation": "ICLR.cc/2021/Conference/Paper1484/-/Official_Review", "content": {"title": "needs better organization and some missing related work", "review": "Summary:\n\nThe paper proposes a theoretical framework for analyzing the error of reinforcement learning algorithms in a fixed dataset policy optimization (FDPO) setting.  In such settings, data has been collected by a single policy that may not be optimal and the learner puts together a model or value function that will have explicit or implicit uncertainty in areas where the data is not dense enough.  The authors provide bounds connecting the uncertainty to the loss.  They then show that explicitly pessimistic algorithms that fill in the uncertainty with the worst case can minimize the worst case error.  Similarly, proximal algorithms that attempt to adhere to the collection policy (as often the case in model-free batch RL) have improved error compared to a naive approach but not as good as an explicitly pessimistic approach.\n\n\nReview:\n\nThe paper provides a general description of the pessimism performance bounds.  The theorems appear to be correct and the reasoning sound.  I also like the connection to the proximal approach, which is how most model-free batch RL algorithms approach the problem (by sampling close to the collection policy).\n\nHowever, the paper does need some improvement.  Specifically, a connection should be made to more existing literature on pessimism in safe, batch, or apprenticeship RL.  In addition, the paper spends a lot of time on definitions and notation that are not explicitly used while the most interesting empirical results are relegated to the appendix, which seems backwards.\n\nOn the connections to the literature, the idea of using pessimism in situations where you are learning from a dataset collected by a non-optimal teacher has been investigated in previous works in apprenticeship RL:\nhttp://proceedings.mlr.press/v125/cohen20a/cohen20a.pdf\nor\nhttps://papers.nips.cc/paper/4240-blending-autonomous-exploration-and-apprenticeship-learning.pdf \n\nSpecifically, the first (Bayesian) paper explicitly reasons about the worst of all possible worlds mentioned in the current submission and seems to have a lot of overlap in the theory.  Can the authors distinguish their results from Cohen et al.?  The second paper is an example where model-learning agents keep track of the uncertainty in their learned transition and reward functions and use pessimism to fill in uncertainty.  So the idea here is not quite new and better connections to this literature need to be made.\n\nThe other issue with the paper is its organization and writing. The theoretical results, while general, are not particularly complicated and don\u2019t seem to warrant the amount of notation and definitions on pages 1-3.  Specifically, the bandit example isn\u2019t really mentioned in the paper but the figure takes up a lot of valuable space.  Over a full page is used to define basic MDP and dataset terms that are widely known and commonly used.  The footnotes are whole paragraphs that seem to be just asides.  Finally, the grid word results are presented in a figure without any real associated text except for some generalities about what algorithms worked well,  Meanwhile, the most interesting and novel contributions of the paper, including the concrete algorithms for applying pessimistic learning, and the empirical analysis on Atari games, are stashed in the (very long) appendix.  I strongly suggest the authors reorganize the paper to highlight these strengths instead of notation and footnotes that are tangential to the paper.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1484/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1484/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Importance of Pessimism in Fixed-Dataset Policy Optimization", "authorids": ["~Jacob_Buckman2", "cgel@openai.com", "~Marc_G_Bellemare1"], "authors": ["Jacob Buckman", "Carles Gelada", "Marc G Bellemare"], "keywords": ["deep learning", "reinforcement learning", "offline reinforcement learning"], "abstract": "We study worst-case guarantees on the expected return of fixed-dataset policy optimization algorithms. Our core contribution is a unified conceptual and mathematical framework for the study of algorithms in this regime. This analysis reveals that for naive approaches, the possibility of erroneous value overestimation leads to a difficult-to-satisfy requirement: in order to guarantee that we select a policy which is near-optimal, we may need the dataset to be informative of the value of every policy. To avoid this, algorithms can follow the pessimism principle, which states that we should choose the policy which acts optimally in the worst possible world. We show why pessimistic algorithms can achieve good performance even when the dataset is not informative of every policy, and derive families of algorithms which follow this principle. These theoretical findings are validated by experiments on a tabular gridworld, and deep learning experiments on four MinAtar environments.", "one-sentence_summary": "A unified conceptual and mathematical framework for fixed-dataset policy optimization algorithms, revealing the importance of uncertainty and pessimism.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "buckman|the_importance_of_pessimism_in_fixeddataset_policy_optimization", "pdf": "/pdf/e3dfbddf4c9078c3d334b54122b25fd90aba6c9b.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nbuckman2021the,\ntitle={The Importance of Pessimism in Fixed-Dataset Policy Optimization},\nauthor={Jacob Buckman and Carles Gelada and Marc G Bellemare},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=E3Ys6a1NTGT}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "E3Ys6a1NTGT", "replyto": "E3Ys6a1NTGT", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1484/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538117566, "tmdate": 1606915798349, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1484/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1484/-/Official_Review"}}}, {"id": "_NJ87T34PYO", "original": null, "number": 1, "cdate": 1603739237786, "ddate": null, "tcdate": 1603739237786, "tmdate": 1605888258947, "tddate": null, "forum": "E3Ys6a1NTGT", "replyto": "E3Ys6a1NTGT", "invitation": "ICLR.cc/2021/Conference/Paper1484/-/Official_Review", "content": {"title": "What are the implications of the decomposition?", "review": "The message of this paper is that naive policy evaluations common in current (deep) RL algorithms, can lead to a dangerous overestimation of the value function. This overestimation of the value function can then lead to policy improvements with poor theoretical guarantees. To combat overestimation, the authors propose to penalize state-action pairs that are rarely visited. As an easier to implement alternative, and closer to existing algorithms in the literature, the authors also study another penalty term that penalizes deviation from the data generating policy. The authors show on a numerical example that the more principled penalty term that depends on visitation counts is better performing, and that the proximal penalty term only yields minor improvements over imitation learning (i.e. returning the data generating policy).\n\nThe main contribution of the paper is to decompose the sub-optimality upper bound into terms that either overestimate or underestimate the total reward that can be collected in the true MDP. The authors argue that the overestimation is especially problematic for (the many) RL algorithms that are subject to such overestimation, as there is a high chance of existence of a policy that performs poorly on the true MDP but has high reward on the empirical MDP (the MDP with empirical estimates of the reward and transitions), resulting in a large sub-optimality. \n\nAs far as I am aware, this decomposition is new. But I wonder if beyond formalizing the sub-optimality of naive algorithms, it has other theoretical or practical applications. The notion of pessimism is typical in the analysis of theoretically grounded algorithms (e.g. CPI in\nApproximately Optimal Approximate Reinforcement Learning, Kakade et al. 2002), where deviation from \u2018known\u2019 state-action pairs is typically maximally penalized with the worst possible value (i.e. a sub-optimality of 1 / (1-\\gamma)). So I wonder if the decomposition in overestimation/underestimation terms in Lemma 1 allows for new theoretical insights and algorithmic developments or if it is more of a rewriting, and similar results can be obtained by more carefully choosing the empirical MDP D, such that the overestimation term disappears with high probability even in the worst case and only the underestimation term remains. As is, I understand the reasons for exhibiting both underestimation/overestimation terms in order to analyze \u2018naive\u2019 algorithms in the sense of Sec. 4, but is there an advantage for this decomposition and for the algorithm in Sec. 5.1 compared to choosing the optimal policy without an penalty term but in a more carefully constructed MDP D\u2019 that doesn\u2019t allow for overestimation? Similarly, is there any benefit in not choosing \\alpha = 1 in Sec. 5.1? Is there an optimal choice for \\alpha for Sec. 5.2?\n\nAs for the practical implications, the results in Sec. 6 are quite depressing since algorithms with a proximal penalty are easier to implement than with the uncertainty penalty. What was the \\alpha in the experiments? I wonder if the results can be improved for the proximal algorithm if a better choice of \\alpha is used depending on the optimality of the data generating policy or on the size of the dataset.\n\nOverall, the paper is an interesting read and its message is well presented and supported. However, I am wondering if the theoretical contributions can serve another purpose than warning about the poor theoretical guarantees of \u2018naive\u2019 algorithms, and hope the authors can correct me if I underappreciated the importance of these derivations.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1484/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1484/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Importance of Pessimism in Fixed-Dataset Policy Optimization", "authorids": ["~Jacob_Buckman2", "cgel@openai.com", "~Marc_G_Bellemare1"], "authors": ["Jacob Buckman", "Carles Gelada", "Marc G Bellemare"], "keywords": ["deep learning", "reinforcement learning", "offline reinforcement learning"], "abstract": "We study worst-case guarantees on the expected return of fixed-dataset policy optimization algorithms. Our core contribution is a unified conceptual and mathematical framework for the study of algorithms in this regime. This analysis reveals that for naive approaches, the possibility of erroneous value overestimation leads to a difficult-to-satisfy requirement: in order to guarantee that we select a policy which is near-optimal, we may need the dataset to be informative of the value of every policy. To avoid this, algorithms can follow the pessimism principle, which states that we should choose the policy which acts optimally in the worst possible world. We show why pessimistic algorithms can achieve good performance even when the dataset is not informative of every policy, and derive families of algorithms which follow this principle. These theoretical findings are validated by experiments on a tabular gridworld, and deep learning experiments on four MinAtar environments.", "one-sentence_summary": "A unified conceptual and mathematical framework for fixed-dataset policy optimization algorithms, revealing the importance of uncertainty and pessimism.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "buckman|the_importance_of_pessimism_in_fixeddataset_policy_optimization", "pdf": "/pdf/e3dfbddf4c9078c3d334b54122b25fd90aba6c9b.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nbuckman2021the,\ntitle={The Importance of Pessimism in Fixed-Dataset Policy Optimization},\nauthor={Jacob Buckman and Carles Gelada and Marc G Bellemare},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=E3Ys6a1NTGT}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "E3Ys6a1NTGT", "replyto": "E3Ys6a1NTGT", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1484/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538117566, "tmdate": 1606915798349, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1484/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1484/-/Official_Review"}}}, {"id": "qUIfLNbAbne", "original": null, "number": 6, "cdate": 1605888211682, "ddate": null, "tcdate": 1605888211682, "tmdate": 1605888211682, "tddate": null, "forum": "E3Ys6a1NTGT", "replyto": "WTAdctltVsw", "invitation": "ICLR.cc/2021/Conference/Paper1484/-/Official_Comment", "content": {"title": " ", "comment": "Thank you for your thorough response.  Regarding '*Our work indicates that there is likely an \u201cuncertainty-aware CPI\u201d, which is able to be more sample-efficient while retaining the key property of safe monotonic improvement*' you might be interested in Safe Policy Iteration, Pirotta et al. 2013. I think it goes into a direction of a more refined and easy to implement pessimism than 1/(1-$\\gamma$), although it is not in the FDPO setting.\n\nOf course, it would have been better if the authors arrived at a concrete solution to close the gap between the naive algorithms with loose theoretical guarantees and the theoretical algorithms that are hard to implement. But I agree with R1 that the derivations of a unified framework might drive future researcher in this direction so I am raising my score accordingly. "}, "signatures": ["ICLR.cc/2021/Conference/Paper1484/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1484/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Importance of Pessimism in Fixed-Dataset Policy Optimization", "authorids": ["~Jacob_Buckman2", "cgel@openai.com", "~Marc_G_Bellemare1"], "authors": ["Jacob Buckman", "Carles Gelada", "Marc G Bellemare"], "keywords": ["deep learning", "reinforcement learning", "offline reinforcement learning"], "abstract": "We study worst-case guarantees on the expected return of fixed-dataset policy optimization algorithms. Our core contribution is a unified conceptual and mathematical framework for the study of algorithms in this regime. This analysis reveals that for naive approaches, the possibility of erroneous value overestimation leads to a difficult-to-satisfy requirement: in order to guarantee that we select a policy which is near-optimal, we may need the dataset to be informative of the value of every policy. To avoid this, algorithms can follow the pessimism principle, which states that we should choose the policy which acts optimally in the worst possible world. We show why pessimistic algorithms can achieve good performance even when the dataset is not informative of every policy, and derive families of algorithms which follow this principle. These theoretical findings are validated by experiments on a tabular gridworld, and deep learning experiments on four MinAtar environments.", "one-sentence_summary": "A unified conceptual and mathematical framework for fixed-dataset policy optimization algorithms, revealing the importance of uncertainty and pessimism.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "buckman|the_importance_of_pessimism_in_fixeddataset_policy_optimization", "pdf": "/pdf/e3dfbddf4c9078c3d334b54122b25fd90aba6c9b.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nbuckman2021the,\ntitle={The Importance of Pessimism in Fixed-Dataset Policy Optimization},\nauthor={Jacob Buckman and Carles Gelada and Marc G Bellemare},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=E3Ys6a1NTGT}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "E3Ys6a1NTGT", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1484/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1484/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1484/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1484/Authors|ICLR.cc/2021/Conference/Paper1484/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1484/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923859167, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1484/-/Official_Comment"}}}, {"id": "WTAdctltVsw", "original": null, "number": 5, "cdate": 1605629031794, "ddate": null, "tcdate": 1605629031794, "tmdate": 1605635472401, "tddate": null, "forum": "E3Ys6a1NTGT", "replyto": "BiTZx391q_g", "invitation": "ICLR.cc/2021/Conference/Paper1484/-/Official_Comment", "content": {"title": "Response, pt 2", "comment": "To respond to some of the other specific points you raised:\n- *Connection to CPI.* Yes, this is a very insightful connection. The setting of CPI is quite different from the setting we consider, in that CPI involves constantly collecting more data, so it only needs to stay proximal to the most recent policy. (In contrast to our work, which stays proximal to the behavior policy on a fixed dataset). CPI\u2019s choice of 1/(1-\u03b3) as a penalty is precisely analogous to the choice made by the proximal algorithms we discuss, and indeed, it implements a form of pessimism. Our work indicates that there is likely an \u201cuncertainty-aware CPI\u201d, which is able to be more sample-efficient while retaining the key property of safe monotonic improvement, by using non-trivial uncertainty estimates (instead of the trivial one of 1/(1-\u03b3). This is an interesting direction for future work.\n- *On a \u201cpessimistic empirical MDP\u201d formulation.* The intuition you describe is a good one, but we believe our fixed-point formulation is better than a pessimistic-MDP formulation for two concrete technical reasons. Firstly, a pessimistic empirical MDP is much less general, and doesn't allow you to derive some of the more interesting approaches. This is clear by simply noting that any pessimistic empirical MDP is still an MDP, and thus has a deterministic optimal policy, whereas the proximal & 'hedging' approaches (Appendix B.2) both have stochastic optimal policies, and as such cannot be represented. (The more general \u201cbounded-parameter MDP\u201d aka \u201crobust MDP\u201d framework, e.g. Givan et al. 2002, is general enough, but it introduces significant analytical complexity relative to our approach.) The second reason is that, although our results were primarily tabular, one aim of this work is to develop a mathematical framework that can easily be extended to the deep learning setting. Many popular deep learning algorithms are based on Q-learning, and the penalty formulation aligns much better with the structure of these algorithms than a pessimistic-MDP formulation.\n- *Similarly, is there any benefit in not choosing \\alpha = 1 in Sec. 5.1? Is there an optimal choice for \\alpha for Sec. 5.2?* Yes. In general the optimal value of alpha that minimizes the bound is neither 0 nor 1.  Since the supremum term will typically be much larger than the infimum term on real-world environments, it is typically the case that we will want an \u03b1 much closer to 1. However, it\u2019s not clear how to compute the true optimal \u03b1, which is data- and environment-dependent.\n- *What was the \\alpha in the experiments? I wonder if the results can be improved for the proximal algorithm if a better choice of \\alpha is used depending on the optimality of the data generating policy or on the size of the dataset.*  In our experiments, hyperparameters were intentionally selected arbitrarily and tuned minimally, to ensure that the qualitative results were not cherry-picked. The tabular experiments used a hyperparameter of \u03b1=.25. Our experiments are designed to test empirically whether algorithms have the properties predicted by our theoretical results. For both proximal and UA algorithms, changing alpha from 0 to 1 gives an interpolation from the naive curve to the imitation curve, and at intermediate values (which includes our selected values), curves look similar to the results we presented. It\u2019s likely that for any specific problem, performance can be improved somewhat by hyperparameter tuning \u03b1, but there is no value of \u03b1 that will make the proximal curve look like the UA curve.\nAlso, note that information on \u201cthe optimality of the data generating policy\u201d is not in general available in the FDPO setting, so we cannot use it to tune. Tuning on \u201cthe size of the dataset\u201d is possible, but does not lead to a principled algorithm -- see our response to Question 1 from Reviewer 1. Finally, note as well that any tuning which requires testing out policies in the real environment is not permitted by the FDPO setting."}, "signatures": ["ICLR.cc/2021/Conference/Paper1484/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1484/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Importance of Pessimism in Fixed-Dataset Policy Optimization", "authorids": ["~Jacob_Buckman2", "cgel@openai.com", "~Marc_G_Bellemare1"], "authors": ["Jacob Buckman", "Carles Gelada", "Marc G Bellemare"], "keywords": ["deep learning", "reinforcement learning", "offline reinforcement learning"], "abstract": "We study worst-case guarantees on the expected return of fixed-dataset policy optimization algorithms. Our core contribution is a unified conceptual and mathematical framework for the study of algorithms in this regime. This analysis reveals that for naive approaches, the possibility of erroneous value overestimation leads to a difficult-to-satisfy requirement: in order to guarantee that we select a policy which is near-optimal, we may need the dataset to be informative of the value of every policy. To avoid this, algorithms can follow the pessimism principle, which states that we should choose the policy which acts optimally in the worst possible world. We show why pessimistic algorithms can achieve good performance even when the dataset is not informative of every policy, and derive families of algorithms which follow this principle. These theoretical findings are validated by experiments on a tabular gridworld, and deep learning experiments on four MinAtar environments.", "one-sentence_summary": "A unified conceptual and mathematical framework for fixed-dataset policy optimization algorithms, revealing the importance of uncertainty and pessimism.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "buckman|the_importance_of_pessimism_in_fixeddataset_policy_optimization", "pdf": "/pdf/e3dfbddf4c9078c3d334b54122b25fd90aba6c9b.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nbuckman2021the,\ntitle={The Importance of Pessimism in Fixed-Dataset Policy Optimization},\nauthor={Jacob Buckman and Carles Gelada and Marc G Bellemare},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=E3Ys6a1NTGT}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "E3Ys6a1NTGT", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1484/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1484/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1484/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1484/Authors|ICLR.cc/2021/Conference/Paper1484/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1484/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923859167, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1484/-/Official_Comment"}}}, {"id": "BiTZx391q_g", "original": null, "number": 4, "cdate": 1605629003178, "ddate": null, "tcdate": 1605629003178, "tmdate": 1605629003178, "tddate": null, "forum": "E3Ys6a1NTGT", "replyto": "_NJ87T34PYO", "invitation": "ICLR.cc/2021/Conference/Paper1484/-/Official_Comment", "content": {"title": "Response, pt 1", "comment": "Thank you for your comments. Your main concerns seem to be around the relevance of the results; to clarify the relevance, we have re-organized and rewritten some sections of the paper in order to emphasize the impactful insights. Here, we also will attempt to briefly explain.\n\nAs you identified, the core contribution of our work is around the importance of pessimism in this setting: mathematically characterizing the specific reason that being pessimistic yields improvements. So, why is understanding the reason useful? Here is our perspective:\n\nIn the past few months, there has been an enormous amount of interest in developing algorithms for FDPO. (See \u201cDeep learning FDPO approaches\u201d in Appendix E for a list of recent related work.) Our contribution serves to inform and motivate the design of pessimistic algorithms for this setting. Whereas other works have mostly proposed solutions, we explain *why* some solutions are better than others.\n\nOne key takeaway from our work, which you correctly identified, is that proximal pessimistic algorithms are in some sense a special case of uncertainty-aware algorithms. A huge proportion of recently-proposed algorithms have been proximal algorithms, but all have the same fundamental limitations. One clear reason our research is valuable is that it conclusively demonstrates that to move beyond, we will need to research uncertainty-aware algorithms. (We have now made this connection more explicit in a new section, 5.3 of the revised draft.)\n\nHere are three further examples of how our work can be used to gain insight into recent algorithms, and how it can serve to inform and guide future research. \n- Wang et al. (2020) propose a new algorithm which uses policy-gradient to find a solution instead of Q-learning; however, since we see from our analysis that its performance is dependent on its fixed-point (which is not impacted by the search procedure), it will have the same suboptimality as any other proximal algorithm. \n- Kumar et al. (2020) propose a new algorithm which they claim improves performance because it guarantees a Q-function lower bound; however, our work shows that it is simply a variant of a proximal algorithm, and the lower-bound property is not relevant to suboptimality. \n- Liu et al. (2020) propose a new algorithm which is proximal to the state-distribution of the empirical policy, rather than the state-action distribution; our analysis reveals that this is still simply a slight variant of proximal algorithms, and its suboptimality will have similar properties.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1484/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1484/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Importance of Pessimism in Fixed-Dataset Policy Optimization", "authorids": ["~Jacob_Buckman2", "cgel@openai.com", "~Marc_G_Bellemare1"], "authors": ["Jacob Buckman", "Carles Gelada", "Marc G Bellemare"], "keywords": ["deep learning", "reinforcement learning", "offline reinforcement learning"], "abstract": "We study worst-case guarantees on the expected return of fixed-dataset policy optimization algorithms. Our core contribution is a unified conceptual and mathematical framework for the study of algorithms in this regime. This analysis reveals that for naive approaches, the possibility of erroneous value overestimation leads to a difficult-to-satisfy requirement: in order to guarantee that we select a policy which is near-optimal, we may need the dataset to be informative of the value of every policy. To avoid this, algorithms can follow the pessimism principle, which states that we should choose the policy which acts optimally in the worst possible world. We show why pessimistic algorithms can achieve good performance even when the dataset is not informative of every policy, and derive families of algorithms which follow this principle. These theoretical findings are validated by experiments on a tabular gridworld, and deep learning experiments on four MinAtar environments.", "one-sentence_summary": "A unified conceptual and mathematical framework for fixed-dataset policy optimization algorithms, revealing the importance of uncertainty and pessimism.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "buckman|the_importance_of_pessimism_in_fixeddataset_policy_optimization", "pdf": "/pdf/e3dfbddf4c9078c3d334b54122b25fd90aba6c9b.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nbuckman2021the,\ntitle={The Importance of Pessimism in Fixed-Dataset Policy Optimization},\nauthor={Jacob Buckman and Carles Gelada and Marc G Bellemare},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=E3Ys6a1NTGT}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "E3Ys6a1NTGT", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1484/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1484/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1484/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1484/Authors|ICLR.cc/2021/Conference/Paper1484/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1484/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923859167, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1484/-/Official_Comment"}}}, {"id": "no73rN8kyQ6", "original": null, "number": 3, "cdate": 1605628425184, "ddate": null, "tcdate": 1605628425184, "tmdate": 1605628543917, "tddate": null, "forum": "E3Ys6a1NTGT", "replyto": "y1onO5W1NGG", "invitation": "ICLR.cc/2021/Conference/Paper1484/-/Official_Comment", "content": {"title": "Response", "comment": "Thank you for the feedback. Your major concerns seem to be the organization of the paper and connection to related work.\n\nAt the suggestion of you and the other reviewers, we have updated the draft to have improved organization by shrinking the notation & background section (moving most details to the appendix), and moving much of the discussion from the appendix to the main body of the paper. We\u2019ve posted the latest revision -- hopefully this clarifies the presentation to your satisfaction.\n\nThank you as well for suggesting these references; we were not previously familiar with the apprenticeship learning literature, and have added it to the related works in Appendix E. In response to your request, we\u2019d be happy to specifically distinguish our work from Cohen at al. Though the intuitions are similar, the specific technical differences from Cohen et al. are quite significant. Firstly, as mentioned previously, the setting is different: the apprenticeship learning setting, as defined in that work, involves the problem of continually observing a mentor and deciding whether to imitate (defer) or take an alternative action. This process of trading off the collection of new data with the use of old knowledge has much more in common with active learning or reinforcement learning than it does with FDPO, which is pure exploitation. Next, their solution implicitly relies on a good prior; while all of our analysis is prior-free. Their key results are around convergence to the quality of the mentor, whereas ours are around the reduction of suboptimality. Also, their approach is considerably more abstract, and less amenable to implementation; their discussion centers around an \u201cidealized agent\u201d, writing, \u201c...this agent is only tractable when the model class is very simple, but it can inspire tractable approximations\u201d. In contrast, our algorithms are concrete and implementable, and we in fact implement them, in both the tabular and deep learning settings. Thus, the work of Cohen et al. is related conceptually, but complimentary when it comes to technical content.\n\nRegarding the related work at a high level: we wish to emphasize that our core contribution is not \u201cpessimism\u201d, in general, but rather a technical analysis of the importance of pessimism in the *specific* context of FDPO. (We've updated the language in the introduction of the paper to make this more clear.) FDPO is also known as batch RL or offline RL, however, it is somewhat different from the apprenticeship setting (where a teacher can be queried for additional data) and the safety setting (several formulations, but in general, the objective is some notion of safety rather than minimizing suboptimality). For readers who are interested in any of these other, related settings, we provide a comprehensive overview of all of these connections in Appendix E. Are there any particular connections which you feel are central enough that they need to be included in the main text?\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1484/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1484/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Importance of Pessimism in Fixed-Dataset Policy Optimization", "authorids": ["~Jacob_Buckman2", "cgel@openai.com", "~Marc_G_Bellemare1"], "authors": ["Jacob Buckman", "Carles Gelada", "Marc G Bellemare"], "keywords": ["deep learning", "reinforcement learning", "offline reinforcement learning"], "abstract": "We study worst-case guarantees on the expected return of fixed-dataset policy optimization algorithms. Our core contribution is a unified conceptual and mathematical framework for the study of algorithms in this regime. This analysis reveals that for naive approaches, the possibility of erroneous value overestimation leads to a difficult-to-satisfy requirement: in order to guarantee that we select a policy which is near-optimal, we may need the dataset to be informative of the value of every policy. To avoid this, algorithms can follow the pessimism principle, which states that we should choose the policy which acts optimally in the worst possible world. We show why pessimistic algorithms can achieve good performance even when the dataset is not informative of every policy, and derive families of algorithms which follow this principle. These theoretical findings are validated by experiments on a tabular gridworld, and deep learning experiments on four MinAtar environments.", "one-sentence_summary": "A unified conceptual and mathematical framework for fixed-dataset policy optimization algorithms, revealing the importance of uncertainty and pessimism.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "buckman|the_importance_of_pessimism_in_fixeddataset_policy_optimization", "pdf": "/pdf/e3dfbddf4c9078c3d334b54122b25fd90aba6c9b.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nbuckman2021the,\ntitle={The Importance of Pessimism in Fixed-Dataset Policy Optimization},\nauthor={Jacob Buckman and Carles Gelada and Marc G Bellemare},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=E3Ys6a1NTGT}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "E3Ys6a1NTGT", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1484/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1484/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1484/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1484/Authors|ICLR.cc/2021/Conference/Paper1484/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1484/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923859167, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1484/-/Official_Comment"}}}, {"id": "rVrOrKefALC", "original": null, "number": 2, "cdate": 1605628361033, "ddate": null, "tcdate": 1605628361033, "tmdate": 1605628361033, "tddate": null, "forum": "E3Ys6a1NTGT", "replyto": "eyt6C-TqAg-", "invitation": "ICLR.cc/2021/Conference/Paper1484/-/Official_Comment", "content": {"title": "Response", "comment": "Thank you for the thorough feedback. It seems your major concern, raised in weaknesses 2-4, is the organization of the paper. Specifically, you disagreed with the choice to include certain things (notation), while leaving others in the appendix (discussion). We agree. The additional notation is needed for the proofs, but indeed not necessary to the main paper since the proofs are in the appendix.\n\nAt the suggestion of you and the other reviewers, we have updated the draft to have improved organization by shrinking the notation & background section (moving most details to the appendix), and moving much of the discussion from the appendix to the main body of the paper. We\u2019ve posted the latest revision -- hopefully this clarifies the presentation to your satisfaction.\n\nWe also wish to respond to the other two points you raised.\n\nWeakness 1:\nThe justification for the claim \"uncertainty-aware algorithms are strictly better than proximal algorithms\" was somewhat buried in the appendix, but it truly does hold rigorously. In the latest draft, we\u2019ve added a new section, 5.3, which makes this point clear. Here\u2019s the argument at a high level: proximal algorithms are equivalent to a special case of uncertainty-aware algorithms, where we use the trivial state-action-wise Bellman uncertainty function which returns 1/(1-\u03b3) everywhere. Thus, not only are uncertainty-aware algorithms strictly more general than proximal algorithms, but proximal algorithms are in a certain sense the \u201cworst\u201d algorithm of this type. As a result, we believe that our strong phrasing in the conclusion is warranted.\n\nYou also raise a very good point about the fact that the suboptimality of a proximal algorithm with \u03b1=0 does not recover the original bound for the proximal algorithm. This is due to a choice of presentation of the result, not a fundamental weakness of the analysis. There are two ways we could write the result. The tighter way is: inf (A + \u03b1B) + sup (A - \u03b1B), where A is the term from the naive analysis. In this case, we\u2019d recover the desired result at \u03b1=0. But, since B >= A, we instead chose to write it as: inf (1+\u03b1)B + sup (1 - \u03b1)B, which emphasizes the fact that the term vanishes when \u03b1=1. However, we agree that losing the equivalence at \u03b1=0 is a real problem, and as such, have switched it to the tighter version. See the updated Theorem 3 in the revised draft for the updated result.\n\nQuestion 1:\nThe approach you propose would make the algorithm consistent, but still run into problems in the finite data regime. Consider what happens if you take a dataset D, choose one transition <s,a,r,s\u2019>, and create a new dataset D\u2019 := D + (1e100 * <s,a,r,s\u2019>), i.e., add 1e100 identical copies of the transition. Consider running your algorithm on D\u2019. Since the size of D\u2019 is greater than 1e100, the pessimism scaling factor (which, per your description, shrinks with dataset size) will be close to 0, meaning we will simply be using the naive algorithm. But of course, adding 1e100 copies of the same transition has not informed us about any other state-actions, so naive suboptimality will be very poor. Thus, the suboptimality of your algorithm on this problem will be poor as well."}, "signatures": ["ICLR.cc/2021/Conference/Paper1484/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1484/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Importance of Pessimism in Fixed-Dataset Policy Optimization", "authorids": ["~Jacob_Buckman2", "cgel@openai.com", "~Marc_G_Bellemare1"], "authors": ["Jacob Buckman", "Carles Gelada", "Marc G Bellemare"], "keywords": ["deep learning", "reinforcement learning", "offline reinforcement learning"], "abstract": "We study worst-case guarantees on the expected return of fixed-dataset policy optimization algorithms. Our core contribution is a unified conceptual and mathematical framework for the study of algorithms in this regime. This analysis reveals that for naive approaches, the possibility of erroneous value overestimation leads to a difficult-to-satisfy requirement: in order to guarantee that we select a policy which is near-optimal, we may need the dataset to be informative of the value of every policy. To avoid this, algorithms can follow the pessimism principle, which states that we should choose the policy which acts optimally in the worst possible world. We show why pessimistic algorithms can achieve good performance even when the dataset is not informative of every policy, and derive families of algorithms which follow this principle. These theoretical findings are validated by experiments on a tabular gridworld, and deep learning experiments on four MinAtar environments.", "one-sentence_summary": "A unified conceptual and mathematical framework for fixed-dataset policy optimization algorithms, revealing the importance of uncertainty and pessimism.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "buckman|the_importance_of_pessimism_in_fixeddataset_policy_optimization", "pdf": "/pdf/e3dfbddf4c9078c3d334b54122b25fd90aba6c9b.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nbuckman2021the,\ntitle={The Importance of Pessimism in Fixed-Dataset Policy Optimization},\nauthor={Jacob Buckman and Carles Gelada and Marc G Bellemare},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=E3Ys6a1NTGT}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "E3Ys6a1NTGT", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1484/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1484/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1484/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1484/Authors|ICLR.cc/2021/Conference/Paper1484/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1484/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923859167, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1484/-/Official_Comment"}}}, {"id": "eyt6C-TqAg-", "original": null, "number": 3, "cdate": 1603890063517, "ddate": null, "tcdate": 1603890063517, "tmdate": 1605024431716, "tddate": null, "forum": "E3Ys6a1NTGT", "replyto": "E3Ys6a1NTGT", "invitation": "ICLR.cc/2021/Conference/Paper1484/-/Official_Review", "content": {"title": "Review", "review": "**Summary:**\n\nThis paper attempts to unify prior work on fixed-dataset (aka \"batch\" or \"offline\") reinforcement learning. Specifically, it emphasizes the importance of pessimism to account for faulty over-estimation from finite datasets. The paper shows that naive algorithms (with no pessimism) can recover the optimal policy with enough data, but do so more efficiently. The pessimistic algorithms are divided into \"uncertainty-aware\" and \"proximal\" algorithms where the uncertainty-aware algorithms are shown to be more principled, but most prior work falls into the computationally easier proximal family of algorithms that is closer to imitation learning. These insights are proven both theoretically and with some small experiments.\n\n--------------------------------------------------------------------\n\n**Strengths:**\n\n1. A nice decomposition of suboptimality. The main workhorse of the paper is the decomposition provided in Lemma 1 which is novel and can provide some good intuition about the necessity of pessimism (although the intuition is only given in appendix G.3, which should definitely find it's way into the main text). The Lemma cleanly and formally demonstrates why we may expect over-estimation to be more damaging than under-estimation.\n2. A clear framework to examine prior work. The paper does well to capture the majority of recent work into a few broad families of algorithms: naive, proximal pessimistic, and uncertainty-aware pessimistic. The bound derived from the main Lemma for each algorithm family provide evidence to prefer uncertainty-aware algorithms. This is supported by the tabular experiments.\n3. The formal statements of Lemmas and Theorems seem to be correct and experimental methodology seems sound.\n\n--------------------------------------------------------------------\n\n**Weaknesses:**\n\n1. I am wary of the comparison of upper bounds done in the paper. Just because one algorithm has a lower upper bound does not prove superior performance. I agree that since all the proofs are derived from Lemma 1 and are very similar, the differences are indeed suggestive. However, the bound in Theorem 3 seems to be more loose than the others. For example, when $\\alpha = 0$ it does not recover the bound for the naive algorithm as would be expected. A more measured tone and careful description of these comparisons is needed. Claims like \"uncertainty-aware algorithms are strictly better than proximal algorithms\" in the conclusion are not substantiated. \n2. Lack of discussion of issues with implementation and function approximation. As the authors get into in Appendix G.6 and Appendix F.2 and briefly in the paper it is not clear how to implement the uncertainty-aware family of algorithms in a scalable way. I am not saying that this paper needs to resolve this issue (it is clearly hard), but this drawback needs to be made more clear in the main text of the paper, so as to not mislead the reader.\n3. Notation is heavy and sometimes nonstandard. I understand that the nature of this paper will lead to a lot of notation, but I think the paper could be made more accessible if the authors go back through the paper and remove notation that may only be needed in the proofs and may be unnecessary to present the main results. For example, the several different notions of uncertainty funtions might be useful in the appendix, but do not seem to all be necessary to present the main results. Similarly, the notion of decomposability is introduced and then largely forgotten for the rest of the paper. Some notation is nonstandard. For example: $d$ is used for number of datapoints (usually it would be dimension) and $ \\Phi$ is used as the data distribution (usually if would be a feature generating function or feature matrix).\n4. Abuse of the appendix. While I understant that the 8 page limit can be difficult, this paper especially abuses the appendix often sending important parts of the discussion and intuition for the results into appendix G. The paper would be stronger with some editing of the notation and organization of the main text to make room for more of the needed discussion and intuition in main body of the paper.\n\n--------------------------------------------------------------------\n\n**Recommendation:**\n\nI gave the paper a score of 7, and recommend acceptance. The paper provides a nice framing of prior work on fixed-dataset RL. While it leaves some things to be desired in terms of carefulness, scalability, and clarity, I think it provides a solid contribution that will be useful to researchers in the field.\n\nIf the authors are able to sufficiently improve the clarity of presentation as discussed in the weaknesses section, I could consider raising my score.\n\n--------------------------------------------------------------------\n\n**Questions for the authors:**\n\n1. It is natural to think that a practical proximal pessimistic algorithm would reduce the level of pessimism with the dataset size (so that it approaches the naive algorithm with infinite data). Do approaches like this resolve many of the issues that you bring up with proximal pessimistic algorithms (albeit by introducing another hyperparameter to tune)?\n\n--------------------------------------------------------------------\n\n**Additional feedback:**\n\nTypos:\n\n- The first sentence on page 4 is not grammatically correct.\n- In the statements of Lemma 1 and Theorem 1, $ \\pi^*_D$ is defined and never used.\n- In the statement of Theorem 1 $ u_{D,\\delta}^\\pi$ is defined but then only $ \\mu_{D,\\delta}^\\pi$ is used without being defined.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1484/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1484/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Importance of Pessimism in Fixed-Dataset Policy Optimization", "authorids": ["~Jacob_Buckman2", "cgel@openai.com", "~Marc_G_Bellemare1"], "authors": ["Jacob Buckman", "Carles Gelada", "Marc G Bellemare"], "keywords": ["deep learning", "reinforcement learning", "offline reinforcement learning"], "abstract": "We study worst-case guarantees on the expected return of fixed-dataset policy optimization algorithms. Our core contribution is a unified conceptual and mathematical framework for the study of algorithms in this regime. This analysis reveals that for naive approaches, the possibility of erroneous value overestimation leads to a difficult-to-satisfy requirement: in order to guarantee that we select a policy which is near-optimal, we may need the dataset to be informative of the value of every policy. To avoid this, algorithms can follow the pessimism principle, which states that we should choose the policy which acts optimally in the worst possible world. We show why pessimistic algorithms can achieve good performance even when the dataset is not informative of every policy, and derive families of algorithms which follow this principle. These theoretical findings are validated by experiments on a tabular gridworld, and deep learning experiments on four MinAtar environments.", "one-sentence_summary": "A unified conceptual and mathematical framework for fixed-dataset policy optimization algorithms, revealing the importance of uncertainty and pessimism.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "buckman|the_importance_of_pessimism_in_fixeddataset_policy_optimization", "pdf": "/pdf/e3dfbddf4c9078c3d334b54122b25fd90aba6c9b.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nbuckman2021the,\ntitle={The Importance of Pessimism in Fixed-Dataset Policy Optimization},\nauthor={Jacob Buckman and Carles Gelada and Marc G Bellemare},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=E3Ys6a1NTGT}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "E3Ys6a1NTGT", "replyto": "E3Ys6a1NTGT", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1484/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538117566, "tmdate": 1606915798349, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1484/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1484/-/Official_Review"}}}], "count": 10}