{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1488353655476, "tcdate": 1478242702561, "number": 134, "id": "rJPcZ3txx", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "rJPcZ3txx", "signatures": ["~Jongsoo_Park1"], "readers": ["everyone"], "content": {"title": "Faster CNNs with Direct Sparse Convolutions and Guided Pruning", "abstract": "Phenomenally successful in practical inference problems, convolutional neural networks (CNN) are widely deployed in mobile devices, data centers, and even supercomputers.\nThe number of parameters needed in CNNs, however, are often large and undesirable. Consequently, various methods have been developed to prune a CNN once it is trained. \nNevertheless, the resulting CNNs offer limited benefits. While pruning the fully connected layers reduces a CNN's size considerably, it does not improve inference speed noticeably as the compute heavy parts lie in convolutions. Pruning CNNs in a way that increase inference speed often imposes specific sparsity structures, thus limiting the achievable sparsity levels.\n\nWe present a method to realize simultaneously size economy and speed improvement while pruning CNNs. Paramount to our success is an efficient general sparse-with-dense matrix\nmultiplication implementation that is applicable to convolution of feature maps with kernels of arbitrary sparsity patterns. Complementing this, we developed a performance model that predicts sweet spots of sparsity levels for different layers and on different computer architectures. Together, these two allow us to demonstrate 3.1-7.3x convolution speedups over dense convolution in AlexNet, on Intel Atom, Xeon, and Xeon Phi processors, spanning the spectrum from mobile devices to supercomputers.\n", "pdf": "/pdf/b64769c6f7937fa521cbcfceb2bc0af1c24aeb00.pdf", "TL;DR": "Highly-performance sparse convolution outperforms dense with only 70% sparsity. Performance model that guides training to find useful sparsity range, applied to AlexNet and GoogLeNet", "paperhash": "park|faster_cnns_with_direct_sparse_convolutions_and_guided_pruning", "authors": ["Jongsoo Park", "Sheng Li", "Wei Wen", "Ping Tak Peter Tang", "Hai Li", "Yiran Chen", "Pradeep Dubey"], "keywords": ["Deep learning", "Optimization"], "conflicts": ["intel.com", "stanford.edu", "pitt.edu"], "authorids": ["jongsoo.park@intel.com", "sheng.r.li@intel.com", "peter.tang@intel.com", "weiwen.web@gmail.com", "HAL66@pitt.edu", "yic52@pitt.edu", "pradeep.dubey@intel.com"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396374906, "tcdate": 1486396374906, "number": 1, "id": "Bky1nfU_g", "invitation": "ICLR.cc/2017/conference/-/paper134/acceptance", "forum": "rJPcZ3txx", "replyto": "rJPcZ3txx", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"title": "ICLR committee final decision", "comment": "While the core ideas explored in this paper are quite limited in algorithmic novelty (e.g., the direct sparse convolutions), the reviewers largely feel that the paper is well written, experiments are carefully done on multiple architectures and system issues are discussed in-depth. Given the interest in the ICLR community around performance characterization and acceleration of CNNs in particular, this paper offers an interesting perspective.", "decision": "Accept (Poster)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Faster CNNs with Direct Sparse Convolutions and Guided Pruning", "abstract": "Phenomenally successful in practical inference problems, convolutional neural networks (CNN) are widely deployed in mobile devices, data centers, and even supercomputers.\nThe number of parameters needed in CNNs, however, are often large and undesirable. Consequently, various methods have been developed to prune a CNN once it is trained. \nNevertheless, the resulting CNNs offer limited benefits. While pruning the fully connected layers reduces a CNN's size considerably, it does not improve inference speed noticeably as the compute heavy parts lie in convolutions. Pruning CNNs in a way that increase inference speed often imposes specific sparsity structures, thus limiting the achievable sparsity levels.\n\nWe present a method to realize simultaneously size economy and speed improvement while pruning CNNs. Paramount to our success is an efficient general sparse-with-dense matrix\nmultiplication implementation that is applicable to convolution of feature maps with kernels of arbitrary sparsity patterns. Complementing this, we developed a performance model that predicts sweet spots of sparsity levels for different layers and on different computer architectures. Together, these two allow us to demonstrate 3.1-7.3x convolution speedups over dense convolution in AlexNet, on Intel Atom, Xeon, and Xeon Phi processors, spanning the spectrum from mobile devices to supercomputers.\n", "pdf": "/pdf/b64769c6f7937fa521cbcfceb2bc0af1c24aeb00.pdf", "TL;DR": "Highly-performance sparse convolution outperforms dense with only 70% sparsity. Performance model that guides training to find useful sparsity range, applied to AlexNet and GoogLeNet", "paperhash": "park|faster_cnns_with_direct_sparse_convolutions_and_guided_pruning", "authors": ["Jongsoo Park", "Sheng Li", "Wei Wen", "Ping Tak Peter Tang", "Hai Li", "Yiran Chen", "Pradeep Dubey"], "keywords": ["Deep learning", "Optimization"], "conflicts": ["intel.com", "stanford.edu", "pitt.edu"], "authorids": ["jongsoo.park@intel.com", "sheng.r.li@intel.com", "peter.tang@intel.com", "weiwen.web@gmail.com", "HAL66@pitt.edu", "yic52@pitt.edu", "pradeep.dubey@intel.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396375379, "id": "ICLR.cc/2017/conference/-/paper134/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "rJPcZ3txx", "replyto": "rJPcZ3txx", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396375379}}}, {"tddate": null, "tmdate": 1482359415413, "tcdate": 1482343110176, "number": 3, "id": "SkCpzH_4g", "invitation": "ICLR.cc/2017/conference/-/paper134/public/comment", "forum": "rJPcZ3txx", "replyto": "SkAX_RbNl", "signatures": ["~Jongsoo_Park1"], "readers": ["everyone"], "writers": ["~Jongsoo_Park1"], "content": {"title": "Summary of related work", "comment": "Please note that, in addition to our fast convolution algorithms, another important contribution is the performance model guiding the pruning process that allows pursuing desired speedup and model size reduction without falling into combinatorial number of choices offered by multiple layers. Our performance model can also guide other methods discussed in related work as shown by our application to dynamic network surgery (GDNS in Figure 4(a)).\n\nThanks for the suggestion on summarizing inference speedups and model size reductions of related work. A quick summary is shown below, which we will also consider including in our paper. It is important to note that our work achieves highest speedup without accuracy loss among all the techniques below. The speedups shown that are not our own measurements should be taken with a grain of salt because 1) many papers only provide relative speedups to a baseline whose efficiency is suboptimal (e.g. in some cases, the baseline is Caffe running on CPU, which is known to be suboptimal as it is tuned for GPU), and 2) what some papers report as \"speedup\" is actually FLOP reduction, not actual timing measurements. As we did in our paper, for more scientific comparison among different CNN speedup techniques, we recommend using dense matrix multiplication (GEMM) FLOP/s of the evaluated platform as the baseline, because many platforms readily have vendor-provided extensively-optimized GEMM implementations which can be a proxy of highly-optimized dense CNN implementation. This also aligns with a long-accepted common practice in the high-performance computing (HPC) community. We omit Denton et al. 2014, Jaderberg et al. 2014, and Lebedev et al. 2015 in the summary because they report improvements in a subset of conv and fc layers.\n \nAlexNet\nGESL (ours, 0% top-1 accuracy drop):                8.5x smaller model,                      2.5x speedup,                                      4.2x FLOP reduction\nDNS (0.5% top-1 accuracy drop):                    17.7x smaller model,                      1.0x speedup (not enough sparsity in conv layers), 2.8x FLOP reduction\nSSL (0% top-1 accuracy drop):                       1.01x smaller model (no sparsity in fc), 1.5x speedup,                                      1.3x FLOP reduction\nLebedev and Lempitsky (1.3% top-1 accuracy drop):   2.9x smaller model,                      3.0x speedup? (not sure if this is a real speedup or FLOP reduction)\nLiu et al. (1% top-1 accuracy drop):                1.04x smaller model (no sparsity in fc), 4.4x speedup (not sure if lowering overhead included)\nKim et al. (1.7% top-5 accuracy drop):              5.5x smaller model,                      1.8x speedup,                                      2.7x FLOP reduction\nTai et al. (0.4% top-5 accuracy drop):              5.0x smaller model,                      1.8x speedup,                                      5.3x FLOP reduction\n\nGoogLeNet\nGESL (0.2% top-1 accuracy drop):                    3.3x smaller model,                      2.0x speedups in conv and fc layers,               3.0x FLOP reduction\nDNS (our own evaluation, 2.5% top-1 accuracy drop): 1.5x smaller model,                      2.0x speedups in conv and fc layers,               2.6x FLOP reduction\nSSL (our own evaluation, 2% top-1 accuracy drop):   2.1x smaller model,                      speedup N/A yet,                                   2.3x FLOP reduction\nKim et al. (0.2% top-5 accuracy drop):              1.3x smaller model,                      1.2x speedup,                                      1.3x FLOP reduction\nIoannou et al. (0.4% top-1 accuracy drop):          1.7x smaller model,                      speedup N/A,                                       1.4x FLOP reduction\nTai et al. (0.4% top-5 accuracy drop):              2.8x smaller model,                      1.2x speedup,                                      2.9x FLOP reduction"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Faster CNNs with Direct Sparse Convolutions and Guided Pruning", "abstract": "Phenomenally successful in practical inference problems, convolutional neural networks (CNN) are widely deployed in mobile devices, data centers, and even supercomputers.\nThe number of parameters needed in CNNs, however, are often large and undesirable. Consequently, various methods have been developed to prune a CNN once it is trained. \nNevertheless, the resulting CNNs offer limited benefits. While pruning the fully connected layers reduces a CNN's size considerably, it does not improve inference speed noticeably as the compute heavy parts lie in convolutions. Pruning CNNs in a way that increase inference speed often imposes specific sparsity structures, thus limiting the achievable sparsity levels.\n\nWe present a method to realize simultaneously size economy and speed improvement while pruning CNNs. Paramount to our success is an efficient general sparse-with-dense matrix\nmultiplication implementation that is applicable to convolution of feature maps with kernels of arbitrary sparsity patterns. Complementing this, we developed a performance model that predicts sweet spots of sparsity levels for different layers and on different computer architectures. Together, these two allow us to demonstrate 3.1-7.3x convolution speedups over dense convolution in AlexNet, on Intel Atom, Xeon, and Xeon Phi processors, spanning the spectrum from mobile devices to supercomputers.\n", "pdf": "/pdf/b64769c6f7937fa521cbcfceb2bc0af1c24aeb00.pdf", "TL;DR": "Highly-performance sparse convolution outperforms dense with only 70% sparsity. Performance model that guides training to find useful sparsity range, applied to AlexNet and GoogLeNet", "paperhash": "park|faster_cnns_with_direct_sparse_convolutions_and_guided_pruning", "authors": ["Jongsoo Park", "Sheng Li", "Wei Wen", "Ping Tak Peter Tang", "Hai Li", "Yiran Chen", "Pradeep Dubey"], "keywords": ["Deep learning", "Optimization"], "conflicts": ["intel.com", "stanford.edu", "pitt.edu"], "authorids": ["jongsoo.park@intel.com", "sheng.r.li@intel.com", "peter.tang@intel.com", "weiwen.web@gmail.com", "HAL66@pitt.edu", "yic52@pitt.edu", "pradeep.dubey@intel.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287714739, "id": "ICLR.cc/2017/conference/-/paper134/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rJPcZ3txx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper134/reviewers", "ICLR.cc/2017/conference/paper134/areachairs"], "cdate": 1485287714739}}}, {"tddate": null, "tmdate": 1482342844805, "tcdate": 1482342844805, "number": 2, "id": "B1B6WH_Ne", "invitation": "ICLR.cc/2017/conference/-/paper134/public/comment", "forum": "rJPcZ3txx", "replyto": "HJTCvJz4e", "signatures": ["~Jongsoo_Park1"], "readers": ["everyone"], "writers": ["~Jongsoo_Park1"], "content": {"title": "Clarification on research contribution", "comment": "We respectfully ask the reviewer to reconsider the assessment that there is a lack of research contribution in the present paper. It is well accepted that deep learning is enabled by three similarly crucial components \u2013 algorithm, data, and performance. Each component benefited from a succession of original research efforts which are still active at present. These include the invention of back propagation (past) to new optimization algorithms for training (present), the creation of the ImageNet dataset (past) to the Re\u2019s \u2018\u2019data programming\u201d approach (present), the lowering method that transforms convolution to matrix products (past) to computer architecture for special-purpose hardware accelerator (present). \n\nThe present paper belongs to the performance research area of sparsification. While sparsification has been effective in memory footprint reduction, it has hitherto limited success in inference throughput enhancement. The greatly enhanced throughput reported here cannot be achieved solely by our direct sparse convolution technique (which is a new fast algorithm by itself). Preserving inference accuracy is an implicit requirement for all sparsification endeavor. Sparsification for performance enhancement therefore cannot be successful without understanding where to \u201cuse the sparsification budget\u201d for most effective performance gain. And a simplistic \u201cengineering\u201d trial-and-error approach is infeasible for the combinatorial number of choices offered by tens of layers and hundreds of channels. We successfully identified sparsification targets by combining a high-resolution performance model and a sparsity allocation methodology, both of which contain original research that are applicable to performance research in general."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Faster CNNs with Direct Sparse Convolutions and Guided Pruning", "abstract": "Phenomenally successful in practical inference problems, convolutional neural networks (CNN) are widely deployed in mobile devices, data centers, and even supercomputers.\nThe number of parameters needed in CNNs, however, are often large and undesirable. Consequently, various methods have been developed to prune a CNN once it is trained. \nNevertheless, the resulting CNNs offer limited benefits. While pruning the fully connected layers reduces a CNN's size considerably, it does not improve inference speed noticeably as the compute heavy parts lie in convolutions. Pruning CNNs in a way that increase inference speed often imposes specific sparsity structures, thus limiting the achievable sparsity levels.\n\nWe present a method to realize simultaneously size economy and speed improvement while pruning CNNs. Paramount to our success is an efficient general sparse-with-dense matrix\nmultiplication implementation that is applicable to convolution of feature maps with kernels of arbitrary sparsity patterns. Complementing this, we developed a performance model that predicts sweet spots of sparsity levels for different layers and on different computer architectures. Together, these two allow us to demonstrate 3.1-7.3x convolution speedups over dense convolution in AlexNet, on Intel Atom, Xeon, and Xeon Phi processors, spanning the spectrum from mobile devices to supercomputers.\n", "pdf": "/pdf/b64769c6f7937fa521cbcfceb2bc0af1c24aeb00.pdf", "TL;DR": "Highly-performance sparse convolution outperforms dense with only 70% sparsity. Performance model that guides training to find useful sparsity range, applied to AlexNet and GoogLeNet", "paperhash": "park|faster_cnns_with_direct_sparse_convolutions_and_guided_pruning", "authors": ["Jongsoo Park", "Sheng Li", "Wei Wen", "Ping Tak Peter Tang", "Hai Li", "Yiran Chen", "Pradeep Dubey"], "keywords": ["Deep learning", "Optimization"], "conflicts": ["intel.com", "stanford.edu", "pitt.edu"], "authorids": ["jongsoo.park@intel.com", "sheng.r.li@intel.com", "peter.tang@intel.com", "weiwen.web@gmail.com", "HAL66@pitt.edu", "yic52@pitt.edu", "pradeep.dubey@intel.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287714739, "id": "ICLR.cc/2017/conference/-/paper134/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rJPcZ3txx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper134/reviewers", "ICLR.cc/2017/conference/paper134/areachairs"], "cdate": 1485287714739}}}, {"tddate": null, "tmdate": 1482342795399, "tcdate": 1482342795399, "number": 1, "id": "SJQqbr_Vg", "invitation": "ICLR.cc/2017/conference/-/paper134/public/comment", "forum": "rJPcZ3txx", "replyto": "SJ6rokQEe", "signatures": ["~Jongsoo_Park1"], "readers": ["everyone"], "writers": ["~Jongsoo_Park1"], "content": {"title": "Our performance model shows a nice direction for future work. Application to GPU", "comment": "We thank the reviewer\u2019s appreciation of our sparse convolution algorithms as well as guided pruning techniques and the comment that the performance-model-guided techniques can be used in future work. Indeed, there could be very interesting directions for future work on building new neural network architectures with our techniques. As shown in GoogLeNet designs, building new network architectures with reduced computing demand is important.  However, as shown in our paper, the actual performance improvement is not a simple linear function of reduced FLOP count; instead, the convolution kernel size, input and output channel dimensions, and characteristics of a target platform among others, determine the actual speed of a neural network  on the target platform. Our performance-model-guided approach can accurately project actual speedup of a neural network on specific target platforms (CPU/GPU/FPGA/ASIC). Consequently, design decisions on new network architecture can be custom made for a specific hardware platform in question. We will add an elaboration on this to our current paper.\n\nOur model is very transferable to other platforms including GPUs. For example, with a typical 90% sparsity, both Pascal Titian-X and P100 GPUs are projected to achieve similar ~3x speedups over their dense baselines. Moreover, our model also reveals that sparse convolution becomes memory bandwidth bound and thus provide diminishing speedups earlier on Titan-X than on P100. This is because Titan-X has a much higher flop to byte ratio than P100 equipped with a new high bandwidth memory technology, HBM2. These insights shed light on designing sparse convolution on GPUs."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Faster CNNs with Direct Sparse Convolutions and Guided Pruning", "abstract": "Phenomenally successful in practical inference problems, convolutional neural networks (CNN) are widely deployed in mobile devices, data centers, and even supercomputers.\nThe number of parameters needed in CNNs, however, are often large and undesirable. Consequently, various methods have been developed to prune a CNN once it is trained. \nNevertheless, the resulting CNNs offer limited benefits. While pruning the fully connected layers reduces a CNN's size considerably, it does not improve inference speed noticeably as the compute heavy parts lie in convolutions. Pruning CNNs in a way that increase inference speed often imposes specific sparsity structures, thus limiting the achievable sparsity levels.\n\nWe present a method to realize simultaneously size economy and speed improvement while pruning CNNs. Paramount to our success is an efficient general sparse-with-dense matrix\nmultiplication implementation that is applicable to convolution of feature maps with kernels of arbitrary sparsity patterns. Complementing this, we developed a performance model that predicts sweet spots of sparsity levels for different layers and on different computer architectures. Together, these two allow us to demonstrate 3.1-7.3x convolution speedups over dense convolution in AlexNet, on Intel Atom, Xeon, and Xeon Phi processors, spanning the spectrum from mobile devices to supercomputers.\n", "pdf": "/pdf/b64769c6f7937fa521cbcfceb2bc0af1c24aeb00.pdf", "TL;DR": "Highly-performance sparse convolution outperforms dense with only 70% sparsity. Performance model that guides training to find useful sparsity range, applied to AlexNet and GoogLeNet", "paperhash": "park|faster_cnns_with_direct_sparse_convolutions_and_guided_pruning", "authors": ["Jongsoo Park", "Sheng Li", "Wei Wen", "Ping Tak Peter Tang", "Hai Li", "Yiran Chen", "Pradeep Dubey"], "keywords": ["Deep learning", "Optimization"], "conflicts": ["intel.com", "stanford.edu", "pitt.edu"], "authorids": ["jongsoo.park@intel.com", "sheng.r.li@intel.com", "peter.tang@intel.com", "weiwen.web@gmail.com", "HAL66@pitt.edu", "yic52@pitt.edu", "pradeep.dubey@intel.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287714739, "id": "ICLR.cc/2017/conference/-/paper134/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rJPcZ3txx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper134/reviewers", "ICLR.cc/2017/conference/paper134/areachairs"], "cdate": 1485287714739}}}, {"tddate": null, "tmdate": 1482165694157, "tcdate": 1481922598129, "number": 1, "id": "SkAX_RbNl", "invitation": "ICLR.cc/2017/conference/-/paper134/official/review", "forum": "rJPcZ3txx", "replyto": "rJPcZ3txx", "signatures": ["ICLR.cc/2017/conference/paper134/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper134/AnonReviewer2"], "content": {"title": "review", "rating": "6: Marginally above acceptance threshold", "review": "This paper tackles the problem of compressing trained convnets with the goal of reducing memory overhead and speeding up the forward pass. As I understand it, the main contribution of this work is to develop fast convolution routines for sparse conv weights int he case of general sparsity (as compared with structured sparsity). They evaluate their method on both AlexNet and GoogLeNet as well as on various platforms. The authors make code available online. The paper is well written and does a good job of putting this work in the context of past model reduction techniques.\n\nMy main request of the authors would be to provide a concise summary of the speedup/memory gains achievable with this new work compared with previously published work. The authors do show the various sparsity level obtained with various methods of pruning but it is unclear to me how to translate the information given in the paper into an understanding of gains relative to other methods.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Faster CNNs with Direct Sparse Convolutions and Guided Pruning", "abstract": "Phenomenally successful in practical inference problems, convolutional neural networks (CNN) are widely deployed in mobile devices, data centers, and even supercomputers.\nThe number of parameters needed in CNNs, however, are often large and undesirable. Consequently, various methods have been developed to prune a CNN once it is trained. \nNevertheless, the resulting CNNs offer limited benefits. While pruning the fully connected layers reduces a CNN's size considerably, it does not improve inference speed noticeably as the compute heavy parts lie in convolutions. Pruning CNNs in a way that increase inference speed often imposes specific sparsity structures, thus limiting the achievable sparsity levels.\n\nWe present a method to realize simultaneously size economy and speed improvement while pruning CNNs. Paramount to our success is an efficient general sparse-with-dense matrix\nmultiplication implementation that is applicable to convolution of feature maps with kernels of arbitrary sparsity patterns. Complementing this, we developed a performance model that predicts sweet spots of sparsity levels for different layers and on different computer architectures. Together, these two allow us to demonstrate 3.1-7.3x convolution speedups over dense convolution in AlexNet, on Intel Atom, Xeon, and Xeon Phi processors, spanning the spectrum from mobile devices to supercomputers.\n", "pdf": "/pdf/b64769c6f7937fa521cbcfceb2bc0af1c24aeb00.pdf", "TL;DR": "Highly-performance sparse convolution outperforms dense with only 70% sparsity. Performance model that guides training to find useful sparsity range, applied to AlexNet and GoogLeNet", "paperhash": "park|faster_cnns_with_direct_sparse_convolutions_and_guided_pruning", "authors": ["Jongsoo Park", "Sheng Li", "Wei Wen", "Ping Tak Peter Tang", "Hai Li", "Yiran Chen", "Pradeep Dubey"], "keywords": ["Deep learning", "Optimization"], "conflicts": ["intel.com", "stanford.edu", "pitt.edu"], "authorids": ["jongsoo.park@intel.com", "sheng.r.li@intel.com", "peter.tang@intel.com", "weiwen.web@gmail.com", "HAL66@pitt.edu", "yic52@pitt.edu", "pradeep.dubey@intel.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512686327, "id": "ICLR.cc/2017/conference/-/paper134/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper134/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper134/AnonReviewer2", "ICLR.cc/2017/conference/paper134/AnonReviewer1", "ICLR.cc/2017/conference/paper134/AnonReviewer3"], "reply": {"forum": "rJPcZ3txx", "replyto": "rJPcZ3txx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper134/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper134/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512686327}}}, {"tddate": null, "tmdate": 1481993029418, "tcdate": 1481993029418, "number": 3, "id": "SJ6rokQEe", "invitation": "ICLR.cc/2017/conference/-/paper134/official/review", "forum": "rJPcZ3txx", "replyto": "rJPcZ3txx", "signatures": ["ICLR.cc/2017/conference/paper134/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper134/AnonReviewer3"], "content": {"title": "review", "rating": "7: Good paper, accept", "review": "The paper details an implementation of sparse-full convolutions and a model to work out the potential speed-up of various sparsity levels for CNNs.\n\nThe first contribution is more about engineering, but the authors make the source code available which is greatly appreciated.\n\nThe second contribution is perhaps more interesting, as so far pruning methods have focused on saving memory, with very modest speed gains. Imbuing knowledge of running speed into a pruning algorithm seems like the proper way to tackle this problem. The authors are very methodical in how they build the model and evaluate it very thoroughly.\n\nIt seems that the same idea could be used not just for pruning existing models, but also when building new architectures: selecting layers and their parameters as to achieve an optimal throughput rate. This could make for a nice direction for future work.\n\nOne point that is missing is some discussion of how transferable the performance model is to GPUs. This would make the technique easier to adopt broadly.\n\nOther areas for improvement: The points in Figure 4 are hard to distinguish (e.g. small red circle vs. small red square), and overall the figure could be made bigger; specifying whether the \"base learning rate\" in Section 3 is the start or end rate of the annealing schedule; typos: \"punning\" (p.4), \"spares\" (p.5).\n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Faster CNNs with Direct Sparse Convolutions and Guided Pruning", "abstract": "Phenomenally successful in practical inference problems, convolutional neural networks (CNN) are widely deployed in mobile devices, data centers, and even supercomputers.\nThe number of parameters needed in CNNs, however, are often large and undesirable. Consequently, various methods have been developed to prune a CNN once it is trained. \nNevertheless, the resulting CNNs offer limited benefits. While pruning the fully connected layers reduces a CNN's size considerably, it does not improve inference speed noticeably as the compute heavy parts lie in convolutions. Pruning CNNs in a way that increase inference speed often imposes specific sparsity structures, thus limiting the achievable sparsity levels.\n\nWe present a method to realize simultaneously size economy and speed improvement while pruning CNNs. Paramount to our success is an efficient general sparse-with-dense matrix\nmultiplication implementation that is applicable to convolution of feature maps with kernels of arbitrary sparsity patterns. Complementing this, we developed a performance model that predicts sweet spots of sparsity levels for different layers and on different computer architectures. Together, these two allow us to demonstrate 3.1-7.3x convolution speedups over dense convolution in AlexNet, on Intel Atom, Xeon, and Xeon Phi processors, spanning the spectrum from mobile devices to supercomputers.\n", "pdf": "/pdf/b64769c6f7937fa521cbcfceb2bc0af1c24aeb00.pdf", "TL;DR": "Highly-performance sparse convolution outperforms dense with only 70% sparsity. Performance model that guides training to find useful sparsity range, applied to AlexNet and GoogLeNet", "paperhash": "park|faster_cnns_with_direct_sparse_convolutions_and_guided_pruning", "authors": ["Jongsoo Park", "Sheng Li", "Wei Wen", "Ping Tak Peter Tang", "Hai Li", "Yiran Chen", "Pradeep Dubey"], "keywords": ["Deep learning", "Optimization"], "conflicts": ["intel.com", "stanford.edu", "pitt.edu"], "authorids": ["jongsoo.park@intel.com", "sheng.r.li@intel.com", "peter.tang@intel.com", "weiwen.web@gmail.com", "HAL66@pitt.edu", "yic52@pitt.edu", "pradeep.dubey@intel.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512686327, "id": "ICLR.cc/2017/conference/-/paper134/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper134/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper134/AnonReviewer2", "ICLR.cc/2017/conference/paper134/AnonReviewer1", "ICLR.cc/2017/conference/paper134/AnonReviewer3"], "reply": {"forum": "rJPcZ3txx", "replyto": "rJPcZ3txx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper134/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper134/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512686327}}}, {"tddate": null, "tmdate": 1481926612601, "tcdate": 1481926612601, "number": 2, "id": "HJTCvJz4e", "invitation": "ICLR.cc/2017/conference/-/paper134/official/review", "forum": "rJPcZ3txx", "replyto": "rJPcZ3txx", "signatures": ["ICLR.cc/2017/conference/paper134/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper134/AnonReviewer1"], "content": {"title": "Impressive empirical results; minimal research contribution", "rating": "6: Marginally above acceptance threshold", "review": "The authors provide a well engineered solution to exploiting sparsity in convolutional layers of a deep network by recasting it as sparse matrix-vector multiplication. This leads to very nice speedups and the analysis of when this is possible is also useful for practitioners. My main concern with this paper is that the \"research\" aspect of it seems rather minimal, and it's mostly about performance engineering and comparisons. It is upto the area chairs to decide how well such a paper fits in at ICLR.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Faster CNNs with Direct Sparse Convolutions and Guided Pruning", "abstract": "Phenomenally successful in practical inference problems, convolutional neural networks (CNN) are widely deployed in mobile devices, data centers, and even supercomputers.\nThe number of parameters needed in CNNs, however, are often large and undesirable. Consequently, various methods have been developed to prune a CNN once it is trained. \nNevertheless, the resulting CNNs offer limited benefits. While pruning the fully connected layers reduces a CNN's size considerably, it does not improve inference speed noticeably as the compute heavy parts lie in convolutions. Pruning CNNs in a way that increase inference speed often imposes specific sparsity structures, thus limiting the achievable sparsity levels.\n\nWe present a method to realize simultaneously size economy and speed improvement while pruning CNNs. Paramount to our success is an efficient general sparse-with-dense matrix\nmultiplication implementation that is applicable to convolution of feature maps with kernels of arbitrary sparsity patterns. Complementing this, we developed a performance model that predicts sweet spots of sparsity levels for different layers and on different computer architectures. Together, these two allow us to demonstrate 3.1-7.3x convolution speedups over dense convolution in AlexNet, on Intel Atom, Xeon, and Xeon Phi processors, spanning the spectrum from mobile devices to supercomputers.\n", "pdf": "/pdf/b64769c6f7937fa521cbcfceb2bc0af1c24aeb00.pdf", "TL;DR": "Highly-performance sparse convolution outperforms dense with only 70% sparsity. Performance model that guides training to find useful sparsity range, applied to AlexNet and GoogLeNet", "paperhash": "park|faster_cnns_with_direct_sparse_convolutions_and_guided_pruning", "authors": ["Jongsoo Park", "Sheng Li", "Wei Wen", "Ping Tak Peter Tang", "Hai Li", "Yiran Chen", "Pradeep Dubey"], "keywords": ["Deep learning", "Optimization"], "conflicts": ["intel.com", "stanford.edu", "pitt.edu"], "authorids": ["jongsoo.park@intel.com", "sheng.r.li@intel.com", "peter.tang@intel.com", "weiwen.web@gmail.com", "HAL66@pitt.edu", "yic52@pitt.edu", "pradeep.dubey@intel.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512686327, "id": "ICLR.cc/2017/conference/-/paper134/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper134/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper134/AnonReviewer2", "ICLR.cc/2017/conference/paper134/AnonReviewer1", "ICLR.cc/2017/conference/paper134/AnonReviewer3"], "reply": {"forum": "rJPcZ3txx", "replyto": "rJPcZ3txx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper134/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper134/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512686327}}}], "count": 8}