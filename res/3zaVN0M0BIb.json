{"notes": [{"id": "3zaVN0M0BIb", "original": "xlxAXPvorQb", "number": 3249, "cdate": 1601308360945, "ddate": null, "tcdate": 1601308360945, "tmdate": 1614985778373, "tddate": null, "forum": "3zaVN0M0BIb", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Learning and Generalization in Univariate Overparameterized Normalizing Flows", "authorids": ["~Kulin_Shah1", "~Amit_Deshpande1", "~Navin_Goyal1"], "authors": ["Kulin Shah", "Amit Deshpande", "Navin Goyal"], "keywords": [], "abstract": "In supervised learning, it is known that overparameterized neural networks with one hidden layer provably and efficiently learn and generalize, when trained using Stochastic Gradient Descent (SGD). In contrast, the benefit of overparameterization in unsupervised learning is not well understood. Normalizing flows (NFs) learn to map complex real-world distributions into simple base distributions, and constitute an important class of models in unsupervised learning for sampling and density estimation. In this paper, we theoretically and empirically analyze these models when the underlying neural network is one hidden layer overparametrized network. On the one hand we provide evidence that for a  class of NFs, overparametrization hurts training. On the other, we prove that another class of NFs, with similar underlying networks can efficiently learn any reasonable data distribution under minimal assumptions. We extend theoretical ideas on learning and generalization from overparameterized neural networks in supervised learning to overparameterized normalizing flows in unsupervised learning. We also provide experimental validation to support our theoretical analysis in practice.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shah|learning_and_generalization_in_univariate_overparameterized_normalizing_flows", "supplementary_material": "/attachment/011444e6383112b1e26c2e123c6e9361068941c5.zip", "pdf": "/pdf/ffcee50f276b53bc7faebe19294a696f331c6616.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=FlSd08LhA3", "_bibtex": "@misc{\nshah2021learning,\ntitle={Learning and Generalization in Univariate Overparameterized Normalizing Flows},\nauthor={Kulin Shah and Amit Deshpande and Navin Goyal},\nyear={2021},\nurl={https://openreview.net/forum?id=3zaVN0M0BIb}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 13, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "EnTsTevQuy", "original": null, "number": 1, "cdate": 1610040352224, "ddate": null, "tcdate": 1610040352224, "tmdate": 1610473941404, "tddate": null, "forum": "3zaVN0M0BIb", "replyto": "3zaVN0M0BIb", "invitation": "ICLR.cc/2021/Conference/Paper3249/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "Motivated by the fact that the benefit of overparameterization in unsupervised learning is not well understood than supervised learning, this paper analyzes normalized flow (NF) when the underlying neural network is one hidden layer overparameterized network and proves that for a certain class of NFs, one can efficiently learn any reasonable data distribution under minimal assumptions. The paper is very well motivated. However, the main concerns from the reviewers include (1) the writing quality and presentation are poor, even after revision during the author\u2019s response; and (2) the analysis is limited in the neural tangent kernel (NTK) regime, which makes the results less significant. I agree with the reviewers\u2019 evaluation and I think the first concern can be addressed by a careful revision, while the second concern needs additional nontrivial effort. Thus, I recommend rejection."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning and Generalization in Univariate Overparameterized Normalizing Flows", "authorids": ["~Kulin_Shah1", "~Amit_Deshpande1", "~Navin_Goyal1"], "authors": ["Kulin Shah", "Amit Deshpande", "Navin Goyal"], "keywords": [], "abstract": "In supervised learning, it is known that overparameterized neural networks with one hidden layer provably and efficiently learn and generalize, when trained using Stochastic Gradient Descent (SGD). In contrast, the benefit of overparameterization in unsupervised learning is not well understood. Normalizing flows (NFs) learn to map complex real-world distributions into simple base distributions, and constitute an important class of models in unsupervised learning for sampling and density estimation. In this paper, we theoretically and empirically analyze these models when the underlying neural network is one hidden layer overparametrized network. On the one hand we provide evidence that for a  class of NFs, overparametrization hurts training. On the other, we prove that another class of NFs, with similar underlying networks can efficiently learn any reasonable data distribution under minimal assumptions. We extend theoretical ideas on learning and generalization from overparameterized neural networks in supervised learning to overparameterized normalizing flows in unsupervised learning. We also provide experimental validation to support our theoretical analysis in practice.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shah|learning_and_generalization_in_univariate_overparameterized_normalizing_flows", "supplementary_material": "/attachment/011444e6383112b1e26c2e123c6e9361068941c5.zip", "pdf": "/pdf/ffcee50f276b53bc7faebe19294a696f331c6616.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=FlSd08LhA3", "_bibtex": "@misc{\nshah2021learning,\ntitle={Learning and Generalization in Univariate Overparameterized Normalizing Flows},\nauthor={Kulin Shah and Amit Deshpande and Navin Goyal},\nyear={2021},\nurl={https://openreview.net/forum?id=3zaVN0M0BIb}\n}"}, "tags": [], "invitation": {"reply": {"forum": "3zaVN0M0BIb", "replyto": "3zaVN0M0BIb", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040352208, "tmdate": 1610473941384, "id": "ICLR.cc/2021/Conference/Paper3249/-/Decision"}}}, {"id": "6pmniE70go3", "original": null, "number": 10, "cdate": 1606296097560, "ddate": null, "tcdate": 1606296097560, "tmdate": 1606296097560, "tddate": null, "forum": "3zaVN0M0BIb", "replyto": "zF84QKnepqX", "invitation": "ICLR.cc/2021/Conference/Paper3249/-/Official_Comment", "content": {"title": "Response (part 2/2)", "comment": "*The authors may want to discuss existing results about optimization and generalization of overparameterized deep neural networks [1-3], which are related to this work. Besides, this work relies on the idea of the existence of a pseudo network which approximates the target function well, which may be related to [4-6]. Can the authors discuss and show the relations between these works?*\n\nThank you for the references. This area has a very large number of papers. We have cited papers that are most directly relevant to our work. Some of the references you provide are somewhat less closely related. In particular, some focus on training and do not treat generalization or are concerned with orthogonal considerations such as Resnets. We will include the ones that are directly relevant after looking at them in more detail.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3249/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3249/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning and Generalization in Univariate Overparameterized Normalizing Flows", "authorids": ["~Kulin_Shah1", "~Amit_Deshpande1", "~Navin_Goyal1"], "authors": ["Kulin Shah", "Amit Deshpande", "Navin Goyal"], "keywords": [], "abstract": "In supervised learning, it is known that overparameterized neural networks with one hidden layer provably and efficiently learn and generalize, when trained using Stochastic Gradient Descent (SGD). In contrast, the benefit of overparameterization in unsupervised learning is not well understood. Normalizing flows (NFs) learn to map complex real-world distributions into simple base distributions, and constitute an important class of models in unsupervised learning for sampling and density estimation. In this paper, we theoretically and empirically analyze these models when the underlying neural network is one hidden layer overparametrized network. On the one hand we provide evidence that for a  class of NFs, overparametrization hurts training. On the other, we prove that another class of NFs, with similar underlying networks can efficiently learn any reasonable data distribution under minimal assumptions. We extend theoretical ideas on learning and generalization from overparameterized neural networks in supervised learning to overparameterized normalizing flows in unsupervised learning. We also provide experimental validation to support our theoretical analysis in practice.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shah|learning_and_generalization_in_univariate_overparameterized_normalizing_flows", "supplementary_material": "/attachment/011444e6383112b1e26c2e123c6e9361068941c5.zip", "pdf": "/pdf/ffcee50f276b53bc7faebe19294a696f331c6616.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=FlSd08LhA3", "_bibtex": "@misc{\nshah2021learning,\ntitle={Learning and Generalization in Univariate Overparameterized Normalizing Flows},\nauthor={Kulin Shah and Amit Deshpande and Navin Goyal},\nyear={2021},\nurl={https://openreview.net/forum?id=3zaVN0M0BIb}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "3zaVN0M0BIb", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3249/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3249/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3249/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3249/Authors|ICLR.cc/2021/Conference/Paper3249/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3249/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923839501, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3249/-/Official_Comment"}}}, {"id": "b21X8Q7gkZe", "original": null, "number": 9, "cdate": 1606295957334, "ddate": null, "tcdate": 1606295957334, "tmdate": 1606295957334, "tddate": null, "forum": "3zaVN0M0BIb", "replyto": "zF84QKnepqX", "invitation": "ICLR.cc/2021/Conference/Paper3249/-/Official_Comment", "content": {"title": "Response (part 1/2)", "comment": "Thank you for reading our paper and for your thoughtful comments.\n\n*The presentation of theoretical results can be further improved.*\n\nThank you for detailed comments. We will incorporate them where appropriate.\n\n*In Lemma 1, what does\u00a0$(\\phi^{\u22121}(F^\u2032))_{|\u03b4}$\u00a0mean?*\n\nPlease see the second line of Result 1 on page 12 in the original submission. \n\n*In Theorem 2, it seems that to derive a finite-sample analysis, the second-order derivative of\u00a0$F^\u2217$\u00a0should be finite. The authors may want to add such a claim in the statement of Theorem 1.*\n\nYou are right and we have rectified this in the revised version. \n\n*What is the definition of\u00a0$\\tilde{w}_i$\u00a0in Lemma 3?*\n\n$\\tilde{w}_i$ denotes weight of function value at $i^{th}$ quadrature point in the quadrature sum in the original version of the paper. To make it simpler and more readable, we have changed this notation to $\\Delta_x$ in the new version because weights of function value at all quadrature points in the sum are same.\n\n*What are the definitions of\u00a0$M_{\\hat{L}}$\u00a0and\u00a0$m_{\\hat{L}}$\u00a0in Lemma 12? Are they related to\u00a0$n$?*\n\nThank you for pointing out this omission. We have included both definitions in Eq.(E.1) and Eq.(E.2). No, $M_{\\hat{L}}$ and $m_{\\hat{L}}$ doesn't depend upon $n$. \n\n*Second line in Page 24, eq.() is a typo.*\n\nCorrected; thanks.\n\n*My main concern is the scalability issue. The main theorem suggests that it is possible to use a neural network to approximate the first-order derivative of the unknown distribution transformation\u00a0$f$, and to use the neural network to construct the original function\u00a0$f$\u00a0with sufficient quadrature points. However, just as Theorem 1 suggests, the number of quadrature points is of the order\u00a0$O(1/\\epsilon)$, where\u00a0$\\epsilon$\u00a0is the approximation error. Thus, it seems that for a\u00a0$d$-dimension case, the number of quadrature points may be the order of\u00a0$O(1/ \\epsilon^d )$. Such an exponential dependence is unacceptable in terms of the scalability. Can the authors explain more about the high-dimension case?*\n\nNo, the number of quadrature points will not be exponential in $d$ (i.e. $O(1/\\epsilon^d)$) for the $d$-dimensional problem as the UNF network of Wehenkel and Louppe does not perform integration over the $d$-dimensional space. Note that in the univariate case, we need $O(1/\\epsilon)$ number of quadrature points to approximate one integral with finite sum within $\\epsilon$-error. In the $d$-dimensional case, we need to approximate $d$ integrals with finite sum (See Eq. (8) in Wehenkel and Louppe https://arxiv.org/abs/1908.05164). To approximate each integral by finite sum within $\\epsilon$-error, we will need $O(1/\\epsilon)$ number of quadrature points therefore, combining all of them, we will need $O(poly(d) / \\epsilon)$ number of quadrature points to approximate the original function value. \n\n\n*In Section 2.1, the authors suggest that overparameterization may hurt the overall performance of CNF by showing experiment results of tanh activation function. Is it true that the failure is actually due to the gradient explosion caused by tanh activation function rather than overparameterization?*\n\nWe believe that it is not due to gradient explosion because in Figure 1, we see that when the learning rate is large, L2 norm of change in $w$ and $b$ is large and training with large learning rate becomes more stable than training with smaller learning rate case but if it was happening due to gradient explosion then larger learning rate should have made learning more unstable. \n\n*Meanwhile, it seems that the reason for the authors to use tanh is because they want activations with continuous derivatives and convexity for loss function. Why do we need such a convexity? Is it due to some theoretical concerns (like to make the derivation go through) or concerns from practice?*\n\nThe properties that are necessary for the activation for CNFs are (1) continuous first derivative,  (2) strictly monotonically increasing, (3) non-convex. These properties (satisfied by tanh) are essential for CNFs to work even in practice as discussed in the beginning of Sec. 2.1. \n\nFor theoretical arguments, we additionally want the loss function for the pseudo network to be convex. "}, "signatures": ["ICLR.cc/2021/Conference/Paper3249/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3249/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning and Generalization in Univariate Overparameterized Normalizing Flows", "authorids": ["~Kulin_Shah1", "~Amit_Deshpande1", "~Navin_Goyal1"], "authors": ["Kulin Shah", "Amit Deshpande", "Navin Goyal"], "keywords": [], "abstract": "In supervised learning, it is known that overparameterized neural networks with one hidden layer provably and efficiently learn and generalize, when trained using Stochastic Gradient Descent (SGD). In contrast, the benefit of overparameterization in unsupervised learning is not well understood. Normalizing flows (NFs) learn to map complex real-world distributions into simple base distributions, and constitute an important class of models in unsupervised learning for sampling and density estimation. In this paper, we theoretically and empirically analyze these models when the underlying neural network is one hidden layer overparametrized network. On the one hand we provide evidence that for a  class of NFs, overparametrization hurts training. On the other, we prove that another class of NFs, with similar underlying networks can efficiently learn any reasonable data distribution under minimal assumptions. We extend theoretical ideas on learning and generalization from overparameterized neural networks in supervised learning to overparameterized normalizing flows in unsupervised learning. We also provide experimental validation to support our theoretical analysis in practice.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shah|learning_and_generalization_in_univariate_overparameterized_normalizing_flows", "supplementary_material": "/attachment/011444e6383112b1e26c2e123c6e9361068941c5.zip", "pdf": "/pdf/ffcee50f276b53bc7faebe19294a696f331c6616.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=FlSd08LhA3", "_bibtex": "@misc{\nshah2021learning,\ntitle={Learning and Generalization in Univariate Overparameterized Normalizing Flows},\nauthor={Kulin Shah and Amit Deshpande and Navin Goyal},\nyear={2021},\nurl={https://openreview.net/forum?id=3zaVN0M0BIb}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "3zaVN0M0BIb", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3249/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3249/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3249/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3249/Authors|ICLR.cc/2021/Conference/Paper3249/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3249/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923839501, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3249/-/Official_Comment"}}}, {"id": "M7GtHerSJd8", "original": null, "number": 8, "cdate": 1606294673844, "ddate": null, "tcdate": 1606294673844, "tmdate": 1606294673844, "tddate": null, "forum": "3zaVN0M0BIb", "replyto": "6UvrYpaoWWq", "invitation": "ICLR.cc/2021/Conference/Paper3249/-/Official_Comment", "content": {"title": "Response (part 2/2)", "comment": "*The experiments in this paper are a bit simple, i.e., the authors only did experiments on synthetic datasets like Gaussian mixture with models whose underlying neural networks have only one hidden layer. This setting is far from empirical settings, which makes the conclusions of the experiments not so convincing.*\n\nWe are not sure if we fully understand this comment. Our experiments are for supporting our theory and not to provide new models in practical settings. As for the mixtures of Gaussian distribution, we note that it is well-known (we don't know of a standard reference but see e.g. https://ieeexplore.ieee.org/document/1100034, https://arxiv.org/abs/0805.3795) that mixtures of Gaussians are universal approximators, i.e. they can approximate any continuous probability density on compact sets if one allows sufficiently many Gaussians and the smoother the distribution the smaller the number of required Gaussians. We agree that more datasets could be tried, and indeed we have done that and have obtained similar results. However, there's no small set of datasets that would \"cover\" the space of all 1D probability distributions. Thus experiments can only provide evidence and not a proof that the training works in all cases.\n\n\n*The authors use \"generalization\" in the title, but I am a bit confused because I do not know what generalization means in this normalizing flow setting. Does this mean something like the KL divergence between the learned distribution and the target distribution?*\n\nYes, this is what we mean. We want the learned distribution and the data distributions to be close. Our Theorem 1 shows this for KL-divergence, and as noted in the paper, this also implies that the two distributions are close in total variation distance. \n\n\n*In the last paragraph of page 5, the authors said that convex activations \"cannot be used\" because then $N(x)$ would also be convex. Does $N(x)$ have to be non-convex?*\n\nThis is the CNF setting where we use $N(x)$ to fit $f(x)$. In general, all we know about $f(x)$ is that it is continuous strictly monotonically increasing and need not be convex. In such a situation, at the end of a successful training, $N(x)$ must be non-convex.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3249/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3249/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning and Generalization in Univariate Overparameterized Normalizing Flows", "authorids": ["~Kulin_Shah1", "~Amit_Deshpande1", "~Navin_Goyal1"], "authors": ["Kulin Shah", "Amit Deshpande", "Navin Goyal"], "keywords": [], "abstract": "In supervised learning, it is known that overparameterized neural networks with one hidden layer provably and efficiently learn and generalize, when trained using Stochastic Gradient Descent (SGD). In contrast, the benefit of overparameterization in unsupervised learning is not well understood. Normalizing flows (NFs) learn to map complex real-world distributions into simple base distributions, and constitute an important class of models in unsupervised learning for sampling and density estimation. In this paper, we theoretically and empirically analyze these models when the underlying neural network is one hidden layer overparametrized network. On the one hand we provide evidence that for a  class of NFs, overparametrization hurts training. On the other, we prove that another class of NFs, with similar underlying networks can efficiently learn any reasonable data distribution under minimal assumptions. We extend theoretical ideas on learning and generalization from overparameterized neural networks in supervised learning to overparameterized normalizing flows in unsupervised learning. We also provide experimental validation to support our theoretical analysis in practice.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shah|learning_and_generalization_in_univariate_overparameterized_normalizing_flows", "supplementary_material": "/attachment/011444e6383112b1e26c2e123c6e9361068941c5.zip", "pdf": "/pdf/ffcee50f276b53bc7faebe19294a696f331c6616.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=FlSd08LhA3", "_bibtex": "@misc{\nshah2021learning,\ntitle={Learning and Generalization in Univariate Overparameterized Normalizing Flows},\nauthor={Kulin Shah and Amit Deshpande and Navin Goyal},\nyear={2021},\nurl={https://openreview.net/forum?id=3zaVN0M0BIb}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "3zaVN0M0BIb", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3249/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3249/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3249/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3249/Authors|ICLR.cc/2021/Conference/Paper3249/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3249/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923839501, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3249/-/Official_Comment"}}}, {"id": "pr6JIfRFm9M", "original": null, "number": 7, "cdate": 1606291550577, "ddate": null, "tcdate": 1606291550577, "tmdate": 1606291550577, "tddate": null, "forum": "3zaVN0M0BIb", "replyto": "6UvrYpaoWWq", "invitation": "ICLR.cc/2021/Conference/Paper3249/-/Official_Comment", "content": {"title": "Response (part 1/2)", "comment": "Thank you for reading our paper and for your thoughtful comments.\n\n*This particular setting of normalizing flow models used in this paper might be a bit limited. The authors only analyzed the univariate case, which is far from the high-dimensional case in practice. It is possible that these two cases work in very different regimes due to the differences between high-dimensional and low-dimensional probabilities.*\n\nWe agree that the case of multivariate NFs is indeed what we ultimately want to solve. Please see our common response for why we believe the univariate result is interesting and what it says about the multivariate case. \n\n\n*The authors also made two important modifications to the unconstrained normalizing flow: changing the base distribution to standard exponential distribution and changing the quadrature to simple rectangle quadrature. These modifications can also make the model work in very different ways from practice, and the authors did not provide enough theoretical or experimental justifications for the modifications.\nThe authors made modifications to the setting so that the optimization becomes convex, and this seems to be the only justification for these modifications.*\n\nIn our paper we provide both theoretical and experimental justifications for our modifications and we restate them here: Both of these modifications are motivated by the need to make the optimization problem for the pseudo network convex. In our experiments, the use of the exponential base distributions does not cause any appreciable change. And the use of rectangle quadrature requires about twice as many points and about twice as much time as required for the Clenshaw--Curtis quadrature. It is also pertinent that our modifications are arguably natural. \n\nThere's nothing canonical about using the Gaussian distribution as a base distribution. It does have some properties that are useful for a base distribution to have (easy sampling and log probability density estimation) but Gaussian is far from the only distribution to have them and the widespread use of Gaussian is probably just a historical accident. We remark that the modification to exponential base distribution is not an obvious modification even though it may appear so in retrospect and we consider it as a contribution of our work. \nIndeed, in connection with this rebuttal we checked if any previous work has considered base distributions other than Gaussian. We found an open problem in the survey https://arxiv.org/pdf/1908.09257 and the paper https://arxiv.org/pdf/1907.04481.pdf  mentioned there (contributions here are unrelated to ours) , and a recent arXiv posting (appearing after our work) showing that base distributions other than the Gaussian could be more effective for some purposes: https://arxiv.org/abs/2010.12059. \n\nOn related note, normalizing flows architectures are evolving and there is no single standard architecture. \n\n\n\n*The techniques used in this paper mainly come from [1], and the proof framework and results are roughly the same.*\n\nAs discussed in detail in our paper, our proof for 1D UNFs builds upon [1]. However, it's completely incorrect to say \"the proof framework and results are roughly the same.\" The concrete challenges, both technical and conceptual, that our work needs to overcome are mentioned after the statement of Theorem 1. We would appreciate more concrete details if the reviewer is suggesting that our results can be proved directly using [1] without significant additional work and ideas. Even identifying UNF as a tractable version of NFs took substantial work on our part. \n\nWe remind the reviewer of our other contributions as mentioned in the paper: \n* Identification of architectural variants of UNFs that admit analysis via overparametrization. \n\n* Identification of \u201cbarriers\u201d to the analysis of CNFs. \n\n\n*The proof lies in the NTK/lazy training regime, which is hard to generalize to non-convex settings such as moderate width or large learning rate.*\n\nThis is true. It must be noted, however, that this is also true for most (all?) of the unconditional theoretical results on deep learning. \n\n*The structure of this paper may need some improvements. The introduction section is a bit too long with perhaps too much background knowledge for normalizing flows. The contribution and related work parts can also be shortened. Section 2 is also a bit too long, and it may be better to re-organize this section and separate it into preliminaries/main results/proof sketch/discussions to make it easier for the readers to understand.*\n\nThank you. We will revisit the structure of the paper. "}, "signatures": ["ICLR.cc/2021/Conference/Paper3249/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3249/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning and Generalization in Univariate Overparameterized Normalizing Flows", "authorids": ["~Kulin_Shah1", "~Amit_Deshpande1", "~Navin_Goyal1"], "authors": ["Kulin Shah", "Amit Deshpande", "Navin Goyal"], "keywords": [], "abstract": "In supervised learning, it is known that overparameterized neural networks with one hidden layer provably and efficiently learn and generalize, when trained using Stochastic Gradient Descent (SGD). In contrast, the benefit of overparameterization in unsupervised learning is not well understood. Normalizing flows (NFs) learn to map complex real-world distributions into simple base distributions, and constitute an important class of models in unsupervised learning for sampling and density estimation. In this paper, we theoretically and empirically analyze these models when the underlying neural network is one hidden layer overparametrized network. On the one hand we provide evidence that for a  class of NFs, overparametrization hurts training. On the other, we prove that another class of NFs, with similar underlying networks can efficiently learn any reasonable data distribution under minimal assumptions. We extend theoretical ideas on learning and generalization from overparameterized neural networks in supervised learning to overparameterized normalizing flows in unsupervised learning. We also provide experimental validation to support our theoretical analysis in practice.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shah|learning_and_generalization_in_univariate_overparameterized_normalizing_flows", "supplementary_material": "/attachment/011444e6383112b1e26c2e123c6e9361068941c5.zip", "pdf": "/pdf/ffcee50f276b53bc7faebe19294a696f331c6616.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=FlSd08LhA3", "_bibtex": "@misc{\nshah2021learning,\ntitle={Learning and Generalization in Univariate Overparameterized Normalizing Flows},\nauthor={Kulin Shah and Amit Deshpande and Navin Goyal},\nyear={2021},\nurl={https://openreview.net/forum?id=3zaVN0M0BIb}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "3zaVN0M0BIb", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3249/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3249/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3249/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3249/Authors|ICLR.cc/2021/Conference/Paper3249/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3249/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923839501, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3249/-/Official_Comment"}}}, {"id": "Jy0ts0Xgpa2", "original": null, "number": 6, "cdate": 1606290732923, "ddate": null, "tcdate": 1606290732923, "tmdate": 1606290732923, "tddate": null, "forum": "3zaVN0M0BIb", "replyto": "nrtPqwNFnlE", "invitation": "ICLR.cc/2021/Conference/Paper3249/-/Official_Comment", "content": {"title": "Response (part 2/2)", "comment": "*In the top right image of Figure 1, I don't see any benefit of large\u00a0$m$\u00a0--- the training curve is too unstable?*\n\nThis image supports our theoretical observation that overparametrization hurts training of CNF models. \n\n*We will only train the $w_r$, $b_r$, and the $a_{r0}$ will remain frozen to their initial value\u00a0What about\u00a0$w_{r0}$\u00a0and\u00a0$b_{r0}$?*\n\n$w_{r0}$\u00a0and\u00a0$b_{r0}$ are the initial random values of the weights (and thus are numerical values and not parameters) and $w_r$ and $b_r$ are the offsets to them and are parameters. Since $a$ is not trained, the offset $a_r$ remains 0 and the parameter remains frozen at $a_{r0}+a_r=a_{r0}$. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3249/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3249/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning and Generalization in Univariate Overparameterized Normalizing Flows", "authorids": ["~Kulin_Shah1", "~Amit_Deshpande1", "~Navin_Goyal1"], "authors": ["Kulin Shah", "Amit Deshpande", "Navin Goyal"], "keywords": [], "abstract": "In supervised learning, it is known that overparameterized neural networks with one hidden layer provably and efficiently learn and generalize, when trained using Stochastic Gradient Descent (SGD). In contrast, the benefit of overparameterization in unsupervised learning is not well understood. Normalizing flows (NFs) learn to map complex real-world distributions into simple base distributions, and constitute an important class of models in unsupervised learning for sampling and density estimation. In this paper, we theoretically and empirically analyze these models when the underlying neural network is one hidden layer overparametrized network. On the one hand we provide evidence that for a  class of NFs, overparametrization hurts training. On the other, we prove that another class of NFs, with similar underlying networks can efficiently learn any reasonable data distribution under minimal assumptions. We extend theoretical ideas on learning and generalization from overparameterized neural networks in supervised learning to overparameterized normalizing flows in unsupervised learning. We also provide experimental validation to support our theoretical analysis in practice.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shah|learning_and_generalization_in_univariate_overparameterized_normalizing_flows", "supplementary_material": "/attachment/011444e6383112b1e26c2e123c6e9361068941c5.zip", "pdf": "/pdf/ffcee50f276b53bc7faebe19294a696f331c6616.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=FlSd08LhA3", "_bibtex": "@misc{\nshah2021learning,\ntitle={Learning and Generalization in Univariate Overparameterized Normalizing Flows},\nauthor={Kulin Shah and Amit Deshpande and Navin Goyal},\nyear={2021},\nurl={https://openreview.net/forum?id=3zaVN0M0BIb}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "3zaVN0M0BIb", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3249/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3249/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3249/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3249/Authors|ICLR.cc/2021/Conference/Paper3249/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3249/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923839501, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3249/-/Official_Comment"}}}, {"id": "jwlsNb7O_Bv", "original": null, "number": 5, "cdate": 1606290167868, "ddate": null, "tcdate": 1606290167868, "tmdate": 1606290167868, "tddate": null, "forum": "3zaVN0M0BIb", "replyto": "nrtPqwNFnlE", "invitation": "ICLR.cc/2021/Conference/Paper3249/-/Official_Comment", "content": {"title": "Response (part 1/2)", "comment": "Thank you for reading our paper and for your thoughtful comments.\n\n*The paper is very difficult to follow because of numerous grammatical issues and lax notations. In particular, parenthetical commas are incorrectly used throughout the paper. The paper should also be reorganized: Theorem 1, which is the main result, appears on Page 7. Please see the comments below for more details.*\n\nThank you for detailed comments about writing. We will incorporate them in our revision where appropriate. \n\n*The results on CNF are unsatisfactory (Section 2.1)*\n\nWe would appreciate more details here. In the paper, we have identified significant barriers for the analysis of CNFs, namely, one would need to analyze moderately-sized neural networks which remains an outstanding open problem. \n\n*I am confused by the term \"constrained normalizing flows (CNFs)\" for\u00a0$a^2,w^2$\u00a0instead of\u00a0$a$\u00a0and\u00a0$w$. After this re-parameterization, the parameters are no longer constrained.*\n\nWhile we understand your concern, we think our terminology is reasonable and descriptive: as explained on page 2, the term Constrained NF refers to the general class of NFs where we model $f(x)$ as a neural network $N(x)$. Since f is strictly monotonically increasing, for this class of models, $N(x)$ is required to have the same property for all possible parameter values. This can be achieved either by re-parametrization or by explicit constraints. The main point here is that we constrain the parameters of a standard neural network in some way to achieve monotonicity (as opposed to UNFs). It is true that after re-parametrization the parameters are no longer constrained but in the original standard network they are.\n\n*As the main result is Theorem 1, UNFs should be discussed earlier and CNFs should be discussed later.*\n\nWe think the current sequence of results is natural: We have three main contributions and Theorem 1 is one of them. While Theorem 1 takes up the bulk of technical work, our observations about CNFs (which are the dominant NF model) naturally lead up to consideration of UNFs and Theorem 1. To our knowledge, there's only one work on UNFs and all the other NFs fall under CNFs which lends extra significance to CNF results. \n\n*Theorem 1 is too informal, and the statement of Theorem 2 should be explained better. The complexity measures $C_1,C_2$ and $C_3$ should be mentioned in theorem statements and discussed in the main text.*\n\nWe haven't included technical details in the main paper such as the complexity measures $C_1,C_2$ and $C_3$  in Theorem 1 because including technical details will  reduce readability of the main paper. Moreover more space would be needed than is available. For $p: R \\to R$, complexity measures $C_1(p),C_2(p)$ and $C_3(p)$ all measure how fast the function $p$ varies. The faster the $p$ varies, the larger the measures.\n  \n\n*Should\u00a0$\\rho$\u00a0be a monotonically strictly increasing function or simply non-decreasing? (See the line after Eq. (4)) If so, why are ReLU networks considered throughout the paper.*\n\n$\\rho$\u00a0needs to be strictly increasing. For UNFs we are considering ReLU networks. For CNFs, we have explained in Sec. 2.1 that ReLU activation cannot be used. \n\n*The notation\u00a0$L(f,x)$\u00a0is over-loaded in different sections*\n\nTheorem 1 statement had a typo: it should have had $L(f_t, x)$ instead of $L(f'_t, x)$. \n\n*```Note that by the initialization,\u00a0$|w_{r0}|$\u00a0and\u00a0$|b_{r0}|$\u00a0are $O(\\sqrt{\\log m / m})$. What is the initialization distribution, and why can we not change the initialization distribution?*\n\nThis was an inadvertent omission. The initialization distribution for $w_{r0}$ is \"half-normal\" distribution with zero-mean and variance=$1/m$ of normal distribution, i.e. $w_{r0}=|X|$ where $X$ has normal distribution with the same parameters. The bias term $b_{r0}$ follows normal distribution with 0 mean and $1/m$ variance. This corresponds to the usual initialization. \nMore generally, for the same initialization as above but variance $\\sigma^2$ at most one, similar approximation issues arise. Details are in Appendix F of the revised version. \n\n\n\n*On page 5, the last line of the first paragraph:\u00a0$L(N_t,x_t)$\u00a0is defined as the squared loss, but it was defined earlier in Eq. (2).*\n\nThe first paragraph of page 5, which had $L(N_t, x_t)$ for square loss indeed had bad overloading. We have changed that notation to $L_S(N_t, x_t)$ to denote the square loss.\n\n\n*Please provide more details for experiments in Section 3. What were the base distribution, target distribution, and training set size? Since 1D distributions are easy to visualize, how does the estimated distribution compare with the target distribution?*\n\nMost of the details were mentioned in the Appendix E in the older version. For the UNF experiments, we used exponential base distribution and for CNF experiments, we used Gaussian base distribution. "}, "signatures": ["ICLR.cc/2021/Conference/Paper3249/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3249/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning and Generalization in Univariate Overparameterized Normalizing Flows", "authorids": ["~Kulin_Shah1", "~Amit_Deshpande1", "~Navin_Goyal1"], "authors": ["Kulin Shah", "Amit Deshpande", "Navin Goyal"], "keywords": [], "abstract": "In supervised learning, it is known that overparameterized neural networks with one hidden layer provably and efficiently learn and generalize, when trained using Stochastic Gradient Descent (SGD). In contrast, the benefit of overparameterization in unsupervised learning is not well understood. Normalizing flows (NFs) learn to map complex real-world distributions into simple base distributions, and constitute an important class of models in unsupervised learning for sampling and density estimation. In this paper, we theoretically and empirically analyze these models when the underlying neural network is one hidden layer overparametrized network. On the one hand we provide evidence that for a  class of NFs, overparametrization hurts training. On the other, we prove that another class of NFs, with similar underlying networks can efficiently learn any reasonable data distribution under minimal assumptions. We extend theoretical ideas on learning and generalization from overparameterized neural networks in supervised learning to overparameterized normalizing flows in unsupervised learning. We also provide experimental validation to support our theoretical analysis in practice.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shah|learning_and_generalization_in_univariate_overparameterized_normalizing_flows", "supplementary_material": "/attachment/011444e6383112b1e26c2e123c6e9361068941c5.zip", "pdf": "/pdf/ffcee50f276b53bc7faebe19294a696f331c6616.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=FlSd08LhA3", "_bibtex": "@misc{\nshah2021learning,\ntitle={Learning and Generalization in Univariate Overparameterized Normalizing Flows},\nauthor={Kulin Shah and Amit Deshpande and Navin Goyal},\nyear={2021},\nurl={https://openreview.net/forum?id=3zaVN0M0BIb}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "3zaVN0M0BIb", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3249/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3249/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3249/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3249/Authors|ICLR.cc/2021/Conference/Paper3249/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3249/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923839501, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3249/-/Official_Comment"}}}, {"id": "xuxKAfhQSi8", "original": null, "number": 2, "cdate": 1606285100139, "ddate": null, "tcdate": 1606285100139, "tmdate": 1606285675083, "tddate": null, "forum": "3zaVN0M0BIb", "replyto": "3zaVN0M0BIb", "invitation": "ICLR.cc/2021/Conference/Paper3249/-/Official_Comment", "content": {"title": "Common Response", "comment": "[We have updated our submission with some minor notational and other improvements.]\n\nWe thank all the reviewers for their comments. One common concern was that the paper focuses on the univariate case which may be too limited compared to the high-dimensional settings which is usually what one is interested in practice. That the applications of NFs generally involve high-dimensional probability distributions is no doubt correct. Then why study the univariate case? Several reasons:\n\n\u2022 For a hard problem, one often needs to first find simpler but illuminating cases that one can solve. Within deep learning theory, there have been multiple recent works on univariate neural networks even in the supervised settings, some of them cited in the paper. \n\n\u2022 The univariate case is of special interest for normalizing flows even when one is concerned only with the high dimensional distributions. (This is not the case with other generative models like GANs.) \nIn the multidimensional case, the flow maps $(x_1, \u2026, x_d)$ to $(z_1, \u2026, z_d)$. More specifically, one has d neural nets $N_1, \u2026, N_d$ with $N_i$ mapping $(x_1, \u2026, x_i)$ to $z_i$. Neural net $N_1$ is a monotone univariate neural net mapping $x_1$ to $z_1$. For $i>1$, for any fixed $(x_1, \u2026, x_i)$, the network $N_i$ is a univariate monotone network mapping $x_i$ to $z_i$.  \n\n\u2022 Our work on the univariate NFs identifies specific barriers for proving results for NFs modeling higher dimensional probability distributions. This was briefly mentioned at the end of the conclusion; we explain it in a little more detail below: \n\nFirst, let us note that despite extensive work, even in the supervised setting the only case that's reasonably understood is when the neural network has one hidden layer and is very wide (also referred to as the NTK regime). Thus, to our knowledge, the following problems remain open (even in 1D): \n(1) Analysis of training and generalization of moderately sized neural networks with one hidden layer (non-NTK regime). \n(2) Analysis of neural networks with more than one hidden layer. This remains open for both moderately-wide as well as very wide networks. \n\nAn analysis of NFs for non-univariate (i.e., $d>1$) distributions will have to solve one of the above two problems:\n\t\u2022 For CNFs, as explained above, the neural net taking $x_1$ to $z_1$ is a univariate monotone neural net. We have shown in the paper that for univariate CNFs, there's strong evidence that one must work with moderately sized neural nets. Thus, one needs to solve problem (1) above to analyze CNFs. \n\t\u2022 To our knowledge, for UNFs there is no construction of the corresponding neural net with only one hidden layer if $d>1$. Thus, one needs to make progress on problem (2) above to analyze UNFs for $d>1$.\n\nA different difficulty in the high-dimensional NTK regime for supervised learning with smooth activations like tanh and sigmoid is that the linear span of the dataset has small dimension then training can become extremely slow (https://arxiv.org/abs/1908.05660). This does not happen for activations like ReLU. As we have shown, the activations for neural net underlying CNFs must satisfy certain admissibility conditions and the only standard activations we know to be admissible are tanh and sigmoid. Thus one cannot expect a general theorem for such activations and one must work with a non-standard activation. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3249/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3249/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning and Generalization in Univariate Overparameterized Normalizing Flows", "authorids": ["~Kulin_Shah1", "~Amit_Deshpande1", "~Navin_Goyal1"], "authors": ["Kulin Shah", "Amit Deshpande", "Navin Goyal"], "keywords": [], "abstract": "In supervised learning, it is known that overparameterized neural networks with one hidden layer provably and efficiently learn and generalize, when trained using Stochastic Gradient Descent (SGD). In contrast, the benefit of overparameterization in unsupervised learning is not well understood. Normalizing flows (NFs) learn to map complex real-world distributions into simple base distributions, and constitute an important class of models in unsupervised learning for sampling and density estimation. In this paper, we theoretically and empirically analyze these models when the underlying neural network is one hidden layer overparametrized network. On the one hand we provide evidence that for a  class of NFs, overparametrization hurts training. On the other, we prove that another class of NFs, with similar underlying networks can efficiently learn any reasonable data distribution under minimal assumptions. We extend theoretical ideas on learning and generalization from overparameterized neural networks in supervised learning to overparameterized normalizing flows in unsupervised learning. We also provide experimental validation to support our theoretical analysis in practice.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shah|learning_and_generalization_in_univariate_overparameterized_normalizing_flows", "supplementary_material": "/attachment/011444e6383112b1e26c2e123c6e9361068941c5.zip", "pdf": "/pdf/ffcee50f276b53bc7faebe19294a696f331c6616.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=FlSd08LhA3", "_bibtex": "@misc{\nshah2021learning,\ntitle={Learning and Generalization in Univariate Overparameterized Normalizing Flows},\nauthor={Kulin Shah and Amit Deshpande and Navin Goyal},\nyear={2021},\nurl={https://openreview.net/forum?id=3zaVN0M0BIb}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "3zaVN0M0BIb", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3249/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3249/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3249/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3249/Authors|ICLR.cc/2021/Conference/Paper3249/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3249/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923839501, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3249/-/Official_Comment"}}}, {"id": "taIB6QWOT6E", "original": null, "number": 3, "cdate": 1606285608313, "ddate": null, "tcdate": 1606285608313, "tmdate": 1606285608313, "tddate": null, "forum": "3zaVN0M0BIb", "replyto": "2K-q-chykAZ", "invitation": "ICLR.cc/2021/Conference/Paper3249/-/Official_Comment", "content": {"title": "Response", "comment": "Thank you for reading our paper and for your thoughtful comments.\n\n*However, I am not sure about the impacts of the contribution to the general multivariate/high dimensional setting.*\n\nWe agree that the case of multivariate NFs is indeed what we ultimately want to solve. Please see our common response for why we believe the univariate result is interesting and what it says about the multivariate case. \n\n*+) The first result on the failure of PGD/quadratic parameterization in the constrained case is interesting, theoretically. But I wonder if there is any artifact in the proof framework using pseudo networks or linear approximation.*\n\nThis is not an artifact of the proof technique: our results in Fig. 1 show that experiments bear out our theoretical observations about CNFs. \n\n*+) Would you see the same observation in Figure 1 with more number of epochs and other activations, says ReLU.*\n\nMore epochs: We experimented with 1000 epochs and large change in the parameters is required in all cases. New experiments are included in the appendix. \nOther activations: Among the standard activations tanh and sigmoid are the only ones that we know to be admissible \n(i.e. non-convex functions with continuous derivative).  We have experimented with one more admissible activation that we designed. The activation is $(e^x - 1) I(x  \\leq 0 ) + (1 - e^{-x}) I(x > 0)$. It gives results similar to tanh.\n\n*Please clarify \u201cGradient-based optimization algorithms are not applicable to problems with discontinuous objectives\u201d around the end of page 5.*\n\nThis statement means that if the objective function is discontinuous then gradient-based methods such as SGD are not applicable because the function may not have (sub-)gradient at some points and the function may fail to have small Lipschitz constant (the function value can change by a large amount for a small change in the argument value) around the points of discontinuity. Gradient-based methods can fail on functions with large Lipschitz constant. In particular, SGD on discontinuous functions may not converge to a stationary point. As remarked in the paper, this is not just a theoretical issue and we observed it in our experiment with CNF models with ReLU activation.\n\n*+) What are the difficulties of the Gaussian base distribution?*\n\nAs pointed out in Remark 1 in Appendix C in the original submission, the main problem in using Gaussian base distribution is that it is not clear whether the loss of normalizing flow using pseudo network will remain convex throughout the training. Moreover, it is also not clear that Lipschitz constant of the loss function with the Gaussian base distribution will remain bounded by an absolute constant and hence independent of the complexity of the target function, $\\epsilon$, etc. throughout the training. Lack of such boundedness property brings up additional challenges for the current analysis. (This last point is also affects the analysis of Allen-Zhu et al. where it was assumed that the loss function was 1-Lipschitz. But this assumption does not seem to be verified in their analysis.)\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3249/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3249/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning and Generalization in Univariate Overparameterized Normalizing Flows", "authorids": ["~Kulin_Shah1", "~Amit_Deshpande1", "~Navin_Goyal1"], "authors": ["Kulin Shah", "Amit Deshpande", "Navin Goyal"], "keywords": [], "abstract": "In supervised learning, it is known that overparameterized neural networks with one hidden layer provably and efficiently learn and generalize, when trained using Stochastic Gradient Descent (SGD). In contrast, the benefit of overparameterization in unsupervised learning is not well understood. Normalizing flows (NFs) learn to map complex real-world distributions into simple base distributions, and constitute an important class of models in unsupervised learning for sampling and density estimation. In this paper, we theoretically and empirically analyze these models when the underlying neural network is one hidden layer overparametrized network. On the one hand we provide evidence that for a  class of NFs, overparametrization hurts training. On the other, we prove that another class of NFs, with similar underlying networks can efficiently learn any reasonable data distribution under minimal assumptions. We extend theoretical ideas on learning and generalization from overparameterized neural networks in supervised learning to overparameterized normalizing flows in unsupervised learning. We also provide experimental validation to support our theoretical analysis in practice.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shah|learning_and_generalization_in_univariate_overparameterized_normalizing_flows", "supplementary_material": "/attachment/011444e6383112b1e26c2e123c6e9361068941c5.zip", "pdf": "/pdf/ffcee50f276b53bc7faebe19294a696f331c6616.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=FlSd08LhA3", "_bibtex": "@misc{\nshah2021learning,\ntitle={Learning and Generalization in Univariate Overparameterized Normalizing Flows},\nauthor={Kulin Shah and Amit Deshpande and Navin Goyal},\nyear={2021},\nurl={https://openreview.net/forum?id=3zaVN0M0BIb}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "3zaVN0M0BIb", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3249/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3249/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3249/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3249/Authors|ICLR.cc/2021/Conference/Paper3249/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3249/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923839501, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3249/-/Official_Comment"}}}, {"id": "6UvrYpaoWWq", "original": null, "number": 2, "cdate": 1603853433836, "ddate": null, "tcdate": 1603853433836, "tmdate": 1605024037367, "tddate": null, "forum": "3zaVN0M0BIb", "replyto": "3zaVN0M0BIb", "invitation": "ICLR.cc/2021/Conference/Paper3249/-/Official_Review", "content": {"title": "Interesting but somewhat limited problem setting with lack of novelty in proof techniques", "review": "Summary:\n\nThis paper proved that for a certain modified version of sufficiently-overparametrized univariate normalizing flows where the underlying neural network has only one hidden layer, with high probability it can learn a distribution that is close enough to the target distribution where the distance can be measured in, e.g., KL divergence. The width of the network, number of samples, and the number of quadrature points are required to be at least polynomial in inverse of error rate and complexity measure of the target distribution. The authors also provided theoretical evidence and did experiments on synthetic Gaussian mixture datasets to show that another variation of the normalizing flow model does not benefit from overparametrization under this one-hidden-layer univariate setting.\n\nPros:\n\n1. Understanding why normalizing flow works and the learning process of the underlying neural networks is an important problem, and the idea of using overparametrization to explain this is interesting.\n\n2. The experimental methodologies and theoretical computations appear to be correct.\n\n3. The intuitions behind the problem setting (including the modifications to the algorithm) and the ideas behind the proof are explained in detail and easy to understand. The limitations of this paper are also discussed.\n\nCons:\n\n1. This particular setting of normalizing flow models used in this paper might be a bit limited. The authors only analyzed the univariate case, which is far from the high-dimensional case in practice. It is possible that these two cases work in very different regimes due to the differences between high-dimensional and low-dimensional probabilities. The authors also made two important modifications to the unconstrained normalizing flow: changing the base distribution to standard exponential distribution and changing the quadrature to simple rectangle quadrature. These modifications can also make the model work in very different ways from practice, and the authors did not provide enough theoretical or experimental justifications for the modifications.\n\n2. The techniques used in this paper mainly come from [1], and the proof framework and results are roughly the same. The authors made modifications to the setting so that the optimization becomes convex, and this seems to be the only justification for these modifications. The proof lies in the NTK/lazy training regime, which is hard to generalize to non-convex settings such as moderate width or large learning rate.\n\n3. The structure of this paper may need some improvements. The introduction section is a bit too long with perhaps too much background knowledge for normalizing flows. The contribution and related work parts can also be shortened. Section 2 is also a bit too long, and it may be better to re-organize this section and separate it into preliminaries/main results/proof sketch/discussions to make it easier for the readers to understand.\n\n4. The experiments in this paper are a bit simple, i.e., the authors only did experiments on synthetic datasets like Gaussian mixture with models whose underlying neural networks have only one hidden layer. This setting is far from empirical settings, which makes the conclusions of the experiments not so convincing.\n\n[1] Allen-Zhu, Zeyuan, Yuanzhi Li, and Yingyu Liang. \"Learning and generalization in overparameterized neural networks, going beyond two layers.\" Advances in neural information processing systems. 2019.\n\nRecommendation:\n\nI vote for rejecting this paper. As mentioned in \"Cons\", my major concern is the limitations of the problem setting in this paper and the novelty of the proof. The setting for normalizing flows in this paper is a bit far from practice, and the theoretical proof highly depends on the convexity of the optimization process, making the theoretical claims hard to generalize to more practical settings.\n\nSupporting arguments for recommendation:\n\nSee \"Cons\", especially points 1 and 2 there.\n\nQuestions for the authors:\n\n1. Please address the cons mentioned above.\n\n2. The authors use \"generalization\" in the title, but I am a bit confused because I do not know what generalization means in this normalizing flow setting. Does this mean something like the KL divergence between the learned distribution and the target distribution?\n\n3. In the last paragraph of page 5, the authors said that convex activations \"cannot be used\" because then N(x) would also be convex. Does N(x) have to be non-convex?\n\nAdditional feedback:\n\n1. It may be better to explain \"quadrature\" by figures or examples in the introduction because this seems to be an important difference between normalizing flow models and normal neural networks but it is not explained in detail in the introduction. Explaining this earlier and clearer can help the readers better understand this paper.\n\n2. Typos: In the abstract, \"On the other\" -> \"On the other hand\". In the paragraph just before Section 3, \"Lemmas 1\" -> \"Lemma 1\".", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3249/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3249/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning and Generalization in Univariate Overparameterized Normalizing Flows", "authorids": ["~Kulin_Shah1", "~Amit_Deshpande1", "~Navin_Goyal1"], "authors": ["Kulin Shah", "Amit Deshpande", "Navin Goyal"], "keywords": [], "abstract": "In supervised learning, it is known that overparameterized neural networks with one hidden layer provably and efficiently learn and generalize, when trained using Stochastic Gradient Descent (SGD). In contrast, the benefit of overparameterization in unsupervised learning is not well understood. Normalizing flows (NFs) learn to map complex real-world distributions into simple base distributions, and constitute an important class of models in unsupervised learning for sampling and density estimation. In this paper, we theoretically and empirically analyze these models when the underlying neural network is one hidden layer overparametrized network. On the one hand we provide evidence that for a  class of NFs, overparametrization hurts training. On the other, we prove that another class of NFs, with similar underlying networks can efficiently learn any reasonable data distribution under minimal assumptions. We extend theoretical ideas on learning and generalization from overparameterized neural networks in supervised learning to overparameterized normalizing flows in unsupervised learning. We also provide experimental validation to support our theoretical analysis in practice.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shah|learning_and_generalization_in_univariate_overparameterized_normalizing_flows", "supplementary_material": "/attachment/011444e6383112b1e26c2e123c6e9361068941c5.zip", "pdf": "/pdf/ffcee50f276b53bc7faebe19294a696f331c6616.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=FlSd08LhA3", "_bibtex": "@misc{\nshah2021learning,\ntitle={Learning and Generalization in Univariate Overparameterized Normalizing Flows},\nauthor={Kulin Shah and Amit Deshpande and Navin Goyal},\nyear={2021},\nurl={https://openreview.net/forum?id=3zaVN0M0BIb}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "3zaVN0M0BIb", "replyto": "3zaVN0M0BIb", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3249/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538079259, "tmdate": 1606915759032, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3249/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3249/-/Official_Review"}}}, {"id": "zF84QKnepqX", "original": null, "number": 1, "cdate": 1603830237313, "ddate": null, "tcdate": 1603830237313, "tmdate": 1605024037292, "tddate": null, "forum": "3zaVN0M0BIb", "replyto": "3zaVN0M0BIb", "invitation": "ICLR.cc/2021/Conference/Paper3249/-/Official_Review", "content": {"title": "Review for `'Learning and Generalization in Univariate Overparameterized Normalizing Flows'", "review": "This paper studies overparameterization over unsupervised learning. In detail, it uses constrained normalizing flows (CNF) and unconstrained normalizing flows (UNF) to learn the underlying unknown one-dimensional distribution, which can be parameterized by a two-layer neural network. The authors propose theoretical results for UNF and suggest that by selecting wide enough neural networks, a great number of random samples and number of quadrature points, a two-layer neural network is able to learn the true UNF up to small error. Experiment results are presented for both CNF and UNF, which back up their claim. \n \nHere are my detailed comments. \n\n\n\n\n\n\n\n- The presentation of theoretical results can be further improved. For instance:\n-- In Lemma 1, what does $(\\phi^{-1}(F\u2019)){\\|\\delta}$ mean?\n-- In Theorem 2, it seems that to derive a finite-sample analysis, the second-order derivative of $F^*$ should be finite. The authors may want to add such a claim in the statement of Theorem 1.\n-- What is the definition of  $\\tilde w_i$ in Lemma 3? \n-- What are the definitions of $M_{\\hat L}$ and $m_{\\hat L}$ in Lemma 12? Are they related to $n$?\n-- Second line in Page 24, eq.() is a typo. \n- My main concern is the scalability issue. The main theorem suggests that it is possible to use a neural network to approximate the first-order derivative of the unknown distribution transformation $f$, and to use the neural network to construct the original function $f$ with sufficient quadrature points. However, just as Theorem 1 suggests, the number of quadrature points is of the order $O(1/\\epsilon)$, where $\\epsilon$ is the approximation error. Thus, it seems that for a $d$-dimension case, the number of quadrature points may be the order of $O(1/\\epsilon^d)$. Such an exponential dependence is unacceptable in terms of the scalability. Can the authors explain more about the high-dimension case? \n- In Section 2.1, the authors suggest that overparameterization may hurt the overall performance of CNF by showing experiment results of tanh activation function. Is it true that the failure is actually due to the gradient explosion caused by tanh activation function rather than overparameterization? Meanwhile, it seems that the reason for the authors to use tanh is because they want activations with continuous derivatives and convexity for loss function. Why do we need such a convexity? Is it due to some theoretical concerns (like to make the derivation go through) or concerns from practice?\n- The authors may want to discuss existing results about optimization and generalization of overparameterized deep neural networks [1-3], which are related to this work. Besides, this work relies on the idea of the existence of a pseudo network which approximates the target function well, which may be related to [4-6]. Can the authors discuss and show the relations between these works?\n\n[1] Zou, Difan, et al. \"Gradient descent optimizes over-parameterized deep ReLU networks.\" Machine Learning 109.3 (2020): 467-492.\n\n[2] Du, Simon, et al. \"Gradient descent finds global minima of deep neural networks.\" International Conference on Machine Learning. 2019.\n\n[3] Allen-Zhu, Zeyuan, Yuanzhi Li, and Zhao Song. \"A convergence theory for deep learning via over-parameterization.\" International Conference on Machine Learning. PMLR, 2019.\n\n[4] Allen-Zhu, Zeyuan, and Yuanzhi Li. \"What Can ResNet Learn Efficiently, Going Beyond Kernels?.\" Advances in Neural Information Processing Systems. 2019.\n\n[5] Chen, Zixiang, et al. \"How Much Over-parameterization Is Sufficient to Learn Deep ReLU Networks?.\" arXiv preprint arXiv:1911.12360 (2019).\n\n[6] Ji, Ziwei, and Matus Telgarsky. \"Polylogarithmic width suffices for gradient descent to achieve arbitrarily small test error with shallow relu networks.\" arXiv preprint arXiv:1909.12292 (2019).\n", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3249/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3249/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning and Generalization in Univariate Overparameterized Normalizing Flows", "authorids": ["~Kulin_Shah1", "~Amit_Deshpande1", "~Navin_Goyal1"], "authors": ["Kulin Shah", "Amit Deshpande", "Navin Goyal"], "keywords": [], "abstract": "In supervised learning, it is known that overparameterized neural networks with one hidden layer provably and efficiently learn and generalize, when trained using Stochastic Gradient Descent (SGD). In contrast, the benefit of overparameterization in unsupervised learning is not well understood. Normalizing flows (NFs) learn to map complex real-world distributions into simple base distributions, and constitute an important class of models in unsupervised learning for sampling and density estimation. In this paper, we theoretically and empirically analyze these models when the underlying neural network is one hidden layer overparametrized network. On the one hand we provide evidence that for a  class of NFs, overparametrization hurts training. On the other, we prove that another class of NFs, with similar underlying networks can efficiently learn any reasonable data distribution under minimal assumptions. We extend theoretical ideas on learning and generalization from overparameterized neural networks in supervised learning to overparameterized normalizing flows in unsupervised learning. We also provide experimental validation to support our theoretical analysis in practice.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shah|learning_and_generalization_in_univariate_overparameterized_normalizing_flows", "supplementary_material": "/attachment/011444e6383112b1e26c2e123c6e9361068941c5.zip", "pdf": "/pdf/ffcee50f276b53bc7faebe19294a696f331c6616.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=FlSd08LhA3", "_bibtex": "@misc{\nshah2021learning,\ntitle={Learning and Generalization in Univariate Overparameterized Normalizing Flows},\nauthor={Kulin Shah and Amit Deshpande and Navin Goyal},\nyear={2021},\nurl={https://openreview.net/forum?id=3zaVN0M0BIb}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "3zaVN0M0BIb", "replyto": "3zaVN0M0BIb", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3249/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538079259, "tmdate": 1606915759032, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3249/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3249/-/Official_Review"}}}, {"id": "nrtPqwNFnlE", "original": null, "number": 3, "cdate": 1603911285432, "ddate": null, "tcdate": 1603911285432, "tmdate": 1605024037225, "tddate": null, "forum": "3zaVN0M0BIb", "replyto": "3zaVN0M0BIb", "invitation": "ICLR.cc/2021/Conference/Paper3249/-/Official_Review", "content": {"title": "Interesting model but the paper is confusing", "review": "**Summary**\n\nThe paper studies the problem of learning univariate normalizing flows with single-layer neural networks. The paper studies two models of normalizing flows: constrained normalizing flows (CNFs) and unconstrained normalizing flows(UNFs). For UNFs, the paper gives finite-sample results for UNFs in Theorem 1.\n\n**Positives**\n\nThe paper studies two models of using neural networks for learning normalizing flows. For CNFs, paper identifies issues with the Taylor expansion in the parameter space. For UNFs, Theorem 1 shows that running SGD with a suitable learning rate leads to a neural network with small error. A theoretical study of normalizing flows looks like a promising research direction.\n\n\n**Negatives**\n\nThe paper is very difficult to follow because of numerous grammatical issues and lax notations. In particular, parenthetical commas are incorrectly used throughout the paper.The paper should also be reorganized: Theorem 1, which is the main result, appears on Page 7. Please see the comments below for more details.\n\n**Score**\n\nI recommend rejection of this paper. The paper is not well-written and difficult to follow. The results on CNF are unsatisfactory (Section 2.1) and it is difficult to parse  the results in UNFs (Theorem 1).  The paper should go through major revisions for clarity. Please see the comments below for more details.\n\n**Major comments**\n\n1. I am confused by the term \"constrained normalizing flows (CNFs)\" for $a^2, w^2$ instead of $a$ and $w$. After this re-parameterization, the parameters are no longer constrained. \n1. As the main result is Theorem 1, UNFs should be discussed earlier and CNFs should be discussed later. \n3. Theorem 1 is too informal, and the statement of Theorem 2 should be explained better. The complexity measures $C_1, C_2,$ and  $C_3$ should be mentioned in theorem statements and discussed in the main text.\n1. Should $\\rho$ be a monotonically strictly increasing function or simply non-decreasing? (See the line after Eq. (4)) If so, why are ReLU networks considered throughout the paper. The first line in Section 2.1 should also be clarified.\n2. The notation $L(f,x)$ is over-loaded in different sections: sometimes it is used with $f$ and sometimes with $f_t'$. This is extremely confusing.\n4. ** Note that by the initialization, $|w_{r0}|$ and $|b_{r0}$ are O(\\sqrt{\\log m / m})**\nWhat is the initialization distribution, and why can we not change the initialization distribution?\n5. On page 5, the last line of the first paragraph: $L(N_t,x_t)$ is defined as the squared loss, but it was defined earlier in Eq. (2).\n6.Please provide more details for experiments in Section 3. What were the base distribution, target distribution, and training set size? Since 1D distributions are easy to visualize, how does the estimated distribution compare with the target distribution?  \n7. In the top right image of Figure 1, I don't see any benefit of large $m$ --- the training curve is too unstable?\n\n\n**Minor comments**\n\n\n1. *Recent work in supervised learning attempts to provide theoretical justification for\nwhy overparameterized neural networks can train and generalize efficiently in the above sense* Add a citation.\n2. *We will only train the wr, br, and the ar0 will remain frozen to their initial value*\nWhat about $w_{r0}$ and $b_{r0}$?\n3. Some terms are defined but they are not used ever again, for example, $L_G$ for the Gaussian distribution.\n\n\n\n\n\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3249/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3249/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning and Generalization in Univariate Overparameterized Normalizing Flows", "authorids": ["~Kulin_Shah1", "~Amit_Deshpande1", "~Navin_Goyal1"], "authors": ["Kulin Shah", "Amit Deshpande", "Navin Goyal"], "keywords": [], "abstract": "In supervised learning, it is known that overparameterized neural networks with one hidden layer provably and efficiently learn and generalize, when trained using Stochastic Gradient Descent (SGD). In contrast, the benefit of overparameterization in unsupervised learning is not well understood. Normalizing flows (NFs) learn to map complex real-world distributions into simple base distributions, and constitute an important class of models in unsupervised learning for sampling and density estimation. In this paper, we theoretically and empirically analyze these models when the underlying neural network is one hidden layer overparametrized network. On the one hand we provide evidence that for a  class of NFs, overparametrization hurts training. On the other, we prove that another class of NFs, with similar underlying networks can efficiently learn any reasonable data distribution under minimal assumptions. We extend theoretical ideas on learning and generalization from overparameterized neural networks in supervised learning to overparameterized normalizing flows in unsupervised learning. We also provide experimental validation to support our theoretical analysis in practice.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shah|learning_and_generalization_in_univariate_overparameterized_normalizing_flows", "supplementary_material": "/attachment/011444e6383112b1e26c2e123c6e9361068941c5.zip", "pdf": "/pdf/ffcee50f276b53bc7faebe19294a696f331c6616.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=FlSd08LhA3", "_bibtex": "@misc{\nshah2021learning,\ntitle={Learning and Generalization in Univariate Overparameterized Normalizing Flows},\nauthor={Kulin Shah and Amit Deshpande and Navin Goyal},\nyear={2021},\nurl={https://openreview.net/forum?id=3zaVN0M0BIb}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "3zaVN0M0BIb", "replyto": "3zaVN0M0BIb", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3249/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538079259, "tmdate": 1606915759032, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3249/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3249/-/Official_Review"}}}, {"id": "2K-q-chykAZ", "original": null, "number": 4, "cdate": 1603996857310, "ddate": null, "tcdate": 1603996857310, "tmdate": 1605024037144, "tddate": null, "forum": "3zaVN0M0BIb", "replyto": "3zaVN0M0BIb", "invitation": "ICLR.cc/2021/Conference/Paper3249/-/Official_Review", "content": {"title": "Official Review", "review": "The paper studies the role of overparameterization in learning normalizing flow models. More specifically, the authors analyze the optimization and generalization of such a model when the transport map f is parameterized by a two-layer neural network with potentially many hidden units (or highly over-parameterized). Importantly, the focus is on univariate data distributions.\n\nFirst, the authors argue that overparameterization hurts the learning of constrained normalizing flows (CNFs) that impose positivity of weights though either projected gradient descent (PGD) or quadratic parameterization. Second, the authors prove that unconstrained NFs (UNFs) by modeling the gradient function f\u2019 rather than f itself can learn the data distribution.\n\nI definitely think this work makes some interesting contributions in terms of provable results for learning over-parameterized NFs. This is given by the fact that the problem is less well-understood compared to supervised learning. However, I am not sure about the impacts of the contribution to the general multivariate/high dimensional setting. Also, I have some other questions:\n\n+) The first result on the failure of PGD/quadratic parameterization in the constrained case is interesting, theoretically. But I wonder if there is any artifact in the proof framework using pseudo networks or linear approximation.\n\n+) Would you see the same observation in Figure 1 with more number of epochs and other activations, says ReLU. Please clarify \u201cGradient-based optimization algorithms are not applicable to problems with discontinuous objectives\u201d around the end of page 5.\n\n+) What are the difficulties of the Gaussian base distribution?\n", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3249/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3249/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning and Generalization in Univariate Overparameterized Normalizing Flows", "authorids": ["~Kulin_Shah1", "~Amit_Deshpande1", "~Navin_Goyal1"], "authors": ["Kulin Shah", "Amit Deshpande", "Navin Goyal"], "keywords": [], "abstract": "In supervised learning, it is known that overparameterized neural networks with one hidden layer provably and efficiently learn and generalize, when trained using Stochastic Gradient Descent (SGD). In contrast, the benefit of overparameterization in unsupervised learning is not well understood. Normalizing flows (NFs) learn to map complex real-world distributions into simple base distributions, and constitute an important class of models in unsupervised learning for sampling and density estimation. In this paper, we theoretically and empirically analyze these models when the underlying neural network is one hidden layer overparametrized network. On the one hand we provide evidence that for a  class of NFs, overparametrization hurts training. On the other, we prove that another class of NFs, with similar underlying networks can efficiently learn any reasonable data distribution under minimal assumptions. We extend theoretical ideas on learning and generalization from overparameterized neural networks in supervised learning to overparameterized normalizing flows in unsupervised learning. We also provide experimental validation to support our theoretical analysis in practice.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shah|learning_and_generalization_in_univariate_overparameterized_normalizing_flows", "supplementary_material": "/attachment/011444e6383112b1e26c2e123c6e9361068941c5.zip", "pdf": "/pdf/ffcee50f276b53bc7faebe19294a696f331c6616.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=FlSd08LhA3", "_bibtex": "@misc{\nshah2021learning,\ntitle={Learning and Generalization in Univariate Overparameterized Normalizing Flows},\nauthor={Kulin Shah and Amit Deshpande and Navin Goyal},\nyear={2021},\nurl={https://openreview.net/forum?id=3zaVN0M0BIb}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "3zaVN0M0BIb", "replyto": "3zaVN0M0BIb", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3249/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538079259, "tmdate": 1606915759032, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3249/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3249/-/Official_Review"}}}], "count": 14}