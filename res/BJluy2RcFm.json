{"notes": [{"id": "Hyx6U8XMIE", "original": null, "number": 19, "cdate": 1551148628683, "ddate": null, "tcdate": 1551148628683, "tmdate": 1551148628683, "tddate": null, "forum": "BJluy2RcFm", "replyto": "B1lSYgAGeN", "invitation": "ICLR.cc/2019/Conference/-/Paper1000/Official_Comment", "content": {"title": "Response to Metareview", "comment": "We would like to thank the reviewers.  We have updated our paper to clarify a few points and address the few remaining reviewer concerns not addressed during the rebuttal phase. In particular, this version adds further refinements to: the verifiability of pi-SGD conditions, capturing higher-order dependencies with k-ary Janossy, and the applicability of Janossy pooling to multisets."}, "signatures": ["ICLR.cc/2019/Conference/Paper1000/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1000/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1000/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Janossy Pooling: Learning Deep Permutation-Invariant Functions for Variable-Size Inputs", "abstract": "We consider a simple and overarching representation for permutation-invariant functions of sequences (or set functions). Our approach, which we call Janossy pooling, expresses a permutation-invariant function as the average of a permutation-sensitive function applied to all reorderings of the input sequence. This allows us to leverage the rich and mature literature on permutation-sensitive functions to construct novel and flexible permutation-invariant functions. If carried out naively, Janossy pooling can be computationally prohibitive. To allow computational tractability, we consider three kinds of approximations: canonical orderings of sequences, functions with k-order interactions, and stochastic optimization algorithms with random permutations. Our framework unifies a variety of existing work in the literature, and suggests possible modeling and algorithmic extensions. We explore a few in our experiments, which demonstrate improved performance over current state-of-the-art methods.", "keywords": ["representation learning", "permutation invariance", "set functions", "feature pooling"], "authorids": ["murph213@purdue.edu", "bsriniv@purdue.edu", "varao@purdue.edu", "ribeiro@cs.purdue.edu"], "authors": ["Ryan L. Murphy", "Balasubramaniam Srinivasan", "Vinayak Rao", "Bruno Ribeiro"], "TL;DR": "We propose Janossy pooling, a method for learning deep permutation invariant functions designed to exploit relationships within the input sequence and tractable inference strategies such as a stochastic optimization procedure we call piSGD", "pdf": "/pdf/245bbcd015272f347e3977e5212fc17c084622c9.pdf", "paperhash": "murphy|janossy_pooling_learning_deep_permutationinvariant_functions_for_variablesize_inputs", "_bibtex": "@inproceedings{\nmurphy2018janossy,\ntitle={Janossy Pooling: Learning Deep Permutation-Invariant Functions for Variable-Size Inputs},\nauthor={Ryan L. Murphy and Balasubramaniam Srinivasan and Vinayak Rao and Bruno Ribeiro},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BJluy2RcFm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1000/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621613982, "tddate": null, "super": null, "final": null, "reply": {"forum": "BJluy2RcFm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1000/Authors", "ICLR.cc/2019/Conference/Paper1000/Reviewers", "ICLR.cc/2019/Conference/Paper1000/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1000/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1000/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1000/Authors|ICLR.cc/2019/Conference/Paper1000/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1000/Reviewers", "ICLR.cc/2019/Conference/Paper1000/Authors", "ICLR.cc/2019/Conference/Paper1000/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621613982}}}, {"id": "BJluy2RcFm", "original": "rJl2fBp5F7", "number": 1000, "cdate": 1538087904322, "ddate": null, "tcdate": 1538087904322, "tmdate": 1550870109076, "tddate": null, "forum": "BJluy2RcFm", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Janossy Pooling: Learning Deep Permutation-Invariant Functions for Variable-Size Inputs", "abstract": "We consider a simple and overarching representation for permutation-invariant functions of sequences (or set functions). Our approach, which we call Janossy pooling, expresses a permutation-invariant function as the average of a permutation-sensitive function applied to all reorderings of the input sequence. This allows us to leverage the rich and mature literature on permutation-sensitive functions to construct novel and flexible permutation-invariant functions. If carried out naively, Janossy pooling can be computationally prohibitive. To allow computational tractability, we consider three kinds of approximations: canonical orderings of sequences, functions with k-order interactions, and stochastic optimization algorithms with random permutations. Our framework unifies a variety of existing work in the literature, and suggests possible modeling and algorithmic extensions. We explore a few in our experiments, which demonstrate improved performance over current state-of-the-art methods.", "keywords": ["representation learning", "permutation invariance", "set functions", "feature pooling"], "authorids": ["murph213@purdue.edu", "bsriniv@purdue.edu", "varao@purdue.edu", "ribeiro@cs.purdue.edu"], "authors": ["Ryan L. Murphy", "Balasubramaniam Srinivasan", "Vinayak Rao", "Bruno Ribeiro"], "TL;DR": "We propose Janossy pooling, a method for learning deep permutation invariant functions designed to exploit relationships within the input sequence and tractable inference strategies such as a stochastic optimization procedure we call piSGD", "pdf": "/pdf/245bbcd015272f347e3977e5212fc17c084622c9.pdf", "paperhash": "murphy|janossy_pooling_learning_deep_permutationinvariant_functions_for_variablesize_inputs", "_bibtex": "@inproceedings{\nmurphy2018janossy,\ntitle={Janossy Pooling: Learning Deep Permutation-Invariant Functions for Variable-Size Inputs},\nauthor={Ryan L. Murphy and Balasubramaniam Srinivasan and Vinayak Rao and Bruno Ribeiro},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BJluy2RcFm},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 13, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "B1lSYgAGeN", "original": null, "number": 1, "cdate": 1544900733238, "ddate": null, "tcdate": 1544900733238, "tmdate": 1545354493396, "tddate": null, "forum": "BJluy2RcFm", "replyto": "BJluy2RcFm", "invitation": "ICLR.cc/2019/Conference/-/Paper1000/Meta_Review", "content": {"metareview": "AR1 is concerned about whether higher-order interactions are modeled explicitly and if pi-SGD convergence conditions can be easily satisfied. AR2 is concerned that basic JP has been conceptually discussed in the literature and \\pi-SGD is not novel because it was realized by Hamilton et al. (2017) and Moore & Neville (2017). However, the authors provide some theoretical analysis for this setting in contrast to prior works. AR1 is also concerned that the effect of higher-order information has not been 'disentangled' experimentally from order invariance. AR4 is concerned about  poor performance of higher order Janossy pooling compared to k =1 case and asks about the number of hyper-parameters. The authors showed a harder task of computing the variance of a sequence of numbers in response.\n\nOn balance, despite justified concerns of AR2 about novelty and AR1 about experimental verification, the work appears to tackle an interesting topic.  Reviewers find the problem interesting and see some hope in the proposed solutions. On balance, AC recommends this paper to be accepted at ICLR. The authors are asked to update manuscript to reflect honestly weaknesses as expressed by reviewers, e.g. issue with effects of 'higher-order information' and 'disentangled' from order invariance.", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Accept (Poster)", "title": "Interesting take on permutation invariances."}, "signatures": ["ICLR.cc/2019/Conference/Paper1000/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper1000/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Janossy Pooling: Learning Deep Permutation-Invariant Functions for Variable-Size Inputs", "abstract": "We consider a simple and overarching representation for permutation-invariant functions of sequences (or set functions). Our approach, which we call Janossy pooling, expresses a permutation-invariant function as the average of a permutation-sensitive function applied to all reorderings of the input sequence. This allows us to leverage the rich and mature literature on permutation-sensitive functions to construct novel and flexible permutation-invariant functions. If carried out naively, Janossy pooling can be computationally prohibitive. To allow computational tractability, we consider three kinds of approximations: canonical orderings of sequences, functions with k-order interactions, and stochastic optimization algorithms with random permutations. Our framework unifies a variety of existing work in the literature, and suggests possible modeling and algorithmic extensions. We explore a few in our experiments, which demonstrate improved performance over current state-of-the-art methods.", "keywords": ["representation learning", "permutation invariance", "set functions", "feature pooling"], "authorids": ["murph213@purdue.edu", "bsriniv@purdue.edu", "varao@purdue.edu", "ribeiro@cs.purdue.edu"], "authors": ["Ryan L. Murphy", "Balasubramaniam Srinivasan", "Vinayak Rao", "Bruno Ribeiro"], "TL;DR": "We propose Janossy pooling, a method for learning deep permutation invariant functions designed to exploit relationships within the input sequence and tractable inference strategies such as a stochastic optimization procedure we call piSGD", "pdf": "/pdf/245bbcd015272f347e3977e5212fc17c084622c9.pdf", "paperhash": "murphy|janossy_pooling_learning_deep_permutationinvariant_functions_for_variablesize_inputs", "_bibtex": "@inproceedings{\nmurphy2018janossy,\ntitle={Janossy Pooling: Learning Deep Permutation-Invariant Functions for Variable-Size Inputs},\nauthor={Ryan L. Murphy and Balasubramaniam Srinivasan and Vinayak Rao and Bruno Ribeiro},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BJluy2RcFm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1000/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353003831, "tddate": null, "super": null, "final": null, "reply": {"forum": "BJluy2RcFm", "replyto": "BJluy2RcFm", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1000/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper1000/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1000/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353003831}}}, {"id": "B1ey5ZDp2X", "original": null, "number": 2, "cdate": 1541398919144, "ddate": null, "tcdate": 1541398919144, "tmdate": 1544327895911, "tddate": null, "forum": "BJluy2RcFm", "replyto": "BJluy2RcFm", "invitation": "ICLR.cc/2019/Conference/-/Paper1000/Official_Review", "content": {"title": "A few of comments and request for clarifications", "review": "In this paper, the authors presented a new pooling method called Janossy Pooling (JP), which is designed to better capture high-order information by addressing two limitations of existing works - fixed pooling function and fixed-size inputs. The studied problem is important and the motivation is clear, where the inputs are sets of objects such as values or vectors and how we can learn a good aggregation function to maximally preserve the information in the original sequence. The authors attacked this problem by firstly formally formulating this problem and introducing a general approach as well as a few of approximation methods to realize it in practice. They also discussed the connections of this work and some existing works such as deep set, which I found is quite useful. \n\nIn general, JP was proposed to learn permutation-invariant function for aggregating the information of the input sequence. The basic idea of JP is to simply take all generated order of sequences from the original sequence input, which however I found is not new since it has been conceptually discussed already in the literature.  Since this approach is computationally prohibitive, there are several ways of approximations to approach the solution. As the authors are aware of the existing works in the literature, these approaches were discussed before either in the same context or in some particular learning tasks. From this perspective, the proposed solutions are not novel either. \n\nThe experimental results are particularly weak. It is little interesting on the first toy problem and the results on graph embedding are not promising. In Table 2, it is clearly shown that the LSTM aggregation functions on the randomly sampled sequences are really beating the simple mean aggregation function. I think the authors need much more experiments to demonstrate why we need LSTM based pooling for realizing JP in terms of both the final accuracy and computational cost. \n\n-------------------------------------------------\nAfter reading the authors' rebuttals, they have addressed part of my concerns but I still think the current form is not below the acceptance threshold due to its weak experimental results and unclear technical details. \n\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1000/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Janossy Pooling: Learning Deep Permutation-Invariant Functions for Variable-Size Inputs", "abstract": "We consider a simple and overarching representation for permutation-invariant functions of sequences (or set functions). Our approach, which we call Janossy pooling, expresses a permutation-invariant function as the average of a permutation-sensitive function applied to all reorderings of the input sequence. This allows us to leverage the rich and mature literature on permutation-sensitive functions to construct novel and flexible permutation-invariant functions. If carried out naively, Janossy pooling can be computationally prohibitive. To allow computational tractability, we consider three kinds of approximations: canonical orderings of sequences, functions with k-order interactions, and stochastic optimization algorithms with random permutations. Our framework unifies a variety of existing work in the literature, and suggests possible modeling and algorithmic extensions. We explore a few in our experiments, which demonstrate improved performance over current state-of-the-art methods.", "keywords": ["representation learning", "permutation invariance", "set functions", "feature pooling"], "authorids": ["murph213@purdue.edu", "bsriniv@purdue.edu", "varao@purdue.edu", "ribeiro@cs.purdue.edu"], "authors": ["Ryan L. Murphy", "Balasubramaniam Srinivasan", "Vinayak Rao", "Bruno Ribeiro"], "TL;DR": "We propose Janossy pooling, a method for learning deep permutation invariant functions designed to exploit relationships within the input sequence and tractable inference strategies such as a stochastic optimization procedure we call piSGD", "pdf": "/pdf/245bbcd015272f347e3977e5212fc17c084622c9.pdf", "paperhash": "murphy|janossy_pooling_learning_deep_permutationinvariant_functions_for_variablesize_inputs", "_bibtex": "@inproceedings{\nmurphy2018janossy,\ntitle={Janossy Pooling: Learning Deep Permutation-Invariant Functions for Variable-Size Inputs},\nauthor={Ryan L. Murphy and Balasubramaniam Srinivasan and Vinayak Rao and Bruno Ribeiro},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BJluy2RcFm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1000/Official_Review", "cdate": 1542234329006, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "BJluy2RcFm", "replyto": "BJluy2RcFm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1000/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335850619, "tmdate": 1552335850619, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1000/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "Skgp6BtKAm", "original": null, "number": 7, "cdate": 1543243205023, "ddate": null, "tcdate": 1543243205023, "tmdate": 1543243205023, "tddate": null, "forum": "BJluy2RcFm", "replyto": "BklEWNrmC7", "invitation": "ICLR.cc/2019/Conference/-/Paper1000/Official_Comment", "content": {"title": "Revised Submission", "comment": "Thanks again for your question.  We have updated the submission to include the comments we made and a few other minor improvements."}, "signatures": ["ICLR.cc/2019/Conference/Paper1000/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1000/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1000/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Janossy Pooling: Learning Deep Permutation-Invariant Functions for Variable-Size Inputs", "abstract": "We consider a simple and overarching representation for permutation-invariant functions of sequences (or set functions). Our approach, which we call Janossy pooling, expresses a permutation-invariant function as the average of a permutation-sensitive function applied to all reorderings of the input sequence. This allows us to leverage the rich and mature literature on permutation-sensitive functions to construct novel and flexible permutation-invariant functions. If carried out naively, Janossy pooling can be computationally prohibitive. To allow computational tractability, we consider three kinds of approximations: canonical orderings of sequences, functions with k-order interactions, and stochastic optimization algorithms with random permutations. Our framework unifies a variety of existing work in the literature, and suggests possible modeling and algorithmic extensions. We explore a few in our experiments, which demonstrate improved performance over current state-of-the-art methods.", "keywords": ["representation learning", "permutation invariance", "set functions", "feature pooling"], "authorids": ["murph213@purdue.edu", "bsriniv@purdue.edu", "varao@purdue.edu", "ribeiro@cs.purdue.edu"], "authors": ["Ryan L. Murphy", "Balasubramaniam Srinivasan", "Vinayak Rao", "Bruno Ribeiro"], "TL;DR": "We propose Janossy pooling, a method for learning deep permutation invariant functions designed to exploit relationships within the input sequence and tractable inference strategies such as a stochastic optimization procedure we call piSGD", "pdf": "/pdf/245bbcd015272f347e3977e5212fc17c084622c9.pdf", "paperhash": "murphy|janossy_pooling_learning_deep_permutationinvariant_functions_for_variablesize_inputs", "_bibtex": "@inproceedings{\nmurphy2018janossy,\ntitle={Janossy Pooling: Learning Deep Permutation-Invariant Functions for Variable-Size Inputs},\nauthor={Ryan L. Murphy and Balasubramaniam Srinivasan and Vinayak Rao and Bruno Ribeiro},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BJluy2RcFm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1000/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621613982, "tddate": null, "super": null, "final": null, "reply": {"forum": "BJluy2RcFm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1000/Authors", "ICLR.cc/2019/Conference/Paper1000/Reviewers", "ICLR.cc/2019/Conference/Paper1000/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1000/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1000/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1000/Authors|ICLR.cc/2019/Conference/Paper1000/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1000/Reviewers", "ICLR.cc/2019/Conference/Paper1000/Authors", "ICLR.cc/2019/Conference/Paper1000/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621613982}}}, {"id": "BklEWNrmC7", "original": null, "number": 6, "cdate": 1542833147654, "ddate": null, "tcdate": 1542833147654, "tmdate": 1542833147654, "tddate": null, "forum": "BJluy2RcFm", "replyto": "SJxGsKnlA7", "invitation": "ICLR.cc/2019/Conference/-/Paper1000/Official_Comment", "content": {"title": "Response to question about the second experiment.", "comment": "Thank you, the comment raises an interesting point.\n\n0. GraphSAGE is an instantiation of Janossy Pooling (JP) -- k-ary JP trained with pi-SGD (see our remark \"Combining pi-SGD and Janossy with k-ary Dependencies\" in our revised submission). This was the motive behind our second set of experiments, where we study the impact of proper test-time inference, in contrast to the ad hoc LSTM aggregator of Hamilton et al., 2017. As predicted by our theory, proper inference led to significant gains in performance. \n\n1. FASTGCN similarly falls under the framework of k-ary JP with pi-SGD optimization, applying importance sampling for variance reduction (the FASTGCN paper may be somewhat confusing to readers because it uses Lebesgue integrals and dP() measures instead of sums, but its Algorithm 2 is just an importance sampling procedure). In our submission we mention one interesting variance-reduction technique used by Zolna et al., 2018, but we now realize that it would be valuable to add additional traditional techniques such as importance sampling (used in FASTGCN), Rao-Blackwellization, and control variates to our discussion. Variance reduction helps the pi-SGD objective in equation 10 (\\doublebar{J}) become more like the \"original\" objective in equation 6 (\\doublebar{L}) (see the paragraph below Proposition 2.2). \n\n2. We are excited by the prospect of applying the lessons learned from Janossy Pooling to Graph Neural Networks (GNNs) in our future work. We believe that these insights will inspire new forms of aggregation functions in GNNs. A promising avenue is to combine JP with other variance-reduction techniques such as layer normalization and ensemble techniques like dropout, which have demonstrated strong performance on GNN tasks like PPI (Chen and Zhu, 2017).\n\n3. Please note that Hamilton et al., 2017 did not use exact mean pooling in their experiments but sampled a subset of neighbors. Our experiments deviate from theirs on this point. Also note that for Cora and PubMed, sampling 25 neighbors (as in our updated draft) effectively samples all neighbors, but not for PPI. \n\n\n(Chen and Zhu, 2017) - J. Chen and J. Zhu. Stochastic training of graph convolutional networks. arXiv preprint arXiv:1710.10568, 2017.\n\n(Zolna et al., 2018) - Konrad Zolna, Devansh Arpit, Dendi Suhubdy, and Yoshua Bengio. Fraternal dropout. ICLR, 2018."}, "signatures": ["ICLR.cc/2019/Conference/Paper1000/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1000/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1000/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Janossy Pooling: Learning Deep Permutation-Invariant Functions for Variable-Size Inputs", "abstract": "We consider a simple and overarching representation for permutation-invariant functions of sequences (or set functions). Our approach, which we call Janossy pooling, expresses a permutation-invariant function as the average of a permutation-sensitive function applied to all reorderings of the input sequence. This allows us to leverage the rich and mature literature on permutation-sensitive functions to construct novel and flexible permutation-invariant functions. If carried out naively, Janossy pooling can be computationally prohibitive. To allow computational tractability, we consider three kinds of approximations: canonical orderings of sequences, functions with k-order interactions, and stochastic optimization algorithms with random permutations. Our framework unifies a variety of existing work in the literature, and suggests possible modeling and algorithmic extensions. We explore a few in our experiments, which demonstrate improved performance over current state-of-the-art methods.", "keywords": ["representation learning", "permutation invariance", "set functions", "feature pooling"], "authorids": ["murph213@purdue.edu", "bsriniv@purdue.edu", "varao@purdue.edu", "ribeiro@cs.purdue.edu"], "authors": ["Ryan L. Murphy", "Balasubramaniam Srinivasan", "Vinayak Rao", "Bruno Ribeiro"], "TL;DR": "We propose Janossy pooling, a method for learning deep permutation invariant functions designed to exploit relationships within the input sequence and tractable inference strategies such as a stochastic optimization procedure we call piSGD", "pdf": "/pdf/245bbcd015272f347e3977e5212fc17c084622c9.pdf", "paperhash": "murphy|janossy_pooling_learning_deep_permutationinvariant_functions_for_variablesize_inputs", "_bibtex": "@inproceedings{\nmurphy2018janossy,\ntitle={Janossy Pooling: Learning Deep Permutation-Invariant Functions for Variable-Size Inputs},\nauthor={Ryan L. Murphy and Balasubramaniam Srinivasan and Vinayak Rao and Bruno Ribeiro},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BJluy2RcFm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1000/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621613982, "tddate": null, "super": null, "final": null, "reply": {"forum": "BJluy2RcFm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1000/Authors", "ICLR.cc/2019/Conference/Paper1000/Reviewers", "ICLR.cc/2019/Conference/Paper1000/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1000/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1000/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1000/Authors|ICLR.cc/2019/Conference/Paper1000/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1000/Reviewers", "ICLR.cc/2019/Conference/Paper1000/Authors", "ICLR.cc/2019/Conference/Paper1000/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621613982}}}, {"id": "SJxGsKnlA7", "original": null, "number": 1, "cdate": 1542666650289, "ddate": null, "tcdate": 1542666650289, "tmdate": 1542666650289, "tddate": null, "forum": "BJluy2RcFm", "replyto": "BJluy2RcFm", "invitation": "ICLR.cc/2019/Conference/-/Paper1000/Public_Comment", "content": {"comment": "Thank you. I also really enjoyed reading the paper. \nFor the vertex classification, without the aggregating function, minibatch sampling approaches (e.g. FASTGCN) are also getting almost similar or better accuracy than exact mean-pool (GRAPHSAGE) according to the paper. FASTGCN samples a few nodes to learn the graph neural network, so it is already fast and can avoid the additional computational burden of the aggregator. This means that the need for aggregation function is not clear for the GCN-baed models. I think more analysis with variance or other better measures are needed to show why it is meaningful for the vertex classification task as well. \n\n It may be useful to test it with the work (Moore & Neville, 2017).\n\n(Chen, Ma, and Xiao, 2018) Jie Chen, Tengfei Ma, Cao Xiao. FASTGCN: Fast Learning with graph convolutional networks via importance sampling.\n(Moore & Neville, 2017) John Moore and Jennifer Neville. Deep collective inference.", "title": "Question about the second experiment."}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Janossy Pooling: Learning Deep Permutation-Invariant Functions for Variable-Size Inputs", "abstract": "We consider a simple and overarching representation for permutation-invariant functions of sequences (or set functions). Our approach, which we call Janossy pooling, expresses a permutation-invariant function as the average of a permutation-sensitive function applied to all reorderings of the input sequence. This allows us to leverage the rich and mature literature on permutation-sensitive functions to construct novel and flexible permutation-invariant functions. If carried out naively, Janossy pooling can be computationally prohibitive. To allow computational tractability, we consider three kinds of approximations: canonical orderings of sequences, functions with k-order interactions, and stochastic optimization algorithms with random permutations. Our framework unifies a variety of existing work in the literature, and suggests possible modeling and algorithmic extensions. We explore a few in our experiments, which demonstrate improved performance over current state-of-the-art methods.", "keywords": ["representation learning", "permutation invariance", "set functions", "feature pooling"], "authorids": ["murph213@purdue.edu", "bsriniv@purdue.edu", "varao@purdue.edu", "ribeiro@cs.purdue.edu"], "authors": ["Ryan L. Murphy", "Balasubramaniam Srinivasan", "Vinayak Rao", "Bruno Ribeiro"], "TL;DR": "We propose Janossy pooling, a method for learning deep permutation invariant functions designed to exploit relationships within the input sequence and tractable inference strategies such as a stochastic optimization procedure we call piSGD", "pdf": "/pdf/245bbcd015272f347e3977e5212fc17c084622c9.pdf", "paperhash": "murphy|janossy_pooling_learning_deep_permutationinvariant_functions_for_variablesize_inputs", "_bibtex": "@inproceedings{\nmurphy2018janossy,\ntitle={Janossy Pooling: Learning Deep Permutation-Invariant Functions for Variable-Size Inputs},\nauthor={Ryan L. Murphy and Balasubramaniam Srinivasan and Vinayak Rao and Bruno Ribeiro},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BJluy2RcFm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1000/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311702455, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "BJluy2RcFm", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1000/Authors", "ICLR.cc/2019/Conference/Paper1000/Reviewers", "ICLR.cc/2019/Conference/Paper1000/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper1000/Authors", "ICLR.cc/2019/Conference/Paper1000/Reviewers", "ICLR.cc/2019/Conference/Paper1000/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311702455}}}, {"id": "r1x2XvP107", "original": null, "number": 5, "cdate": 1542580003693, "ddate": null, "tcdate": 1542580003693, "tmdate": 1542580003693, "tddate": null, "forum": "BJluy2RcFm", "replyto": "B1ey5ZDp2X", "invitation": "ICLR.cc/2019/Conference/-/Paper1000/Official_Comment", "content": {"title": "Response to Reviewer2", "comment": "Thank you for your comments. We address the two issues, experimental evaluation and novelty, below.\n\n1) \"Experiments weak\"\n\n(1.a) \"Toy problems:\" Our arithmetic tasks are more challenging than those of Deep Sets, whose experimental methodology we extend. That paper evaluated the model on a task that adds a set of digits. While summation does not require exploiting dependencies among elements in the sequence, we consider tasks such as \u201crange\u201d where doing so is imperative.  Our updated submission adds the task of computing the variance of a sequence of 10 integers. This update also makes our former preliminary results part of the main paper with a discussion of the new insights (here summarized in points 1.b and 1.c.)\n\nOverall, our work is focused on generalizing today's pooling methods rather than any specific task. With that view, we make our tasks as simple as they can be to avoid spurious effects, but not so simple that the effects of increased pooling expressiveness do not apply. We welcome suggestions of ways to improve them.\n\n(1.b) \"No significant gains\". \"LSTM does not beat mean pooling.\"  The new variance task shows pi-SGD + GRUs + MLP \\rho significantly outperforms other methods, including sum-pooling (variance requires better modeling of high-order interactions). Overall, pi-SGD + GRUs + MLP \\rho  yields equal or superior performance across all arithmetic tasks. In general, using RNNs has the benefits of accepting variable-length sequences and seamlessly exploiting dependencies within the sequence.\n\n(1.c) \"Graph tasks\": We followed the tasks found in Hamilton et al. (2017), which we found are quite easy. Our main interest was in evaluating differences between the different JP approaches (different choices of k and the impact of proper inference) on a task distinct from the arithmetic ones, and the results confirmed the anticipated benefits of using better inference at test time. In particular, proper inference of the \\pi-SGD + LSTM model via Remark 2.2 can yield performance gains \"for free\" simply by averaging over forwarded permutations of the input sequence at test time.  \n\n2) \"Novelty\": \"The basic idea of JP is to simply take all generated order of sequences from the original sequence input, which however I found ... has been conceptually discussed already in the literature.\" We will try to answer this in a few different ways.\n\n(2.a) \"There is prior modeling work summing permutations in pooling layers\". To the best of our knowledge, our pooling framework is the first that generalizes pooling, unless the reviewer is aware of other work we haven't cited. As we answer next, Hamilton et al. (2017) and Moore & Neville (2017) performed pi-SGD in ad hoc manner, at the time it was not clear that it was a sound optimization procedure. Our work provides the theoretical underpinnings for their approach. \n\n(2.b) \"\\pi-SGD is not novel because it has already been tried\". Hamilton et al. (2017) and Moore & Neville (2017) did not provide a theoretical justification for their approach, and it was not obvious how to extend it. Our framework provides a theoretical justification for why and how pi-SGD works (SGD on the aforementioned ideal), as well as a characterization of the *correct* way to do inference at test time (missing in Hamilton et al. (2017)).\n\n(2.c) \"Novelty of k-ary Janossy pooling\". To the best of our knowledge, k=3,... in full generality has not been tried. Deep Sets (with k=1) shows that sum-pooling is a universal approximator to permutation-invariant functions if the upper layers are universal approximators. We show that if the upper layers are not universal approximators (or if the universal approximation is hard to learn), k-ary Janossy pooling (k > 1) is more powerful. Moreover, we show that this pooling approach is equivalent to summing over permutation-sensitive functions and achieves tractability via a restricted model class (functions with k inputs) rather than an approximate algorithm. Thus, our framework links two views of pooling: Inductive biases imposed on the model to capture dependencies in the sequence is inextricably linked with tractability strategies and present a tradeoff with learnability. \n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1000/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1000/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1000/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Janossy Pooling: Learning Deep Permutation-Invariant Functions for Variable-Size Inputs", "abstract": "We consider a simple and overarching representation for permutation-invariant functions of sequences (or set functions). Our approach, which we call Janossy pooling, expresses a permutation-invariant function as the average of a permutation-sensitive function applied to all reorderings of the input sequence. This allows us to leverage the rich and mature literature on permutation-sensitive functions to construct novel and flexible permutation-invariant functions. If carried out naively, Janossy pooling can be computationally prohibitive. To allow computational tractability, we consider three kinds of approximations: canonical orderings of sequences, functions with k-order interactions, and stochastic optimization algorithms with random permutations. Our framework unifies a variety of existing work in the literature, and suggests possible modeling and algorithmic extensions. We explore a few in our experiments, which demonstrate improved performance over current state-of-the-art methods.", "keywords": ["representation learning", "permutation invariance", "set functions", "feature pooling"], "authorids": ["murph213@purdue.edu", "bsriniv@purdue.edu", "varao@purdue.edu", "ribeiro@cs.purdue.edu"], "authors": ["Ryan L. Murphy", "Balasubramaniam Srinivasan", "Vinayak Rao", "Bruno Ribeiro"], "TL;DR": "We propose Janossy pooling, a method for learning deep permutation invariant functions designed to exploit relationships within the input sequence and tractable inference strategies such as a stochastic optimization procedure we call piSGD", "pdf": "/pdf/245bbcd015272f347e3977e5212fc17c084622c9.pdf", "paperhash": "murphy|janossy_pooling_learning_deep_permutationinvariant_functions_for_variablesize_inputs", "_bibtex": "@inproceedings{\nmurphy2018janossy,\ntitle={Janossy Pooling: Learning Deep Permutation-Invariant Functions for Variable-Size Inputs},\nauthor={Ryan L. Murphy and Balasubramaniam Srinivasan and Vinayak Rao and Bruno Ribeiro},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BJluy2RcFm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1000/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621613982, "tddate": null, "super": null, "final": null, "reply": {"forum": "BJluy2RcFm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1000/Authors", "ICLR.cc/2019/Conference/Paper1000/Reviewers", "ICLR.cc/2019/Conference/Paper1000/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1000/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1000/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1000/Authors|ICLR.cc/2019/Conference/Paper1000/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1000/Reviewers", "ICLR.cc/2019/Conference/Paper1000/Authors", "ICLR.cc/2019/Conference/Paper1000/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621613982}}}, {"id": "B1gpO5L10X", "original": null, "number": 4, "cdate": 1542576757351, "ddate": null, "tcdate": 1542576757351, "tmdate": 1542576757351, "tddate": null, "forum": "BJluy2RcFm", "replyto": "S1xNEC4q2X", "invitation": "ICLR.cc/2019/Conference/-/Paper1000/Official_Comment", "content": {"title": "Response to Reviewer1", "comment": "Thank you for your positive comments.  We also see Janossy Pooling as simultaneously providing theory that highlights limitations of existing methods and an overarching framework for developing pooling functions.  We also agree that more experiments (as always) are beneficial; our revision includes a more thorough experiments section upon which we elaborate below.\n\n(1) In the experimental section, the authors show that [k-ary Janossy Pooling] recovers some of the performances lost by using sum / mean pooling...Is it the fact that you're explicitly modelling higher-order interactions that improves performance? Or is it that you're doing Janossy pooling over the higher order interactions (i.e. summing over permutations of non-invariant functions)?\n\nOur development of k-ary Janossy Pooling (JP) demonstrates that the increased performance associated with k>1 does not rely upon using permutation-sensitive Janossy functions \\harrow{f}. Indeed, our proof that (k-1)-ary JP is less expressive than k-ary JP constructs a permutation-invariant k-ary Janossy function (\\harrow{f}) which cannot be expressed by any (k-1)-ary Janossy function, permutation-sensitive or otherwise.  We modeled \\harrow{f} as permutation-sensitive in our experiments since basic neural network building blocks are permutation-sensitive. \n\n(2) I don't follow the relevance of Proposition 2.2? I see that it gives conditions under which we can expect \\pi-SGD to converge, but we aren't provided with any guidance about how likely those conditions are to be satisfied? Furthermore - these conditions don't seem to be specific to \\pi-SGD - any SGD algorithm with ``slightly biased'' gradients that satisfy these conditions would converge.\n\nWe sought to reassure the reader of the appropriateness of randomly sampling and forwarding just one permutation of the sequence during training -- which at first glance may appear inappropriate.  We agree that this can be achieved simply by pointing out the similarity to ''typical'' SGD and we have revised our paper accordingly and moved the detailed proof to the appendix.\n\n(3) My view is that the experimental section is too limited to support reading (1) which asserts that k-ary pooling or LSTM + sampling approaches are the right solution to this problem. \n\nWhile we agree that reading (2) is our preferred reading too, we have added experiments to the revised version which provide further support of the power of proposed JP models.  These include (a) the addition of a more complex \\rho (the function composed with the output of pooling) to all models for the arithmetic tasks, which presents a more competitive baseline, (b) the addition of a harder arithmetic task -- computing the variance of a sequence of integers -- and (c) further analysis of the impact of increasing the number of permutations sampled at test-time for prediction in a pi-SGD model.  The latter was performed on the PPI graph, a new dataset evaluated in this submission.\n\n(a and b) Whereas \\rho was previously a linear layer only, we have added results where \\rho is an MLP with a single hidden layer.  Our results show that Janossy pooling architectures achieve superior or similar performance to the baseline of sum pooling across all tasks -- including the variance task -- for either choice of \\rho.  Notice that the GRU model with an MLP \\rho achieves a mean RMSE of 0.40 on the variance task, beating the sum-pooling baseline by a substantial margin.\n\n(c) Our arithmetic tasks show a clear benefit of averaging over more permutations at test time; doing so either improved the mean performance or left it unchanged (especially when performance was already saturated).  We have also expanded our investigation of this phenomenon in the graphs tasks, where we plotted performance as a function of the number of permutations sampled  at test time across different models.  We saw that simply sampling just a few permutations led to consistent and statistically significant gains in performance.  These gains level off but do not degrade as more permutations are sampled.  \n\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1000/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1000/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1000/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Janossy Pooling: Learning Deep Permutation-Invariant Functions for Variable-Size Inputs", "abstract": "We consider a simple and overarching representation for permutation-invariant functions of sequences (or set functions). Our approach, which we call Janossy pooling, expresses a permutation-invariant function as the average of a permutation-sensitive function applied to all reorderings of the input sequence. This allows us to leverage the rich and mature literature on permutation-sensitive functions to construct novel and flexible permutation-invariant functions. If carried out naively, Janossy pooling can be computationally prohibitive. To allow computational tractability, we consider three kinds of approximations: canonical orderings of sequences, functions with k-order interactions, and stochastic optimization algorithms with random permutations. Our framework unifies a variety of existing work in the literature, and suggests possible modeling and algorithmic extensions. We explore a few in our experiments, which demonstrate improved performance over current state-of-the-art methods.", "keywords": ["representation learning", "permutation invariance", "set functions", "feature pooling"], "authorids": ["murph213@purdue.edu", "bsriniv@purdue.edu", "varao@purdue.edu", "ribeiro@cs.purdue.edu"], "authors": ["Ryan L. Murphy", "Balasubramaniam Srinivasan", "Vinayak Rao", "Bruno Ribeiro"], "TL;DR": "We propose Janossy pooling, a method for learning deep permutation invariant functions designed to exploit relationships within the input sequence and tractable inference strategies such as a stochastic optimization procedure we call piSGD", "pdf": "/pdf/245bbcd015272f347e3977e5212fc17c084622c9.pdf", "paperhash": "murphy|janossy_pooling_learning_deep_permutationinvariant_functions_for_variablesize_inputs", "_bibtex": "@inproceedings{\nmurphy2018janossy,\ntitle={Janossy Pooling: Learning Deep Permutation-Invariant Functions for Variable-Size Inputs},\nauthor={Ryan L. Murphy and Balasubramaniam Srinivasan and Vinayak Rao and Bruno Ribeiro},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BJluy2RcFm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1000/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621613982, "tddate": null, "super": null, "final": null, "reply": {"forum": "BJluy2RcFm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1000/Authors", "ICLR.cc/2019/Conference/Paper1000/Reviewers", "ICLR.cc/2019/Conference/Paper1000/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1000/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1000/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1000/Authors|ICLR.cc/2019/Conference/Paper1000/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1000/Reviewers", "ICLR.cc/2019/Conference/Paper1000/Authors", "ICLR.cc/2019/Conference/Paper1000/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621613982}}}, {"id": "B1eC2L8J0m", "original": null, "number": 3, "cdate": 1542575797589, "ddate": null, "tcdate": 1542575797589, "tmdate": 1542575797589, "tddate": null, "forum": "BJluy2RcFm", "replyto": "HklWWHbbT7", "invitation": "ICLR.cc/2019/Conference/-/Paper1000/Official_Comment", "content": {"title": "Response to Reviewer4", "comment": "Thank you for your positive comments. We address your concerns below.\n\n\"- Is there any reason as to why higher order Janossy poolings do not perform as good as k =1 for the sum experiment?\" \n\nThe sum task is an easy task, designed for k=1. Our revised manuscript shows sum task results with more runs and more epochs and the difference is not statistically significant. \n\n\u201c- The whole development seems not as effective as k =1 in Table.2....\u201d\n\nTheorem 2.1 shows that Janossy Pooling (JP) with k-ary dependencies includes and is more expressive than JP with (k-1)-ary dependencies, but there will be tasks where it is sufficient to let k=1 (and also easier to optimize).  This is especially true for easy tasks like the sum task which do not require exploiting dependencies within the input sequence.  Our revised manuscript now considers the harder task of computing the variance of a sequence of numbers. For this harder task, full-sequence Janossy (k = |h|) is significantly more accurate than k = 1,2,3, by using pi-SGD to train the model (which optimizes \\doublebar{J} rather than \\doublebar{L}). In the range task, full Janossy (k = |h|) + GRU + pi-SGD also shows significant gains over k=1,2,3. For all other tasks, Janossy k =|h| + GRU + pi-SGD performs as well as the other approaches. \n\n\"- One wonders, why for k =2, k =1 is not included? That is, can the formulation be changed in a way that \\downarrow operator represents l \\in {1 \\cdots k} projections?  In the end, the method creates k tuples and feed them through specific fs so why not having smaller tuples?\"\n\nTheoretically it is not necessary (by Theorem 2.1) but is an interesting direction for future work that could help in practice. It is clear, however, that Janossy k = |h| with GRU + pi-SGD is hard to beat in more challenging tasks.\n\n\"-Can you report the number of parameters for the developments (Janossy -k)? Some examples according to the experiments help. \"\n\nWe have added the number of parameters in the Supplementary Material (Table 7 and Table 9) together with more details about our experimental setting. We have also tested k=2,3 with more complex models for \\arrow{f}, the Supplementary Material shows the improved results.\n\n\"- I am a bit lost to grasp the paragraph below Eq.4, can you rephrase it and possibly provide references?\"\n\nThank you, we rephrased our observations to simplify the exposition. We also considered the pros and cons of including a proof that Eq.4 captures any permutation-invariant function with an expressive-enough set of permutation-sensitive functions: the proof is straightforward as one can simply add all possible asymmetries (that cancel out when summing over all permutations) to the set of all permutation-invariant functions and make this a set of permutation-sensitive functions. It could be useful as a Proposition but, given the page limit, we have chosen to omit this straightforward proof in favor of other observations.\n\n\u201c- When it comes to testing, how do you use Eq.13? Do you sample a few permutation and compute 13? If yes, how many in practice?\u201d\n\nWe have rewritten our experimental section to clarify how Eq.13 is used. We recommend looking at the new Table 1 which now more clearly defines \"infr samples\" to describe how many samples we use to estimate Eq.13. \n\n\" - In preposition 2.1, n seems confusing, why not |h| \"\n\nThat was a typo, we have changed to |h|. Thank you!\n\n- In P6, x_i is a sequence. this needs to be mentioned \n\nThank you. We have made changes in the notation to clarify that x(i) is the i-th sequence from the training (test) data."}, "signatures": ["ICLR.cc/2019/Conference/Paper1000/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1000/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1000/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Janossy Pooling: Learning Deep Permutation-Invariant Functions for Variable-Size Inputs", "abstract": "We consider a simple and overarching representation for permutation-invariant functions of sequences (or set functions). Our approach, which we call Janossy pooling, expresses a permutation-invariant function as the average of a permutation-sensitive function applied to all reorderings of the input sequence. This allows us to leverage the rich and mature literature on permutation-sensitive functions to construct novel and flexible permutation-invariant functions. If carried out naively, Janossy pooling can be computationally prohibitive. To allow computational tractability, we consider three kinds of approximations: canonical orderings of sequences, functions with k-order interactions, and stochastic optimization algorithms with random permutations. Our framework unifies a variety of existing work in the literature, and suggests possible modeling and algorithmic extensions. We explore a few in our experiments, which demonstrate improved performance over current state-of-the-art methods.", "keywords": ["representation learning", "permutation invariance", "set functions", "feature pooling"], "authorids": ["murph213@purdue.edu", "bsriniv@purdue.edu", "varao@purdue.edu", "ribeiro@cs.purdue.edu"], "authors": ["Ryan L. Murphy", "Balasubramaniam Srinivasan", "Vinayak Rao", "Bruno Ribeiro"], "TL;DR": "We propose Janossy pooling, a method for learning deep permutation invariant functions designed to exploit relationships within the input sequence and tractable inference strategies such as a stochastic optimization procedure we call piSGD", "pdf": "/pdf/245bbcd015272f347e3977e5212fc17c084622c9.pdf", "paperhash": "murphy|janossy_pooling_learning_deep_permutationinvariant_functions_for_variablesize_inputs", "_bibtex": "@inproceedings{\nmurphy2018janossy,\ntitle={Janossy Pooling: Learning Deep Permutation-Invariant Functions for Variable-Size Inputs},\nauthor={Ryan L. Murphy and Balasubramaniam Srinivasan and Vinayak Rao and Bruno Ribeiro},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BJluy2RcFm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1000/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621613982, "tddate": null, "super": null, "final": null, "reply": {"forum": "BJluy2RcFm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1000/Authors", "ICLR.cc/2019/Conference/Paper1000/Reviewers", "ICLR.cc/2019/Conference/Paper1000/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1000/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1000/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1000/Authors|ICLR.cc/2019/Conference/Paper1000/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1000/Reviewers", "ICLR.cc/2019/Conference/Paper1000/Authors", "ICLR.cc/2019/Conference/Paper1000/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621613982}}}, {"id": "B1gYh_tiTm", "original": null, "number": 2, "cdate": 1542326448669, "ddate": null, "tcdate": 1542326448669, "tmdate": 1542326448669, "tddate": null, "forum": "BJluy2RcFm", "replyto": "HJlOsS79pm", "invitation": "ICLR.cc/2019/Conference/-/Paper1000/Official_Comment", "content": {"title": "Clarification regarding 'unique count' and 'unique sum' tasks", "comment": "Our primary interest was in permutation-invariant functions, and we only used the term \u201cset function\u201d to follow Zaheer 2017.  Please note that Deep Sets performs the sum task on \u201csets\u201d of integers {0, 1, 2, \u2026, 9} of size 50 which must have duplicates.  In following their design, our input sequences also have duplicates.  \n\nWe also note that extensions of the Deep Sets theorem relating permutation-invariance and sum pooling was recently extended to include multisets in [Xu et al 2018].\n\nWe will add a line to the paper to clarify this.  Thanks.\n\n[Xu et al 2018] Xu, Keyulu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. \"How Powerful are Graph Neural Networks?.\"\u00a0arXiv preprint arXiv:1810.00826\u00a0(2018)."}, "signatures": ["ICLR.cc/2019/Conference/Paper1000/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1000/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1000/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Janossy Pooling: Learning Deep Permutation-Invariant Functions for Variable-Size Inputs", "abstract": "We consider a simple and overarching representation for permutation-invariant functions of sequences (or set functions). Our approach, which we call Janossy pooling, expresses a permutation-invariant function as the average of a permutation-sensitive function applied to all reorderings of the input sequence. This allows us to leverage the rich and mature literature on permutation-sensitive functions to construct novel and flexible permutation-invariant functions. If carried out naively, Janossy pooling can be computationally prohibitive. To allow computational tractability, we consider three kinds of approximations: canonical orderings of sequences, functions with k-order interactions, and stochastic optimization algorithms with random permutations. Our framework unifies a variety of existing work in the literature, and suggests possible modeling and algorithmic extensions. We explore a few in our experiments, which demonstrate improved performance over current state-of-the-art methods.", "keywords": ["representation learning", "permutation invariance", "set functions", "feature pooling"], "authorids": ["murph213@purdue.edu", "bsriniv@purdue.edu", "varao@purdue.edu", "ribeiro@cs.purdue.edu"], "authors": ["Ryan L. Murphy", "Balasubramaniam Srinivasan", "Vinayak Rao", "Bruno Ribeiro"], "TL;DR": "We propose Janossy pooling, a method for learning deep permutation invariant functions designed to exploit relationships within the input sequence and tractable inference strategies such as a stochastic optimization procedure we call piSGD", "pdf": "/pdf/245bbcd015272f347e3977e5212fc17c084622c9.pdf", "paperhash": "murphy|janossy_pooling_learning_deep_permutationinvariant_functions_for_variablesize_inputs", "_bibtex": "@inproceedings{\nmurphy2018janossy,\ntitle={Janossy Pooling: Learning Deep Permutation-Invariant Functions for Variable-Size Inputs},\nauthor={Ryan L. Murphy and Balasubramaniam Srinivasan and Vinayak Rao and Bruno Ribeiro},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BJluy2RcFm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1000/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621613982, "tddate": null, "super": null, "final": null, "reply": {"forum": "BJluy2RcFm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1000/Authors", "ICLR.cc/2019/Conference/Paper1000/Reviewers", "ICLR.cc/2019/Conference/Paper1000/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1000/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1000/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1000/Authors|ICLR.cc/2019/Conference/Paper1000/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1000/Reviewers", "ICLR.cc/2019/Conference/Paper1000/Authors", "ICLR.cc/2019/Conference/Paper1000/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621613982}}}, {"id": "HJlOsS79pm", "original": null, "number": 1, "cdate": 1542235552006, "ddate": null, "tcdate": 1542235552006, "tmdate": 1542235552006, "tddate": null, "forum": "BJluy2RcFm", "replyto": "S1xNEC4q2X", "invitation": "ICLR.cc/2019/Conference/-/Paper1000/Official_Comment", "content": {"title": "Unique sum and unique count require a multiset not a set", "comment": "I didn't pick this up in my initial review, but the \"unique sum\" and \"unique count\" tasks in the synthetic experiments go beyond the scope of the deep sets work since that paper refers to sets not multisets. \"Unique\" doesn't make sense for sets. These experiments should be removed (or at the very least this should be made clear). Similarly for the other tasks, sampling without replacement makes more sense to ensure the input is in fact a set.  "}, "signatures": ["ICLR.cc/2019/Conference/Paper1000/AnonReviewer1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1000/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1000/AnonReviewer1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Janossy Pooling: Learning Deep Permutation-Invariant Functions for Variable-Size Inputs", "abstract": "We consider a simple and overarching representation for permutation-invariant functions of sequences (or set functions). Our approach, which we call Janossy pooling, expresses a permutation-invariant function as the average of a permutation-sensitive function applied to all reorderings of the input sequence. This allows us to leverage the rich and mature literature on permutation-sensitive functions to construct novel and flexible permutation-invariant functions. If carried out naively, Janossy pooling can be computationally prohibitive. To allow computational tractability, we consider three kinds of approximations: canonical orderings of sequences, functions with k-order interactions, and stochastic optimization algorithms with random permutations. Our framework unifies a variety of existing work in the literature, and suggests possible modeling and algorithmic extensions. We explore a few in our experiments, which demonstrate improved performance over current state-of-the-art methods.", "keywords": ["representation learning", "permutation invariance", "set functions", "feature pooling"], "authorids": ["murph213@purdue.edu", "bsriniv@purdue.edu", "varao@purdue.edu", "ribeiro@cs.purdue.edu"], "authors": ["Ryan L. Murphy", "Balasubramaniam Srinivasan", "Vinayak Rao", "Bruno Ribeiro"], "TL;DR": "We propose Janossy pooling, a method for learning deep permutation invariant functions designed to exploit relationships within the input sequence and tractable inference strategies such as a stochastic optimization procedure we call piSGD", "pdf": "/pdf/245bbcd015272f347e3977e5212fc17c084622c9.pdf", "paperhash": "murphy|janossy_pooling_learning_deep_permutationinvariant_functions_for_variablesize_inputs", "_bibtex": "@inproceedings{\nmurphy2018janossy,\ntitle={Janossy Pooling: Learning Deep Permutation-Invariant Functions for Variable-Size Inputs},\nauthor={Ryan L. Murphy and Balasubramaniam Srinivasan and Vinayak Rao and Bruno Ribeiro},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BJluy2RcFm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1000/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621613982, "tddate": null, "super": null, "final": null, "reply": {"forum": "BJluy2RcFm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1000/Authors", "ICLR.cc/2019/Conference/Paper1000/Reviewers", "ICLR.cc/2019/Conference/Paper1000/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1000/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1000/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1000/Authors|ICLR.cc/2019/Conference/Paper1000/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1000/Reviewers", "ICLR.cc/2019/Conference/Paper1000/Authors", "ICLR.cc/2019/Conference/Paper1000/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621613982}}}, {"id": "HklWWHbbT7", "original": null, "number": 3, "cdate": 1541637368598, "ddate": null, "tcdate": 1541637368598, "tmdate": 1541637368598, "tddate": null, "forum": "BJluy2RcFm", "replyto": "BJluy2RcFm", "invitation": "ICLR.cc/2019/Conference/-/Paper1000/Official_Review", "content": {"title": "Emergency review for Janossy Pooling", "review": "I have found the ideas proposed in the paper very insightful and interesting. The paper, in general, is written very well and is accessible.  My most important concern is \n\n The whole development seems not as effective as k =1 in Table.2 (BTW, there is a typo there). One wonders, why for k =2, k =1 is not included? That is, can the formulation be changed in a way that \\downarrow operator represents l \\in {1 \\cdots k} projections?  In the end, the method creates k tuples and feed them through specific fs so why not having smaller tuples?\n\nThe rest of my review below hopefully can help improving the paper;\n\n\n- Is there any reason as to why higher order Janossy poolings do not perform as good as k =1 for the sum experiment? \n\n- Can you report the number of parameters for the developments (Janossy -k)? Some examples according to the experiments help.\n\n- I am a bit lost to grasp the paragraph below Eq.4, can you rephrase it and possibly provide references?\n\n- When it comes to testing, how do you use Eq.13? Do you sample a few permutation and compute 13? If yes, how many in practice? \n\n- In preposition 2.1,  n seems confusing, why not |h|\n\n- In P6, x_i is a sequence. this needs to be mentioned \n\n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1000/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Janossy Pooling: Learning Deep Permutation-Invariant Functions for Variable-Size Inputs", "abstract": "We consider a simple and overarching representation for permutation-invariant functions of sequences (or set functions). Our approach, which we call Janossy pooling, expresses a permutation-invariant function as the average of a permutation-sensitive function applied to all reorderings of the input sequence. This allows us to leverage the rich and mature literature on permutation-sensitive functions to construct novel and flexible permutation-invariant functions. If carried out naively, Janossy pooling can be computationally prohibitive. To allow computational tractability, we consider three kinds of approximations: canonical orderings of sequences, functions with k-order interactions, and stochastic optimization algorithms with random permutations. Our framework unifies a variety of existing work in the literature, and suggests possible modeling and algorithmic extensions. We explore a few in our experiments, which demonstrate improved performance over current state-of-the-art methods.", "keywords": ["representation learning", "permutation invariance", "set functions", "feature pooling"], "authorids": ["murph213@purdue.edu", "bsriniv@purdue.edu", "varao@purdue.edu", "ribeiro@cs.purdue.edu"], "authors": ["Ryan L. Murphy", "Balasubramaniam Srinivasan", "Vinayak Rao", "Bruno Ribeiro"], "TL;DR": "We propose Janossy pooling, a method for learning deep permutation invariant functions designed to exploit relationships within the input sequence and tractable inference strategies such as a stochastic optimization procedure we call piSGD", "pdf": "/pdf/245bbcd015272f347e3977e5212fc17c084622c9.pdf", "paperhash": "murphy|janossy_pooling_learning_deep_permutationinvariant_functions_for_variablesize_inputs", "_bibtex": "@inproceedings{\nmurphy2018janossy,\ntitle={Janossy Pooling: Learning Deep Permutation-Invariant Functions for Variable-Size Inputs},\nauthor={Ryan L. Murphy and Balasubramaniam Srinivasan and Vinayak Rao and Bruno Ribeiro},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BJluy2RcFm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1000/Official_Review", "cdate": 1542234329006, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "BJluy2RcFm", "replyto": "BJluy2RcFm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1000/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335850619, "tmdate": 1552335850619, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1000/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "S1xNEC4q2X", "original": null, "number": 1, "cdate": 1541193260508, "ddate": null, "tcdate": 1541193260508, "tmdate": 1541533507903, "tddate": null, "forum": "BJluy2RcFm", "replyto": "BJluy2RcFm", "invitation": "ICLR.cc/2019/Conference/-/Paper1000/Official_Review", "content": {"title": "Interesting demonstration that standard pooling methods are insufficiently flexible", "review": "I really enjoyed this paper. It takes an idea which at first glance seems to be obviously bad (if you want permutation invariance, build a model that considers all permutations) and uses it to make the important point that the universal approximation results contained in Deep Sets [Zaheer et al. 2017] are not the last word on pooling. Janossy Pooling is intractable for most problems of interest (because it sums over all n! permutations of the input set) so the authors suggest 3 tractable alternatives: canonical orderings, k-ary dependencies and SGD / sampling-based approaches. Only the latter two are explored in detail, so I\u2019ll focus on them:\n\nK-ary dependencies\nFunctions that are restricted to k-ary dependencies in Janossy Pooling require summing over only |h|! / (|h| - k)! terms - that is they sum over the permutations of subsets of h of length k. In the experimental section, the authors show that this recovers some of the performances lost by using sum / mean pooling (as in Deep Sets), but this suggests the natural question: is it the fact that you\u2019re explicitly modelling higher-order interactions that improves performance? Or is it that you\u2019re doing Janossy pooling over the higher order interactions (i.e. summing over permutations of non-invariant functions)? \n\nThese two effects could be separated by comparing to invariant models that allow higher order interactions. E.g. you could compare against Santoro et al. [2017] who explicitly model pairwise interactions (or similarly any of the graph convolutional models [Kipf and Welling 2016, Hamilton et al 2017, etc.] with a fully connected graph would do the same); similarly Hartford, et al. [2018] allow for k-wise interactions by extending Deep Sets to exchangeable tensors - the permutation invariant analog of k-ary Janossy Pooling. All of these approaches model k-wise interactions through sum-pooling over permutation invariant functions so this lets you address the question - is it the permutation invariance that\u2019s the problem (necessitating k-ary Janossy pooling) or is it the lack of higher-order interaction terms? \n\nSGD approaches:\nI think that the point that the sampling-based approaches are bias with respect to the Janossy sum is important to make and I liked the discussion around it, but I don\u2019t follow the relevance of Proposition 2.2? I see that it gives conditions under which we can expect \\pi-SGD to converge, but we aren\u2019t provided with any guidance about how likely those conditions are to be satisfied? Furthermore - these conditions don\u2019t seem to be specific to \\pi-SGD - any SGD algorithm with \u201cslightly biased\u201d gradients that satisfy these conditions would converge. The regularization idea is interesting, but it isn\u2019t evaluated so we\u2019re left with theory that doesn\u2019t provide guidance and isn\u2019t evaluated.\n\nSummary:\nThere are two ways to read this paper:\n 1. Janossy pooling as a framework & proposed pooling approach implemented in one of the two ways discussed above.\n 2. Janossy pooling as an intractable upper bound on what we might want from a pooling method (with approximations in the form of the LSTM approaches) and a demonstration that our current invariant pooling methods are insufficient.\n\nI liked the paper based on reading (2). Janossy pooling clearly demonstrates limitations to sum / mean pooling which is widely used in practice which shows the need for better alternatives and it is on this basis that I\u2019m arguing for it\u2019s acceptance. My view is that the experimental section is too limited to support reading (1) which asserts that k-ary pooling or LSTM + sampling approaches are the right solution to this problem. \n\n[Zaheer et al. 2017] - Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Ruslan Salakhutdinov, and\nAlexander Smola. Deep Sets\n[Santoro et al. 2017] - Adam Santoro, David Raposo, David G Barrett, Mateusz Malinowski, Razvan Pascanu, Peter Battaglia, and Tim Lillicrap. A simple neural network module for relational reasoning.\n[Kipf and Welling 2016] - Thomas N. Kipf and Max Welling. Semi-Supervised Classification with Graph Convolutional Net- works\n[Hamilton et al 2017] - William L. Hamilton, Rex Ying, and Jure Leskovec. Inductive Representation Learning on Large Graphs\n[Hartford, et al. 2018] - Jason S. Hartford, Devon R. Graham, Kevin Leyton-Brown, and Siamak Ravanbakhsh. Deep models of interactions across sets", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1000/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Janossy Pooling: Learning Deep Permutation-Invariant Functions for Variable-Size Inputs", "abstract": "We consider a simple and overarching representation for permutation-invariant functions of sequences (or set functions). Our approach, which we call Janossy pooling, expresses a permutation-invariant function as the average of a permutation-sensitive function applied to all reorderings of the input sequence. This allows us to leverage the rich and mature literature on permutation-sensitive functions to construct novel and flexible permutation-invariant functions. If carried out naively, Janossy pooling can be computationally prohibitive. To allow computational tractability, we consider three kinds of approximations: canonical orderings of sequences, functions with k-order interactions, and stochastic optimization algorithms with random permutations. Our framework unifies a variety of existing work in the literature, and suggests possible modeling and algorithmic extensions. We explore a few in our experiments, which demonstrate improved performance over current state-of-the-art methods.", "keywords": ["representation learning", "permutation invariance", "set functions", "feature pooling"], "authorids": ["murph213@purdue.edu", "bsriniv@purdue.edu", "varao@purdue.edu", "ribeiro@cs.purdue.edu"], "authors": ["Ryan L. Murphy", "Balasubramaniam Srinivasan", "Vinayak Rao", "Bruno Ribeiro"], "TL;DR": "We propose Janossy pooling, a method for learning deep permutation invariant functions designed to exploit relationships within the input sequence and tractable inference strategies such as a stochastic optimization procedure we call piSGD", "pdf": "/pdf/245bbcd015272f347e3977e5212fc17c084622c9.pdf", "paperhash": "murphy|janossy_pooling_learning_deep_permutationinvariant_functions_for_variablesize_inputs", "_bibtex": "@inproceedings{\nmurphy2018janossy,\ntitle={Janossy Pooling: Learning Deep Permutation-Invariant Functions for Variable-Size Inputs},\nauthor={Ryan L. Murphy and Balasubramaniam Srinivasan and Vinayak Rao and Bruno Ribeiro},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BJluy2RcFm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1000/Official_Review", "cdate": 1542234329006, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "BJluy2RcFm", "replyto": "BJluy2RcFm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1000/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335850619, "tmdate": 1552335850619, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1000/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 14}