{"notes": [{"id": "BkE8NjCqYm", "original": "Hyxdt-7YYm", "number": 6, "cdate": 1538087726374, "ddate": null, "tcdate": 1538087726374, "tmdate": 1545355398839, "tddate": null, "forum": "BkE8NjCqYm", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "(Unconstrained) Beam Search is Sensitive to Large Search Discrepancies", "abstract": "Beam search is the most popular inference algorithm for decoding neural sequence models. Unlike greedy search, beam search allows for a non-greedy local decisions that can potentially lead to a sequence with a higher overall probability. However, previous work found that the performance of beam search tends to degrade with large beam widths. In this work, we perform an empirical study of the behavior of the beam search algorithm across three sequence synthesis tasks. We find that increasing the beam width leads to sequences that are disproportionately based on early and highly non-greedy decisions. These sequences typically include a very low probability token that is followed by a sequence of tokens with higher (conditional) probability leading to an overall higher probability sequence. However, as beam width increases, such sequences are more likely to have a lower evaluation score. Based on our empirical analysis we propose to constrain the beam search from taking highly non-greedy decisions early in the search. We evaluate two methods to constrain the search and show that constrained beam search effectively eliminates the problem of beam search degradation and in some cases even leads to higher evaluation scores. Our results generalize and improve upon previous observations on copies and training set predictions.", "paperhash": "cohen|unconstrained_beam_search_is_sensitive_to_large_search_discrepancies", "keywords": ["beam search", "sequence models", "search", "sequence to sequence"], "authorids": ["ecohen@mie.utoronto.ca", "jcb@mie.utoronto.ca"], "authors": ["Eldan Cohen", "J. Christopher Beck"], "TL;DR": "Analysis of the performance degradation in beam search and how constraining the the search can help avoiding it", "pdf": "/pdf/e5d14c2ac7a3b6de5f0815e706d0214f56b09c1a.pdf", "_bibtex": "@misc{\ncohen2019unconstrained,\ntitle={(Unconstrained) Beam Search is Sensitive to Large Search Discrepancies},\nauthor={Eldan Cohen and J. Christopher Beck},\nyear={2019},\nurl={https://openreview.net/forum?id=BkE8NjCqYm},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 14, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "BJeZvvBel4", "original": null, "number": 1, "cdate": 1544734553257, "ddate": null, "tcdate": 1544734553257, "tmdate": 1545354513390, "tddate": null, "forum": "BkE8NjCqYm", "replyto": "BkE8NjCqYm", "invitation": "ICLR.cc/2019/Conference/-/Paper6/Meta_Review", "content": {"metareview": "This paper examines a concept (also coined by the paper) of \"search discrepancies\" where the search algorithm behaves differently with large beam sizes. It then proposes heuristics to help prevent the model from performing worse when the size of the beam is increased.\n\nI think there are some interesting insights in this paper with respect to how search works in modern neural models, but most reviewers (and me) were concerned by the heuristic approach taken to fix these errors. I still think that within a search paper, a clear separation between modeling errors and search errors is useful, and adding heuristics on top has a potential to making things more complicated down the road when, for example, we change our model or we change our training algorithm.\n\nIt would be nice if the nice insights in the paper could be turned into a more theoretically clean framework that could be re-submitted to a future conference.", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Reject", "title": "Interesting insights but heuristics in approach worrisome"}, "signatures": ["ICLR.cc/2019/Conference/Paper6/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper6/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "(Unconstrained) Beam Search is Sensitive to Large Search Discrepancies", "abstract": "Beam search is the most popular inference algorithm for decoding neural sequence models. Unlike greedy search, beam search allows for a non-greedy local decisions that can potentially lead to a sequence with a higher overall probability. However, previous work found that the performance of beam search tends to degrade with large beam widths. In this work, we perform an empirical study of the behavior of the beam search algorithm across three sequence synthesis tasks. We find that increasing the beam width leads to sequences that are disproportionately based on early and highly non-greedy decisions. These sequences typically include a very low probability token that is followed by a sequence of tokens with higher (conditional) probability leading to an overall higher probability sequence. However, as beam width increases, such sequences are more likely to have a lower evaluation score. Based on our empirical analysis we propose to constrain the beam search from taking highly non-greedy decisions early in the search. We evaluate two methods to constrain the search and show that constrained beam search effectively eliminates the problem of beam search degradation and in some cases even leads to higher evaluation scores. Our results generalize and improve upon previous observations on copies and training set predictions.", "paperhash": "cohen|unconstrained_beam_search_is_sensitive_to_large_search_discrepancies", "keywords": ["beam search", "sequence models", "search", "sequence to sequence"], "authorids": ["ecohen@mie.utoronto.ca", "jcb@mie.utoronto.ca"], "authors": ["Eldan Cohen", "J. Christopher Beck"], "TL;DR": "Analysis of the performance degradation in beam search and how constraining the the search can help avoiding it", "pdf": "/pdf/e5d14c2ac7a3b6de5f0815e706d0214f56b09c1a.pdf", "_bibtex": "@misc{\ncohen2019unconstrained,\ntitle={(Unconstrained) Beam Search is Sensitive to Large Search Discrepancies},\nauthor={Eldan Cohen and J. Christopher Beck},\nyear={2019},\nurl={https://openreview.net/forum?id=BkE8NjCqYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper6/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353372600, "tddate": null, "super": null, "final": null, "reply": {"forum": "BkE8NjCqYm", "replyto": "BkE8NjCqYm", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper6/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper6/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper6/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353372600}}}, {"id": "B1l5OApA14", "original": null, "number": 10, "cdate": 1544638066326, "ddate": null, "tcdate": 1544638066326, "tmdate": 1544638066326, "tddate": null, "forum": "BkE8NjCqYm", "replyto": "Bkgfzvd6kN", "invitation": "ICLR.cc/2019/Conference/-/Paper6/Official_Comment", "content": {"title": "Thanks for your comment", "comment": "Thanks for your comment, we are glad that we have largely resolved some of your concerns.\n\nFor your remaining concern, we think the dichotomy you propose (\"principled and well-motivated\" vs. \"heuristic\") is a false one. Our approach is both. Through extensive experiments, we've provided an understanding of one of the \"six challenges for neural machine translation\" (Koehn & Knowles, 2017), shown that it is a much more general phenomenon than the existence of \"copies\" (the current best explanation), and shown that the same understanding applies to two very different sequence-to-sequence neural decoding tasks (summarization and captioning). We think we've established the novel principles and motivations for our algorithmic approach.\n\nOur heuristic approach demonstrates how the degradation of wider beam widths can be removed but does not unambiguously lead to higher evaluation. While we would have obviously liked our approach to do so, the more important contribution is the analysis and identification of this search phenomenon which is (apparently) widespread in neural sequence decoding tasks. Other researchers (and we too) can now work on the development of better algorithmic approaches informed by the deeper understanding.\n\nWe believe that the analysis and heuristic approach together represent a contribution significant enough to merit acceptance."}, "signatures": ["ICLR.cc/2019/Conference/Paper6/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper6/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper6/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "(Unconstrained) Beam Search is Sensitive to Large Search Discrepancies", "abstract": "Beam search is the most popular inference algorithm for decoding neural sequence models. Unlike greedy search, beam search allows for a non-greedy local decisions that can potentially lead to a sequence with a higher overall probability. However, previous work found that the performance of beam search tends to degrade with large beam widths. In this work, we perform an empirical study of the behavior of the beam search algorithm across three sequence synthesis tasks. We find that increasing the beam width leads to sequences that are disproportionately based on early and highly non-greedy decisions. These sequences typically include a very low probability token that is followed by a sequence of tokens with higher (conditional) probability leading to an overall higher probability sequence. However, as beam width increases, such sequences are more likely to have a lower evaluation score. Based on our empirical analysis we propose to constrain the beam search from taking highly non-greedy decisions early in the search. We evaluate two methods to constrain the search and show that constrained beam search effectively eliminates the problem of beam search degradation and in some cases even leads to higher evaluation scores. Our results generalize and improve upon previous observations on copies and training set predictions.", "paperhash": "cohen|unconstrained_beam_search_is_sensitive_to_large_search_discrepancies", "keywords": ["beam search", "sequence models", "search", "sequence to sequence"], "authorids": ["ecohen@mie.utoronto.ca", "jcb@mie.utoronto.ca"], "authors": ["Eldan Cohen", "J. Christopher Beck"], "TL;DR": "Analysis of the performance degradation in beam search and how constraining the the search can help avoiding it", "pdf": "/pdf/e5d14c2ac7a3b6de5f0815e706d0214f56b09c1a.pdf", "_bibtex": "@misc{\ncohen2019unconstrained,\ntitle={(Unconstrained) Beam Search is Sensitive to Large Search Discrepancies},\nauthor={Eldan Cohen and J. Christopher Beck},\nyear={2019},\nurl={https://openreview.net/forum?id=BkE8NjCqYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper6/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621612636, "tddate": null, "super": null, "final": null, "reply": {"forum": "BkE8NjCqYm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper6/Authors", "ICLR.cc/2019/Conference/Paper6/Reviewers", "ICLR.cc/2019/Conference/Paper6/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper6/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper6/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper6/Authors|ICLR.cc/2019/Conference/Paper6/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper6/Reviewers", "ICLR.cc/2019/Conference/Paper6/Authors", "ICLR.cc/2019/Conference/Paper6/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621612636}}}, {"id": "Bkgfzvd6kN", "original": null, "number": 9, "cdate": 1544550154461, "ddate": null, "tcdate": 1544550154461, "tmdate": 1544550154461, "tddate": null, "forum": "BkE8NjCqYm", "replyto": "S1leOha4aQ", "invitation": "ICLR.cc/2019/Conference/-/Paper6/Official_Comment", "content": {"title": "Thank you for your comprehensive response to my comments", "comment": "\nThe additional results for English:Chinese, and additional metrics, references etc. largely resolve some of my smaller concerns. I guess my remaining (more significant concern) is about the effectiveness of the approach. Some techniques are important because they are principled and well-motivated. Other techniques might be heuristics, but they can still be very important if they are effective. In this case, the proposed approach is a heuristic, but it is not particularly effective at improving the scores of the resulting sequences. In my review, I argued that 'Although the proposed method allows larger beam widths to be used without degradation during decoding, of course this is not actually beneficial unless the larger beam can improve the score.' I would still maintain this view, and I would disagree with the the authors comment to the AC (above) that 'Our search algorithms are not supposed to find solutions with higher search score', since I cannot see having higher evaluation score for larger beams is beneficial, unless those scores are also the highest overall (across all lengths). \n\nIn summary, I will retain my rating - marginally below acceptance."}, "signatures": ["ICLR.cc/2019/Conference/Paper6/AnonReviewer3"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper6/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper6/AnonReviewer3", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "(Unconstrained) Beam Search is Sensitive to Large Search Discrepancies", "abstract": "Beam search is the most popular inference algorithm for decoding neural sequence models. Unlike greedy search, beam search allows for a non-greedy local decisions that can potentially lead to a sequence with a higher overall probability. However, previous work found that the performance of beam search tends to degrade with large beam widths. In this work, we perform an empirical study of the behavior of the beam search algorithm across three sequence synthesis tasks. We find that increasing the beam width leads to sequences that are disproportionately based on early and highly non-greedy decisions. These sequences typically include a very low probability token that is followed by a sequence of tokens with higher (conditional) probability leading to an overall higher probability sequence. However, as beam width increases, such sequences are more likely to have a lower evaluation score. Based on our empirical analysis we propose to constrain the beam search from taking highly non-greedy decisions early in the search. We evaluate two methods to constrain the search and show that constrained beam search effectively eliminates the problem of beam search degradation and in some cases even leads to higher evaluation scores. Our results generalize and improve upon previous observations on copies and training set predictions.", "paperhash": "cohen|unconstrained_beam_search_is_sensitive_to_large_search_discrepancies", "keywords": ["beam search", "sequence models", "search", "sequence to sequence"], "authorids": ["ecohen@mie.utoronto.ca", "jcb@mie.utoronto.ca"], "authors": ["Eldan Cohen", "J. Christopher Beck"], "TL;DR": "Analysis of the performance degradation in beam search and how constraining the the search can help avoiding it", "pdf": "/pdf/e5d14c2ac7a3b6de5f0815e706d0214f56b09c1a.pdf", "_bibtex": "@misc{\ncohen2019unconstrained,\ntitle={(Unconstrained) Beam Search is Sensitive to Large Search Discrepancies},\nauthor={Eldan Cohen and J. Christopher Beck},\nyear={2019},\nurl={https://openreview.net/forum?id=BkE8NjCqYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper6/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621612636, "tddate": null, "super": null, "final": null, "reply": {"forum": "BkE8NjCqYm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper6/Authors", "ICLR.cc/2019/Conference/Paper6/Reviewers", "ICLR.cc/2019/Conference/Paper6/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper6/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper6/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper6/Authors|ICLR.cc/2019/Conference/Paper6/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper6/Reviewers", "ICLR.cc/2019/Conference/Paper6/Authors", "ICLR.cc/2019/Conference/Paper6/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621612636}}}, {"id": "rJxYtH-m07", "original": null, "number": 8, "cdate": 1542817152786, "ddate": null, "tcdate": 1542817152786, "tmdate": 1542817152786, "tddate": null, "forum": "BkE8NjCqYm", "replyto": "HJeT62PxRX", "invitation": "ICLR.cc/2019/Conference/-/Paper6/Official_Comment", "content": {"title": "Thanks for your questions", "comment": "Thanks for your questions.\n\nRegarding disentangling the effects:\nOur algorithms do not find solutions with higher search score (i.e., normalized log-prob). Intuitively, we are constraining the explored hypotheses space and it can lead to decreased search score. However, due to modeling errors discussed in our previous response, they do lead to a higher evaluation (BLEU, Rouge, SPICE or CIDEr) for larger beams. Section 4.4 explains how degraded solutions (i.e., solutions with lower evaluation score) end up getting a higher search score and, together with the example in Section 4.6, provides the intuition for why we propose to constrain the hypotheses space and effectively prefer, in some cases, lower search score solutions. Our search algorithms are not supposed to find solutions with higher search score. They are supposed to mitigate the effect of the large search discrepancies, lead to a higher evaluation score for larger beams, and validate our empirical analysis in Section 4.\n\n\nRegarding methods like Ranzato et al. (2016):\nThis work is focused on studying the previously reported search phenomenon of performance degradation in beam search with large beams. We believe understanding search-related phenomena provides deeper insight into the neural decoding process and can help design better search algorithms (see our response to AnonReviewer2 for a detailed motivation). As Ott et al. (2018) pointed out, Ranzato et al. (2016) do not provide an analysis of the problem or the impact of their solution on the search space. Specifically, as they only analyze beam search up to a width of 10, we cannot tell if performance degradation occurs and if our methods are needed. However, we believe the framework of search discrepancies will be useful in analyzing search-based neural decoding algorithms beyond the phenomenon of the performance degradation phenomenon - but this remains to be demonstrated in the future. Similarly, applying a similar analysis to other works such as Ranzato et al. (2016) is a direction for future work.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper6/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper6/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper6/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "(Unconstrained) Beam Search is Sensitive to Large Search Discrepancies", "abstract": "Beam search is the most popular inference algorithm for decoding neural sequence models. Unlike greedy search, beam search allows for a non-greedy local decisions that can potentially lead to a sequence with a higher overall probability. However, previous work found that the performance of beam search tends to degrade with large beam widths. In this work, we perform an empirical study of the behavior of the beam search algorithm across three sequence synthesis tasks. We find that increasing the beam width leads to sequences that are disproportionately based on early and highly non-greedy decisions. These sequences typically include a very low probability token that is followed by a sequence of tokens with higher (conditional) probability leading to an overall higher probability sequence. However, as beam width increases, such sequences are more likely to have a lower evaluation score. Based on our empirical analysis we propose to constrain the beam search from taking highly non-greedy decisions early in the search. We evaluate two methods to constrain the search and show that constrained beam search effectively eliminates the problem of beam search degradation and in some cases even leads to higher evaluation scores. Our results generalize and improve upon previous observations on copies and training set predictions.", "paperhash": "cohen|unconstrained_beam_search_is_sensitive_to_large_search_discrepancies", "keywords": ["beam search", "sequence models", "search", "sequence to sequence"], "authorids": ["ecohen@mie.utoronto.ca", "jcb@mie.utoronto.ca"], "authors": ["Eldan Cohen", "J. Christopher Beck"], "TL;DR": "Analysis of the performance degradation in beam search and how constraining the the search can help avoiding it", "pdf": "/pdf/e5d14c2ac7a3b6de5f0815e706d0214f56b09c1a.pdf", "_bibtex": "@misc{\ncohen2019unconstrained,\ntitle={(Unconstrained) Beam Search is Sensitive to Large Search Discrepancies},\nauthor={Eldan Cohen and J. Christopher Beck},\nyear={2019},\nurl={https://openreview.net/forum?id=BkE8NjCqYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper6/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621612636, "tddate": null, "super": null, "final": null, "reply": {"forum": "BkE8NjCqYm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper6/Authors", "ICLR.cc/2019/Conference/Paper6/Reviewers", "ICLR.cc/2019/Conference/Paper6/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper6/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper6/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper6/Authors|ICLR.cc/2019/Conference/Paper6/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper6/Reviewers", "ICLR.cc/2019/Conference/Paper6/Authors", "ICLR.cc/2019/Conference/Paper6/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621612636}}}, {"id": "HJeT62PxRX", "original": null, "number": 7, "cdate": 1542646980930, "ddate": null, "tcdate": 1542646980930, "tmdate": 1542646980930, "tddate": null, "forum": "BkE8NjCqYm", "replyto": "SylvelA4pQ", "invitation": "ICLR.cc/2019/Conference/-/Paper6/Official_Comment", "content": {"title": "To further disentangle the effects", "comment": "First, thank you for the response and clarification regarding length normalization. I had missed the detail in the paper.\n\nSecond, as a suggestion to disentangle the effect of search and model errors, I have a simple suggestion: could you please report the value of the search criterion (in this case, \"length normalized model score\") for each of the search algorithms? This would help show whether the algorithms you're proposing are actually better search algorithms, or whether they're exploiting some systematic difference between model scores and outputs that give high BLEU scores.\n\nThird, while I focused on length normalization in my previous comment, this is actually a band-aid over the true problem of not optimizing directly for the evaluation score. There are more direct methods to do so (e.g. \"sequence level training for recurrent neural networks\" by Ranzato et al.), and this made me wonder whether the proposed algorithm would be useful in a situation where the model has been tuned with one of these objectives."}, "signatures": ["ICLR.cc/2019/Conference/Paper6/Area_Chair1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper6/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper6/Area_Chair1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "(Unconstrained) Beam Search is Sensitive to Large Search Discrepancies", "abstract": "Beam search is the most popular inference algorithm for decoding neural sequence models. Unlike greedy search, beam search allows for a non-greedy local decisions that can potentially lead to a sequence with a higher overall probability. However, previous work found that the performance of beam search tends to degrade with large beam widths. In this work, we perform an empirical study of the behavior of the beam search algorithm across three sequence synthesis tasks. We find that increasing the beam width leads to sequences that are disproportionately based on early and highly non-greedy decisions. These sequences typically include a very low probability token that is followed by a sequence of tokens with higher (conditional) probability leading to an overall higher probability sequence. However, as beam width increases, such sequences are more likely to have a lower evaluation score. Based on our empirical analysis we propose to constrain the beam search from taking highly non-greedy decisions early in the search. We evaluate two methods to constrain the search and show that constrained beam search effectively eliminates the problem of beam search degradation and in some cases even leads to higher evaluation scores. Our results generalize and improve upon previous observations on copies and training set predictions.", "paperhash": "cohen|unconstrained_beam_search_is_sensitive_to_large_search_discrepancies", "keywords": ["beam search", "sequence models", "search", "sequence to sequence"], "authorids": ["ecohen@mie.utoronto.ca", "jcb@mie.utoronto.ca"], "authors": ["Eldan Cohen", "J. Christopher Beck"], "TL;DR": "Analysis of the performance degradation in beam search and how constraining the the search can help avoiding it", "pdf": "/pdf/e5d14c2ac7a3b6de5f0815e706d0214f56b09c1a.pdf", "_bibtex": "@misc{\ncohen2019unconstrained,\ntitle={(Unconstrained) Beam Search is Sensitive to Large Search Discrepancies},\nauthor={Eldan Cohen and J. Christopher Beck},\nyear={2019},\nurl={https://openreview.net/forum?id=BkE8NjCqYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper6/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621612636, "tddate": null, "super": null, "final": null, "reply": {"forum": "BkE8NjCqYm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper6/Authors", "ICLR.cc/2019/Conference/Paper6/Reviewers", "ICLR.cc/2019/Conference/Paper6/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper6/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper6/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper6/Authors|ICLR.cc/2019/Conference/Paper6/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper6/Reviewers", "ICLR.cc/2019/Conference/Paper6/Authors", "ICLR.cc/2019/Conference/Paper6/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621612636}}}, {"id": "BkevCeCNp7", "original": null, "number": 6, "cdate": 1541886159234, "ddate": null, "tcdate": 1541886159234, "tmdate": 1541886159234, "tddate": null, "forum": "BkE8NjCqYm", "replyto": "S1lH_kUq2Q", "invitation": "ICLR.cc/2019/Conference/-/Paper6/Official_Comment", "content": {"title": "Our algorithms are tuned on a held-out dev set (part 2)", "comment": "(This is part 2 of our response)\n\n- Example in Section 4.6:\nIn Section 4.6 we consider one case of training set prediction and explain how it exhibits our analysis on the search discrepancies. For the Gigaword dataset, we found that increasing the beam width leads to more predictions of \"<weekday>'s sports scoreboard\" that are all evaluated poorly against their reference (note that for lower beam widths we do not observe such predictions at all).\n   Our analysis in Section 4 explains that increasing the beam width can lead to large search discrepancies that are being selected since they are followed by a sequence of high (conditional) probability tokens that yield overall higher probability.\n    In Section 4.6 we present evidence that this scenario explains the increasing frequency of this (incorrect) summary:\n\t* The first token, <weekday>, has a relatively low probability (an average a discrepancy gap of 3.63 vs. a first token discrepancy gap average of 0.39 across the dataset).\n\t* In order for a sequence with such a low-probability first token to be chosen the most likely, the rest of tokens are significantly higher. The reason is that the training set has 2971 instances of \"<weekday>'s sports scoreboard\" as a target. This makes the \"sports\" and then \"scoreboard\" very likely, conditioned on the prefix. Due to this exposure bias in the training set and the fact that probabilities are locally normalized to 1 (label bias), these consecutive tokens of the low-probability first one end up \"contributing\" much more to the overall probability compared to consecutive tokens for the higher probability first token, leading this (incorrect) summary to be the most likely one.\n\t* For lower beam width, this first low-probability token would not have been considered as it would not be one of the top B.\n\n\n- Applying the constraints on early steps only:\nThanks for the interesting question. We have performed an initial analysis of the result when only constraining the early positions. While it is still beneficial to do it, it seems that it might not be enough to mitigate the performance degradation as new discrepancies will appear in the positions following the ones that are constrained (can be thought of \"early\" positions after the constrained ones).\n    A thorough analysis requires more time. We will do the analysis by the end of the review period and if accepted, we will include it as an appendix in the final version of the paper.\n\n\n- Comparison to Huang et al.: \nOur work is focused on the analyzing and eliminating the problem of performance degradation in large beam width. The problem addressed by Huang et al. is when to stop searching for new hypotheses when we already have completed hypotheses. Specifically, their work does not deal with large beam widths (up to 20 in their work) and as a result they do not observe a performance degradation. Trying to implement our constraints into other beam search algorithms (such as Huang et al.'s) is a direction for future work.\n\n\n- Clarifying eq.2:\nThe $y$ inside is meant to be a token rather than a sequence (which we denote $\\ry$; we mistakenly refer to it as $y$ in the line before the equation), and the equation simply means that the conditional probability of token y_t in the generated sequence is smaller than the conditional probability of the token with the highest conditional probability. We understand that the current notation is a bit confusing and we will revise it to make it clear.\n\n\nPlease let us know if you have further questions. We also ask the reviewer to see the new results we report in the response to AnonReviewer3 on Chinese translation (that shows the applicability of our analysis to languages that are significantly different from English) and the success of our methods with respect to two alternative evaluation metrics in image captioning."}, "signatures": ["ICLR.cc/2019/Conference/Paper6/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper6/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper6/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "(Unconstrained) Beam Search is Sensitive to Large Search Discrepancies", "abstract": "Beam search is the most popular inference algorithm for decoding neural sequence models. Unlike greedy search, beam search allows for a non-greedy local decisions that can potentially lead to a sequence with a higher overall probability. However, previous work found that the performance of beam search tends to degrade with large beam widths. In this work, we perform an empirical study of the behavior of the beam search algorithm across three sequence synthesis tasks. We find that increasing the beam width leads to sequences that are disproportionately based on early and highly non-greedy decisions. These sequences typically include a very low probability token that is followed by a sequence of tokens with higher (conditional) probability leading to an overall higher probability sequence. However, as beam width increases, such sequences are more likely to have a lower evaluation score. Based on our empirical analysis we propose to constrain the beam search from taking highly non-greedy decisions early in the search. We evaluate two methods to constrain the search and show that constrained beam search effectively eliminates the problem of beam search degradation and in some cases even leads to higher evaluation scores. Our results generalize and improve upon previous observations on copies and training set predictions.", "paperhash": "cohen|unconstrained_beam_search_is_sensitive_to_large_search_discrepancies", "keywords": ["beam search", "sequence models", "search", "sequence to sequence"], "authorids": ["ecohen@mie.utoronto.ca", "jcb@mie.utoronto.ca"], "authors": ["Eldan Cohen", "J. Christopher Beck"], "TL;DR": "Analysis of the performance degradation in beam search and how constraining the the search can help avoiding it", "pdf": "/pdf/e5d14c2ac7a3b6de5f0815e706d0214f56b09c1a.pdf", "_bibtex": "@misc{\ncohen2019unconstrained,\ntitle={(Unconstrained) Beam Search is Sensitive to Large Search Discrepancies},\nauthor={Eldan Cohen and J. Christopher Beck},\nyear={2019},\nurl={https://openreview.net/forum?id=BkE8NjCqYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper6/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621612636, "tddate": null, "super": null, "final": null, "reply": {"forum": "BkE8NjCqYm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper6/Authors", "ICLR.cc/2019/Conference/Paper6/Reviewers", "ICLR.cc/2019/Conference/Paper6/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper6/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper6/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper6/Authors|ICLR.cc/2019/Conference/Paper6/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper6/Reviewers", "ICLR.cc/2019/Conference/Paper6/Authors", "ICLR.cc/2019/Conference/Paper6/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621612636}}}, {"id": "HygNil0Vpm", "original": null, "number": 5, "cdate": 1541886108045, "ddate": null, "tcdate": 1541886108045, "tmdate": 1541886108045, "tddate": null, "forum": "BkE8NjCqYm", "replyto": "S1lH_kUq2Q", "invitation": "ICLR.cc/2019/Conference/-/Paper6/Official_Comment", "content": {"title": "Our algorithms are tuned on a held-out dev set (part 1)", "comment": "Thank you for your review. We will incorporate your comments and suggestions in the final version of the paper.\n\nWe would like to address the reviewer questions and concerns:\n\n- The use of test set in our analysis:\nWhile the empirical results in Section 4 are on the test set, our algorithms (discrepancy-constrained beam search variants) are tuned only on a dev set (with *no* information from the test set analysis). The reason why we present the empirical analysis in Section 4 on the test set is that we are focused on explaining the performance degradation that was previously reported on the test set. This analysis only provides a better understanding of the what is going on: no information from that analysis is later used in our algorithmic improvements. We did perform a similar analysis on the dev set and observed similar trends, however we thought presenting the test set is consistent with the previously reported results.\n   In Section 5, we propose two discrepancy-constrained variants of beam search. In order to tune these $\\mathcal{M}$ and $\\mathcal{N}$ that control the constraints, we used a held-out validation set, and then evaluated the performance on the test set (we also repeat our analysis of search discrepancy and it can be compared to the one in Section 4). Again, we would like to stress that no information from the test set were used to tune the algorithms (if the phenomenon did not occur on the dev set, the tuning would not yield useful values that will eliminate the performance degradation in the test set). We agree with the reviewer that this was not completely clear in the paper and we will clarify it in our final version.\n\n- Length ratio changes:\nWe specifically addressed the topic of length ratio changes in Appendix D, to demonstrate that the performance degradation on the baseline is not due to significant bias in length. Since the reviewer asked about the length ratio changes in our algorithms as well, we provide an updated table that also includes our discrepancy-constrained algorithms. Note that this is not the brevity term but it shows whether the length of the prediction has changed between the different beam widths and when comparing the baseline to our algorithms (and allows us to analyze other metrics that are not BLEU). \n   The table below includes the average length relative to the average length of the top performing baseline configuration (marked with *). As we state in Appendix D, the performance degradation in the baseline is not due to significant change in the length ratio. We can also see that our algorithm keeps nearly the same length ratio across the different beam widths (and is even marginally more stable than the baseline).\n\n\n+----------+--------------+------+-------+-------+------+-------+-------+\n| Dataset  |  Algorithm   | B=1  |  B=3  |  B=5  | B=25 | B=100 | B=250 |\n+----------+--------------+------+-------+-------+------+-------+-------+\n| En-De    | Baseline     | 0.99 | 1.00  | 1.00* | 1.00 |  0.99 |  0.98 |\n| En-De    | Constr. Gap  | 0.99 | 1.00  | 1.00  | 1.00 |  1.00 |  1.00 |\n| En-De    | Constr. Rank | 0.99 | 1.00  | 1.00  | 1.00 |  1.00 |  0.99 |\n+----------+--------------+------+-------+-------+------+-------+-------+\n| En-Fr    | Baseline     | 0.99 | 1.00  | 1.00* | 1.00 |  0.99 |  0.91 |\n| En-Fr    | Constr. Gap  | 0.99 | 1.00  | 1.00  | 1.00 |  1.00 |  0.99 |\n| En-Fr    | Constr. Rank | 0.99 | 1.00  | 1.00  | 1.00 |  1.00 |  1.00 |\n+----------+--------------+------+-------+-------+------+-------+-------+\n| Gigaword | Baseline     | 1.03 | 1.00* | 0.99  | 0.99 |  1.00 |  1.01 |\n| Gigaword | Constr. Gap  | 1.03 | 0.99  | 0.99  | 0.99 |  0.99 |  0.99 |\n| Gigaword | Constr. Rank | 1.03 | 1.00  | 0.99  | 0.99 |  0.99 |  0.99 |\n+----------+--------------+------+-------+-------+------+-------+-------+\n| COCO     | Baseline     | 1.04 | 1.00* | 0.99  | 0.98 |  0.98 |  0.98 |\n| COCO     | Constr. Gap  | 1.04 | 1.00  | 0.99  | 0.99 |  0.99 |  0.99 |\n| COCO     | Constr. Rank | 1.04 | 1.00  | 0.99  | 0.99 |  0.99 |  0.99 |\n+----------+--------------+------+-------+-------+------+-------+-------+"}, "signatures": ["ICLR.cc/2019/Conference/Paper6/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper6/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper6/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "(Unconstrained) Beam Search is Sensitive to Large Search Discrepancies", "abstract": "Beam search is the most popular inference algorithm for decoding neural sequence models. Unlike greedy search, beam search allows for a non-greedy local decisions that can potentially lead to a sequence with a higher overall probability. However, previous work found that the performance of beam search tends to degrade with large beam widths. In this work, we perform an empirical study of the behavior of the beam search algorithm across three sequence synthesis tasks. We find that increasing the beam width leads to sequences that are disproportionately based on early and highly non-greedy decisions. These sequences typically include a very low probability token that is followed by a sequence of tokens with higher (conditional) probability leading to an overall higher probability sequence. However, as beam width increases, such sequences are more likely to have a lower evaluation score. Based on our empirical analysis we propose to constrain the beam search from taking highly non-greedy decisions early in the search. We evaluate two methods to constrain the search and show that constrained beam search effectively eliminates the problem of beam search degradation and in some cases even leads to higher evaluation scores. Our results generalize and improve upon previous observations on copies and training set predictions.", "paperhash": "cohen|unconstrained_beam_search_is_sensitive_to_large_search_discrepancies", "keywords": ["beam search", "sequence models", "search", "sequence to sequence"], "authorids": ["ecohen@mie.utoronto.ca", "jcb@mie.utoronto.ca"], "authors": ["Eldan Cohen", "J. Christopher Beck"], "TL;DR": "Analysis of the performance degradation in beam search and how constraining the the search can help avoiding it", "pdf": "/pdf/e5d14c2ac7a3b6de5f0815e706d0214f56b09c1a.pdf", "_bibtex": "@misc{\ncohen2019unconstrained,\ntitle={(Unconstrained) Beam Search is Sensitive to Large Search Discrepancies},\nauthor={Eldan Cohen and J. Christopher Beck},\nyear={2019},\nurl={https://openreview.net/forum?id=BkE8NjCqYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper6/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621612636, "tddate": null, "super": null, "final": null, "reply": {"forum": "BkE8NjCqYm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper6/Authors", "ICLR.cc/2019/Conference/Paper6/Reviewers", "ICLR.cc/2019/Conference/Paper6/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper6/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper6/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper6/Authors|ICLR.cc/2019/Conference/Paper6/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper6/Reviewers", "ICLR.cc/2019/Conference/Paper6/Authors", "ICLR.cc/2019/Conference/Paper6/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621612636}}}, {"id": "SylvelA4pQ", "original": null, "number": 4, "cdate": 1541885935108, "ddate": null, "tcdate": 1541885935108, "tmdate": 1541885935108, "tddate": null, "forum": "BkE8NjCqYm", "replyto": "HygoNFt7a7", "invitation": "ICLR.cc/2019/Conference/-/Paper6/Official_Comment", "content": {"title": "Length Bias and Search vs. Model errors", "comment": "Thanks for your comments. We will address the two issues separately.\n\nLength bias\nThe review claims that the problem of performance degradation is well known and is due to the fact that MLE-trained models tend to prefer shorter sentences. However, this is not the case we are addressing. We are considering the case of performance degradation that is *not* due to length bias. Even when length-normalization is used, performance degradation is observed in previous work that we cite and in our baseline results (which explicitly use length-normalization). More specifically:\n\t* For the translation and summarization models we perform length normalization as a baseline (see Section 3) and observe search degradation.\n\t* Appendix D analyzes the length bias and shows that the performance degradation we observe is not associated with significant length bias. \n\t* The problem of ``copies'' (that we generalize in this work) was observed by Ott et al. (2018) when using length normalization.\n\t* In \"Six Challenges for Neural Machine Translation\" (Koehn & Knowles, 2017) that the reviewer refers to, the authors show that while normalization reduces the problem of performance degradation of beam search, it does not eliminate it.  As a consequebnce, performance degradation is still listed as Challenge #6.\n\t* Besides BLEU we analyze rouge for summarization, and in our response for AnonReviewer3 we added CIDEr and SPICE for image captioning.\n\n\nModel errors vs. search errors\nWe agree with the distinction between search errors and modeling errors and think this distinction is useful in highlighting our contributions. We do not agree that we are conflating model error and search error.\n   \t In the context of beam search, wider beams lead to higher likelihood hypotheses (fairly trivially because we are searching a larger space). The phenomenon we are exploring is the observation over a number of tasks that the higher likelihood hypotheses result in lower quality results due to the misalignment between the learned probability model and metric; that is, due to model errors.\n\tOur position is that modeling errors are unavoidable (due to e.g., noisy training data, training data that is necessarily a small sample of a huge space, etc.).  By understanding how these modeling errors interact with search we can improve task performance. Note that we do not consider search discrepancies as ``search errors''. The notion of discrepancies is well established in the search literature and limited discrepancy search is meant to deal with ``heuristic mistakes'': decision points in a tree search where the guiding heuristic prefers what is not actually the best option in terms of the final solution quality. We believe that there is a meaningful analogy here: just as a human-designed heuristic is not infallible (e.g. due to a myopic perspective), the learned model is not infallible due to modeling errors. \n\tWe are proposing a more nuanced search, based on our analysis and identification of a common phenomenon, to better deal with the modeling error. We note that the previous works on \"copies\" and training set predictions also addressed these problems using changes in search. Ott et al. (2018) added a pruning constraint to the beam search, while Vinyals et al. (2017) intentionally reduce the beam size to avoid training set predictions. However, these changes were aimed at the narrow phenomena observed (copies and training set predictions) which we generalize.\n\tSection 6 discusses the cause for the observed phenomenon, namely a combination of exposure bias and label bias. Our example in Section 4.6 (that is further clarified in our response to AnonReviewer 1) provide a detailed example."}, "signatures": ["ICLR.cc/2019/Conference/Paper6/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper6/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper6/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "(Unconstrained) Beam Search is Sensitive to Large Search Discrepancies", "abstract": "Beam search is the most popular inference algorithm for decoding neural sequence models. Unlike greedy search, beam search allows for a non-greedy local decisions that can potentially lead to a sequence with a higher overall probability. However, previous work found that the performance of beam search tends to degrade with large beam widths. In this work, we perform an empirical study of the behavior of the beam search algorithm across three sequence synthesis tasks. We find that increasing the beam width leads to sequences that are disproportionately based on early and highly non-greedy decisions. These sequences typically include a very low probability token that is followed by a sequence of tokens with higher (conditional) probability leading to an overall higher probability sequence. However, as beam width increases, such sequences are more likely to have a lower evaluation score. Based on our empirical analysis we propose to constrain the beam search from taking highly non-greedy decisions early in the search. We evaluate two methods to constrain the search and show that constrained beam search effectively eliminates the problem of beam search degradation and in some cases even leads to higher evaluation scores. Our results generalize and improve upon previous observations on copies and training set predictions.", "paperhash": "cohen|unconstrained_beam_search_is_sensitive_to_large_search_discrepancies", "keywords": ["beam search", "sequence models", "search", "sequence to sequence"], "authorids": ["ecohen@mie.utoronto.ca", "jcb@mie.utoronto.ca"], "authors": ["Eldan Cohen", "J. Christopher Beck"], "TL;DR": "Analysis of the performance degradation in beam search and how constraining the the search can help avoiding it", "pdf": "/pdf/e5d14c2ac7a3b6de5f0815e706d0214f56b09c1a.pdf", "_bibtex": "@misc{\ncohen2019unconstrained,\ntitle={(Unconstrained) Beam Search is Sensitive to Large Search Discrepancies},\nauthor={Eldan Cohen and J. Christopher Beck},\nyear={2019},\nurl={https://openreview.net/forum?id=BkE8NjCqYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper6/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621612636, "tddate": null, "super": null, "final": null, "reply": {"forum": "BkE8NjCqYm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper6/Authors", "ICLR.cc/2019/Conference/Paper6/Reviewers", "ICLR.cc/2019/Conference/Paper6/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper6/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper6/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper6/Authors|ICLR.cc/2019/Conference/Paper6/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper6/Reviewers", "ICLR.cc/2019/Conference/Paper6/Authors", "ICLR.cc/2019/Conference/Paper6/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621612636}}}, {"id": "S1leOha4aQ", "original": null, "number": 3, "cdate": 1541885031972, "ddate": null, "tcdate": 1541885031972, "tmdate": 1541885031972, "tddate": null, "forum": "BkE8NjCqYm", "replyto": "HyeSeu4jhQ", "invitation": "ICLR.cc/2019/Conference/-/Paper6/Official_Comment", "content": {"title": "Addressing the reviewer concerns", "comment": "Thank you for your detailed review. We will incorporate your comments and suggestions in the final version of the paper.\n\nWe would like to address the reviewer concerns:\n\n- Understanding the discrepancy phenomenon and applicability to languages significantly different from English:\nWe believe that the search discrepancies occur due to the combination of exposure and label bias as explained in our discussion. We provide a concrete example in Section 4.6 (see our response for AnonReviewer 1 for a detailed explanation of this example).\n    The reviewer raises a potential concern that the discrepancy phenomenon is actually a linguistic phenomenon associated with English or similar languages. We therefore perform an experiment of generating translations in a language that is significantly different than English: Chinese. We train and evaluate the convolutional translation model by Gehring et al. (2017) on the WMT'17 En-Zh dataset. We performed the analysis in the paper on this dataset and found similar trends (results follow). Specifically:\n\t* The dataset exhibits significant performance degradation for large beam width\n\t* Our analysis of the frequency and size of the early discrepancies and the comparison between improved vs. degraded sequences yielded similar trends to the other languages\n\t* We used a gap-constrained beam search, tuned on a held-out validation set, and successfully eliminated the performance degradation. We will have results on the rank constraint soon.\n\nThe results for the baseline vs. the discrepancy-constrained beam search are described in the following table:\n\n|    Dataset     |   Method    |    Threshold    |  B=1  |  B=3  |  B=5  | B=25  | B=100 | B=250 |\n\n| En-Zh (BLEU-4) | Baseline    |                 | 15.20 | 17.47 | 17.68 | 16.48 |  9.44 |  6.28 |\n| En-Zh (BLEU-4) | Constr. Gap | \\mathcal{M}=1.0 | 15.20 | 15.49 | 17.71 | 17.73 | 17.79 | 17.83 |\n\n\n- Evaluation metrics and need for human evals:\nOur work is done from a search perspective and is focused on analyzing, explaining, and eliminating the performance degradation in beam search with respect to a given evaluation metric. Previous work reporting the performance degradation are also based only on automatic evaluation.\n    The reviewer raises a potential concern regarding using BLEU-4 to evaluate image captioning, as there are indication that CIDEr and SPICE are better correlated with human judgment. We therefore present results for these two metrics on the image captioning dataset (COCO). Our analysis finds similar results to the ones found for BLEU-4:\n\t* There is a significant performance degradation for the image captioning tasks, with respect to both CIDEr and SPICE\n\t* We used a gap-constrained beam search, tuned on a held-out validation set, and significantly reduced (and almost eliminated) the performance degradation.\n\nThe results for the baseline vs. the discrepancy-constrained beam search are described in the following table:\n\n|   Dataset    |   Method    |    Threshold     |  B=1  |  B=3  |  B=5  | B=25  | B=100 | B=250 |\n\n| COCO (CIDEr) | Baseline    |                  | 0.974 | 1.018 | 1.005 | 0.953 | 0.946 | 0.945 |\n| COCO (CIDEr) | Constr. Gap | \\mathcal{M}=0.4  | 0.974 | 1.016 | 1.018 | 1.016 | 1.016 | 1.016 |\n| COCO (SPICE) | Baseline    |                  | 18.13 | 18.54 | 18.43 | 17.76 | 17.68 | 17.64 |\n| COCO (SPICE) | Constr. Gap | \\mathcal{M}=0.45 | 18.13 | 18.41 | 18.44 | 18.43 | 18.43 | 18.43 |\n\n- Rare words:\n\t* One main problem with rare words is the limited size of vocabulary (Koehn and Knowles, 2017). We do not change the vocabulary size. In fact, our machine translation model is using BPE (Sennrich et al., 2016) to allow translation of rare words using subword units.\n\t* Regarding the frequency of rare words that are in the vocabulary, we are definitely reducing the frequency of such rare words compared to the large beams of the baseline: for example, \"copies\" are all sequences of rare words that we eliminate. However, we analyzed the occurrence of rare words that are also in the reference for En-De translation and found no significant difference between the baseline and our algorithm in that respect. Note that the baseline itself poorly represents rare words that are indeed in the reference.\n\t* For image captioning and summarization, the related problem of novel captions/summaries vs. ones from the training set is thoroughly discussed in the paper (see Section 4.5 and Appendix B).\n\n- Constrained beam search:\nThanks for pointing that out. We are familiar with these works but did not include them as they are not directly related (they are addressing different problems such as forcing the inclusion of a selected token in the sequence). However, we agree with the reviewer that mentioning these works in the related work section will help reduce confusion with other constrained variants of beam search. We will add these to the related work section.\n\nPlease let us know if you have further questions.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper6/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper6/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper6/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "(Unconstrained) Beam Search is Sensitive to Large Search Discrepancies", "abstract": "Beam search is the most popular inference algorithm for decoding neural sequence models. Unlike greedy search, beam search allows for a non-greedy local decisions that can potentially lead to a sequence with a higher overall probability. However, previous work found that the performance of beam search tends to degrade with large beam widths. In this work, we perform an empirical study of the behavior of the beam search algorithm across three sequence synthesis tasks. We find that increasing the beam width leads to sequences that are disproportionately based on early and highly non-greedy decisions. These sequences typically include a very low probability token that is followed by a sequence of tokens with higher (conditional) probability leading to an overall higher probability sequence. However, as beam width increases, such sequences are more likely to have a lower evaluation score. Based on our empirical analysis we propose to constrain the beam search from taking highly non-greedy decisions early in the search. We evaluate two methods to constrain the search and show that constrained beam search effectively eliminates the problem of beam search degradation and in some cases even leads to higher evaluation scores. Our results generalize and improve upon previous observations on copies and training set predictions.", "paperhash": "cohen|unconstrained_beam_search_is_sensitive_to_large_search_discrepancies", "keywords": ["beam search", "sequence models", "search", "sequence to sequence"], "authorids": ["ecohen@mie.utoronto.ca", "jcb@mie.utoronto.ca"], "authors": ["Eldan Cohen", "J. Christopher Beck"], "TL;DR": "Analysis of the performance degradation in beam search and how constraining the the search can help avoiding it", "pdf": "/pdf/e5d14c2ac7a3b6de5f0815e706d0214f56b09c1a.pdf", "_bibtex": "@misc{\ncohen2019unconstrained,\ntitle={(Unconstrained) Beam Search is Sensitive to Large Search Discrepancies},\nauthor={Eldan Cohen and J. Christopher Beck},\nyear={2019},\nurl={https://openreview.net/forum?id=BkE8NjCqYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper6/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621612636, "tddate": null, "super": null, "final": null, "reply": {"forum": "BkE8NjCqYm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper6/Authors", "ICLR.cc/2019/Conference/Paper6/Reviewers", "ICLR.cc/2019/Conference/Paper6/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper6/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper6/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper6/Authors|ICLR.cc/2019/Conference/Paper6/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper6/Reviewers", "ICLR.cc/2019/Conference/Paper6/Authors", "ICLR.cc/2019/Conference/Paper6/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621612636}}}, {"id": "BkllHKTV6m", "original": null, "number": 2, "cdate": 1541884215946, "ddate": null, "tcdate": 1541884215946, "tmdate": 1541884215946, "tddate": null, "forum": "BkE8NjCqYm", "replyto": "S1gTFivu3m", "invitation": "ICLR.cc/2019/Conference/-/Paper6/Official_Comment", "content": {"title": "Thanks for your review", "comment": "We thank the review for the review and the comments. We will incorporate your comments and suggestions in the final version of the paper.\n\nRegarding the concern about the minor empirical improvement:\nThis work is focused on studying the previously reported search phenomenon of performance degradation in beam search with large beams. We believe understanding search-related phenomena provides deeper insight into the neural decoding process and can help design better search algorithms. Our proposed variants of discrepancy-constrained beam search are meant to \"fix\" the performance based on our analysis, and the success in doing so validates our analysis (together with our analysis of search discrepancies on the results of the discrepancy-constrained beam search). As such, we did not expect our methods to significantly outperform the best beam width but to eliminate the effect of the beam degradation and validate our analysis of the performance degradation phenomenon.\n\nRegarding your questions:\n- You are correct about the mistake in Section 5: we do mean \"modify Eq. 1\" instead of \"modify Eq. 3\". We will fix it.\n\n- We can definitely tune a different $\\mathcal{M}$ and $\\mathcal{N}$ for each beam width and we expect it to provide further improvement, however we preferred to show the robustness of a relatively simple tuning in effectively eliminating the performance degradation (this is why we are also interested in the easier-to-tune rank constraint). This is, again, consistent with our motivation stated above of studying the performance degradation, explaining this phenomenon, and mitigating it.\n\nPlease let us know if you have further questions. We also ask the reviewer to see the new results we report in the response to AnonReviewer3 on Chinese translation (that shows the applicability of our analysis to languages that are significantly different than English) and the success of our methods with respect to two alternative evaluation metrics in image captioning."}, "signatures": ["ICLR.cc/2019/Conference/Paper6/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper6/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper6/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "(Unconstrained) Beam Search is Sensitive to Large Search Discrepancies", "abstract": "Beam search is the most popular inference algorithm for decoding neural sequence models. Unlike greedy search, beam search allows for a non-greedy local decisions that can potentially lead to a sequence with a higher overall probability. However, previous work found that the performance of beam search tends to degrade with large beam widths. In this work, we perform an empirical study of the behavior of the beam search algorithm across three sequence synthesis tasks. We find that increasing the beam width leads to sequences that are disproportionately based on early and highly non-greedy decisions. These sequences typically include a very low probability token that is followed by a sequence of tokens with higher (conditional) probability leading to an overall higher probability sequence. However, as beam width increases, such sequences are more likely to have a lower evaluation score. Based on our empirical analysis we propose to constrain the beam search from taking highly non-greedy decisions early in the search. We evaluate two methods to constrain the search and show that constrained beam search effectively eliminates the problem of beam search degradation and in some cases even leads to higher evaluation scores. Our results generalize and improve upon previous observations on copies and training set predictions.", "paperhash": "cohen|unconstrained_beam_search_is_sensitive_to_large_search_discrepancies", "keywords": ["beam search", "sequence models", "search", "sequence to sequence"], "authorids": ["ecohen@mie.utoronto.ca", "jcb@mie.utoronto.ca"], "authors": ["Eldan Cohen", "J. Christopher Beck"], "TL;DR": "Analysis of the performance degradation in beam search and how constraining the the search can help avoiding it", "pdf": "/pdf/e5d14c2ac7a3b6de5f0815e706d0214f56b09c1a.pdf", "_bibtex": "@misc{\ncohen2019unconstrained,\ntitle={(Unconstrained) Beam Search is Sensitive to Large Search Discrepancies},\nauthor={Eldan Cohen and J. Christopher Beck},\nyear={2019},\nurl={https://openreview.net/forum?id=BkE8NjCqYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper6/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621612636, "tddate": null, "super": null, "final": null, "reply": {"forum": "BkE8NjCqYm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper6/Authors", "ICLR.cc/2019/Conference/Paper6/Reviewers", "ICLR.cc/2019/Conference/Paper6/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper6/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper6/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper6/Authors|ICLR.cc/2019/Conference/Paper6/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper6/Reviewers", "ICLR.cc/2019/Conference/Paper6/Authors", "ICLR.cc/2019/Conference/Paper6/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621612636}}}, {"id": "HygoNFt7a7", "original": null, "number": 1, "cdate": 1541802290704, "ddate": null, "tcdate": 1541802290704, "tmdate": 1541802290704, "tddate": null, "forum": "BkE8NjCqYm", "replyto": "BkE8NjCqYm", "invitation": "ICLR.cc/2019/Conference/-/Paper6/Official_Comment", "content": {"title": "Distinction between search errors and modeling errors not clear", "comment": "This paper investigates the problem of search degrading translation accuracy as measured by BLEU score.\n\nHowever, the paper seems to be conflating two fundamental issues, search errors and modeling errors. Search errors are errors where the search algorithm is not able to find the highest-scoring hypothesis, and modeling errors are errors where the highest-scoring hypothesis is actually not a good one according to the model.\n\nThe widely-known problem of BLEU (or other) scores degrading with larger beams is due to modeling errors: MLE-trained models tend to prefer shorter sentences, and this can be (largely) fixed by better modeling of length. The simplest method for doing so is length normalization, searching for the hypothesis that has the highest average likelihood per word rather than the highest likelihood overall per sentence (see \"On the Properties of Neural Machine Translation: Encoder\u2013Decoder Approaches\" SSST 2014). This largely fixes the problem of large beams degrading accuracy (see \"Six Challenges for Neural Machine Translation\" WNMT 2017), although there are other methods for length normalization as well.\n\nIn contrast, this paper attempts to indirectly fix the problem of modeling errors by changing the search algorithm. Hobbling the search algorithm seems like a rather indirect way to solve a problem that is essentially a modeling problem (and has already been largely fixed by other methods). In addition, the discussion seems pretty incomplete without a discussion of whether the search algorithm is actually achieving better model scores, which is the fundamental job of the search algorithm in the first place.\n\nIt would be nice to see a discussion of these issues in the author response if possible."}, "signatures": ["ICLR.cc/2019/Conference/Paper6/Area_Chair1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper6/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper6/Area_Chair1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "(Unconstrained) Beam Search is Sensitive to Large Search Discrepancies", "abstract": "Beam search is the most popular inference algorithm for decoding neural sequence models. Unlike greedy search, beam search allows for a non-greedy local decisions that can potentially lead to a sequence with a higher overall probability. However, previous work found that the performance of beam search tends to degrade with large beam widths. In this work, we perform an empirical study of the behavior of the beam search algorithm across three sequence synthesis tasks. We find that increasing the beam width leads to sequences that are disproportionately based on early and highly non-greedy decisions. These sequences typically include a very low probability token that is followed by a sequence of tokens with higher (conditional) probability leading to an overall higher probability sequence. However, as beam width increases, such sequences are more likely to have a lower evaluation score. Based on our empirical analysis we propose to constrain the beam search from taking highly non-greedy decisions early in the search. We evaluate two methods to constrain the search and show that constrained beam search effectively eliminates the problem of beam search degradation and in some cases even leads to higher evaluation scores. Our results generalize and improve upon previous observations on copies and training set predictions.", "paperhash": "cohen|unconstrained_beam_search_is_sensitive_to_large_search_discrepancies", "keywords": ["beam search", "sequence models", "search", "sequence to sequence"], "authorids": ["ecohen@mie.utoronto.ca", "jcb@mie.utoronto.ca"], "authors": ["Eldan Cohen", "J. Christopher Beck"], "TL;DR": "Analysis of the performance degradation in beam search and how constraining the the search can help avoiding it", "pdf": "/pdf/e5d14c2ac7a3b6de5f0815e706d0214f56b09c1a.pdf", "_bibtex": "@misc{\ncohen2019unconstrained,\ntitle={(Unconstrained) Beam Search is Sensitive to Large Search Discrepancies},\nauthor={Eldan Cohen and J. Christopher Beck},\nyear={2019},\nurl={https://openreview.net/forum?id=BkE8NjCqYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper6/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621612636, "tddate": null, "super": null, "final": null, "reply": {"forum": "BkE8NjCqYm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper6/Authors", "ICLR.cc/2019/Conference/Paper6/Reviewers", "ICLR.cc/2019/Conference/Paper6/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper6/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper6/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper6/Authors|ICLR.cc/2019/Conference/Paper6/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper6/Reviewers", "ICLR.cc/2019/Conference/Paper6/Authors", "ICLR.cc/2019/Conference/Paper6/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621612636}}}, {"id": "HyeSeu4jhQ", "original": null, "number": 3, "cdate": 1541257196794, "ddate": null, "tcdate": 1541257196794, "tmdate": 1541534367085, "tddate": null, "forum": "BkE8NjCqYm", "replyto": "BkE8NjCqYm", "invitation": "ICLR.cc/2019/Conference/-/Paper6/Official_Review", "content": {"title": "Interesting direction, although more work required", "review": "This paper addresses issues with the beam search decoding algorithm that is commonly applied to recurrent models during inference. In particular, the paper investigates why using larger beam widths, resulting in output sequences with higher log-probabilities, often leads to worse performance on evaluation metrics of interest such as BLEU. The paper argues that this effect is related to \u2018search discrepancies\u2019 (deviations from greedy choices early in decoding), and proposes a constrained decoding mechanism as a heuristic fix. \n\nStrengths:\n- The reduction in performance from using larger beam widths has been often reported and needs more investigation.\n- The paper views beam search decoding through the lens of heuristic and combinatorial search, and suggests an interesting connection with methods such as limited discrepancy search (Harvey and Ginsberg 1995) that seek to eliminate early \u2018wrong turns\u2019. \n- In most areas the paper is clear and well-written, although it may help to be more careful about explaining and / or defining terms such as \u2018highly non-greedy\u2019, \u2018search discrepancies\u2019 in the introduction. \n\nWeaknesses and suggestions for improvement:\n\n- Understanding: The paper does not offer much in the way of a deeper understanding of search discrepancies. For example, are search discrepancies caused by exposure bias or label bias, i.e. an artifact of local normalization at each time step during training, as suggested in the conclusion? Or are they actually a linguistic phenomenon (noting that English, French and German have common roots)? As there are neural network methods that attempt to do approximate global normalization (e.g. https://www.aclweb.org/anthology/P16-1231), there may be ways to investigate this question by looking at whether search discrepancies are reduced in these models (although I haven\u2019t looked deeply into this).\n\n- Evaluation: In the empirical evaluation, the results seem quite marginal. Taking the best performing beam size for the proposed method, and comparing the score to the best performing beam size for the baseline, the scores appear to be within around 1% for each task. Although the proposed method allows larger beam widths to be used without degradation during decoding, of course this is not actually beneficial unless the larger beam can improve the score. In the end, the evidence that search discrepancies are the cause of the problems with large beam widths, and therefore the best way to mitigate these problems, is not that strong.\n\n- Evaluation metrics and need for human evals: The limitations of automatic linguistic evaluations such as BLEU are well known. For image captioning, the SPICE (ECCV 2016 https://arxiv.org/abs/1607.08822) and CIDEr (CVPR 2015 https://arxiv.org/abs/1411.5726) metrics show much greater correlation with human judgements of caption quality, and should be reported in preference (or in addition) to BLEU. More generally, it is quite possible that the proposed fix based on constraining discrepancies could improve the generated output in the eyes of humans, even if this is not strongly reflected in automatic evaluation metrics. Therefore, it would be interesting to see human evaluations for the generated outputs in each task.  \n\n- Rare words: The authors reference Koehn and Knowles\u2019 (2017) six challenges for NMT, which includes beam search decoding. One of the other six challenges is low-frequency words. However, the impact of the proposed constrained decoding approach on the generation of rare words is not explored. It seems reasonable that limiting search discrepancies might also further limit the generation of rare words. Therefore, I would like to suggest that an analysis of the diversity of the generated outputs for each approach be included in the evaluation.\n\n- Constrained beam search: There is a bunch of prior work on constrained beam search. For example, an algorithm called constrained beam search was introduced at EMNLP 2017 (http://aclweb.org/anthology/D17-1098). This is a general algorithm for decoding RNNs with constraints defined by a finite state acceptor. Other works have also been proposed that are variations on this idea, e.g. http://aclweb.org/anthology/P17-1141, http://aclweb.org/anthology/N18-1119). It might be helpful to identify these in the related work section to help limit confusion when talking about this \u2018constrained beam search\u2019 algorithm.  \n\nMinor issues:\n- Section 3. The image captioning splits used by Xu et al. 2015 were actually first proposed by Karpathy & Li, \u2018Deep visual-semantic alignments for generating image descriptions\u2019, CVPR 2015, and should be cited as such. (Some papers actually refer to them as the \u2018Karpathy splits\u2019.)\n- In Table 4 it is somewhat difficult to interpret the comparison between the baseline results and the constrained beam search methods, because the best results appear in different columns. Bolding the highest score in every row would be helpful.\n\nSummary:\nIn summary, improving beam search is an important direction, and to the best of my knowledge the idea of looking at beam search through the lens of search discrepancies is novel. Having said, I don't feel that this paper in it's current form contributes very much to our understanding of RNN decoding, since it is not clear if search discrepancies are actually a problem. Limiting search discrepancies during decoding has minimal impact on BLEU scores, and it seems possible that search discrepancies could just be an aspect of linguistic structure. I rate this paper marginally below acceptance, although I would encourage the authors to keep working in this direction and have tried to provide some suggestions for improvement.   ", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper6/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "(Unconstrained) Beam Search is Sensitive to Large Search Discrepancies", "abstract": "Beam search is the most popular inference algorithm for decoding neural sequence models. Unlike greedy search, beam search allows for a non-greedy local decisions that can potentially lead to a sequence with a higher overall probability. However, previous work found that the performance of beam search tends to degrade with large beam widths. In this work, we perform an empirical study of the behavior of the beam search algorithm across three sequence synthesis tasks. We find that increasing the beam width leads to sequences that are disproportionately based on early and highly non-greedy decisions. These sequences typically include a very low probability token that is followed by a sequence of tokens with higher (conditional) probability leading to an overall higher probability sequence. However, as beam width increases, such sequences are more likely to have a lower evaluation score. Based on our empirical analysis we propose to constrain the beam search from taking highly non-greedy decisions early in the search. We evaluate two methods to constrain the search and show that constrained beam search effectively eliminates the problem of beam search degradation and in some cases even leads to higher evaluation scores. Our results generalize and improve upon previous observations on copies and training set predictions.", "paperhash": "cohen|unconstrained_beam_search_is_sensitive_to_large_search_discrepancies", "keywords": ["beam search", "sequence models", "search", "sequence to sequence"], "authorids": ["ecohen@mie.utoronto.ca", "jcb@mie.utoronto.ca"], "authors": ["Eldan Cohen", "J. Christopher Beck"], "TL;DR": "Analysis of the performance degradation in beam search and how constraining the the search can help avoiding it", "pdf": "/pdf/e5d14c2ac7a3b6de5f0815e706d0214f56b09c1a.pdf", "_bibtex": "@misc{\ncohen2019unconstrained,\ntitle={(Unconstrained) Beam Search is Sensitive to Large Search Discrepancies},\nauthor={Eldan Cohen and J. Christopher Beck},\nyear={2019},\nurl={https://openreview.net/forum?id=BkE8NjCqYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper6/Official_Review", "cdate": 1542234559389, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "BkE8NjCqYm", "replyto": "BkE8NjCqYm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper6/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335627834, "tmdate": 1552335627834, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper6/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "S1lH_kUq2Q", "original": null, "number": 2, "cdate": 1541197677007, "ddate": null, "tcdate": 1541197677007, "tmdate": 1541534366755, "tddate": null, "forum": "BkE8NjCqYm", "replyto": "BkE8NjCqYm", "invitation": "ICLR.cc/2019/Conference/-/Paper6/Official_Review", "content": {"title": "it is not right to do analysis on test set.", "review": "\nThis work does extensive experiments on three different text generation tasks and shows the relationship between wider beam degradation and more and larger early discrepancies. This is an interesting observation but the reason behind the scene are still unclear to me. A lot of the statements in the paper lack of theoretical analysis. \n\nThe proposed solutions addressing the beam discrepancies are effective, which further proves the relationship between beam size and early discrepancies. My questions/suggestions are as follows:\n* It\u2019s better to show the dataset statistics along with Fig1,3. So that readers know how much of test set have discrepancies in early steps.\n* It is not right to conduct your analysis on the test set. You have to be very clear about which results are from test set or dev set.\n* All the results with BLEU score must include the brevity penalty as well. It is very useful to analyze the length ratio changes between baseline, other methods, and your proposal.\n* The example in Sec. 4.6 is unclear to me, maybe you could illustrate it more clearly.\n* Your approaches eliminate the discrepancies along with the diversity with a wider beam. I am curious what if you only apply those constraints on early steps.\n* I suggest comparing your proposal to the word reward model in [1] since it is also about improving beam search quality. Your threshold-based method is also kind of word reward method.\n* In eq.2, what do you mean by sequence y \\in V? y is a sequence, V just a set of vocabulary.  What do you mean by P (y|x;{y_0..y_t}). Why the whole sequence y is conditioned on a prefix of y?\n\n[1] Huang et al, \"When to Finish? Optimal Beam Search for Neural Text Generation\" 2017", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper6/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "(Unconstrained) Beam Search is Sensitive to Large Search Discrepancies", "abstract": "Beam search is the most popular inference algorithm for decoding neural sequence models. Unlike greedy search, beam search allows for a non-greedy local decisions that can potentially lead to a sequence with a higher overall probability. However, previous work found that the performance of beam search tends to degrade with large beam widths. In this work, we perform an empirical study of the behavior of the beam search algorithm across three sequence synthesis tasks. We find that increasing the beam width leads to sequences that are disproportionately based on early and highly non-greedy decisions. These sequences typically include a very low probability token that is followed by a sequence of tokens with higher (conditional) probability leading to an overall higher probability sequence. However, as beam width increases, such sequences are more likely to have a lower evaluation score. Based on our empirical analysis we propose to constrain the beam search from taking highly non-greedy decisions early in the search. We evaluate two methods to constrain the search and show that constrained beam search effectively eliminates the problem of beam search degradation and in some cases even leads to higher evaluation scores. Our results generalize and improve upon previous observations on copies and training set predictions.", "paperhash": "cohen|unconstrained_beam_search_is_sensitive_to_large_search_discrepancies", "keywords": ["beam search", "sequence models", "search", "sequence to sequence"], "authorids": ["ecohen@mie.utoronto.ca", "jcb@mie.utoronto.ca"], "authors": ["Eldan Cohen", "J. Christopher Beck"], "TL;DR": "Analysis of the performance degradation in beam search and how constraining the the search can help avoiding it", "pdf": "/pdf/e5d14c2ac7a3b6de5f0815e706d0214f56b09c1a.pdf", "_bibtex": "@misc{\ncohen2019unconstrained,\ntitle={(Unconstrained) Beam Search is Sensitive to Large Search Discrepancies},\nauthor={Eldan Cohen and J. Christopher Beck},\nyear={2019},\nurl={https://openreview.net/forum?id=BkE8NjCqYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper6/Official_Review", "cdate": 1542234559389, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "BkE8NjCqYm", "replyto": "BkE8NjCqYm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper6/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335627834, "tmdate": 1552335627834, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper6/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "S1gTFivu3m", "original": null, "number": 1, "cdate": 1541073796984, "ddate": null, "tcdate": 1541073796984, "tmdate": 1541534366547, "tddate": null, "forum": "BkE8NjCqYm", "replyto": "BkE8NjCqYm", "invitation": "ICLR.cc/2019/Conference/-/Paper6/Official_Review", "content": {"title": "A thorough analysis of (and two heuristic solutions to) the failures of beam search when applied to modern neural models", "review": "Pros:\n- The paper generalizes upon past observations by Ott et al. that NMT models might decode \"copies\" (of the source sentence) when using large beam widths, which results in degraded results. In particular, the present paper observes similar shortcomings in two additional tasks (summarization and captioning), where decoding with large beam widths results in \"training set predictions.\" It's unclear if this observation is novel, but in any case the connection between these observations across NMT and summarization/captioning tasks is novel.\n- The paper draws a connection between the observed degradation and \"label bias\", whereby prefixes with a low likelihood are selected merely because they lead to (nearly-)deterministic transitions later in decoding.\n- The paper suggests two simple heuristics for mitigating the observed degradation with large beam widths, and evaluates these heuristics across three tasks. The results are convincing.\n- The paper is very well written. The analysis throughout the paper is easy to follow and convincing.\n\nCons:\n- Although the analysis is very valuable, the quantitive impact of the proposed heuristics is relatively minor.\n\nComments/questions:\n- In Eq. 2, consider using $v$ or $w$ for the max instead of overloading $y$.\n- To save space, you might compress Figure 1 into a single figure with three differently-styled bars per position that indicate the beam width (somewhat like how Figure 3 is presented). You can do this for Figure 2 as well, and these compressed figures could then be collapsed into a single row.\n- In Section 5, when describing the \"Discrepancy gap\" constraint, you say that you \"modify Eq. 3 to include the constraint\", but I suspect you meant that you modify Eq. 1 to include this constraint.\n- In Table 4, why didn't you tune $\\mathcal{M}$ and $\\mathcal{N}$ separately for each beam width?", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper6/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "(Unconstrained) Beam Search is Sensitive to Large Search Discrepancies", "abstract": "Beam search is the most popular inference algorithm for decoding neural sequence models. Unlike greedy search, beam search allows for a non-greedy local decisions that can potentially lead to a sequence with a higher overall probability. However, previous work found that the performance of beam search tends to degrade with large beam widths. In this work, we perform an empirical study of the behavior of the beam search algorithm across three sequence synthesis tasks. We find that increasing the beam width leads to sequences that are disproportionately based on early and highly non-greedy decisions. These sequences typically include a very low probability token that is followed by a sequence of tokens with higher (conditional) probability leading to an overall higher probability sequence. However, as beam width increases, such sequences are more likely to have a lower evaluation score. Based on our empirical analysis we propose to constrain the beam search from taking highly non-greedy decisions early in the search. We evaluate two methods to constrain the search and show that constrained beam search effectively eliminates the problem of beam search degradation and in some cases even leads to higher evaluation scores. Our results generalize and improve upon previous observations on copies and training set predictions.", "paperhash": "cohen|unconstrained_beam_search_is_sensitive_to_large_search_discrepancies", "keywords": ["beam search", "sequence models", "search", "sequence to sequence"], "authorids": ["ecohen@mie.utoronto.ca", "jcb@mie.utoronto.ca"], "authors": ["Eldan Cohen", "J. Christopher Beck"], "TL;DR": "Analysis of the performance degradation in beam search and how constraining the the search can help avoiding it", "pdf": "/pdf/e5d14c2ac7a3b6de5f0815e706d0214f56b09c1a.pdf", "_bibtex": "@misc{\ncohen2019unconstrained,\ntitle={(Unconstrained) Beam Search is Sensitive to Large Search Discrepancies},\nauthor={Eldan Cohen and J. Christopher Beck},\nyear={2019},\nurl={https://openreview.net/forum?id=BkE8NjCqYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper6/Official_Review", "cdate": 1542234559389, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "BkE8NjCqYm", "replyto": "BkE8NjCqYm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper6/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335627834, "tmdate": 1552335627834, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper6/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 15}