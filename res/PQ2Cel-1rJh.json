{"notes": [{"id": "PQ2Cel-1rJh", "original": "FUiBm2Zxs0", "number": 3113, "cdate": 1601308345298, "ddate": null, "tcdate": 1601308345298, "tmdate": 1614985734948, "tddate": null, "forum": "PQ2Cel-1rJh", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Pea-KD: Parameter-efficient and accurate Knowledge Distillation", "authorids": ["~IKHYUN_CHO1", "~U_Kang1"], "authors": ["IKHYUN CHO", "U Kang"], "keywords": ["BERT", "Deep Learning", "Natural Language Processing", "Transformer", "Knowledge Distillation", "Parameter Sharing"], "abstract": "How can we efficiently compress a model while maintaining its performance?  Knowledge Distillation (KD) is one of the widely known methods for model compression. In essence, KD trains a smaller student model based on a larger teacher model and tries to retain the teacher model's level of performance as much as possible. However, the existing KD methods suffer from the following limitations. First, since the student model is small in absolute size, it inherently lacks model complexity. Second, the absence of an initial guide for the student model makes it difficult for the student to imitate the teacher model to its fullest. Conventional KD methods yield low performance due to these limitations.\n\nIn this paper, we propose Pea-KD (Parameter-efficient and accurate Knowledge Distillation), a novel approach to KD. Pea-KD consists of two main parts: Shuffled Parameter Sharing (SPS) and Pretraining with Teacher's Predictions (PTP). Using this combination, we are capable of alleviating the KD's limitations. SPS is a new parameter sharing method that allows greater model complexity for the student model. PTP is a KD-specialized initialization method, which can act as a good initial guide for the student. When combined, this method yields a significant increase in student model's performance. Experiments conducted on different datasets and tasks show that the proposed approach improves the student model's performance by 4.4% on average in four GLUE tasks, outperforming existing KD baselines by significant margins.\n", "one-sentence_summary": "It introduces a new Knowledge Distillation method. It improves the performance of the student model with 2 main modules:  novel parameter sharing and new pretraining which uses the teacher model's predictions. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "cho|peakd_parameterefficient_and_accurate_knowledge_distillation", "supplementary_material": "/attachment/6ba2955df8bb8f79fe91bfbfc55d8343c3e74e7a.zip", "pdf": "/pdf/73e1fa211b7a950aef7587202346d058d92c3558.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=pgcw7UOeGy", "_bibtex": "@misc{\ncho2021peakd,\ntitle={Pea-{\\{}KD{\\}}: Parameter-efficient and accurate Knowledge Distillation},\nauthor={IKHYUN CHO and U Kang},\nyear={2021},\nurl={https://openreview.net/forum?id=PQ2Cel-1rJh}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 13, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "WsbmlEoft-Q", "original": null, "number": 1, "cdate": 1610040403355, "ddate": null, "tcdate": 1610040403355, "tmdate": 1610473999566, "tddate": null, "forum": "PQ2Cel-1rJh", "replyto": "PQ2Cel-1rJh", "invitation": "ICLR.cc/2021/Conference/Paper3113/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "This paper got mixed reviews. One for acceptance, one for reject and two borderline. After the rebuttal, AR2 raises the review to borderline.  AR1 gives the highest recommendation but did not provide detailed supporting evidence. Other reviews provide comment on the strength and also share the concerns. Most of the concerns concentrate on the motivation (whether the proposed method is violating the objective of knowledge distillation) and the brought additional computation overhead. Also the scope of this paper was defined wider than the actual one. The authors only did experiments for BERT but did not consider and compare with existing KD method. Overall, AC read the paper and also has the similar concerns, the novelty is limited. the brought increase in inference time is violating the KD objective and the scope of this paper was not defined clearly. The authors should improve the submission in these aspects. At its current status, AC does not recommend acceptance. "}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pea-KD: Parameter-efficient and accurate Knowledge Distillation", "authorids": ["~IKHYUN_CHO1", "~U_Kang1"], "authors": ["IKHYUN CHO", "U Kang"], "keywords": ["BERT", "Deep Learning", "Natural Language Processing", "Transformer", "Knowledge Distillation", "Parameter Sharing"], "abstract": "How can we efficiently compress a model while maintaining its performance?  Knowledge Distillation (KD) is one of the widely known methods for model compression. In essence, KD trains a smaller student model based on a larger teacher model and tries to retain the teacher model's level of performance as much as possible. However, the existing KD methods suffer from the following limitations. First, since the student model is small in absolute size, it inherently lacks model complexity. Second, the absence of an initial guide for the student model makes it difficult for the student to imitate the teacher model to its fullest. Conventional KD methods yield low performance due to these limitations.\n\nIn this paper, we propose Pea-KD (Parameter-efficient and accurate Knowledge Distillation), a novel approach to KD. Pea-KD consists of two main parts: Shuffled Parameter Sharing (SPS) and Pretraining with Teacher's Predictions (PTP). Using this combination, we are capable of alleviating the KD's limitations. SPS is a new parameter sharing method that allows greater model complexity for the student model. PTP is a KD-specialized initialization method, which can act as a good initial guide for the student. When combined, this method yields a significant increase in student model's performance. Experiments conducted on different datasets and tasks show that the proposed approach improves the student model's performance by 4.4% on average in four GLUE tasks, outperforming existing KD baselines by significant margins.\n", "one-sentence_summary": "It introduces a new Knowledge Distillation method. It improves the performance of the student model with 2 main modules:  novel parameter sharing and new pretraining which uses the teacher model's predictions. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "cho|peakd_parameterefficient_and_accurate_knowledge_distillation", "supplementary_material": "/attachment/6ba2955df8bb8f79fe91bfbfc55d8343c3e74e7a.zip", "pdf": "/pdf/73e1fa211b7a950aef7587202346d058d92c3558.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=pgcw7UOeGy", "_bibtex": "@misc{\ncho2021peakd,\ntitle={Pea-{\\{}KD{\\}}: Parameter-efficient and accurate Knowledge Distillation},\nauthor={IKHYUN CHO and U Kang},\nyear={2021},\nurl={https://openreview.net/forum?id=PQ2Cel-1rJh}\n}"}, "tags": [], "invitation": {"reply": {"forum": "PQ2Cel-1rJh", "replyto": "PQ2Cel-1rJh", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040403342, "tmdate": 1610473999548, "id": "ICLR.cc/2021/Conference/Paper3113/-/Decision"}}}, {"id": "JhWFWPoSzy", "original": null, "number": 2, "cdate": 1603712413311, "ddate": null, "tcdate": 1603712413311, "tmdate": 1606964384011, "tddate": null, "forum": "PQ2Cel-1rJh", "replyto": "PQ2Cel-1rJh", "invitation": "ICLR.cc/2021/Conference/Paper3113/-/Official_Review", "content": {"title": "The paper is poorly written. The motivation and the proposed method are problematic.", "review": "This paper proposes a distillation method for BERT. The work is based on two-fold main ideas. First, as the student model is usually smaller in the number of parameters, the model capacity is limited. The authors propose to stack the layers that share parameters to counter this limitation. Second, the authors argue that the initialization of the student model is crucial, so they propose an pre-training strategy for boosting the student's performance.\n\nPros:\nThe idea of learning good initializations for the student model is interesting.\n\n\nCons:\nThis paper has several writing problems which need be carefully addressed before its publication.\n\n- As all the experiments are conducted on GLUE and only BERT models are considered in the paper, the proposed method seems to be tailored for BERT. The authors should stress this point clearly and early in the paper (in the title, abstract or introduction). Otherwise, the authors should provide some experimental results on other models or tasks, e.g., the typical image recognition task in computer vision.\n\n- In the section of PROPOSED METHODs (section 3.1), the motivation and the main idea of the paper are introduced again. As no any new information is provided here compared to the abstraction and the introduction sections, it seems very redundant.\n\n- The introduction of SPS in Section 3.2 is quite confusing. What do Key  and Query parameters stand for?  I have no idea what the author is talking about here. Maybe it is because I have little background in NLP and BERT. However, even so the authors are still responsible for making the paper easy to follow for readers like me.\n\n\nAs for the motivation and the proposed method, my comments are as follows.\n\n- The motivation and the proposed method are somewhat problematic. Firstly, the authors argue that small student with few parameters are limited in capacity, so they propose to stack repeating layers to enlarge the model capacity. However, stacking repeating layer will introduce much more computation cost, which violates the goal of distillation.\n\n-  Secondly, the authors propose to pre-train the student model with the teacher predictions to initialize the student. However, it is odd to view this step as pre-training as it actually adopts the teacher predictions to train the student. It is actually doing distillation! The improvement  in performance may simply come from the more training epochs.\n\n======================\n\npost-rebuttal:\n\nI have read all the comments from other reviewers and replies from the authors. The revised version partially addresses my concerns so I raise the score from 4 to 5. However, my concerns about the motivation of the work still exist, so I am still slightly leaning to reject this paper.\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3113/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3113/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pea-KD: Parameter-efficient and accurate Knowledge Distillation", "authorids": ["~IKHYUN_CHO1", "~U_Kang1"], "authors": ["IKHYUN CHO", "U Kang"], "keywords": ["BERT", "Deep Learning", "Natural Language Processing", "Transformer", "Knowledge Distillation", "Parameter Sharing"], "abstract": "How can we efficiently compress a model while maintaining its performance?  Knowledge Distillation (KD) is one of the widely known methods for model compression. In essence, KD trains a smaller student model based on a larger teacher model and tries to retain the teacher model's level of performance as much as possible. However, the existing KD methods suffer from the following limitations. First, since the student model is small in absolute size, it inherently lacks model complexity. Second, the absence of an initial guide for the student model makes it difficult for the student to imitate the teacher model to its fullest. Conventional KD methods yield low performance due to these limitations.\n\nIn this paper, we propose Pea-KD (Parameter-efficient and accurate Knowledge Distillation), a novel approach to KD. Pea-KD consists of two main parts: Shuffled Parameter Sharing (SPS) and Pretraining with Teacher's Predictions (PTP). Using this combination, we are capable of alleviating the KD's limitations. SPS is a new parameter sharing method that allows greater model complexity for the student model. PTP is a KD-specialized initialization method, which can act as a good initial guide for the student. When combined, this method yields a significant increase in student model's performance. Experiments conducted on different datasets and tasks show that the proposed approach improves the student model's performance by 4.4% on average in four GLUE tasks, outperforming existing KD baselines by significant margins.\n", "one-sentence_summary": "It introduces a new Knowledge Distillation method. It improves the performance of the student model with 2 main modules:  novel parameter sharing and new pretraining which uses the teacher model's predictions. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "cho|peakd_parameterefficient_and_accurate_knowledge_distillation", "supplementary_material": "/attachment/6ba2955df8bb8f79fe91bfbfc55d8343c3e74e7a.zip", "pdf": "/pdf/73e1fa211b7a950aef7587202346d058d92c3558.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=pgcw7UOeGy", "_bibtex": "@misc{\ncho2021peakd,\ntitle={Pea-{\\{}KD{\\}}: Parameter-efficient and accurate Knowledge Distillation},\nauthor={IKHYUN CHO and U Kang},\nyear={2021},\nurl={https://openreview.net/forum?id=PQ2Cel-1rJh}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "PQ2Cel-1rJh", "replyto": "PQ2Cel-1rJh", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3113/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538081986, "tmdate": 1606915773536, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3113/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3113/-/Official_Review"}}}, {"id": "XgmNsnwOauR", "original": null, "number": 8, "cdate": 1606227825843, "ddate": null, "tcdate": 1606227825843, "tmdate": 1606291679783, "tddate": null, "forum": "PQ2Cel-1rJh", "replyto": "1i0kSUhZtL-", "invitation": "ICLR.cc/2021/Conference/Paper3113/-/Official_Comment", "content": {"title": "Response to Reviewer # 4 - general thanks and comments (2/3)", "comment": "\n(continued)\n\nAs you mentioned, despite the drawback of greater inference time, the underlying techniques of SPS provide a new approach to thinking about Parameter Sharing. To the best of our knowledge, this is the first approach in Parameter Sharing that uses shared internal parameters in different positions in order to train parameters more efficiently. We believe SPS is meaningful, because it has shown considerable and consistent performance improvement in a relatively intuitive manner. Due to its simplicity, we expect SPS to be widely applicable to many models that apply parameter sharing. \n\n[Comment 4]\nIntroduction:\n\"stacking the layers that share parameters\" => \"stacking layers that share parameters\" (the former form of language is using 'that share parameters' as a filter to choose from existing layers; the latter form implies that we create new layers, which we then stack, and which we configure to have shared parameters)\n\n=> Thank you for the correction. We updated the statement accordingly.\n\n[Comment 5] \n\" 3.3 PTP\nI feel that the column for 'PTP label' in table 1 should be on the right hand side:\n\u2022\tthe inputs should be I feel on the left, ie 'teachers prediction correct', and 'confidence > t',\n\u2022\tand then we can read off the 'output' of the table as the right-hand column\n\u2022\t(currently I find the table hard to read) \" \n\n=>  Thank you. We made modifications based on your comments.\n\n[Comment 6] \"The pretraining task itself seems interesting to me.\nHow do you get the student to predict this result? Do you put a linear layer on the output from the student?\"\n\n=> We used a linear layer on top of BERT to classify the PTP labels.\n\n[Comment 7] \"Why do you use a classification task (4 classes) instead of using a regression task to predict the confidence? did you try a regression task to predict the confidence? Why use a classification into 4 classes, rather than two classifiers into two classes each?\" \n\n=> The starting point of creating the PTP labels focused on the teacher model\u2019s correctness. We initially started with just one set of binary labels denoting teacher's correctness. It was not until later on in our research that we came up with the idea of adding in confidence-based labels. This is how we decided on the four classes.\nUsing a regression task also seems like a great idea. However, we would be predicting only the level of confidence not the correctness. Because we started from correctness labels, we had not thought of using regression yet. We did not have enough time to incorporate this feedback into our paper but will certainly try this for future research. The same for using two classifiers also. We will try this in the future work. \n\n[Comment 8] \"I kind of feel that I might not be the only person who might want to see the PeaBERT numbers in the context of such lower and upperbound baselines? Maybe consider adding these baselinse into table 2?\" \n\n=> We think it is a good idea. Since we are working on BERT compression, adding in BERT-base as baseline will make the comparison more robust. We have added this to Table 2 in section 4.2 of our paper. Thank you for the feedback.\n\n[Comment 9] \"Basically, my take away from this is that PeaBERT1 is barely better than the simple BiLSTM+Attn baseline, but PeaBERT3 approaches the score of a full BERT-base model?\"\n\n=> We are not sure if we agree with this comment. Even though PeaBERT3 performs much better than BERT3 we do not agree with the opinion that PeaBERT3 reaches the score of full BERT-base model. The performance gap between PeaBERT3 and BERT-base is 1.9\\% for RTE, 3.9\\% for MRPC, 3.1\\% for SST2, and 3.5\\% for QNLI. From our experience, once BERT model reaches a certain level of accuracy, it is extremely difficult to achieve even one percent of improvement, so we believe the current gap between our PeaBERT3 and BERT-base is acceptable. \n\n[Comment 10] \"why only show results on a subset of the GLUE tasks?\" \n\n=> There are several reasons why we assume the four datasets are actually quite sufficient for our case. First of all, PeaBERT uses both SPS and PTP to improve its performance. However, to apply PTP it has to be a classification task since the labels are constructed from the information of whether the teacher predicts correctly or not. QQP is not a classification task so we excluded it. Also, for MNLI-m and MNLI-mm they are similar to the tasks QNLI and RTE that they are all Natural Language Inference tasks. Therefore, we chose QNLI and RTE instead of MNLI considering the dataset size. However, we appreciate your suggestion to examine the method with more tasks and will try our best to do additional experiments.\n "}, "signatures": ["ICLR.cc/2021/Conference/Paper3113/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3113/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pea-KD: Parameter-efficient and accurate Knowledge Distillation", "authorids": ["~IKHYUN_CHO1", "~U_Kang1"], "authors": ["IKHYUN CHO", "U Kang"], "keywords": ["BERT", "Deep Learning", "Natural Language Processing", "Transformer", "Knowledge Distillation", "Parameter Sharing"], "abstract": "How can we efficiently compress a model while maintaining its performance?  Knowledge Distillation (KD) is one of the widely known methods for model compression. In essence, KD trains a smaller student model based on a larger teacher model and tries to retain the teacher model's level of performance as much as possible. However, the existing KD methods suffer from the following limitations. First, since the student model is small in absolute size, it inherently lacks model complexity. Second, the absence of an initial guide for the student model makes it difficult for the student to imitate the teacher model to its fullest. Conventional KD methods yield low performance due to these limitations.\n\nIn this paper, we propose Pea-KD (Parameter-efficient and accurate Knowledge Distillation), a novel approach to KD. Pea-KD consists of two main parts: Shuffled Parameter Sharing (SPS) and Pretraining with Teacher's Predictions (PTP). Using this combination, we are capable of alleviating the KD's limitations. SPS is a new parameter sharing method that allows greater model complexity for the student model. PTP is a KD-specialized initialization method, which can act as a good initial guide for the student. When combined, this method yields a significant increase in student model's performance. Experiments conducted on different datasets and tasks show that the proposed approach improves the student model's performance by 4.4% on average in four GLUE tasks, outperforming existing KD baselines by significant margins.\n", "one-sentence_summary": "It introduces a new Knowledge Distillation method. It improves the performance of the student model with 2 main modules:  novel parameter sharing and new pretraining which uses the teacher model's predictions. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "cho|peakd_parameterefficient_and_accurate_knowledge_distillation", "supplementary_material": "/attachment/6ba2955df8bb8f79fe91bfbfc55d8343c3e74e7a.zip", "pdf": "/pdf/73e1fa211b7a950aef7587202346d058d92c3558.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=pgcw7UOeGy", "_bibtex": "@misc{\ncho2021peakd,\ntitle={Pea-{\\{}KD{\\}}: Parameter-efficient and accurate Knowledge Distillation},\nauthor={IKHYUN CHO and U Kang},\nyear={2021},\nurl={https://openreview.net/forum?id=PQ2Cel-1rJh}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "PQ2Cel-1rJh", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3113/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3113/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3113/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3113/Authors|ICLR.cc/2021/Conference/Paper3113/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3113/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923840963, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3113/-/Official_Comment"}}}, {"id": "0EWHBhG6Sis", "original": null, "number": 9, "cdate": 1606228666732, "ddate": null, "tcdate": 1606228666732, "tmdate": 1606228666732, "tddate": null, "forum": "PQ2Cel-1rJh", "replyto": "XgmNsnwOauR", "invitation": "ICLR.cc/2021/Conference/Paper3113/-/Official_Comment", "content": {"title": "Response to Reviewer # 4 - general thanks and comments (3/3)", "comment": "\n[Comment 11 & 12] \"For ablation studies, I feel it would be interesting to see an ablation study that removes each of various losses in equation 3 in turn.\" & \" Nor was a rigorous comparison carried out to compare using just this pre-training approach with just the softmax-matching approach, where both using appropriately tuned hyper-parameters, I feel.\"\n\n=> In your comments, you had suggested including an ablation study that removes each loss terms in the equation (3) (equation (5) in the revised version of the paper). This would be comparing PatientKD, KD, and finetuning (FT) baselines. We did not do this study by ourselves, because this study is included in the Patient Knowledge Distillation paper. We thought it would not add much value to replicate this study. Rather, we thought it would be insightful to compare (a) 'BERT+PKD' with (b) BERT+PTP+FT'. Since the teacher\u2019s knowledge is not used in the finetuning process of (b), we were able to examine whether PTP is fulfilling our intention of helping the student learn the teacher\u2019s generalized knowledge. The results are attached below, where the scores are measured by an average of 5 runs with random seeds for each dataset. \n\nFrom the results summarized in the table below, we can see that applying PTP and then finetuning gives better results than the conventional PatientKD by a margin of 0.5\\% in accuracy. \n\nThis supports that PTP is a better way of using the teacher\u2019s softmax output than the conventional KD baselines! \n\nThis result also verifies that PTP works exactly as our motivation: student can learn the generalized teacher\u2019s knowledge more easily. \n\n\n|models|MRPC|RTE|SST-2|QNLI|AVG|\n|:----|:----|:---|:----|:-----|:-----|\n|BERT$_{3}$+PatientKD|   84. 6| 61.9|88.6|84.9|80.0|\n|BERT$_{3}$+PTP+FT| 85.3 | 62.5 | 89.4 | 84.9| 80.5|\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3113/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3113/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pea-KD: Parameter-efficient and accurate Knowledge Distillation", "authorids": ["~IKHYUN_CHO1", "~U_Kang1"], "authors": ["IKHYUN CHO", "U Kang"], "keywords": ["BERT", "Deep Learning", "Natural Language Processing", "Transformer", "Knowledge Distillation", "Parameter Sharing"], "abstract": "How can we efficiently compress a model while maintaining its performance?  Knowledge Distillation (KD) is one of the widely known methods for model compression. In essence, KD trains a smaller student model based on a larger teacher model and tries to retain the teacher model's level of performance as much as possible. However, the existing KD methods suffer from the following limitations. First, since the student model is small in absolute size, it inherently lacks model complexity. Second, the absence of an initial guide for the student model makes it difficult for the student to imitate the teacher model to its fullest. Conventional KD methods yield low performance due to these limitations.\n\nIn this paper, we propose Pea-KD (Parameter-efficient and accurate Knowledge Distillation), a novel approach to KD. Pea-KD consists of two main parts: Shuffled Parameter Sharing (SPS) and Pretraining with Teacher's Predictions (PTP). Using this combination, we are capable of alleviating the KD's limitations. SPS is a new parameter sharing method that allows greater model complexity for the student model. PTP is a KD-specialized initialization method, which can act as a good initial guide for the student. When combined, this method yields a significant increase in student model's performance. Experiments conducted on different datasets and tasks show that the proposed approach improves the student model's performance by 4.4% on average in four GLUE tasks, outperforming existing KD baselines by significant margins.\n", "one-sentence_summary": "It introduces a new Knowledge Distillation method. It improves the performance of the student model with 2 main modules:  novel parameter sharing and new pretraining which uses the teacher model's predictions. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "cho|peakd_parameterefficient_and_accurate_knowledge_distillation", "supplementary_material": "/attachment/6ba2955df8bb8f79fe91bfbfc55d8343c3e74e7a.zip", "pdf": "/pdf/73e1fa211b7a950aef7587202346d058d92c3558.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=pgcw7UOeGy", "_bibtex": "@misc{\ncho2021peakd,\ntitle={Pea-{\\{}KD{\\}}: Parameter-efficient and accurate Knowledge Distillation},\nauthor={IKHYUN CHO and U Kang},\nyear={2021},\nurl={https://openreview.net/forum?id=PQ2Cel-1rJh}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "PQ2Cel-1rJh", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3113/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3113/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3113/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3113/Authors|ICLR.cc/2021/Conference/Paper3113/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3113/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923840963, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3113/-/Official_Comment"}}}, {"id": "1i0kSUhZtL-", "original": null, "number": 7, "cdate": 1606224281225, "ddate": null, "tcdate": 1606224281225, "tmdate": 1606227841986, "tddate": null, "forum": "PQ2Cel-1rJh", "replyto": "3TKjVowcbP8", "invitation": "ICLR.cc/2021/Conference/Paper3113/-/Official_Comment", "content": {"title": "Response to Reviewer # 4 - general thanks and comments (1/3)", "comment": "\n[Comment 1]  \"the approaches used in the paper feel to me only weakly motivated. Little insight is given into why the approaches were chosen, and why they should work. For example, flipping the keys and queries was not well justified I felt. Nor was the PTP task relative to traditional softmax matching.\"\n\n=> SPS Step 2 (Shuffling)\n\nWe believe the main factor contributing to the power of SPS step 2 is as follows:\n\nMaximizing features learned by parameters by taking on different roles within the model.\n\nTaking the BERT$_{3}$ case as an example, let us look at the difference between SPS-1 and SPS-2 and how that contributes to the learning process of the parameters. As a recap, while SPS-1 applies only step1 to BERT, SPS-2 applies both step 1 and step 2, shuffling the parameters around.\nConsider the first layer's Query parameter. Under SPS-1, this parameter is used as a Query parameter in both the first layer and in the shared fourth layer. Since this parameter is used only as Query, it will learn only the information relevant to Query.\nUnder SPS-2, however, the first layer\u2019s Query parameter\u2019s function changes due to shuffling. The first layer\u2019s Query parameter is used as a Key parameter in the shared fourth layer. This one parameter has had the opportunity to learn the important features of both the Query and the Key functions, gaining a more diverse and wider breadth of knowledge. Based on the average accuracy increase of 1.9 percent driven by shuffling (Table 4 of the paper), we believe that the shuffled parameters were able to learn a more diverse set of information, and this improvement in parameter efficiency contributed to enriching model capacity. We also see this in a similar but slightly different point of view. Since the parameters get to learn diverse features and get to function in diverse positions we believe this could act as an additional regularization. Therefore it helps the model prevent overfitting and leads to improvement in performance.\n\n=> PTP\n\nThe core idea is to make PTP labels by explicitly expressing important information from the teachers\u2019 softmax outputs, such as whether the teacher model predicted correctly or how confident the teacher model is. Pretraining using these labels would help the student acquire the teacher\u2019s generalized knowledge latent in the teacher\u2019s softmax output more easily. This pretraining makes the student model better prepared for the actual KD process. For example, if a teacher makes an incorrect prediction for a data instance x, then we also know that x is generally a difficult one to predict. Since this knowledge is obtainable only by directly comparing the true label with the teachers output, it would be difficult for the student to acquire this in the conventional KD process. Representing this type of information through PTP labels and training the student to predict them could help the student acquire deeply latent knowledge included in the teacher\u2019s output much easily. Intuitively, we expect that a student model that has undergone such a pretraining session is better prepared for the actual KD process and will likely achieve better results.\n\n[Comment 2 & 3]  \"the paper avoids discussing inference times, and it's unclear to me whether a distillation approach that results in few parameters but long inference times would be useful in practice?\"\n\n\u2022\t of course, the underlying techniques can still be interesting from a theoretical point of view\n\n&\n\nSo, it looks like SPS means the number of unique parameters is kept low whilst the effective model complexity is unchanged compared to original BERT. How does this affect inference speed? It sounds to me like the inference speed under SPS will be unchanged from a BERT model of equivalent model complexity? Often, a use-case of distillation is to reduce inference time, and requirements in terms of power, eg for mobile devices, or production inference. Assuming that the inference time is unchanged from the original model of equal complexity, what is the target use-case of SPS/Pea-KD?\n\n=> Our primary focus is on improving performance without needing additional memory resources. Of the many important factors that we consider in model compression, such as memory storage, performance, and inference time, some factors are prioritized over others depending on the circumstances. In our paper, we prioritized maximum performance improvement while keeping memory constant, over speed. As you have pointed out, the SPS method has a drawback of additional inference time. However, despite the longer inference time, our model does perform significantly better. We believe that the additional time could be acceptable in certain cases where the performance is much more important than inference time, with limited memory storage. We do acknowledge that the increased inference time is an important limitation of our SPS method and will certainly aim to reduce this in our future work.\n\n(Continued)"}, "signatures": ["ICLR.cc/2021/Conference/Paper3113/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3113/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pea-KD: Parameter-efficient and accurate Knowledge Distillation", "authorids": ["~IKHYUN_CHO1", "~U_Kang1"], "authors": ["IKHYUN CHO", "U Kang"], "keywords": ["BERT", "Deep Learning", "Natural Language Processing", "Transformer", "Knowledge Distillation", "Parameter Sharing"], "abstract": "How can we efficiently compress a model while maintaining its performance?  Knowledge Distillation (KD) is one of the widely known methods for model compression. In essence, KD trains a smaller student model based on a larger teacher model and tries to retain the teacher model's level of performance as much as possible. However, the existing KD methods suffer from the following limitations. First, since the student model is small in absolute size, it inherently lacks model complexity. Second, the absence of an initial guide for the student model makes it difficult for the student to imitate the teacher model to its fullest. Conventional KD methods yield low performance due to these limitations.\n\nIn this paper, we propose Pea-KD (Parameter-efficient and accurate Knowledge Distillation), a novel approach to KD. Pea-KD consists of two main parts: Shuffled Parameter Sharing (SPS) and Pretraining with Teacher's Predictions (PTP). Using this combination, we are capable of alleviating the KD's limitations. SPS is a new parameter sharing method that allows greater model complexity for the student model. PTP is a KD-specialized initialization method, which can act as a good initial guide for the student. When combined, this method yields a significant increase in student model's performance. Experiments conducted on different datasets and tasks show that the proposed approach improves the student model's performance by 4.4% on average in four GLUE tasks, outperforming existing KD baselines by significant margins.\n", "one-sentence_summary": "It introduces a new Knowledge Distillation method. It improves the performance of the student model with 2 main modules:  novel parameter sharing and new pretraining which uses the teacher model's predictions. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "cho|peakd_parameterefficient_and_accurate_knowledge_distillation", "supplementary_material": "/attachment/6ba2955df8bb8f79fe91bfbfc55d8343c3e74e7a.zip", "pdf": "/pdf/73e1fa211b7a950aef7587202346d058d92c3558.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=pgcw7UOeGy", "_bibtex": "@misc{\ncho2021peakd,\ntitle={Pea-{\\{}KD{\\}}: Parameter-efficient and accurate Knowledge Distillation},\nauthor={IKHYUN CHO and U Kang},\nyear={2021},\nurl={https://openreview.net/forum?id=PQ2Cel-1rJh}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "PQ2Cel-1rJh", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3113/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3113/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3113/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3113/Authors|ICLR.cc/2021/Conference/Paper3113/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3113/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923840963, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3113/-/Official_Comment"}}}, {"id": "0wM6Won5oKp", "original": null, "number": 6, "cdate": 1606222989819, "ddate": null, "tcdate": 1606222989819, "tmdate": 1606222989819, "tddate": null, "forum": "PQ2Cel-1rJh", "replyto": "PHmFQdn4BI", "invitation": "ICLR.cc/2021/Conference/Paper3113/-/Official_Comment", "content": {"title": "Response to Reviewer # 2 - general thanks and comments (2/2)", "comment": "[Comment 5] \u201cSecondly, the authors propose to pre-train the student model with the teacher predictions to initialize the student. However, it is odd to view this step as pre-training as it actually adopts the teacher predictions to train the student. It is actually doing distillation! The improvement in performance may simply come from the more training epochs.\u201d\n\n=> Clarification of the factors contributing to performance improvement in PTP.\n\nDuring PTP, we are training the student with artificial labels created from the teacher\u2019s output.\nWe are not directly matching the student\u2019s output to the teacher\u2019s output, which is what is done in conventional Knowledge Distillation.  We understand why PTP might seem like distillation depending on how \u201cdistillation\u201d is defined. PTP can loosely be understood as a type of distillation, but it is different from and outperforms the conventional distillation methods available.\nEven when interpreted as a type of distillation, PTP still meaningfully contributes to performance improvement.\n\nTo address your comment about the performance improvement possibly coming from simply training over a greater number of epochs, we have conducted an experiment as outlined below:\nTo support our claim, we ran experiments comparing two cases. (a) 'BERT + 8 epoch training' and (b) 'BERT$_{3}$+ 2 epoch PTP + 6 epoch training' where training is done with PatientKD. Both models have the same number of total epochs of 8.  The results are the average of 5 runs with random seeds for each dataset.\nAccording to the results reported below, (b) shows better accuracy of 0.7\\% on average. This shows that increasing the number of epochs is not the main cause of the improvement in PTP. Therefore, we strongly believe that PTP has its own merits. \n\nWe have also conducted an additional ablation study regarding PTP in the first paragraph of Section 4.4 of the revised paper, in order to further verify the effectiveness of PTP. \nWe would truly appreciate it if you could review our updated paper once more. Thank you.\n\t\n|models|MRPC|RTE|SST-2|QNLI|AVG|\n|:----|:----|:---|:----|:-----|:-----|\n|(a) BERT$_{3}$+training(8 epoch)| 85.1 | 62.5 | 88.4 | 85.3| 80.3|\n|(b) BERT$_{3}$+PTP(2 epoch)+training(6 epoch)|   85.5| 64.0|88.9|85.6|81.0|\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3113/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3113/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pea-KD: Parameter-efficient and accurate Knowledge Distillation", "authorids": ["~IKHYUN_CHO1", "~U_Kang1"], "authors": ["IKHYUN CHO", "U Kang"], "keywords": ["BERT", "Deep Learning", "Natural Language Processing", "Transformer", "Knowledge Distillation", "Parameter Sharing"], "abstract": "How can we efficiently compress a model while maintaining its performance?  Knowledge Distillation (KD) is one of the widely known methods for model compression. In essence, KD trains a smaller student model based on a larger teacher model and tries to retain the teacher model's level of performance as much as possible. However, the existing KD methods suffer from the following limitations. First, since the student model is small in absolute size, it inherently lacks model complexity. Second, the absence of an initial guide for the student model makes it difficult for the student to imitate the teacher model to its fullest. Conventional KD methods yield low performance due to these limitations.\n\nIn this paper, we propose Pea-KD (Parameter-efficient and accurate Knowledge Distillation), a novel approach to KD. Pea-KD consists of two main parts: Shuffled Parameter Sharing (SPS) and Pretraining with Teacher's Predictions (PTP). Using this combination, we are capable of alleviating the KD's limitations. SPS is a new parameter sharing method that allows greater model complexity for the student model. PTP is a KD-specialized initialization method, which can act as a good initial guide for the student. When combined, this method yields a significant increase in student model's performance. Experiments conducted on different datasets and tasks show that the proposed approach improves the student model's performance by 4.4% on average in four GLUE tasks, outperforming existing KD baselines by significant margins.\n", "one-sentence_summary": "It introduces a new Knowledge Distillation method. It improves the performance of the student model with 2 main modules:  novel parameter sharing and new pretraining which uses the teacher model's predictions. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "cho|peakd_parameterefficient_and_accurate_knowledge_distillation", "supplementary_material": "/attachment/6ba2955df8bb8f79fe91bfbfc55d8343c3e74e7a.zip", "pdf": "/pdf/73e1fa211b7a950aef7587202346d058d92c3558.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=pgcw7UOeGy", "_bibtex": "@misc{\ncho2021peakd,\ntitle={Pea-{\\{}KD{\\}}: Parameter-efficient and accurate Knowledge Distillation},\nauthor={IKHYUN CHO and U Kang},\nyear={2021},\nurl={https://openreview.net/forum?id=PQ2Cel-1rJh}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "PQ2Cel-1rJh", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3113/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3113/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3113/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3113/Authors|ICLR.cc/2021/Conference/Paper3113/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3113/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923840963, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3113/-/Official_Comment"}}}, {"id": "PHmFQdn4BI", "original": null, "number": 5, "cdate": 1606222687125, "ddate": null, "tcdate": 1606222687125, "tmdate": 1606222687125, "tddate": null, "forum": "PQ2Cel-1rJh", "replyto": "JhWFWPoSzy", "invitation": "ICLR.cc/2021/Conference/Paper3113/-/Official_Comment", "content": {"title": "Response to Reviewer # 2 - general thanks and comments (1/2)", "comment": "We would like to thank Reviewer 2 for the insightful comments.\nEach comment and how we have addressed them in our paper is summarized below.\n\n[Comment 1] \u201cAs all the experiments are conducted on GLUE and only BERT models are considered in the paper, the proposed method seems to be tailored for BERT. The authors should stress this point clearly and early in the paper (in the title, abstract or introduction). Otherwise, the authors should provide some experimental results on other models or tasks, e.g., the typical image recognition task in computer vision.\u201d\n\n=> Based on your comment, we have modified our title to explicitly mention BERT. The new title is \u201cPea-KD: Parameter-efficient and accurate Knowledge Distillation on BERT.\u201d We also clarified in our paper that these experiments were only conducted on BERT.\n\n[Comment 2] \u201cIn the section of PROPOSED METHODs (section 3.1), the motivation and the main idea of the paper are introduced again. As no any new information is provided here compared to the abstraction and the introduction sections, it seems very redundant.\u201d\n\n=> \tWe apologize if our paper structure was not concise. Since we are introducing a multi-step procedure, we thought it would help the reader follow the paper better if we provided a quick recap of our main ideas mentioned in the introduction once again in Section 3.1. Since the overview and the introduction are both intended to provide a general summary of the proposed method, we hope you will allow for some overlap. To differentiate the introduction from Section 3.1, we have added more information based on multiple Reviewers\u2019 feedback, including discussions around the motivation and intuition behind how we came up with the process, why we intuitively thought this method will work, and the contributing factors to the increased model representation. We have also added more information about SPS and PTP in Section 3.1 based on other reviewers\u2019 feedback.\n\n[Comment 3] \u201cThe introduction of SPS in Section 3.2 is quite confusing. What do Key and Query parameters stand for? I have no idea what the author is talking about here. Maybe it is because I have little background in NLP and BERT. However, even so the authors are still responsible for making the paper easy to follow for readers like me.\u201d\n\n=> We apologize for not providing enough background on the jargons used. We updated the first paragraph in Section 2 to include explanations of the transformer layers.\n\n[Comment 4] \u201cThe motivation and the proposed method are somewhat problematic. Firstly, the authors argue that small student with few parameters are limited in capacity, so they propose to stack repeating layers to enlarge the model capacity. However, stacking repeating layer will introduce much more computation cost, which violates the goal of distillation.\u201d\n\n=> Our primary focus was on improving performance without needing additional memory resources. Of the many important factors that we consider in model compression, such as memory storage, performance, and inference time, some factors are prioritized over others depending on the circumstances. In our paper, we prioritized maximum performance improvement while keeping memory constant, over speed.\nAs you have pointed out, the SPS method has a drawback of additional inference time. Despite the longer inference time, our model does perform significantly better. We believe that the additional time could be acceptable in certain cases where the performance is much more important than inference time, with limited memory storage. We do acknowledge that the increased inference time is an important limitation of our SPS method and will certainly aim to reduce this in our future work.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3113/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3113/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pea-KD: Parameter-efficient and accurate Knowledge Distillation", "authorids": ["~IKHYUN_CHO1", "~U_Kang1"], "authors": ["IKHYUN CHO", "U Kang"], "keywords": ["BERT", "Deep Learning", "Natural Language Processing", "Transformer", "Knowledge Distillation", "Parameter Sharing"], "abstract": "How can we efficiently compress a model while maintaining its performance?  Knowledge Distillation (KD) is one of the widely known methods for model compression. In essence, KD trains a smaller student model based on a larger teacher model and tries to retain the teacher model's level of performance as much as possible. However, the existing KD methods suffer from the following limitations. First, since the student model is small in absolute size, it inherently lacks model complexity. Second, the absence of an initial guide for the student model makes it difficult for the student to imitate the teacher model to its fullest. Conventional KD methods yield low performance due to these limitations.\n\nIn this paper, we propose Pea-KD (Parameter-efficient and accurate Knowledge Distillation), a novel approach to KD. Pea-KD consists of two main parts: Shuffled Parameter Sharing (SPS) and Pretraining with Teacher's Predictions (PTP). Using this combination, we are capable of alleviating the KD's limitations. SPS is a new parameter sharing method that allows greater model complexity for the student model. PTP is a KD-specialized initialization method, which can act as a good initial guide for the student. When combined, this method yields a significant increase in student model's performance. Experiments conducted on different datasets and tasks show that the proposed approach improves the student model's performance by 4.4% on average in four GLUE tasks, outperforming existing KD baselines by significant margins.\n", "one-sentence_summary": "It introduces a new Knowledge Distillation method. It improves the performance of the student model with 2 main modules:  novel parameter sharing and new pretraining which uses the teacher model's predictions. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "cho|peakd_parameterefficient_and_accurate_knowledge_distillation", "supplementary_material": "/attachment/6ba2955df8bb8f79fe91bfbfc55d8343c3e74e7a.zip", "pdf": "/pdf/73e1fa211b7a950aef7587202346d058d92c3558.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=pgcw7UOeGy", "_bibtex": "@misc{\ncho2021peakd,\ntitle={Pea-{\\{}KD{\\}}: Parameter-efficient and accurate Knowledge Distillation},\nauthor={IKHYUN CHO and U Kang},\nyear={2021},\nurl={https://openreview.net/forum?id=PQ2Cel-1rJh}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "PQ2Cel-1rJh", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3113/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3113/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3113/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3113/Authors|ICLR.cc/2021/Conference/Paper3113/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3113/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923840963, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3113/-/Official_Comment"}}}, {"id": "Y938gdO05XY", "original": null, "number": 3, "cdate": 1606222369982, "ddate": null, "tcdate": 1606222369982, "tmdate": 1606222441893, "tddate": null, "forum": "PQ2Cel-1rJh", "replyto": "RbZxI_6CYC", "invitation": "ICLR.cc/2021/Conference/Paper3113/-/Official_Comment", "content": {"title": "Response to reviewer #3 - general thanks and comments (1/2)", "comment": "We would like to thank Reviewer 3 for the insightful and detailed feedback. The order of responses to your questions is intentionally switched for easier explanation.\n\n1. Potential reasons for why shuffling can enrich model capacity\n\nWe believe the main factor contributing to the power of SPS step 2 is as follows:\n\nIncreased model capacity by learning diverse set of information with the same parameters. \n\nTaking the BERT$_{3}$ case as an example, let us look at the difference between SPS-1 and SPS-2 and how that contributes to the learning process of the parameters. As a recap, while SPS-1 applies only step1 to BERT, SPS-2 applies both step 1 and step 2, shuffling the parameters around.\nConsider the first layer's Query parameter. Under SPS-1, this parameter is used as a Query parameter in both the first layer and in the shared fourth layer. Since this parameter is only used as Query, it will only learn the information relevant to Query.\nUnder SPS-2, however, the first layer\u2019s Query parameter\u2019s function changes due to shuffling. The first layer\u2019s Query parameter is used as a Key parameter in the shared fourth layer. This one parameter has had the opportunity to learn the important features of both the Query and the Key functions, gaining a more diverse and wider breadth of knowledge. Based on the average accuracy increase of 1.9 percent driven by shuffling (Table 4 of the paper), we believe that the shuffled parameters were able to learn a more diverse set of information, and this improvement in parameter efficiency contributed to enriching model capacity. We also see this in a similar but slightly different point of view. Since the parameters get to learn diverse features and get to function in diverse positions we believe this could act as an additional regularization. Therefore it helps the model prevent overfitting and leads to improvement in performance.\n\n2. Regarding only applying the SPS step 2 to the student model.\n\nWe initially did not try applying SPS step 2 on a standalone basis, because we created step 2 with the intention of applying it to the shuffled parameters derived from step 1. We believe step 2 would be meaningful only when preceded by step 1. Since Query and Key are identical (768, 768) linear layers with different names, the performance of BERT and BERT with only step 2 applied would yield the same results. Without step 1, step 2 would not make an impact.\nHowever, thanks to reviewer 3, we realized that, since we are using the pre-trained BERT supplied by Huggingface, BERT and `BERT + only step 2' could potentially yield different results in this case. \nGiven the pre-trained Query and Key parameters by Huggingface, we switched the order of them and performed finetuning. This experiment of using 'BERT + only step 2' showed that applying step 2 alone actually showed decreased average accuracy of 0.6\\%. As we expected, step1 needs to be preceded for step2 to have desirable effects.\n\n|models|MRPC|RTE|SST-2|QNLI|AVG|\n|:----|:----|:---|:----|:-----|:-----|\n|BERT$_{3}$|   84. 7| 62.0|88.3|85.1|80.0|\n|BERT$_{3}$+step2| 84.8 | 61.9 | 87.4 | 83.5| 79.4|\n\n3. Regarding the increased inference time.\n\nCompared to the original BERT model, our SPS model uses the same amount of memory storage to load and run but achieves much higher accuracy, an average improvement of 4.4\\% . However, we are aware and fully acknowledge that our approach has a drawback of additional inference time. This is mostly because we use additional shared layers as the reviewer supposed, which takes about 50\\% to 100\\% additional time to run for PeaBERT (50\\% for PeaBERT6 and 100\\% for PeaBERT3). Despite the longer inference time, our model does perform significantly better. We believe that the additional time could be acceptable in certain cases where the performance is much more important than inference time, with limited memory storage. We do acknowledge that the increased inference time is an important limitation of our SPS method and will definitely aim to reduce this in our future work."}, "signatures": ["ICLR.cc/2021/Conference/Paper3113/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3113/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pea-KD: Parameter-efficient and accurate Knowledge Distillation", "authorids": ["~IKHYUN_CHO1", "~U_Kang1"], "authors": ["IKHYUN CHO", "U Kang"], "keywords": ["BERT", "Deep Learning", "Natural Language Processing", "Transformer", "Knowledge Distillation", "Parameter Sharing"], "abstract": "How can we efficiently compress a model while maintaining its performance?  Knowledge Distillation (KD) is one of the widely known methods for model compression. In essence, KD trains a smaller student model based on a larger teacher model and tries to retain the teacher model's level of performance as much as possible. However, the existing KD methods suffer from the following limitations. First, since the student model is small in absolute size, it inherently lacks model complexity. Second, the absence of an initial guide for the student model makes it difficult for the student to imitate the teacher model to its fullest. Conventional KD methods yield low performance due to these limitations.\n\nIn this paper, we propose Pea-KD (Parameter-efficient and accurate Knowledge Distillation), a novel approach to KD. Pea-KD consists of two main parts: Shuffled Parameter Sharing (SPS) and Pretraining with Teacher's Predictions (PTP). Using this combination, we are capable of alleviating the KD's limitations. SPS is a new parameter sharing method that allows greater model complexity for the student model. PTP is a KD-specialized initialization method, which can act as a good initial guide for the student. When combined, this method yields a significant increase in student model's performance. Experiments conducted on different datasets and tasks show that the proposed approach improves the student model's performance by 4.4% on average in four GLUE tasks, outperforming existing KD baselines by significant margins.\n", "one-sentence_summary": "It introduces a new Knowledge Distillation method. It improves the performance of the student model with 2 main modules:  novel parameter sharing and new pretraining which uses the teacher model's predictions. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "cho|peakd_parameterefficient_and_accurate_knowledge_distillation", "supplementary_material": "/attachment/6ba2955df8bb8f79fe91bfbfc55d8343c3e74e7a.zip", "pdf": "/pdf/73e1fa211b7a950aef7587202346d058d92c3558.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=pgcw7UOeGy", "_bibtex": "@misc{\ncho2021peakd,\ntitle={Pea-{\\{}KD{\\}}: Parameter-efficient and accurate Knowledge Distillation},\nauthor={IKHYUN CHO and U Kang},\nyear={2021},\nurl={https://openreview.net/forum?id=PQ2Cel-1rJh}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "PQ2Cel-1rJh", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3113/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3113/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3113/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3113/Authors|ICLR.cc/2021/Conference/Paper3113/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3113/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923840963, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3113/-/Official_Comment"}}}, {"id": "bcpla1Xm-BN", "original": null, "number": 4, "cdate": 1606222422846, "ddate": null, "tcdate": 1606222422846, "tmdate": 1606222422846, "tddate": null, "forum": "PQ2Cel-1rJh", "replyto": "Y938gdO05XY", "invitation": "ICLR.cc/2021/Conference/Paper3113/-/Official_Comment", "content": {"title": "Response to reviewer #3 - general thanks and comments (2/2)", "comment": "\nIn summary, our SPS method provides a new approach in Parameter Sharing, in which we use a shuffling mechanism to improve performance. To the best of our knowledge, this is the first approach in Parameter Sharing that uses shared internal parameters in different positions to train the parameters more efficiently. We believe our SPS is meaningful, because it has shown considerable and consistent performance improvement in a relatively simple and intuitive manner. SPS is also widely applicable to many other models that apply parameter sharing. For example, although not mentioned in the paper, we applied SPS in transformer encoder-decoder models for Neural Machine Translation tasks and found that SPS improves the performance by an average of 0.5 BLUE scores in IWSLT\u201914 datasets. We believe that further studies on shuffling mechanism can derive a number of invaluable subsequent studies, such as designing more parameter-efficient models by determining which parameters are interchangable or distinct on their own. \n\nwe have also included more ablation studies to prove the effectiveness of PTP (first paragraph in Section 4.4) and additional explanation behind how we came up with our approach and why we intuitively thought this will improve performance (last paragraphs in Sections 3.2 and 3.3). We hope these revisions made our paper more robust. We would truly appreciate it if you could review our updated paper once more. Thank you."}, "signatures": ["ICLR.cc/2021/Conference/Paper3113/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3113/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pea-KD: Parameter-efficient and accurate Knowledge Distillation", "authorids": ["~IKHYUN_CHO1", "~U_Kang1"], "authors": ["IKHYUN CHO", "U Kang"], "keywords": ["BERT", "Deep Learning", "Natural Language Processing", "Transformer", "Knowledge Distillation", "Parameter Sharing"], "abstract": "How can we efficiently compress a model while maintaining its performance?  Knowledge Distillation (KD) is one of the widely known methods for model compression. In essence, KD trains a smaller student model based on a larger teacher model and tries to retain the teacher model's level of performance as much as possible. However, the existing KD methods suffer from the following limitations. First, since the student model is small in absolute size, it inherently lacks model complexity. Second, the absence of an initial guide for the student model makes it difficult for the student to imitate the teacher model to its fullest. Conventional KD methods yield low performance due to these limitations.\n\nIn this paper, we propose Pea-KD (Parameter-efficient and accurate Knowledge Distillation), a novel approach to KD. Pea-KD consists of two main parts: Shuffled Parameter Sharing (SPS) and Pretraining with Teacher's Predictions (PTP). Using this combination, we are capable of alleviating the KD's limitations. SPS is a new parameter sharing method that allows greater model complexity for the student model. PTP is a KD-specialized initialization method, which can act as a good initial guide for the student. When combined, this method yields a significant increase in student model's performance. Experiments conducted on different datasets and tasks show that the proposed approach improves the student model's performance by 4.4% on average in four GLUE tasks, outperforming existing KD baselines by significant margins.\n", "one-sentence_summary": "It introduces a new Knowledge Distillation method. It improves the performance of the student model with 2 main modules:  novel parameter sharing and new pretraining which uses the teacher model's predictions. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "cho|peakd_parameterefficient_and_accurate_knowledge_distillation", "supplementary_material": "/attachment/6ba2955df8bb8f79fe91bfbfc55d8343c3e74e7a.zip", "pdf": "/pdf/73e1fa211b7a950aef7587202346d058d92c3558.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=pgcw7UOeGy", "_bibtex": "@misc{\ncho2021peakd,\ntitle={Pea-{\\{}KD{\\}}: Parameter-efficient and accurate Knowledge Distillation},\nauthor={IKHYUN CHO and U Kang},\nyear={2021},\nurl={https://openreview.net/forum?id=PQ2Cel-1rJh}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "PQ2Cel-1rJh", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3113/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3113/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3113/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3113/Authors|ICLR.cc/2021/Conference/Paper3113/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3113/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923840963, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3113/-/Official_Comment"}}}, {"id": "X0qYf_FDI5z", "original": null, "number": 2, "cdate": 1606217810010, "ddate": null, "tcdate": 1606217810010, "tmdate": 1606217810010, "tddate": null, "forum": "PQ2Cel-1rJh", "replyto": "CLijjy3CItp", "invitation": "ICLR.cc/2021/Conference/Paper3113/-/Official_Comment", "content": {"title": "Response to reviewer #1 - general thanks and comments", "comment": "We would like to thank Reviewer 1 for the detailed and thoughtful review. We were delighted to hear that you found our core ideas interesting. Based on reviewers' feedback, we have included more ablation studies to prove the effectiveness of our method (Section 4.4) and additional explanation behind how we came up with our approach and why we intuitively thought this will improve performance (Sections 3.2 and 3.3). We hope these revisions made our paper more robust. We would truly appreciate it if you could review our updated paper once more. Thank you."}, "signatures": ["ICLR.cc/2021/Conference/Paper3113/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3113/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pea-KD: Parameter-efficient and accurate Knowledge Distillation", "authorids": ["~IKHYUN_CHO1", "~U_Kang1"], "authors": ["IKHYUN CHO", "U Kang"], "keywords": ["BERT", "Deep Learning", "Natural Language Processing", "Transformer", "Knowledge Distillation", "Parameter Sharing"], "abstract": "How can we efficiently compress a model while maintaining its performance?  Knowledge Distillation (KD) is one of the widely known methods for model compression. In essence, KD trains a smaller student model based on a larger teacher model and tries to retain the teacher model's level of performance as much as possible. However, the existing KD methods suffer from the following limitations. First, since the student model is small in absolute size, it inherently lacks model complexity. Second, the absence of an initial guide for the student model makes it difficult for the student to imitate the teacher model to its fullest. Conventional KD methods yield low performance due to these limitations.\n\nIn this paper, we propose Pea-KD (Parameter-efficient and accurate Knowledge Distillation), a novel approach to KD. Pea-KD consists of two main parts: Shuffled Parameter Sharing (SPS) and Pretraining with Teacher's Predictions (PTP). Using this combination, we are capable of alleviating the KD's limitations. SPS is a new parameter sharing method that allows greater model complexity for the student model. PTP is a KD-specialized initialization method, which can act as a good initial guide for the student. When combined, this method yields a significant increase in student model's performance. Experiments conducted on different datasets and tasks show that the proposed approach improves the student model's performance by 4.4% on average in four GLUE tasks, outperforming existing KD baselines by significant margins.\n", "one-sentence_summary": "It introduces a new Knowledge Distillation method. It improves the performance of the student model with 2 main modules:  novel parameter sharing and new pretraining which uses the teacher model's predictions. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "cho|peakd_parameterefficient_and_accurate_knowledge_distillation", "supplementary_material": "/attachment/6ba2955df8bb8f79fe91bfbfc55d8343c3e74e7a.zip", "pdf": "/pdf/73e1fa211b7a950aef7587202346d058d92c3558.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=pgcw7UOeGy", "_bibtex": "@misc{\ncho2021peakd,\ntitle={Pea-{\\{}KD{\\}}: Parameter-efficient and accurate Knowledge Distillation},\nauthor={IKHYUN CHO and U Kang},\nyear={2021},\nurl={https://openreview.net/forum?id=PQ2Cel-1rJh}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "PQ2Cel-1rJh", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3113/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3113/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3113/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3113/Authors|ICLR.cc/2021/Conference/Paper3113/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3113/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923840963, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3113/-/Official_Comment"}}}, {"id": "3TKjVowcbP8", "original": null, "number": 1, "cdate": 1602644048257, "ddate": null, "tcdate": 1602644048257, "tmdate": 1605024065279, "tddate": null, "forum": "PQ2Cel-1rJh", "replyto": "PQ2Cel-1rJh", "invitation": "ICLR.cc/2021/Conference/Paper3113/-/Official_Review", "content": {"title": "Interesting ideas, but each idea seems only weakly motivated to me; inference times are not mentioned or measured.", "review": "The paper claims to present an improved form of knowledge distillation which tackles the following perceived weaknesses of existing kd systems:\n- student's expressive power is less than original teacher model, and\n- unclear how to initialize the student model weights in a principled way\n\nThe paper proposes two methods to improve kd:\n- using stacked layers, with weight sharing between the weights, and keys and queries swapped in the upper layers\n   - this is proposed because it means the effective student geometry matches that of the teacher, or at least is significantly larger than without the shared layers, whilst the number of unique parameters remains small\n- getting the student to predict the confidence of the teacher on each example, rather than just the softmax output of the teacher\n   - the confidence is classified as either 'strong' or 'weak'\n   - the student must also predict whether the teacher gets the example correct or not (which we know, since this is for training set examples, for which we have ground truth)\n\nStrong points of the paper:\n- SPS sounds novel, but I felt it was only very weakly motivated\n    - in addition it sounds to me like the inference time will be similar to the original model? I feel that reducing the number of parameters in the student is not the only goal: the student model should be fast to execute.\n    - the paper does admittedly not claim to provide fast inference times as a goal; nevertheless I have a hard time imagining a BERT model running on an edge device with a limited amount of resources in a reasonable time, so I think that inference time is a critical metric which I feel this paper does not consider, or measure\n- PTP sounds to me only weakly novel, since typically the student will be trained to predict the full softmax output of the teacher, whereas in PTP, the student must predict whether the highest value of the softmax is high or low. I feel that little insight or motivation is given as to why this auxiliary task was used, or is better than predicting the full softmax distribution. Nor was a rigorous comparison carried out to compare using just this pre-training approach with just the softmax-matching approach, where both using appropriately tuned hyper-parameters, I feel. There was an experiment in 4.4 where the PTP task is dropped, but I got the impression that this was after tuning the hyper-parameters to rely on PTP task? The hyper-parameters would I feel need to be retuned in the absence of the PTP task I think?\n\nI mostly like the paper. I found it interesting. However, I have two main concerns:\n- the approaches used in the paper feel to me only weakly motivated. Little insight is given into why the approaches were chosen, and why they should work. For example, flipping the keys and queries was not well justified I felt. Nor was the PTP task relative to traditional softmax matching.\n- the paper avoids discussing inference times, and it's unclear to me whether a distillation approach that results in few parameters but long inference times would be useful in practice?\n    - of course, the underlying techniques can still be interesting from a theoretical point of view\n\nWhilst I was reading through the paper, I wrote down some more detailed reactions:\n\nIntroduction:\n\n\"stacking the layers that share parameters\" => \"stacking layers that share parameters\" (the former form of language is using 'that share parameters' as a filter to choose from existing layers; the latter form implies that we create new layers, which we then stack, and which we configure to have shared parameters)\n\n3.1 Overview\n\nSo, it looks like SPS means the number of unique parameters is kept low whilst the effective model complexity is unchanged compared to original BERT. How does this affect inference speed? It sounds to me like the inference speed under SPS will be unchanged from a BERT model of equivalent model complexity? Often, a use-case of distillation is to reduce inference time, and requirements in terms of power, eg for mobile devices, or production inference. Assuming that the inference time is unchanged from the original model of equal complexity, what is the target use-case of SPS/Pea-KD?\n\n3.2 SPS\n\nNo insight or intuition is given as to why swapping query and key is likely to be a useful thing to do. I think it would be good to provide such insight and intuition. Like, you say that it increases the expressive power, but you don't justify why you feel this is true.\n\nFor the six-layer student, I couldnt quite undersatnd why it looks like the 6-layer student has 9 layers? Please consider clarifying this point:\n- why do you call it '6 layer' if it has 9 effective layers? Perhaps consider renaming it to 9-layer student?\n- why do you make the 6 layer model become 9 layers, and not 12 layers? ie, following the initial recipe detailed in section 3.2, of doubling all the layers.\n\n3.3 PTP\n\nI feel that the column for 'PTP label' in table 1 should be on the right hand side:\n- the inputs should be I feel on the left, ie 'teachers prediction correct', and 'confidence > t',\n- and then we can read off the 'output' of the table as the right-hand column\n- (currently I find the table hard to read)\n\nThe pretraining task itself seems interesting to me.\n\nHow do you get the student to predict this result? Do you put a linear layer on the output from the student? Why do you use a classification task (4 classes) instead of using a regression task to predict the confidence? did you try a regression task to predict the confidence? Why use a classification into 4 classes, rather than two classifiers into two classes each?\n\n4. Experiments:\n\nFor table 2, I wasn't really sure how these numbers compare to two key baselines:\n- a simple bilstm with attention, without any pretraining, and\n- a BERT-base model\n\nSo, I dug these out, and here are the peabert numbers from table 2, put into the context of these two baselines (which I feel are lower and upperbound really for what we'd expect to see from PeaBERT):\n\n     BiLSTM+Attn\n     (single-task training)   BERT-base   PeaBERT1  PeaBERT2  PeaBERT3\n    RTE: 51.9                     66.4        53.0     64.1       64.5\n    MRPC 68.5                     88.9        81.0     82.7        85.0\n    SST2 85.9                     93.5        86.9     88.2       90.4\n    QNLI 77.2                     90.5        78.8      86.0       87.0\n\nI got the BiLSTM+Attn numbers from GLUE paper, and\nBERT numbers from https://arxiv.org/pdf/1910.03176.pdf\n\nI kind of feel that I might not be the only person who might want to see the PeaBERT numbers in the context of such lower and upperbound baselines? Maybe consider adding these baselinse into table 2?\n\nBasically, my take away from this is that PeaBERT1 is barely better than the simple BiLSTM+Attn baseline, but PeaBERT3 approaches the score of a full BERT-base model?\n\nI kind of think the sentence \"For example, DistilBERT took approximately 90 hours with eight 16GB V100 GPUs while PeaBERT took a minimum of one minute (PeaBERT1 with RTE) to a maximum of one hour (PeaBERT3 with QNLI) using just two NVIDIA T4 GPUs\" should be highlighted in a table somewhere somehow perhaps, rather than buried in text? Not sure if that's a good idea, just occurred to me though.\n\nQuestion: why only show results on a subset of the GLUE tasks? eg patient-kd paper shows results on additionally:\n- QQP\n- MNLI-m\n- MNLI-mm\n(whilst also showing results for: SST-2, MRPC, QNLI and RTE, as here)\n\nIt is unclear from this whether you ran against all, and only showed the four tasks that show a benefit, or whether you simply didn't have time to run on all 7 tasks. Preference to show results for all 7 tasks that Patient-KD paper reports results for.\n\nFor ablation studies, I feel it would be interesting to see an ablation study that removes each of various losses in equation 3 in turn.\n\nImportantly, none of the experiments mention inference time, which I feel is a key metric to report for distillation?\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3113/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3113/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pea-KD: Parameter-efficient and accurate Knowledge Distillation", "authorids": ["~IKHYUN_CHO1", "~U_Kang1"], "authors": ["IKHYUN CHO", "U Kang"], "keywords": ["BERT", "Deep Learning", "Natural Language Processing", "Transformer", "Knowledge Distillation", "Parameter Sharing"], "abstract": "How can we efficiently compress a model while maintaining its performance?  Knowledge Distillation (KD) is one of the widely known methods for model compression. In essence, KD trains a smaller student model based on a larger teacher model and tries to retain the teacher model's level of performance as much as possible. However, the existing KD methods suffer from the following limitations. First, since the student model is small in absolute size, it inherently lacks model complexity. Second, the absence of an initial guide for the student model makes it difficult for the student to imitate the teacher model to its fullest. Conventional KD methods yield low performance due to these limitations.\n\nIn this paper, we propose Pea-KD (Parameter-efficient and accurate Knowledge Distillation), a novel approach to KD. Pea-KD consists of two main parts: Shuffled Parameter Sharing (SPS) and Pretraining with Teacher's Predictions (PTP). Using this combination, we are capable of alleviating the KD's limitations. SPS is a new parameter sharing method that allows greater model complexity for the student model. PTP is a KD-specialized initialization method, which can act as a good initial guide for the student. When combined, this method yields a significant increase in student model's performance. Experiments conducted on different datasets and tasks show that the proposed approach improves the student model's performance by 4.4% on average in four GLUE tasks, outperforming existing KD baselines by significant margins.\n", "one-sentence_summary": "It introduces a new Knowledge Distillation method. It improves the performance of the student model with 2 main modules:  novel parameter sharing and new pretraining which uses the teacher model's predictions. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "cho|peakd_parameterefficient_and_accurate_knowledge_distillation", "supplementary_material": "/attachment/6ba2955df8bb8f79fe91bfbfc55d8343c3e74e7a.zip", "pdf": "/pdf/73e1fa211b7a950aef7587202346d058d92c3558.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=pgcw7UOeGy", "_bibtex": "@misc{\ncho2021peakd,\ntitle={Pea-{\\{}KD{\\}}: Parameter-efficient and accurate Knowledge Distillation},\nauthor={IKHYUN CHO and U Kang},\nyear={2021},\nurl={https://openreview.net/forum?id=PQ2Cel-1rJh}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "PQ2Cel-1rJh", "replyto": "PQ2Cel-1rJh", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3113/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538081986, "tmdate": 1606915773536, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3113/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3113/-/Official_Review"}}}, {"id": "RbZxI_6CYC", "original": null, "number": 3, "cdate": 1603854733242, "ddate": null, "tcdate": 1603854733242, "tmdate": 1605024065137, "tddate": null, "forum": "PQ2Cel-1rJh", "replyto": "PQ2Cel-1rJh", "invitation": "ICLR.cc/2021/Conference/Paper3113/-/Official_Review", "content": {"title": "Review of AnonReviewer3", "review": "This paper proposes a parameter-efficient KD which consists of two main parts: Shuffled Parameter Sharing (SPS) and Pretraining with Teacher\u2019s Predictions (PTP). This work explores some new framework like parameters-sharing and shuffling in KD and obtain promising results. \n\n\nStrengths:\n1. The paper is well-written and easy to follow.\n2. The experiment part explores the overall performance, the effects of SPS, and PTP separately, which is clear and makes readers easy to understand the effects of each part (SPS: step1+step2, PTP) in Pea-KD.  \n\nWeaknesses:\n1. When applying SPS, the number of layers in the student model is double than the normal case, even the parameters are the same as the counterpart, I guess the FLOPs will increase or be doubled than the original one. I think for a small student model, the number of parameters should not be the only metric but also consider the FLOPs. Why this paper doesn\u2019t discuss this? This is my main concern.\n2. From Table4, we can find that the shuffling in SPS has great effects. step1+step2 would have much greater improvement than only applying step1, how about only conduct shuffling without parameters sharing (only step2 by shuffling the original parameters without step1)?  On the other hand, what is the potential reason why shuffling can enrich the model capacity?\n\n", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3113/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3113/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pea-KD: Parameter-efficient and accurate Knowledge Distillation", "authorids": ["~IKHYUN_CHO1", "~U_Kang1"], "authors": ["IKHYUN CHO", "U Kang"], "keywords": ["BERT", "Deep Learning", "Natural Language Processing", "Transformer", "Knowledge Distillation", "Parameter Sharing"], "abstract": "How can we efficiently compress a model while maintaining its performance?  Knowledge Distillation (KD) is one of the widely known methods for model compression. In essence, KD trains a smaller student model based on a larger teacher model and tries to retain the teacher model's level of performance as much as possible. However, the existing KD methods suffer from the following limitations. First, since the student model is small in absolute size, it inherently lacks model complexity. Second, the absence of an initial guide for the student model makes it difficult for the student to imitate the teacher model to its fullest. Conventional KD methods yield low performance due to these limitations.\n\nIn this paper, we propose Pea-KD (Parameter-efficient and accurate Knowledge Distillation), a novel approach to KD. Pea-KD consists of two main parts: Shuffled Parameter Sharing (SPS) and Pretraining with Teacher's Predictions (PTP). Using this combination, we are capable of alleviating the KD's limitations. SPS is a new parameter sharing method that allows greater model complexity for the student model. PTP is a KD-specialized initialization method, which can act as a good initial guide for the student. When combined, this method yields a significant increase in student model's performance. Experiments conducted on different datasets and tasks show that the proposed approach improves the student model's performance by 4.4% on average in four GLUE tasks, outperforming existing KD baselines by significant margins.\n", "one-sentence_summary": "It introduces a new Knowledge Distillation method. It improves the performance of the student model with 2 main modules:  novel parameter sharing and new pretraining which uses the teacher model's predictions. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "cho|peakd_parameterefficient_and_accurate_knowledge_distillation", "supplementary_material": "/attachment/6ba2955df8bb8f79fe91bfbfc55d8343c3e74e7a.zip", "pdf": "/pdf/73e1fa211b7a950aef7587202346d058d92c3558.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=pgcw7UOeGy", "_bibtex": "@misc{\ncho2021peakd,\ntitle={Pea-{\\{}KD{\\}}: Parameter-efficient and accurate Knowledge Distillation},\nauthor={IKHYUN CHO and U Kang},\nyear={2021},\nurl={https://openreview.net/forum?id=PQ2Cel-1rJh}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "PQ2Cel-1rJh", "replyto": "PQ2Cel-1rJh", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3113/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538081986, "tmdate": 1606915773536, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3113/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3113/-/Official_Review"}}}, {"id": "CLijjy3CItp", "original": null, "number": 4, "cdate": 1604028668674, "ddate": null, "tcdate": 1604028668674, "tmdate": 1605024065073, "tddate": null, "forum": "PQ2Cel-1rJh", "replyto": "PQ2Cel-1rJh", "invitation": "ICLR.cc/2021/Conference/Paper3113/-/Official_Review", "content": {"title": "A Method to Expand and Initialize Student Model of Knowledge Distillation", "review": "This paper proposed a framework for knowledge distillation with smaller number of parameters.The authors proposed a new parameter sharing method that allows a greater model complexity for the student model. Another contribution is that a KD-specialized initialization method named Pretraining with Teacher\u2019s Predictions can improve the student's performance. The author combined these two methods to improve the performance of the student model on existing tasks, which has surpassed the existing knowledge distillation baseline.\n\nThe SPS method is a very natural idea of extending the student model. It expands the model complexity of the student model and improves the representation ability of the model without increasing the number of parameters. The idea of PTP is an excellent initializing method. At the same time, the teacher model's generalization of knowledge is given to students through initialization, and then fine-tuned.\n\nThe experimental part of this article is also quite sufficient. The ablation experiment illustrates the effectiveness of the two steps for the optimization of results. ", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3113/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3113/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pea-KD: Parameter-efficient and accurate Knowledge Distillation", "authorids": ["~IKHYUN_CHO1", "~U_Kang1"], "authors": ["IKHYUN CHO", "U Kang"], "keywords": ["BERT", "Deep Learning", "Natural Language Processing", "Transformer", "Knowledge Distillation", "Parameter Sharing"], "abstract": "How can we efficiently compress a model while maintaining its performance?  Knowledge Distillation (KD) is one of the widely known methods for model compression. In essence, KD trains a smaller student model based on a larger teacher model and tries to retain the teacher model's level of performance as much as possible. However, the existing KD methods suffer from the following limitations. First, since the student model is small in absolute size, it inherently lacks model complexity. Second, the absence of an initial guide for the student model makes it difficult for the student to imitate the teacher model to its fullest. Conventional KD methods yield low performance due to these limitations.\n\nIn this paper, we propose Pea-KD (Parameter-efficient and accurate Knowledge Distillation), a novel approach to KD. Pea-KD consists of two main parts: Shuffled Parameter Sharing (SPS) and Pretraining with Teacher's Predictions (PTP). Using this combination, we are capable of alleviating the KD's limitations. SPS is a new parameter sharing method that allows greater model complexity for the student model. PTP is a KD-specialized initialization method, which can act as a good initial guide for the student. When combined, this method yields a significant increase in student model's performance. Experiments conducted on different datasets and tasks show that the proposed approach improves the student model's performance by 4.4% on average in four GLUE tasks, outperforming existing KD baselines by significant margins.\n", "one-sentence_summary": "It introduces a new Knowledge Distillation method. It improves the performance of the student model with 2 main modules:  novel parameter sharing and new pretraining which uses the teacher model's predictions. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "cho|peakd_parameterefficient_and_accurate_knowledge_distillation", "supplementary_material": "/attachment/6ba2955df8bb8f79fe91bfbfc55d8343c3e74e7a.zip", "pdf": "/pdf/73e1fa211b7a950aef7587202346d058d92c3558.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=pgcw7UOeGy", "_bibtex": "@misc{\ncho2021peakd,\ntitle={Pea-{\\{}KD{\\}}: Parameter-efficient and accurate Knowledge Distillation},\nauthor={IKHYUN CHO and U Kang},\nyear={2021},\nurl={https://openreview.net/forum?id=PQ2Cel-1rJh}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "PQ2Cel-1rJh", "replyto": "PQ2Cel-1rJh", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3113/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538081986, "tmdate": 1606915773536, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3113/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3113/-/Official_Review"}}}], "count": 14}