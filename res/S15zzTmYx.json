{"notes": [{"tddate": null, "ddate": null, "cdate": null, "original": null, "tmdate": 1490028571870, "tcdate": 1490028571870, "number": 1, "id": "r14mdKTjg", "invitation": "ICLR.cc/2017/workshop/-/paper52/acceptance", "forum": "S15zzTmYx", "replyto": "S15zzTmYx", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "INCREMENTAL LEARNING WITH PRE-TRAINED CONVOLUTIONAL NEURAL NETWORKS AND BINARY ASSOCIATIVE MEMORIES", "abstract": "Thanks to their ability to absorb large amounts of data, Convolutional Neural Networks (CNNs) have become the state-of-the-art in various vision challenges, sometimes even on par with biological vision. CNNs rely on optimisation routines that typically require intensive computational power, thus the question of implementing CNNs on embedded architectures is a very active field of research. Of particular interest is the problem of incremental learning, where the device adapts to new observations or classes. To tackle this challenging problem, we propose to combine pre-trained CNNs with Binary Associative Memories, using product random sampling as an intermediate between the two methods. The obtained architecture requires significantly less computational power and memory usage than existing counterparts. Moreover, using various challenging vision datasets we\nshow that the proposed architecture is able to perform one-shot learning \u2013 even using only part of the dataset \u2013, while keeping very good accuracy.", "pdf": "/pdf/3d8bcf5d8d57750a4652db2da7f24118620f43df.pdf", "TL;DR": "Combining associative memories with quantized outputs of a Deep CNN enables lightweight one shot incremental learning.", "paperhash": "hacene|incremental_learning_with_pretrained_convolutional_neural_networks_and_binary_associative_memories", "conflicts": ["imt-atlantique.fr"], "keywords": ["Computer vision", "Deep learning", "Supervised Learning", "Transfer Learning"], "authors": ["Ghouthi Boukli Hacene", "Vincent Gripon", "Nicolas Farrugia", "Mattieu Arzel", "Michel Jezequel"], "authorids": ["Ghouthi.bouklihacene@imt-atlantique.fr", "vincent.gripon@imt-atlantique.fr", "nicolas.farrugia@imt-atlantique.fr", "matthieu.arzel@imt-atlantique.fr", "michel.jezequel@imt-atlantique.fr"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1490028572464, "id": "ICLR.cc/2017/workshop/-/paper52/acceptance", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "S15zzTmYx", "replyto": "S15zzTmYx", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept", "Reject"]}}}, "nonreaders": [], "cdate": 1490028572464}}}, {"tddate": null, "tmdate": 1489571823891, "tcdate": 1489571823891, "number": 2, "id": "ry_elqIog", "invitation": "ICLR.cc/2017/workshop/-/paper52/public/comment", "forum": "S15zzTmYx", "replyto": "SknwBcBie", "signatures": ["~Ghouthi_BOUKLI_HACENE1"], "readers": ["everyone"], "writers": ["~Ghouthi_BOUKLI_HACENE1"], "content": {"title": "Added baseline results using a linear softmax classifer", "comment": "The authors wish to thank the anonymous reviewer for his encouraging feedback on the proposed method. We just uploaded a new version of the paper, in which we provide results from a simple baseline, using a linear softmax classifier (logistic regression) trained on the output of the CNN."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "INCREMENTAL LEARNING WITH PRE-TRAINED CONVOLUTIONAL NEURAL NETWORKS AND BINARY ASSOCIATIVE MEMORIES", "abstract": "Thanks to their ability to absorb large amounts of data, Convolutional Neural Networks (CNNs) have become the state-of-the-art in various vision challenges, sometimes even on par with biological vision. CNNs rely on optimisation routines that typically require intensive computational power, thus the question of implementing CNNs on embedded architectures is a very active field of research. Of particular interest is the problem of incremental learning, where the device adapts to new observations or classes. To tackle this challenging problem, we propose to combine pre-trained CNNs with Binary Associative Memories, using product random sampling as an intermediate between the two methods. The obtained architecture requires significantly less computational power and memory usage than existing counterparts. Moreover, using various challenging vision datasets we\nshow that the proposed architecture is able to perform one-shot learning \u2013 even using only part of the dataset \u2013, while keeping very good accuracy.", "pdf": "/pdf/3d8bcf5d8d57750a4652db2da7f24118620f43df.pdf", "TL;DR": "Combining associative memories with quantized outputs of a Deep CNN enables lightweight one shot incremental learning.", "paperhash": "hacene|incremental_learning_with_pretrained_convolutional_neural_networks_and_binary_associative_memories", "conflicts": ["imt-atlantique.fr"], "keywords": ["Computer vision", "Deep learning", "Supervised Learning", "Transfer Learning"], "authors": ["Ghouthi Boukli Hacene", "Vincent Gripon", "Nicolas Farrugia", "Mattieu Arzel", "Michel Jezequel"], "authorids": ["Ghouthi.bouklihacene@imt-atlantique.fr", "vincent.gripon@imt-atlantique.fr", "nicolas.farrugia@imt-atlantique.fr", "matthieu.arzel@imt-atlantique.fr", "michel.jezequel@imt-atlantique.fr"]}, "tags": [], "invitation": {"tddate": null, "tmdate": 1487290898171, "tcdate": 1487290898171, "id": "ICLR.cc/2017/workshop/-/paper52/public/comment", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2017/workshop/paper52/reviewers"], "reply": {"forum": "S15zzTmYx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/workshop/reviewers", "ICLR.cc/2017/pcs"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1487290898171}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1489571765733, "tcdate": 1487290897611, "number": 52, "id": "S15zzTmYx", "invitation": "ICLR.cc/2017/workshop/-/submission", "forum": "S15zzTmYx", "signatures": ["~Ghouthi_BOUKLI_HACENE1"], "readers": ["everyone"], "content": {"title": "INCREMENTAL LEARNING WITH PRE-TRAINED CONVOLUTIONAL NEURAL NETWORKS AND BINARY ASSOCIATIVE MEMORIES", "abstract": "Thanks to their ability to absorb large amounts of data, Convolutional Neural Networks (CNNs) have become the state-of-the-art in various vision challenges, sometimes even on par with biological vision. CNNs rely on optimisation routines that typically require intensive computational power, thus the question of implementing CNNs on embedded architectures is a very active field of research. Of particular interest is the problem of incremental learning, where the device adapts to new observations or classes. To tackle this challenging problem, we propose to combine pre-trained CNNs with Binary Associative Memories, using product random sampling as an intermediate between the two methods. The obtained architecture requires significantly less computational power and memory usage than existing counterparts. Moreover, using various challenging vision datasets we\nshow that the proposed architecture is able to perform one-shot learning \u2013 even using only part of the dataset \u2013, while keeping very good accuracy.", "pdf": "/pdf/3d8bcf5d8d57750a4652db2da7f24118620f43df.pdf", "TL;DR": "Combining associative memories with quantized outputs of a Deep CNN enables lightweight one shot incremental learning.", "paperhash": "hacene|incremental_learning_with_pretrained_convolutional_neural_networks_and_binary_associative_memories", "conflicts": ["imt-atlantique.fr"], "keywords": ["Computer vision", "Deep learning", "Supervised Learning", "Transfer Learning"], "authors": ["Ghouthi Boukli Hacene", "Vincent Gripon", "Nicolas Farrugia", "Mattieu Arzel", "Michel Jezequel"], "authorids": ["Ghouthi.bouklihacene@imt-atlantique.fr", "vincent.gripon@imt-atlantique.fr", "nicolas.farrugia@imt-atlantique.fr", "matthieu.arzel@imt-atlantique.fr", "michel.jezequel@imt-atlantique.fr"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 5, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1487690420000, "tmdate": 1484242559574, "id": "ICLR.cc/2017/workshop/-/submission", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1495466420000, "cdate": 1484242559574}}}, {"tddate": null, "nonreaders": null, "tmdate": 1489514282115, "tcdate": 1489507684135, "number": 2, "id": "SknwBcBie", "invitation": "ICLR.cc/2017/workshop/-/paper52/official/review", "forum": "S15zzTmYx", "replyto": "S15zzTmYx", "signatures": ["ICLR.cc/2017/workshop/paper52/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper52/AnonReviewer1"], "content": {"title": "Elegant approach, but poor performance and missing baselines", "rating": "5: Marginally below acceptance threshold", "review": "The paper proposes a technique for doing one-shot learning in computation/memory constrained settings, by taking features from a pre-trained CNN, product-quantizing them (with random anchors) to obtain an alphabet, and linking \"words\" from this alphabet (the PQ chunk centers comprising used to approximate a given feature vector) with class labels via a binary associative memory.\n\nThe setting is interesting and basic idea is (to the best of my knowledge) novel and appealing. However the experimental results aren't compelling and lack natural baselines. At least as a reference, it would have been nice to see the performance of a simple linear classifier trained on the feature vectors, if not a comparison against previous techniques that attempt to learn such linear classifiers incrementally. As it is it is hard to judge the merit of the proposed approach relative to the state of the art, and on their own the results are somewhat underwhelming.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "INCREMENTAL LEARNING WITH PRE-TRAINED CONVOLUTIONAL NEURAL NETWORKS AND BINARY ASSOCIATIVE MEMORIES", "abstract": "Thanks to their ability to absorb large amounts of data, Convolutional Neural Networks (CNNs) have become the state-of-the-art in various vision challenges, sometimes even on par with biological vision. CNNs rely on optimisation routines that typically require intensive computational power, thus the question of implementing CNNs on embedded architectures is a very active field of research. Of particular interest is the problem of incremental learning, where the device adapts to new observations or classes. To tackle this challenging problem, we propose to combine pre-trained CNNs with Binary Associative Memories, using product random sampling as an intermediate between the two methods. The obtained architecture requires significantly less computational power and memory usage than existing counterparts. Moreover, using various challenging vision datasets we\nshow that the proposed architecture is able to perform one-shot learning \u2013 even using only part of the dataset \u2013, while keeping very good accuracy.", "pdf": "/pdf/3d8bcf5d8d57750a4652db2da7f24118620f43df.pdf", "TL;DR": "Combining associative memories with quantized outputs of a Deep CNN enables lightweight one shot incremental learning.", "paperhash": "hacene|incremental_learning_with_pretrained_convolutional_neural_networks_and_binary_associative_memories", "conflicts": ["imt-atlantique.fr"], "keywords": ["Computer vision", "Deep learning", "Supervised Learning", "Transfer Learning"], "authors": ["Ghouthi Boukli Hacene", "Vincent Gripon", "Nicolas Farrugia", "Mattieu Arzel", "Michel Jezequel"], "authorids": ["Ghouthi.bouklihacene@imt-atlantique.fr", "vincent.gripon@imt-atlantique.fr", "nicolas.farrugia@imt-atlantique.fr", "matthieu.arzel@imt-atlantique.fr", "michel.jezequel@imt-atlantique.fr"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489507684872, "id": "ICLR.cc/2017/workshop/-/paper52/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper52/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper52/AnonReviewer2", "ICLR.cc/2017/workshop/paper52/AnonReviewer1"], "reply": {"forum": "S15zzTmYx", "replyto": "S15zzTmYx", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper52/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper52/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489507684872}}}, {"tddate": null, "tmdate": 1489496735320, "tcdate": 1489496735320, "number": 1, "id": "ryDiqwSol", "invitation": "ICLR.cc/2017/workshop/-/paper52/public/comment", "forum": "S15zzTmYx", "replyto": "Bkv-e4Sig", "signatures": ["~Ghouthi_BOUKLI_HACENE1"], "readers": ["everyone"], "writers": ["~Ghouthi_BOUKLI_HACENE1"], "content": {"title": "Results updated with ImageNet subset", "comment": "We wish to thank the anonymous reviewer for his encouraging comment. We just updated the paper, adding results from another dataset (Imagenet subset 2, disjoint from the one used to train the CNN) in table 1, in addition to CIFAR. Note that CIFAR is already challenging using Inception feature extraction (87 % success with a K-NN). Importantly, our approach enables important reductions in both memory and processing complexity, which was the main motivation for this work."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "INCREMENTAL LEARNING WITH PRE-TRAINED CONVOLUTIONAL NEURAL NETWORKS AND BINARY ASSOCIATIVE MEMORIES", "abstract": "Thanks to their ability to absorb large amounts of data, Convolutional Neural Networks (CNNs) have become the state-of-the-art in various vision challenges, sometimes even on par with biological vision. CNNs rely on optimisation routines that typically require intensive computational power, thus the question of implementing CNNs on embedded architectures is a very active field of research. Of particular interest is the problem of incremental learning, where the device adapts to new observations or classes. To tackle this challenging problem, we propose to combine pre-trained CNNs with Binary Associative Memories, using product random sampling as an intermediate between the two methods. The obtained architecture requires significantly less computational power and memory usage than existing counterparts. Moreover, using various challenging vision datasets we\nshow that the proposed architecture is able to perform one-shot learning \u2013 even using only part of the dataset \u2013, while keeping very good accuracy.", "pdf": "/pdf/3d8bcf5d8d57750a4652db2da7f24118620f43df.pdf", "TL;DR": "Combining associative memories with quantized outputs of a Deep CNN enables lightweight one shot incremental learning.", "paperhash": "hacene|incremental_learning_with_pretrained_convolutional_neural_networks_and_binary_associative_memories", "conflicts": ["imt-atlantique.fr"], "keywords": ["Computer vision", "Deep learning", "Supervised Learning", "Transfer Learning"], "authors": ["Ghouthi Boukli Hacene", "Vincent Gripon", "Nicolas Farrugia", "Mattieu Arzel", "Michel Jezequel"], "authorids": ["Ghouthi.bouklihacene@imt-atlantique.fr", "vincent.gripon@imt-atlantique.fr", "nicolas.farrugia@imt-atlantique.fr", "matthieu.arzel@imt-atlantique.fr", "michel.jezequel@imt-atlantique.fr"]}, "tags": [], "invitation": {"tddate": null, "tmdate": 1487290898171, "tcdate": 1487290898171, "id": "ICLR.cc/2017/workshop/-/paper52/public/comment", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2017/workshop/paper52/reviewers"], "reply": {"forum": "S15zzTmYx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/workshop/reviewers", "ICLR.cc/2017/pcs"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1487290898171}}}, {"tddate": null, "tmdate": 1489481726628, "tcdate": 1489481726628, "number": 1, "id": "Bkv-e4Sig", "invitation": "ICLR.cc/2017/workshop/-/paper52/official/review", "forum": "S15zzTmYx", "replyto": "S15zzTmYx", "signatures": ["ICLR.cc/2017/workshop/paper52/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper52/AnonReviewer2"], "content": {"title": "Important topic, but surprisingly modest performance", "rating": "5: Marginally below acceptance threshold", "review": "The paper introduces a combination of transfer learning based on pre-trained DNNs with associative memory.\nThe idea itself makes sense and I am surprised no one has explored it before.\n\nHowever, the results seem to be relatively poor and thus the method does not seem to be very valuable as is.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "INCREMENTAL LEARNING WITH PRE-TRAINED CONVOLUTIONAL NEURAL NETWORKS AND BINARY ASSOCIATIVE MEMORIES", "abstract": "Thanks to their ability to absorb large amounts of data, Convolutional Neural Networks (CNNs) have become the state-of-the-art in various vision challenges, sometimes even on par with biological vision. CNNs rely on optimisation routines that typically require intensive computational power, thus the question of implementing CNNs on embedded architectures is a very active field of research. Of particular interest is the problem of incremental learning, where the device adapts to new observations or classes. To tackle this challenging problem, we propose to combine pre-trained CNNs with Binary Associative Memories, using product random sampling as an intermediate between the two methods. The obtained architecture requires significantly less computational power and memory usage than existing counterparts. Moreover, using various challenging vision datasets we\nshow that the proposed architecture is able to perform one-shot learning \u2013 even using only part of the dataset \u2013, while keeping very good accuracy.", "pdf": "/pdf/3d8bcf5d8d57750a4652db2da7f24118620f43df.pdf", "TL;DR": "Combining associative memories with quantized outputs of a Deep CNN enables lightweight one shot incremental learning.", "paperhash": "hacene|incremental_learning_with_pretrained_convolutional_neural_networks_and_binary_associative_memories", "conflicts": ["imt-atlantique.fr"], "keywords": ["Computer vision", "Deep learning", "Supervised Learning", "Transfer Learning"], "authors": ["Ghouthi Boukli Hacene", "Vincent Gripon", "Nicolas Farrugia", "Mattieu Arzel", "Michel Jezequel"], "authorids": ["Ghouthi.bouklihacene@imt-atlantique.fr", "vincent.gripon@imt-atlantique.fr", "nicolas.farrugia@imt-atlantique.fr", "matthieu.arzel@imt-atlantique.fr", "michel.jezequel@imt-atlantique.fr"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489507684872, "id": "ICLR.cc/2017/workshop/-/paper52/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper52/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper52/AnonReviewer2", "ICLR.cc/2017/workshop/paper52/AnonReviewer1"], "reply": {"forum": "S15zzTmYx", "replyto": "S15zzTmYx", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper52/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper52/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489507684872}}}], "count": 6}