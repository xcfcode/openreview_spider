{"notes": [{"id": "SJg9z6VFDr", "original": "S1esP6sUDr", "number": 422, "cdate": 1569438993790, "ddate": null, "tcdate": 1569438993790, "tmdate": 1577168267137, "tddate": null, "forum": "SJg9z6VFDr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["j.zhuang@yale.edu", "nicha.dvornek@yale.edu", "xiaoxiao.li@yale.edu", "james.duncan@yale.edu"], "title": "Ordinary differential equations on graph networks", "authors": ["Juntang Zhuang", "Nicha Dvornek", "Xiaoxiao Li", "James S. Duncan"], "pdf": "/pdf/5c8a94186856d657aa0a2d8b80c095bf6ddcf282.pdf", "TL;DR": "Apply ordinary differential equation model on graph structured data", "abstract": "Recently various neural networks have been proposed for irregularly structured data such as graphs and manifolds. To our knowledge, all existing graph networks have discrete depth. Inspired by neural ordinary differential equation (NODE) for data in the Euclidean domain, we extend the idea of continuous-depth models to graph data, and propose graph ordinary differential equation (GODE). The derivative of hidden node states are parameterized with a graph neural network, and the output states are the solution to this ordinary differential equation. We demonstrate two end-to-end methods for efficient training of GODE: (1) indirect back-propagation with the adjoint method; (2) direct back-propagation through the ODE solver, which accurately computes the gradient. We demonstrate that direct backprop outperforms the adjoint method in experiments. We then introduce a family of bijective blocks, which enables $\\mathcal{O}(1)$ memory consumption. We demonstrate that GODE can be easily adapted to different existing graph neural networks and improve accuracy. We validate the performance of GODE in both semi-supervised node classification tasks and graph classification tasks. Our GODE model achieves a continuous model in time, memory efficiency, accurate gradient estimation, and generalizability with different graph networks.", "keywords": ["Graph Networks", "Ordinary differential equation"], "paperhash": "zhuang|ordinary_differential_equations_on_graph_networks", "code": "https://www.dropbox.com/sh/sgaid3efh4eqmjl/AAB-DFXvNq_Pf313UqSLl4VPa?dl=0", "original_pdf": "/attachment/d2b601834ddcc9611d4dca58f62fabb6213cdee8.pdf", "_bibtex": "@misc{\nzhuang2020ordinary,\ntitle={Ordinary differential equations on graph networks},\nauthor={Juntang Zhuang and Nicha Dvornek and Xiaoxiao Li and James S. Duncan},\nyear={2020},\nurl={https://openreview.net/forum?id=SJg9z6VFDr}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 17, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "DVBlUw7Iox", "original": null, "number": 1, "cdate": 1576798695965, "ddate": null, "tcdate": 1576798695965, "tmdate": 1576800939664, "tddate": null, "forum": "SJg9z6VFDr", "replyto": "SJg9z6VFDr", "invitation": "ICLR.cc/2020/Conference/Paper422/-/Decision", "content": {"decision": "Reject", "comment": "This paper introduces a few ideas to potentially improve the performance of neural ODEs on graph networks.  However, the reviewers disagreed about the motivations for the proposed modifications.  Specifically, it's not clear that neural ODEs provide a more advantageous parameterization in this setting than standard discrete networks.\n\nIt's also not clear at all why the authors are discussion graph neural networks in particular, as all of their proposed changes would apply to all types of network.\n\nAnother major problem I had with this paper was the assertion that the running the original system backwards leads to large numerical error.  This is a plausible claim, but it was never verified.  It's extremely easy to check (e.g. by comparing the reconstructed initial state at t0 with the true original state at t0, or by comparing gradients computed by different methods).  It's also not clear if the authors enforced the constraints on their dynamics function needed to ensure that a unique solution exists in the first place.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["j.zhuang@yale.edu", "nicha.dvornek@yale.edu", "xiaoxiao.li@yale.edu", "james.duncan@yale.edu"], "title": "Ordinary differential equations on graph networks", "authors": ["Juntang Zhuang", "Nicha Dvornek", "Xiaoxiao Li", "James S. Duncan"], "pdf": "/pdf/5c8a94186856d657aa0a2d8b80c095bf6ddcf282.pdf", "TL;DR": "Apply ordinary differential equation model on graph structured data", "abstract": "Recently various neural networks have been proposed for irregularly structured data such as graphs and manifolds. To our knowledge, all existing graph networks have discrete depth. Inspired by neural ordinary differential equation (NODE) for data in the Euclidean domain, we extend the idea of continuous-depth models to graph data, and propose graph ordinary differential equation (GODE). The derivative of hidden node states are parameterized with a graph neural network, and the output states are the solution to this ordinary differential equation. We demonstrate two end-to-end methods for efficient training of GODE: (1) indirect back-propagation with the adjoint method; (2) direct back-propagation through the ODE solver, which accurately computes the gradient. We demonstrate that direct backprop outperforms the adjoint method in experiments. We then introduce a family of bijective blocks, which enables $\\mathcal{O}(1)$ memory consumption. We demonstrate that GODE can be easily adapted to different existing graph neural networks and improve accuracy. We validate the performance of GODE in both semi-supervised node classification tasks and graph classification tasks. Our GODE model achieves a continuous model in time, memory efficiency, accurate gradient estimation, and generalizability with different graph networks.", "keywords": ["Graph Networks", "Ordinary differential equation"], "paperhash": "zhuang|ordinary_differential_equations_on_graph_networks", "code": "https://www.dropbox.com/sh/sgaid3efh4eqmjl/AAB-DFXvNq_Pf313UqSLl4VPa?dl=0", "original_pdf": "/attachment/d2b601834ddcc9611d4dca58f62fabb6213cdee8.pdf", "_bibtex": "@misc{\nzhuang2020ordinary,\ntitle={Ordinary differential equations on graph networks},\nauthor={Juntang Zhuang and Nicha Dvornek and Xiaoxiao Li and James S. Duncan},\nyear={2020},\nurl={https://openreview.net/forum?id=SJg9z6VFDr}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "SJg9z6VFDr", "replyto": "SJg9z6VFDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795712259, "tmdate": 1576800261611, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper422/-/Decision"}}}, {"id": "ryxO66ok5r", "original": null, "number": 3, "cdate": 1571958208085, "ddate": null, "tcdate": 1571958208085, "tmdate": 1574388376630, "tddate": null, "forum": "SJg9z6VFDr", "replyto": "SJg9z6VFDr", "invitation": "ICLR.cc/2020/Conference/Paper422/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #2", "review": "Summary: This paper proposed a deep model called graph-ODE (GODE), which extends the continuous deep model, neural ODE [1], to graph structured data. Two methods of computing the gradient are proposed, one is the adjoint method proposed in [1] which is memory-efficient but not perfectly accurate, and the other method is to discretize the ODE and compute an accurate gradient, which is also memory-efficient but relies on an invertible architecture.\n\nIn general, this paper contains many interesting thoughts, but to me, it seems these ideas are not new and have been proposed before[1,2,3,4]. I could not find a strong contribution of this paper. \n\n- - - about the ODE stability issue\n\nAmong invertible deep models, the advantage of ODE-based continuous models [1] is: there is *no restriction* on the form of f. The drawback is the computed gradient has an error, depending on discretization step size and stability. This paper pointed out the stability problem (which is also discussed in [2,3]), but do not provide a solution in the domain of continuous deep models. \n\nInstead, the solution they provided is to use a discretized version and compute the gradient accurately. Then it becomes a standard invertible DL model with discrete layers, where the invertible building blocks have a specific *restricted form*. The 'adjoint method with discrete-time' in Eq (10) is the same as the chain rule, which has also be pointed out in [4]. To this point, I think GODE is in the class of discrete invertible DL models trained by gradient descent. I think it less related to continuous models, except the step size can be adaptive in the forward pass.\n\n- - - about the invertible building block\n\nThe proposed invertible building block replaces 'sum' in [5] by a function \\psi. This is not novel enough to serve as a contribution.\n\n- - - comparison to graph neural network\n\nI think it is interesting to apply ODE techniques or other invertible architectures to graph-structured data, for which I didn't see similar works before and could be a contribution of this paper. However, for the experimental results shown in table 3 and 4, the improvement is really small. A stronger result is needed to demonstrate the advantages.\n\n[1] Chen, Tian Qi, et al. \"Neural ordinary differential equations.\" Advances in neural information processing systems. 2018.\n[2] Chang, Bo, et al. \"Reversible architectures for arbitrarily deep residual neural networks.\" Thirty-Second AAAI Conference on Artificial Intelligence. 2018.\n[3] Behrmann, Jens, et al. \"Invertible Residual Networks.\" International Conference on Machine Learning. 2019.\n[4] Li, Qianxiao, et al. \"Maximum principle based algorithms for deep learning.\" The Journal of Machine Learning Research 18.1 (2017): 5998-6026.\n[5] Gomez, Aidan N., et al. \"The reversible residual network: Backpropagation without storing activations.\" Advances in neural information processing systems. 2017.\n\n\n\n-------------after reading the response\n\nI'd like to thank the authors for their explanations.\nHowever, the authors' explanation of the benefit of transforming the invertible building block into an ODE model is still not convincing to me.\n\nThe authors explained that ODE solutions can represent highly nonlinear functions. However, discrete NN also represents highly nonlinear functions (e.g. with Relu activation, they are *piecewise* linear, but they are highly nonlinear)! From my point of view, their difference is that ODE model is more smooth. However, the benefit of using a smoother model is still unclear. For the example that the authors provided, $\\sin x$, why being able to represent that kind of function is an advantage for the graph classification problem? Why is this a good model bias? I think the authors' responses are still not convincing enough, so I choose to retain the score.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper422/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper422/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["j.zhuang@yale.edu", "nicha.dvornek@yale.edu", "xiaoxiao.li@yale.edu", "james.duncan@yale.edu"], "title": "Ordinary differential equations on graph networks", "authors": ["Juntang Zhuang", "Nicha Dvornek", "Xiaoxiao Li", "James S. Duncan"], "pdf": "/pdf/5c8a94186856d657aa0a2d8b80c095bf6ddcf282.pdf", "TL;DR": "Apply ordinary differential equation model on graph structured data", "abstract": "Recently various neural networks have been proposed for irregularly structured data such as graphs and manifolds. To our knowledge, all existing graph networks have discrete depth. Inspired by neural ordinary differential equation (NODE) for data in the Euclidean domain, we extend the idea of continuous-depth models to graph data, and propose graph ordinary differential equation (GODE). The derivative of hidden node states are parameterized with a graph neural network, and the output states are the solution to this ordinary differential equation. We demonstrate two end-to-end methods for efficient training of GODE: (1) indirect back-propagation with the adjoint method; (2) direct back-propagation through the ODE solver, which accurately computes the gradient. We demonstrate that direct backprop outperforms the adjoint method in experiments. We then introduce a family of bijective blocks, which enables $\\mathcal{O}(1)$ memory consumption. We demonstrate that GODE can be easily adapted to different existing graph neural networks and improve accuracy. We validate the performance of GODE in both semi-supervised node classification tasks and graph classification tasks. Our GODE model achieves a continuous model in time, memory efficiency, accurate gradient estimation, and generalizability with different graph networks.", "keywords": ["Graph Networks", "Ordinary differential equation"], "paperhash": "zhuang|ordinary_differential_equations_on_graph_networks", "code": "https://www.dropbox.com/sh/sgaid3efh4eqmjl/AAB-DFXvNq_Pf313UqSLl4VPa?dl=0", "original_pdf": "/attachment/d2b601834ddcc9611d4dca58f62fabb6213cdee8.pdf", "_bibtex": "@misc{\nzhuang2020ordinary,\ntitle={Ordinary differential equations on graph networks},\nauthor={Juntang Zhuang and Nicha Dvornek and Xiaoxiao Li and James S. Duncan},\nyear={2020},\nurl={https://openreview.net/forum?id=SJg9z6VFDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SJg9z6VFDr", "replyto": "SJg9z6VFDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper422/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper422/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574722376000, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper422/Reviewers"], "noninvitees": [], "tcdate": 1570237752369, "tmdate": 1574723082307, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper422/-/Official_Review"}}}, {"id": "rkxwDhWojS", "original": null, "number": 14, "cdate": 1573751902946, "ddate": null, "tcdate": 1573751902946, "tmdate": 1573832151917, "tddate": null, "forum": "SJg9z6VFDr", "replyto": "S1ePzPs_jS", "invitation": "ICLR.cc/2020/Conference/Paper422/-/Official_Comment", "content": {"title": "Clarification on contribution to accurate gradient estimation in version (1)  (Part 2/2):", "comment": "Thank you for pointing out the importance of free-form for version (1). We want to clarify our contributions in version (1):\nIt\u2019s not only a new application to graph data, but also a generic method to accurately estimate the gradient for neural-ODEs. This is a fundamental problem to deep learning models, which is not extensively studied in the original Neural ODE paper.\n\nPlease see end of intro in the revised paper for an updated list of our contribution."}, "signatures": ["ICLR.cc/2020/Conference/Paper422/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper422/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["j.zhuang@yale.edu", "nicha.dvornek@yale.edu", "xiaoxiao.li@yale.edu", "james.duncan@yale.edu"], "title": "Ordinary differential equations on graph networks", "authors": ["Juntang Zhuang", "Nicha Dvornek", "Xiaoxiao Li", "James S. Duncan"], "pdf": "/pdf/5c8a94186856d657aa0a2d8b80c095bf6ddcf282.pdf", "TL;DR": "Apply ordinary differential equation model on graph structured data", "abstract": "Recently various neural networks have been proposed for irregularly structured data such as graphs and manifolds. To our knowledge, all existing graph networks have discrete depth. Inspired by neural ordinary differential equation (NODE) for data in the Euclidean domain, we extend the idea of continuous-depth models to graph data, and propose graph ordinary differential equation (GODE). The derivative of hidden node states are parameterized with a graph neural network, and the output states are the solution to this ordinary differential equation. We demonstrate two end-to-end methods for efficient training of GODE: (1) indirect back-propagation with the adjoint method; (2) direct back-propagation through the ODE solver, which accurately computes the gradient. We demonstrate that direct backprop outperforms the adjoint method in experiments. We then introduce a family of bijective blocks, which enables $\\mathcal{O}(1)$ memory consumption. We demonstrate that GODE can be easily adapted to different existing graph neural networks and improve accuracy. We validate the performance of GODE in both semi-supervised node classification tasks and graph classification tasks. Our GODE model achieves a continuous model in time, memory efficiency, accurate gradient estimation, and generalizability with different graph networks.", "keywords": ["Graph Networks", "Ordinary differential equation"], "paperhash": "zhuang|ordinary_differential_equations_on_graph_networks", "code": "https://www.dropbox.com/sh/sgaid3efh4eqmjl/AAB-DFXvNq_Pf313UqSLl4VPa?dl=0", "original_pdf": "/attachment/d2b601834ddcc9611d4dca58f62fabb6213cdee8.pdf", "_bibtex": "@misc{\nzhuang2020ordinary,\ntitle={Ordinary differential equations on graph networks},\nauthor={Juntang Zhuang and Nicha Dvornek and Xiaoxiao Li and James S. Duncan},\nyear={2020},\nurl={https://openreview.net/forum?id=SJg9z6VFDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJg9z6VFDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper422/Authors", "ICLR.cc/2020/Conference/Paper422/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper422/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper422/Reviewers", "ICLR.cc/2020/Conference/Paper422/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper422/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper422/Authors|ICLR.cc/2020/Conference/Paper422/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504171711, "tmdate": 1576860540355, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper422/Authors", "ICLR.cc/2020/Conference/Paper422/Reviewers", "ICLR.cc/2020/Conference/Paper422/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper422/-/Official_Comment"}}}, {"id": "HkgYBCWYjB", "original": null, "number": 10, "cdate": 1573621313051, "ddate": null, "tcdate": 1573621313051, "tmdate": 1573832065072, "tddate": null, "forum": "SJg9z6VFDr", "replyto": "S1ePzPs_jS", "invitation": "ICLR.cc/2020/Conference/Paper422/-/Official_Comment", "content": {"title": "Benefits of modeling invertible blocks as ODE (Part 1/2)", "comment": "Thank you for the comments, we explain the benefits to model invertible blocks as ODEs.\n\n1.   In terms of practical performance, ODE version of invertible blocks has the potential to outperform discrete-layer version if it's well trained, which is shown in table of thread C. (We also observe NODE18 outperforms ResNet101 in image classification in thread A)\n\nThis could be because when modifying an invertible block into an ODE, the model has the potential to capture some highly nonlinear functions. For example, it takes many layers of discrete model with ReLU activations to capture the function $y=sint$, because ReLU activations make the network behave piecewise linear; but it can be captured by a 1-layer ODE, \n$$\n\\begin{bmatrix}\n\\frac{da}{dt} \\\\ \\frac{db}{dt}\n\\end{bmatrix}\n= \\begin{bmatrix}   0& 1\\\\ -1 & 0 \\end{bmatrix}  \\begin{bmatrix} a \\\\ b  \\end{bmatrix}\n$$\nWhere $a = y, b = \\frac{dy}{dt}$.\n\n2.    Other benefits include:\n(1) Explicit error estimation with most ODE solvers, so the output error estimation of ODE can be used to determine decision confidence.\n(2) Speed-accuracy trade-off by controlling error tolerance.\n(3) Robustness to input perturbation and adversarial attack, which is empirically and theoretically analyzed in [1]\n\n[1] Yan, Hanshu, et al. \"On Robustness of Neural Ordinary Differential Equations.\" arXiv preprint arXiv:1910.05513 (2019)."}, "signatures": ["ICLR.cc/2020/Conference/Paper422/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper422/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["j.zhuang@yale.edu", "nicha.dvornek@yale.edu", "xiaoxiao.li@yale.edu", "james.duncan@yale.edu"], "title": "Ordinary differential equations on graph networks", "authors": ["Juntang Zhuang", "Nicha Dvornek", "Xiaoxiao Li", "James S. Duncan"], "pdf": "/pdf/5c8a94186856d657aa0a2d8b80c095bf6ddcf282.pdf", "TL;DR": "Apply ordinary differential equation model on graph structured data", "abstract": "Recently various neural networks have been proposed for irregularly structured data such as graphs and manifolds. To our knowledge, all existing graph networks have discrete depth. Inspired by neural ordinary differential equation (NODE) for data in the Euclidean domain, we extend the idea of continuous-depth models to graph data, and propose graph ordinary differential equation (GODE). The derivative of hidden node states are parameterized with a graph neural network, and the output states are the solution to this ordinary differential equation. We demonstrate two end-to-end methods for efficient training of GODE: (1) indirect back-propagation with the adjoint method; (2) direct back-propagation through the ODE solver, which accurately computes the gradient. We demonstrate that direct backprop outperforms the adjoint method in experiments. We then introduce a family of bijective blocks, which enables $\\mathcal{O}(1)$ memory consumption. We demonstrate that GODE can be easily adapted to different existing graph neural networks and improve accuracy. We validate the performance of GODE in both semi-supervised node classification tasks and graph classification tasks. Our GODE model achieves a continuous model in time, memory efficiency, accurate gradient estimation, and generalizability with different graph networks.", "keywords": ["Graph Networks", "Ordinary differential equation"], "paperhash": "zhuang|ordinary_differential_equations_on_graph_networks", "code": "https://www.dropbox.com/sh/sgaid3efh4eqmjl/AAB-DFXvNq_Pf313UqSLl4VPa?dl=0", "original_pdf": "/attachment/d2b601834ddcc9611d4dca58f62fabb6213cdee8.pdf", "_bibtex": "@misc{\nzhuang2020ordinary,\ntitle={Ordinary differential equations on graph networks},\nauthor={Juntang Zhuang and Nicha Dvornek and Xiaoxiao Li and James S. Duncan},\nyear={2020},\nurl={https://openreview.net/forum?id=SJg9z6VFDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJg9z6VFDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper422/Authors", "ICLR.cc/2020/Conference/Paper422/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper422/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper422/Reviewers", "ICLR.cc/2020/Conference/Paper422/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper422/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper422/Authors|ICLR.cc/2020/Conference/Paper422/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504171711, "tmdate": 1576860540355, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper422/Authors", "ICLR.cc/2020/Conference/Paper422/Reviewers", "ICLR.cc/2020/Conference/Paper422/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper422/-/Official_Comment"}}}, {"id": "r1lWEi3tsH", "original": null, "number": 12, "cdate": 1573665576699, "ddate": null, "tcdate": 1573665576699, "tmdate": 1573829868344, "tddate": null, "forum": "SJg9z6VFDr", "replyto": "SJg9z6VFDr", "invitation": "ICLR.cc/2020/Conference/Paper422/-/Official_Comment", "content": {"title": "Paper revision", "comment": "We edit our abstract and intro, and re-order contents for better clarity on the generalizability of our method to general free-form NODE, and contribution to a fundamental problem: accurate gradient estimation.\n\n1.    Re-order of contents in the original submission:\nSection in revision    Section in original submission                        Contents\n3.1                                    3.2                                                    From discrete to continuous models\n3.2                                    4.1                                                    Analysis of adjoint method\n3.3 (Algo 1)               4.2 (Algo  2 in Appendix B)                Our method for accurate gradient estimation\n4.1                                   3.1                                                     Message passing in GNN\n5.3                                 Appendix C                                       Application to general NODE\nAppendix B                   4.3                                                     Invertible blocks\n\n2.    Modification of contents:\n (1)  We added paired t-test results to table 3 and 4\n (2)  We addressed the relation to the over-smooth issue in Sec4.2 and Table 5 \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper422/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper422/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["j.zhuang@yale.edu", "nicha.dvornek@yale.edu", "xiaoxiao.li@yale.edu", "james.duncan@yale.edu"], "title": "Ordinary differential equations on graph networks", "authors": ["Juntang Zhuang", "Nicha Dvornek", "Xiaoxiao Li", "James S. Duncan"], "pdf": "/pdf/5c8a94186856d657aa0a2d8b80c095bf6ddcf282.pdf", "TL;DR": "Apply ordinary differential equation model on graph structured data", "abstract": "Recently various neural networks have been proposed for irregularly structured data such as graphs and manifolds. To our knowledge, all existing graph networks have discrete depth. Inspired by neural ordinary differential equation (NODE) for data in the Euclidean domain, we extend the idea of continuous-depth models to graph data, and propose graph ordinary differential equation (GODE). The derivative of hidden node states are parameterized with a graph neural network, and the output states are the solution to this ordinary differential equation. We demonstrate two end-to-end methods for efficient training of GODE: (1) indirect back-propagation with the adjoint method; (2) direct back-propagation through the ODE solver, which accurately computes the gradient. We demonstrate that direct backprop outperforms the adjoint method in experiments. We then introduce a family of bijective blocks, which enables $\\mathcal{O}(1)$ memory consumption. We demonstrate that GODE can be easily adapted to different existing graph neural networks and improve accuracy. We validate the performance of GODE in both semi-supervised node classification tasks and graph classification tasks. Our GODE model achieves a continuous model in time, memory efficiency, accurate gradient estimation, and generalizability with different graph networks.", "keywords": ["Graph Networks", "Ordinary differential equation"], "paperhash": "zhuang|ordinary_differential_equations_on_graph_networks", "code": "https://www.dropbox.com/sh/sgaid3efh4eqmjl/AAB-DFXvNq_Pf313UqSLl4VPa?dl=0", "original_pdf": "/attachment/d2b601834ddcc9611d4dca58f62fabb6213cdee8.pdf", "_bibtex": "@misc{\nzhuang2020ordinary,\ntitle={Ordinary differential equations on graph networks},\nauthor={Juntang Zhuang and Nicha Dvornek and Xiaoxiao Li and James S. Duncan},\nyear={2020},\nurl={https://openreview.net/forum?id=SJg9z6VFDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJg9z6VFDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper422/Authors", "ICLR.cc/2020/Conference/Paper422/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper422/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper422/Reviewers", "ICLR.cc/2020/Conference/Paper422/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper422/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper422/Authors|ICLR.cc/2020/Conference/Paper422/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504171711, "tmdate": 1576860540355, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper422/Authors", "ICLR.cc/2020/Conference/Paper422/Reviewers", "ICLR.cc/2020/Conference/Paper422/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper422/-/Official_Comment"}}}, {"id": "Ske6GVVSjB", "original": null, "number": 8, "cdate": 1573368852997, "ddate": null, "tcdate": 1573368852997, "tmdate": 1573752093554, "tddate": null, "forum": "SJg9z6VFDr", "replyto": "HJe4NPehKS", "invitation": "ICLR.cc/2020/Conference/Paper422/-/Official_Comment", "content": {"title": "Response to Reviewer #3", "comment": "We sincerely thank the reviewer for your feedback. We address the reviewer\u2019s concerns:\n\n1.    Difference between neural ODE and graph ODE.\nFrom the perspective of the model, they are both ODE models $\\frac{dz}{dt}=f(z,t)$, except the actual form of $f$ is different. The adjoint and gradient are calculated in the same way. \nOur method is a generic solution and can handle both cases.\n\n2. Relation to discrete adjoint method\n    To our knowledge, discrete adjoint method is a discretize-then-optimize method, where the continuous function is discretized at pre-defined grids [1]. With pre-defined grids, the continuous problem is actually transformed into a discrete problem with more constraints. This is related to R2\u2019s concern that we are not directly solving the continuous problem.\n    Our method is an optimize-then-discretize approach. The grid points are not predefined, instead they are adaptively computed under error control. This is related to our discussion in thread A.2. \n    For our method, because the grid is adaptively determined, the grid discretization will be very different for different input data; while the grid remains the same for all data in discrete-adjoint methods.\n    Furthermore, because our methods are adaptive, the integration value is robust to different ODE solvers, as long as the error tolerance is small enough. However, with fixed grid, there\u2019s no control on error, hence different solvers could generate very different results.\n    This argument is related to thread A.2, clarifying that our method is truly solving a continuous problem, rather than converting it to a discrete problem.\n\n3. Experiments\n    The numbers reported in our paper are averages among 10 runs, with standard deviation also reported. We will clarify this in the paper.\n\n4. Running time\n    For a ResNet18, and its direct modification into an ODE, the discrete model has the shortest running time; the running time of ODE is roughly 4 times longer than the discrete model, when trained both with our method and the adjoint method in [2]. The longer running time comes from the ODE model, where multiple steps need to be determined for numerical integration.\n \n5. Code and pretrained weights: https://www.dropbox.com/sh/sgaid3efh4eqmjl/AAB-DFXvNq_Pf313UqSLl4VPa?dl=0\n\n6.    Considering the comments and confusion from all reviewers, we clarify our contributions:\n\n(a)        A general method for ODE solver that works well in practice\n\n        ----Our method is applicable to both CNN-ODE and graph-ODE of different structures, and deals with a fundamental problem: how to estimate the gradient.\n        ----To our knowledge, this is the first paper that makes neural-ODE achieve comparable or even higher accuracy in benchmark classification tasks compared to a state-of-the-art discrete-layer network.\n        -----A general method for free-form functions, and truly designed for continuous models, as discussed in thread A.\n\n(b)    Application to graph networks enables new theoretical aspects, such as over-smoothing phenomena, which is in part discussed in response to R1 (https://openreview.net/forum?id=SJg9z6VFDr&noteId=B1esk0ZHjS )\n\n[1] Nadarajah, Siva, and Antony Jameson. \"Studies of the continuous and discrete adjoint approaches to viscous automatic aerodynamic shape optimization.\"2001.\n[2] Chen, Tian Qi, et al. \"Neural ordinary differential equations.\" Advances in neural information processing systems. 2018.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper422/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper422/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["j.zhuang@yale.edu", "nicha.dvornek@yale.edu", "xiaoxiao.li@yale.edu", "james.duncan@yale.edu"], "title": "Ordinary differential equations on graph networks", "authors": ["Juntang Zhuang", "Nicha Dvornek", "Xiaoxiao Li", "James S. Duncan"], "pdf": "/pdf/5c8a94186856d657aa0a2d8b80c095bf6ddcf282.pdf", "TL;DR": "Apply ordinary differential equation model on graph structured data", "abstract": "Recently various neural networks have been proposed for irregularly structured data such as graphs and manifolds. To our knowledge, all existing graph networks have discrete depth. Inspired by neural ordinary differential equation (NODE) for data in the Euclidean domain, we extend the idea of continuous-depth models to graph data, and propose graph ordinary differential equation (GODE). The derivative of hidden node states are parameterized with a graph neural network, and the output states are the solution to this ordinary differential equation. We demonstrate two end-to-end methods for efficient training of GODE: (1) indirect back-propagation with the adjoint method; (2) direct back-propagation through the ODE solver, which accurately computes the gradient. We demonstrate that direct backprop outperforms the adjoint method in experiments. We then introduce a family of bijective blocks, which enables $\\mathcal{O}(1)$ memory consumption. We demonstrate that GODE can be easily adapted to different existing graph neural networks and improve accuracy. We validate the performance of GODE in both semi-supervised node classification tasks and graph classification tasks. Our GODE model achieves a continuous model in time, memory efficiency, accurate gradient estimation, and generalizability with different graph networks.", "keywords": ["Graph Networks", "Ordinary differential equation"], "paperhash": "zhuang|ordinary_differential_equations_on_graph_networks", "code": "https://www.dropbox.com/sh/sgaid3efh4eqmjl/AAB-DFXvNq_Pf313UqSLl4VPa?dl=0", "original_pdf": "/attachment/d2b601834ddcc9611d4dca58f62fabb6213cdee8.pdf", "_bibtex": "@misc{\nzhuang2020ordinary,\ntitle={Ordinary differential equations on graph networks},\nauthor={Juntang Zhuang and Nicha Dvornek and Xiaoxiao Li and James S. Duncan},\nyear={2020},\nurl={https://openreview.net/forum?id=SJg9z6VFDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJg9z6VFDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper422/Authors", "ICLR.cc/2020/Conference/Paper422/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper422/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper422/Reviewers", "ICLR.cc/2020/Conference/Paper422/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper422/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper422/Authors|ICLR.cc/2020/Conference/Paper422/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504171711, "tmdate": 1576860540355, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper422/Authors", "ICLR.cc/2020/Conference/Paper422/Reviewers", "ICLR.cc/2020/Conference/Paper422/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper422/-/Official_Comment"}}}, {"id": "B1esk0ZHjS", "original": null, "number": 7, "cdate": 1573359074780, "ddate": null, "tcdate": 1573359074780, "tmdate": 1573698646100, "tddate": null, "forum": "SJg9z6VFDr", "replyto": "Syg40shatr", "invitation": "ICLR.cc/2020/Conference/Paper422/-/Official_Comment", "content": {"title": "Response to Reviewer # 1", "comment": "We sincerely thank the reviewer for your efforts and comments. We will address the reviewer\u2019s concerns and state our contributions:\n\n1. Contribution to graph neural networks, connection with over-smoothing phenomena\n \n-----(a) intuition\nWe sincerely thank the reviewer for pointing out that ODE and the over-smoothing issue of GCN could be related. In fact, we did not start this project with neural ODE; instead we started with an intuition that GNN could be related to a continuous \u201cheat transfer\u201d model, then we tried to build a good model for the continuous process, so we started looking at ODEs but found current method can not train an ODE well. \n\nIn the heat-transfer model, the feature of each node is analogous to temperature, and the messaging passing process is analogous to heat transfer between nodes. Time of heat transfer process is analogous to depths of discrete-layer GCN models. If time is too short, there will be insufficient heat transfer, equivalent to a node not having enough information from its neighbors to update; if time is too long, all nodes end up in the same temperature, corresponding to the over-smoothing issue with GNN.\n\n----(b) Mathematical analysis\nWe sincerely thank the reviewer for pointing out previous work on over-smoothing, which leads us from intuition to some mathematical analysis. We show our analysis below:\n\nIt\u2019s demonstrated that graph convolution is a special case of Laplacian smoothing in [1], which can be written as \n$Y = (I - \\gamma \\tilde{D}^{-1} \\tilde{L})X$ \nwhere $X$ and $Y$ are the input and output of a graph-conv layer respectively, $\\tilde{A}=A+I$ where $A$ is the adjacency matrix, and $\\tilde{D}$ is the corresponding degree matrix of $\\tilde{A}$, and $\\gamma$ is a positive scaling constant.  \nIf we replace normalized Laplacian with symmetricly normalized Laplacian, the continuous process becomes:\n$Y = (I - \\gamma \\tilde{D}^{-1/2} \\tilde{L} \\tilde{D}^{-1/2} )X$\nWhen it\u2019s modified from a discrete model to a continuous model, the continuous smoothing process is:\n$\\frac{dX}{dt} = - \\gamma \\tilde{D}^{-1/2} \\tilde{L} \\tilde{D}^{-1/2} X$\nSince all eigenvalues of the symmetrically normalized Laplacian are real and non-negative, then all eigenvalues of the above ODE are real and non-positive. \nSuppose all eigenvalues of the normalized Laplacian are non-zero. In this case, the ODE has only negative eigenvalues, hence the ODE above is $asymptotically \\ stable$ [2]. Hence as time $t$ grows sufficiently large, all trajectories are close enough. \nIn experiments, this suggests if integration time $T$ is large enough, all nodes (from different classes) will have very similar features, thus the classification accuracy will drop.\n\n----(c) Experiments\nWe validate this with experiments. Node classification accuracy varying with integration time is measured on the Cora dataset. Results are shown below:\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\nIntegration time    0.5      1.0       1.5       2.0       5.0      10.0     20.0    100.0\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\nAccuracy                  80.5   81.6     80.1    80.1     79.3     77.6     68.9     35.1\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\n\nWhen integration time is small (0.5), nodes do not aggregate neighbor information. When integration time is too long, node attributes tend to the same value because of asymptotic behaviour of ODE, as mentioned above. Both cases generate inferior accuracy.\n\n2.  Generalization to general ODEs.\n     We show that our method can be applied to general ODEs, with free-form functions, can deal with continuous models, and are robust to different ODE solvers. Details are in appendix C of original version (Sec 5.3 in revision), and we post it in thread A (https://openreview.net/forum?id=SJg9z6VFDr&noteId=B1eeWWA7sr ).\n\n3.  Our method supports free-form functions and does not depend on invertible block\n    Details are in thread B (https://openreview.net/forum?id=SJg9z6VFDr&noteId=SkgzfxCmoH ).  Our method is memory efficient for free-form functions, with the special case of invertible block we achieve even lower memory. Hence the contribution cannot be simply attributed to invertible block.\n\n4.  Better performance for both graph network and CNN\n     We validate our method both in CNN-ODE (thread A.3) and GNN-ODE (thread C https://openreview.net/forum?id=SJg9z6VFDr&noteId=HklsTJ07jB ). The CNN-ODE is a *free-form* function; for GNN-ODE, we tested both free-form and invertible block (restricted form). Our method significantly outperforms (under paired t-test) discrete models for graph networks; it reduces error rate from 19% to 5% on CIFAR10 compared with results using the adjoint method [3] and Augmented-NODE[4].\n\n\n[1] Li, Qimai, et al. \"Deeper insights into graph convolutional networks for semi-supervised learning\"2018.\n[2] Lyapunov, A. M. The General Problem of the Stability of Motion\n[3] Chen, Tian Qi, et al. \"Neural ordinary differential equations.\" 2018\n[4] Dupont, et al. \"Augmented neural odes.\" (2019)."}, "signatures": ["ICLR.cc/2020/Conference/Paper422/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper422/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["j.zhuang@yale.edu", "nicha.dvornek@yale.edu", "xiaoxiao.li@yale.edu", "james.duncan@yale.edu"], "title": "Ordinary differential equations on graph networks", "authors": ["Juntang Zhuang", "Nicha Dvornek", "Xiaoxiao Li", "James S. Duncan"], "pdf": "/pdf/5c8a94186856d657aa0a2d8b80c095bf6ddcf282.pdf", "TL;DR": "Apply ordinary differential equation model on graph structured data", "abstract": "Recently various neural networks have been proposed for irregularly structured data such as graphs and manifolds. To our knowledge, all existing graph networks have discrete depth. Inspired by neural ordinary differential equation (NODE) for data in the Euclidean domain, we extend the idea of continuous-depth models to graph data, and propose graph ordinary differential equation (GODE). The derivative of hidden node states are parameterized with a graph neural network, and the output states are the solution to this ordinary differential equation. We demonstrate two end-to-end methods for efficient training of GODE: (1) indirect back-propagation with the adjoint method; (2) direct back-propagation through the ODE solver, which accurately computes the gradient. We demonstrate that direct backprop outperforms the adjoint method in experiments. We then introduce a family of bijective blocks, which enables $\\mathcal{O}(1)$ memory consumption. We demonstrate that GODE can be easily adapted to different existing graph neural networks and improve accuracy. We validate the performance of GODE in both semi-supervised node classification tasks and graph classification tasks. Our GODE model achieves a continuous model in time, memory efficiency, accurate gradient estimation, and generalizability with different graph networks.", "keywords": ["Graph Networks", "Ordinary differential equation"], "paperhash": "zhuang|ordinary_differential_equations_on_graph_networks", "code": "https://www.dropbox.com/sh/sgaid3efh4eqmjl/AAB-DFXvNq_Pf313UqSLl4VPa?dl=0", "original_pdf": "/attachment/d2b601834ddcc9611d4dca58f62fabb6213cdee8.pdf", "_bibtex": "@misc{\nzhuang2020ordinary,\ntitle={Ordinary differential equations on graph networks},\nauthor={Juntang Zhuang and Nicha Dvornek and Xiaoxiao Li and James S. Duncan},\nyear={2020},\nurl={https://openreview.net/forum?id=SJg9z6VFDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJg9z6VFDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper422/Authors", "ICLR.cc/2020/Conference/Paper422/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper422/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper422/Reviewers", "ICLR.cc/2020/Conference/Paper422/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper422/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper422/Authors|ICLR.cc/2020/Conference/Paper422/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504171711, "tmdate": 1576860540355, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper422/Authors", "ICLR.cc/2020/Conference/Paper422/Reviewers", "ICLR.cc/2020/Conference/Paper422/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper422/-/Official_Comment"}}}, {"id": "SJgPlF0Kjr", "original": null, "number": 13, "cdate": 1573673198954, "ddate": null, "tcdate": 1573673198954, "tmdate": 1573673398154, "tddate": null, "forum": "SJg9z6VFDr", "replyto": "BkxLkK2Yor", "invitation": "ICLR.cc/2020/Conference/Paper422/-/Official_Comment", "content": {"title": "Potential direction to overcome oversmoothing for graph ODE", "comment": "Thanks for your discussion, we agree that the oversmoothing phenomena on graph ODE can be interpreted as the ODE systems converge to equilibrium. From this point, here are several potential solutions:\n\n1.    Limit the integration time to a small value, since the convergence phenomena occurs when time $t$ goes sufficiently large.\n\n2.   Limit the amplitude of the eigenvalue of $f$. \nIgnoring the constant due to initial value, the solution to a linear ODE system $\\frac{dX}{dt}=AX$ is $X(t) = e^{At}X(0)$\nWriting in the form of eigenvalue $\\lambda_i$ and eigenvectors $u_i$ of matrix $A$, the solution is:\n$X(t) = c_1 e^{\\lambda_1}u_1 + c_2 e^{\\lambda_2}u_2 + ...$.\n\nFor GCN-ODE specifically, all eigenvalues are non-positive. The larger $\\vert \\lambda_i \\vert$, the faster $X(t)$ decays, and the system reaches equilibrium. Reducing $\\vert \\lambda \\vert$ (or limit the Lipschitz constant of $A$) is a possible solution, which can potentially be achieved by:\n\n(1)  Clip weights within a fixed range to control the Lipschitz constant as in Wasserstein GAN [1]\n(2)  Estimate the spectral norm of $A$ and limit it in the loss function. In practice, the bound on spectral norm can be estimated with power iterations. [2]\n\n[1] Arjovsky, Martin, Soumith Chintala, and L\u00e9on Bottou. \"Wasserstein gan.\" arXiv preprint arXiv:1701.07875 (2017).\n[2] Miyato, Takeru, et al. \"Spectral normalization for generative adversarial networks.\" arXiv preprint arXiv:1802.05957 (2018)."}, "signatures": ["ICLR.cc/2020/Conference/Paper422/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper422/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["j.zhuang@yale.edu", "nicha.dvornek@yale.edu", "xiaoxiao.li@yale.edu", "james.duncan@yale.edu"], "title": "Ordinary differential equations on graph networks", "authors": ["Juntang Zhuang", "Nicha Dvornek", "Xiaoxiao Li", "James S. Duncan"], "pdf": "/pdf/5c8a94186856d657aa0a2d8b80c095bf6ddcf282.pdf", "TL;DR": "Apply ordinary differential equation model on graph structured data", "abstract": "Recently various neural networks have been proposed for irregularly structured data such as graphs and manifolds. To our knowledge, all existing graph networks have discrete depth. Inspired by neural ordinary differential equation (NODE) for data in the Euclidean domain, we extend the idea of continuous-depth models to graph data, and propose graph ordinary differential equation (GODE). The derivative of hidden node states are parameterized with a graph neural network, and the output states are the solution to this ordinary differential equation. We demonstrate two end-to-end methods for efficient training of GODE: (1) indirect back-propagation with the adjoint method; (2) direct back-propagation through the ODE solver, which accurately computes the gradient. We demonstrate that direct backprop outperforms the adjoint method in experiments. We then introduce a family of bijective blocks, which enables $\\mathcal{O}(1)$ memory consumption. We demonstrate that GODE can be easily adapted to different existing graph neural networks and improve accuracy. We validate the performance of GODE in both semi-supervised node classification tasks and graph classification tasks. Our GODE model achieves a continuous model in time, memory efficiency, accurate gradient estimation, and generalizability with different graph networks.", "keywords": ["Graph Networks", "Ordinary differential equation"], "paperhash": "zhuang|ordinary_differential_equations_on_graph_networks", "code": "https://www.dropbox.com/sh/sgaid3efh4eqmjl/AAB-DFXvNq_Pf313UqSLl4VPa?dl=0", "original_pdf": "/attachment/d2b601834ddcc9611d4dca58f62fabb6213cdee8.pdf", "_bibtex": "@misc{\nzhuang2020ordinary,\ntitle={Ordinary differential equations on graph networks},\nauthor={Juntang Zhuang and Nicha Dvornek and Xiaoxiao Li and James S. Duncan},\nyear={2020},\nurl={https://openreview.net/forum?id=SJg9z6VFDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJg9z6VFDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper422/Authors", "ICLR.cc/2020/Conference/Paper422/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper422/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper422/Reviewers", "ICLR.cc/2020/Conference/Paper422/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper422/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper422/Authors|ICLR.cc/2020/Conference/Paper422/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504171711, "tmdate": 1576860540355, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper422/Authors", "ICLR.cc/2020/Conference/Paper422/Reviewers", "ICLR.cc/2020/Conference/Paper422/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper422/-/Official_Comment"}}}, {"id": "S1lyjmc4sS", "original": null, "number": 6, "cdate": 1573327766621, "ddate": null, "tcdate": 1573327766621, "tmdate": 1573665879587, "tddate": null, "forum": "SJg9z6VFDr", "replyto": "ryxO66ok5r", "invitation": "ICLR.cc/2020/Conference/Paper422/-/Official_Comment", "content": {"title": "Response to Reviewer #2", "comment": "We sincerely thank you for review and totally agree with your comments on ODE solver for free-form continuous functions. However, we want to point out that our method is a free-form solver for continuous models, and extensive experiments are conducted in appendix C (moved to Sec 5.3 in revision). The reviewer might overlooked this part because we did not put enough emphasis in the main paper. We will address your concerns in the following.\n\n1.         about the ODE stability issue\nWe put a detailed description on this issue in thread A https://openreview.net/forum?id=SJg9z6VFDr&noteId=B1eeWWA7sr \nWe will briefly address your concern here.\n\n(a) Our method can handle free-form functions, which is theoretically discussed in thread A.1. For experiments in thread A.3, we directly modified a ResNet18 into an ODE, where $f$ is a stack of conv-bn-relu-conv-bn layers WITHOUT the invertible structure.\n\nIn fact, invertible block is not the key to our ODE solver; it\u2019s only the key to low memory consumption. This is discussed in thread B, section 1 to 3 https://openreview.net/forum?id=SJg9z6VFDr&noteId=SkgzfxCmoH \nMemory consumption for naive direct backprop is $O(N_f\\times N_t\\times K)$, our method for free-form can be reduced to $O(N_f + N_t)$, for invertible blocks is $O(N_f)$\n\nWe apologize for the confusion; we put too little description of the ability to handle free-form, and wrote too much about invertible block with the thinking that most readers might not be familiar. We will revise this in the paper.\n\n(b) Our method is dealing with a continuous model.\nThis is theoretically discussed in thread A.2 and supported with experiments in A.3.(d).\n\nWe agree with reviewers that uncareful discretization will convert a continuous model into a discrete model. However, we clarify that this is not the case with our method. \n\nA continuous function is robust to different ODE solvers, while discrete models are sensitive to solvers, because different ODE solvers are equivalent to different depths.\n \nA test for continuous model is: at inference time, switch to different ODE solvers of different orders; a continuous model outputs very robust results.\n\nOur method passed this test, as shown in thread A.3.(d). You can check this with our code and pre-trained weights https://www.dropbox.com/sh/sgaid3efh4eqmjl/AAB-DFXvNq_Pf313UqSLl4VPa?dl=0 \n\n\n2.   about the invertible building block\n    We admit that we did not extensively experiment with different forms of $\\psi$; we will revise our paper and not claim this as our main contribution. However, we want to clarify that we define a space to search for invertible blocks, which is important for the field of \u201cnormalizing flow\u201d. There are many works on normalizing flow which uses different $\\psi$. For example, $\\psi(\\alpha,\\beta) = \\alpha \\cdot sigmoid(\\beta)$ in [2], $\\psi(\\alpha,\\beta) = \\alpha + \\beta$ in [3], $\\psi(\\alpha,\\beta) = \\alpha \\cdot exp(\\beta)$ in [4], $\\psi(\\alpha,\\beta) = \\alpha \\cdot \\beta$ in [5]. To our knowledge, we are the first to define the searching space of $\\psi$. \n\n\n3.    empirical performance\n     We validate our method both in CNN-ODE (thread A.3) and GNN-ODE (thread C). The CNN-ODE is a *free-form* function; for GNN-ODE, we tested both free-form and invertible block (restricted form).\n    \n     For experiments on CIFAR with CNN-ODE, our method reduces error rate from 19% to 5% compared with training ODE using the solver in [1]. Furthermore, our model is robust to different ODE solvers during inference. Finally, our model has the same number of parameters as a ResNet18, but outperforms standard ResNet50 and ResNet101.\n\n   For experiments with GNN, we re-run experiments and performed paired t-tests. Results are summarized in Thread C ( https://openreview.net/forum?id=SJg9z6VFDr&noteId=HklsTJ07jB  ). For most cases, our model outperforms the discrete baseline at a 1% significance level. \n\n\n[1] Chen, Tian Qi, et al. \"Neural ordinary differential equations.\" Advances in neural information processing systems. 2018.\n[2] Huang, Chin-Wei, et al. \"Neural autoregressive flows.\" arXiv preprint arXiv:1804.00779 (2018).\n[3] Dinh, Laurent, David Krueger, and Yoshua Bengio. \"Nice: Non-linear independent components estimation.\" arXiv preprint arXiv:1410.8516 (2014).\n[4] Dinh, Laurent, Jascha Sohl-Dickstein, and Samy Bengio. \"Density estimation using real nvp.\" arXiv preprint arXiv:1605.08803 (2016).\n[5] Liao, Huadong, Jiawei He, and Kunxian Shu. \"Generative Model with Dynamic Linear Flow.\" arXiv preprint arXiv:1905.03239 (2019)."}, "signatures": ["ICLR.cc/2020/Conference/Paper422/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper422/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["j.zhuang@yale.edu", "nicha.dvornek@yale.edu", "xiaoxiao.li@yale.edu", "james.duncan@yale.edu"], "title": "Ordinary differential equations on graph networks", "authors": ["Juntang Zhuang", "Nicha Dvornek", "Xiaoxiao Li", "James S. Duncan"], "pdf": "/pdf/5c8a94186856d657aa0a2d8b80c095bf6ddcf282.pdf", "TL;DR": "Apply ordinary differential equation model on graph structured data", "abstract": "Recently various neural networks have been proposed for irregularly structured data such as graphs and manifolds. To our knowledge, all existing graph networks have discrete depth. Inspired by neural ordinary differential equation (NODE) for data in the Euclidean domain, we extend the idea of continuous-depth models to graph data, and propose graph ordinary differential equation (GODE). The derivative of hidden node states are parameterized with a graph neural network, and the output states are the solution to this ordinary differential equation. We demonstrate two end-to-end methods for efficient training of GODE: (1) indirect back-propagation with the adjoint method; (2) direct back-propagation through the ODE solver, which accurately computes the gradient. We demonstrate that direct backprop outperforms the adjoint method in experiments. We then introduce a family of bijective blocks, which enables $\\mathcal{O}(1)$ memory consumption. We demonstrate that GODE can be easily adapted to different existing graph neural networks and improve accuracy. We validate the performance of GODE in both semi-supervised node classification tasks and graph classification tasks. Our GODE model achieves a continuous model in time, memory efficiency, accurate gradient estimation, and generalizability with different graph networks.", "keywords": ["Graph Networks", "Ordinary differential equation"], "paperhash": "zhuang|ordinary_differential_equations_on_graph_networks", "code": "https://www.dropbox.com/sh/sgaid3efh4eqmjl/AAB-DFXvNq_Pf313UqSLl4VPa?dl=0", "original_pdf": "/attachment/d2b601834ddcc9611d4dca58f62fabb6213cdee8.pdf", "_bibtex": "@misc{\nzhuang2020ordinary,\ntitle={Ordinary differential equations on graph networks},\nauthor={Juntang Zhuang and Nicha Dvornek and Xiaoxiao Li and James S. Duncan},\nyear={2020},\nurl={https://openreview.net/forum?id=SJg9z6VFDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJg9z6VFDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper422/Authors", "ICLR.cc/2020/Conference/Paper422/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper422/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper422/Reviewers", "ICLR.cc/2020/Conference/Paper422/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper422/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper422/Authors|ICLR.cc/2020/Conference/Paper422/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504171711, "tmdate": 1576860540355, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper422/Authors", "ICLR.cc/2020/Conference/Paper422/Reviewers", "ICLR.cc/2020/Conference/Paper422/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper422/-/Official_Comment"}}}, {"id": "SkgzfxCmoH", "original": null, "number": 3, "cdate": 1573277706186, "ddate": null, "tcdate": 1573277706186, "tmdate": 1573665795180, "tddate": null, "forum": "SJg9z6VFDr", "replyto": "SJg9z6VFDr", "invitation": "ICLR.cc/2020/Conference/Paper422/-/Official_Comment", "content": {"title": "B:  Details of algorithm for free-form ODE with accurate gradient estimation", "comment": "In the original submission in Appendix B (moved to Sec 3.3 in revision) we detailed the algorithm for using invertible blocks, the most memory efficient form of our solver. In the revision we will include the following algorithm for free-form models.\n\n+ \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014+\nDefine model $\\frac{\\mathrm{d} z(t)}{ \\mathrm{d} t} = f(z(t), t)$, where $f$ is a free-form function. Denote integration time as $T$.\nForward ($f, T, z_0, tolerance$): \n         $t=0, z = z_0$ \n         $state_0 = f.state\\_dict()$, cache.save($state_0$) \n         Select initial step size $h=h_0$ (adaptively with adaptive step-size solver).\n         $time\\_points = empty\\_list()$\n         While $t<T$: \n                $state = f.state\\_dict()$,   $accept\\_step = False$ \n                While Not  $accept\\_step$: \n                            $f.load\\_state\\_dict(state)$ \n                            with $grad\\_disabled$:\n                                     $z\\_new, error\\_estimate = step(f, z, t, h)$ \n                            If $error\\_estimate < tolerance$: \n                                    $accept\\_step = True$ \n                                    $z = z\\_new,   t = t + h,   time\\_points.append(t)$ \n                            else: \n                                     reduce stepsize $h$ according to $error\\_estimate$\n                                     delete $z\\_new, error\\_estimate$ and related local computation graph \n        cache.save($time\\_points$) \n        return $z$, cache \n+\u2014\u2014\u2014\u2014--\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014-+\n Backward ($f, T, z_0, tolerance, cache$): \n         $\\{t_0, t_1, t_2, ...t_{N-1}, t_N\\} = cache.time\\_points$ \n         For $t_i$ in $\\{t_0, t_1, t_2, ...t_{N-1}, t_N\\}:$ \n                 $z(t_{i+1}) = step(f, z(t_i), t_i, h=t_{i+1} - t_i)$ \n         For $t_i$ in $\\{ t_N, t_{N-1}, ..., t_1, t_0 \\}:$ \n                  Determine $a(t_i)=\\frac{\\partial L}{\\partial z(t_i)}$ and $\\frac{\\partial z(t_i)}{\\partial \\theta}$ \n         $\\frac{\\mathrm{d} L}{\\mathrm{d} \\theta} = \\sum_{i=1}^{N} a_i \\frac{\\partial z(t_i)}{ \\partial \\theta}$ \n        return $\\frac{\\mathrm{d} L}{\\mathrm{d} \\theta}$\n+------------\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014-+\n\n\nMemory consumption analysis:\n1. Suppose $f$ has $N_f$ layers, the number of forward evaluation step is $N_t$ on average, and the evaluations to adaptively search for an optimal stepsize is $K$. A naive solver will take $O(N_f \\times N_t \\times K)$, while our method consumes $O(N_f \\times N_t)$ because all middle activations are deleted during forward pass, and we don\u2019t need to search for optimal stepsize in backward pass. \n\n2. In fact, if we perform step-wise checkpoint method, where we only store $z(t_i)$ for all $t_i$, and compute the gradient $\\frac{\\partial z(t_{i+1})}{\\partial z(t_i)}$ for one $t_i$ at a time, then the memory consumption can be reduced to $O(N_f + N_t)$.\n\n3. Since the solver can handle free-form functions, it can also handle restricted form invertible block. In this case, we don\u2019t need to store $z(t_i)$, the memory consumption can reduce to $O(N_f)$.  In fact we use invertible blocks only for memory consideration, it does not imply invertible blocks are necessary.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper422/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper422/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["j.zhuang@yale.edu", "nicha.dvornek@yale.edu", "xiaoxiao.li@yale.edu", "james.duncan@yale.edu"], "title": "Ordinary differential equations on graph networks", "authors": ["Juntang Zhuang", "Nicha Dvornek", "Xiaoxiao Li", "James S. Duncan"], "pdf": "/pdf/5c8a94186856d657aa0a2d8b80c095bf6ddcf282.pdf", "TL;DR": "Apply ordinary differential equation model on graph structured data", "abstract": "Recently various neural networks have been proposed for irregularly structured data such as graphs and manifolds. To our knowledge, all existing graph networks have discrete depth. Inspired by neural ordinary differential equation (NODE) for data in the Euclidean domain, we extend the idea of continuous-depth models to graph data, and propose graph ordinary differential equation (GODE). The derivative of hidden node states are parameterized with a graph neural network, and the output states are the solution to this ordinary differential equation. We demonstrate two end-to-end methods for efficient training of GODE: (1) indirect back-propagation with the adjoint method; (2) direct back-propagation through the ODE solver, which accurately computes the gradient. We demonstrate that direct backprop outperforms the adjoint method in experiments. We then introduce a family of bijective blocks, which enables $\\mathcal{O}(1)$ memory consumption. We demonstrate that GODE can be easily adapted to different existing graph neural networks and improve accuracy. We validate the performance of GODE in both semi-supervised node classification tasks and graph classification tasks. Our GODE model achieves a continuous model in time, memory efficiency, accurate gradient estimation, and generalizability with different graph networks.", "keywords": ["Graph Networks", "Ordinary differential equation"], "paperhash": "zhuang|ordinary_differential_equations_on_graph_networks", "code": "https://www.dropbox.com/sh/sgaid3efh4eqmjl/AAB-DFXvNq_Pf313UqSLl4VPa?dl=0", "original_pdf": "/attachment/d2b601834ddcc9611d4dca58f62fabb6213cdee8.pdf", "_bibtex": "@misc{\nzhuang2020ordinary,\ntitle={Ordinary differential equations on graph networks},\nauthor={Juntang Zhuang and Nicha Dvornek and Xiaoxiao Li and James S. Duncan},\nyear={2020},\nurl={https://openreview.net/forum?id=SJg9z6VFDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJg9z6VFDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper422/Authors", "ICLR.cc/2020/Conference/Paper422/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper422/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper422/Reviewers", "ICLR.cc/2020/Conference/Paper422/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper422/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper422/Authors|ICLR.cc/2020/Conference/Paper422/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504171711, "tmdate": 1576860540355, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper422/Authors", "ICLR.cc/2020/Conference/Paper422/Reviewers", "ICLR.cc/2020/Conference/Paper422/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper422/-/Official_Comment"}}}, {"id": "B1eeWWA7sr", "original": null, "number": 4, "cdate": 1573277944397, "ddate": null, "tcdate": 1573277944397, "tmdate": 1573665736779, "tddate": null, "forum": "SJg9z6VFDr", "replyto": "SJg9z6VFDr", "invitation": "ICLR.cc/2020/Conference/Paper422/-/Official_Comment", "content": {"title": "A:  Accurate gradient estimation in ODE solver for free-form functions, with application to general NODEs", "comment": "We sincerely thank all reviewers for their insightful comments, and agree on the importance of free-form ODE solver for continuous models, and the application to general ODEs, as mentioned by all reviewers. We want to point out this is exactly covered in appendix C in the original submission (Sec 5.3 in revision), but reviewers might overlook this part because we did not put sufficient emphasis in the main paper. Here we clarify our contributions. We will also revise our paper for better understanding.\n\n1. An ODE solver for free-form functions. Since it can handle free-form functions, it can handle restricted forms. In fact we use invertible blocks only for memory consideration, it does not imply invertible blocks are necessary.\nThe details of our free-form ODE solver is in thread B(https://openreview.net/forum?id=SJg9z6VFDr&noteId=SkgzfxCmoH ), here we briefly summarize it: \n-- During forward pass, the solver performs a numerical integration, with the stepsize adaptively varying with error estimation. \n-- During forward pass, the solver outputs integrated value, and the *evaluation time points* {$t_0, t_1, \u2026 t_N$}. All middle activations are deleted to save memory.\n-- During backward pass, the solver re-builds the computation graph, by *directly* evaluating at saved time points, without adaptive searching.\n-- During backward pass, the solver is performing a numerical reverse-time integration $\\frac{\\mathrm{d} L}{\\mathrm{d} \\theta} = - \\int_T^0 a(t)^T \\frac{\\partial f(z(t), t, \\theta)}{\\partial \\theta} \\mathrm{d}t$. \n-- Here we emphasize $z(t_i)$ is accurate, because the backward pass uses the same ${t_i}$ as in the forward pass. Therefore, the adjoint $a(t_i)$ and gradient can be accurately determined.\n-- Our method does not require $f$ to be invertible, it can be free-form.\n\n2. A solver truly for continuous models\n    We strongly agree with reviewers that uncareful discretization will turn a continuous model into a discrete model. However, we clarify our model is a truly continuous model:\n----(a) Numerical discretization in ODE solvers does NOT turn a continuous model into a discrete model. To our knowledge, all existing numerical ODE solvers have a discretization step. They are widely used in physical and mathematical problems such as fluid dynamics, but researchers are still dealing with continuous problems. \n\n----(b) Continuous models are robust to solvers. \nA naive fixed stepsize discretization will turn a continuous model into a discrete model. For example, a 1-step discretization is equivalent to a residual block; a 2-step discretization is equivalent to reusing every layer twice. During inference, any change to the discretization is equivalent to changing model depth; this will cause severe error for discrete models.\n\nHowever, for a continuous model, during inference, different solvers generate very close results as long as the error tolerance is small. Hence, continuous models remain robust to solvers of different orders.  (we call this \"robustness test\")\n\nWe demonstrate our method passes robustness test, shown below in A.3\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper422/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper422/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["j.zhuang@yale.edu", "nicha.dvornek@yale.edu", "xiaoxiao.li@yale.edu", "james.duncan@yale.edu"], "title": "Ordinary differential equations on graph networks", "authors": ["Juntang Zhuang", "Nicha Dvornek", "Xiaoxiao Li", "James S. Duncan"], "pdf": "/pdf/5c8a94186856d657aa0a2d8b80c095bf6ddcf282.pdf", "TL;DR": "Apply ordinary differential equation model on graph structured data", "abstract": "Recently various neural networks have been proposed for irregularly structured data such as graphs and manifolds. To our knowledge, all existing graph networks have discrete depth. Inspired by neural ordinary differential equation (NODE) for data in the Euclidean domain, we extend the idea of continuous-depth models to graph data, and propose graph ordinary differential equation (GODE). The derivative of hidden node states are parameterized with a graph neural network, and the output states are the solution to this ordinary differential equation. We demonstrate two end-to-end methods for efficient training of GODE: (1) indirect back-propagation with the adjoint method; (2) direct back-propagation through the ODE solver, which accurately computes the gradient. We demonstrate that direct backprop outperforms the adjoint method in experiments. We then introduce a family of bijective blocks, which enables $\\mathcal{O}(1)$ memory consumption. We demonstrate that GODE can be easily adapted to different existing graph neural networks and improve accuracy. We validate the performance of GODE in both semi-supervised node classification tasks and graph classification tasks. Our GODE model achieves a continuous model in time, memory efficiency, accurate gradient estimation, and generalizability with different graph networks.", "keywords": ["Graph Networks", "Ordinary differential equation"], "paperhash": "zhuang|ordinary_differential_equations_on_graph_networks", "code": "https://www.dropbox.com/sh/sgaid3efh4eqmjl/AAB-DFXvNq_Pf313UqSLl4VPa?dl=0", "original_pdf": "/attachment/d2b601834ddcc9611d4dca58f62fabb6213cdee8.pdf", "_bibtex": "@misc{\nzhuang2020ordinary,\ntitle={Ordinary differential equations on graph networks},\nauthor={Juntang Zhuang and Nicha Dvornek and Xiaoxiao Li and James S. Duncan},\nyear={2020},\nurl={https://openreview.net/forum?id=SJg9z6VFDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJg9z6VFDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper422/Authors", "ICLR.cc/2020/Conference/Paper422/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper422/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper422/Reviewers", "ICLR.cc/2020/Conference/Paper422/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper422/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper422/Authors|ICLR.cc/2020/Conference/Paper422/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504171711, "tmdate": 1576860540355, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper422/Authors", "ICLR.cc/2020/Conference/Paper422/Reviewers", "ICLR.cc/2020/Conference/Paper422/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper422/-/Official_Comment"}}}, {"id": "BkxLkK2Yor", "original": null, "number": 11, "cdate": 1573664989519, "ddate": null, "tcdate": 1573664989519, "tmdate": 1573665244950, "tddate": null, "forum": "SJg9z6VFDr", "replyto": "B1esk0ZHjS", "invitation": "ICLR.cc/2020/Conference/Paper422/-/Official_Comment", "content": {"title": "Oversmoothing of graph ODEs.", "comment": "Thank you for discussing the relationship between graph ODEs and the oversmoothing phenomena and sharing the additional experiment results.\n\nIt is interesting that the performance consistently drops as the integration time increases. Although the model authors used do not have non-linear activation functions, as opposed to usual GCNs, it seems that this is supporting evidence that oversmoothing happens to graph ODEs, too. Considering that the resulting model is a diffusion process on graphs, I guess we can interpret that the process converges to an equilibrium.\n\nTaking this experiments into account, it would be great if there is a suggestion on how to overcome the oversmoothing phenomena through the lens of ODEs."}, "signatures": ["ICLR.cc/2020/Conference/Paper422/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper422/AnonReviewer1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["j.zhuang@yale.edu", "nicha.dvornek@yale.edu", "xiaoxiao.li@yale.edu", "james.duncan@yale.edu"], "title": "Ordinary differential equations on graph networks", "authors": ["Juntang Zhuang", "Nicha Dvornek", "Xiaoxiao Li", "James S. Duncan"], "pdf": "/pdf/5c8a94186856d657aa0a2d8b80c095bf6ddcf282.pdf", "TL;DR": "Apply ordinary differential equation model on graph structured data", "abstract": "Recently various neural networks have been proposed for irregularly structured data such as graphs and manifolds. To our knowledge, all existing graph networks have discrete depth. Inspired by neural ordinary differential equation (NODE) for data in the Euclidean domain, we extend the idea of continuous-depth models to graph data, and propose graph ordinary differential equation (GODE). The derivative of hidden node states are parameterized with a graph neural network, and the output states are the solution to this ordinary differential equation. We demonstrate two end-to-end methods for efficient training of GODE: (1) indirect back-propagation with the adjoint method; (2) direct back-propagation through the ODE solver, which accurately computes the gradient. We demonstrate that direct backprop outperforms the adjoint method in experiments. We then introduce a family of bijective blocks, which enables $\\mathcal{O}(1)$ memory consumption. We demonstrate that GODE can be easily adapted to different existing graph neural networks and improve accuracy. We validate the performance of GODE in both semi-supervised node classification tasks and graph classification tasks. Our GODE model achieves a continuous model in time, memory efficiency, accurate gradient estimation, and generalizability with different graph networks.", "keywords": ["Graph Networks", "Ordinary differential equation"], "paperhash": "zhuang|ordinary_differential_equations_on_graph_networks", "code": "https://www.dropbox.com/sh/sgaid3efh4eqmjl/AAB-DFXvNq_Pf313UqSLl4VPa?dl=0", "original_pdf": "/attachment/d2b601834ddcc9611d4dca58f62fabb6213cdee8.pdf", "_bibtex": "@misc{\nzhuang2020ordinary,\ntitle={Ordinary differential equations on graph networks},\nauthor={Juntang Zhuang and Nicha Dvornek and Xiaoxiao Li and James S. Duncan},\nyear={2020},\nurl={https://openreview.net/forum?id=SJg9z6VFDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJg9z6VFDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper422/Authors", "ICLR.cc/2020/Conference/Paper422/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper422/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper422/Reviewers", "ICLR.cc/2020/Conference/Paper422/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper422/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper422/Authors|ICLR.cc/2020/Conference/Paper422/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504171711, "tmdate": 1576860540355, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper422/Authors", "ICLR.cc/2020/Conference/Paper422/Reviewers", "ICLR.cc/2020/Conference/Paper422/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper422/-/Official_Comment"}}}, {"id": "S1ePzPs_jS", "original": null, "number": 9, "cdate": 1573594894812, "ddate": null, "tcdate": 1573594894812, "tmdate": 1573620656203, "tddate": null, "forum": "SJg9z6VFDr", "replyto": "S1lyjmc4sS", "invitation": "ICLR.cc/2020/Conference/Paper422/-/Official_Comment", "content": {"title": "For the invertible building block version, what is the benefit of modeling it as a solution of ODE?", "comment": "Now my understanding is the proposed method has two versions:\n(1) the free-form continuous version.\n(2) restricted form, and compute the gradient exactly.\n\nFor version (1), I do not have many questions. It is the same as the Neural ODE approach, but applied to Graph data. I think if the experimental results are very good, then this new application can be counted as a contribution.\n\nVersion (2) constitutes the major contents in this paper, but I couldn't see how we benefit from viewing it as an ODE model.\n\nTo be more clear, we need to notice that we are solving *a machine learning problem*  (e.g. node classification or graph classification) *not solving an ODE*. Why do we need to use a small step size in this case? Why do we need to make the output as an approximate solution of ODE? Why is it a good inductive bias? I will think using a larger step size and not approximating the solution of ODE (just like what other discrete invertible models do) may be even better.\n\nI think the advantages/disadvantages of version (2) are the same as other discrete models: memory-efficient but restricted form. I personally think version (2) is more like a discrete model because I don't see the additional benefit of viewing it as an ODE in this machine learning problem.\n\nCould the authors explain a bit about:\n\nFor version (2), what's the benefit of modeling it as a solution of ODE? Wouldn't it be better to choose a larger step size just like in other discrete models?\n\nFor version (1), the benefit is clear to me: free-form."}, "signatures": ["ICLR.cc/2020/Conference/Paper422/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper422/AnonReviewer2", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["j.zhuang@yale.edu", "nicha.dvornek@yale.edu", "xiaoxiao.li@yale.edu", "james.duncan@yale.edu"], "title": "Ordinary differential equations on graph networks", "authors": ["Juntang Zhuang", "Nicha Dvornek", "Xiaoxiao Li", "James S. Duncan"], "pdf": "/pdf/5c8a94186856d657aa0a2d8b80c095bf6ddcf282.pdf", "TL;DR": "Apply ordinary differential equation model on graph structured data", "abstract": "Recently various neural networks have been proposed for irregularly structured data such as graphs and manifolds. To our knowledge, all existing graph networks have discrete depth. Inspired by neural ordinary differential equation (NODE) for data in the Euclidean domain, we extend the idea of continuous-depth models to graph data, and propose graph ordinary differential equation (GODE). The derivative of hidden node states are parameterized with a graph neural network, and the output states are the solution to this ordinary differential equation. We demonstrate two end-to-end methods for efficient training of GODE: (1) indirect back-propagation with the adjoint method; (2) direct back-propagation through the ODE solver, which accurately computes the gradient. We demonstrate that direct backprop outperforms the adjoint method in experiments. We then introduce a family of bijective blocks, which enables $\\mathcal{O}(1)$ memory consumption. We demonstrate that GODE can be easily adapted to different existing graph neural networks and improve accuracy. We validate the performance of GODE in both semi-supervised node classification tasks and graph classification tasks. Our GODE model achieves a continuous model in time, memory efficiency, accurate gradient estimation, and generalizability with different graph networks.", "keywords": ["Graph Networks", "Ordinary differential equation"], "paperhash": "zhuang|ordinary_differential_equations_on_graph_networks", "code": "https://www.dropbox.com/sh/sgaid3efh4eqmjl/AAB-DFXvNq_Pf313UqSLl4VPa?dl=0", "original_pdf": "/attachment/d2b601834ddcc9611d4dca58f62fabb6213cdee8.pdf", "_bibtex": "@misc{\nzhuang2020ordinary,\ntitle={Ordinary differential equations on graph networks},\nauthor={Juntang Zhuang and Nicha Dvornek and Xiaoxiao Li and James S. Duncan},\nyear={2020},\nurl={https://openreview.net/forum?id=SJg9z6VFDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJg9z6VFDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper422/Authors", "ICLR.cc/2020/Conference/Paper422/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper422/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper422/Reviewers", "ICLR.cc/2020/Conference/Paper422/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper422/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper422/Authors|ICLR.cc/2020/Conference/Paper422/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504171711, "tmdate": 1576860540355, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper422/Authors", "ICLR.cc/2020/Conference/Paper422/Reviewers", "ICLR.cc/2020/Conference/Paper422/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper422/-/Official_Comment"}}}, {"id": "HklsTJ07jB", "original": null, "number": 2, "cdate": 1573277635481, "ddate": null, "tcdate": 1573277635481, "tmdate": 1573619966039, "tddate": null, "forum": "SJg9z6VFDr", "replyto": "SJg9z6VFDr", "invitation": "ICLR.cc/2020/Conference/Paper422/-/Official_Comment", "content": {"title": "C:   Empirical performance of graph-ode", "comment": "We thank all reviewers for their comments on the empirical performance. We re-run the experiments on graph networks, with both invertible blocks and free-form functions (as suggested by R2). We performed a paired t-test. We use * (**) to mark GODE models that outperform corresponding discrete-layer baselines at a 5% (1%) significance level.  \n\nAccuracy on a 10-fold cross validation is reported below. We will also update results in the main paper. Discrete layer models are marked with \u201cDISC\u201d, GODE with invertible blocks are marked with \u201crestrict\u201d, and with free-form functions are marked with \u201cfree\u201d. For various baselines, most of the GODE models significantly outperform the baseline.\n                         Accuracy(%) in graph-classification tasks\n+------------------------------+----------------+------------------+-----------------+---------------+\n| Model                           | MUTAG       | PROTEINS    | IMDB          | REDDIT      |\n+---------+-------------------+-----------------+------------------+----------------+----------------+\n| GCN  | DISC                 | 73.3+-5.2    | 72.4+-3.1       | 74.2+-3.6   | 85.9+-1.8   |\n|           +--------+----------+-----------------+------------------+----------------+---------------+\n|           | ODE | restrict |74.7+-4.3** | 74.7+-4.3** | 75.3+-5.3*  | 89.2+-3.2** |\n|           |           +----------+-----------------+------------------+----------------+----------------+\n|           |           | free     | 75.1+-5.3** | 76.6+-3.9** | 73.9+-4.6   | 88.5+-3.0** |\n+---------+--------+----------+-----------------+------------------+----------------+-------------+\n| Cheb | DISC                 | 84.0+-6.4    | 70.6+-3.9       | 71.9+-3.8   | 91.0+-1.5   |\n|            +--------+----------+----------------+-------------------+----------------+-------------+\n|            | ODE | restrict | 85.0+-8.3   | 72.7+-3.7** | 72.0+-5.0*  | 91.2+-1.5   |\n|            |          +----------+-----------------+-----------------+-----------------+-------------+\n|            |          | free     | 86.1+-6.3*  | 72.5+-4.7** | 73.6+-4.0** | 92.4+-1.6** |\n+---------+--------+----------+-----------------+-----------------+-----------------+---------------+\n| GIN    | DISC                | 85.0+-6.4    | 73.0+-3.1      | 73.3+-5.1     | 89.2+-2.5   |\n|            +--------+----------+-----------------+-----------------+-----------------+-------------+\n|            | ODE | restrict | 87.7+-5.2*  | 74.5+-3.5*  | 75.4+-3.9** | 90.5+-1.5** |\n|            |           +----------+-----------------+----------------+------------------+-------------+\n|            |           | free     | 87.8+-5.1** | 73.1+-3.7   | 74.8+-4.1** | 91.0+-2.4** |\n+----------+-------+----------+------------------+----------------+-----------------+-------------+"}, "signatures": ["ICLR.cc/2020/Conference/Paper422/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper422/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["j.zhuang@yale.edu", "nicha.dvornek@yale.edu", "xiaoxiao.li@yale.edu", "james.duncan@yale.edu"], "title": "Ordinary differential equations on graph networks", "authors": ["Juntang Zhuang", "Nicha Dvornek", "Xiaoxiao Li", "James S. Duncan"], "pdf": "/pdf/5c8a94186856d657aa0a2d8b80c095bf6ddcf282.pdf", "TL;DR": "Apply ordinary differential equation model on graph structured data", "abstract": "Recently various neural networks have been proposed for irregularly structured data such as graphs and manifolds. To our knowledge, all existing graph networks have discrete depth. Inspired by neural ordinary differential equation (NODE) for data in the Euclidean domain, we extend the idea of continuous-depth models to graph data, and propose graph ordinary differential equation (GODE). The derivative of hidden node states are parameterized with a graph neural network, and the output states are the solution to this ordinary differential equation. We demonstrate two end-to-end methods for efficient training of GODE: (1) indirect back-propagation with the adjoint method; (2) direct back-propagation through the ODE solver, which accurately computes the gradient. We demonstrate that direct backprop outperforms the adjoint method in experiments. We then introduce a family of bijective blocks, which enables $\\mathcal{O}(1)$ memory consumption. We demonstrate that GODE can be easily adapted to different existing graph neural networks and improve accuracy. We validate the performance of GODE in both semi-supervised node classification tasks and graph classification tasks. Our GODE model achieves a continuous model in time, memory efficiency, accurate gradient estimation, and generalizability with different graph networks.", "keywords": ["Graph Networks", "Ordinary differential equation"], "paperhash": "zhuang|ordinary_differential_equations_on_graph_networks", "code": "https://www.dropbox.com/sh/sgaid3efh4eqmjl/AAB-DFXvNq_Pf313UqSLl4VPa?dl=0", "original_pdf": "/attachment/d2b601834ddcc9611d4dca58f62fabb6213cdee8.pdf", "_bibtex": "@misc{\nzhuang2020ordinary,\ntitle={Ordinary differential equations on graph networks},\nauthor={Juntang Zhuang and Nicha Dvornek and Xiaoxiao Li and James S. Duncan},\nyear={2020},\nurl={https://openreview.net/forum?id=SJg9z6VFDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJg9z6VFDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper422/Authors", "ICLR.cc/2020/Conference/Paper422/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper422/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper422/Reviewers", "ICLR.cc/2020/Conference/Paper422/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper422/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper422/Authors|ICLR.cc/2020/Conference/Paper422/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504171711, "tmdate": 1576860540355, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper422/Authors", "ICLR.cc/2020/Conference/Paper422/Reviewers", "ICLR.cc/2020/Conference/Paper422/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper422/-/Official_Comment"}}}, {"id": "r1lmobR7sH", "original": null, "number": 5, "cdate": 1573278107206, "ddate": null, "tcdate": 1573278107206, "tmdate": 1573485834235, "tddate": null, "forum": "SJg9z6VFDr", "replyto": "B1eeWWA7sr", "invitation": "ICLR.cc/2020/Conference/Paper422/-/Official_Comment", "content": {"title": "Empirical performance of free-form CNN-ODE on image classification", "comment": "                                                  \n+---------------+-----------------------------------------------------------------+-------------------------------------------------+\n|                    |                             ours                                                | Literature                                             |\n|                    +------------------------------------+----------------------------+-------------+--------+-----------+-----------+\n|                    | Adaptive Solvers                | Fixed step solvers   | adjoint   |Res18  |Res50   | Res101 |\n|                    +--------------+---------+----------+--------+--------+--------+                |             |              |              |\n|                    |HeunEuler| RK23  | RK45 | Euler | RK2   | RK4   |                |             |             |               |\n+----------------+-------------+---------+---------+---------+--------+---------+------------+----------+----------+-----------+\n| CIFAR10    | 4.85         | 4.92    | 5.29   | 5.52   | 5.27    | 5.24   | 19.2       | 6.98     | 6.38     | 6.25      |\n+---------------+-------------+---------+---------+---------+---------+---------+------------+--------- -+----------+-----------+\n| CIFAR100  | 22.66      | 24.13 | 23.56 | 24.44  | 24.44 | 24.43 | 37.6        | 27.08   | 25.73   | 24.84    |\n+---------------+-------------+---------+---------+---------+---------+---------+------------+----------+-----------+-----------+\n                                                          Error Rate % on CIFAR10 and CIFAR100 test set\n3.  Applications to a convolutional neural-ODE (code and pretrained weights: https://www.dropbox.com/sh/sgaid3efh4eqmjl/AAB-DFXvNq_Pf313UqSLl4VPa?dl=0 )\n     We explain the contents in the appendix of the original submission, with results shown in the table above. We will move these results to the main text in the revision.\n\n----(a)We directly modify a ResNet18 into a corresponding ODE. In a residual block, the forward pass is $y = x + f(x)$; when directly modified into ODE, the forward pass is $y(T) = y_0 + \\int_0^T f(y) dt$.\n\n----(b)Since it\u2019s a direct modification from ResNet, $f$ is $conv-bn-relu-conv-bn$, the same as the residual branch in a ResNet, and it is a *free-form* function.\n\n----(c)Accurate gradient estimation is the key to high performance\nWe trained a single model with the HeunEuler solver (adaptive), and tested with several different solvers. Our model modified achieved 4.85% and 22.66% error rate on CIFAR10 and CIFAR100 respectively, it has the same number of parameters as a ResNet18, but outperforms a standard ResNet101.\n\nOur model outperforms ODE trained with the adjoint method (reported in literature) by a large margin (75% reduction in error rate on CIFAR10), validating the importance of accurate gradient estimation.\n\n----(d) A continuous model is robust to different solvers.\nFor the same model, we trained it with the HeunEuler solver, but tested with different solvers (of different orders) during inference. We observed only 1% increase in error rate. This implies that our model is robust to different equivalent depths caused by different order of solvers, as discussed in 2(b)."}, "signatures": ["ICLR.cc/2020/Conference/Paper422/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper422/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["j.zhuang@yale.edu", "nicha.dvornek@yale.edu", "xiaoxiao.li@yale.edu", "james.duncan@yale.edu"], "title": "Ordinary differential equations on graph networks", "authors": ["Juntang Zhuang", "Nicha Dvornek", "Xiaoxiao Li", "James S. Duncan"], "pdf": "/pdf/5c8a94186856d657aa0a2d8b80c095bf6ddcf282.pdf", "TL;DR": "Apply ordinary differential equation model on graph structured data", "abstract": "Recently various neural networks have been proposed for irregularly structured data such as graphs and manifolds. To our knowledge, all existing graph networks have discrete depth. Inspired by neural ordinary differential equation (NODE) for data in the Euclidean domain, we extend the idea of continuous-depth models to graph data, and propose graph ordinary differential equation (GODE). The derivative of hidden node states are parameterized with a graph neural network, and the output states are the solution to this ordinary differential equation. We demonstrate two end-to-end methods for efficient training of GODE: (1) indirect back-propagation with the adjoint method; (2) direct back-propagation through the ODE solver, which accurately computes the gradient. We demonstrate that direct backprop outperforms the adjoint method in experiments. We then introduce a family of bijective blocks, which enables $\\mathcal{O}(1)$ memory consumption. We demonstrate that GODE can be easily adapted to different existing graph neural networks and improve accuracy. We validate the performance of GODE in both semi-supervised node classification tasks and graph classification tasks. Our GODE model achieves a continuous model in time, memory efficiency, accurate gradient estimation, and generalizability with different graph networks.", "keywords": ["Graph Networks", "Ordinary differential equation"], "paperhash": "zhuang|ordinary_differential_equations_on_graph_networks", "code": "https://www.dropbox.com/sh/sgaid3efh4eqmjl/AAB-DFXvNq_Pf313UqSLl4VPa?dl=0", "original_pdf": "/attachment/d2b601834ddcc9611d4dca58f62fabb6213cdee8.pdf", "_bibtex": "@misc{\nzhuang2020ordinary,\ntitle={Ordinary differential equations on graph networks},\nauthor={Juntang Zhuang and Nicha Dvornek and Xiaoxiao Li and James S. Duncan},\nyear={2020},\nurl={https://openreview.net/forum?id=SJg9z6VFDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJg9z6VFDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper422/Authors", "ICLR.cc/2020/Conference/Paper422/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper422/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper422/Reviewers", "ICLR.cc/2020/Conference/Paper422/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper422/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper422/Authors|ICLR.cc/2020/Conference/Paper422/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504171711, "tmdate": 1576860540355, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper422/Authors", "ICLR.cc/2020/Conference/Paper422/Reviewers", "ICLR.cc/2020/Conference/Paper422/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper422/-/Official_Comment"}}}, {"id": "HJe4NPehKS", "original": null, "number": 1, "cdate": 1571714860121, "ddate": null, "tcdate": 1571714860121, "tmdate": 1572972597239, "tddate": null, "forum": "SJg9z6VFDr", "replyto": "SJg9z6VFDr", "invitation": "ICLR.cc/2020/Conference/Paper422/-/Official_Review", "content": {"rating": "6: Weak Accept", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "Summary:  This work extends Neural ODE to graph networks and compares the continuous adjoint method with propagation through the ODE solver.\n\nThe paper addresses an interesting and important problem and it is well-written in general.\nTo determine the significance of this work, I have two questions:\n\nQuestion: \n1. What is the major difference between the original Neural ODE and the Graph Neural ODE? \nFor example, In graph networks, each node\u2019s representation may depend on its neighbor nodes. Will this impact the way you formulate the adjoints or compute the derivative?\n\n2. It seems in Mechanical engineering, various adjoint methods such as Discrete adjoint (e.g. [1])  has been studied.\nHow does the direct propagation through solver related to the these Discrete adjoint methods?\n\nSome minor comments for experiments:\nThe authors have 10 runs and take the best one. How about the average? which maybe a better indicator for stability.\nHow is the runtime comparing normal NN, adjoint, and direct propagation. Runtime has been a major disadvantage for Neural ODE.\n\nDecision:\nOverall, the novelty seems somewhat incremental, but I still feel the work is concrete and meaningful. I vote for weak accept.\nLooking forward to the code.\n\n[1] A Discrete Adjoint-Based Approach for Optimization Problems on Three-Dimensional Unstructured Meshes. Dimitri J. Mavriplis"}, "signatures": ["ICLR.cc/2020/Conference/Paper422/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper422/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["j.zhuang@yale.edu", "nicha.dvornek@yale.edu", "xiaoxiao.li@yale.edu", "james.duncan@yale.edu"], "title": "Ordinary differential equations on graph networks", "authors": ["Juntang Zhuang", "Nicha Dvornek", "Xiaoxiao Li", "James S. Duncan"], "pdf": "/pdf/5c8a94186856d657aa0a2d8b80c095bf6ddcf282.pdf", "TL;DR": "Apply ordinary differential equation model on graph structured data", "abstract": "Recently various neural networks have been proposed for irregularly structured data such as graphs and manifolds. To our knowledge, all existing graph networks have discrete depth. Inspired by neural ordinary differential equation (NODE) for data in the Euclidean domain, we extend the idea of continuous-depth models to graph data, and propose graph ordinary differential equation (GODE). The derivative of hidden node states are parameterized with a graph neural network, and the output states are the solution to this ordinary differential equation. We demonstrate two end-to-end methods for efficient training of GODE: (1) indirect back-propagation with the adjoint method; (2) direct back-propagation through the ODE solver, which accurately computes the gradient. We demonstrate that direct backprop outperforms the adjoint method in experiments. We then introduce a family of bijective blocks, which enables $\\mathcal{O}(1)$ memory consumption. We demonstrate that GODE can be easily adapted to different existing graph neural networks and improve accuracy. We validate the performance of GODE in both semi-supervised node classification tasks and graph classification tasks. Our GODE model achieves a continuous model in time, memory efficiency, accurate gradient estimation, and generalizability with different graph networks.", "keywords": ["Graph Networks", "Ordinary differential equation"], "paperhash": "zhuang|ordinary_differential_equations_on_graph_networks", "code": "https://www.dropbox.com/sh/sgaid3efh4eqmjl/AAB-DFXvNq_Pf313UqSLl4VPa?dl=0", "original_pdf": "/attachment/d2b601834ddcc9611d4dca58f62fabb6213cdee8.pdf", "_bibtex": "@misc{\nzhuang2020ordinary,\ntitle={Ordinary differential equations on graph networks},\nauthor={Juntang Zhuang and Nicha Dvornek and Xiaoxiao Li and James S. Duncan},\nyear={2020},\nurl={https://openreview.net/forum?id=SJg9z6VFDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SJg9z6VFDr", "replyto": "SJg9z6VFDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper422/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper422/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574722376000, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper422/Reviewers"], "noninvitees": [], "tcdate": 1570237752369, "tmdate": 1574723082307, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper422/-/Official_Review"}}}, {"id": "Syg40shatr", "original": null, "number": 2, "cdate": 1571830732026, "ddate": null, "tcdate": 1571830732026, "tmdate": 1572972597198, "tddate": null, "forum": "SJg9z6VFDr", "replyto": "SJg9z6VFDr", "invitation": "ICLR.cc/2020/Conference/Paper422/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary\n\nThe authors discussed that most graph NNs to date considered discrete layers and hence are difficult to model diffusion processes on graphs. This paper proposed the Neural ODE on a graph, termed with a graph ODE, to tackle this problem. The authors gave a sufficient condition under which the adjoint method gets instable and pointed out potential issues of training Neural ODEs using it. To overcome the instability issue, the author proposed to backpropagate errors directly at discretized points. Since the naive implementation of the direct method is memory-consuming, the authors used invertible blocks as building blocks of graph ODEs, which do not store the intermediate activations for backward propagation. Finally, the authors conducted empirical studies to see the effectiveness of the proposed method.\n\n\nDecision\n\nI recommend rejecting the paper weakly because I think the extension of Neural ODEs to graphs is straightforward and that the empirical study is not strong enough to compensate for the weakness of the novelty.\nTheoretical justification of numerical instability of Neural ODEs (Proposition 1) and its empirical verification (Section 5.4) give new insights for understanding Neural ODEs. However, if I understand correctly, the formulation of graph ODEs do not use the internal structures of graph NNs, even the fact that the underlying neural network is a graph NN. Therefore, I think the extension from Neural ODEs to graphs is a bit too straightforward. The authors proposed a method which improves the stability and memory-efficiency of training. We can apply this method to general Neural ODEs, too. In addition, we can attribute the idea of using invertible blocks to existing works (Gomez et al., 2017). Finally, regarding the empirical performance of graph ODEs, the performance improvement from existing GNNs is within the standard deviations. Therefore, I think the empirical result is not sufficiently strong to justify the novelty of applying Neural ODEs to graphs. Taking these things into account, although the authors gave a new result on Neural ODEs, I think the contribution is limited from the viewpoint of the study of graph NNs.\n\n\nSuggestion\n\n- As I wrote in the Decision section, the theoretical results are not restricted to graph ODEs but valid for general neural ODEs. Therefore, I think the authors do not have to restrict the application areas to graph ODEs. The possible direction of the paper is to further analysis of training neural ODEs (e.g., instability). On the other hand, if the authors are interested in the extension of neural ODEs to graphs, I expect a more detailed relationship between the neural ODEs framework and underlying GNNs. For example, I am curious how the topological information of graphs affects the graph ODEs via spectral-type graph convolution operations and what is the relationship to the oversmoothing phenomena (Li et al., 2018).\n- Since Theorem 1 is applicable not only graph ODEs but also Neural ODEs, it implies that ordinal Neural ODEs are also vulnerable to the instability. The experiments in Dupont (2019), which this paper referenced, pointed out the instability of Neural ODEs. I am wondering the proposed method can enhance the training of neural ODEs, too.\n\n[Li et al., 2018] Li, Qimai, Zhichao Han, and Xiao-Ming Wu. \"Deeper insights into graph convolutional networks for semi-supervised learning.\" Thirty-Second AAAI Conference on Artificial Intelligence. 2018."}, "signatures": ["ICLR.cc/2020/Conference/Paper422/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper422/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["j.zhuang@yale.edu", "nicha.dvornek@yale.edu", "xiaoxiao.li@yale.edu", "james.duncan@yale.edu"], "title": "Ordinary differential equations on graph networks", "authors": ["Juntang Zhuang", "Nicha Dvornek", "Xiaoxiao Li", "James S. Duncan"], "pdf": "/pdf/5c8a94186856d657aa0a2d8b80c095bf6ddcf282.pdf", "TL;DR": "Apply ordinary differential equation model on graph structured data", "abstract": "Recently various neural networks have been proposed for irregularly structured data such as graphs and manifolds. To our knowledge, all existing graph networks have discrete depth. Inspired by neural ordinary differential equation (NODE) for data in the Euclidean domain, we extend the idea of continuous-depth models to graph data, and propose graph ordinary differential equation (GODE). The derivative of hidden node states are parameterized with a graph neural network, and the output states are the solution to this ordinary differential equation. We demonstrate two end-to-end methods for efficient training of GODE: (1) indirect back-propagation with the adjoint method; (2) direct back-propagation through the ODE solver, which accurately computes the gradient. We demonstrate that direct backprop outperforms the adjoint method in experiments. We then introduce a family of bijective blocks, which enables $\\mathcal{O}(1)$ memory consumption. We demonstrate that GODE can be easily adapted to different existing graph neural networks and improve accuracy. We validate the performance of GODE in both semi-supervised node classification tasks and graph classification tasks. Our GODE model achieves a continuous model in time, memory efficiency, accurate gradient estimation, and generalizability with different graph networks.", "keywords": ["Graph Networks", "Ordinary differential equation"], "paperhash": "zhuang|ordinary_differential_equations_on_graph_networks", "code": "https://www.dropbox.com/sh/sgaid3efh4eqmjl/AAB-DFXvNq_Pf313UqSLl4VPa?dl=0", "original_pdf": "/attachment/d2b601834ddcc9611d4dca58f62fabb6213cdee8.pdf", "_bibtex": "@misc{\nzhuang2020ordinary,\ntitle={Ordinary differential equations on graph networks},\nauthor={Juntang Zhuang and Nicha Dvornek and Xiaoxiao Li and James S. Duncan},\nyear={2020},\nurl={https://openreview.net/forum?id=SJg9z6VFDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SJg9z6VFDr", "replyto": "SJg9z6VFDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper422/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper422/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574722376000, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper422/Reviewers"], "noninvitees": [], "tcdate": 1570237752369, "tmdate": 1574723082307, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper422/-/Official_Review"}}}], "count": 18}