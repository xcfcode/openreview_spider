{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1488470091021, "tcdate": 1478281348477, "number": 260, "id": "ry2YOrcge", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "ry2YOrcge", "signatures": ["~Arvind_Neelakantan1"], "readers": ["everyone"], "content": {"title": "Learning a Natural Language Interface with Neural Programmer", "abstract": "Learning a natural language interface for database tables is a challenging task that involves deep language understanding and multi-step reasoning. The task is often approached by mapping natural language queries to logical forms or programs that provide the desired response when executed on the database. To our knowledge, this paper presents the first weakly supervised, end-to-end neural network model to induce such programs on a real-world dataset. We enhance the objective function of Neural Programmer, a neural network with built-in discrete operations, and apply it on WikiTableQuestions, a natural language question-answering dataset. The model is trained end-to-end with weak supervision of question-answer pairs, and does not require domain-specific grammars, rules, or annotations that are key elements in previous approaches to program induction. The main experimental result in this paper is that a single Neural Programmer model achieves 34.2% accuracy using only 10,000 examples with weak supervision. An ensemble of 15 models, with a trivial combination technique, achieves 37.7% accuracy, which is competitive to the current state-of-the-art accuracy of 37.1% obtained by a traditional natural language semantic parser.", "pdf": "/pdf/9aafb921d591ef83b4288e36d124c2fd0a9234c5.pdf", "TL;DR": "To our knowledge, this paper presents the first weakly supervised, end-to-end neural network model to induce programs on a real-world  dataset.", "paperhash": "neelakantan|learning_a_natural_language_interface_with_neural_programmer", "keywords": ["Natural language processing", "Deep learning"], "conflicts": ["umass.edu", "google.com", "openai.com"], "authors": ["Arvind Neelakantan", "Quoc V. Le", "Martin Abadi", "Andrew McCallum", "Dario Amodei"], "authorids": ["arvind@cs.umass.edu", "qvl@google.com", "abadi@google.com", "mccallum@cs.umass.edu", "damodei@openai.com"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 16, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "tmdate": 1486432681139, "tcdate": 1486432681139, "number": 11, "id": "r1bhYiI_l", "invitation": "ICLR.cc/2017/conference/-/paper260/public/comment", "forum": "ry2YOrcge", "replyto": "Hyi43fIdx", "signatures": ["~Arvind_Neelakantan1"], "readers": ["everyone"], "writers": ["~Arvind_Neelakantan1"], "content": {"title": "Thanks for the thorough feedback!", "comment": "Thanks for taking time to fully understand the paper, its pros and cons, and giving a thorough feedback."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning a Natural Language Interface with Neural Programmer", "abstract": "Learning a natural language interface for database tables is a challenging task that involves deep language understanding and multi-step reasoning. The task is often approached by mapping natural language queries to logical forms or programs that provide the desired response when executed on the database. To our knowledge, this paper presents the first weakly supervised, end-to-end neural network model to induce such programs on a real-world dataset. We enhance the objective function of Neural Programmer, a neural network with built-in discrete operations, and apply it on WikiTableQuestions, a natural language question-answering dataset. The model is trained end-to-end with weak supervision of question-answer pairs, and does not require domain-specific grammars, rules, or annotations that are key elements in previous approaches to program induction. The main experimental result in this paper is that a single Neural Programmer model achieves 34.2% accuracy using only 10,000 examples with weak supervision. An ensemble of 15 models, with a trivial combination technique, achieves 37.7% accuracy, which is competitive to the current state-of-the-art accuracy of 37.1% obtained by a traditional natural language semantic parser.", "pdf": "/pdf/9aafb921d591ef83b4288e36d124c2fd0a9234c5.pdf", "TL;DR": "To our knowledge, this paper presents the first weakly supervised, end-to-end neural network model to induce programs on a real-world  dataset.", "paperhash": "neelakantan|learning_a_natural_language_interface_with_neural_programmer", "keywords": ["Natural language processing", "Deep learning"], "conflicts": ["umass.edu", "google.com", "openai.com"], "authors": ["Arvind Neelakantan", "Quoc V. Le", "Martin Abadi", "Andrew McCallum", "Dario Amodei"], "authorids": ["arvind@cs.umass.edu", "qvl@google.com", "abadi@google.com", "mccallum@cs.umass.edu", "damodei@openai.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287661022, "id": "ICLR.cc/2017/conference/-/paper260/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "ry2YOrcge", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper260/reviewers", "ICLR.cc/2017/conference/paper260/areachairs"], "cdate": 1485287661022}}}, {"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396467017, "tcdate": 1486396467017, "number": 1, "id": "Hyi43fIdx", "invitation": "ICLR.cc/2017/conference/-/paper260/acceptance", "forum": "ry2YOrcge", "replyto": "ry2YOrcge", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"title": "ICLR committee final decision", "comment": "The paper applies a previously introduced method (from ICLR '16) to the challenging question answering dataset (wikitables). The results are strong and quite close to the performance obtained by a semantic parser. There reviewers generally agree that this is an interesting and promising direction / results. The application of the neural programmer to this dataset required model modifications which are reasonable though quite straightforward, so, in that respect, the work is incremental. Still, achieving strong results on this moderately sized dataset with an expressive \n model is far from trivial. Though the approach, as has been discussed, does not directly generalize to QA with large knowledge bases (as well as other end-to-end differentiable methods for the QA task proposed so far), it is an important step forward and the task is already realistic and important.\n \n Pros\n \n + interesting direction\n + strong results on a interesting dataset\n \n Cons\n - incremental, the model is largely the same as in the previous paper", "decision": "Accept (Poster)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning a Natural Language Interface with Neural Programmer", "abstract": "Learning a natural language interface for database tables is a challenging task that involves deep language understanding and multi-step reasoning. The task is often approached by mapping natural language queries to logical forms or programs that provide the desired response when executed on the database. To our knowledge, this paper presents the first weakly supervised, end-to-end neural network model to induce such programs on a real-world dataset. We enhance the objective function of Neural Programmer, a neural network with built-in discrete operations, and apply it on WikiTableQuestions, a natural language question-answering dataset. The model is trained end-to-end with weak supervision of question-answer pairs, and does not require domain-specific grammars, rules, or annotations that are key elements in previous approaches to program induction. The main experimental result in this paper is that a single Neural Programmer model achieves 34.2% accuracy using only 10,000 examples with weak supervision. An ensemble of 15 models, with a trivial combination technique, achieves 37.7% accuracy, which is competitive to the current state-of-the-art accuracy of 37.1% obtained by a traditional natural language semantic parser.", "pdf": "/pdf/9aafb921d591ef83b4288e36d124c2fd0a9234c5.pdf", "TL;DR": "To our knowledge, this paper presents the first weakly supervised, end-to-end neural network model to induce programs on a real-world  dataset.", "paperhash": "neelakantan|learning_a_natural_language_interface_with_neural_programmer", "keywords": ["Natural language processing", "Deep learning"], "conflicts": ["umass.edu", "google.com", "openai.com"], "authors": ["Arvind Neelakantan", "Quoc V. Le", "Martin Abadi", "Andrew McCallum", "Dario Amodei"], "authorids": ["arvind@cs.umass.edu", "qvl@google.com", "abadi@google.com", "mccallum@cs.umass.edu", "damodei@openai.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396467535, "id": "ICLR.cc/2017/conference/-/paper260/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "ry2YOrcge", "replyto": "ry2YOrcge", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396467535}}}, {"tddate": null, "tmdate": 1484022357281, "tcdate": 1484017342612, "number": 9, "id": "ByPT0pZUl", "invitation": "ICLR.cc/2017/conference/-/paper260/public/comment", "forum": "ry2YOrcge", "replyto": "rka7qs-Ll", "signatures": ["~Arvind_Neelakantan1"], "readers": ["everyone"], "writers": ["~Arvind_Neelakantan1"], "content": {"title": "response", "comment": "Hi Kelly Zhang,\nThanks for the interest in our work.\n1) As mentioned in the paper, about 20% of the questions are not answerable because of table normalization and other data pre-processing issues. \nWe ran experiments by increasing the number of time steps till 7 and did not see any significant improvements.\nWhile the performance can be potentially improved by adding more operations, the number of training examples that require these operations are very small in number. We are not able to avoid overfitting in this setting. Hence, we do not think that the performance can be increased simply by adding more operations. \n2) We ran an experiment with logic operations (\"and\", \"or\" and \"not\") but the model ended up not using them. We think this is because there are fewer examples in the data that requires those operations, hence the model does not learn to use them."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning a Natural Language Interface with Neural Programmer", "abstract": "Learning a natural language interface for database tables is a challenging task that involves deep language understanding and multi-step reasoning. The task is often approached by mapping natural language queries to logical forms or programs that provide the desired response when executed on the database. To our knowledge, this paper presents the first weakly supervised, end-to-end neural network model to induce such programs on a real-world dataset. We enhance the objective function of Neural Programmer, a neural network with built-in discrete operations, and apply it on WikiTableQuestions, a natural language question-answering dataset. The model is trained end-to-end with weak supervision of question-answer pairs, and does not require domain-specific grammars, rules, or annotations that are key elements in previous approaches to program induction. The main experimental result in this paper is that a single Neural Programmer model achieves 34.2% accuracy using only 10,000 examples with weak supervision. An ensemble of 15 models, with a trivial combination technique, achieves 37.7% accuracy, which is competitive to the current state-of-the-art accuracy of 37.1% obtained by a traditional natural language semantic parser.", "pdf": "/pdf/9aafb921d591ef83b4288e36d124c2fd0a9234c5.pdf", "TL;DR": "To our knowledge, this paper presents the first weakly supervised, end-to-end neural network model to induce programs on a real-world  dataset.", "paperhash": "neelakantan|learning_a_natural_language_interface_with_neural_programmer", "keywords": ["Natural language processing", "Deep learning"], "conflicts": ["umass.edu", "google.com", "openai.com"], "authors": ["Arvind Neelakantan", "Quoc V. Le", "Martin Abadi", "Andrew McCallum", "Dario Amodei"], "authorids": ["arvind@cs.umass.edu", "qvl@google.com", "abadi@google.com", "mccallum@cs.umass.edu", "damodei@openai.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287661022, "id": "ICLR.cc/2017/conference/-/paper260/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "ry2YOrcge", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper260/reviewers", "ICLR.cc/2017/conference/paper260/areachairs"], "cdate": 1485287661022}}}, {"tddate": null, "tmdate": 1484007996348, "tcdate": 1484007973290, "number": 8, "id": "rka7qs-Ll", "invitation": "ICLR.cc/2017/conference/-/paper260/public/comment", "forum": "ry2YOrcge", "replyto": "ry2YOrcge", "signatures": ["~Kelly_Zhang1"], "readers": ["everyone"], "writers": ["~Kelly_Zhang1"], "content": {"title": "Questions regarding oracle and model design", "comment": "I have some questions about your paper:\n\n1. Why is the oracle performance only about 50%? Is it limited by the type of operations you provide? Also is the oracle also only run for 4 timesteps? (If so would increasing the number of timesteps improve the oracle?)\n\n2. In the paper that first introduces your neural programmer model (\"Neural Programmer: Inducing Latent Programs with Gradient Descent\"), you incorporate \"logic\" operations (\"or\" and \"and\"), why did you not include them as operations for this model?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning a Natural Language Interface with Neural Programmer", "abstract": "Learning a natural language interface for database tables is a challenging task that involves deep language understanding and multi-step reasoning. The task is often approached by mapping natural language queries to logical forms or programs that provide the desired response when executed on the database. To our knowledge, this paper presents the first weakly supervised, end-to-end neural network model to induce such programs on a real-world dataset. We enhance the objective function of Neural Programmer, a neural network with built-in discrete operations, and apply it on WikiTableQuestions, a natural language question-answering dataset. The model is trained end-to-end with weak supervision of question-answer pairs, and does not require domain-specific grammars, rules, or annotations that are key elements in previous approaches to program induction. The main experimental result in this paper is that a single Neural Programmer model achieves 34.2% accuracy using only 10,000 examples with weak supervision. An ensemble of 15 models, with a trivial combination technique, achieves 37.7% accuracy, which is competitive to the current state-of-the-art accuracy of 37.1% obtained by a traditional natural language semantic parser.", "pdf": "/pdf/9aafb921d591ef83b4288e36d124c2fd0a9234c5.pdf", "TL;DR": "To our knowledge, this paper presents the first weakly supervised, end-to-end neural network model to induce programs on a real-world  dataset.", "paperhash": "neelakantan|learning_a_natural_language_interface_with_neural_programmer", "keywords": ["Natural language processing", "Deep learning"], "conflicts": ["umass.edu", "google.com", "openai.com"], "authors": ["Arvind Neelakantan", "Quoc V. Le", "Martin Abadi", "Andrew McCallum", "Dario Amodei"], "authorids": ["arvind@cs.umass.edu", "qvl@google.com", "abadi@google.com", "mccallum@cs.umass.edu", "damodei@openai.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287661022, "id": "ICLR.cc/2017/conference/-/paper260/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "ry2YOrcge", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper260/reviewers", "ICLR.cc/2017/conference/paper260/areachairs"], "cdate": 1485287661022}}}, {"tddate": null, "tmdate": 1482613392690, "tcdate": 1482601871880, "number": 6, "id": "B1d5HVnEl", "invitation": "ICLR.cc/2017/conference/-/paper260/public/comment", "forum": "ry2YOrcge", "replyto": "ry2YOrcge", "signatures": ["~Arvind_Neelakantan1"], "readers": ["everyone"], "writers": ["~Arvind_Neelakantan1"], "content": {"title": "Update from the authors", "comment": "We thank all the reviewers for the constructive feedback. We performed more experiments and added significant new material to the paper. To summarize:\n1) We open-sourced the implementation of our model: https://github.com/tensorflow/models/tree/master/neural_programmer . The results reported in the paper can be reproduced using the provided code.\n2) We performed two kinds of model ablation studies (AnonReviewer3 and AnonReviewer4) :\n     a) The effect of different model design choices (Table 2 and Section 3.4.1).\n     b) The effect of different built-in operations (Table 4 and Section 3.4.3).\n3) We report oracle score and discuss how the model's performance can be potentially improved (Section 3.4.4). (AnonReviewer5)\n4) We added an appendix section discussing how the operations are exactly defined and the role of the variables. (AnonReviewer4)\nWe request the reviewers to check our revised submission, and our separate reply to each reviewer below."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning a Natural Language Interface with Neural Programmer", "abstract": "Learning a natural language interface for database tables is a challenging task that involves deep language understanding and multi-step reasoning. The task is often approached by mapping natural language queries to logical forms or programs that provide the desired response when executed on the database. To our knowledge, this paper presents the first weakly supervised, end-to-end neural network model to induce such programs on a real-world dataset. We enhance the objective function of Neural Programmer, a neural network with built-in discrete operations, and apply it on WikiTableQuestions, a natural language question-answering dataset. The model is trained end-to-end with weak supervision of question-answer pairs, and does not require domain-specific grammars, rules, or annotations that are key elements in previous approaches to program induction. The main experimental result in this paper is that a single Neural Programmer model achieves 34.2% accuracy using only 10,000 examples with weak supervision. An ensemble of 15 models, with a trivial combination technique, achieves 37.7% accuracy, which is competitive to the current state-of-the-art accuracy of 37.1% obtained by a traditional natural language semantic parser.", "pdf": "/pdf/9aafb921d591ef83b4288e36d124c2fd0a9234c5.pdf", "TL;DR": "To our knowledge, this paper presents the first weakly supervised, end-to-end neural network model to induce programs on a real-world  dataset.", "paperhash": "neelakantan|learning_a_natural_language_interface_with_neural_programmer", "keywords": ["Natural language processing", "Deep learning"], "conflicts": ["umass.edu", "google.com", "openai.com"], "authors": ["Arvind Neelakantan", "Quoc V. Le", "Martin Abadi", "Andrew McCallum", "Dario Amodei"], "authorids": ["arvind@cs.umass.edu", "qvl@google.com", "abadi@google.com", "mccallum@cs.umass.edu", "damodei@openai.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287661022, "id": "ICLR.cc/2017/conference/-/paper260/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "ry2YOrcge", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper260/reviewers", "ICLR.cc/2017/conference/paper260/areachairs"], "cdate": 1485287661022}}}, {"tddate": null, "tmdate": 1482602532221, "tcdate": 1482602068107, "number": 7, "id": "ry2ULVhNl", "invitation": "ICLR.cc/2017/conference/-/paper260/public/comment", "forum": "ry2YOrcge", "replyto": "S1V4jFBNg", "signatures": ["~Arvind_Neelakantan1"], "readers": ["everyone"], "writers": ["~Arvind_Neelakantan1"], "content": {"title": "Code open sourced,  Revised Submission with  Appendix and Model Ablation Studies", "comment": "We thank the reviewer for the constructive feedback. \n1) We open-sourced the implementation of our model: https://github.com/tensorflow/models/tree/master/neural_programmer . The results reported in the paper can be reproduced using the provided code.\n2) We added an appendix section discussing how the operations are exactly defined and the role of the variables.\n3) We agree that since we do full attention the model does not scale easily to huge databases. We have few ideas to scale up the model and leave that for future work. We would also like to point out the fact that public datasets requiring rich semantic parsing on large databases are not currently available. For example, current semantic parsing datasets on Freebase require much simpler programs to be induced than those considered in this work."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning a Natural Language Interface with Neural Programmer", "abstract": "Learning a natural language interface for database tables is a challenging task that involves deep language understanding and multi-step reasoning. The task is often approached by mapping natural language queries to logical forms or programs that provide the desired response when executed on the database. To our knowledge, this paper presents the first weakly supervised, end-to-end neural network model to induce such programs on a real-world dataset. We enhance the objective function of Neural Programmer, a neural network with built-in discrete operations, and apply it on WikiTableQuestions, a natural language question-answering dataset. The model is trained end-to-end with weak supervision of question-answer pairs, and does not require domain-specific grammars, rules, or annotations that are key elements in previous approaches to program induction. The main experimental result in this paper is that a single Neural Programmer model achieves 34.2% accuracy using only 10,000 examples with weak supervision. An ensemble of 15 models, with a trivial combination technique, achieves 37.7% accuracy, which is competitive to the current state-of-the-art accuracy of 37.1% obtained by a traditional natural language semantic parser.", "pdf": "/pdf/9aafb921d591ef83b4288e36d124c2fd0a9234c5.pdf", "TL;DR": "To our knowledge, this paper presents the first weakly supervised, end-to-end neural network model to induce programs on a real-world  dataset.", "paperhash": "neelakantan|learning_a_natural_language_interface_with_neural_programmer", "keywords": ["Natural language processing", "Deep learning"], "conflicts": ["umass.edu", "google.com", "openai.com"], "authors": ["Arvind Neelakantan", "Quoc V. Le", "Martin Abadi", "Andrew McCallum", "Dario Amodei"], "authorids": ["arvind@cs.umass.edu", "qvl@google.com", "abadi@google.com", "mccallum@cs.umass.edu", "damodei@openai.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287661022, "id": "ICLR.cc/2017/conference/-/paper260/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "ry2YOrcge", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper260/reviewers", "ICLR.cc/2017/conference/paper260/areachairs"], "cdate": 1485287661022}}}, {"tddate": null, "tmdate": 1482165036091, "tcdate": 1482165036091, "number": 3, "id": "S1V4jFBNg", "invitation": "ICLR.cc/2017/conference/-/paper260/official/review", "forum": "ry2YOrcge", "replyto": "ry2YOrcge", "signatures": ["ICLR.cc/2017/conference/paper260/AnonReviewer4"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper260/AnonReviewer4"], "content": {"title": "An interesting paper for a rather hard problem. ", "rating": "7: Good paper, accept", "review": "The paper presents an end-to-end neural network model for the problem of designing natural language interfaces for database queries. The proposed approach uses only weak supervision signals to learn the parameters of the model. Unlike in traditional approaches, where the problem is solved by semantically parsing a natural language query into logical forms and executing those logical forms over the given data base, the proposed approach trains a neural network in an end-to-end manner which goes directly from the natural language query to the final answer obtained by processing the data base. This is achieved by formulating a collection of operations to be performed over the data base as continuous operations, the distributions over which is learnt using the now-standard soft attention mechanisms. The model is validated on the smallish WikiTableQuestions dataset, where the authors show that a single model performs worse than the approach which uses the traditional Semantic Parsing technique. However an ensemble of 15 models (trained in a variety of ways) results in comparable performance to the state of the art. \n\nI feel that the paper proposes an interesting solution to the hard problem of learning natural language interfaces for data bases. The model is an extension of the previously proposed models of Neelakantan 2016. The experimental section is rather weak though. The authors only show their model work on a single smallish dataset. Would love to see more ablation studies of their model and comparison against fancier version of memnns (i do not buy their initial response to not testing against memory networks). \n\nI do have a few objections though. \n\n-- The details of the model are rather convoluted and the Section 2.1 is not very clearly written. In particular with the absence of the accompanying code the model will be super hard to replicate. I wish the authors do a better job in explaining the details as to how exactly the discrete operations are modeled, what is the role of the \"row selector\", the \"scalar answer\" and the \"lookup answer\" etc. \n\n-- The authors do a full attention over the entire database. Do they think this approach would scale when the data bases are huge (millions of rows)? Wish they experimented with larger datasets as well. ", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning a Natural Language Interface with Neural Programmer", "abstract": "Learning a natural language interface for database tables is a challenging task that involves deep language understanding and multi-step reasoning. The task is often approached by mapping natural language queries to logical forms or programs that provide the desired response when executed on the database. To our knowledge, this paper presents the first weakly supervised, end-to-end neural network model to induce such programs on a real-world dataset. We enhance the objective function of Neural Programmer, a neural network with built-in discrete operations, and apply it on WikiTableQuestions, a natural language question-answering dataset. The model is trained end-to-end with weak supervision of question-answer pairs, and does not require domain-specific grammars, rules, or annotations that are key elements in previous approaches to program induction. The main experimental result in this paper is that a single Neural Programmer model achieves 34.2% accuracy using only 10,000 examples with weak supervision. An ensemble of 15 models, with a trivial combination technique, achieves 37.7% accuracy, which is competitive to the current state-of-the-art accuracy of 37.1% obtained by a traditional natural language semantic parser.", "pdf": "/pdf/9aafb921d591ef83b4288e36d124c2fd0a9234c5.pdf", "TL;DR": "To our knowledge, this paper presents the first weakly supervised, end-to-end neural network model to induce programs on a real-world  dataset.", "paperhash": "neelakantan|learning_a_natural_language_interface_with_neural_programmer", "keywords": ["Natural language processing", "Deep learning"], "conflicts": ["umass.edu", "google.com", "openai.com"], "authors": ["Arvind Neelakantan", "Quoc V. Le", "Martin Abadi", "Andrew McCallum", "Dario Amodei"], "authorids": ["arvind@cs.umass.edu", "qvl@google.com", "abadi@google.com", "mccallum@cs.umass.edu", "damodei@openai.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512644711, "id": "ICLR.cc/2017/conference/-/paper260/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper260/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper260/AnonReviewer5", "ICLR.cc/2017/conference/paper260/AnonReviewer3", "ICLR.cc/2017/conference/paper260/AnonReviewer4"], "reply": {"forum": "ry2YOrcge", "replyto": "ry2YOrcge", "writers": {"values-regex": "ICLR.cc/2017/conference/paper260/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper260/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512644711}}}, {"tddate": null, "tmdate": 1482111479735, "tcdate": 1481910462914, "number": 4, "id": "S1Da_j-Ee", "invitation": "ICLR.cc/2017/conference/-/paper260/public/comment", "forum": "ry2YOrcge", "replyto": "BkNSquWEx", "signatures": ["~Arvind_Neelakantan1"], "readers": ["everyone"], "writers": ["~Arvind_Neelakantan1"], "content": {"title": "Revised submission with Oracle scores and Error Analysis", "comment": "We thank the reviewer for the constructive feedback. If the reviewer has more suggestions for error analysis, we would be happy to perform them. \n\n1) The oracle score is 50.5% which indicates that there is still a lot of room for improvement.\n2) We think there are three reasons why the accuracy is lower compared to other NLP tasks: \na) Weak Supervision: most of the standard NLP tasks like parsing, part-of-speech tagging and machine translation have full supervision. Whereas in this task, the program that needs to be induced is not annotated and the model learns from a weak supervision signal consisting of only the final answer.\nb) Small Training set: The dataset considered in this work has only 10k training examples. The datasets for other related language understanding tasks like textual entailment (http://nlp.stanford.edu/projects/snli/) and reading comprehension (https://rajpurkar.github.io/SQuAD-explorer/) have at least an order of magnitude more examples. The accuracy of our model in the training set is 53% while it is only 34% in the development and test set indicating that there is significant overfitting even after employing strong regularization. We think that with more training data, this gap can be reduced. \nc) 21% questions not answerable: The paper that introduced this dataset (http://cs.stanford.edu/~ppasupat/resource/ACL2015-paper.pdf) reports that 21% of questions (on a random sample of 200 examples) cannot be answered because of various issues like annotation errors, tables requiring advanced normalization etc.\n\nWe have revised our submission incorporating the above analysis."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning a Natural Language Interface with Neural Programmer", "abstract": "Learning a natural language interface for database tables is a challenging task that involves deep language understanding and multi-step reasoning. The task is often approached by mapping natural language queries to logical forms or programs that provide the desired response when executed on the database. To our knowledge, this paper presents the first weakly supervised, end-to-end neural network model to induce such programs on a real-world dataset. We enhance the objective function of Neural Programmer, a neural network with built-in discrete operations, and apply it on WikiTableQuestions, a natural language question-answering dataset. The model is trained end-to-end with weak supervision of question-answer pairs, and does not require domain-specific grammars, rules, or annotations that are key elements in previous approaches to program induction. The main experimental result in this paper is that a single Neural Programmer model achieves 34.2% accuracy using only 10,000 examples with weak supervision. An ensemble of 15 models, with a trivial combination technique, achieves 37.7% accuracy, which is competitive to the current state-of-the-art accuracy of 37.1% obtained by a traditional natural language semantic parser.", "pdf": "/pdf/9aafb921d591ef83b4288e36d124c2fd0a9234c5.pdf", "TL;DR": "To our knowledge, this paper presents the first weakly supervised, end-to-end neural network model to induce programs on a real-world  dataset.", "paperhash": "neelakantan|learning_a_natural_language_interface_with_neural_programmer", "keywords": ["Natural language processing", "Deep learning"], "conflicts": ["umass.edu", "google.com", "openai.com"], "authors": ["Arvind Neelakantan", "Quoc V. Le", "Martin Abadi", "Andrew McCallum", "Dario Amodei"], "authorids": ["arvind@cs.umass.edu", "qvl@google.com", "abadi@google.com", "mccallum@cs.umass.edu", "damodei@openai.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287661022, "id": "ICLR.cc/2017/conference/-/paper260/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "ry2YOrcge", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper260/reviewers", "ICLR.cc/2017/conference/paper260/areachairs"], "cdate": 1485287661022}}}, {"tddate": null, "tmdate": 1481995260823, "tcdate": 1481995260823, "number": 5, "id": "BkBZ4g7Nx", "invitation": "ICLR.cc/2017/conference/-/paper260/public/comment", "forum": "ry2YOrcge", "replyto": "r1CLSZMNe", "signatures": ["~Arvind_Neelakantan1"], "readers": ["everyone"], "writers": ["~Arvind_Neelakantan1"], "content": {"title": "Code open sourced and model ablation studies added", "comment": "We thank the reviewer for the constructive feedback.\n\n1) As mentioned in the paper, we have open sourced our code and the code is here: https://github.com/tensorflow/models/tree/master/neural_programmer . The results presented in the paper can be reproduced using the code.\n2) We have included an ablation study under the title \"contribution of different operations\", analyzing the contribution of different operations. We revised our submission and request the reviewer to check the latest version of the paper. The paper also discusses the effect of adding regularization to the model. We are currently running experiments to study the effect of anonymizing matched phrases and the addition of boolean features. Apart from these changes, to the best of our knowledge, the work presented in the paper does not make any significant new model design choices. For example, the newly proposed training objective is a requirement to apply Neural Programmer on this dataset and not a choice.  If the reviewer has more model ablation studies in mind, we would be happy to include them.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning a Natural Language Interface with Neural Programmer", "abstract": "Learning a natural language interface for database tables is a challenging task that involves deep language understanding and multi-step reasoning. The task is often approached by mapping natural language queries to logical forms or programs that provide the desired response when executed on the database. To our knowledge, this paper presents the first weakly supervised, end-to-end neural network model to induce such programs on a real-world dataset. We enhance the objective function of Neural Programmer, a neural network with built-in discrete operations, and apply it on WikiTableQuestions, a natural language question-answering dataset. The model is trained end-to-end with weak supervision of question-answer pairs, and does not require domain-specific grammars, rules, or annotations that are key elements in previous approaches to program induction. The main experimental result in this paper is that a single Neural Programmer model achieves 34.2% accuracy using only 10,000 examples with weak supervision. An ensemble of 15 models, with a trivial combination technique, achieves 37.7% accuracy, which is competitive to the current state-of-the-art accuracy of 37.1% obtained by a traditional natural language semantic parser.", "pdf": "/pdf/9aafb921d591ef83b4288e36d124c2fd0a9234c5.pdf", "TL;DR": "To our knowledge, this paper presents the first weakly supervised, end-to-end neural network model to induce programs on a real-world  dataset.", "paperhash": "neelakantan|learning_a_natural_language_interface_with_neural_programmer", "keywords": ["Natural language processing", "Deep learning"], "conflicts": ["umass.edu", "google.com", "openai.com"], "authors": ["Arvind Neelakantan", "Quoc V. Le", "Martin Abadi", "Andrew McCallum", "Dario Amodei"], "authorids": ["arvind@cs.umass.edu", "qvl@google.com", "abadi@google.com", "mccallum@cs.umass.edu", "damodei@openai.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287661022, "id": "ICLR.cc/2017/conference/-/paper260/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "ry2YOrcge", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper260/reviewers", "ICLR.cc/2017/conference/paper260/areachairs"], "cdate": 1485287661022}}}, {"tddate": null, "tmdate": 1481934180795, "tcdate": 1481934166379, "number": 2, "id": "r1CLSZMNe", "invitation": "ICLR.cc/2017/conference/-/paper260/official/review", "forum": "ry2YOrcge", "replyto": "ry2YOrcge", "signatures": ["ICLR.cc/2017/conference/paper260/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper260/AnonReviewer3"], "content": {"title": "review", "rating": "6: Marginally above acceptance threshold", "review": "This paper proposes a weakly supervised, end-to-end neural network model for solving a challenging natural language understanding task. \nAs an extension of the Neural Programmer, this work aims at overcoming the ambiguities imposed by natural language. \nBy predefining a set of operations, the model is able to learn the interface between the language reasoning and answer composition using backpropagation. \nOn the WikiTableQuestions dataset, it is able to achieve a slightly better performance than the traditional semantic parser methods. \n\nOverall, this is a very interesting and promising work as it involves a lot of real-world challenges about natural language understanding. \nThe intuitions and design of the model are very clear, but the complication makes the paper a bit difficult to read, which means the model is also difficult to be reimplemented. I would expect to see more details about model ablation and it would help us figure out the prominent parts of the model design. \n\n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning a Natural Language Interface with Neural Programmer", "abstract": "Learning a natural language interface for database tables is a challenging task that involves deep language understanding and multi-step reasoning. The task is often approached by mapping natural language queries to logical forms or programs that provide the desired response when executed on the database. To our knowledge, this paper presents the first weakly supervised, end-to-end neural network model to induce such programs on a real-world dataset. We enhance the objective function of Neural Programmer, a neural network with built-in discrete operations, and apply it on WikiTableQuestions, a natural language question-answering dataset. The model is trained end-to-end with weak supervision of question-answer pairs, and does not require domain-specific grammars, rules, or annotations that are key elements in previous approaches to program induction. The main experimental result in this paper is that a single Neural Programmer model achieves 34.2% accuracy using only 10,000 examples with weak supervision. An ensemble of 15 models, with a trivial combination technique, achieves 37.7% accuracy, which is competitive to the current state-of-the-art accuracy of 37.1% obtained by a traditional natural language semantic parser.", "pdf": "/pdf/9aafb921d591ef83b4288e36d124c2fd0a9234c5.pdf", "TL;DR": "To our knowledge, this paper presents the first weakly supervised, end-to-end neural network model to induce programs on a real-world  dataset.", "paperhash": "neelakantan|learning_a_natural_language_interface_with_neural_programmer", "keywords": ["Natural language processing", "Deep learning"], "conflicts": ["umass.edu", "google.com", "openai.com"], "authors": ["Arvind Neelakantan", "Quoc V. Le", "Martin Abadi", "Andrew McCallum", "Dario Amodei"], "authorids": ["arvind@cs.umass.edu", "qvl@google.com", "abadi@google.com", "mccallum@cs.umass.edu", "damodei@openai.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512644711, "id": "ICLR.cc/2017/conference/-/paper260/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper260/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper260/AnonReviewer5", "ICLR.cc/2017/conference/paper260/AnonReviewer3", "ICLR.cc/2017/conference/paper260/AnonReviewer4"], "reply": {"forum": "ry2YOrcge", "replyto": "ry2YOrcge", "writers": {"values-regex": "ICLR.cc/2017/conference/paper260/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper260/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512644711}}}, {"tddate": null, "tmdate": 1481898556159, "tcdate": 1481898556159, "number": 1, "id": "BkNSquWEx", "invitation": "ICLR.cc/2017/conference/-/paper260/official/review", "forum": "ry2YOrcge", "replyto": "ry2YOrcge", "signatures": ["ICLR.cc/2017/conference/paper260/AnonReviewer5"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper260/AnonReviewer5"], "content": {"title": "A paper on a challenging task", "rating": "6: Marginally above acceptance threshold", "review": "This paper proposes a weakly supervised, end-to-end neural network model to learn a natural language interface for tables. The neural programmer is applied to the WikiTableQuestions, a natural language QA dataset and achieves reasonable accuracy. An ensemble further boosts the performance by combining components built with different configurations, and achieves comparable performance as the traditional natural language semantic parser baseline. Dropout and weight decay seem to play a significant role.\n\nIt'll be interesting to see more error analysis and the major reason for the still low accuracy compared to many other NLP tasks. What's the headroom and oracle number with the current approach?\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning a Natural Language Interface with Neural Programmer", "abstract": "Learning a natural language interface for database tables is a challenging task that involves deep language understanding and multi-step reasoning. The task is often approached by mapping natural language queries to logical forms or programs that provide the desired response when executed on the database. To our knowledge, this paper presents the first weakly supervised, end-to-end neural network model to induce such programs on a real-world dataset. We enhance the objective function of Neural Programmer, a neural network with built-in discrete operations, and apply it on WikiTableQuestions, a natural language question-answering dataset. The model is trained end-to-end with weak supervision of question-answer pairs, and does not require domain-specific grammars, rules, or annotations that are key elements in previous approaches to program induction. The main experimental result in this paper is that a single Neural Programmer model achieves 34.2% accuracy using only 10,000 examples with weak supervision. An ensemble of 15 models, with a trivial combination technique, achieves 37.7% accuracy, which is competitive to the current state-of-the-art accuracy of 37.1% obtained by a traditional natural language semantic parser.", "pdf": "/pdf/9aafb921d591ef83b4288e36d124c2fd0a9234c5.pdf", "TL;DR": "To our knowledge, this paper presents the first weakly supervised, end-to-end neural network model to induce programs on a real-world  dataset.", "paperhash": "neelakantan|learning_a_natural_language_interface_with_neural_programmer", "keywords": ["Natural language processing", "Deep learning"], "conflicts": ["umass.edu", "google.com", "openai.com"], "authors": ["Arvind Neelakantan", "Quoc V. Le", "Martin Abadi", "Andrew McCallum", "Dario Amodei"], "authorids": ["arvind@cs.umass.edu", "qvl@google.com", "abadi@google.com", "mccallum@cs.umass.edu", "damodei@openai.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512644711, "id": "ICLR.cc/2017/conference/-/paper260/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper260/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper260/AnonReviewer5", "ICLR.cc/2017/conference/paper260/AnonReviewer3", "ICLR.cc/2017/conference/paper260/AnonReviewer4"], "reply": {"forum": "ry2YOrcge", "replyto": "ry2YOrcge", "writers": {"values-regex": "ICLR.cc/2017/conference/paper260/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper260/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512644711}}}, {"tddate": null, "tmdate": 1480955120200, "tcdate": 1480955120194, "number": 3, "id": "ry_gBGQXe", "invitation": "ICLR.cc/2017/conference/-/paper260/public/comment", "forum": "ry2YOrcge", "replyto": "Bya_XemQl", "signatures": ["~Arvind_Neelakantan1"], "readers": ["everyone"], "writers": ["~Arvind_Neelakantan1"], "content": {"title": "Modified pointer network baseline is close to Memory Networks and a stronger baseline for this task", "comment": "1) The selector module induces two probability distributions at every time step: one over the set of operations and other over the set of columns. Operations that output a scalar modify the scalar_answer variable. In the current version, only the count operation outputs a scalar. Hence, scalar_answer is set as the output of the count operation multiplied by the probability assigned to that operation by the model. Similarly, a column of the lookup_answer is updated using the probability assigned to that column and the probability assigned to the print operation. While the set of operations are different, the definitions of these variables follow prior work: https://arxiv.org/pdf/1511.04834v2.pdf \nWe made small changes to the notation to make it more clear and revised our submission.\n2) We selected T=4 after looking at the programs induced in the baseline paper: http://cs.stanford.edu/~ppasupat/resource/ACL2015-paper.pdf\nWe experimented with larger values till T=7 but we did not see performance improvements which makes it likely that the dataset does not contain questions that require longer programs.\n3) Section 3.3.1 talks about neural network baselines. Memory Networks generate answers that need not be present in the memory. However in this task, a majority of the questions require the model to pick entries from the table as the final answer. Questions that require an answer that is not in the table are mostly numbers which neural networks are not good at generating. This can be seen in the Seq2Seq experiment discussed in the same section. Hence, a pointer network model which selects entries from the input is a more suitable model for this task. We modify the pointer network model to select entries only from the table (and not from the question) after reading the question with many latent pondering steps. So, the baseline discussed in the paper is very similar to the end-to-end memory network model with differences being that the memory is a table and the model selects entries from the table as the final answer instead of generating a new answer. These differences are motivated by the task in hand."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning a Natural Language Interface with Neural Programmer", "abstract": "Learning a natural language interface for database tables is a challenging task that involves deep language understanding and multi-step reasoning. The task is often approached by mapping natural language queries to logical forms or programs that provide the desired response when executed on the database. To our knowledge, this paper presents the first weakly supervised, end-to-end neural network model to induce such programs on a real-world dataset. We enhance the objective function of Neural Programmer, a neural network with built-in discrete operations, and apply it on WikiTableQuestions, a natural language question-answering dataset. The model is trained end-to-end with weak supervision of question-answer pairs, and does not require domain-specific grammars, rules, or annotations that are key elements in previous approaches to program induction. The main experimental result in this paper is that a single Neural Programmer model achieves 34.2% accuracy using only 10,000 examples with weak supervision. An ensemble of 15 models, with a trivial combination technique, achieves 37.7% accuracy, which is competitive to the current state-of-the-art accuracy of 37.1% obtained by a traditional natural language semantic parser.", "pdf": "/pdf/9aafb921d591ef83b4288e36d124c2fd0a9234c5.pdf", "TL;DR": "To our knowledge, this paper presents the first weakly supervised, end-to-end neural network model to induce programs on a real-world  dataset.", "paperhash": "neelakantan|learning_a_natural_language_interface_with_neural_programmer", "keywords": ["Natural language processing", "Deep learning"], "conflicts": ["umass.edu", "google.com", "openai.com"], "authors": ["Arvind Neelakantan", "Quoc V. Le", "Martin Abadi", "Andrew McCallum", "Dario Amodei"], "authorids": ["arvind@cs.umass.edu", "qvl@google.com", "abadi@google.com", "mccallum@cs.umass.edu", "damodei@openai.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287661022, "id": "ICLR.cc/2017/conference/-/paper260/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "ry2YOrcge", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper260/reviewers", "ICLR.cc/2017/conference/paper260/areachairs"], "cdate": 1485287661022}}}, {"tddate": null, "tmdate": 1480946548579, "tcdate": 1480946548575, "number": 2, "id": "Bya_XemQl", "invitation": "ICLR.cc/2017/conference/-/paper260/pre-review/question", "forum": "ry2YOrcge", "replyto": "ry2YOrcge", "signatures": ["ICLR.cc/2017/conference/paper260/AnonReviewer4"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper260/AnonReviewer4"], "content": {"title": "Model questions", "question": "-- Definitions of lookup_answer and scalar_answer are not clear. \n-- Why run the model for only T=4 time steps? Where did this choice come from? \n-- Baselines seem very weak. Did you compare with models which have explicit memory, such as, Memory Networks? "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning a Natural Language Interface with Neural Programmer", "abstract": "Learning a natural language interface for database tables is a challenging task that involves deep language understanding and multi-step reasoning. The task is often approached by mapping natural language queries to logical forms or programs that provide the desired response when executed on the database. To our knowledge, this paper presents the first weakly supervised, end-to-end neural network model to induce such programs on a real-world dataset. We enhance the objective function of Neural Programmer, a neural network with built-in discrete operations, and apply it on WikiTableQuestions, a natural language question-answering dataset. The model is trained end-to-end with weak supervision of question-answer pairs, and does not require domain-specific grammars, rules, or annotations that are key elements in previous approaches to program induction. The main experimental result in this paper is that a single Neural Programmer model achieves 34.2% accuracy using only 10,000 examples with weak supervision. An ensemble of 15 models, with a trivial combination technique, achieves 37.7% accuracy, which is competitive to the current state-of-the-art accuracy of 37.1% obtained by a traditional natural language semantic parser.", "pdf": "/pdf/9aafb921d591ef83b4288e36d124c2fd0a9234c5.pdf", "TL;DR": "To our knowledge, this paper presents the first weakly supervised, end-to-end neural network model to induce programs on a real-world  dataset.", "paperhash": "neelakantan|learning_a_natural_language_interface_with_neural_programmer", "keywords": ["Natural language processing", "Deep learning"], "conflicts": ["umass.edu", "google.com", "openai.com"], "authors": ["Arvind Neelakantan", "Quoc V. Le", "Martin Abadi", "Andrew McCallum", "Dario Amodei"], "authorids": ["arvind@cs.umass.edu", "qvl@google.com", "abadi@google.com", "mccallum@cs.umass.edu", "damodei@openai.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959374396, "id": "ICLR.cc/2017/conference/-/paper260/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper260/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper260/AnonReviewer3", "ICLR.cc/2017/conference/paper260/AnonReviewer4"], "reply": {"forum": "ry2YOrcge", "replyto": "ry2YOrcge", "writers": {"values-regex": "ICLR.cc/2017/conference/paper260/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper260/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959374396}}}, {"tddate": null, "tmdate": 1480892588219, "tcdate": 1480892588214, "number": 2, "id": "By4nxQGme", "invitation": "ICLR.cc/2017/conference/-/paper260/public/comment", "forum": "ry2YOrcge", "replyto": "BkdVLybQg", "signatures": ["~Arvind_Neelakantan1"], "readers": ["everyone"], "writers": ["~Arvind_Neelakantan1"], "content": {"title": "Typo in equation", "comment": "We found a typo in the lookup loss equation (y[i][j] should be g[i][j]). We have fixed it and revised our submission."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning a Natural Language Interface with Neural Programmer", "abstract": "Learning a natural language interface for database tables is a challenging task that involves deep language understanding and multi-step reasoning. The task is often approached by mapping natural language queries to logical forms or programs that provide the desired response when executed on the database. To our knowledge, this paper presents the first weakly supervised, end-to-end neural network model to induce such programs on a real-world dataset. We enhance the objective function of Neural Programmer, a neural network with built-in discrete operations, and apply it on WikiTableQuestions, a natural language question-answering dataset. The model is trained end-to-end with weak supervision of question-answer pairs, and does not require domain-specific grammars, rules, or annotations that are key elements in previous approaches to program induction. The main experimental result in this paper is that a single Neural Programmer model achieves 34.2% accuracy using only 10,000 examples with weak supervision. An ensemble of 15 models, with a trivial combination technique, achieves 37.7% accuracy, which is competitive to the current state-of-the-art accuracy of 37.1% obtained by a traditional natural language semantic parser.", "pdf": "/pdf/9aafb921d591ef83b4288e36d124c2fd0a9234c5.pdf", "TL;DR": "To our knowledge, this paper presents the first weakly supervised, end-to-end neural network model to induce programs on a real-world  dataset.", "paperhash": "neelakantan|learning_a_natural_language_interface_with_neural_programmer", "keywords": ["Natural language processing", "Deep learning"], "conflicts": ["umass.edu", "google.com", "openai.com"], "authors": ["Arvind Neelakantan", "Quoc V. Le", "Martin Abadi", "Andrew McCallum", "Dario Amodei"], "authorids": ["arvind@cs.umass.edu", "qvl@google.com", "abadi@google.com", "mccallum@cs.umass.edu", "damodei@openai.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287661022, "id": "ICLR.cc/2017/conference/-/paper260/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "ry2YOrcge", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper260/reviewers", "ICLR.cc/2017/conference/paper260/areachairs"], "cdate": 1485287661022}}}, {"tddate": null, "tmdate": 1480812080036, "tcdate": 1480812080030, "number": 1, "id": "BkdVLybQg", "invitation": "ICLR.cc/2017/conference/-/paper260/public/comment", "forum": "ry2YOrcge", "replyto": "SyPSqhJme", "signatures": ["~Arvind_Neelakantan1"], "readers": ["everyone"], "writers": ["~Arvind_Neelakantan1"], "content": {"title": "Lookup Loss and Regularization", "comment": "1) At a high level, the lookup loss follows the one described in: https://arxiv.org/pdf/1511.04834v3.pdf . Each entry in the variable \"lookup_answer\" stores the probability that the corresponding table entry is part of the model prediction. The lookup loss has two terms: the first term computes the loss for entries that are part of the ground-truth answer, while the second term computes it for entries that are not part of the ground-truth answer. We use log loss in both cases. \nIn this work, the first term additionally handles the ambiguity that arises when the ground-truth answer is simply written down instead of being explicitly marked in the table. When the ground-truth answer occurs in multiple table entries, we do not know which entry (or entries) in the table is actually responsible for the answer. So, we pick the entry that has the minimum loss. Intuitively, we encourage the model to select at least one path among the many possible paths to reach the final answer.    \n2) We exactly follow prior work in applying dropout and weight decay (or L2 regularization). The dropout techniques we use  along with the hyperparameter settings for dropout and weight decay are discussed in the subsection \"Training Details\" in the \"Experiment\" section.\nWe would be happy to answer further questions on the above explanation."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning a Natural Language Interface with Neural Programmer", "abstract": "Learning a natural language interface for database tables is a challenging task that involves deep language understanding and multi-step reasoning. The task is often approached by mapping natural language queries to logical forms or programs that provide the desired response when executed on the database. To our knowledge, this paper presents the first weakly supervised, end-to-end neural network model to induce such programs on a real-world dataset. We enhance the objective function of Neural Programmer, a neural network with built-in discrete operations, and apply it on WikiTableQuestions, a natural language question-answering dataset. The model is trained end-to-end with weak supervision of question-answer pairs, and does not require domain-specific grammars, rules, or annotations that are key elements in previous approaches to program induction. The main experimental result in this paper is that a single Neural Programmer model achieves 34.2% accuracy using only 10,000 examples with weak supervision. An ensemble of 15 models, with a trivial combination technique, achieves 37.7% accuracy, which is competitive to the current state-of-the-art accuracy of 37.1% obtained by a traditional natural language semantic parser.", "pdf": "/pdf/9aafb921d591ef83b4288e36d124c2fd0a9234c5.pdf", "TL;DR": "To our knowledge, this paper presents the first weakly supervised, end-to-end neural network model to induce programs on a real-world  dataset.", "paperhash": "neelakantan|learning_a_natural_language_interface_with_neural_programmer", "keywords": ["Natural language processing", "Deep learning"], "conflicts": ["umass.edu", "google.com", "openai.com"], "authors": ["Arvind Neelakantan", "Quoc V. Le", "Martin Abadi", "Andrew McCallum", "Dario Amodei"], "authorids": ["arvind@cs.umass.edu", "qvl@google.com", "abadi@google.com", "mccallum@cs.umass.edu", "damodei@openai.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287661022, "id": "ICLR.cc/2017/conference/-/paper260/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "ry2YOrcge", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper260/reviewers", "ICLR.cc/2017/conference/paper260/areachairs"], "cdate": 1485287661022}}}, {"tddate": null, "tmdate": 1480735294630, "tcdate": 1480735294626, "number": 1, "id": "SyPSqhJme", "invitation": "ICLR.cc/2017/conference/-/paper260/pre-review/question", "forum": "ry2YOrcge", "replyto": "ry2YOrcge", "signatures": ["ICLR.cc/2017/conference/paper260/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper260/AnonReviewer3"], "content": {"title": "clarify the definition of the lookup answer loss", "question": "Could you please clarify the definition of the lookup answer loss? The notation there is a bit confusing. And could you give more explanations on model ablation? The model is improved a lot by using regularization, but there seems no contents related to the way of carrying out the regularization. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning a Natural Language Interface with Neural Programmer", "abstract": "Learning a natural language interface for database tables is a challenging task that involves deep language understanding and multi-step reasoning. The task is often approached by mapping natural language queries to logical forms or programs that provide the desired response when executed on the database. To our knowledge, this paper presents the first weakly supervised, end-to-end neural network model to induce such programs on a real-world dataset. We enhance the objective function of Neural Programmer, a neural network with built-in discrete operations, and apply it on WikiTableQuestions, a natural language question-answering dataset. The model is trained end-to-end with weak supervision of question-answer pairs, and does not require domain-specific grammars, rules, or annotations that are key elements in previous approaches to program induction. The main experimental result in this paper is that a single Neural Programmer model achieves 34.2% accuracy using only 10,000 examples with weak supervision. An ensemble of 15 models, with a trivial combination technique, achieves 37.7% accuracy, which is competitive to the current state-of-the-art accuracy of 37.1% obtained by a traditional natural language semantic parser.", "pdf": "/pdf/9aafb921d591ef83b4288e36d124c2fd0a9234c5.pdf", "TL;DR": "To our knowledge, this paper presents the first weakly supervised, end-to-end neural network model to induce programs on a real-world  dataset.", "paperhash": "neelakantan|learning_a_natural_language_interface_with_neural_programmer", "keywords": ["Natural language processing", "Deep learning"], "conflicts": ["umass.edu", "google.com", "openai.com"], "authors": ["Arvind Neelakantan", "Quoc V. Le", "Martin Abadi", "Andrew McCallum", "Dario Amodei"], "authorids": ["arvind@cs.umass.edu", "qvl@google.com", "abadi@google.com", "mccallum@cs.umass.edu", "damodei@openai.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959374396, "id": "ICLR.cc/2017/conference/-/paper260/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper260/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper260/AnonReviewer3", "ICLR.cc/2017/conference/paper260/AnonReviewer4"], "reply": {"forum": "ry2YOrcge", "replyto": "ry2YOrcge", "writers": {"values-regex": "ICLR.cc/2017/conference/paper260/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper260/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959374396}}}], "count": 17}