{"notes": [{"id": "0EJjoRbFEcX", "original": "QEAeR5zs5uu", "number": 477, "cdate": 1601308060526, "ddate": null, "tcdate": 1601308060526, "tmdate": 1614985623540, "tddate": null, "forum": "0EJjoRbFEcX", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Understanding Classifiers with Generative Models", "authorids": ["~La\u00ebtitia_Shao1", "~Yang_Song1", "~Stefano_Ermon1"], "authors": ["La\u00ebtitia Shao", "Yang Song", "Stefano Ermon"], "keywords": ["OOD detection", "adversarial samples detection", "deep learning", "classification"], "abstract": "Although deep neural networks are effective on supervised learning tasks, they have been shown to be brittle. They are prone to overfitting on their training distribution and are easily fooled by small adversarial perturbations. In this paper, we leverage generative models to identify and characterize instances where classifiers fail to generalize.\nWe propose a generative model of the features extracted by a classifier, and show using rigorous hypothesis testing that errors tend to occur when features are assigned low-probability by our model. From this observation, we develop a detection criteria that we test against different sources of classification mistakes: mistakes made on the test set due to poor model generalization, adversarial samples and out-of-distribution samples. Our approach is agnostic to class labels from the training set which makes it applicable to models trained in a semi-supervised way.", "one-sentence_summary": "We show a new characterization of classification mistakes using a generative model learned on the feature space", "pdf": "/pdf/1b1096a18581d8a92cdcbe8e9011b9513d90fa7c.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shao|understanding_classifiers_with_generative_models", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=oLWOrRuL19", "_bibtex": "@misc{\nshao2021understanding,\ntitle={Understanding Classifiers with Generative Models},\nauthor={La{\\\"e}titia Shao and Yang Song and Stefano Ermon},\nyear={2021},\nurl={https://openreview.net/forum?id=0EJjoRbFEcX}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "PW-IrTIMkJS", "original": null, "number": 1, "cdate": 1610040536282, "ddate": null, "tcdate": 1610040536282, "tmdate": 1610474146308, "tddate": null, "forum": "0EJjoRbFEcX", "replyto": "0EJjoRbFEcX", "invitation": "ICLR.cc/2021/Conference/Paper477/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "As several reviewers pointed out, the contribution is  too incremental from previous work."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Understanding Classifiers with Generative Models", "authorids": ["~La\u00ebtitia_Shao1", "~Yang_Song1", "~Stefano_Ermon1"], "authors": ["La\u00ebtitia Shao", "Yang Song", "Stefano Ermon"], "keywords": ["OOD detection", "adversarial samples detection", "deep learning", "classification"], "abstract": "Although deep neural networks are effective on supervised learning tasks, they have been shown to be brittle. They are prone to overfitting on their training distribution and are easily fooled by small adversarial perturbations. In this paper, we leverage generative models to identify and characterize instances where classifiers fail to generalize.\nWe propose a generative model of the features extracted by a classifier, and show using rigorous hypothesis testing that errors tend to occur when features are assigned low-probability by our model. From this observation, we develop a detection criteria that we test against different sources of classification mistakes: mistakes made on the test set due to poor model generalization, adversarial samples and out-of-distribution samples. Our approach is agnostic to class labels from the training set which makes it applicable to models trained in a semi-supervised way.", "one-sentence_summary": "We show a new characterization of classification mistakes using a generative model learned on the feature space", "pdf": "/pdf/1b1096a18581d8a92cdcbe8e9011b9513d90fa7c.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shao|understanding_classifiers_with_generative_models", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=oLWOrRuL19", "_bibtex": "@misc{\nshao2021understanding,\ntitle={Understanding Classifiers with Generative Models},\nauthor={La{\\\"e}titia Shao and Yang Song and Stefano Ermon},\nyear={2021},\nurl={https://openreview.net/forum?id=0EJjoRbFEcX}\n}"}, "tags": [], "invitation": {"reply": {"forum": "0EJjoRbFEcX", "replyto": "0EJjoRbFEcX", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040536268, "tmdate": 1610474146292, "id": "ICLR.cc/2021/Conference/Paper477/-/Decision"}}}, {"id": "AS8JeVZV5kD", "original": null, "number": 2, "cdate": 1603821475463, "ddate": null, "tcdate": 1603821475463, "tmdate": 1606774071705, "tddate": null, "forum": "0EJjoRbFEcX", "replyto": "0EJjoRbFEcX", "invitation": "ICLR.cc/2021/Conference/Paper477/-/Official_Review", "content": {"title": "official review", "review": "**Update after rebuttal:** The author rebuttal clarified some minor issues for me, but it did nothing to address my main concern, which is that very similar methods have been proposed before. I'm therefore keeping my score the same.  \n\n---------------------------------------------\nThis paper proposes a simple method for out-of-distribution detection. The basic idea is to fit a GMM to training examples in the feature space (as opposed to the pixel space). The experiments are generally rigorous and well executed, however my main problem with the paper is that it seems a bit too incremental to justify another paper. As the authors correctly point out very similar methods have already been proposed before (Zheng & Hong, 2018; Lee et al., 2018). \n\nOn page 2, the authors claim \u201cZheng & Hong (2018) and Lee et al. (2018) train a conditional generative model on the feature space learned by the classifier and derive a confidence score based on the Mahalanobis distance between a test sample and its predicted class representation\u201d. However, this is not correct, Zheng and Hong (2018) don\u2019t use the Mahalanobis metric. They use the exact same likelihood-based criterion in this paper, except they do this on a class-conditional basis as the authors correctly point out, but that seems like a small difference (as labels will obviously be available for the training data in a supervised setting, and the method can be easily adapted to the semi-supervised setting with some tweaks). I just don't see how this small difference could have a huge effect on performance.\n\nThe authors do have some comparisons with these earlier methods, but I\u2019m wondering if these comparisons are done fairly: for example Zheng and Hong (2018) have a similar K parameter in their GMM model, what is the value used for that parameter in this paper? Is it the same as the one used in the class-agnostic GMMs? Have you tried tuning that parameter for these earlier models? How was the threshold parameter chosen? As far as I can see, these important experimental details are not discussed at all anywhere in the paper. Similarly, the semi-supervised setting in Figure 5c is not explained at all. How exactly does it work for the different models shown in that figure? Also, why does the Mahalanobis metric seem to work much better than the GMM in Figure 5b?\n\nAnother question I have is whether the authors have tried using features other than the final embedding layer features (and possibly a combination of features from multiple layers: something like this was done before in the deep k-nn paper by Papernot & McDaniel: https://arxiv.org/abs/1803.04765).\n\nMore minor comments:\n\nPage 6: please make sure you mention what BPD means (binned probability distribution), I don\u2019t think this is as commonly known as CDF or PDF.\n\nTypos: \u201cAims at modeling confidence score that are\u201d (p. 2), \u201ctask of detection out of distribution samples\u201d (p. 7).\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper477/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper477/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Understanding Classifiers with Generative Models", "authorids": ["~La\u00ebtitia_Shao1", "~Yang_Song1", "~Stefano_Ermon1"], "authors": ["La\u00ebtitia Shao", "Yang Song", "Stefano Ermon"], "keywords": ["OOD detection", "adversarial samples detection", "deep learning", "classification"], "abstract": "Although deep neural networks are effective on supervised learning tasks, they have been shown to be brittle. They are prone to overfitting on their training distribution and are easily fooled by small adversarial perturbations. In this paper, we leverage generative models to identify and characterize instances where classifiers fail to generalize.\nWe propose a generative model of the features extracted by a classifier, and show using rigorous hypothesis testing that errors tend to occur when features are assigned low-probability by our model. From this observation, we develop a detection criteria that we test against different sources of classification mistakes: mistakes made on the test set due to poor model generalization, adversarial samples and out-of-distribution samples. Our approach is agnostic to class labels from the training set which makes it applicable to models trained in a semi-supervised way.", "one-sentence_summary": "We show a new characterization of classification mistakes using a generative model learned on the feature space", "pdf": "/pdf/1b1096a18581d8a92cdcbe8e9011b9513d90fa7c.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shao|understanding_classifiers_with_generative_models", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=oLWOrRuL19", "_bibtex": "@misc{\nshao2021understanding,\ntitle={Understanding Classifiers with Generative Models},\nauthor={La{\\\"e}titia Shao and Yang Song and Stefano Ermon},\nyear={2021},\nurl={https://openreview.net/forum?id=0EJjoRbFEcX}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "0EJjoRbFEcX", "replyto": "0EJjoRbFEcX", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper477/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538142262, "tmdate": 1606915810118, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper477/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper477/-/Official_Review"}}}, {"id": "hBxR1dbJ9Vj", "original": null, "number": 4, "cdate": 1606188700096, "ddate": null, "tcdate": 1606188700096, "tmdate": 1606188713149, "tddate": null, "forum": "0EJjoRbFEcX", "replyto": "WXB4wds4BmE", "invitation": "ICLR.cc/2021/Conference/Paper477/-/Official_Comment", "content": {"title": "Rebuttal", "comment": "Thank you for taking the time to review our paper. \nWe'd like to address a particular point of your review: \n\nRemark: The authors claim several times that they perform \"rigorous hypothesis testing\" (including the abstract and conclusions so not a minor point in the paper) but it doesn't really mean that the mistakes can be identified as one can understand from the text. It just means that the distributions are distinct (but can have large overlap as can be seen in fig.2)\n\nAnswer: Rigorous Hypothesis Testing refers to the statistical testing we describe in the paper. We do not claim that  mistakes can be identified from the hypothesis testing but merely that the statistical testing shows the distributions are distinct. This is not a trivial observation as we show in the paper that the distributions learned by a PixelCNN trained on the input space are not distinct. The statistical test gives us a measure of how distinct the distributions are. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper477/Authors"], "readers": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper477/Authors", "everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper477/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Understanding Classifiers with Generative Models", "authorids": ["~La\u00ebtitia_Shao1", "~Yang_Song1", "~Stefano_Ermon1"], "authors": ["La\u00ebtitia Shao", "Yang Song", "Stefano Ermon"], "keywords": ["OOD detection", "adversarial samples detection", "deep learning", "classification"], "abstract": "Although deep neural networks are effective on supervised learning tasks, they have been shown to be brittle. They are prone to overfitting on their training distribution and are easily fooled by small adversarial perturbations. In this paper, we leverage generative models to identify and characterize instances where classifiers fail to generalize.\nWe propose a generative model of the features extracted by a classifier, and show using rigorous hypothesis testing that errors tend to occur when features are assigned low-probability by our model. From this observation, we develop a detection criteria that we test against different sources of classification mistakes: mistakes made on the test set due to poor model generalization, adversarial samples and out-of-distribution samples. Our approach is agnostic to class labels from the training set which makes it applicable to models trained in a semi-supervised way.", "one-sentence_summary": "We show a new characterization of classification mistakes using a generative model learned on the feature space", "pdf": "/pdf/1b1096a18581d8a92cdcbe8e9011b9513d90fa7c.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shao|understanding_classifiers_with_generative_models", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=oLWOrRuL19", "_bibtex": "@misc{\nshao2021understanding,\ntitle={Understanding Classifiers with Generative Models},\nauthor={La{\\\"e}titia Shao and Yang Song and Stefano Ermon},\nyear={2021},\nurl={https://openreview.net/forum?id=0EJjoRbFEcX}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "0EJjoRbFEcX", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper477/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper477/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper477/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper477/Authors|ICLR.cc/2021/Conference/Paper477/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper477/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923870557, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper477/-/Official_Comment"}}}, {"id": "n1fnFvv_Kom", "original": null, "number": 3, "cdate": 1606188641145, "ddate": null, "tcdate": 1606188641145, "tmdate": 1606188641145, "tddate": null, "forum": "0EJjoRbFEcX", "replyto": "AS8JeVZV5kD", "invitation": "ICLR.cc/2021/Conference/Paper477/-/Official_Comment", "content": {"title": "Rebuttal", "comment": "Thank you for your review. Below are questions we would like to address:\n\n* Question: The authors do have some comparisons with these earlier methods, but I\u2019m wondering if these comparisons are done fairly: for example Zheng and Hong (2018) have a similar K parameter in their GMM model, what is the value used for that parameter in this paper? Is it the same as the one used in the class-agnostic GMMs?\n\n* Answer: To make the comparison fair between Zheng and our method, we used a **GMM-1000 for our method and a GMM-10 per class ** for Zheng\u2019s method so that we have the same number of parameters for each method.\n\n* Question: How was the threshold parameter chosen? As far as I can see, these important experimental details are not discussed at all anywhere in the paper.\n\n* Answer: We do not choose the threshold parameter explicitly in the experiments, instead reporting the ROC and PR curves.\n\n* Question: Similarly, the semi-supervised setting in Figure 5c is not explained at all. How exactly does it work for the different models shown in that figure?\n\n* Answer: The Temporal Ensembling setting is explained in the first paragraph of section 4 on page 5. We explain how many samples per labels we keep and cite the paper from which the Temporal Ensembling is implemented.\n\n* Page 6: please make sure you mention what BPD means (binned probability distribution), I don\u2019t think this is as commonly known as CDF or PDF.\n\n* Answer: Apologies for the confusion. BPD actually stands for Bits per Dimension and is a scaled version of log-likelihood that takes into account the size of features. This will be corrected in the final version. (BPD will not be mentioned and we will use log-likelihood instead)\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper477/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper477/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Understanding Classifiers with Generative Models", "authorids": ["~La\u00ebtitia_Shao1", "~Yang_Song1", "~Stefano_Ermon1"], "authors": ["La\u00ebtitia Shao", "Yang Song", "Stefano Ermon"], "keywords": ["OOD detection", "adversarial samples detection", "deep learning", "classification"], "abstract": "Although deep neural networks are effective on supervised learning tasks, they have been shown to be brittle. They are prone to overfitting on their training distribution and are easily fooled by small adversarial perturbations. In this paper, we leverage generative models to identify and characterize instances where classifiers fail to generalize.\nWe propose a generative model of the features extracted by a classifier, and show using rigorous hypothesis testing that errors tend to occur when features are assigned low-probability by our model. From this observation, we develop a detection criteria that we test against different sources of classification mistakes: mistakes made on the test set due to poor model generalization, adversarial samples and out-of-distribution samples. Our approach is agnostic to class labels from the training set which makes it applicable to models trained in a semi-supervised way.", "one-sentence_summary": "We show a new characterization of classification mistakes using a generative model learned on the feature space", "pdf": "/pdf/1b1096a18581d8a92cdcbe8e9011b9513d90fa7c.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shao|understanding_classifiers_with_generative_models", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=oLWOrRuL19", "_bibtex": "@misc{\nshao2021understanding,\ntitle={Understanding Classifiers with Generative Models},\nauthor={La{\\\"e}titia Shao and Yang Song and Stefano Ermon},\nyear={2021},\nurl={https://openreview.net/forum?id=0EJjoRbFEcX}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "0EJjoRbFEcX", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper477/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper477/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper477/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper477/Authors|ICLR.cc/2021/Conference/Paper477/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper477/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923870557, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper477/-/Official_Comment"}}}, {"id": "DoT5dwsL-Wv", "original": null, "number": 2, "cdate": 1606188534987, "ddate": null, "tcdate": 1606188534987, "tmdate": 1606188534987, "tddate": null, "forum": "0EJjoRbFEcX", "replyto": "9aWUUo2nKJO", "invitation": "ICLR.cc/2021/Conference/Paper477/-/Official_Comment", "content": {"title": "Rebuttal", "comment": "Thank you for your feedback. Below are a couple of specific points of the review we would like to address. \n\n* Remark: p.1, line 4 from bottom, \"... capture the distribution of the learned feature space\" -> \"capture the class-conditional distributions in the learned feature space\"\n\n* Answer: Please note the the distribution we learn with the GMM are not class-conditional since we\u2019re not using class labels to train the GMM on the feature space.\n\n\n* Remark: p.4, line 5, \"... state-of-the-art models assigning higher likelihoods to samples ... \" -> What kind of models does this refer to? What is the model supposed to do? Why do they assign higher likelihoods to out-of-distribution samples? There is a lot to be filled in here; citing an external reference is not enough.\n\n* Answer: We refer here to generative models that learn the probability distribution of their training dataset. (For example a PixelCNN trained on CIFAR10). Given an input sample, the model yields an estimate of the sample\u2019s log-likelihood under the data distribution. Out-of-distribution samples that differ from the training data should be predicted to have lower log-likelihood. \n\n* Remark: Question: p.6, line 7 from bottom, \"... assigned higher BPDs ...\", please expand the acronym BPD at its first mention.\n\n* Answer: Apologies for the confusion. BPD refers to \u201cBit-per-dimension\u201d, a scaled measure of log-likelihood that takes into account the size of the feature. This should be replaced by \u201cassigned lower log-likelihood\u201d. \n\n* Remark: p.8, conclusion, \"... verified that features extracted from inputs consistently lie outside of the training distribution and can be detected by their low predicted log-probability.\" This sentence is garbled. What have you verified? Do you mean to say this for a special type of input? What consistently lie outside of what?\n\n* Answer: The sentence should be corrected to \u201cverified that features extracted from inputs **leading to classification mistakes**, consistently lie outside of the training distribution and can be detected by their low predicted log-probability.\u201d"}, "signatures": ["ICLR.cc/2021/Conference/Paper477/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper477/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Understanding Classifiers with Generative Models", "authorids": ["~La\u00ebtitia_Shao1", "~Yang_Song1", "~Stefano_Ermon1"], "authors": ["La\u00ebtitia Shao", "Yang Song", "Stefano Ermon"], "keywords": ["OOD detection", "adversarial samples detection", "deep learning", "classification"], "abstract": "Although deep neural networks are effective on supervised learning tasks, they have been shown to be brittle. They are prone to overfitting on their training distribution and are easily fooled by small adversarial perturbations. In this paper, we leverage generative models to identify and characterize instances where classifiers fail to generalize.\nWe propose a generative model of the features extracted by a classifier, and show using rigorous hypothesis testing that errors tend to occur when features are assigned low-probability by our model. From this observation, we develop a detection criteria that we test against different sources of classification mistakes: mistakes made on the test set due to poor model generalization, adversarial samples and out-of-distribution samples. Our approach is agnostic to class labels from the training set which makes it applicable to models trained in a semi-supervised way.", "one-sentence_summary": "We show a new characterization of classification mistakes using a generative model learned on the feature space", "pdf": "/pdf/1b1096a18581d8a92cdcbe8e9011b9513d90fa7c.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shao|understanding_classifiers_with_generative_models", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=oLWOrRuL19", "_bibtex": "@misc{\nshao2021understanding,\ntitle={Understanding Classifiers with Generative Models},\nauthor={La{\\\"e}titia Shao and Yang Song and Stefano Ermon},\nyear={2021},\nurl={https://openreview.net/forum?id=0EJjoRbFEcX}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "0EJjoRbFEcX", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper477/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper477/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper477/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper477/Authors|ICLR.cc/2021/Conference/Paper477/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper477/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923870557, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper477/-/Official_Comment"}}}, {"id": "WXB4wds4BmE", "original": null, "number": 1, "cdate": 1603284226470, "ddate": null, "tcdate": 1603284226470, "tmdate": 1605024680190, "tddate": null, "forum": "0EJjoRbFEcX", "replyto": "0EJjoRbFEcX", "invitation": "ICLR.cc/2021/Conference/Paper477/-/Official_Review", "content": {"title": "Interesting observation but not enough", "review": "The main novel contribution is that the authors use generative models for several tasks but on the feature space and not the input pixel space. They show interesting behavior that mistakes tend to have low density in feature space, which isn't obvious as the NN that computes the features can map them wrongly to a high likelihood location.\nHowever, the authors claims of usefulness is not properly demonstrated in the experiences.\n\n- The authors claim several times that they perform \"rigorous hypothesis testing\" (including the abstract and conclusions so not a minor point in the paper) but it doesn't really mean that the mistakes can be identified as one can understand from the text. It just means that the distributions are distinct (but can have large overlap as can be seen in fig.2)\n- While GMM on feature space are better then other models on feature space and pixel space, it isn't convincing that the error detection can be of any use. \n- Adversarial detection was only tested on adversaries that do not try to fool the detector. As was shown in \"Adversarial Examples Are Not Easily Detected: Bypassing Ten Detection Methods\" by Carlini&Wagner this can lead to wrong detection claims.\n ", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper477/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper477/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Understanding Classifiers with Generative Models", "authorids": ["~La\u00ebtitia_Shao1", "~Yang_Song1", "~Stefano_Ermon1"], "authors": ["La\u00ebtitia Shao", "Yang Song", "Stefano Ermon"], "keywords": ["OOD detection", "adversarial samples detection", "deep learning", "classification"], "abstract": "Although deep neural networks are effective on supervised learning tasks, they have been shown to be brittle. They are prone to overfitting on their training distribution and are easily fooled by small adversarial perturbations. In this paper, we leverage generative models to identify and characterize instances where classifiers fail to generalize.\nWe propose a generative model of the features extracted by a classifier, and show using rigorous hypothesis testing that errors tend to occur when features are assigned low-probability by our model. From this observation, we develop a detection criteria that we test against different sources of classification mistakes: mistakes made on the test set due to poor model generalization, adversarial samples and out-of-distribution samples. Our approach is agnostic to class labels from the training set which makes it applicable to models trained in a semi-supervised way.", "one-sentence_summary": "We show a new characterization of classification mistakes using a generative model learned on the feature space", "pdf": "/pdf/1b1096a18581d8a92cdcbe8e9011b9513d90fa7c.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shao|understanding_classifiers_with_generative_models", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=oLWOrRuL19", "_bibtex": "@misc{\nshao2021understanding,\ntitle={Understanding Classifiers with Generative Models},\nauthor={La{\\\"e}titia Shao and Yang Song and Stefano Ermon},\nyear={2021},\nurl={https://openreview.net/forum?id=0EJjoRbFEcX}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "0EJjoRbFEcX", "replyto": "0EJjoRbFEcX", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper477/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538142262, "tmdate": 1606915810118, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper477/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper477/-/Official_Review"}}}, {"id": "SYUZ5xzOp_L", "original": null, "number": 3, "cdate": 1603908012262, "ddate": null, "tcdate": 1603908012262, "tmdate": 1605024680124, "tddate": null, "forum": "0EJjoRbFEcX", "replyto": "0EJjoRbFEcX", "invitation": "ICLR.cc/2021/Conference/Paper477/-/Official_Review", "content": {"title": "Weak recommendation to accept", "review": "The authors provide a new method for detecting when deep networks are likely to fail and demonstrate through extensive experimentation its accuracy against generalization errors, out of distribution samples and adversarial attacks. The method builds on prior Mahalanobis metric of (Kimin Lee, et al., A unified framework for detecting out-of-distribution and adversarial samples. 2018) in two respects. First the authors use a single GMM fit to the model parameters that is class agnostic rather than a set of GMMs for each class, thus making it suitable for application in semi-supervised datasets. Second, the authors show ability to detect instances from a test set that are likely to cause a misclassification due to a failure to generalize. Surprisingly, the proposed approach performs better in most cases than the prior Mahalanobis approach even though it requires less information (no labels). \n\nPros:\n\n1. Well written and organized paper.\n2. More general and simpler approach than prior art\n3. Extensive empirical results that are competitive with or improved over prior art.\n\nCons:\n\n1. This paper is very similar to and provides only a relatively small incremental improvement over prior art (Lee, et al.)\n2. Like the Mahalanobis method, the proposed hypothesis testing method requires fitting a GMM to an \"incorrect\" distribution. Please make it clear what this distribution is for the experiments. It seems this would require knowledge of the type of attack, or the out-distribution making it non-blind and unrealistic for real-world out-of-distribution or adversarial attack scenarios.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper477/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper477/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Understanding Classifiers with Generative Models", "authorids": ["~La\u00ebtitia_Shao1", "~Yang_Song1", "~Stefano_Ermon1"], "authors": ["La\u00ebtitia Shao", "Yang Song", "Stefano Ermon"], "keywords": ["OOD detection", "adversarial samples detection", "deep learning", "classification"], "abstract": "Although deep neural networks are effective on supervised learning tasks, they have been shown to be brittle. They are prone to overfitting on their training distribution and are easily fooled by small adversarial perturbations. In this paper, we leverage generative models to identify and characterize instances where classifiers fail to generalize.\nWe propose a generative model of the features extracted by a classifier, and show using rigorous hypothesis testing that errors tend to occur when features are assigned low-probability by our model. From this observation, we develop a detection criteria that we test against different sources of classification mistakes: mistakes made on the test set due to poor model generalization, adversarial samples and out-of-distribution samples. Our approach is agnostic to class labels from the training set which makes it applicable to models trained in a semi-supervised way.", "one-sentence_summary": "We show a new characterization of classification mistakes using a generative model learned on the feature space", "pdf": "/pdf/1b1096a18581d8a92cdcbe8e9011b9513d90fa7c.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shao|understanding_classifiers_with_generative_models", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=oLWOrRuL19", "_bibtex": "@misc{\nshao2021understanding,\ntitle={Understanding Classifiers with Generative Models},\nauthor={La{\\\"e}titia Shao and Yang Song and Stefano Ermon},\nyear={2021},\nurl={https://openreview.net/forum?id=0EJjoRbFEcX}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "0EJjoRbFEcX", "replyto": "0EJjoRbFEcX", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper477/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538142262, "tmdate": 1606915810118, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper477/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper477/-/Official_Review"}}}, {"id": "9aWUUo2nKJO", "original": null, "number": 4, "cdate": 1603929925946, "ddate": null, "tcdate": 1603929925946, "tmdate": 1605024680061, "tddate": null, "forum": "0EJjoRbFEcX", "replyto": "0EJjoRbFEcX", "invitation": "ICLR.cc/2021/Conference/Paper477/-/Official_Review", "content": {"title": "Interesting study of learned neural features, with premature conclusion due to experiments on limited data", "review": "Motivated by the need to assess a classifier's confidence on its decision,\nthe paper proposes to use predicted likelihood of the learned features\nof an unknown sample, and shows that samples with low feature\nlikelihood tend to receive a wrong decision.\nFurthermore, the paper argues that using a GMM do model the feature\ndistributions is better than other methods like VAE or AR flow.\n\nThis study is interesting in the sence that it attempts to analyze the\ndistributions of the learned features, which is much needed for better\nunderstanding of what a deep neural net attempts to do.\nThe findings appear to make good sense as the network training process\nis designed to push samples of like-class towards tight clusters when\nmapped to the learned feature space.  Those lying in the outskirts signal\ndifficulties in this process,  therefore they are likely to get\nerroneous decisions.   The feature distributions are modeled\nclass-blind, so that the prediction can be applied to unseen cases\nwithout class labels.\n\nThe work can be improved by performing this analysis at different stages\nof a deep network,  as one expects that the levels closer to the\noutput layer demonstrate more of such clustering effect.  It\nwill be useful to confirm with this analysis.\n\nOther than showing numerical results, it will be more convincing if\nexample images are shown that are identified by this method to\nhave unreliable decisions by the classifier, and what error the classifier\nmakes on them.\n\nA more significant weakness is that the experiments are done only with\nan image classification problem, and a single dataset.  This makes the\nconclusion somewhat premature,  as images of physical objects tend to\nform good patterns.  Is the GMM estimation good just because the\ndata happen to be well clustered?  Will this conclusion be confirmed\nby classification tasks on other types of data, e.g. text?\nWhat if the class labels are scrambled?\nWhat makes some network settings better than others in learning\nsuch confidence-suggestive features?\n\nMisc.:\n\np.1, line 4 from bottom, \n\"... capture the distribution of the learned feature space\" ->\n\"capture the class-conditional distributions in the learned feature space\"\n\np.4, line 5,\n\"... state-of-the-art models assigning higher likelihoods to samples\n... \" ->\nWhat kind of models does this refer to?  What is the model supposed to\ndo?  Why do they assign higher likelihoods to out-of-distribution samples?\nThere is a lot to be filled in here; citing an external reference is\nnot enough.\n\np.6, line 7 from bottom,\n\"... assigned higher BPDs ...\", please expand the acronym BPD at its\nfirst mention.\n\np.8, conclusion,\n\"... verified that features extracted from inputs consistently lie\noutside of the training distribution and can be detected by their low\npredicted log-probability.\"   This sentence is garbled.  What have\nyou verified?  Do you mean to say this for a special type of input?\nWhat consistently lie outside of what?\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper477/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper477/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Understanding Classifiers with Generative Models", "authorids": ["~La\u00ebtitia_Shao1", "~Yang_Song1", "~Stefano_Ermon1"], "authors": ["La\u00ebtitia Shao", "Yang Song", "Stefano Ermon"], "keywords": ["OOD detection", "adversarial samples detection", "deep learning", "classification"], "abstract": "Although deep neural networks are effective on supervised learning tasks, they have been shown to be brittle. They are prone to overfitting on their training distribution and are easily fooled by small adversarial perturbations. In this paper, we leverage generative models to identify and characterize instances where classifiers fail to generalize.\nWe propose a generative model of the features extracted by a classifier, and show using rigorous hypothesis testing that errors tend to occur when features are assigned low-probability by our model. From this observation, we develop a detection criteria that we test against different sources of classification mistakes: mistakes made on the test set due to poor model generalization, adversarial samples and out-of-distribution samples. Our approach is agnostic to class labels from the training set which makes it applicable to models trained in a semi-supervised way.", "one-sentence_summary": "We show a new characterization of classification mistakes using a generative model learned on the feature space", "pdf": "/pdf/1b1096a18581d8a92cdcbe8e9011b9513d90fa7c.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shao|understanding_classifiers_with_generative_models", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=oLWOrRuL19", "_bibtex": "@misc{\nshao2021understanding,\ntitle={Understanding Classifiers with Generative Models},\nauthor={La{\\\"e}titia Shao and Yang Song and Stefano Ermon},\nyear={2021},\nurl={https://openreview.net/forum?id=0EJjoRbFEcX}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "0EJjoRbFEcX", "replyto": "0EJjoRbFEcX", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper477/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538142262, "tmdate": 1606915810118, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper477/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper477/-/Official_Review"}}}], "count": 9}