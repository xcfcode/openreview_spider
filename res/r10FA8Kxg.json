{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1488587272192, "tcdate": 1478221446026, "number": 102, "id": "r10FA8Kxg", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "r10FA8Kxg", "signatures": ["~Gregor_Urban1"], "readers": ["everyone"], "content": {"title": "Do Deep Convolutional Nets Really Need to be Deep and Convolutional?", "abstract": "Yes, they do.  This paper provides the first empirical demonstration that deep convolutional models really need to be both deep and convolutional, even when trained with methods such as distillation that allow small or shallow models of high accuracy to be trained.  Although previous research showed that shallow feed-forward nets sometimes can learn the complex functions previously learned by deep nets while using the same number of parameters as the deep models they mimic, in this paper we demonstrate that the same methods cannot be used to train accurate models on CIFAR-10 unless the student models contain multiple layers of convolution.  Although the student models do not have to be as deep as the teacher model they mimic, the students need multiple convolutional layers to learn functions of comparable accuracy as the deep convolutional teacher.  ", "pdf": "/pdf/8d987f288955ca4836e35e40ffe710638a00e1ef.pdf", "TL;DR": "This paper provides the first empirical demonstration that deep convolutional models really need to be both deep and convolutional, even when trained with model distillation and heavy hyperparameter optimization.", "paperhash": "urban|do_deep_convolutional_nets_really_need_to_be_deep_and_convolutional", "keywords": ["Deep learning", "Transfer Learning"], "conflicts": ["uci.edu", "sms.ed.ac.uk", "polymtl.ca", "cs.ualberta.ca", "cs.washington.edu", "microsoft.com", "uni-heidelberg.de", "mpimf-heidelberg.mpg.de"], "authors": ["Gregor Urban", "Krzysztof J. Geras", "Samira Ebrahimi Kahou", "Ozlem Aslan", "Shengjie Wang", "Abdelrahman Mohamed", "Matthai Philipose", "Matt Richardson", "Rich Caruana"], "authorids": ["gurban@uci.edu", "k.j.geras@sms.ed.ac.uk", "samira.ebrahimi-kahou@polymtl.ca", "ozlem@cs.ualberta.ca", "wangsj@cs.washington.edu", "asamir@microsoft.com", "matthaip@microsoft.com", "mattri@microsoft.com", "rcaruana@microsoft.com"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396357583, "tcdate": 1486396357583, "number": 1, "id": "S10TiGI_x", "invitation": "ICLR.cc/2017/conference/-/paper102/acceptance", "forum": "r10FA8Kxg", "replyto": "r10FA8Kxg", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"title": "ICLR committee final decision", "comment": "The reviewers unanimously recommend accepting this paper.", "decision": "Accept (Poster)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Do Deep Convolutional Nets Really Need to be Deep and Convolutional?", "abstract": "Yes, they do.  This paper provides the first empirical demonstration that deep convolutional models really need to be both deep and convolutional, even when trained with methods such as distillation that allow small or shallow models of high accuracy to be trained.  Although previous research showed that shallow feed-forward nets sometimes can learn the complex functions previously learned by deep nets while using the same number of parameters as the deep models they mimic, in this paper we demonstrate that the same methods cannot be used to train accurate models on CIFAR-10 unless the student models contain multiple layers of convolution.  Although the student models do not have to be as deep as the teacher model they mimic, the students need multiple convolutional layers to learn functions of comparable accuracy as the deep convolutional teacher.  ", "pdf": "/pdf/8d987f288955ca4836e35e40ffe710638a00e1ef.pdf", "TL;DR": "This paper provides the first empirical demonstration that deep convolutional models really need to be both deep and convolutional, even when trained with model distillation and heavy hyperparameter optimization.", "paperhash": "urban|do_deep_convolutional_nets_really_need_to_be_deep_and_convolutional", "keywords": ["Deep learning", "Transfer Learning"], "conflicts": ["uci.edu", "sms.ed.ac.uk", "polymtl.ca", "cs.ualberta.ca", "cs.washington.edu", "microsoft.com", "uni-heidelberg.de", "mpimf-heidelberg.mpg.de"], "authors": ["Gregor Urban", "Krzysztof J. Geras", "Samira Ebrahimi Kahou", "Ozlem Aslan", "Shengjie Wang", "Abdelrahman Mohamed", "Matthai Philipose", "Matt Richardson", "Rich Caruana"], "authorids": ["gurban@uci.edu", "k.j.geras@sms.ed.ac.uk", "samira.ebrahimi-kahou@polymtl.ca", "ozlem@cs.ualberta.ca", "wangsj@cs.washington.edu", "asamir@microsoft.com", "matthaip@microsoft.com", "mattri@microsoft.com", "rcaruana@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396358116, "id": "ICLR.cc/2017/conference/-/paper102/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "r10FA8Kxg", "replyto": "r10FA8Kxg", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396358116}}}, {"tddate": null, "tmdate": 1484382022695, "tcdate": 1484358919424, "number": 5, "id": "B11zrbw8e", "invitation": "ICLR.cc/2017/conference/-/paper102/public/comment", "forum": "r10FA8Kxg", "replyto": "rkQMjyy4l", "signatures": ["~Gregor_Urban1"], "readers": ["everyone"], "writers": ["~Gregor_Urban1"], "content": {"title": "Re: Regularization effect of distillation", "comment": "We added a discussion about regularization to the discussion section of the paper.  Thanks for the suggestion."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Do Deep Convolutional Nets Really Need to be Deep and Convolutional?", "abstract": "Yes, they do.  This paper provides the first empirical demonstration that deep convolutional models really need to be both deep and convolutional, even when trained with methods such as distillation that allow small or shallow models of high accuracy to be trained.  Although previous research showed that shallow feed-forward nets sometimes can learn the complex functions previously learned by deep nets while using the same number of parameters as the deep models they mimic, in this paper we demonstrate that the same methods cannot be used to train accurate models on CIFAR-10 unless the student models contain multiple layers of convolution.  Although the student models do not have to be as deep as the teacher model they mimic, the students need multiple convolutional layers to learn functions of comparable accuracy as the deep convolutional teacher.  ", "pdf": "/pdf/8d987f288955ca4836e35e40ffe710638a00e1ef.pdf", "TL;DR": "This paper provides the first empirical demonstration that deep convolutional models really need to be both deep and convolutional, even when trained with model distillation and heavy hyperparameter optimization.", "paperhash": "urban|do_deep_convolutional_nets_really_need_to_be_deep_and_convolutional", "keywords": ["Deep learning", "Transfer Learning"], "conflicts": ["uci.edu", "sms.ed.ac.uk", "polymtl.ca", "cs.ualberta.ca", "cs.washington.edu", "microsoft.com", "uni-heidelberg.de", "mpimf-heidelberg.mpg.de"], "authors": ["Gregor Urban", "Krzysztof J. Geras", "Samira Ebrahimi Kahou", "Ozlem Aslan", "Shengjie Wang", "Abdelrahman Mohamed", "Matthai Philipose", "Matt Richardson", "Rich Caruana"], "authorids": ["gurban@uci.edu", "k.j.geras@sms.ed.ac.uk", "samira.ebrahimi-kahou@polymtl.ca", "ozlem@cs.ualberta.ca", "wangsj@cs.washington.edu", "asamir@microsoft.com", "matthaip@microsoft.com", "mattri@microsoft.com", "rcaruana@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287727622, "id": "ICLR.cc/2017/conference/-/paper102/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "r10FA8Kxg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper102/reviewers", "ICLR.cc/2017/conference/paper102/areachairs"], "cdate": 1485287727622}}}, {"tddate": null, "tmdate": 1484359040369, "tcdate": 1484359040369, "number": 6, "id": "rJuYrWPIx", "invitation": "ICLR.cc/2017/conference/-/paper102/public/comment", "forum": "r10FA8Kxg", "replyto": "BkaSqlzEe", "signatures": ["~Gregor_Urban1"], "readers": ["everyone"], "writers": ["~Gregor_Urban1"], "content": {"title": "Re: other data sets such as ImageNet", "comment": "We are currently running experiments on ImageNet.  These experiments are very expensive compared to those on CIFAR-10.  So far the intermediate results do not suggest that the story will be qualitatively different than it was for CIFAR-10, but we agree that the ImageNet results will make the paper that much stronger. Stay tuned....  The reason why we believe shallow nets will perform relatively worse on ImageNet is because ImageNet favors deeper models than CIFAR-10, and we suspect this extra depth may be needed for 1000 classes.  But we're running a thorough set of experiments so that we don't have to guess."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Do Deep Convolutional Nets Really Need to be Deep and Convolutional?", "abstract": "Yes, they do.  This paper provides the first empirical demonstration that deep convolutional models really need to be both deep and convolutional, even when trained with methods such as distillation that allow small or shallow models of high accuracy to be trained.  Although previous research showed that shallow feed-forward nets sometimes can learn the complex functions previously learned by deep nets while using the same number of parameters as the deep models they mimic, in this paper we demonstrate that the same methods cannot be used to train accurate models on CIFAR-10 unless the student models contain multiple layers of convolution.  Although the student models do not have to be as deep as the teacher model they mimic, the students need multiple convolutional layers to learn functions of comparable accuracy as the deep convolutional teacher.  ", "pdf": "/pdf/8d987f288955ca4836e35e40ffe710638a00e1ef.pdf", "TL;DR": "This paper provides the first empirical demonstration that deep convolutional models really need to be both deep and convolutional, even when trained with model distillation and heavy hyperparameter optimization.", "paperhash": "urban|do_deep_convolutional_nets_really_need_to_be_deep_and_convolutional", "keywords": ["Deep learning", "Transfer Learning"], "conflicts": ["uci.edu", "sms.ed.ac.uk", "polymtl.ca", "cs.ualberta.ca", "cs.washington.edu", "microsoft.com", "uni-heidelberg.de", "mpimf-heidelberg.mpg.de"], "authors": ["Gregor Urban", "Krzysztof J. Geras", "Samira Ebrahimi Kahou", "Ozlem Aslan", "Shengjie Wang", "Abdelrahman Mohamed", "Matthai Philipose", "Matt Richardson", "Rich Caruana"], "authorids": ["gurban@uci.edu", "k.j.geras@sms.ed.ac.uk", "samira.ebrahimi-kahou@polymtl.ca", "ozlem@cs.ualberta.ca", "wangsj@cs.washington.edu", "asamir@microsoft.com", "matthaip@microsoft.com", "mattri@microsoft.com", "rcaruana@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287727622, "id": "ICLR.cc/2017/conference/-/paper102/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "r10FA8Kxg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper102/reviewers", "ICLR.cc/2017/conference/paper102/areachairs"], "cdate": 1485287727622}}}, {"tddate": null, "tmdate": 1484358734611, "tcdate": 1481836455940, "number": 2, "id": "BJg2PFx4g", "invitation": "ICLR.cc/2017/conference/-/paper102/public/comment", "forum": "r10FA8Kxg", "replyto": "S1xwYXsXg", "signatures": ["~Gregor_Urban1"], "readers": ["everyone"], "writers": ["~Gregor_Urban1"], "content": {"title": "Re: deep, but not too deep ?", "comment": "One hypothesis is that the problem with the deeper MLPs is that they may not have sufficient capacity per layer.  Figure 1 compares models with the same number of parameters.  Because of this, deeper nets have fewer nodes per layer, i.e., the deeper models are narrower.  We are running experiments to train 4- and 5-layer MLPs that use the same number of hidden units per layer as the best 2- or 3-layer MLPs.  If that works as well or better than the shallower MLPs, then it's a question of capacity per layer. An alternate hypothesis, is that optimization is having trouble training deeper MLPs, or possibly that the deeper MLPs overfit more easily.  After the experiments finish we\u2019ll add a discussion to the paper of the relative accuracy on the train and test sets to determine why the deeper MLPs have less accuracy."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Do Deep Convolutional Nets Really Need to be Deep and Convolutional?", "abstract": "Yes, they do.  This paper provides the first empirical demonstration that deep convolutional models really need to be both deep and convolutional, even when trained with methods such as distillation that allow small or shallow models of high accuracy to be trained.  Although previous research showed that shallow feed-forward nets sometimes can learn the complex functions previously learned by deep nets while using the same number of parameters as the deep models they mimic, in this paper we demonstrate that the same methods cannot be used to train accurate models on CIFAR-10 unless the student models contain multiple layers of convolution.  Although the student models do not have to be as deep as the teacher model they mimic, the students need multiple convolutional layers to learn functions of comparable accuracy as the deep convolutional teacher.  ", "pdf": "/pdf/8d987f288955ca4836e35e40ffe710638a00e1ef.pdf", "TL;DR": "This paper provides the first empirical demonstration that deep convolutional models really need to be both deep and convolutional, even when trained with model distillation and heavy hyperparameter optimization.", "paperhash": "urban|do_deep_convolutional_nets_really_need_to_be_deep_and_convolutional", "keywords": ["Deep learning", "Transfer Learning"], "conflicts": ["uci.edu", "sms.ed.ac.uk", "polymtl.ca", "cs.ualberta.ca", "cs.washington.edu", "microsoft.com", "uni-heidelberg.de", "mpimf-heidelberg.mpg.de"], "authors": ["Gregor Urban", "Krzysztof J. Geras", "Samira Ebrahimi Kahou", "Ozlem Aslan", "Shengjie Wang", "Abdelrahman Mohamed", "Matthai Philipose", "Matt Richardson", "Rich Caruana"], "authorids": ["gurban@uci.edu", "k.j.geras@sms.ed.ac.uk", "samira.ebrahimi-kahou@polymtl.ca", "ozlem@cs.ualberta.ca", "wangsj@cs.washington.edu", "asamir@microsoft.com", "matthaip@microsoft.com", "mattri@microsoft.com", "rcaruana@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287727622, "id": "ICLR.cc/2017/conference/-/paper102/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "r10FA8Kxg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper102/reviewers", "ICLR.cc/2017/conference/paper102/areachairs"], "cdate": 1485287727622}}}, {"tddate": null, "tmdate": 1484358691367, "tcdate": 1484358691367, "number": 4, "id": "BJiX4ZDIx", "invitation": "ICLR.cc/2017/conference/-/paper102/public/comment", "forum": "r10FA8Kxg", "replyto": "HkuWJgkNg", "signatures": ["~Gregor_Urban1"], "readers": ["everyone"], "writers": ["~Gregor_Urban1"], "content": {"title": "Many small fixes", "comment": "Thank you for the nice review, and for catching many of our typos.  We made all of the changes you suggested, including fixing the citation format."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Do Deep Convolutional Nets Really Need to be Deep and Convolutional?", "abstract": "Yes, they do.  This paper provides the first empirical demonstration that deep convolutional models really need to be both deep and convolutional, even when trained with methods such as distillation that allow small or shallow models of high accuracy to be trained.  Although previous research showed that shallow feed-forward nets sometimes can learn the complex functions previously learned by deep nets while using the same number of parameters as the deep models they mimic, in this paper we demonstrate that the same methods cannot be used to train accurate models on CIFAR-10 unless the student models contain multiple layers of convolution.  Although the student models do not have to be as deep as the teacher model they mimic, the students need multiple convolutional layers to learn functions of comparable accuracy as the deep convolutional teacher.  ", "pdf": "/pdf/8d987f288955ca4836e35e40ffe710638a00e1ef.pdf", "TL;DR": "This paper provides the first empirical demonstration that deep convolutional models really need to be both deep and convolutional, even when trained with model distillation and heavy hyperparameter optimization.", "paperhash": "urban|do_deep_convolutional_nets_really_need_to_be_deep_and_convolutional", "keywords": ["Deep learning", "Transfer Learning"], "conflicts": ["uci.edu", "sms.ed.ac.uk", "polymtl.ca", "cs.ualberta.ca", "cs.washington.edu", "microsoft.com", "uni-heidelberg.de", "mpimf-heidelberg.mpg.de"], "authors": ["Gregor Urban", "Krzysztof J. Geras", "Samira Ebrahimi Kahou", "Ozlem Aslan", "Shengjie Wang", "Abdelrahman Mohamed", "Matthai Philipose", "Matt Richardson", "Rich Caruana"], "authorids": ["gurban@uci.edu", "k.j.geras@sms.ed.ac.uk", "samira.ebrahimi-kahou@polymtl.ca", "ozlem@cs.ualberta.ca", "wangsj@cs.washington.edu", "asamir@microsoft.com", "matthaip@microsoft.com", "mattri@microsoft.com", "rcaruana@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287727622, "id": "ICLR.cc/2017/conference/-/paper102/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "r10FA8Kxg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper102/reviewers", "ICLR.cc/2017/conference/paper102/areachairs"], "cdate": 1485287727622}}}, {"tddate": null, "tmdate": 1484358559733, "tcdate": 1484358559733, "number": 3, "id": "Hy_imWw8e", "invitation": "ICLR.cc/2017/conference/-/paper102/public/comment", "forum": "r10FA8Kxg", "replyto": "r1w-zAZ4e", "signatures": ["~Gregor_Urban1"], "readers": ["everyone"], "writers": ["~Gregor_Urban1"], "content": {"title": "Re: more parameters & experiments on ImageNet", "comment": "The experiments in Figure 1 stop at 30M parameters only because of the computational cost of Bayesian hyper-parameter optimization with 100M or more parameters.  But there is time for us to extend the graph to 100M parameters for ICLR.  We don't expect any surprises, but we agree with the reviewer that it would be interesting to see the graphs asymptote.  We added a preliminary figure at the end of the appendix that shows the results we have so far with 100M parameters.\n\nWe are currently running experiments on ImageNet.  These experiments are very expensive compared to those on CIFAR-10.  So far the intermediate results do not suggest that the story will be qualitatively different than it was for CIFAR-10, but we agree that the ImageNet results will make the paper that much stronger. Stay tuned....  The reason why we believe shallow nets will perform relatively worse on ImageNet is because ImageNet favors deeper models than CIFAR-10, and we suspect this extra depth may be needed for 1000 classes.  But we're running a thorough set of experiments so that we don't have to guess."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Do Deep Convolutional Nets Really Need to be Deep and Convolutional?", "abstract": "Yes, they do.  This paper provides the first empirical demonstration that deep convolutional models really need to be both deep and convolutional, even when trained with methods such as distillation that allow small or shallow models of high accuracy to be trained.  Although previous research showed that shallow feed-forward nets sometimes can learn the complex functions previously learned by deep nets while using the same number of parameters as the deep models they mimic, in this paper we demonstrate that the same methods cannot be used to train accurate models on CIFAR-10 unless the student models contain multiple layers of convolution.  Although the student models do not have to be as deep as the teacher model they mimic, the students need multiple convolutional layers to learn functions of comparable accuracy as the deep convolutional teacher.  ", "pdf": "/pdf/8d987f288955ca4836e35e40ffe710638a00e1ef.pdf", "TL;DR": "This paper provides the first empirical demonstration that deep convolutional models really need to be both deep and convolutional, even when trained with model distillation and heavy hyperparameter optimization.", "paperhash": "urban|do_deep_convolutional_nets_really_need_to_be_deep_and_convolutional", "keywords": ["Deep learning", "Transfer Learning"], "conflicts": ["uci.edu", "sms.ed.ac.uk", "polymtl.ca", "cs.ualberta.ca", "cs.washington.edu", "microsoft.com", "uni-heidelberg.de", "mpimf-heidelberg.mpg.de"], "authors": ["Gregor Urban", "Krzysztof J. Geras", "Samira Ebrahimi Kahou", "Ozlem Aslan", "Shengjie Wang", "Abdelrahman Mohamed", "Matthai Philipose", "Matt Richardson", "Rich Caruana"], "authorids": ["gurban@uci.edu", "k.j.geras@sms.ed.ac.uk", "samira.ebrahimi-kahou@polymtl.ca", "ozlem@cs.ualberta.ca", "wangsj@cs.washington.edu", "asamir@microsoft.com", "matthaip@microsoft.com", "mattri@microsoft.com", "rcaruana@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287727622, "id": "ICLR.cc/2017/conference/-/paper102/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "r10FA8Kxg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper102/reviewers", "ICLR.cc/2017/conference/paper102/areachairs"], "cdate": 1485287727622}}}, {"tddate": null, "tmdate": 1481931332592, "tcdate": 1481931332592, "number": 3, "id": "BkaSqlzEe", "invitation": "ICLR.cc/2017/conference/-/paper102/official/review", "forum": "r10FA8Kxg", "replyto": "r10FA8Kxg", "signatures": ["ICLR.cc/2017/conference/paper102/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper102/AnonReviewer3"], "content": {"title": "Experimental paper with interesting results. Well written. Solid experiments. ", "rating": "7: Good paper, accept", "review": "Description.\nThis paper describes experiments testing whether deep convolutional networks can be replaced with shallow networks with the same number of parameters without loss of accuracy. The experiments are performed on he CIFAR 10 dataset where deep convolutional teacher networks are used to train shallow student networks using L2 regression on logit outputs.  The results show that similar accuracy on the same parameter budget can be only obtained when multiple layers of convolution are used. \n\nStrong  points.\n- The experiments are carefully done with thorough selection of hyperparameters. \n- The paper shows interesting results that go partially against conclusions from the previous work in this area (Ba and Caruana 2014).\n- The paper is well and clearly written.\n\nWeak points:\n- CIFAR is still somewhat toy dataset with only 10 classes. It would be interesting to see some results on a more challenging problem such as ImageNet. Would the results for a large number of classes be similar?\n\nOriginality:\n- This is mainly an experimental paper, but the question it asks is interesting and worth investigation. The experimental results are solid and provide new insights.\n\nQuality:\n- The experiments are well done.\n\nClarity:\n- The paper is well written and clear.\n\nSignificance:\n- The results go against some of the conclusions from previous work, so should be published and discussed.\n\nOverall:\nExperimental paper with interesting results. Well written. Solid experiments. \n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Do Deep Convolutional Nets Really Need to be Deep and Convolutional?", "abstract": "Yes, they do.  This paper provides the first empirical demonstration that deep convolutional models really need to be both deep and convolutional, even when trained with methods such as distillation that allow small or shallow models of high accuracy to be trained.  Although previous research showed that shallow feed-forward nets sometimes can learn the complex functions previously learned by deep nets while using the same number of parameters as the deep models they mimic, in this paper we demonstrate that the same methods cannot be used to train accurate models on CIFAR-10 unless the student models contain multiple layers of convolution.  Although the student models do not have to be as deep as the teacher model they mimic, the students need multiple convolutional layers to learn functions of comparable accuracy as the deep convolutional teacher.  ", "pdf": "/pdf/8d987f288955ca4836e35e40ffe710638a00e1ef.pdf", "TL;DR": "This paper provides the first empirical demonstration that deep convolutional models really need to be both deep and convolutional, even when trained with model distillation and heavy hyperparameter optimization.", "paperhash": "urban|do_deep_convolutional_nets_really_need_to_be_deep_and_convolutional", "keywords": ["Deep learning", "Transfer Learning"], "conflicts": ["uci.edu", "sms.ed.ac.uk", "polymtl.ca", "cs.ualberta.ca", "cs.washington.edu", "microsoft.com", "uni-heidelberg.de", "mpimf-heidelberg.mpg.de"], "authors": ["Gregor Urban", "Krzysztof J. Geras", "Samira Ebrahimi Kahou", "Ozlem Aslan", "Shengjie Wang", "Abdelrahman Mohamed", "Matthai Philipose", "Matt Richardson", "Rich Caruana"], "authorids": ["gurban@uci.edu", "k.j.geras@sms.ed.ac.uk", "samira.ebrahimi-kahou@polymtl.ca", "ozlem@cs.ualberta.ca", "wangsj@cs.washington.edu", "asamir@microsoft.com", "matthaip@microsoft.com", "mattri@microsoft.com", "rcaruana@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512696399, "id": "ICLR.cc/2017/conference/-/paper102/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper102/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper102/AnonReviewer1", "ICLR.cc/2017/conference/paper102/AnonReviewer4", "ICLR.cc/2017/conference/paper102/AnonReviewer3"], "reply": {"forum": "r10FA8Kxg", "replyto": "r10FA8Kxg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper102/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper102/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512696399}}}, {"tddate": null, "tmdate": 1481921024046, "tcdate": 1481921024046, "number": 2, "id": "r1w-zAZ4e", "invitation": "ICLR.cc/2017/conference/-/paper102/official/review", "forum": "r10FA8Kxg", "replyto": "r10FA8Kxg", "signatures": ["ICLR.cc/2017/conference/paper102/AnonReviewer4"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper102/AnonReviewer4"], "content": {"title": "Experimental comparison of shallow, deep, and (non)-convolutional architectures with a fixed parameter budget", "rating": "7: Good paper, accept", "review": "This paper aims to investigate the question if shallow non-convolutional networks can be as affective as deep convolutional ones for image classification, given that both architectures use the same number of parameters. \nTo this end the authors conducted a series of experiments on the CIFAR10 dataset.\nThey find that there is a significant performance gap between the two approaches, in favour of deep CNNs. \nThe experiments are well designed and involve a distillation training approach, and the results are presented in a comprehensive manner.\nThey also observe (as others have before) that student models can be shallower than the teacher model from which they are trained for comparable performance.\n\nMy take on these results is that they suggest that using (deep) conv nets is more effective, since this model class encodes a form of a-prori or domain knowledge that images exhibit a certain degree of translation invariance in the way they should be processed for high-level recognition tasks. The results are therefore perhaps not quite surprising, but not completely obvious either.\n\nAn interesting point on which the authors comment only very briefly is that among the non-convolutional architectures the ones using 2 or 3 hidden layers outperform those with 1, 4 or 5 hidden layers. Do you have an interpretation / hypothesis of why this is the case? It  would be interesting to discuss the point a bit more in the paper.\n\nIt was not quite clear to me why were the experiments were limited to use  30M parameters at most. None of the experiments in Figure 1 seem to be saturated. Although the performance gap between CNN and MLP is large, I think it would be worthwhile to push the experiment further for the final version of the paper.\n\nThe authors state in the last paragraph that they expect shallow nets to be relatively worse in an ImageNet classification experiment. \nCould the authors argue why they think this to be the case? \nOne could argue that the much larger training dataset size could compensate for shallow and/or non-convolutional choices of the architecture. \nSince MLPs are universal function approximators, one could understand architecture choices as expressions of certain priors over the function space, and in a large-data regimes such priors could be expected to be of lesser importance.\nThis issue could for example be examined on ImageNet when varying the amount of training data.\nAlso, the much higher resolution of ImageNet images might have a non-trivial impact on the CNN-MLP comparison as compared to the results established on the CIFAR10 dataset.\n\nExperiments on a second data set would also help to corroborate the findings, demonstrating to what extent such findings are variable across datasets.\n\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Do Deep Convolutional Nets Really Need to be Deep and Convolutional?", "abstract": "Yes, they do.  This paper provides the first empirical demonstration that deep convolutional models really need to be both deep and convolutional, even when trained with methods such as distillation that allow small or shallow models of high accuracy to be trained.  Although previous research showed that shallow feed-forward nets sometimes can learn the complex functions previously learned by deep nets while using the same number of parameters as the deep models they mimic, in this paper we demonstrate that the same methods cannot be used to train accurate models on CIFAR-10 unless the student models contain multiple layers of convolution.  Although the student models do not have to be as deep as the teacher model they mimic, the students need multiple convolutional layers to learn functions of comparable accuracy as the deep convolutional teacher.  ", "pdf": "/pdf/8d987f288955ca4836e35e40ffe710638a00e1ef.pdf", "TL;DR": "This paper provides the first empirical demonstration that deep convolutional models really need to be both deep and convolutional, even when trained with model distillation and heavy hyperparameter optimization.", "paperhash": "urban|do_deep_convolutional_nets_really_need_to_be_deep_and_convolutional", "keywords": ["Deep learning", "Transfer Learning"], "conflicts": ["uci.edu", "sms.ed.ac.uk", "polymtl.ca", "cs.ualberta.ca", "cs.washington.edu", "microsoft.com", "uni-heidelberg.de", "mpimf-heidelberg.mpg.de"], "authors": ["Gregor Urban", "Krzysztof J. Geras", "Samira Ebrahimi Kahou", "Ozlem Aslan", "Shengjie Wang", "Abdelrahman Mohamed", "Matthai Philipose", "Matt Richardson", "Rich Caruana"], "authorids": ["gurban@uci.edu", "k.j.geras@sms.ed.ac.uk", "samira.ebrahimi-kahou@polymtl.ca", "ozlem@cs.ualberta.ca", "wangsj@cs.washington.edu", "asamir@microsoft.com", "matthaip@microsoft.com", "mattri@microsoft.com", "rcaruana@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512696399, "id": "ICLR.cc/2017/conference/-/paper102/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper102/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper102/AnonReviewer1", "ICLR.cc/2017/conference/paper102/AnonReviewer4", "ICLR.cc/2017/conference/paper102/AnonReviewer3"], "reply": {"forum": "r10FA8Kxg", "replyto": "r10FA8Kxg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper102/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper102/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512696399}}}, {"tddate": null, "tmdate": 1481700357355, "tcdate": 1481700357349, "number": 1, "id": "H1aWVO0ml", "invitation": "ICLR.cc/2017/conference/-/paper102/public/comment", "forum": "r10FA8Kxg", "replyto": "H1NFwMRGx", "signatures": ["~Gregor_Urban1"], "readers": ["everyone"], "writers": ["~Gregor_Urban1"], "content": {"title": "Re: Regularization effect of distillation", "comment": "We agree with you that emphasizing the regularization aspect of distillation and citing the existing literature will strengthen the paper.  We will add a brief discussion about regularization, and will reference to the following papers in that discussion - please let us know of any work that you think is missing from this list:\n\n\"Distilling the Knowledge in a Neural Network\" -- G. Hinton et al.\n\"Fitnets\" -- A. Romero et al.\n\"Distillation as a defense to adversarial perturbations against beep neural networks\" -- N. Papernot et al.\n\"Defensive Distillation is not robust\" -- N. Carlini\n\"Regularizing neural networks by penalizing output distributions\" -- G. Pereyra et al.\n\"Regularizing deep learning ensembles by distillation\" -- A. Mosca et al. (CIMA 2016)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Do Deep Convolutional Nets Really Need to be Deep and Convolutional?", "abstract": "Yes, they do.  This paper provides the first empirical demonstration that deep convolutional models really need to be both deep and convolutional, even when trained with methods such as distillation that allow small or shallow models of high accuracy to be trained.  Although previous research showed that shallow feed-forward nets sometimes can learn the complex functions previously learned by deep nets while using the same number of parameters as the deep models they mimic, in this paper we demonstrate that the same methods cannot be used to train accurate models on CIFAR-10 unless the student models contain multiple layers of convolution.  Although the student models do not have to be as deep as the teacher model they mimic, the students need multiple convolutional layers to learn functions of comparable accuracy as the deep convolutional teacher.  ", "pdf": "/pdf/8d987f288955ca4836e35e40ffe710638a00e1ef.pdf", "TL;DR": "This paper provides the first empirical demonstration that deep convolutional models really need to be both deep and convolutional, even when trained with model distillation and heavy hyperparameter optimization.", "paperhash": "urban|do_deep_convolutional_nets_really_need_to_be_deep_and_convolutional", "keywords": ["Deep learning", "Transfer Learning"], "conflicts": ["uci.edu", "sms.ed.ac.uk", "polymtl.ca", "cs.ualberta.ca", "cs.washington.edu", "microsoft.com", "uni-heidelberg.de", "mpimf-heidelberg.mpg.de"], "authors": ["Gregor Urban", "Krzysztof J. Geras", "Samira Ebrahimi Kahou", "Ozlem Aslan", "Shengjie Wang", "Abdelrahman Mohamed", "Matthai Philipose", "Matt Richardson", "Rich Caruana"], "authorids": ["gurban@uci.edu", "k.j.geras@sms.ed.ac.uk", "samira.ebrahimi-kahou@polymtl.ca", "ozlem@cs.ualberta.ca", "wangsj@cs.washington.edu", "asamir@microsoft.com", "matthaip@microsoft.com", "mattri@microsoft.com", "rcaruana@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287727622, "id": "ICLR.cc/2017/conference/-/paper102/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "r10FA8Kxg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper102/reviewers", "ICLR.cc/2017/conference/paper102/areachairs"], "cdate": 1485287727622}}}, {"tddate": null, "tmdate": 1481484631639, "tcdate": 1481484631632, "number": 2, "id": "S1xwYXsXg", "invitation": "ICLR.cc/2017/conference/-/paper102/pre-review/question", "forum": "r10FA8Kxg", "replyto": "r10FA8Kxg", "signatures": ["ICLR.cc/2017/conference/paper102/AnonReviewer4"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper102/AnonReviewer4"], "content": {"title": "deep, but not too deep ?", "question": "An interesting point on which the authors comment only very briefly is that among the non-convolutional architectures the ones using 2 or 3 hidden layers outperform those with 1, 4 or 5 hidden layers. Do you have an interpretation / hypothesis of why this is the case? It  would be interesting to discuss the point a bit more in the paper."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Do Deep Convolutional Nets Really Need to be Deep and Convolutional?", "abstract": "Yes, they do.  This paper provides the first empirical demonstration that deep convolutional models really need to be both deep and convolutional, even when trained with methods such as distillation that allow small or shallow models of high accuracy to be trained.  Although previous research showed that shallow feed-forward nets sometimes can learn the complex functions previously learned by deep nets while using the same number of parameters as the deep models they mimic, in this paper we demonstrate that the same methods cannot be used to train accurate models on CIFAR-10 unless the student models contain multiple layers of convolution.  Although the student models do not have to be as deep as the teacher model they mimic, the students need multiple convolutional layers to learn functions of comparable accuracy as the deep convolutional teacher.  ", "pdf": "/pdf/8d987f288955ca4836e35e40ffe710638a00e1ef.pdf", "TL;DR": "This paper provides the first empirical demonstration that deep convolutional models really need to be both deep and convolutional, even when trained with model distillation and heavy hyperparameter optimization.", "paperhash": "urban|do_deep_convolutional_nets_really_need_to_be_deep_and_convolutional", "keywords": ["Deep learning", "Transfer Learning"], "conflicts": ["uci.edu", "sms.ed.ac.uk", "polymtl.ca", "cs.ualberta.ca", "cs.washington.edu", "microsoft.com", "uni-heidelberg.de", "mpimf-heidelberg.mpg.de"], "authors": ["Gregor Urban", "Krzysztof J. Geras", "Samira Ebrahimi Kahou", "Ozlem Aslan", "Shengjie Wang", "Abdelrahman Mohamed", "Matthai Philipose", "Matt Richardson", "Rich Caruana"], "authorids": ["gurban@uci.edu", "k.j.geras@sms.ed.ac.uk", "samira.ebrahimi-kahou@polymtl.ca", "ozlem@cs.ualberta.ca", "wangsj@cs.washington.edu", "asamir@microsoft.com", "matthaip@microsoft.com", "mattri@microsoft.com", "rcaruana@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481484632128, "id": "ICLR.cc/2017/conference/-/paper102/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper102/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper102/AnonReviewer1", "ICLR.cc/2017/conference/paper102/AnonReviewer4"], "reply": {"forum": "r10FA8Kxg", "replyto": "r10FA8Kxg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper102/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper102/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481484632128}}}], "count": 11}