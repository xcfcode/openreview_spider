{"notes": [{"id": "HJgVisRqtX", "original": "HkeMAO2qt7", "number": 613, "cdate": 1538087835791, "ddate": null, "tcdate": 1538087835791, "tmdate": 1545355434091, "tddate": null, "forum": "HJgVisRqtX", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "SEGEN: SAMPLE-ENSEMBLE GENETIC EVOLUTIONARY NETWORK MODEL", "abstract": "Deep learning, a rebranding of deep neural network research works, has achieved a remarkable success in recent years. With multiple hidden layers, deep learning models aim at computing the hierarchical feature representations of the observational data. Meanwhile, due to its severe disadvantages in data consumption, computational resources, parameter tuning costs and the lack of result explainability, deep learning has also suffered from lots of criticism. In this paper, we will introduce a new representation learning model, namely \u201cSample-Ensemble Genetic Evolutionary Network\u201d (SEGEN), which can serve as an alternative approach to deep learning models. Instead of building one single deep model, based on a set of sampled sub-instances, SEGEN adopts a genetic-evolutionary learning strategy to build a group of unit models generations by generations. The unit models incorporated in SEGEN can be either traditional machine learning models or the recent deep learning models with a much \u201cnarrower\u201d and \u201cshallower\u201d architecture. The learning results of each instance at the final generation will be effectively combined from each unit model via diffusive propagation and ensemble learning strategies. From the computational perspective, SEGEN requires far less data, fewer computational resources and parameter tuning efforts, but has sound theoretic interpretability of the learning process and results. Extensive experiments have been done on several different real-world benchmark datasets, and the experimental results obtained by SEGEN have demonstrated its advantages over the state-of-the-art representation learning models.", "keywords": ["Genetic Evolutionary Network", "Deep Learning", "Genetic Algorithm", "Ensemble Learning", "Representation Learning"], "authorids": ["jiawei@ifmlab.org", "lmcui932@163.com", "fisherbgouza@gmail.com"], "authors": ["Jiawei Zhang", "Limeng Cui", "Fisher B. Gouza"], "TL;DR": "We introduce a new representation learning model, namely \u201cSample-Ensemble Genetic Evolutionary Network\u201d (SEGEN), which can serve as an alternative approach to deep learning models.", "pdf": "/pdf/72acae32af5730c669efa51298f7d276b3dbc41f.pdf", "paperhash": "zhang|segen_sampleensemble_genetic_evolutionary_network_model", "_bibtex": "@misc{\nzhang2019segen,\ntitle={{SEGEN}: {SAMPLE}-{ENSEMBLE} {GENETIC} {EVOLUTIONARY} {NETWORK} {MODEL}},\nauthor={Jiawei Zhang and Limeng Cui and Fisher B. Gouza},\nyear={2019},\nurl={https://openreview.net/forum?id=HJgVisRqtX},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "S1xpnM8SxE", "original": null, "number": 1, "cdate": 1545065141153, "ddate": null, "tcdate": 1545065141153, "tmdate": 1545354482615, "tddate": null, "forum": "HJgVisRqtX", "replyto": "HJgVisRqtX", "invitation": "ICLR.cc/2019/Conference/-/Paper613/Meta_Review", "content": {"metareview": "This paper endeavors to combine genetic evolutionary algorithms with subsampling techniques. As noted by reviewers, this is an interesting topic and the paper is intriguing, but more work is required to make it convincing (fairer baselines, more detailed / clearer presentation, ablation studies to justify the claims made int he paper). Authors are encouraged to strengthen the paper by following reviewers' suggestions.", "confidence": "5: The area chair is absolutely certain", "recommendation": "Reject", "title": "Interesting topic but requires more work"}, "signatures": ["ICLR.cc/2019/Conference/Paper613/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper613/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SEGEN: SAMPLE-ENSEMBLE GENETIC EVOLUTIONARY NETWORK MODEL", "abstract": "Deep learning, a rebranding of deep neural network research works, has achieved a remarkable success in recent years. With multiple hidden layers, deep learning models aim at computing the hierarchical feature representations of the observational data. Meanwhile, due to its severe disadvantages in data consumption, computational resources, parameter tuning costs and the lack of result explainability, deep learning has also suffered from lots of criticism. In this paper, we will introduce a new representation learning model, namely \u201cSample-Ensemble Genetic Evolutionary Network\u201d (SEGEN), which can serve as an alternative approach to deep learning models. Instead of building one single deep model, based on a set of sampled sub-instances, SEGEN adopts a genetic-evolutionary learning strategy to build a group of unit models generations by generations. The unit models incorporated in SEGEN can be either traditional machine learning models or the recent deep learning models with a much \u201cnarrower\u201d and \u201cshallower\u201d architecture. The learning results of each instance at the final generation will be effectively combined from each unit model via diffusive propagation and ensemble learning strategies. From the computational perspective, SEGEN requires far less data, fewer computational resources and parameter tuning efforts, but has sound theoretic interpretability of the learning process and results. Extensive experiments have been done on several different real-world benchmark datasets, and the experimental results obtained by SEGEN have demonstrated its advantages over the state-of-the-art representation learning models.", "keywords": ["Genetic Evolutionary Network", "Deep Learning", "Genetic Algorithm", "Ensemble Learning", "Representation Learning"], "authorids": ["jiawei@ifmlab.org", "lmcui932@163.com", "fisherbgouza@gmail.com"], "authors": ["Jiawei Zhang", "Limeng Cui", "Fisher B. Gouza"], "TL;DR": "We introduce a new representation learning model, namely \u201cSample-Ensemble Genetic Evolutionary Network\u201d (SEGEN), which can serve as an alternative approach to deep learning models.", "pdf": "/pdf/72acae32af5730c669efa51298f7d276b3dbc41f.pdf", "paperhash": "zhang|segen_sampleensemble_genetic_evolutionary_network_model", "_bibtex": "@misc{\nzhang2019segen,\ntitle={{SEGEN}: {SAMPLE}-{ENSEMBLE} {GENETIC} {EVOLUTIONARY} {NETWORK} {MODEL}},\nauthor={Jiawei Zhang and Limeng Cui and Fisher B. Gouza},\nyear={2019},\nurl={https://openreview.net/forum?id=HJgVisRqtX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper613/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353152496, "tddate": null, "super": null, "final": null, "reply": {"forum": "HJgVisRqtX", "replyto": "HJgVisRqtX", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper613/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper613/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper613/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353152496}}}, {"id": "ryejOUP7RQ", "original": null, "number": 5, "cdate": 1542841971317, "ddate": null, "tcdate": 1542841971317, "tmdate": 1543174504049, "tddate": null, "forum": "HJgVisRqtX", "replyto": "B1x636G56Q", "invitation": "ICLR.cc/2019/Conference/-/Paper613/Official_Comment", "content": {"title": "Response to the authors' rebuttal", "comment": "Thanks for the response. Some of the concerns are resolved through the rebuttal. Here are some important issues: \n- Point <2> needs to be clarified in the manuscript.\n- Point <3> is not raised or mentioned in the manuscript! Needs clear clarification, and then ablation studies to show how they help.\n- With respect to point <4>, the limited space notations cannot be a good reason. The authors have the option of providing supplementary material, which does not have any space limitations or constraints, and the authors did not use that. Furthermore, referring to external material (other papers, or papers on arxiv) in the rebuttal to answer a direct concern of the reviewers does not seem correct. If something is required for the paper to be understood and to justify the correctness of the work, it should be provided in the paper or the supplementary material!\n- For point <6>, the authors seem not to understand what the fitness function means here. Fitness in the GA (in your setting) would be defined as evaluating a single unit model. The evaluation consists of building a unit model, fine-tuning, and evaluating. All these are functions of the data size (or portions of it). Then, the deep generation by generation (k \\times m) adds to it and is a constant factor. \n- For point <7>, the authors again refer to an external publication, without enough discussions and justifications here.\n\nIf every question raised by this reviewer (and also other reviewers) can be answered by other papers that the authors keep referring to in the rebuttal, then, what is the need for publishing this new paper!?\n\nAlthough this reviewer appreciates the response from the authors, I still think the paper is not mature and is not ready for publication. "}, "signatures": ["ICLR.cc/2019/Conference/Paper613/AnonReviewer1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper613/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper613/AnonReviewer1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SEGEN: SAMPLE-ENSEMBLE GENETIC EVOLUTIONARY NETWORK MODEL", "abstract": "Deep learning, a rebranding of deep neural network research works, has achieved a remarkable success in recent years. With multiple hidden layers, deep learning models aim at computing the hierarchical feature representations of the observational data. Meanwhile, due to its severe disadvantages in data consumption, computational resources, parameter tuning costs and the lack of result explainability, deep learning has also suffered from lots of criticism. In this paper, we will introduce a new representation learning model, namely \u201cSample-Ensemble Genetic Evolutionary Network\u201d (SEGEN), which can serve as an alternative approach to deep learning models. Instead of building one single deep model, based on a set of sampled sub-instances, SEGEN adopts a genetic-evolutionary learning strategy to build a group of unit models generations by generations. The unit models incorporated in SEGEN can be either traditional machine learning models or the recent deep learning models with a much \u201cnarrower\u201d and \u201cshallower\u201d architecture. The learning results of each instance at the final generation will be effectively combined from each unit model via diffusive propagation and ensemble learning strategies. From the computational perspective, SEGEN requires far less data, fewer computational resources and parameter tuning efforts, but has sound theoretic interpretability of the learning process and results. Extensive experiments have been done on several different real-world benchmark datasets, and the experimental results obtained by SEGEN have demonstrated its advantages over the state-of-the-art representation learning models.", "keywords": ["Genetic Evolutionary Network", "Deep Learning", "Genetic Algorithm", "Ensemble Learning", "Representation Learning"], "authorids": ["jiawei@ifmlab.org", "lmcui932@163.com", "fisherbgouza@gmail.com"], "authors": ["Jiawei Zhang", "Limeng Cui", "Fisher B. Gouza"], "TL;DR": "We introduce a new representation learning model, namely \u201cSample-Ensemble Genetic Evolutionary Network\u201d (SEGEN), which can serve as an alternative approach to deep learning models.", "pdf": "/pdf/72acae32af5730c669efa51298f7d276b3dbc41f.pdf", "paperhash": "zhang|segen_sampleensemble_genetic_evolutionary_network_model", "_bibtex": "@misc{\nzhang2019segen,\ntitle={{SEGEN}: {SAMPLE}-{ENSEMBLE} {GENETIC} {EVOLUTIONARY} {NETWORK} {MODEL}},\nauthor={Jiawei Zhang and Limeng Cui and Fisher B. Gouza},\nyear={2019},\nurl={https://openreview.net/forum?id=HJgVisRqtX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper613/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621624022, "tddate": null, "super": null, "final": null, "reply": {"forum": "HJgVisRqtX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper613/Authors", "ICLR.cc/2019/Conference/Paper613/Reviewers", "ICLR.cc/2019/Conference/Paper613/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper613/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper613/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper613/Authors|ICLR.cc/2019/Conference/Paper613/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper613/Reviewers", "ICLR.cc/2019/Conference/Paper613/Authors", "ICLR.cc/2019/Conference/Paper613/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621624022}}}, {"id": "rkgRexJdAX", "original": null, "number": 7, "cdate": 1543135222353, "ddate": null, "tcdate": 1543135222353, "tmdate": 1543135222353, "tddate": null, "forum": "HJgVisRqtX", "replyto": "SJxPI_zcp7", "invitation": "ICLR.cc/2019/Conference/-/Paper613/Official_Comment", "content": {"title": "response to rebuttal", "comment": "I appreciate the authors' effort in clarifying their contributions and the explanation.\n\n<1.1> The solution is subsampling a large graph and the ways of subsampling have been adopted by others. As mentioned in the comments, noveling is limited.\n<1.2> Maybe the use of the word \"component\" confuses the authors. The three \"components\" refer to subsampling, GA and ensemble learning.\n<3> After reading your response, I still think that the key contribution of the paper is combination of subsampling, GA and simple ensemble learning. \n<4> Using GA for global optimization has long been proposed. The use of GA for evolving neural networks has also been considered and tested with different success. And the way being adopted by the authors is not particularly novel.\n<5> As explained, combinations of subsampling + existing network embedding methods (LINE/DeepWalk/...) + ensembling may also give good results.\n<6> As explained, the fact that GA was proposed for global optimization is a well known fact.\n<7> I think this is an open-ended question leaving for the authors to think about.\n<8> \"GA + Ensemble together not sound\" Again I think the authors misunderstand some of my comments and hard to provide further comments here.\n\nAfter reading the rebuttal, I think I still uphold my original rating."}, "signatures": ["ICLR.cc/2019/Conference/Paper613/AnonReviewer2"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper613/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper613/AnonReviewer2", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SEGEN: SAMPLE-ENSEMBLE GENETIC EVOLUTIONARY NETWORK MODEL", "abstract": "Deep learning, a rebranding of deep neural network research works, has achieved a remarkable success in recent years. With multiple hidden layers, deep learning models aim at computing the hierarchical feature representations of the observational data. Meanwhile, due to its severe disadvantages in data consumption, computational resources, parameter tuning costs and the lack of result explainability, deep learning has also suffered from lots of criticism. In this paper, we will introduce a new representation learning model, namely \u201cSample-Ensemble Genetic Evolutionary Network\u201d (SEGEN), which can serve as an alternative approach to deep learning models. Instead of building one single deep model, based on a set of sampled sub-instances, SEGEN adopts a genetic-evolutionary learning strategy to build a group of unit models generations by generations. The unit models incorporated in SEGEN can be either traditional machine learning models or the recent deep learning models with a much \u201cnarrower\u201d and \u201cshallower\u201d architecture. The learning results of each instance at the final generation will be effectively combined from each unit model via diffusive propagation and ensemble learning strategies. From the computational perspective, SEGEN requires far less data, fewer computational resources and parameter tuning efforts, but has sound theoretic interpretability of the learning process and results. Extensive experiments have been done on several different real-world benchmark datasets, and the experimental results obtained by SEGEN have demonstrated its advantages over the state-of-the-art representation learning models.", "keywords": ["Genetic Evolutionary Network", "Deep Learning", "Genetic Algorithm", "Ensemble Learning", "Representation Learning"], "authorids": ["jiawei@ifmlab.org", "lmcui932@163.com", "fisherbgouza@gmail.com"], "authors": ["Jiawei Zhang", "Limeng Cui", "Fisher B. Gouza"], "TL;DR": "We introduce a new representation learning model, namely \u201cSample-Ensemble Genetic Evolutionary Network\u201d (SEGEN), which can serve as an alternative approach to deep learning models.", "pdf": "/pdf/72acae32af5730c669efa51298f7d276b3dbc41f.pdf", "paperhash": "zhang|segen_sampleensemble_genetic_evolutionary_network_model", "_bibtex": "@misc{\nzhang2019segen,\ntitle={{SEGEN}: {SAMPLE}-{ENSEMBLE} {GENETIC} {EVOLUTIONARY} {NETWORK} {MODEL}},\nauthor={Jiawei Zhang and Limeng Cui and Fisher B. Gouza},\nyear={2019},\nurl={https://openreview.net/forum?id=HJgVisRqtX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper613/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621624022, "tddate": null, "super": null, "final": null, "reply": {"forum": "HJgVisRqtX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper613/Authors", "ICLR.cc/2019/Conference/Paper613/Reviewers", "ICLR.cc/2019/Conference/Paper613/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper613/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper613/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper613/Authors|ICLR.cc/2019/Conference/Paper613/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper613/Reviewers", "ICLR.cc/2019/Conference/Paper613/Authors", "ICLR.cc/2019/Conference/Paper613/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621624022}}}, {"id": "SJlE9Qm32m", "original": null, "number": 1, "cdate": 1541317515570, "ddate": null, "tcdate": 1541317515570, "tmdate": 1542842008111, "tddate": null, "forum": "HJgVisRqtX", "replyto": "HJgVisRqtX", "invitation": "ICLR.cc/2019/Conference/-/Paper613/Official_Review", "content": {"title": "Interesting topic but several issues with the paper", "review": "This manuscript introduces SEGEN, a model based on Evolutionary Computation for building deep models. Interestingly, the authors define deep models in a different way. Instead of stacking several hidden layers one after the other (as in traditional deep learning models), SEGEN uses the idea of generations in evolutionary models (Genetic Algorithms or GA) and puts the unit models in the successive generations into layers, i.e., \u201cevolutionary layer\u201d. Each layer then performs the validation, selection, crossover, and mutation operations, as in GA. Another interesting point of the proposed method is that the choice of unit models in SEGEN can be traditional machine learning or recent deep learning models.\nThe paper touches an interesting topic and proposes a sound method. However, there are several issues with the paper. There are several ungrounded and untested claims, as well as many unclear points in the method.\n-\tIn page 5, Section 4.2.4, the authors introduce the loss function used to define the fitness for the evolutionary model. It is not clear why they use the difference between the latent representations of the autoencoders (z) from pairwise nodes to define the loss. There are no motivations or discussion for this. Two different representations of two nodes may both be good (e.g., in terms of classification of data), but they do not have to be necessarily identical. \n-\tGiven the loss defined in Section 4.2.4, it is not clear how the authors ran their model for MNIST and other datasets, for which they used CNN and MLP unit models. In CNN and MLP there is not latent representation z.\n-\tBased on the model descriptions in Section 4.2 (and its Subsections), the proposed method transfers the learned models in previous generations to the next ones. But there is no explanation if the new models are again fine-tuned on the data? For instance, take the autoencoders, for two different unit models, the cross-over operator defuses the variables (weights and bias) from the two selected models to create an offspring. There is no guarantee that the new autoencoder model works properly on the same dataset. As a na\u00efve example, if there are correlated and redundant features in the data, different autoencoders may separately focus on one/some of these features. Defusing weights of the two autoencoders (built upon different aspects of the data) may most probably ruin the whole model. \n-\tThere are four claims in the paper on the advantages of the proposed model, compared to other deep learning algorithms. None of these claims are discussed in depth or at least illustrated experimentally. \n*** Less Data for Unit Model Learning. The authors could have reported the number of variables used in each model in the experiments. It is important to see with how many of a larger number of variables a traditional deep model can result in comparable results to SEGEN. \n*** Less Computational Resources. The model operates in several generations and in each generation, many unit models are built. It is not fair to say and not clear how it can occupy less space or time complexity than a regular GCNN or MLP.\n*** Less Parameter Tuning. Again experiments could clarify this issue.\n*** Sound Theoretic Explanation. The authors only refer to (Rudolph 1994) for the performance bounds of their model and claim that since they are using GA they are better than other deep learning models. However, performance bounds for GA models are very shallow and proximal. \n-\tTo calculate the computational complexity of the model, the authors analyzed the time for learning one unit model. However, in GA models, the complexity is calculated using the bounds on the number of times the fitness function is called since the fitness function is the most computationally intensive task (please see: Pelikan and Lobo 1999 \u2018Parameterless Genetic Algorithm A Worst-case Time and Space Complexity Analysis\u2019). \n-\tOne of the main fallacies of GAs and evolutionary algorithms is that they may lead to premature convergence. This is very common, especially at the presence of trap functions, such as non-convex functions that real-world problems deal with (please see: Goldberg et al. 1991 \u2018Massive Multimodality, Deception, and Genetic Algorithms\u2019). There are no discussions/experiments on how SEGEN may overcome the premature convergence, or even if it converges at all.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper613/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "SEGEN: SAMPLE-ENSEMBLE GENETIC EVOLUTIONARY NETWORK MODEL", "abstract": "Deep learning, a rebranding of deep neural network research works, has achieved a remarkable success in recent years. With multiple hidden layers, deep learning models aim at computing the hierarchical feature representations of the observational data. Meanwhile, due to its severe disadvantages in data consumption, computational resources, parameter tuning costs and the lack of result explainability, deep learning has also suffered from lots of criticism. In this paper, we will introduce a new representation learning model, namely \u201cSample-Ensemble Genetic Evolutionary Network\u201d (SEGEN), which can serve as an alternative approach to deep learning models. Instead of building one single deep model, based on a set of sampled sub-instances, SEGEN adopts a genetic-evolutionary learning strategy to build a group of unit models generations by generations. The unit models incorporated in SEGEN can be either traditional machine learning models or the recent deep learning models with a much \u201cnarrower\u201d and \u201cshallower\u201d architecture. The learning results of each instance at the final generation will be effectively combined from each unit model via diffusive propagation and ensemble learning strategies. From the computational perspective, SEGEN requires far less data, fewer computational resources and parameter tuning efforts, but has sound theoretic interpretability of the learning process and results. Extensive experiments have been done on several different real-world benchmark datasets, and the experimental results obtained by SEGEN have demonstrated its advantages over the state-of-the-art representation learning models.", "keywords": ["Genetic Evolutionary Network", "Deep Learning", "Genetic Algorithm", "Ensemble Learning", "Representation Learning"], "authorids": ["jiawei@ifmlab.org", "lmcui932@163.com", "fisherbgouza@gmail.com"], "authors": ["Jiawei Zhang", "Limeng Cui", "Fisher B. Gouza"], "TL;DR": "We introduce a new representation learning model, namely \u201cSample-Ensemble Genetic Evolutionary Network\u201d (SEGEN), which can serve as an alternative approach to deep learning models.", "pdf": "/pdf/72acae32af5730c669efa51298f7d276b3dbc41f.pdf", "paperhash": "zhang|segen_sampleensemble_genetic_evolutionary_network_model", "_bibtex": "@misc{\nzhang2019segen,\ntitle={{SEGEN}: {SAMPLE}-{ENSEMBLE} {GENETIC} {EVOLUTIONARY} {NETWORK} {MODEL}},\nauthor={Jiawei Zhang and Limeng Cui and Fisher B. Gouza},\nyear={2019},\nurl={https://openreview.net/forum?id=HJgVisRqtX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper613/Official_Review", "cdate": 1542234419676, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HJgVisRqtX", "replyto": "HJgVisRqtX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper613/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335763672, "tmdate": 1552335763672, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper613/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "B1x636G56Q", "original": null, "number": 3, "cdate": 1542233524944, "ddate": null, "tcdate": 1542233524944, "tmdate": 1542233661986, "tddate": null, "forum": "HJgVisRqtX", "replyto": "SJlE9Qm32m", "invitation": "ICLR.cc/2019/Conference/-/Paper613/Official_Comment", "content": {"title": "Author Response to Reviewer 3", "comment": "Thank you for your comments. Please find the response as follows. Hope we resolve your concerns and questions. Welcome to let us know if you have any other questions.\n\n<1> Section 4.2.4 in Page 5 on the loss function. We clarify that for each node we can compute its latent feature representation z with the auto-encoder model. However, graph embedding is slightly different form other existing embedding problems, since the nodes are connected. Generally, in graph embedding, we may hope the learned representation features can capture the network structure: connected nodes will have closer representations. Therefore, given two nodes, v_i, v_j, if they are connected, i.e., s_ij = 1, then we may want to project them into close regions; if they are not connected, i.e., s_ij = 0, then we will not count the loss introduced by them, i.e., projecting them to any regions will not matter any more.\n\n<2> The auto-encoder model as well as the z vectors is for the network embedding task only (auto-encoder as the base model, we further consider the graph connections). We use it as an example to introduce the overall SEGEN framework settings. The task and the unit models used in it can be changed to any other models, where the detailed loss function and the descriptions will be different. When it comes to CNN+MNIST or MLP+OtherDatasets, we will learn CNN unit models and MLP unit models instead, which will not contain the z vectors or the loss function in section 4.2.4. Instead, we will have some other loss functions on the CNN output, e.g., the cross-entropy on the CNN outputs compared against the true labels.\n\n<3> New models are fine-tuned? We claim that the new models will be fine-tuned in the next generation on the dataset. Since in the new generation, the first step is to learn the unit models before involving them in the genetic evolutionary part. They will be trained on the training set. \n\n<4> We clarify that we have provide the proof ready, but due to the limited space we remove many important proofs. We demonstrate that via GA and ensemble, we can achieve better performance. The reviewer is suggested to refer to Section 4 in the recent article (https://arxiv.org/abs/1805.07500) for more information. Especially Equation 14 in that article, it indicates that via generations the learning loss will be non-increasing. \n\n<5> We clarify that our time and space cost analysis is not for one unit model, it is for the whole SEGEN model with multiple generations. The time complexiety provided before Section 4.4.3 contains K as the generation number, m as the population size.\n\n<6> We clarify that for the SEGEN model introduced in this part, the fitness function computation is not the most computationally intensive task actually, since the unit model learning with gradient descent in Section 4.2.3 will be much time consuming. The time costs in learning the models may grow exponentially as the model size (I mean the input data size) increases. That is the reason we try to sample the sub-graphs in this paper instead to lower down the time cost compared against the existing deep models (it is also our main contribution and advantages). Fitness function computation is the most computationally intensive task in (Pelikan and Lobo 1999 mentioned in your comments), mainly compared against the mutation and cross operations. Here, the learning settings change to the deep learning model learning + evolutionary. Compared against model learning, GA based evolutionary time cost is not significant any more, not to mention the fitness function computation part.\n\n<7> As to the convergence part, we clarify that we have provide the proof ready, but due to the limited space we remove many important proofs. The reviewer is also suggested to refer to Section 4 in the recent article (https://arxiv.org/abs/1805.07500) for more information. Especially Equation 14 in that article, it indicates that via generations the learning loss will be non-increasing, and it will converge generations by generations.\n\n\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper613/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper613/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper613/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SEGEN: SAMPLE-ENSEMBLE GENETIC EVOLUTIONARY NETWORK MODEL", "abstract": "Deep learning, a rebranding of deep neural network research works, has achieved a remarkable success in recent years. With multiple hidden layers, deep learning models aim at computing the hierarchical feature representations of the observational data. Meanwhile, due to its severe disadvantages in data consumption, computational resources, parameter tuning costs and the lack of result explainability, deep learning has also suffered from lots of criticism. In this paper, we will introduce a new representation learning model, namely \u201cSample-Ensemble Genetic Evolutionary Network\u201d (SEGEN), which can serve as an alternative approach to deep learning models. Instead of building one single deep model, based on a set of sampled sub-instances, SEGEN adopts a genetic-evolutionary learning strategy to build a group of unit models generations by generations. The unit models incorporated in SEGEN can be either traditional machine learning models or the recent deep learning models with a much \u201cnarrower\u201d and \u201cshallower\u201d architecture. The learning results of each instance at the final generation will be effectively combined from each unit model via diffusive propagation and ensemble learning strategies. From the computational perspective, SEGEN requires far less data, fewer computational resources and parameter tuning efforts, but has sound theoretic interpretability of the learning process and results. Extensive experiments have been done on several different real-world benchmark datasets, and the experimental results obtained by SEGEN have demonstrated its advantages over the state-of-the-art representation learning models.", "keywords": ["Genetic Evolutionary Network", "Deep Learning", "Genetic Algorithm", "Ensemble Learning", "Representation Learning"], "authorids": ["jiawei@ifmlab.org", "lmcui932@163.com", "fisherbgouza@gmail.com"], "authors": ["Jiawei Zhang", "Limeng Cui", "Fisher B. Gouza"], "TL;DR": "We introduce a new representation learning model, namely \u201cSample-Ensemble Genetic Evolutionary Network\u201d (SEGEN), which can serve as an alternative approach to deep learning models.", "pdf": "/pdf/72acae32af5730c669efa51298f7d276b3dbc41f.pdf", "paperhash": "zhang|segen_sampleensemble_genetic_evolutionary_network_model", "_bibtex": "@misc{\nzhang2019segen,\ntitle={{SEGEN}: {SAMPLE}-{ENSEMBLE} {GENETIC} {EVOLUTIONARY} {NETWORK} {MODEL}},\nauthor={Jiawei Zhang and Limeng Cui and Fisher B. Gouza},\nyear={2019},\nurl={https://openreview.net/forum?id=HJgVisRqtX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper613/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621624022, "tddate": null, "super": null, "final": null, "reply": {"forum": "HJgVisRqtX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper613/Authors", "ICLR.cc/2019/Conference/Paper613/Reviewers", "ICLR.cc/2019/Conference/Paper613/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper613/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper613/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper613/Authors|ICLR.cc/2019/Conference/Paper613/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper613/Reviewers", "ICLR.cc/2019/Conference/Paper613/Authors", "ICLR.cc/2019/Conference/Paper613/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621624022}}}, {"id": "SkeQ8n-caQ", "original": null, "number": 1, "cdate": 1542229067272, "ddate": null, "tcdate": 1542229067272, "tmdate": 1542233606176, "tddate": null, "forum": "HJgVisRqtX", "replyto": "HkgIko9h2m", "invitation": "ICLR.cc/2019/Conference/-/Paper613/Official_Comment", "content": {"title": "Author Response to Reviewer 1", "comment": "Thank you for your comments. Please find the response as follows. Hope we resolve your concerns and questions. Welcome to let us know if you have any other questions.\n\n<1> We clarify that we introduce the model chromosomes as the variables of the models. You can refer to the last two sentences in section 4.2.1. as well as section 4.2.5. We also paste the sentences as follows.\n4.2.1: Formally, the variables involved in each unit model, e.g., M_i^1, can be denoted as vector \u03b8_i^1, which covers the weight and bias terms in the model (which will be treated as the model genes in the evolution to be introduced later) ).\n4.2.5: For the k_th pair of parent unit model (M_i^1,M_j^1)k \u2208 P^1, we can denote their genes as their variables \u03b8_i^1,\u03b8_j^1 respectively (since the differences among the unit models mainly lie in their variables), which are actually their chromosomes for crossover and mutation.\n\n<2> We clarify that we introduce the computational analysis in Section 4.4, including its performance analysis, space and time cost analysis, as well as advantages analysis.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper613/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper613/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper613/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SEGEN: SAMPLE-ENSEMBLE GENETIC EVOLUTIONARY NETWORK MODEL", "abstract": "Deep learning, a rebranding of deep neural network research works, has achieved a remarkable success in recent years. With multiple hidden layers, deep learning models aim at computing the hierarchical feature representations of the observational data. Meanwhile, due to its severe disadvantages in data consumption, computational resources, parameter tuning costs and the lack of result explainability, deep learning has also suffered from lots of criticism. In this paper, we will introduce a new representation learning model, namely \u201cSample-Ensemble Genetic Evolutionary Network\u201d (SEGEN), which can serve as an alternative approach to deep learning models. Instead of building one single deep model, based on a set of sampled sub-instances, SEGEN adopts a genetic-evolutionary learning strategy to build a group of unit models generations by generations. The unit models incorporated in SEGEN can be either traditional machine learning models or the recent deep learning models with a much \u201cnarrower\u201d and \u201cshallower\u201d architecture. The learning results of each instance at the final generation will be effectively combined from each unit model via diffusive propagation and ensemble learning strategies. From the computational perspective, SEGEN requires far less data, fewer computational resources and parameter tuning efforts, but has sound theoretic interpretability of the learning process and results. Extensive experiments have been done on several different real-world benchmark datasets, and the experimental results obtained by SEGEN have demonstrated its advantages over the state-of-the-art representation learning models.", "keywords": ["Genetic Evolutionary Network", "Deep Learning", "Genetic Algorithm", "Ensemble Learning", "Representation Learning"], "authorids": ["jiawei@ifmlab.org", "lmcui932@163.com", "fisherbgouza@gmail.com"], "authors": ["Jiawei Zhang", "Limeng Cui", "Fisher B. Gouza"], "TL;DR": "We introduce a new representation learning model, namely \u201cSample-Ensemble Genetic Evolutionary Network\u201d (SEGEN), which can serve as an alternative approach to deep learning models.", "pdf": "/pdf/72acae32af5730c669efa51298f7d276b3dbc41f.pdf", "paperhash": "zhang|segen_sampleensemble_genetic_evolutionary_network_model", "_bibtex": "@misc{\nzhang2019segen,\ntitle={{SEGEN}: {SAMPLE}-{ENSEMBLE} {GENETIC} {EVOLUTIONARY} {NETWORK} {MODEL}},\nauthor={Jiawei Zhang and Limeng Cui and Fisher B. Gouza},\nyear={2019},\nurl={https://openreview.net/forum?id=HJgVisRqtX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper613/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621624022, "tddate": null, "super": null, "final": null, "reply": {"forum": "HJgVisRqtX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper613/Authors", "ICLR.cc/2019/Conference/Paper613/Reviewers", "ICLR.cc/2019/Conference/Paper613/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper613/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper613/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper613/Authors|ICLR.cc/2019/Conference/Paper613/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper613/Reviewers", "ICLR.cc/2019/Conference/Paper613/Authors", "ICLR.cc/2019/Conference/Paper613/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621624022}}}, {"id": "SJxPI_zcp7", "original": null, "number": 2, "cdate": 1542232143106, "ddate": null, "tcdate": 1542232143106, "tmdate": 1542233583677, "tddate": null, "forum": "HJgVisRqtX", "replyto": "rkxb8KF2hQ", "invitation": "ICLR.cc/2019/Conference/-/Paper613/Official_Comment", "content": {"title": "Author Response to Reviewer 2", "comment": "\nThank you for your comments. Please find the response as follows. Hope we resolve your concerns and questions.\n\n<1> First of all, we need to re-clarify the contributions of this article.\n\n<1.1> For the learning settings with extremely large-sized but small-numbered data instances (i.e., each data instance is large, but the total number of available data instance is small), training large and deep neural networks is an impossible mission. In this paper, we propose a solution to such a problem. \n\n<1.2> This paper doesn\u2019t really like existing deep learning works focusing in stacking components together. It is not a good idea to interpret our contribution as \u201cputting three known components together\u201d. According to <1.1>, to solve the lack of data instance problem, we propose to divide the large graph into small-sized sub-graphs. To ensure the sub-graphs can capture the properties of the large graph, we use different sampling methods. Meanwhile, in the training process, to ensure the learning effectiveness, we also introduce a new learning framework, with both the gradient descent based algorithm with the genetic algorithm. As to the ensemble part, it not merely because we decompose the large graph into smaller graphs. The main reason is we have a group of small models, each one is trained on sub-graphs achieved by a sampling method, we need to integrate the outputs of these models together.\n\n<1.3> The model learning part of the model proposed in this paper is based on both the gradient decent based algorithms (for each unit model), as well as the genetic algorithm (between different generations). This part should be notable to the reader and the reviewer.\n\n<2> How the gain in performance is resulted? We clarify that with a small number of large data instance inputs, we cannot train effective deep models due to the lack of data instances. By decomposing the large graphs into smaller ones, we will be able to learn effective model variables.\n\n<3> sampling+existing embedding model+ensemble should also be useful. The answer is yes, since our framework and our new learning algorithm is useful, replacing the auto-encoder based embedding algorithm with the other shallow or deep embedding models should also work fine. The reviewer is suggested to read the paper again to understand what we do, so as to understanding our contributions, instead of treating it is a combination of sampling+ensembling.\n\n<4> Originality and significance is limited: This is the first paper to introduce the genetic evolutional neural network! Different from the existing deep learning model works, we propose a novel network model trainable with a small-set of extremely large data instances. We introduce a new model learning algorithm with both gradient decent based algorithms and the genetic algorithm. I assume the reviewer cannot find another paper with these two novelty and contributions.\n\n<5> The baseline methods, LINE, DeepWalk, Node2Vec, and HPE are the state-of-the-art methods in network embedding introduced in recent years.\n\n<6> Crossover on models. Crossover operation is to help learn a much better unit model actually. Based on the gradient descent algorithms, we will be able to learn good unit models. However, once the unit model achieving the local optima, it cannot be further improved any more. Genetic algorithm (including crossover, mutation, etc.) allows the models to jump out from the local optima and achieve better performance. The generated child models will be updated with gradient decent algorithm again to achieve the local optimas. Will this make the models worse? The answer is it is possible. However, in the proposed architecture, we will select top m models among the parent models and the newly generated child models. If the child models are bad, they will not be selected for the next generation. In other words, we can ensure the crossover will not degrade the learning performance of the unit models for the next generation. The readers and reviewers are suggested to refer to the recent article (https://arxiv.org/abs/1805.07500) to understand the advantages of incorporating genetic algorithm into the model optimization part.\n\n<7> The method is not end-to-end. Since the genetic algorithm involves crossover and mutation, this part involves probabilities into the model, it is impossible to train the crossover and mutation operations with the existing error-backpropagation algorithm. In other words, training the method in an end-to-end is an impossible mission.\n\n<8> GA + Ensemble together not sound. We clarify that we have provide the proof ready, but due to the limited space we remove many important proofs. We demonstrate that via GA and ensemble, we can achieve better performance. The reviewer is suggested to refer to Section 4 in the recent article (https://arxiv.org/abs/1805.07500) for more information. Especially Equation 14 in that article, it indicates that via generations the learning loss will be non-increasing. \n"}, "signatures": ["ICLR.cc/2019/Conference/Paper613/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper613/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper613/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SEGEN: SAMPLE-ENSEMBLE GENETIC EVOLUTIONARY NETWORK MODEL", "abstract": "Deep learning, a rebranding of deep neural network research works, has achieved a remarkable success in recent years. With multiple hidden layers, deep learning models aim at computing the hierarchical feature representations of the observational data. Meanwhile, due to its severe disadvantages in data consumption, computational resources, parameter tuning costs and the lack of result explainability, deep learning has also suffered from lots of criticism. In this paper, we will introduce a new representation learning model, namely \u201cSample-Ensemble Genetic Evolutionary Network\u201d (SEGEN), which can serve as an alternative approach to deep learning models. Instead of building one single deep model, based on a set of sampled sub-instances, SEGEN adopts a genetic-evolutionary learning strategy to build a group of unit models generations by generations. The unit models incorporated in SEGEN can be either traditional machine learning models or the recent deep learning models with a much \u201cnarrower\u201d and \u201cshallower\u201d architecture. The learning results of each instance at the final generation will be effectively combined from each unit model via diffusive propagation and ensemble learning strategies. From the computational perspective, SEGEN requires far less data, fewer computational resources and parameter tuning efforts, but has sound theoretic interpretability of the learning process and results. Extensive experiments have been done on several different real-world benchmark datasets, and the experimental results obtained by SEGEN have demonstrated its advantages over the state-of-the-art representation learning models.", "keywords": ["Genetic Evolutionary Network", "Deep Learning", "Genetic Algorithm", "Ensemble Learning", "Representation Learning"], "authorids": ["jiawei@ifmlab.org", "lmcui932@163.com", "fisherbgouza@gmail.com"], "authors": ["Jiawei Zhang", "Limeng Cui", "Fisher B. Gouza"], "TL;DR": "We introduce a new representation learning model, namely \u201cSample-Ensemble Genetic Evolutionary Network\u201d (SEGEN), which can serve as an alternative approach to deep learning models.", "pdf": "/pdf/72acae32af5730c669efa51298f7d276b3dbc41f.pdf", "paperhash": "zhang|segen_sampleensemble_genetic_evolutionary_network_model", "_bibtex": "@misc{\nzhang2019segen,\ntitle={{SEGEN}: {SAMPLE}-{ENSEMBLE} {GENETIC} {EVOLUTIONARY} {NETWORK} {MODEL}},\nauthor={Jiawei Zhang and Limeng Cui and Fisher B. Gouza},\nyear={2019},\nurl={https://openreview.net/forum?id=HJgVisRqtX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper613/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621624022, "tddate": null, "super": null, "final": null, "reply": {"forum": "HJgVisRqtX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper613/Authors", "ICLR.cc/2019/Conference/Paper613/Reviewers", "ICLR.cc/2019/Conference/Paper613/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper613/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper613/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper613/Authors|ICLR.cc/2019/Conference/Paper613/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper613/Reviewers", "ICLR.cc/2019/Conference/Paper613/Authors", "ICLR.cc/2019/Conference/Paper613/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621624022}}}, {"id": "HkgIko9h2m", "original": null, "number": 3, "cdate": 1541348061891, "ddate": null, "tcdate": 1541348061891, "tmdate": 1541533843413, "tddate": null, "forum": "HJgVisRqtX", "replyto": "HJgVisRqtX", "invitation": "ICLR.cc/2019/Conference/-/Paper613/Official_Review", "content": {"title": "Evolutionary part is not clear", "review": "The paper introduces Sample-Ensemble Genetic Evolutionary Network, which adopts a genetic-evolutionary learning strategy to build a group of unit models. Explanation on the evolutionary network part is not enough. For example, there is no clear explanation on how chromosomes are defined. Also, detailed analysis on computational aspect is needed.", "rating": "5: Marginally below acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2019/Conference/Paper613/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SEGEN: SAMPLE-ENSEMBLE GENETIC EVOLUTIONARY NETWORK MODEL", "abstract": "Deep learning, a rebranding of deep neural network research works, has achieved a remarkable success in recent years. With multiple hidden layers, deep learning models aim at computing the hierarchical feature representations of the observational data. Meanwhile, due to its severe disadvantages in data consumption, computational resources, parameter tuning costs and the lack of result explainability, deep learning has also suffered from lots of criticism. In this paper, we will introduce a new representation learning model, namely \u201cSample-Ensemble Genetic Evolutionary Network\u201d (SEGEN), which can serve as an alternative approach to deep learning models. Instead of building one single deep model, based on a set of sampled sub-instances, SEGEN adopts a genetic-evolutionary learning strategy to build a group of unit models generations by generations. The unit models incorporated in SEGEN can be either traditional machine learning models or the recent deep learning models with a much \u201cnarrower\u201d and \u201cshallower\u201d architecture. The learning results of each instance at the final generation will be effectively combined from each unit model via diffusive propagation and ensemble learning strategies. From the computational perspective, SEGEN requires far less data, fewer computational resources and parameter tuning efforts, but has sound theoretic interpretability of the learning process and results. Extensive experiments have been done on several different real-world benchmark datasets, and the experimental results obtained by SEGEN have demonstrated its advantages over the state-of-the-art representation learning models.", "keywords": ["Genetic Evolutionary Network", "Deep Learning", "Genetic Algorithm", "Ensemble Learning", "Representation Learning"], "authorids": ["jiawei@ifmlab.org", "lmcui932@163.com", "fisherbgouza@gmail.com"], "authors": ["Jiawei Zhang", "Limeng Cui", "Fisher B. Gouza"], "TL;DR": "We introduce a new representation learning model, namely \u201cSample-Ensemble Genetic Evolutionary Network\u201d (SEGEN), which can serve as an alternative approach to deep learning models.", "pdf": "/pdf/72acae32af5730c669efa51298f7d276b3dbc41f.pdf", "paperhash": "zhang|segen_sampleensemble_genetic_evolutionary_network_model", "_bibtex": "@misc{\nzhang2019segen,\ntitle={{SEGEN}: {SAMPLE}-{ENSEMBLE} {GENETIC} {EVOLUTIONARY} {NETWORK} {MODEL}},\nauthor={Jiawei Zhang and Limeng Cui and Fisher B. Gouza},\nyear={2019},\nurl={https://openreview.net/forum?id=HJgVisRqtX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper613/Official_Review", "cdate": 1542234419676, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HJgVisRqtX", "replyto": "HJgVisRqtX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper613/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335763672, "tmdate": 1552335763672, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper613/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "rkxb8KF2hQ", "original": null, "number": 2, "cdate": 1541343561416, "ddate": null, "tcdate": 1541343561416, "tmdate": 1541533843203, "tddate": null, "forum": "HJgVisRqtX", "replyto": "HJgVisRqtX", "invitation": "ICLR.cc/2019/Conference/-/Paper613/Official_Review", "content": {"title": "Using Subsampling + Genetic Algorithm for Network Embedding", "review": "This paper proposes to subsample a large network into sub-networks, learn a network model (autoencoder) from each subgraph, perform crossover and mutation operations over the network parameters of different model pairs, and combine the latent representations following the ensemble idea.\n\nThe paper is clearly presented. Originality and significance is limited. Putting the three knowns components - subsampling, generation algorithm and ensembling together seems to be the main contribution of this paper. However, the ways of doing subsampling, performing the crossover and mutation operations and doing the ensembling are relatively straightforward ways of applying them. The fact that combining them to obtain better results is not a surprising result. And according to the experimental results, it is not clear how the gain in performance is resulted and to what extent each of the three components is contributing. For instance, I just guess the combination of subsampling + existing network embedding methods (LINE/DeepWalk/...) + ensembling may also give good results. Currently, the performance comparison is done with the original forms of LINE and DeepWalk. That makes the empirical results not very convincing to explain the key strengths of this work.\n\n+ve:\n`1. The paper is clearly presented.\n2. The design is reasonable one.\n3. A number of benchmark datasets are used for the evaluation.\n\n-ve:\n1. The originality and significance is limited.\n2. The performance comparison should be done with references to more competitive candidates as explained above.\n3. The nodes of different sub-networks are essentially projected to different embedding spaces. The validity and interpretation of performing the crossover operation on two different models (two different embedding spaces) will need more justifications.\n4. The proposed methodology is not an end-to-end. The ensembling being evaluated is just simple addition.\n5. The paper claims that \"The unit learning model, genetic algorithm and ensemble learning can all provide the theoretic foundation for SEGEN, which will lead to sound theoretic explanation of both the learning result and the SEGEN model itself\". Individually being sound does not imply that the way to combine them is sound. Currently, I cannot see the uniqueness of this particular combination.\n", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper613/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SEGEN: SAMPLE-ENSEMBLE GENETIC EVOLUTIONARY NETWORK MODEL", "abstract": "Deep learning, a rebranding of deep neural network research works, has achieved a remarkable success in recent years. With multiple hidden layers, deep learning models aim at computing the hierarchical feature representations of the observational data. Meanwhile, due to its severe disadvantages in data consumption, computational resources, parameter tuning costs and the lack of result explainability, deep learning has also suffered from lots of criticism. In this paper, we will introduce a new representation learning model, namely \u201cSample-Ensemble Genetic Evolutionary Network\u201d (SEGEN), which can serve as an alternative approach to deep learning models. Instead of building one single deep model, based on a set of sampled sub-instances, SEGEN adopts a genetic-evolutionary learning strategy to build a group of unit models generations by generations. The unit models incorporated in SEGEN can be either traditional machine learning models or the recent deep learning models with a much \u201cnarrower\u201d and \u201cshallower\u201d architecture. The learning results of each instance at the final generation will be effectively combined from each unit model via diffusive propagation and ensemble learning strategies. From the computational perspective, SEGEN requires far less data, fewer computational resources and parameter tuning efforts, but has sound theoretic interpretability of the learning process and results. Extensive experiments have been done on several different real-world benchmark datasets, and the experimental results obtained by SEGEN have demonstrated its advantages over the state-of-the-art representation learning models.", "keywords": ["Genetic Evolutionary Network", "Deep Learning", "Genetic Algorithm", "Ensemble Learning", "Representation Learning"], "authorids": ["jiawei@ifmlab.org", "lmcui932@163.com", "fisherbgouza@gmail.com"], "authors": ["Jiawei Zhang", "Limeng Cui", "Fisher B. Gouza"], "TL;DR": "We introduce a new representation learning model, namely \u201cSample-Ensemble Genetic Evolutionary Network\u201d (SEGEN), which can serve as an alternative approach to deep learning models.", "pdf": "/pdf/72acae32af5730c669efa51298f7d276b3dbc41f.pdf", "paperhash": "zhang|segen_sampleensemble_genetic_evolutionary_network_model", "_bibtex": "@misc{\nzhang2019segen,\ntitle={{SEGEN}: {SAMPLE}-{ENSEMBLE} {GENETIC} {EVOLUTIONARY} {NETWORK} {MODEL}},\nauthor={Jiawei Zhang and Limeng Cui and Fisher B. Gouza},\nyear={2019},\nurl={https://openreview.net/forum?id=HJgVisRqtX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper613/Official_Review", "cdate": 1542234419676, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HJgVisRqtX", "replyto": "HJgVisRqtX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper613/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335763672, "tmdate": 1552335763672, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper613/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 10}