{"notes": [{"id": "H1eiZnAqKm", "original": "SklIyVT5Ym", "number": 1203, "cdate": 1538087938890, "ddate": null, "tcdate": 1538087938890, "tmdate": 1545355412087, "tddate": null, "forum": "H1eiZnAqKm", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "The Expressive Power of Gated Recurrent Units as a Continuous Dynamical System", "abstract": "Gated recurrent units (GRUs) were inspired by the common gated recurrent unit, long short-term memory (LSTM), as a means of capturing temporal structure with less complex memory unit architecture. Despite their incredible success in tasks such as natural and artificial language processing, speech, video, and polyphonic music, very little is understood about the specific dynamic features representable in a GRU network. As a result, it is difficult to know a priori how successful a GRU-RNN will perform on a given data set. In this paper, we develop a new theoretical framework to analyze one and two dimensional GRUs as a continuous dynamical system, and classify the dynamical features obtainable with such system.\nWe found rich repertoire that includes stable limit cycles over time (nonlinear oscillations), multi-stable state transitions with various topologies, and homoclinic orbits. In addition, we show that any finite dimensional GRU cannot precisely replicate the dynamics of a ring attractor, or more generally, any continuous attractor, and is limited to finitely many isolated fixed points in theory. These findings were then experimentally verified in two dimensions by means of time series prediction.", "keywords": ["Gated Recurrent Units", "Recurrent Neural Network", "Time Series Predictions", "interpretable", "Nonlinear Dynamics", "Dynamical Systems"], "authorids": ["ian.jordan@stonybrook.edu", "piotr.sokol@stonybrook.edu", "memming.park@stonybrook.edu"], "authors": ["Ian D. Jordan", "Piotr Aleksander Sokol", "Il Memming Park"], "TL;DR": "We classify the the dynamical features one and two GRU cells can and cannot capture in continuous time, and verify our findings experimentally with k-step time series prediction. ", "pdf": "/pdf/d4e8cf48846cdb7f9018616de7481edb6f4e0e46.pdf", "paperhash": "jordan|the_expressive_power_of_gated_recurrent_units_as_a_continuous_dynamical_system", "_bibtex": "@misc{\njordan2019the,\ntitle={The Expressive Power of Gated Recurrent Units as a Continuous Dynamical System},\nauthor={Ian D. Jordan and Piotr Aleksander Sokol and Il Memming Park},\nyear={2019},\nurl={https://openreview.net/forum?id=H1eiZnAqKm},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "SJgDE630y4", "original": null, "number": 1, "cdate": 1544633646946, "ddate": null, "tcdate": 1544633646946, "tmdate": 1545354502376, "tddate": null, "forum": "H1eiZnAqKm", "replyto": "H1eiZnAqKm", "invitation": "ICLR.cc/2019/Conference/-/Paper1203/Meta_Review", "content": {"metareview": "The paper analyses GRUs using dynamic systems theory.  The paper is well-written and the theory seems to be solid.\n\nBut there is agreement amongst the reviewers that the application of the method might not scale well beyond rather simple 1- or 2-D GRUs (i.e., with one or two GRUs).  This limitation, which is an increasingly serious problem in machine-learning papers, should be solved before the paper should be published.  A very recent extension of the simulations to 16 GRUs improves this, but a rigorous analysis of higher-dimensional systems is pending and poses a considerable block for acceptance.", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Reject", "title": "scaling issue"}, "signatures": ["ICLR.cc/2019/Conference/Paper1203/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper1203/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Expressive Power of Gated Recurrent Units as a Continuous Dynamical System", "abstract": "Gated recurrent units (GRUs) were inspired by the common gated recurrent unit, long short-term memory (LSTM), as a means of capturing temporal structure with less complex memory unit architecture. Despite their incredible success in tasks such as natural and artificial language processing, speech, video, and polyphonic music, very little is understood about the specific dynamic features representable in a GRU network. As a result, it is difficult to know a priori how successful a GRU-RNN will perform on a given data set. In this paper, we develop a new theoretical framework to analyze one and two dimensional GRUs as a continuous dynamical system, and classify the dynamical features obtainable with such system.\nWe found rich repertoire that includes stable limit cycles over time (nonlinear oscillations), multi-stable state transitions with various topologies, and homoclinic orbits. In addition, we show that any finite dimensional GRU cannot precisely replicate the dynamics of a ring attractor, or more generally, any continuous attractor, and is limited to finitely many isolated fixed points in theory. These findings were then experimentally verified in two dimensions by means of time series prediction.", "keywords": ["Gated Recurrent Units", "Recurrent Neural Network", "Time Series Predictions", "interpretable", "Nonlinear Dynamics", "Dynamical Systems"], "authorids": ["ian.jordan@stonybrook.edu", "piotr.sokol@stonybrook.edu", "memming.park@stonybrook.edu"], "authors": ["Ian D. Jordan", "Piotr Aleksander Sokol", "Il Memming Park"], "TL;DR": "We classify the the dynamical features one and two GRU cells can and cannot capture in continuous time, and verify our findings experimentally with k-step time series prediction. ", "pdf": "/pdf/d4e8cf48846cdb7f9018616de7481edb6f4e0e46.pdf", "paperhash": "jordan|the_expressive_power_of_gated_recurrent_units_as_a_continuous_dynamical_system", "_bibtex": "@misc{\njordan2019the,\ntitle={The Expressive Power of Gated Recurrent Units as a Continuous Dynamical System},\nauthor={Ian D. Jordan and Piotr Aleksander Sokol and Il Memming Park},\nyear={2019},\nurl={https://openreview.net/forum?id=H1eiZnAqKm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1203/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545352926546, "tddate": null, "super": null, "final": null, "reply": {"forum": "H1eiZnAqKm", "replyto": "H1eiZnAqKm", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1203/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper1203/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1203/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545352926546}}}, {"id": "rJxfcnhT1V", "original": null, "number": 9, "cdate": 1544567945747, "ddate": null, "tcdate": 1544567945747, "tmdate": 1544567945747, "tddate": null, "forum": "H1eiZnAqKm", "replyto": "BJxemc5ayV", "invitation": "ICLR.cc/2019/Conference/-/Paper1203/Official_Comment", "content": {"title": "Re: Better now", "comment": "Thank you for the comment and upon thinking about it we agree that the reference to machine translation better be left out - we will remove this remark from the camera ready version of the manuscript. "}, "signatures": ["ICLR.cc/2019/Conference/Paper1203/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1203/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1203/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Expressive Power of Gated Recurrent Units as a Continuous Dynamical System", "abstract": "Gated recurrent units (GRUs) were inspired by the common gated recurrent unit, long short-term memory (LSTM), as a means of capturing temporal structure with less complex memory unit architecture. Despite their incredible success in tasks such as natural and artificial language processing, speech, video, and polyphonic music, very little is understood about the specific dynamic features representable in a GRU network. As a result, it is difficult to know a priori how successful a GRU-RNN will perform on a given data set. In this paper, we develop a new theoretical framework to analyze one and two dimensional GRUs as a continuous dynamical system, and classify the dynamical features obtainable with such system.\nWe found rich repertoire that includes stable limit cycles over time (nonlinear oscillations), multi-stable state transitions with various topologies, and homoclinic orbits. In addition, we show that any finite dimensional GRU cannot precisely replicate the dynamics of a ring attractor, or more generally, any continuous attractor, and is limited to finitely many isolated fixed points in theory. These findings were then experimentally verified in two dimensions by means of time series prediction.", "keywords": ["Gated Recurrent Units", "Recurrent Neural Network", "Time Series Predictions", "interpretable", "Nonlinear Dynamics", "Dynamical Systems"], "authorids": ["ian.jordan@stonybrook.edu", "piotr.sokol@stonybrook.edu", "memming.park@stonybrook.edu"], "authors": ["Ian D. Jordan", "Piotr Aleksander Sokol", "Il Memming Park"], "TL;DR": "We classify the the dynamical features one and two GRU cells can and cannot capture in continuous time, and verify our findings experimentally with k-step time series prediction. ", "pdf": "/pdf/d4e8cf48846cdb7f9018616de7481edb6f4e0e46.pdf", "paperhash": "jordan|the_expressive_power_of_gated_recurrent_units_as_a_continuous_dynamical_system", "_bibtex": "@misc{\njordan2019the,\ntitle={The Expressive Power of Gated Recurrent Units as a Continuous Dynamical System},\nauthor={Ian D. Jordan and Piotr Aleksander Sokol and Il Memming Park},\nyear={2019},\nurl={https://openreview.net/forum?id=H1eiZnAqKm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1203/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621605284, "tddate": null, "super": null, "final": null, "reply": {"forum": "H1eiZnAqKm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1203/Authors", "ICLR.cc/2019/Conference/Paper1203/Reviewers", "ICLR.cc/2019/Conference/Paper1203/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1203/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1203/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1203/Authors|ICLR.cc/2019/Conference/Paper1203/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1203/Reviewers", "ICLR.cc/2019/Conference/Paper1203/Authors", "ICLR.cc/2019/Conference/Paper1203/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621605284}}}, {"id": "HkxD98Zv2Q", "original": null, "number": 2, "cdate": 1540982414926, "ddate": null, "tcdate": 1540982414926, "tmdate": 1544559218508, "tddate": null, "forum": "H1eiZnAqKm", "replyto": "H1eiZnAqKm", "invitation": "ICLR.cc/2019/Conference/-/Paper1203/Official_Review", "content": {"title": "There are several issues with this interesting analysis ", "review": "The authors analyse GRUs with hidden sizes of one and two as continuous-time dynamical systems, claiming that the expressive power of the hidden state representation can provide prior knowledge on how well a GRU will perform on a given dataset. Their analysis shows what kind of hidden state dynamics the GRU can approximate in one and two dimensions. In the experimental part, they show how a GRU with two hidden states trained on a multistep prediction task can learn such dynamics.\n\nAlthough RNNs are important for Machine Learning, the paper seems to contain flaws in the theoretical part, which seem to invalidate some of the claimed results. But we may change our rating in case of a convincing rebuttal.\n\nThe Proof of Lemma 2 claims that h(t) achieves all values on the real set, which is false (h(t) assumes values in (-1,1)). Nevertheless, the theorem should hold since there is always at least one intersection between h and tanh(f(h)).\n\nLemma 1 claims that for any choice of parameters, there exist only finitely many fixed points. However, in the proof the authors only show that the number of fixed points cannot be uncountable, without taking into consideration the possibility that there are countably many fixed points. The proof also omits steps concerning the Taylor expansion which would make the proof clearer: We suggest adding those steps in the appendix. Furthermore, when equation (12) is Taylor-expanded, the authors do not consider the case where the GRU parameters are such that the argument of function \u201csech\u201d is outside its convergence radius. These might be parameters for which there are infinitely many fixed points, even if we are unable to provide a Taylor expansion. The Lemma may still be correct, but this does not seem to be a complete proof.\n\nThe authors claim that an arbitrarily close approximation of a line attractor can be created using two GRUs, but no proof is provided.\n\nThe experimental part is difficult to evaluate since there are no learning curves for the three tasks. For instance, it is difficult to judge whether the GRUs are unable to learn the dynamics of a ring attractor because of theoretical limitations or because the model has not been properly trained for the specific task.\n\nThe paper is easy to read, except for certain parts where it is not clear if some of the statements are true in general or just have not been proven false by the authors. It is not clear why Figure 3 is representing all possible simple fixed points and bifurcation fixed points: is there a theoretical result stating that these are the only possible topologies, or are these the only ones found? The same question applies for the 36 images in figure 9. The range of the parameters used for finding these configurations is not specified.\n\nSince the hidden state assumes values in (-1,1)^2, why is its range in most of the images (-1.5,1.5)?\n\nWe are not familiar with related work on transformations from discrete to continuous dynamical systems: are the dynamics of the discrete time GRU model preserved in the transformation? If so, is there a reference for this? Are the phase portraits in the middle row of figure 8 generated by letting the discrete GRU system evolve, or is the continuous system used with the parameters of the trained GRU?\n\nWe would like to see more explanations of why various topologies are useful for the applications mentioned in the paper. Given a generic dataset, how can these results help to understand how well a GRU will perform?\n\nWhat is the reason behind the belief that the analysis extends to higher dimensions? The effects of a 1D -> 2D extension are far from trivial - why should that be different for higher dimensions?\n\nThe problem the authors want to solve seems important, and some of the theoretical results are promising, but we think that this paper has to be further polished before acceptance.\n\nIt is possible that we will increase the score if the authors can provide clarifications on the above questions.\n\nAdditional comments:\n\nIntroduction\n\n-The vanishing gradient problem was not discovered in 1994, but in 1991 by Hochreiter: \n\nSepp Hochreiter. Untersuchungen zu dynamischen neuronalen Netzen. Diploma thesis, TU Munich, 1991. Advisor J. Schmidhuber.\n\n- Make clear that GRU is a variant of vanilla LSTM with forget gates (where one gate is missing):\n\nGers et al. \u201cLearning to Forget: Continual Prediction with LSTM.\u201c Neural Computation, 12(10):2451-2471, 2000. \n\n- The intro says that GRU has become widely popular and cites Britz et al 2017, but Britz et al actually show that LSTM consistently outperforms its variant GRU in Neural Machine Translation. Please clarify this. \n\n- Also mention Weiss et al (\u201cOn the Practical Computational Power of Finite Precision RNNs for Language Recognition\u201d) who exhibited basic limitations of GRU when compared to LSTM. \n\n- Is the result by Weiss et al actually related to the result of the authors who found that 2 GRUS cannot accurately a line attractor without near zero constant curvature in the phase space?\n\n\nSection 2\n\n-Wrong brackets in equation (4)\n-Missing bracket before citing Laurent & Brecht\n\nSection 4\n\n-\u201dWe conjecture that the system depicted in figure 2A..\u201d Should be figure 3A\n- Lemma1: UZ has capital Z subscript\n\nSection 5.2\n\n-\u201dThe added affine transformation allows for a sufficiently long subinterval\u201d: \u201csufficiently long\u201d is too vague\n\nSection 5.3\n\n\u201cA manifold with without near zero constant curvature\u201d: should be \u201ca manifold without near zero constant curvature\u201d\n\nAppendix A\n\n-Wrong brackets in equation (20)\n\nAppendix B\n\n- In the proof of Theorem 1, the derivative is of (29), not of (12)\n\nAppendix C\n\nFigure 9: \u201cwho\u2019s initial conditions\u201d should be \u201cwhose initial conditions\u201d\n\nAfter rebuttal:\n\nIt's better now. However, the revised introduction still says:  \"GRU has become wildly popular in the machine learning community thanks to its performance in machine translation (Britz et al., 2017) ... LSTM has been shown to outperform GRU on neural machine translation (Britz et al., 2017).... specifically unbounded counting, come easy to LSTM networks but not to GRU networks (Weiss et al., 2018).\" \n\nSo better remove the first statement on Britz et al: \"GRU has become wildly popular ... in machine translation (Britz et al., 2017)\" because they actually show why GRU is NOT wildly popular in machine translation, as correctly justified later in the same paragraph.\n\nPending the above revision, we'd like to increase our evaluation by 2 points, up to 6!\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1203/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "The Expressive Power of Gated Recurrent Units as a Continuous Dynamical System", "abstract": "Gated recurrent units (GRUs) were inspired by the common gated recurrent unit, long short-term memory (LSTM), as a means of capturing temporal structure with less complex memory unit architecture. Despite their incredible success in tasks such as natural and artificial language processing, speech, video, and polyphonic music, very little is understood about the specific dynamic features representable in a GRU network. As a result, it is difficult to know a priori how successful a GRU-RNN will perform on a given data set. In this paper, we develop a new theoretical framework to analyze one and two dimensional GRUs as a continuous dynamical system, and classify the dynamical features obtainable with such system.\nWe found rich repertoire that includes stable limit cycles over time (nonlinear oscillations), multi-stable state transitions with various topologies, and homoclinic orbits. In addition, we show that any finite dimensional GRU cannot precisely replicate the dynamics of a ring attractor, or more generally, any continuous attractor, and is limited to finitely many isolated fixed points in theory. These findings were then experimentally verified in two dimensions by means of time series prediction.", "keywords": ["Gated Recurrent Units", "Recurrent Neural Network", "Time Series Predictions", "interpretable", "Nonlinear Dynamics", "Dynamical Systems"], "authorids": ["ian.jordan@stonybrook.edu", "piotr.sokol@stonybrook.edu", "memming.park@stonybrook.edu"], "authors": ["Ian D. Jordan", "Piotr Aleksander Sokol", "Il Memming Park"], "TL;DR": "We classify the the dynamical features one and two GRU cells can and cannot capture in continuous time, and verify our findings experimentally with k-step time series prediction. ", "pdf": "/pdf/d4e8cf48846cdb7f9018616de7481edb6f4e0e46.pdf", "paperhash": "jordan|the_expressive_power_of_gated_recurrent_units_as_a_continuous_dynamical_system", "_bibtex": "@misc{\njordan2019the,\ntitle={The Expressive Power of Gated Recurrent Units as a Continuous Dynamical System},\nauthor={Ian D. Jordan and Piotr Aleksander Sokol and Il Memming Park},\nyear={2019},\nurl={https://openreview.net/forum?id=H1eiZnAqKm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1203/Official_Review", "cdate": 1542234281940, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "H1eiZnAqKm", "replyto": "H1eiZnAqKm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1203/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335895310, "tmdate": 1552335895310, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1203/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "rylOpfu5AQ", "original": null, "number": 6, "cdate": 1543303871649, "ddate": null, "tcdate": 1543303871649, "tmdate": 1543303871649, "tddate": null, "forum": "H1eiZnAqKm", "replyto": "H1l8eaK1hQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1203/Official_Comment", "content": {"title": "re: Review for the expressive power of GRUs as a continuous dynamical system. ", "comment": ">> Relevance of studying a continuous time version of the GRU\n\nRegardless of dimension, the discrete time GRU-RNN (any RNN or residual network for that matter) can be seen as a forward Euler discretization of an underlying continuous dynamical system, where the topology is dependent on the parameters. As such, there is a relation between the continuous system we derived, and the discrete dynamics of the GRU, which are trying to approximate it. This explanation has also been added to our manuscript.  \n\nMoreover, the continuous time limit of residual networks and recurrent neural networks has recently garnered substantial interest. While most of the interest was theoretically motivated, the recent popularization of Neural Ordinary Differential Equations [4], [5]shows the feasibility and usefulness of the continuous-time limit. Our work is highly relevant to these exciting developments and hopefully will provide a useful framework for analyzing the latent representations and, as mentioned in the general introduction, for analyzing the dynamics of the gradients with respect to parameters.\n\n>> \u201cIn order to show this major limitation of GRUs \u2026\u201d but then a 2-gru is used, which means that it\u2019s not a general problem for GRUs with higher dim, right? Also, won\u2019t approximate slow points would also be fine here? I think this language needs to be more heavily qualified.\n\nWe agree with this concern and have changed the language used in our manuscript to better express what is meant by a limitation, keeping in mind the use of low dimensional latent dynamics, and comparing the results to the more general GRU problem.\n\nThe use of a pseudo-line attractor depends on forcing the nullclines to be sufficiently close to one another in order to cause the arbitrarily slow flow in that subregion of phase space to dip below machine epsilon; a more general form of slow point.\n\nHowever, as a limitation, a general requirement to use approximate slow points in this system (given all functions are smooth, and all parameters can vary smoothly) is that there must exist a separate topology where the slow point in question is a saddle-node bifurcation fixed point (cite bifurcation theory), which limits the number of slow points that can exist for any specific set of parameters. \n\n>> GRU almost always refers to the network, even though it is Gated Recurrent Unit, this means that when you write \u2018two GRUs\u2019, the naive interpretation (to me) is that you are speaking about two networks and not a GRU network with two units.\n\nWe agree, and have made an explicit note on this point in our updated manuscript as to avoid confusion.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1203/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1203/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1203/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Expressive Power of Gated Recurrent Units as a Continuous Dynamical System", "abstract": "Gated recurrent units (GRUs) were inspired by the common gated recurrent unit, long short-term memory (LSTM), as a means of capturing temporal structure with less complex memory unit architecture. Despite their incredible success in tasks such as natural and artificial language processing, speech, video, and polyphonic music, very little is understood about the specific dynamic features representable in a GRU network. As a result, it is difficult to know a priori how successful a GRU-RNN will perform on a given data set. In this paper, we develop a new theoretical framework to analyze one and two dimensional GRUs as a continuous dynamical system, and classify the dynamical features obtainable with such system.\nWe found rich repertoire that includes stable limit cycles over time (nonlinear oscillations), multi-stable state transitions with various topologies, and homoclinic orbits. In addition, we show that any finite dimensional GRU cannot precisely replicate the dynamics of a ring attractor, or more generally, any continuous attractor, and is limited to finitely many isolated fixed points in theory. These findings were then experimentally verified in two dimensions by means of time series prediction.", "keywords": ["Gated Recurrent Units", "Recurrent Neural Network", "Time Series Predictions", "interpretable", "Nonlinear Dynamics", "Dynamical Systems"], "authorids": ["ian.jordan@stonybrook.edu", "piotr.sokol@stonybrook.edu", "memming.park@stonybrook.edu"], "authors": ["Ian D. Jordan", "Piotr Aleksander Sokol", "Il Memming Park"], "TL;DR": "We classify the the dynamical features one and two GRU cells can and cannot capture in continuous time, and verify our findings experimentally with k-step time series prediction. ", "pdf": "/pdf/d4e8cf48846cdb7f9018616de7481edb6f4e0e46.pdf", "paperhash": "jordan|the_expressive_power_of_gated_recurrent_units_as_a_continuous_dynamical_system", "_bibtex": "@misc{\njordan2019the,\ntitle={The Expressive Power of Gated Recurrent Units as a Continuous Dynamical System},\nauthor={Ian D. Jordan and Piotr Aleksander Sokol and Il Memming Park},\nyear={2019},\nurl={https://openreview.net/forum?id=H1eiZnAqKm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1203/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621605284, "tddate": null, "super": null, "final": null, "reply": {"forum": "H1eiZnAqKm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1203/Authors", "ICLR.cc/2019/Conference/Paper1203/Reviewers", "ICLR.cc/2019/Conference/Paper1203/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1203/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1203/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1203/Authors|ICLR.cc/2019/Conference/Paper1203/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1203/Reviewers", "ICLR.cc/2019/Conference/Paper1203/Authors", "ICLR.cc/2019/Conference/Paper1203/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621605284}}}, {"id": "r1g-jzu90m", "original": null, "number": 5, "cdate": 1543303832625, "ddate": null, "tcdate": 1543303832625, "tmdate": 1543303832625, "tddate": null, "forum": "H1eiZnAqKm", "replyto": "HkxD98Zv2Q", "invitation": "ICLR.cc/2019/Conference/-/Paper1203/Official_Comment", "content": {"title": "cnt'd", "comment": ">> We are not familiar with related work on transformations from discrete to continuous dynamical systems: are the dynamics of the discrete time GRU model preserved in the transformation? If so, is there a reference for this?\n\nA GRU-RNN (more generally any RNN or residual network) can be seen as a forward Euler discretization of an underlying continuous time dynamical system (see [4] and references within). As such, the discrete and continuous time systems have very similar forms, as the GRU is attempting to approximate the system we analyzed. The continuous-time dynamical systems framework preserves the smooth temporal structures and ignore the possible quirky/jumpy features of discrete maps which powers our analysis. However, generally speaking, the dynamic \nproperties are not always preserved when converting from discrete to continuous time. For example [3] showed that the 2D discrete GRU can exhibit chaos. However, a 2D continuous time dynamical system cannot show signs of chaos, a result of the Poincare-Bendixson theorem (J. Meiss. Differential Dynamical Systems).\n\n>> Are the phase portraits in the middle row of figure 8 generated by letting the discrete GRU system evolve, or is the continuous system used with the parameters of the trained GRU?\n\nThe dynamics shown in the middle row of figure 8 are those of the trained GRU on the continuous time system.\n\nThe reviewer then lists out a series of additional comments. We have gone through each individually and made the suggested corrections.\n\n>> Is the result by Weiss et al actually related to the result of the authors who found that 2 GRUS cannot accurately approximate a line attractor without near zero constant curvature in the phase space?\n\nYes, the mechanistic act of counting in the [Weiss et al.] paper using LSTMs has a continuous time analog to a line attractor, with a forcing term propelling the state parallel to the attractor. Since the GRU lacks an output gate, its hidden state acts as a hybrid between the LSTM\u2019s cell state and output state. As a result, the GRU hidden state must exist asymptotically on a compact set, which is not true for the LSTM cell state. This limitation is necessary in proving that the finite dimensional GRU cannot exhibit a line attractor.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1203/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1203/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1203/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Expressive Power of Gated Recurrent Units as a Continuous Dynamical System", "abstract": "Gated recurrent units (GRUs) were inspired by the common gated recurrent unit, long short-term memory (LSTM), as a means of capturing temporal structure with less complex memory unit architecture. Despite their incredible success in tasks such as natural and artificial language processing, speech, video, and polyphonic music, very little is understood about the specific dynamic features representable in a GRU network. As a result, it is difficult to know a priori how successful a GRU-RNN will perform on a given data set. In this paper, we develop a new theoretical framework to analyze one and two dimensional GRUs as a continuous dynamical system, and classify the dynamical features obtainable with such system.\nWe found rich repertoire that includes stable limit cycles over time (nonlinear oscillations), multi-stable state transitions with various topologies, and homoclinic orbits. In addition, we show that any finite dimensional GRU cannot precisely replicate the dynamics of a ring attractor, or more generally, any continuous attractor, and is limited to finitely many isolated fixed points in theory. These findings were then experimentally verified in two dimensions by means of time series prediction.", "keywords": ["Gated Recurrent Units", "Recurrent Neural Network", "Time Series Predictions", "interpretable", "Nonlinear Dynamics", "Dynamical Systems"], "authorids": ["ian.jordan@stonybrook.edu", "piotr.sokol@stonybrook.edu", "memming.park@stonybrook.edu"], "authors": ["Ian D. Jordan", "Piotr Aleksander Sokol", "Il Memming Park"], "TL;DR": "We classify the the dynamical features one and two GRU cells can and cannot capture in continuous time, and verify our findings experimentally with k-step time series prediction. ", "pdf": "/pdf/d4e8cf48846cdb7f9018616de7481edb6f4e0e46.pdf", "paperhash": "jordan|the_expressive_power_of_gated_recurrent_units_as_a_continuous_dynamical_system", "_bibtex": "@misc{\njordan2019the,\ntitle={The Expressive Power of Gated Recurrent Units as a Continuous Dynamical System},\nauthor={Ian D. Jordan and Piotr Aleksander Sokol and Il Memming Park},\nyear={2019},\nurl={https://openreview.net/forum?id=H1eiZnAqKm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1203/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621605284, "tddate": null, "super": null, "final": null, "reply": {"forum": "H1eiZnAqKm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1203/Authors", "ICLR.cc/2019/Conference/Paper1203/Reviewers", "ICLR.cc/2019/Conference/Paper1203/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1203/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1203/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1203/Authors|ICLR.cc/2019/Conference/Paper1203/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1203/Reviewers", "ICLR.cc/2019/Conference/Paper1203/Authors", "ICLR.cc/2019/Conference/Paper1203/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621605284}}}, {"id": "SygKuf_qAQ", "original": null, "number": 4, "cdate": 1543303793440, "ddate": null, "tcdate": 1543303793440, "tmdate": 1543303793440, "tddate": null, "forum": "H1eiZnAqKm", "replyto": "HkxD98Zv2Q", "invitation": "ICLR.cc/2019/Conference/-/Paper1203/Official_Comment", "content": {"title": "re:There are several issues with this interesting analysis", "comment": "Reviewer Specific Concerns:\n\n>> The Proof of Lemma 2 claims that h(t) achieves all values on the real set, which is false (h(t) assumes values in (-1,1)).\n\nWe\u2019ve reworked this proof to avoid previous confusion. h(t), while asymptotically bounded to (-1,1), can be initialized anywhere on the reals, whereby it will eventually fall into the designated trapping region. Since h(t) can be realized as line of unit slope, it is unbounded and bijective, obtaining all values on the reals.\n\n>> Lemma 1 does not seem like a complete proof.\n\nLemma 1 has been rewritten in its entirety as a means to (1) improve readability, and (2) include the countably infinite case. The authors believe this approach is better suited for the paper, as proof for the countable case is contained within the argument, and no information is left out, as Taylor series approximation is not used. Moreover, we have now extended the proof to arbitrary dimensions using differential geometry arguments. These are the new Theorem 1 and 2.\n\n>> The authors claim that an arbitrarily close approximation of a line attractor can be created using two GRUs, but no proof is provided.\n\nWe apologize for the confusion on this concern. We show by existence that a 2D GRU can approximate a straight (or nearly straight) line attractor. However, an arbitrary line attractor cannot be mimicked to machine precision. We\u2019ve adjusted the language of the paper to avoid misinterpretation.\n\n>> The experimental part is difficult to evaluate since there are no learning curves for the three tasks. For instance, it is difficult to judge whether the GRUs are unable to learn the dynamics of a ring attractor because of theoretical limitations or because the model has not been properly trained for the specific task.\n\nWe have extended our experiments to illustrate the inability of the 2D GRU to capture the dynamics of a ring attractor. We compared the k-step MSE as a function of number of epochs and as a function of latent dimensionality. We observe that for the ring attractor the MSE decreases as the latent dimensionality increases. On the other hand, the MSE for the FitzHugh-Nagumo does not decrease appreciably as the latent dimensionality increases.\n\n>> The paper is easy to read, except for certain parts where it is not clear if some of the statements are true in general or just have not been proven false by the authors. It is not clear why Figure 3 is representing all possible simple fixed points and bifurcation fixed points: is there a theoretical result stating that these are the only possible topologies, or are these the only ones found? The same question applies for the 36 images in figure 9. The range of the parameters used for finding these configurations is not specified.\n\nThank you for bringing our attention to this. Indeed our language was ambiguous at places. We have updated the language we use in order to make these points clearer. Generally speaking, the 1D analysis is exhaustive, as nothing beyond what we show exists. Figure 3 depicts all local structures found by the authors. Similarly, figure 9 depicts all global structures found by the authors.\nThe range of parameters was not specified as no set range was considered in discovering these structures. Rather, all structures were found by a combinatorial systematic procedure, by means of considering all possible ways the nullclines can intersect, given their geometric structure. We\u2019ve added an explanation of this method in the appendix.\n\n>> Since the hidden state assumes values in (-1,1)^2, why is its range in most of the images (-1.5,1.5)?\n\nWe agree that all interesting/practical behavior takes place in this bounded region in 2D. However, in section 2 it is stated that \u201cthe hidden state is asymptotically contained within [-1,1]^d.\u201d As a result, the hidden state only assumes values in (-1,1)^2 if and only if initialized within that region of phase space. The range of (-1.5,1.5) was used to better improve visualization of global dynamics, by including trajectories initialized outside the trapping region. This point is further emphasized in the updated paper.\n\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1203/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1203/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1203/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Expressive Power of Gated Recurrent Units as a Continuous Dynamical System", "abstract": "Gated recurrent units (GRUs) were inspired by the common gated recurrent unit, long short-term memory (LSTM), as a means of capturing temporal structure with less complex memory unit architecture. Despite their incredible success in tasks such as natural and artificial language processing, speech, video, and polyphonic music, very little is understood about the specific dynamic features representable in a GRU network. As a result, it is difficult to know a priori how successful a GRU-RNN will perform on a given data set. In this paper, we develop a new theoretical framework to analyze one and two dimensional GRUs as a continuous dynamical system, and classify the dynamical features obtainable with such system.\nWe found rich repertoire that includes stable limit cycles over time (nonlinear oscillations), multi-stable state transitions with various topologies, and homoclinic orbits. In addition, we show that any finite dimensional GRU cannot precisely replicate the dynamics of a ring attractor, or more generally, any continuous attractor, and is limited to finitely many isolated fixed points in theory. These findings were then experimentally verified in two dimensions by means of time series prediction.", "keywords": ["Gated Recurrent Units", "Recurrent Neural Network", "Time Series Predictions", "interpretable", "Nonlinear Dynamics", "Dynamical Systems"], "authorids": ["ian.jordan@stonybrook.edu", "piotr.sokol@stonybrook.edu", "memming.park@stonybrook.edu"], "authors": ["Ian D. Jordan", "Piotr Aleksander Sokol", "Il Memming Park"], "TL;DR": "We classify the the dynamical features one and two GRU cells can and cannot capture in continuous time, and verify our findings experimentally with k-step time series prediction. ", "pdf": "/pdf/d4e8cf48846cdb7f9018616de7481edb6f4e0e46.pdf", "paperhash": "jordan|the_expressive_power_of_gated_recurrent_units_as_a_continuous_dynamical_system", "_bibtex": "@misc{\njordan2019the,\ntitle={The Expressive Power of Gated Recurrent Units as a Continuous Dynamical System},\nauthor={Ian D. Jordan and Piotr Aleksander Sokol and Il Memming Park},\nyear={2019},\nurl={https://openreview.net/forum?id=H1eiZnAqKm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1203/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621605284, "tddate": null, "super": null, "final": null, "reply": {"forum": "H1eiZnAqKm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1203/Authors", "ICLR.cc/2019/Conference/Paper1203/Reviewers", "ICLR.cc/2019/Conference/Paper1203/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1203/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1203/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1203/Authors|ICLR.cc/2019/Conference/Paper1203/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1203/Reviewers", "ICLR.cc/2019/Conference/Paper1203/Authors", "ICLR.cc/2019/Conference/Paper1203/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621605284}}}, {"id": "S1xTRZu5AX", "original": null, "number": 3, "cdate": 1543303636795, "ddate": null, "tcdate": 1543303636795, "tmdate": 1543303636795, "tddate": null, "forum": "H1eiZnAqKm", "replyto": "BJlZ9PDCnm", "invitation": "ICLR.cc/2019/Conference/-/Paper1203/Official_Comment", "content": {"title": "re: a dynamical systems analysis of 1d and 2d gated recurrent units", "comment": "Reviewer Specific Concerns:\n\n>> Readability and validity of the continuous time system derivation.\n\nWe\u2019ve added several intermediate steps to the derivation (Appendix A) as a means of improving readability. Note that a GRU-RNN (any RNN or residual network for that matter) can be seen as a forward Euler discretization of an underlying continuous time dynamical system. Under this discretization, the derivative with respect to time appears on the right-hand side of the equation 23, implying both the continuous and discrete time systems are of the same functional form. Furthermore, to clarify the reason for the seemingly missing $\\delta t$, let us point out that the Euler discretization is valid for general ODEs with arbitrary step sizes. In the GRU it\u2019s implicitly assumed that $\\delta t = 1$, which is why the time step doesn\u2019t appear in the GRU update equation for $h_{t+1}$. In the derivation we made use of this fact, but failed to make it explicit. We amend it in the new version of the manuscript.\n\n>> Small typo on the top of page 4\nWe have corrected it.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1203/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1203/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1203/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Expressive Power of Gated Recurrent Units as a Continuous Dynamical System", "abstract": "Gated recurrent units (GRUs) were inspired by the common gated recurrent unit, long short-term memory (LSTM), as a means of capturing temporal structure with less complex memory unit architecture. Despite their incredible success in tasks such as natural and artificial language processing, speech, video, and polyphonic music, very little is understood about the specific dynamic features representable in a GRU network. As a result, it is difficult to know a priori how successful a GRU-RNN will perform on a given data set. In this paper, we develop a new theoretical framework to analyze one and two dimensional GRUs as a continuous dynamical system, and classify the dynamical features obtainable with such system.\nWe found rich repertoire that includes stable limit cycles over time (nonlinear oscillations), multi-stable state transitions with various topologies, and homoclinic orbits. In addition, we show that any finite dimensional GRU cannot precisely replicate the dynamics of a ring attractor, or more generally, any continuous attractor, and is limited to finitely many isolated fixed points in theory. These findings were then experimentally verified in two dimensions by means of time series prediction.", "keywords": ["Gated Recurrent Units", "Recurrent Neural Network", "Time Series Predictions", "interpretable", "Nonlinear Dynamics", "Dynamical Systems"], "authorids": ["ian.jordan@stonybrook.edu", "piotr.sokol@stonybrook.edu", "memming.park@stonybrook.edu"], "authors": ["Ian D. Jordan", "Piotr Aleksander Sokol", "Il Memming Park"], "TL;DR": "We classify the the dynamical features one and two GRU cells can and cannot capture in continuous time, and verify our findings experimentally with k-step time series prediction. ", "pdf": "/pdf/d4e8cf48846cdb7f9018616de7481edb6f4e0e46.pdf", "paperhash": "jordan|the_expressive_power_of_gated_recurrent_units_as_a_continuous_dynamical_system", "_bibtex": "@misc{\njordan2019the,\ntitle={The Expressive Power of Gated Recurrent Units as a Continuous Dynamical System},\nauthor={Ian D. Jordan and Piotr Aleksander Sokol and Il Memming Park},\nyear={2019},\nurl={https://openreview.net/forum?id=H1eiZnAqKm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1203/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621605284, "tddate": null, "super": null, "final": null, "reply": {"forum": "H1eiZnAqKm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1203/Authors", "ICLR.cc/2019/Conference/Paper1203/Reviewers", "ICLR.cc/2019/Conference/Paper1203/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1203/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1203/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1203/Authors|ICLR.cc/2019/Conference/Paper1203/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1203/Reviewers", "ICLR.cc/2019/Conference/Paper1203/Authors", "ICLR.cc/2019/Conference/Paper1203/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621605284}}}, {"id": "H1glN-d507", "original": null, "number": 2, "cdate": 1543303464401, "ddate": null, "tcdate": 1543303464401, "tmdate": 1543303464401, "tddate": null, "forum": "H1eiZnAqKm", "replyto": "H1eiZnAqKm", "invitation": "ICLR.cc/2019/Conference/-/Paper1203/Official_Comment", "content": {"title": "General response to all reviewers", "comment": "We thank the reviewers for their careful reading of our manuscript and many helpful suggestions. We are flattered that the reviewers found the manuscript well written and original.\n\nFirst, we would like to briefly emphasize the importance of our work. Originally, GRUs were designed to mitigate the difficulty of training recurrent neural networks on tasks with long temporal dependence. In their ingenious use of different gates both LSTMs and GRUs were believed to store information until it is needed at a later time. However, prior to our analysis, little was formally known about how hidden states store information in their dynamics. We extend this understanding by exhaustively listing the types of dynamics that GRU network can generate. These include stable limit cycles over time (nonlinear oscillations), multi-stable state transitions with various topologies, or generating stereotypical temporal responses to perturbations (homoclinic orbit). We were pleasantly surprised to discover this rich expressive power of the 2D GRU system. This was possible thanks to the continuous-time dynamical systems framework that allows us to focus on the smooth temporal structures and ignore the possible quirky/jumpy features of discrete maps.\n\nFurthermore, the analysis of 1-D and planar hidden state dynamics offers a new approach to the analysis of recurrent neural networks. Existing approaches have chose other simplifications, ranging from the analysis of linear dynamics to mean field approaches [1], [2]. The latter has extended our understanding of the dynamics of large, randomly initialized networks. The approach championed in this work, considers, in a sense, the opposite simplification to that used in the mean field analysis. Here we have considered networks with 1 or 2 neurons in the hidden layer, but have derived the classes of dynamics that these simple networks always fall into, both at random initialization and throughout training. We believe that our analysis will be helpful in the future in deepening our understanding of the learning dynamics since the backward pass (for backpropagating gradients) requires computing a linearization of the forward dynamics. The dynamical systems perspective is intimately connected to learning, since the stability of equilibria is measured with the eigenvalues of the same linearization. Therefore understanding the topological properties of the forward dynamics gives insight into the topology of learning  dynamics. In future work we hope to leverage this connection to better understand gradient dynamics during learning.\n\nMajor changes:\nWe replaced previous claims about point and line attractors to arbitrary dimensions. (Theorem 1 & 2)\nNew experiments showing learning curves for higher dimensional GRU networks (Fig. 9)\nOverall improvement in writing.\n\nReferences:\n[1]M. Hardt, T. Ma, and B. Recht, \u201cGradient Descent Learns Linear Dynamical Systems,\u201d J. Mach. Learn. Res., vol. 19, no. 29.\n[2]M. Chen, J. Pennington, and S. Schoenholz, \u201cDynamical Isometry and a Mean Field Theory of RNNs: Gating Enables Signal Propagation in Recurrent Neural Networks,\u201d in International Conference on Machine Learning, 2018, pp. 873\u2013882.\n[3]T. Laurent and J. von Brecht, \u201cA recurrent neural network without chaos,\u201d Nov. 2016.\n[4]R. T. Q. Chen, Y. Rubanova, J. Bettencourt, and D. Duvenaud, \u201cNeural Ordinary Differential Equations,\u201d ArXiv180607366 Cs Stat, Jun. 2018.\n[5]W. Grathwohl, R. T. Q. Chen, J. Bettencourt, I. Sutskever, and D. Duvenaud, \u201cFFJORD: Free-form Continuous Dynamics for Scalable Reversible Generative Models,\u201d ArXiv181001367 Cs Stat, Oct. 2018."}, "signatures": ["ICLR.cc/2019/Conference/Paper1203/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1203/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1203/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Expressive Power of Gated Recurrent Units as a Continuous Dynamical System", "abstract": "Gated recurrent units (GRUs) were inspired by the common gated recurrent unit, long short-term memory (LSTM), as a means of capturing temporal structure with less complex memory unit architecture. Despite their incredible success in tasks such as natural and artificial language processing, speech, video, and polyphonic music, very little is understood about the specific dynamic features representable in a GRU network. As a result, it is difficult to know a priori how successful a GRU-RNN will perform on a given data set. In this paper, we develop a new theoretical framework to analyze one and two dimensional GRUs as a continuous dynamical system, and classify the dynamical features obtainable with such system.\nWe found rich repertoire that includes stable limit cycles over time (nonlinear oscillations), multi-stable state transitions with various topologies, and homoclinic orbits. In addition, we show that any finite dimensional GRU cannot precisely replicate the dynamics of a ring attractor, or more generally, any continuous attractor, and is limited to finitely many isolated fixed points in theory. These findings were then experimentally verified in two dimensions by means of time series prediction.", "keywords": ["Gated Recurrent Units", "Recurrent Neural Network", "Time Series Predictions", "interpretable", "Nonlinear Dynamics", "Dynamical Systems"], "authorids": ["ian.jordan@stonybrook.edu", "piotr.sokol@stonybrook.edu", "memming.park@stonybrook.edu"], "authors": ["Ian D. Jordan", "Piotr Aleksander Sokol", "Il Memming Park"], "TL;DR": "We classify the the dynamical features one and two GRU cells can and cannot capture in continuous time, and verify our findings experimentally with k-step time series prediction. ", "pdf": "/pdf/d4e8cf48846cdb7f9018616de7481edb6f4e0e46.pdf", "paperhash": "jordan|the_expressive_power_of_gated_recurrent_units_as_a_continuous_dynamical_system", "_bibtex": "@misc{\njordan2019the,\ntitle={The Expressive Power of Gated Recurrent Units as a Continuous Dynamical System},\nauthor={Ian D. Jordan and Piotr Aleksander Sokol and Il Memming Park},\nyear={2019},\nurl={https://openreview.net/forum?id=H1eiZnAqKm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1203/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621605284, "tddate": null, "super": null, "final": null, "reply": {"forum": "H1eiZnAqKm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1203/Authors", "ICLR.cc/2019/Conference/Paper1203/Reviewers", "ICLR.cc/2019/Conference/Paper1203/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1203/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1203/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1203/Authors|ICLR.cc/2019/Conference/Paper1203/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1203/Reviewers", "ICLR.cc/2019/Conference/Paper1203/Authors", "ICLR.cc/2019/Conference/Paper1203/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621605284}}}, {"id": "BJlZ9PDCnm", "original": null, "number": 3, "cdate": 1541465993310, "ddate": null, "tcdate": 1541465993310, "tmdate": 1541533335823, "tddate": null, "forum": "H1eiZnAqKm", "replyto": "H1eiZnAqKm", "invitation": "ICLR.cc/2019/Conference/-/Paper1203/Official_Review", "content": {"title": "a dynamical systems analysis of 1d and 2d gated recurrent units", "review": "This paper analyzes GRUs from a dynamical systems perspective, i.e. phase diagrams, fixed points, and bifurcations. The abstract and intro are well written and motivate the need for more theoretical framework to understand RNNs, especially how well they are able to represent and express temporal features in the training data. The dynamical systems analysis is well presented and visualized nicely. \n\nMost of the paper concentrates on the 1d (one single GRU) and 2d (two GRU's) case.  They show that 2d GRUs can be trained to adopt a variety of fixed points, can approximate a line attractors (an important feature for short-term memory), but cannot mimic a ring attractor.\n\nMy concerns are:\n\n- The derivation of the continuous time dynamical system (Appendix A) is confusing to me. Unless I'm not following the derivation correctly, should there be another \\Delta t in the denominator of the right-hand side of (23), from (22)? It's confusing to me that the continuous-time version in (26) has essentially the same form as the discrete-time version in (22).\n\n- The applicability of this analysis to RNNs of even modest size is unclear. Generically, there's no reason to believe the intuitions from 2d should necessarily generalize to higher dimensions, and rigorous analysis of higher dimensional systems of this kind can be fairly challenging, even if one starts from a continuation analysis.\n\n- Small typo: top of Page 4, figure should refer to 3A, not 2A.\n\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1203/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Expressive Power of Gated Recurrent Units as a Continuous Dynamical System", "abstract": "Gated recurrent units (GRUs) were inspired by the common gated recurrent unit, long short-term memory (LSTM), as a means of capturing temporal structure with less complex memory unit architecture. Despite their incredible success in tasks such as natural and artificial language processing, speech, video, and polyphonic music, very little is understood about the specific dynamic features representable in a GRU network. As a result, it is difficult to know a priori how successful a GRU-RNN will perform on a given data set. In this paper, we develop a new theoretical framework to analyze one and two dimensional GRUs as a continuous dynamical system, and classify the dynamical features obtainable with such system.\nWe found rich repertoire that includes stable limit cycles over time (nonlinear oscillations), multi-stable state transitions with various topologies, and homoclinic orbits. In addition, we show that any finite dimensional GRU cannot precisely replicate the dynamics of a ring attractor, or more generally, any continuous attractor, and is limited to finitely many isolated fixed points in theory. These findings were then experimentally verified in two dimensions by means of time series prediction.", "keywords": ["Gated Recurrent Units", "Recurrent Neural Network", "Time Series Predictions", "interpretable", "Nonlinear Dynamics", "Dynamical Systems"], "authorids": ["ian.jordan@stonybrook.edu", "piotr.sokol@stonybrook.edu", "memming.park@stonybrook.edu"], "authors": ["Ian D. Jordan", "Piotr Aleksander Sokol", "Il Memming Park"], "TL;DR": "We classify the the dynamical features one and two GRU cells can and cannot capture in continuous time, and verify our findings experimentally with k-step time series prediction. ", "pdf": "/pdf/d4e8cf48846cdb7f9018616de7481edb6f4e0e46.pdf", "paperhash": "jordan|the_expressive_power_of_gated_recurrent_units_as_a_continuous_dynamical_system", "_bibtex": "@misc{\njordan2019the,\ntitle={The Expressive Power of Gated Recurrent Units as a Continuous Dynamical System},\nauthor={Ian D. Jordan and Piotr Aleksander Sokol and Il Memming Park},\nyear={2019},\nurl={https://openreview.net/forum?id=H1eiZnAqKm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1203/Official_Review", "cdate": 1542234281940, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "H1eiZnAqKm", "replyto": "H1eiZnAqKm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1203/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335895310, "tmdate": 1552335895310, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1203/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "H1l8eaK1hQ", "original": null, "number": 1, "cdate": 1540492526370, "ddate": null, "tcdate": 1540492526370, "tmdate": 1541533335358, "tddate": null, "forum": "H1eiZnAqKm", "replyto": "H1eiZnAqKm", "invitation": "ICLR.cc/2019/Conference/-/Paper1203/Official_Review", "content": {"title": "Review for the expressive power of GRUs as a continuous dynamical system.", "review": "Here the authors convert the GRU equations into continuous time and use theory and experiments to study 1- and 2-dimensional GRU networks. The authors showcase every variety of dynamical topology available in these systems and point out that the desirable line and ring attractors are not achievable, except in gross approximation.  The paper is extremely well written.\n\nI am deeply conflicted about this paper.  Is the analysis of 1 or 2 dimensional GRUs interesting or significant? That\u2019s a main question of this paper.  There is no question of quality, or clarity, and I am reasonably certain nobody has analyzed the GRU in this way before.\n\nOn the one hand, the authors bring a rigor and language to the discussion of recurrent networks that is both revealing (for these examples) and may to bear fruit in the future.  On the other hand, the paper is exclusively focused on 1- and 2-dimensional examples which have precisely no relevance to the recurrent neural networks as used and studied by machine learning practitioners and researchers, respectively. If the authors have proved something more general for higher dimensional (>2) cases, they should make it as clear as possible.\n \nA second, lesser question of relevance is studying a continuous time version.  It is my understanding that discrete time dynamics may exhibit significantly more complex dynamical phenomenon and again practitioners primarily deploy discrete time GRUs.  I understand that theoretical progress often requires retreating to lower dimensionality and (e.g. linearization, etc.) but in this case it is not clear to me that the end justifies the means.  On the other hand, a publication such as this will not only help to change the language of RNNs in the deep learning community, but also potentially bring in more dynamical systems specialists into the deep learning field, which I thoroughly endorse.\n\nModerate concern\n\n\u201cIn order to show this major limitation of GRUs \u2026\u201d but then a 2-gru is used, which means that it\u2019s not a general problem for GRUs with higher dim, right?  Also, won\u2019t approximate slow points would also be fine here? I think this language needs to be more heavily qualified.\n\nMinor\n\nGRU almost always refers to the network, even though it is Gated Recurrrent Unit, this means that when you write \u2018two GRUs\u2019, the naive interpretation (to me) is that you are speaking about two networks and not a GRU network with two units.\n\nSide note requiring no response: It might be interesting to study dynamical portrait as a function of training for the two-d GRU.\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1203/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Expressive Power of Gated Recurrent Units as a Continuous Dynamical System", "abstract": "Gated recurrent units (GRUs) were inspired by the common gated recurrent unit, long short-term memory (LSTM), as a means of capturing temporal structure with less complex memory unit architecture. Despite their incredible success in tasks such as natural and artificial language processing, speech, video, and polyphonic music, very little is understood about the specific dynamic features representable in a GRU network. As a result, it is difficult to know a priori how successful a GRU-RNN will perform on a given data set. In this paper, we develop a new theoretical framework to analyze one and two dimensional GRUs as a continuous dynamical system, and classify the dynamical features obtainable with such system.\nWe found rich repertoire that includes stable limit cycles over time (nonlinear oscillations), multi-stable state transitions with various topologies, and homoclinic orbits. In addition, we show that any finite dimensional GRU cannot precisely replicate the dynamics of a ring attractor, or more generally, any continuous attractor, and is limited to finitely many isolated fixed points in theory. These findings were then experimentally verified in two dimensions by means of time series prediction.", "keywords": ["Gated Recurrent Units", "Recurrent Neural Network", "Time Series Predictions", "interpretable", "Nonlinear Dynamics", "Dynamical Systems"], "authorids": ["ian.jordan@stonybrook.edu", "piotr.sokol@stonybrook.edu", "memming.park@stonybrook.edu"], "authors": ["Ian D. Jordan", "Piotr Aleksander Sokol", "Il Memming Park"], "TL;DR": "We classify the the dynamical features one and two GRU cells can and cannot capture in continuous time, and verify our findings experimentally with k-step time series prediction. ", "pdf": "/pdf/d4e8cf48846cdb7f9018616de7481edb6f4e0e46.pdf", "paperhash": "jordan|the_expressive_power_of_gated_recurrent_units_as_a_continuous_dynamical_system", "_bibtex": "@misc{\njordan2019the,\ntitle={The Expressive Power of Gated Recurrent Units as a Continuous Dynamical System},\nauthor={Ian D. Jordan and Piotr Aleksander Sokol and Il Memming Park},\nyear={2019},\nurl={https://openreview.net/forum?id=H1eiZnAqKm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1203/Official_Review", "cdate": 1542234281940, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "H1eiZnAqKm", "replyto": "H1eiZnAqKm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1203/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335895310, "tmdate": 1552335895310, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1203/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 11}