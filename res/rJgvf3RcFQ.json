{"notes": [{"id": "rJgvf3RcFQ", "original": "HklvwfC5YQ", "number": 1271, "cdate": 1538087950617, "ddate": null, "tcdate": 1538087950617, "tmdate": 1545355374583, "tddate": null, "forum": "rJgvf3RcFQ", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "On Inductive Biases in Deep Reinforcement Learning", "abstract": "Many deep reinforcement learning algorithms contain inductive biases that sculpt the agent's objective and its interface to the environment. These inductive biases can take many forms, including domain knowledge and pretuned hyper-parameters. In general, there is a trade-off between generality and performance when we use such biases. Stronger biases can lead to faster learning, but weaker biases can potentially lead to more general algorithms that work on a wider class of problems.\nThis trade-off is relevant because these inductive biases are not free; substantial effort may be required to obtain relevant domain knowledge or to tune hyper-parameters effectively. In this paper, we re-examine several domain-specific components that modify the agent's objective and environmental interface.  We investigated whether the performance deteriorates when all these fixed components are replaced with adaptive solutions from the literature.  In our experiments, performance sometimes decreased with the adaptive components, as one might expect when comparing to components crafted for the domain, but sometimes the adaptive components performed better. We then investigated the main benefit of having fewer domain-specific components, by comparing the learning performance of the two systems on a different set of continuous control problems, without additional tuning of either system.  As hypothesized, the system with adaptive components performed better on many of the tasks.", "keywords": [], "authorids": ["mtthss@google.com", "hado@google.com", "modayil@google.com", "davidsilver@google.com"], "authors": ["Matteo Hessel", "Hado van Hasselt", "Joseph Modayil", "David Silver"], "pdf": "/pdf/cb4e289ebf704d6b7d066a967dc219cc0a67a7f9.pdf", "paperhash": "hessel|on_inductive_biases_in_deep_reinforcement_learning", "_bibtex": "@misc{\nhessel2019on,\ntitle={On Inductive Biases in Deep Reinforcement Learning},\nauthor={Matteo Hessel and Hado van Hasselt and Joseph Modayil and David Silver},\nyear={2019},\nurl={https://openreview.net/forum?id=rJgvf3RcFQ},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "B1xQ0nIMl4", "original": null, "number": 1, "cdate": 1544871115351, "ddate": null, "tcdate": 1544871115351, "tmdate": 1545354533563, "tddate": null, "forum": "rJgvf3RcFQ", "replyto": "rJgvf3RcFQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1271/Meta_Review", "content": {"metareview": "The paper studies inductive biases in DRL, by comparing with different reward shaping, and curriculums. The authors performed comparative experiments where they replace domain specific heuristics by such adaptive components.\n\nThe paper includes very little (new) scientific contributions, and, as such, is not suitable for publication at ICLR.", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Reject", "title": "Not enough novel technical content nor insights"}, "signatures": ["ICLR.cc/2019/Conference/Paper1271/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper1271/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On Inductive Biases in Deep Reinforcement Learning", "abstract": "Many deep reinforcement learning algorithms contain inductive biases that sculpt the agent's objective and its interface to the environment. These inductive biases can take many forms, including domain knowledge and pretuned hyper-parameters. In general, there is a trade-off between generality and performance when we use such biases. Stronger biases can lead to faster learning, but weaker biases can potentially lead to more general algorithms that work on a wider class of problems.\nThis trade-off is relevant because these inductive biases are not free; substantial effort may be required to obtain relevant domain knowledge or to tune hyper-parameters effectively. In this paper, we re-examine several domain-specific components that modify the agent's objective and environmental interface.  We investigated whether the performance deteriorates when all these fixed components are replaced with adaptive solutions from the literature.  In our experiments, performance sometimes decreased with the adaptive components, as one might expect when comparing to components crafted for the domain, but sometimes the adaptive components performed better. We then investigated the main benefit of having fewer domain-specific components, by comparing the learning performance of the two systems on a different set of continuous control problems, without additional tuning of either system.  As hypothesized, the system with adaptive components performed better on many of the tasks.", "keywords": [], "authorids": ["mtthss@google.com", "hado@google.com", "modayil@google.com", "davidsilver@google.com"], "authors": ["Matteo Hessel", "Hado van Hasselt", "Joseph Modayil", "David Silver"], "pdf": "/pdf/cb4e289ebf704d6b7d066a967dc219cc0a67a7f9.pdf", "paperhash": "hessel|on_inductive_biases_in_deep_reinforcement_learning", "_bibtex": "@misc{\nhessel2019on,\ntitle={On Inductive Biases in Deep Reinforcement Learning},\nauthor={Matteo Hessel and Hado van Hasselt and Joseph Modayil and David Silver},\nyear={2019},\nurl={https://openreview.net/forum?id=rJgvf3RcFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1271/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545352900191, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJgvf3RcFQ", "replyto": "rJgvf3RcFQ", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1271/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper1271/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1271/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545352900191}}}, {"id": "H1eGa3Z5R7", "original": null, "number": 5, "cdate": 1543277754417, "ddate": null, "tcdate": 1543277754417, "tmdate": 1543277754417, "tddate": null, "forum": "rJgvf3RcFQ", "replyto": "SJx1tmP3pQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1271/Official_Comment", "content": {"title": "Did not misunderstand but also not a big factor in review", "comment": "Thanks for clarifying question: I did not misunderstand the empirical efforts. As you can see, my review mentions single \"environment\". I believe you conduct 57 games experiment in Arcade environment. Perhaps when I used Atari environment, it confused you.\n\nIn any case, the major issue of no substantial technical contributions and/or theoretical analysis still stands and hence my score also stands. With these two ingredients, the current empirical evaluation may (not necessarily) have been adequate but the paper needs more work in terms of contributions before it can be accepted."}, "signatures": ["ICLR.cc/2019/Conference/Paper1271/AnonReviewer3"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1271/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1271/AnonReviewer3", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On Inductive Biases in Deep Reinforcement Learning", "abstract": "Many deep reinforcement learning algorithms contain inductive biases that sculpt the agent's objective and its interface to the environment. These inductive biases can take many forms, including domain knowledge and pretuned hyper-parameters. In general, there is a trade-off between generality and performance when we use such biases. Stronger biases can lead to faster learning, but weaker biases can potentially lead to more general algorithms that work on a wider class of problems.\nThis trade-off is relevant because these inductive biases are not free; substantial effort may be required to obtain relevant domain knowledge or to tune hyper-parameters effectively. In this paper, we re-examine several domain-specific components that modify the agent's objective and environmental interface.  We investigated whether the performance deteriorates when all these fixed components are replaced with adaptive solutions from the literature.  In our experiments, performance sometimes decreased with the adaptive components, as one might expect when comparing to components crafted for the domain, but sometimes the adaptive components performed better. We then investigated the main benefit of having fewer domain-specific components, by comparing the learning performance of the two systems on a different set of continuous control problems, without additional tuning of either system.  As hypothesized, the system with adaptive components performed better on many of the tasks.", "keywords": [], "authorids": ["mtthss@google.com", "hado@google.com", "modayil@google.com", "davidsilver@google.com"], "authors": ["Matteo Hessel", "Hado van Hasselt", "Joseph Modayil", "David Silver"], "pdf": "/pdf/cb4e289ebf704d6b7d066a967dc219cc0a67a7f9.pdf", "paperhash": "hessel|on_inductive_biases_in_deep_reinforcement_learning", "_bibtex": "@misc{\nhessel2019on,\ntitle={On Inductive Biases in Deep Reinforcement Learning},\nauthor={Matteo Hessel and Hado van Hasselt and Joseph Modayil and David Silver},\nyear={2019},\nurl={https://openreview.net/forum?id=rJgvf3RcFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1271/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621625918, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJgvf3RcFQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1271/Authors", "ICLR.cc/2019/Conference/Paper1271/Reviewers", "ICLR.cc/2019/Conference/Paper1271/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1271/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1271/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1271/Authors|ICLR.cc/2019/Conference/Paper1271/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1271/Reviewers", "ICLR.cc/2019/Conference/Paper1271/Authors", "ICLR.cc/2019/Conference/Paper1271/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621625918}}}, {"id": "BJlyABDhaQ", "original": null, "number": 3, "cdate": 1542383047377, "ddate": null, "tcdate": 1542383047377, "tmdate": 1542383047377, "tddate": null, "forum": "rJgvf3RcFQ", "replyto": "SJlnow4qh7", "invitation": "ICLR.cc/2019/Conference/-/Paper1271/Official_Comment", "content": {"title": "Thanks for the comments and suggestions", "comment": "We thank the reviewer for the many positive comments. \nWe will add a figure for each of the 3 motivating examples in the Appendix, thanks for the suggestion!\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1271/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1271/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1271/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On Inductive Biases in Deep Reinforcement Learning", "abstract": "Many deep reinforcement learning algorithms contain inductive biases that sculpt the agent's objective and its interface to the environment. These inductive biases can take many forms, including domain knowledge and pretuned hyper-parameters. In general, there is a trade-off between generality and performance when we use such biases. Stronger biases can lead to faster learning, but weaker biases can potentially lead to more general algorithms that work on a wider class of problems.\nThis trade-off is relevant because these inductive biases are not free; substantial effort may be required to obtain relevant domain knowledge or to tune hyper-parameters effectively. In this paper, we re-examine several domain-specific components that modify the agent's objective and environmental interface.  We investigated whether the performance deteriorates when all these fixed components are replaced with adaptive solutions from the literature.  In our experiments, performance sometimes decreased with the adaptive components, as one might expect when comparing to components crafted for the domain, but sometimes the adaptive components performed better. We then investigated the main benefit of having fewer domain-specific components, by comparing the learning performance of the two systems on a different set of continuous control problems, without additional tuning of either system.  As hypothesized, the system with adaptive components performed better on many of the tasks.", "keywords": [], "authorids": ["mtthss@google.com", "hado@google.com", "modayil@google.com", "davidsilver@google.com"], "authors": ["Matteo Hessel", "Hado van Hasselt", "Joseph Modayil", "David Silver"], "pdf": "/pdf/cb4e289ebf704d6b7d066a967dc219cc0a67a7f9.pdf", "paperhash": "hessel|on_inductive_biases_in_deep_reinforcement_learning", "_bibtex": "@misc{\nhessel2019on,\ntitle={On Inductive Biases in Deep Reinforcement Learning},\nauthor={Matteo Hessel and Hado van Hasselt and Joseph Modayil and David Silver},\nyear={2019},\nurl={https://openreview.net/forum?id=rJgvf3RcFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1271/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621625918, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJgvf3RcFQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1271/Authors", "ICLR.cc/2019/Conference/Paper1271/Reviewers", "ICLR.cc/2019/Conference/Paper1271/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1271/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1271/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1271/Authors|ICLR.cc/2019/Conference/Paper1271/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1271/Reviewers", "ICLR.cc/2019/Conference/Paper1271/Authors", "ICLR.cc/2019/Conference/Paper1271/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621625918}}}, {"id": "HJg5KSw26Q", "original": null, "number": 2, "cdate": 1542382978169, "ddate": null, "tcdate": 1542382978169, "tmdate": 1542382978169, "tddate": null, "forum": "rJgvf3RcFQ", "replyto": "HJgVj1k327", "invitation": "ICLR.cc/2019/Conference/-/Paper1271/Official_Comment", "content": {"title": "On \"inductive biases\", \"generality\" and other questions.", "comment": "Some of the questions raised by the reviewer suggest that there may have been a misunderstanding of the term \u201cinductive bias\u201d, possibly interpreted as referring to some form of statistical bias. \u201cInductive Bias\u201d is a well defined concept from the Machine Learning and Neuroscience literature and refers to the set of assumptions that go into a learning system (such as domain knowledge and heuristics). In the context of this paper we define and classify the various types of inductive biases under consideration in Section 2.\n\nRegarding how to measure \"generality\": in this paper we propose to measure the \"generality\" of an RL algorithm as the degree to which such algorithm can be ported to a different domain from the one it was proposed for, without forcing the practitioner to revisit the inductive biases that were incorporated in the original agent. Our experiments on Continuous Control show that adaptive solutions perform better in this respect than other heuristic inductive biases. \n\nAs always, the Actor-Critic update in equation 2 of Section1 subsumes the tabular case, which can be seen by noting that in a tabular representation the gradient would only update the corresponding entry in the table.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1271/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1271/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1271/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On Inductive Biases in Deep Reinforcement Learning", "abstract": "Many deep reinforcement learning algorithms contain inductive biases that sculpt the agent's objective and its interface to the environment. These inductive biases can take many forms, including domain knowledge and pretuned hyper-parameters. In general, there is a trade-off between generality and performance when we use such biases. Stronger biases can lead to faster learning, but weaker biases can potentially lead to more general algorithms that work on a wider class of problems.\nThis trade-off is relevant because these inductive biases are not free; substantial effort may be required to obtain relevant domain knowledge or to tune hyper-parameters effectively. In this paper, we re-examine several domain-specific components that modify the agent's objective and environmental interface.  We investigated whether the performance deteriorates when all these fixed components are replaced with adaptive solutions from the literature.  In our experiments, performance sometimes decreased with the adaptive components, as one might expect when comparing to components crafted for the domain, but sometimes the adaptive components performed better. We then investigated the main benefit of having fewer domain-specific components, by comparing the learning performance of the two systems on a different set of continuous control problems, without additional tuning of either system.  As hypothesized, the system with adaptive components performed better on many of the tasks.", "keywords": [], "authorids": ["mtthss@google.com", "hado@google.com", "modayil@google.com", "davidsilver@google.com"], "authors": ["Matteo Hessel", "Hado van Hasselt", "Joseph Modayil", "David Silver"], "pdf": "/pdf/cb4e289ebf704d6b7d066a967dc219cc0a67a7f9.pdf", "paperhash": "hessel|on_inductive_biases_in_deep_reinforcement_learning", "_bibtex": "@misc{\nhessel2019on,\ntitle={On Inductive Biases in Deep Reinforcement Learning},\nauthor={Matteo Hessel and Hado van Hasselt and Joseph Modayil and David Silver},\nyear={2019},\nurl={https://openreview.net/forum?id=rJgvf3RcFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1271/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621625918, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJgvf3RcFQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1271/Authors", "ICLR.cc/2019/Conference/Paper1271/Reviewers", "ICLR.cc/2019/Conference/Paper1271/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1271/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1271/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1271/Authors|ICLR.cc/2019/Conference/Paper1271/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1271/Reviewers", "ICLR.cc/2019/Conference/Paper1271/Authors", "ICLR.cc/2019/Conference/Paper1271/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621625918}}}, {"id": "SJx1tmP3pQ", "original": null, "number": 1, "cdate": 1542382454629, "ddate": null, "tcdate": 1542382454629, "tmdate": 1542382454629, "tddate": null, "forum": "rJgvf3RcFQ", "replyto": "rJg_hF_TnQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1271/Official_Comment", "content": {"title": "On the empirical evidence provided in the paper", "comment": "In addition to 3 grid-world domains (designed specifically to highlight specific properties of the inductive biases considered in the paper), we also provide extensive experiments at scale on 57 Atari games and 28 continuous control tasks. This is a larger set of non-trivial environments than in the vast majority of deep RL papers. Perhaps the reviewer interpreted the Atari experiments (on 57 games) as having been run on a single Atari game?"}, "signatures": ["ICLR.cc/2019/Conference/Paper1271/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1271/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1271/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On Inductive Biases in Deep Reinforcement Learning", "abstract": "Many deep reinforcement learning algorithms contain inductive biases that sculpt the agent's objective and its interface to the environment. These inductive biases can take many forms, including domain knowledge and pretuned hyper-parameters. In general, there is a trade-off between generality and performance when we use such biases. Stronger biases can lead to faster learning, but weaker biases can potentially lead to more general algorithms that work on a wider class of problems.\nThis trade-off is relevant because these inductive biases are not free; substantial effort may be required to obtain relevant domain knowledge or to tune hyper-parameters effectively. In this paper, we re-examine several domain-specific components that modify the agent's objective and environmental interface.  We investigated whether the performance deteriorates when all these fixed components are replaced with adaptive solutions from the literature.  In our experiments, performance sometimes decreased with the adaptive components, as one might expect when comparing to components crafted for the domain, but sometimes the adaptive components performed better. We then investigated the main benefit of having fewer domain-specific components, by comparing the learning performance of the two systems on a different set of continuous control problems, without additional tuning of either system.  As hypothesized, the system with adaptive components performed better on many of the tasks.", "keywords": [], "authorids": ["mtthss@google.com", "hado@google.com", "modayil@google.com", "davidsilver@google.com"], "authors": ["Matteo Hessel", "Hado van Hasselt", "Joseph Modayil", "David Silver"], "pdf": "/pdf/cb4e289ebf704d6b7d066a967dc219cc0a67a7f9.pdf", "paperhash": "hessel|on_inductive_biases_in_deep_reinforcement_learning", "_bibtex": "@misc{\nhessel2019on,\ntitle={On Inductive Biases in Deep Reinforcement Learning},\nauthor={Matteo Hessel and Hado van Hasselt and Joseph Modayil and David Silver},\nyear={2019},\nurl={https://openreview.net/forum?id=rJgvf3RcFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1271/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621625918, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJgvf3RcFQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1271/Authors", "ICLR.cc/2019/Conference/Paper1271/Reviewers", "ICLR.cc/2019/Conference/Paper1271/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1271/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1271/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1271/Authors|ICLR.cc/2019/Conference/Paper1271/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1271/Reviewers", "ICLR.cc/2019/Conference/Paper1271/Authors", "ICLR.cc/2019/Conference/Paper1271/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621625918}}}, {"id": "rJg_hF_TnQ", "original": null, "number": 3, "cdate": 1541405103639, "ddate": null, "tcdate": 1541405103639, "tmdate": 1541533279501, "tddate": null, "forum": "rJgvf3RcFQ", "replyto": "rJgvf3RcFQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1271/Official_Review", "content": {"title": "Review for the paper: \"On Inductive Biases in Deep Reinforcement Learning\"", "review": "This paper focuses on deep reinforcement learning methods and discusses the presence of inductive biases in the existingRL algorithm. Specifically, they discuss biases that take the form of domain knowledge or hyper-parameter tuning. The authors state that such biases rise the tradeoff between generality and performance wherein strong biases can lead to efficient performance but deteriorate generalization across domains. Further, it motivates that most inductive biases has a cost associated to it and hence it is important to study and analyze the effect of such biases. \n\nTo support their insights, the authors investigate the performance of well known actor-critic model in the Atari environment after replacing domain specific heuristics with the adaptive components. The author considers two ways of injecting biases: i) sculpting agents objective and ii) sculpting agent's environment. They show empirical evidence that replacing carefully designed heuristics to induce biases with more adaptive counterparts preserves performance and generalizes without additional fine tuning.\n\nThe paper focuses on an important concept and problem of inductive biases in deep reinforcement learning techniques. \nAnalysis of such biases and methods to use them judiciously is an interesting future direction. The paper covers a lot of related work in terms of various algorithms and corresponding biases.\nHowever, this paper only discusses such concepts at high level and provides short empirical evidences in a single environment to support their arguments. Further, both the heuristics used in practice and the adaptive counterparts that the paper uses to replace those heuristics are all available in existing approaches and there is no novel contribution in that direction too.\nFinally, the adaptive methods based on parallel environment and RNNs have several limitation, as per author's own admission.\n\nOverall, the paper does not have any novel technical contributions or theoretical analysis on the effect of such inductive biases which makes it very weak. Further, there is nothing surprising about the author's claims and many of the outcomes from the analysis are expected. The authors are recommended to consider this task more rigorously and provide stronger and concrete analysis on the effects of inductive biases on variety of algorithms and variety of environments.\n\n\n\n", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1271/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On Inductive Biases in Deep Reinforcement Learning", "abstract": "Many deep reinforcement learning algorithms contain inductive biases that sculpt the agent's objective and its interface to the environment. These inductive biases can take many forms, including domain knowledge and pretuned hyper-parameters. In general, there is a trade-off between generality and performance when we use such biases. Stronger biases can lead to faster learning, but weaker biases can potentially lead to more general algorithms that work on a wider class of problems.\nThis trade-off is relevant because these inductive biases are not free; substantial effort may be required to obtain relevant domain knowledge or to tune hyper-parameters effectively. In this paper, we re-examine several domain-specific components that modify the agent's objective and environmental interface.  We investigated whether the performance deteriorates when all these fixed components are replaced with adaptive solutions from the literature.  In our experiments, performance sometimes decreased with the adaptive components, as one might expect when comparing to components crafted for the domain, but sometimes the adaptive components performed better. We then investigated the main benefit of having fewer domain-specific components, by comparing the learning performance of the two systems on a different set of continuous control problems, without additional tuning of either system.  As hypothesized, the system with adaptive components performed better on many of the tasks.", "keywords": [], "authorids": ["mtthss@google.com", "hado@google.com", "modayil@google.com", "davidsilver@google.com"], "authors": ["Matteo Hessel", "Hado van Hasselt", "Joseph Modayil", "David Silver"], "pdf": "/pdf/cb4e289ebf704d6b7d066a967dc219cc0a67a7f9.pdf", "paperhash": "hessel|on_inductive_biases_in_deep_reinforcement_learning", "_bibtex": "@misc{\nhessel2019on,\ntitle={On Inductive Biases in Deep Reinforcement Learning},\nauthor={Matteo Hessel and Hado van Hasselt and Joseph Modayil and David Silver},\nyear={2019},\nurl={https://openreview.net/forum?id=rJgvf3RcFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1271/Official_Review", "cdate": 1542234266548, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "rJgvf3RcFQ", "replyto": "rJgvf3RcFQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1271/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335910293, "tmdate": 1552335910293, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1271/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "HJgVj1k327", "original": null, "number": 2, "cdate": 1541300124272, "ddate": null, "tcdate": 1541300124272, "tmdate": 1541533279302, "tddate": null, "forum": "rJgvf3RcFQ", "replyto": "rJgvf3RcFQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1271/Official_Review", "content": {"title": "This paper contains various numerical experiments to see the effects of some heuristics in reinforcement learning, but no definite answers are given.", "review": "This paper contains various numerical experiments to see the effects of some heuristics in reinforcement learning. Those heuristics include reward clipping, discounting for effective learning, repeating actions, and different network structures. However, since the training algorithms also greatly affect the performance of RL agents, it seems hard to draw any quantitive conclusions from this paper.\n\nDetailed comments:\n\n1. It seems that actor-critic algorithms are defined for RL with function approximation. What is the tabular A2C algorithm? A reference in Section 3.1 would be better.\n\n2. This paper claims to study the \"inductive biases\", which is not clearly defined. How to quantify those biases and how to measure \"generality\"?\n\n3. Are there any quantitive conclusions that can be drawn from the experiments?\n\n4. Since the performance of RL agents also relies on initialization and the training algorithms. There are a lot of tricks of optimization for deep learning. How to measure the \"inductive biases\" by ruling out the effects of training algorithms?\n", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1271/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On Inductive Biases in Deep Reinforcement Learning", "abstract": "Many deep reinforcement learning algorithms contain inductive biases that sculpt the agent's objective and its interface to the environment. These inductive biases can take many forms, including domain knowledge and pretuned hyper-parameters. In general, there is a trade-off between generality and performance when we use such biases. Stronger biases can lead to faster learning, but weaker biases can potentially lead to more general algorithms that work on a wider class of problems.\nThis trade-off is relevant because these inductive biases are not free; substantial effort may be required to obtain relevant domain knowledge or to tune hyper-parameters effectively. In this paper, we re-examine several domain-specific components that modify the agent's objective and environmental interface.  We investigated whether the performance deteriorates when all these fixed components are replaced with adaptive solutions from the literature.  In our experiments, performance sometimes decreased with the adaptive components, as one might expect when comparing to components crafted for the domain, but sometimes the adaptive components performed better. We then investigated the main benefit of having fewer domain-specific components, by comparing the learning performance of the two systems on a different set of continuous control problems, without additional tuning of either system.  As hypothesized, the system with adaptive components performed better on many of the tasks.", "keywords": [], "authorids": ["mtthss@google.com", "hado@google.com", "modayil@google.com", "davidsilver@google.com"], "authors": ["Matteo Hessel", "Hado van Hasselt", "Joseph Modayil", "David Silver"], "pdf": "/pdf/cb4e289ebf704d6b7d066a967dc219cc0a67a7f9.pdf", "paperhash": "hessel|on_inductive_biases_in_deep_reinforcement_learning", "_bibtex": "@misc{\nhessel2019on,\ntitle={On Inductive Biases in Deep Reinforcement Learning},\nauthor={Matteo Hessel and Hado van Hasselt and Joseph Modayil and David Silver},\nyear={2019},\nurl={https://openreview.net/forum?id=rJgvf3RcFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1271/Official_Review", "cdate": 1542234266548, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "rJgvf3RcFQ", "replyto": "rJgvf3RcFQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1271/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335910293, "tmdate": 1552335910293, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1271/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "SJlnow4qh7", "original": null, "number": 1, "cdate": 1541191588372, "ddate": null, "tcdate": 1541191588372, "tmdate": 1541533279104, "tddate": null, "forum": "rJgvf3RcFQ", "replyto": "rJgvf3RcFQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1271/Official_Review", "content": {"title": "Good summary and experimental evaluation of various inducive biases in deep reinforcement learning", "review": "The paper presents and evaluates different common inductive biases in Deep RL. These are systematically evaluated on different experimental settings.\n\nThe paper is easy to read and the authors explain well the setting and their findings. The comparison and evaluations is well conducted and valuable contribution to the literature.  I would have liked some more details on the motivating example in section 3.1, maybe with a figure supporting the explanation of the example. ", "rating": "7: Good paper, accept", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2019/Conference/Paper1271/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On Inductive Biases in Deep Reinforcement Learning", "abstract": "Many deep reinforcement learning algorithms contain inductive biases that sculpt the agent's objective and its interface to the environment. These inductive biases can take many forms, including domain knowledge and pretuned hyper-parameters. In general, there is a trade-off between generality and performance when we use such biases. Stronger biases can lead to faster learning, but weaker biases can potentially lead to more general algorithms that work on a wider class of problems.\nThis trade-off is relevant because these inductive biases are not free; substantial effort may be required to obtain relevant domain knowledge or to tune hyper-parameters effectively. In this paper, we re-examine several domain-specific components that modify the agent's objective and environmental interface.  We investigated whether the performance deteriorates when all these fixed components are replaced with adaptive solutions from the literature.  In our experiments, performance sometimes decreased with the adaptive components, as one might expect when comparing to components crafted for the domain, but sometimes the adaptive components performed better. We then investigated the main benefit of having fewer domain-specific components, by comparing the learning performance of the two systems on a different set of continuous control problems, without additional tuning of either system.  As hypothesized, the system with adaptive components performed better on many of the tasks.", "keywords": [], "authorids": ["mtthss@google.com", "hado@google.com", "modayil@google.com", "davidsilver@google.com"], "authors": ["Matteo Hessel", "Hado van Hasselt", "Joseph Modayil", "David Silver"], "pdf": "/pdf/cb4e289ebf704d6b7d066a967dc219cc0a67a7f9.pdf", "paperhash": "hessel|on_inductive_biases_in_deep_reinforcement_learning", "_bibtex": "@misc{\nhessel2019on,\ntitle={On Inductive Biases in Deep Reinforcement Learning},\nauthor={Matteo Hessel and Hado van Hasselt and Joseph Modayil and David Silver},\nyear={2019},\nurl={https://openreview.net/forum?id=rJgvf3RcFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1271/Official_Review", "cdate": 1542234266548, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "rJgvf3RcFQ", "replyto": "rJgvf3RcFQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1271/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335910293, "tmdate": 1552335910293, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1271/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 9}