{"notes": [{"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396698591, "tcdate": 1486396698591, "number": 1, "id": "By7X6fUue", "invitation": "ICLR.cc/2017/conference/-/paper598/acceptance", "forum": "Byx5BTilg", "replyto": "Byx5BTilg", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision", "comment": "The reviewers unanimously recommend rejection."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Exploring the Application of Deep Learning for Supervised Learning Problems", "abstract": "One of the main difficulties in applying deep neural nets (DNNs) to new domains is the need to explore multiple architectures in order to discover ones that perform well. We analyze a large set of DNNs across multiple domains and derive insights regarding their effectiveness. We also analyze the characteristics of various DNNs and the general effect they may have on performance. Finally, we explore the application of meta-learning to the problem of architecture ranking. We demonstrate that by using topological features and modeling the changes in its weights, biases and activation functions layers of the initial training steps, we are able to rank architectures based on their predicted performance. We consider this work to be a first step in the important and challenging direction of exploring the space of different neural network architectures. \n", "pdf": "/pdf/2282dfd38ddb3862d97adcea4fdb685118cbb855.pdf", "TL;DR": "We explore the multiple DNN architectures on a large set of general supervised datasets. We also propose a meta-learning approach for DNN performance prediciton and ranking", "paperhash": "rozanec|exploring_the_application_of_deep_learning_for_supervised_learning_problems", "conflicts": ["berkeley.edu"], "keywords": ["Deep learning", "Supervised Learning"], "authors": ["Jose Rozanec", "Gilad Katz", "Eui Chul Richard Shin", "Dawn Song"], "authorids": ["jmrozanec@gmail.com", "giladk@berkeley.edu", "ricshin@berkeley.edu", "dawnsong@eecs.berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396699157, "id": "ICLR.cc/2017/conference/-/paper598/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "Byx5BTilg", "replyto": "Byx5BTilg", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396699157}}}, {"tddate": null, "tmdate": 1482182706824, "tcdate": 1482182706824, "number": 3, "id": "Bys4eCH4x", "invitation": "ICLR.cc/2017/conference/-/paper598/official/review", "forum": "Byx5BTilg", "replyto": "Byx5BTilg", "signatures": ["ICLR.cc/2017/conference/paper598/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper598/AnonReviewer3"], "content": {"title": "Not convincing", "rating": "4: Ok but not good enough - rejection", "review": "The topic is very interesting, but the paper is not convincing. Specifically, the experiment part is weak. The study should include datasets that are familiar to the community as well as the ones \"that are not often addressed by deep learning\". The comparison to other approaches is not comprehensive.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Exploring the Application of Deep Learning for Supervised Learning Problems", "abstract": "One of the main difficulties in applying deep neural nets (DNNs) to new domains is the need to explore multiple architectures in order to discover ones that perform well. We analyze a large set of DNNs across multiple domains and derive insights regarding their effectiveness. We also analyze the characteristics of various DNNs and the general effect they may have on performance. Finally, we explore the application of meta-learning to the problem of architecture ranking. We demonstrate that by using topological features and modeling the changes in its weights, biases and activation functions layers of the initial training steps, we are able to rank architectures based on their predicted performance. We consider this work to be a first step in the important and challenging direction of exploring the space of different neural network architectures. \n", "pdf": "/pdf/2282dfd38ddb3862d97adcea4fdb685118cbb855.pdf", "TL;DR": "We explore the multiple DNN architectures on a large set of general supervised datasets. We also propose a meta-learning approach for DNN performance prediciton and ranking", "paperhash": "rozanec|exploring_the_application_of_deep_learning_for_supervised_learning_problems", "conflicts": ["berkeley.edu"], "keywords": ["Deep learning", "Supervised Learning"], "authors": ["Jose Rozanec", "Gilad Katz", "Eui Chul Richard Shin", "Dawn Song"], "authorids": ["jmrozanec@gmail.com", "giladk@berkeley.edu", "ricshin@berkeley.edu", "dawnsong@eecs.berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512528430, "id": "ICLR.cc/2017/conference/-/paper598/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper598/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper598/AnonReviewer2", "ICLR.cc/2017/conference/paper598/AnonReviewer1", "ICLR.cc/2017/conference/paper598/AnonReviewer3"], "reply": {"forum": "Byx5BTilg", "replyto": "Byx5BTilg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper598/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper598/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512528430}}}, {"tddate": null, "tmdate": 1481925124734, "tcdate": 1481925124734, "number": 4, "id": "HJaWf1MNx", "invitation": "ICLR.cc/2017/conference/-/paper598/public/comment", "forum": "Byx5BTilg", "replyto": "rymsM8RXx", "signatures": ["~Gilad_Katz1"], "readers": ["everyone"], "writers": ["~Gilad_Katz1"], "content": {"title": "Response", "comment": "We thank the reviewer for his thoughtful comments. We would like to respond to some of the points raised in the review:\n\n1) \u201cthe actual details of the [ranking] classifier do not seem to be described in detail\u201d \n\n- as explained in section 6.3, we use the Random Forest algorithm as the ranking classifier. In Section 5 we provide a description of the meta-features categories used by our algorithm and in Appendix A we provide a comprehensive list of all the meta-features used by our ranking classifier and the way they were generated.\n\n2) \u201cOne notable comment from the paper is that the authors fix some important hyper-parameters for all the networks. I am of the opinion that optimizing the learning rate (and its decay schedule) is actually quite important. I hypothesize that a lot of the conclusions of this paper may change quite a bit if the authors did an actual search over the rates instead. I suspect that instead of training 11k nets, one can train 2k nets with 5 learning rates each and get a much better result that is actually compelling.\u201d \n\n- We agree with the reviewer that the absence of different parameter configurations is a shortcoming of the paper (and this is one of the main reasons for our description of this work as preliminary). Our reasoning in prioritizing a large set of architectures over multiple parameter settings is as follows:\na) Because of the relatively unexplored domain in which we run our experiments, we had little idea which type of architectures are likely to perform well. We therefore preferred to create a large and diverse set of architectures.\nb) We were interested in creating a sufficiently diverse set of architectures to ensure robustness when we trained the meta-model ranking classifier.\nc) One of the goals of this work was to compare the performance of serial and parallel architectures. For our results to be valid, we needed to evaluate a large number of both types.\n\nNonetheless, we acknowledge that the lack of multiple parameters is a highly important subject that we intend to address in future work.\t\n\n3) \u201cI am not convinced that the protocol for generating the various architectures is doing a good job at creating a diversity of architecture (simply because of the max depth of 8 layers and 14 components overall). I suspect that most of these generated architectures are actually almost identical performance-wise and that it\u2019s a waste to train so many of them on so many tasks\u201d \n\n- As shown in Figure 2 and Appendix D, the variance in the performance of the various architectures changes significantly across datasets. These results demonstrate that the architectures are not similar performance-wise. It is also worth pointing out that this large variance in performance distribution demonstrates the usefulness of the meta-learning ranking approach: using the ranker, one needs only evaluate a small number of architectures in order to obtain a top-performing architectures.\n\nIn order to further investigate the hypothesis raised by the reviewer, we analyzed all parent-child architectures and their differences in performance. Our conclusion is that adding even a single component can induce significant changes in accuracy. Our results have been added to the paper - please see tables 1 and 2.\n\nThat being said, we fully acknowledge the limitations of our work mentioned but the reviewer. As we state in the submitted paper, we consider this work a first step in a relatively unexplored domain.\n\n\n4) \u201cThe batch normalization experiments in Table 2 seem odd and under-explained. It is also well-known that the optimal learning rates when using batch norm vs. not using batch norm can differ by an order of magnitude so given the fixed learning rate throughout all experiments, I take these results with some grain of salt.\u201d \n\n- We were also surprised by these results. We updated the paper to indicate that we intend to explore this subject further in future work. (please note that table 2 is now table 4)\n\n5) \u201cI am not sure we got many insights into the kinds of architectures that ended up being at the top. Either visualizations, or trends (or both), would be great\u201d - \n\nPlease note that in Appendix C we provide both an analysis of the percentage of architectures with parallel layers in the 100 top-performing architectures (it is clear that the number is significant for all datasets) and also a figure presenting one of these architectures. In order to further investigate the point raised by the reviewer we have added an additional analysis of the components composition of the 100 top-performing architectures in each dataset. The most interesting point found in this analysis is that the relatively shallow networks (~4 FC layers) seem to achieve the best performance across all datasets. For space considerations, this information has also been added to Appendix C. We have updates the text of Section 6.2 accordingly.\n\n6) \u201cThis work seems to conflate the study of parallel vs. serial architectures with the study of meta learning, which are somewhat distinct issues.\u201d \n\n- We designed the meta-features to be applicable to both serial and parallel architectures, so that our meta-learning technique would remain agnostic to architectures structure. The comparison of serial vs. parallel architectures and the analysis of their performance is a separate subject studied in this paper. \n\n7) \u201cI take issue with the table that compares parallel vs. serial performance (table 2) simply because the right way would be to filter the architectures by the same number of parameters / capacity\u201d \n\n- The goal of the analysis in Table 2 was to evaluate the contribution of adding parallel layers to a given architecture. Given the fact that the ordering of the layers can be critical to the performance of almost every architectures, we believe the presented evaluation to be important. This assumption is supported by the results presented in Appendix C, where it is shown that parallel layers make up a large percentage of the 100 top-performing architectures for all datasets. \n\n8) \u201cUltimately the conclusion seems to be that when applying deep nets in a new domain, it is difficult to come up with a good architecture in advance. In that sense, it is hard to see the paper as a constructive result, because it\u2019s conclusions are that while the ranker may do a good job often-times, it\u2019s not that reliable. Thus I am not convinced that this particular result will be of practical use to folks who are intending to use deep nets for a new domain.\u201d \n\n- In the first version of the paper, the ranking approach achieved high performance for 7 out of 10 evaluated datasets. In the current version we have increased the number of datasets in our experiments from 10 to 13 and that led to an improvement in performance (the ranking classifier had more datasets to learn from). As shown in Table 4, we now outperform random sampling on ALL datasets, often by a large margin.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Exploring the Application of Deep Learning for Supervised Learning Problems", "abstract": "One of the main difficulties in applying deep neural nets (DNNs) to new domains is the need to explore multiple architectures in order to discover ones that perform well. We analyze a large set of DNNs across multiple domains and derive insights regarding their effectiveness. We also analyze the characteristics of various DNNs and the general effect they may have on performance. Finally, we explore the application of meta-learning to the problem of architecture ranking. We demonstrate that by using topological features and modeling the changes in its weights, biases and activation functions layers of the initial training steps, we are able to rank architectures based on their predicted performance. We consider this work to be a first step in the important and challenging direction of exploring the space of different neural network architectures. \n", "pdf": "/pdf/2282dfd38ddb3862d97adcea4fdb685118cbb855.pdf", "TL;DR": "We explore the multiple DNN architectures on a large set of general supervised datasets. We also propose a meta-learning approach for DNN performance prediciton and ranking", "paperhash": "rozanec|exploring_the_application_of_deep_learning_for_supervised_learning_problems", "conflicts": ["berkeley.edu"], "keywords": ["Deep learning", "Supervised Learning"], "authors": ["Jose Rozanec", "Gilad Katz", "Eui Chul Richard Shin", "Dawn Song"], "authorids": ["jmrozanec@gmail.com", "giladk@berkeley.edu", "ricshin@berkeley.edu", "dawnsong@eecs.berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287505965, "id": "ICLR.cc/2017/conference/-/paper598/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Byx5BTilg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper598/reviewers", "ICLR.cc/2017/conference/paper598/areachairs"], "cdate": 1485287505965}}}, {"tddate": null, "tmdate": 1481924826760, "tcdate": 1481924826760, "number": 3, "id": "H1QyZkGEe", "invitation": "ICLR.cc/2017/conference/-/paper598/public/comment", "forum": "Byx5BTilg", "replyto": "Bkm4q5eVe", "signatures": ["~Gilad_Katz1"], "readers": ["everyone"], "writers": ["~Gilad_Katz1"], "content": {"title": "Response", "comment": "We thank the reviewer for his thoughtful comments. We would like to respond to some of the points raised in the review:\n\n1. \u201cThey only explored really simple networks (feed-forward DNNs). While this significantly limited the search space, it also limited the value of the experiments. In fact, the best model architecture is highly task (domain) dependent and the type of model (DNN vs CNN vs LSTM) is often much more important than size of the network itself.\u201d \n\n- As we explain in Section 1, we explore the application of deep learning to tabular datasets (\u201cclassic\u201d machine learning problems). In this setting CNNs make little sense because there is no meaningful way of grouping the features together (unlike in image classification, for example) and there is no definition of proximity. The same reasoning applies to the use of RNNs. Because of this we focus our experiments on different activation functions and parallel architectures. \n\n2. \u201cTheir experiments were conducted with some important hyper parameters (e.g., learning rate schedule) fixed. However, it is well known  that learning rate often is the most important hyper parameter during training. Without adjusting these important hyper parameters the conclusion on the best model architecture is not convincing.\u201d  \n\n- We agree with the reviewer that the absence of different parameter configurations is a shortcoming of the paper (and this is one of the main reasons for our description of this work as preliminary). Our reasoning in prioritizing a large set of architectures over multiple parameter settings is as follows:\na) Because of the relatively unexplored domain in which we run our experiments, we had little idea which type of architectures are likely to perform well. We therefore preferred to create a large and diverse set of architectures.\nb) We were interested in creating a sufficiently diverse set of architectures to ensure robustness when we trained the meta-model ranking classifier.\nc) One of the goals of this work was to compare the performance of serial and parallel architectures. For our results to be valid, we needed to evaluate a large number of both types.\n\nNonetheless, we acknowledge that the lack of multiple parameters is a highly important subject that we intend to address in future work.\t\n\n3. \"Their experiments seem to indicate that the training data difference is not important. However, this is unlikely to be true as you would definitely want to use larger models (total number of parameters) when your training set is magnitude larger (i.e., log(datasize) can be an important feature). This is likely because they did not run experiments on large datasets.\"\n\n - We agree with the reviewer that the some of the datasets are not very large and we plan to address this issue in future work (hence our definition of this work as preliminary). Nonetheless, as shown in appendix B, the sizes of the datasets vary from 1,000 to over 45,000 data points. This diversity demonstrates the robustness of our meta learning-based ranking approach. Moreover, as shown in Table 3 (Table 1 in the version read by the reviewer) the performance of the architectures vary significantly from one dataset to another. This is demonstrated by the column \u201cArchitecture with best average ranking overall\u201d, where we show that simply choosing the architecture that performed best on average on other datasets yields inferior results.\n\n4. \u201cthis paper cited Sainath et al. 2015 as the work that leads to breakthrough in speech recognition. However, the breakthrough in ASR happened much earlier. The first paper with all three key components was published in 2010\u201d \n\n- We thank the reviewer for his suggestion. We have updated the paper accordingly.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Exploring the Application of Deep Learning for Supervised Learning Problems", "abstract": "One of the main difficulties in applying deep neural nets (DNNs) to new domains is the need to explore multiple architectures in order to discover ones that perform well. We analyze a large set of DNNs across multiple domains and derive insights regarding their effectiveness. We also analyze the characteristics of various DNNs and the general effect they may have on performance. Finally, we explore the application of meta-learning to the problem of architecture ranking. We demonstrate that by using topological features and modeling the changes in its weights, biases and activation functions layers of the initial training steps, we are able to rank architectures based on their predicted performance. We consider this work to be a first step in the important and challenging direction of exploring the space of different neural network architectures. \n", "pdf": "/pdf/2282dfd38ddb3862d97adcea4fdb685118cbb855.pdf", "TL;DR": "We explore the multiple DNN architectures on a large set of general supervised datasets. We also propose a meta-learning approach for DNN performance prediciton and ranking", "paperhash": "rozanec|exploring_the_application_of_deep_learning_for_supervised_learning_problems", "conflicts": ["berkeley.edu"], "keywords": ["Deep learning", "Supervised Learning"], "authors": ["Jose Rozanec", "Gilad Katz", "Eui Chul Richard Shin", "Dawn Song"], "authorids": ["jmrozanec@gmail.com", "giladk@berkeley.edu", "ricshin@berkeley.edu", "dawnsong@eecs.berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287505965, "id": "ICLR.cc/2017/conference/-/paper598/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Byx5BTilg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper598/reviewers", "ICLR.cc/2017/conference/paper598/areachairs"], "cdate": 1485287505965}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1481924671462, "tcdate": 1478378888218, "number": 598, "id": "Byx5BTilg", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "Byx5BTilg", "signatures": ["~Gilad_Katz1"], "readers": ["everyone"], "content": {"title": "Exploring the Application of Deep Learning for Supervised Learning Problems", "abstract": "One of the main difficulties in applying deep neural nets (DNNs) to new domains is the need to explore multiple architectures in order to discover ones that perform well. We analyze a large set of DNNs across multiple domains and derive insights regarding their effectiveness. We also analyze the characteristics of various DNNs and the general effect they may have on performance. Finally, we explore the application of meta-learning to the problem of architecture ranking. We demonstrate that by using topological features and modeling the changes in its weights, biases and activation functions layers of the initial training steps, we are able to rank architectures based on their predicted performance. We consider this work to be a first step in the important and challenging direction of exploring the space of different neural network architectures. \n", "pdf": "/pdf/2282dfd38ddb3862d97adcea4fdb685118cbb855.pdf", "TL;DR": "We explore the multiple DNN architectures on a large set of general supervised datasets. We also propose a meta-learning approach for DNN performance prediciton and ranking", "paperhash": "rozanec|exploring_the_application_of_deep_learning_for_supervised_learning_problems", "conflicts": ["berkeley.edu"], "keywords": ["Deep learning", "Supervised Learning"], "authors": ["Jose Rozanec", "Gilad Katz", "Eui Chul Richard Shin", "Dawn Song"], "authorids": ["jmrozanec@gmail.com", "giladk@berkeley.edu", "ricshin@berkeley.edu", "dawnsong@eecs.berkeley.edu"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "tmdate": 1481841194572, "tcdate": 1481841194567, "number": 2, "id": "Bkm4q5eVe", "invitation": "ICLR.cc/2017/conference/-/paper598/official/review", "forum": "Byx5BTilg", "replyto": "Byx5BTilg", "signatures": ["ICLR.cc/2017/conference/paper598/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper598/AnonReviewer1"], "content": {"title": "Interesting first step but not ready for publishing", "rating": "3: Clear rejection", "review": "This paper aims at attacking the problem of preselecting deep learning model structures for new domains. It reported a series of experiments on various small tasks and feed-forward DNNs. It claims that some ranking algorithm can be learned based on these results to guide the selection of model structures for new domains.\n\nAlthough the goal is interesting I found their conclusion is neither convincing nor useful in practice for several reasons:\n\n1. They only explored really simple networks (feed-forward DNNs). While this significantly limited the search space, it also limited the value of the experiments. In fact, the best model architecture is highly task (domain) dependent and the type of model (DNN vs CNN vs LSTM) is often much more important than size of the network itself.\n2. Their experiments were conduced with some important hyper parameters (e.g., learning rate schedule) fixed. However, it is well known  that learning rate often is the most important hyper parameter during training. Without adjusting these important hyper parameters the conclusion on the best model architecture is not convincing.\n3. Their experiments seem to indicate that the training data difference is not important. However, this is unlikely to be true as you would definitely want to use larger models (total number of parameters) when your training set is magnitude larger (i.e., log(datasize) can be an important feature). This is likely because they did not run experiments on large datasets.\n\nIn addition, I think the title of the paper does not accurately reflect what the paper is about and should be modified. Also, this paper cited Sainath et al. 2015 as the work that leads to breakthrough in speech recognition. However, the breakthrough in ASR happened much earlier. The first paper with all three key components was published in 2010:\n\nYu, D., Deng, L. and Dahl, G., 2010, December. Roles of pre-training and fine-tuning in context-dependent DBN-HMMs for real-world speech recognition. In Proc. NIPS Workshop on Deep Learning and Unsupervised Feature Learning.\n\nand the more detailed paper was published in 2012\n\nDahl, G.E., Yu, D., Deng, L. and Acero, A., 2012. Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition. IEEE Transactions on Audio, Speech, and Language Processing, 20(1), pp.30-42.\n\nAs a conclusion, this paper presented some very preliminary result. Although it's interesting it's not ready for publishing.", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Exploring the Application of Deep Learning for Supervised Learning Problems", "abstract": "One of the main difficulties in applying deep neural nets (DNNs) to new domains is the need to explore multiple architectures in order to discover ones that perform well. We analyze a large set of DNNs across multiple domains and derive insights regarding their effectiveness. We also analyze the characteristics of various DNNs and the general effect they may have on performance. Finally, we explore the application of meta-learning to the problem of architecture ranking. We demonstrate that by using topological features and modeling the changes in its weights, biases and activation functions layers of the initial training steps, we are able to rank architectures based on their predicted performance. We consider this work to be a first step in the important and challenging direction of exploring the space of different neural network architectures. \n", "pdf": "/pdf/2282dfd38ddb3862d97adcea4fdb685118cbb855.pdf", "TL;DR": "We explore the multiple DNN architectures on a large set of general supervised datasets. We also propose a meta-learning approach for DNN performance prediciton and ranking", "paperhash": "rozanec|exploring_the_application_of_deep_learning_for_supervised_learning_problems", "conflicts": ["berkeley.edu"], "keywords": ["Deep learning", "Supervised Learning"], "authors": ["Jose Rozanec", "Gilad Katz", "Eui Chul Richard Shin", "Dawn Song"], "authorids": ["jmrozanec@gmail.com", "giladk@berkeley.edu", "ricshin@berkeley.edu", "dawnsong@eecs.berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512528430, "id": "ICLR.cc/2017/conference/-/paper598/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper598/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper598/AnonReviewer2", "ICLR.cc/2017/conference/paper598/AnonReviewer1", "ICLR.cc/2017/conference/paper598/AnonReviewer3"], "reply": {"forum": "Byx5BTilg", "replyto": "Byx5BTilg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper598/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper598/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512528430}}}, {"tddate": null, "tmdate": 1481691803082, "tcdate": 1481691803076, "number": 1, "id": "rymsM8RXx", "invitation": "ICLR.cc/2017/conference/-/paper598/official/review", "forum": "Byx5BTilg", "replyto": "Byx5BTilg", "signatures": ["ICLR.cc/2017/conference/paper598/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper598/AnonReviewer2"], "content": {"title": "An interesting but somewhat underwhelming study", "rating": "5: Marginally below acceptance threshold", "review": "This paper presents an intriguing study of how one can pose architecture search as a meta learning problem. By collecting features from networks trained on various datasets and training a \u201cranking classifier\u201d (the actual details of the classifier do not seem to be described in detail) one can potentially infer what a good architecture for a new problem could be by simply running the ranker on the extracted features for a new problem setup.\n\nOne notable comment from the paper is that the authors fix some important hyper-parameters for all the networks. I am of the opinion that optimizing the learning rate (and its decay schedule) is actually quite important. I hypothesize that a lot of the conclusions of this paper may change quite a bit if the authors did an actual search over the rates instead. I suspect that instead of training 11k nets, one can train 2k nets with 5 learning rates each and get a much better result that is actually compelling.\n\nI am not convinced that the protocol for generating the various architectures is doing a good job at creating a diversity of architecture (simply because of the max depth of 8 layers and 14 components overall). I suspect that most of these generated architectures are actually almost identical performance-wise and that it\u2019s a waste to train so many of them on so many tasks. Unless the authors are already doing this, they should define a pruning mechanism that filters out nets that are too similar to already existing ones.\n\nThe batch normalization experiments in Table 2 seem odd and under-explained. It is also well-known that the optimal learning rates when using batch norm vs. not using batch norm can differ by an order of magnitude so given the fixed learning rate throughout all experiments, I take these results with some grain of salt.\n\nI am not sure we got many insights into the kinds of architectures that ended up being at the top. Either visualizations, or trends (or both), would be great.\n\nThis work seems to conflate the study of parallel vs. serial architectures with the study of meta learning, which are somewhat distinct issues. I take issue with the table that compares parallel vs. serial performance (table 2) simply because the right way would be to filter the architectures by the same number of parameters / capacity.\n\nUltimately the conclusion seems to be that when applying deep nets in a new domain, it is difficult to come up with a good architecture in advance. In that sense, it is hard to see the paper as a constructive result, because it\u2019s conclusions are that while the ranker may do a good job often-times, it\u2019s not that reliable. Thus I am not convinced that this particular result will be of practical use to folks who are intending to use deep nets for a new domain.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Exploring the Application of Deep Learning for Supervised Learning Problems", "abstract": "One of the main difficulties in applying deep neural nets (DNNs) to new domains is the need to explore multiple architectures in order to discover ones that perform well. We analyze a large set of DNNs across multiple domains and derive insights regarding their effectiveness. We also analyze the characteristics of various DNNs and the general effect they may have on performance. Finally, we explore the application of meta-learning to the problem of architecture ranking. We demonstrate that by using topological features and modeling the changes in its weights, biases and activation functions layers of the initial training steps, we are able to rank architectures based on their predicted performance. We consider this work to be a first step in the important and challenging direction of exploring the space of different neural network architectures. \n", "pdf": "/pdf/2282dfd38ddb3862d97adcea4fdb685118cbb855.pdf", "TL;DR": "We explore the multiple DNN architectures on a large set of general supervised datasets. We also propose a meta-learning approach for DNN performance prediciton and ranking", "paperhash": "rozanec|exploring_the_application_of_deep_learning_for_supervised_learning_problems", "conflicts": ["berkeley.edu"], "keywords": ["Deep learning", "Supervised Learning"], "authors": ["Jose Rozanec", "Gilad Katz", "Eui Chul Richard Shin", "Dawn Song"], "authorids": ["jmrozanec@gmail.com", "giladk@berkeley.edu", "ricshin@berkeley.edu", "dawnsong@eecs.berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512528430, "id": "ICLR.cc/2017/conference/-/paper598/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper598/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper598/AnonReviewer2", "ICLR.cc/2017/conference/paper598/AnonReviewer1", "ICLR.cc/2017/conference/paper598/AnonReviewer3"], "reply": {"forum": "Byx5BTilg", "replyto": "Byx5BTilg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper598/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper598/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512528430}}}, {"tddate": null, "tmdate": 1481677165026, "tcdate": 1481677165016, "number": 2, "id": "HkrdKGC7l", "invitation": "ICLR.cc/2017/conference/-/paper598/public/comment", "forum": "Byx5BTilg", "replyto": "H1oC5q_7g", "signatures": ["~Gilad_Katz1"], "readers": ["everyone"], "writers": ["~Gilad_Katz1"], "content": {"title": "Response", "comment": "Thank you for the question. You are indeed correct in your understanding that we do not use convolusions or RNNs. In section 4 we state the components used in our generated architectures:  \u201cfully-connected layers, softmax, batch normalization, dropout and the ReLU, sigmoid and tanh activation functions\u201d. \n\nOur reason for not including components such as convolusions in our architecture is that it does not \u201cmake sense\u201d to apply them to the types of problems that we analyze. Convolusions are only relevant in domains such as image classification, where the grouping of features based on their proximity is logical. This is not the case for general classification problems on tabular datasets (such as determining credit risk, for example). We intentionally chose these domains because there is little available knowledge on designing DNNs for them.\n\nAs we state in Section 6.1 the maximal number of components in each of our generated architectures is 14. While this depth is not close to that of state-of-the-art networks used in fields such as image classification, we have been able to obtain results that are close to (and often exceed) the performance of the well-known ensemble algorithms Random Forest (see Table 1). Another reason for limiting the depth of the architectures is our exploration of wider architectures with parallel layers (see Figure 1) which significantly increased the number of possible architectures for each increase in depth. \n\nFinally, we would like to stress again (as we have done in the paper) that this is a preliminary work whose goal is to explore the application of DL to relatively little explored domains and to begin to evaluate our proposed meta-learning approach. We intend to experiment with additional types of networks and larger depths in future work.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Exploring the Application of Deep Learning for Supervised Learning Problems", "abstract": "One of the main difficulties in applying deep neural nets (DNNs) to new domains is the need to explore multiple architectures in order to discover ones that perform well. We analyze a large set of DNNs across multiple domains and derive insights regarding their effectiveness. We also analyze the characteristics of various DNNs and the general effect they may have on performance. Finally, we explore the application of meta-learning to the problem of architecture ranking. We demonstrate that by using topological features and modeling the changes in its weights, biases and activation functions layers of the initial training steps, we are able to rank architectures based on their predicted performance. We consider this work to be a first step in the important and challenging direction of exploring the space of different neural network architectures. \n", "pdf": "/pdf/2282dfd38ddb3862d97adcea4fdb685118cbb855.pdf", "TL;DR": "We explore the multiple DNN architectures on a large set of general supervised datasets. We also propose a meta-learning approach for DNN performance prediciton and ranking", "paperhash": "rozanec|exploring_the_application_of_deep_learning_for_supervised_learning_problems", "conflicts": ["berkeley.edu"], "keywords": ["Deep learning", "Supervised Learning"], "authors": ["Jose Rozanec", "Gilad Katz", "Eui Chul Richard Shin", "Dawn Song"], "authorids": ["jmrozanec@gmail.com", "giladk@berkeley.edu", "ricshin@berkeley.edu", "dawnsong@eecs.berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287505965, "id": "ICLR.cc/2017/conference/-/paper598/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Byx5BTilg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper598/reviewers", "ICLR.cc/2017/conference/paper598/areachairs"], "cdate": 1485287505965}}}, {"tddate": null, "tmdate": 1481317075168, "tcdate": 1481317075161, "number": 2, "id": "H1oC5q_7g", "invitation": "ICLR.cc/2017/conference/-/paper598/pre-review/question", "forum": "Byx5BTilg", "replyto": "Byx5BTilg", "signatures": ["ICLR.cc/2017/conference/paper598/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper598/AnonReviewer3"], "content": {"title": "Scope and Limitation", "question": "Does the paper target any kind of DNN including CNN, RNN, GAN, NTM, or any combinations of them? I think at this stage the paper addresses rather small-layered vanilla MLPs. Is this correct?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Exploring the Application of Deep Learning for Supervised Learning Problems", "abstract": "One of the main difficulties in applying deep neural nets (DNNs) to new domains is the need to explore multiple architectures in order to discover ones that perform well. We analyze a large set of DNNs across multiple domains and derive insights regarding their effectiveness. We also analyze the characteristics of various DNNs and the general effect they may have on performance. Finally, we explore the application of meta-learning to the problem of architecture ranking. We demonstrate that by using topological features and modeling the changes in its weights, biases and activation functions layers of the initial training steps, we are able to rank architectures based on their predicted performance. We consider this work to be a first step in the important and challenging direction of exploring the space of different neural network architectures. \n", "pdf": "/pdf/2282dfd38ddb3862d97adcea4fdb685118cbb855.pdf", "TL;DR": "We explore the multiple DNN architectures on a large set of general supervised datasets. We also propose a meta-learning approach for DNN performance prediciton and ranking", "paperhash": "rozanec|exploring_the_application_of_deep_learning_for_supervised_learning_problems", "conflicts": ["berkeley.edu"], "keywords": ["Deep learning", "Supervised Learning"], "authors": ["Jose Rozanec", "Gilad Katz", "Eui Chul Richard Shin", "Dawn Song"], "authorids": ["jmrozanec@gmail.com", "giladk@berkeley.edu", "ricshin@berkeley.edu", "dawnsong@eecs.berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481317075767, "id": "ICLR.cc/2017/conference/-/paper598/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper598/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper598/AnonReviewer2", "ICLR.cc/2017/conference/paper598/AnonReviewer3"], "reply": {"forum": "Byx5BTilg", "replyto": "Byx5BTilg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper598/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper598/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481317075767}}}, {"tddate": null, "tmdate": 1480975210316, "tcdate": 1480975210306, "number": 1, "id": "Bkf_7v7Xx", "invitation": "ICLR.cc/2017/conference/-/paper598/public/comment", "forum": "Byx5BTilg", "replyto": "rJupzHJme", "signatures": ["~Gilad_Katz1"], "readers": ["everyone"], "writers": ["~Gilad_Katz1"], "content": {"title": "Response", "comment": "We are not aware of any publications that evaluated DNNs on these datasets. As we explain in Section 1, one of the problems we are attempting to begin to address is the application of deep learning to domains for which there is little or no existing knowledge on the application of deep learning. We therefore chose a type of problems that is not commonly addressed.\n\nIn order to enable the reader to evaluate the performance of our meta-learning approach we do the following: 1) Compare the performance of the best architecture found by our approach (in the ranked top 10) to the best architecture overall; 2) To provide perspective on the performance compared to other classification methods, we present the results obtained by running the Rotation Forest algorithm, a popular ensemble algorithm that is often applied to these types of problems (we used the Weka ML framework with default parameters in order to ensure reproducibility). These results are presented in Table 1.\n\nIt should also be noted that the goal of the meta-learning (in the scope this work, at least) is not to outperform a specific baseline but rather to successfully identify the top-performing architectures in a given set. For this reason we also compare the performance of our approach to random sampling (Table 4) and calculate the precision@K for various points (Table 3)."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Exploring the Application of Deep Learning for Supervised Learning Problems", "abstract": "One of the main difficulties in applying deep neural nets (DNNs) to new domains is the need to explore multiple architectures in order to discover ones that perform well. We analyze a large set of DNNs across multiple domains and derive insights regarding their effectiveness. We also analyze the characteristics of various DNNs and the general effect they may have on performance. Finally, we explore the application of meta-learning to the problem of architecture ranking. We demonstrate that by using topological features and modeling the changes in its weights, biases and activation functions layers of the initial training steps, we are able to rank architectures based on their predicted performance. We consider this work to be a first step in the important and challenging direction of exploring the space of different neural network architectures. \n", "pdf": "/pdf/2282dfd38ddb3862d97adcea4fdb685118cbb855.pdf", "TL;DR": "We explore the multiple DNN architectures on a large set of general supervised datasets. We also propose a meta-learning approach for DNN performance prediciton and ranking", "paperhash": "rozanec|exploring_the_application_of_deep_learning_for_supervised_learning_problems", "conflicts": ["berkeley.edu"], "keywords": ["Deep learning", "Supervised Learning"], "authors": ["Jose Rozanec", "Gilad Katz", "Eui Chul Richard Shin", "Dawn Song"], "authorids": ["jmrozanec@gmail.com", "giladk@berkeley.edu", "ricshin@berkeley.edu", "dawnsong@eecs.berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287505965, "id": "ICLR.cc/2017/conference/-/paper598/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Byx5BTilg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper598/reviewers", "ICLR.cc/2017/conference/paper598/areachairs"], "cdate": 1485287505965}}}, {"tddate": null, "tmdate": 1480704703798, "tcdate": 1480704703793, "number": 1, "id": "rJupzHJme", "invitation": "ICLR.cc/2017/conference/-/paper598/pre-review/question", "forum": "Byx5BTilg", "replyto": "Byx5BTilg", "signatures": ["ICLR.cc/2017/conference/paper598/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper598/AnonReviewer2"], "content": {"title": "comparisons with other methods?", "question": "For the datasets used, would be nice to understand if the proposed meta learning method actually achieves  good results compared to what's out there (e.g. other papers that evaluate on the same data)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Exploring the Application of Deep Learning for Supervised Learning Problems", "abstract": "One of the main difficulties in applying deep neural nets (DNNs) to new domains is the need to explore multiple architectures in order to discover ones that perform well. We analyze a large set of DNNs across multiple domains and derive insights regarding their effectiveness. We also analyze the characteristics of various DNNs and the general effect they may have on performance. Finally, we explore the application of meta-learning to the problem of architecture ranking. We demonstrate that by using topological features and modeling the changes in its weights, biases and activation functions layers of the initial training steps, we are able to rank architectures based on their predicted performance. We consider this work to be a first step in the important and challenging direction of exploring the space of different neural network architectures. \n", "pdf": "/pdf/2282dfd38ddb3862d97adcea4fdb685118cbb855.pdf", "TL;DR": "We explore the multiple DNN architectures on a large set of general supervised datasets. We also propose a meta-learning approach for DNN performance prediciton and ranking", "paperhash": "rozanec|exploring_the_application_of_deep_learning_for_supervised_learning_problems", "conflicts": ["berkeley.edu"], "keywords": ["Deep learning", "Supervised Learning"], "authors": ["Jose Rozanec", "Gilad Katz", "Eui Chul Richard Shin", "Dawn Song"], "authorids": ["jmrozanec@gmail.com", "giladk@berkeley.edu", "ricshin@berkeley.edu", "dawnsong@eecs.berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481317075767, "id": "ICLR.cc/2017/conference/-/paper598/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper598/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper598/AnonReviewer2", "ICLR.cc/2017/conference/paper598/AnonReviewer3"], "reply": {"forum": "Byx5BTilg", "replyto": "Byx5BTilg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper598/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper598/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481317075767}}}], "count": 11}