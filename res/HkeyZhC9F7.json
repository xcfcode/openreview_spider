{"notes": [{"id": "HkeyZhC9F7", "original": "rJx_Yj15KQ", "number": 1133, "cdate": 1538087927163, "ddate": null, "tcdate": 1538087927163, "tmdate": 1545355435387, "tddate": null, "forum": "HkeyZhC9F7", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Learning Heuristics for Automated Reasoning through Reinforcement Learning", "abstract": "We demonstrate how to learn efficient heuristics for automated reasoning algorithms through deep reinforcement learning. We focus on backtracking search algorithms for quantified Boolean logics, which already can solve formulas of impressive size - up to 100s of thousands of variables. The main challenge is to find a representation of these formulas that lends itself to making predictions in a scalable way. For challenging problems, the heuristic learned through our approach reduces execution time by a factor of 10 compared to the existing handwritten heuristics.", "keywords": ["reinforcement learning", "deep learning", "logics", "formal methods", "automated reasoning", "backtracking search", "satisfiability", "quantified Boolean formulas"], "authorids": ["gilled@berkeley.edu", "markus.norman.rabe@gmail.com", "eal@berkeley.edu", "sseshia@eecs.berkeley.edu"], "authors": ["Gil Lederman", "Markus N. Rabe", "Edward A. Lee", "Sanjit A. Seshia"], "TL;DR": "RL finds better heuristics for automated reasoning algorithms.", "pdf": "/pdf/0cd39db88fadedf14c8a841fd359f62a53297a39.pdf", "paperhash": "lederman|learning_heuristics_for_automated_reasoning_through_reinforcement_learning", "_bibtex": "@misc{\nlederman2019learning,\ntitle={Learning Heuristics for Automated Reasoning through Reinforcement Learning},\nauthor={Gil Lederman and Markus N. Rabe and Edward A. Lee and Sanjit A. Seshia},\nyear={2019},\nurl={https://openreview.net/forum?id=HkeyZhC9F7},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "r1gcvUSlxV", "original": null, "number": 1, "cdate": 1544734305711, "ddate": null, "tcdate": 1544734305711, "tmdate": 1545354481452, "tddate": null, "forum": "HkeyZhC9F7", "replyto": "HkeyZhC9F7", "invitation": "ICLR.cc/2019/Conference/-/Paper1133/Meta_Review", "content": {"metareview": "The paper proposes the use of reinforcement learning to learn heuristics in backtracking search algorithm for quantified boolean formulas, using a neural network to learn a suitable representation of literals and clauses to predict actions. The writing and the description of the method and results are generally clear. The main novelty lies in finding a good architecture/representation of the input, and demonstrating the use of RL in a new domain. While there is no theoretical justification for why this heuristic should work better than existing ones, the experimental results look convincing, although they are somewhat limited and the improvements are dataset dependent. In practice, the overhead of the proposed method could be an issue. There was some disagreement among the reviewers as to whether the improvements and the results are significant enough for publication.", "confidence": "3: The area chair is somewhat confident", "recommendation": "Reject", "title": "Borderline paper"}, "signatures": ["ICLR.cc/2019/Conference/Paper1133/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper1133/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Heuristics for Automated Reasoning through Reinforcement Learning", "abstract": "We demonstrate how to learn efficient heuristics for automated reasoning algorithms through deep reinforcement learning. We focus on backtracking search algorithms for quantified Boolean logics, which already can solve formulas of impressive size - up to 100s of thousands of variables. The main challenge is to find a representation of these formulas that lends itself to making predictions in a scalable way. For challenging problems, the heuristic learned through our approach reduces execution time by a factor of 10 compared to the existing handwritten heuristics.", "keywords": ["reinforcement learning", "deep learning", "logics", "formal methods", "automated reasoning", "backtracking search", "satisfiability", "quantified Boolean formulas"], "authorids": ["gilled@berkeley.edu", "markus.norman.rabe@gmail.com", "eal@berkeley.edu", "sseshia@eecs.berkeley.edu"], "authors": ["Gil Lederman", "Markus N. Rabe", "Edward A. Lee", "Sanjit A. Seshia"], "TL;DR": "RL finds better heuristics for automated reasoning algorithms.", "pdf": "/pdf/0cd39db88fadedf14c8a841fd359f62a53297a39.pdf", "paperhash": "lederman|learning_heuristics_for_automated_reasoning_through_reinforcement_learning", "_bibtex": "@misc{\nlederman2019learning,\ntitle={Learning Heuristics for Automated Reasoning through Reinforcement Learning},\nauthor={Gil Lederman and Markus N. Rabe and Edward A. Lee and Sanjit A. Seshia},\nyear={2019},\nurl={https://openreview.net/forum?id=HkeyZhC9F7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1133/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545352952810, "tddate": null, "super": null, "final": null, "reply": {"forum": "HkeyZhC9F7", "replyto": "HkeyZhC9F7", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1133/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper1133/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1133/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545352952810}}}, {"id": "BJlrwT-cR7", "original": null, "number": 7, "cdate": 1543277917381, "ddate": null, "tcdate": 1543277917381, "tmdate": 1543277917381, "tddate": null, "forum": "HkeyZhC9F7", "replyto": "Skx240VFpQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1133/Official_Comment", "content": {"title": "Additional experiments", "comment": "As promised, we added additional experiments to the appendix. Please find them in Appendix E and Appendix F."}, "signatures": ["ICLR.cc/2019/Conference/Paper1133/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1133/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1133/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Heuristics for Automated Reasoning through Reinforcement Learning", "abstract": "We demonstrate how to learn efficient heuristics for automated reasoning algorithms through deep reinforcement learning. We focus on backtracking search algorithms for quantified Boolean logics, which already can solve formulas of impressive size - up to 100s of thousands of variables. The main challenge is to find a representation of these formulas that lends itself to making predictions in a scalable way. For challenging problems, the heuristic learned through our approach reduces execution time by a factor of 10 compared to the existing handwritten heuristics.", "keywords": ["reinforcement learning", "deep learning", "logics", "formal methods", "automated reasoning", "backtracking search", "satisfiability", "quantified Boolean formulas"], "authorids": ["gilled@berkeley.edu", "markus.norman.rabe@gmail.com", "eal@berkeley.edu", "sseshia@eecs.berkeley.edu"], "authors": ["Gil Lederman", "Markus N. Rabe", "Edward A. Lee", "Sanjit A. Seshia"], "TL;DR": "RL finds better heuristics for automated reasoning algorithms.", "pdf": "/pdf/0cd39db88fadedf14c8a841fd359f62a53297a39.pdf", "paperhash": "lederman|learning_heuristics_for_automated_reasoning_through_reinforcement_learning", "_bibtex": "@misc{\nlederman2019learning,\ntitle={Learning Heuristics for Automated Reasoning through Reinforcement Learning},\nauthor={Gil Lederman and Markus N. Rabe and Edward A. Lee and Sanjit A. Seshia},\nyear={2019},\nurl={https://openreview.net/forum?id=HkeyZhC9F7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1133/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621607142, "tddate": null, "super": null, "final": null, "reply": {"forum": "HkeyZhC9F7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1133/Authors", "ICLR.cc/2019/Conference/Paper1133/Reviewers", "ICLR.cc/2019/Conference/Paper1133/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1133/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1133/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1133/Authors|ICLR.cc/2019/Conference/Paper1133/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1133/Reviewers", "ICLR.cc/2019/Conference/Paper1133/Authors", "ICLR.cc/2019/Conference/Paper1133/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621607142}}}, {"id": "Hke2JfcoT7", "original": null, "number": 5, "cdate": 1542328803989, "ddate": null, "tcdate": 1542328803989, "tmdate": 1542328803989, "tddate": null, "forum": "HkeyZhC9F7", "replyto": "ryxELVAF6m", "invitation": "ICLR.cc/2019/Conference/-/Paper1133/Official_Comment", "content": {"title": "Thanks for the interesting pointer!", "comment": "We were not aware of this work, and will discuss it in our related work section. There are several key differences compared to our work: Khalil et al. present an approach to learn to predict an existing heuristic called SB using SVMs, while we attempt to learn an entirely new heuristics using deep reinforcement learning. Further, they learn within a single run of the solver, while we learn from executions on a set of formulas. In some sense, the approaches are quite orthogonal, and not necessarily competing against each other. It is unclear to us, if there is a meaningful way to compare the methods experimentally.\n\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1133/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1133/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1133/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Heuristics for Automated Reasoning through Reinforcement Learning", "abstract": "We demonstrate how to learn efficient heuristics for automated reasoning algorithms through deep reinforcement learning. We focus on backtracking search algorithms for quantified Boolean logics, which already can solve formulas of impressive size - up to 100s of thousands of variables. The main challenge is to find a representation of these formulas that lends itself to making predictions in a scalable way. For challenging problems, the heuristic learned through our approach reduces execution time by a factor of 10 compared to the existing handwritten heuristics.", "keywords": ["reinforcement learning", "deep learning", "logics", "formal methods", "automated reasoning", "backtracking search", "satisfiability", "quantified Boolean formulas"], "authorids": ["gilled@berkeley.edu", "markus.norman.rabe@gmail.com", "eal@berkeley.edu", "sseshia@eecs.berkeley.edu"], "authors": ["Gil Lederman", "Markus N. Rabe", "Edward A. Lee", "Sanjit A. Seshia"], "TL;DR": "RL finds better heuristics for automated reasoning algorithms.", "pdf": "/pdf/0cd39db88fadedf14c8a841fd359f62a53297a39.pdf", "paperhash": "lederman|learning_heuristics_for_automated_reasoning_through_reinforcement_learning", "_bibtex": "@misc{\nlederman2019learning,\ntitle={Learning Heuristics for Automated Reasoning through Reinforcement Learning},\nauthor={Gil Lederman and Markus N. Rabe and Edward A. Lee and Sanjit A. Seshia},\nyear={2019},\nurl={https://openreview.net/forum?id=HkeyZhC9F7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1133/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621607142, "tddate": null, "super": null, "final": null, "reply": {"forum": "HkeyZhC9F7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1133/Authors", "ICLR.cc/2019/Conference/Paper1133/Reviewers", "ICLR.cc/2019/Conference/Paper1133/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1133/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1133/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1133/Authors|ICLR.cc/2019/Conference/Paper1133/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1133/Reviewers", "ICLR.cc/2019/Conference/Paper1133/Authors", "ICLR.cc/2019/Conference/Paper1133/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621607142}}}, {"id": "ryxELVAF6m", "original": null, "number": 4, "cdate": 1542214731879, "ddate": null, "tcdate": 1542214731879, "tmdate": 1542214731879, "tddate": null, "forum": "HkeyZhC9F7", "replyto": "Skx240VFpQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1133/Official_Comment", "content": {"title": "Related Work", "comment": "The most relevant paper to the work you're proposing here is probably \"Learning to Branch in Mixed Integer Programming\", https://dl.acm.org/citation.cfm?id=3015920."}, "signatures": ["ICLR.cc/2019/Conference/Paper1133/AnonReviewer1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1133/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1133/AnonReviewer1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Heuristics for Automated Reasoning through Reinforcement Learning", "abstract": "We demonstrate how to learn efficient heuristics for automated reasoning algorithms through deep reinforcement learning. We focus on backtracking search algorithms for quantified Boolean logics, which already can solve formulas of impressive size - up to 100s of thousands of variables. The main challenge is to find a representation of these formulas that lends itself to making predictions in a scalable way. For challenging problems, the heuristic learned through our approach reduces execution time by a factor of 10 compared to the existing handwritten heuristics.", "keywords": ["reinforcement learning", "deep learning", "logics", "formal methods", "automated reasoning", "backtracking search", "satisfiability", "quantified Boolean formulas"], "authorids": ["gilled@berkeley.edu", "markus.norman.rabe@gmail.com", "eal@berkeley.edu", "sseshia@eecs.berkeley.edu"], "authors": ["Gil Lederman", "Markus N. Rabe", "Edward A. Lee", "Sanjit A. Seshia"], "TL;DR": "RL finds better heuristics for automated reasoning algorithms.", "pdf": "/pdf/0cd39db88fadedf14c8a841fd359f62a53297a39.pdf", "paperhash": "lederman|learning_heuristics_for_automated_reasoning_through_reinforcement_learning", "_bibtex": "@misc{\nlederman2019learning,\ntitle={Learning Heuristics for Automated Reasoning through Reinforcement Learning},\nauthor={Gil Lederman and Markus N. Rabe and Edward A. Lee and Sanjit A. Seshia},\nyear={2019},\nurl={https://openreview.net/forum?id=HkeyZhC9F7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1133/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621607142, "tddate": null, "super": null, "final": null, "reply": {"forum": "HkeyZhC9F7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1133/Authors", "ICLR.cc/2019/Conference/Paper1133/Reviewers", "ICLR.cc/2019/Conference/Paper1133/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1133/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1133/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1133/Authors|ICLR.cc/2019/Conference/Paper1133/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1133/Reviewers", "ICLR.cc/2019/Conference/Paper1133/Authors", "ICLR.cc/2019/Conference/Paper1133/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621607142}}}, {"id": "Skx240VFpQ", "original": null, "number": 3, "cdate": 1542176307550, "ddate": null, "tcdate": 1542176307550, "tmdate": 1542176307550, "tddate": null, "forum": "HkeyZhC9F7", "replyto": "ryxJA8gF2Q", "invitation": "ICLR.cc/2019/Conference/-/Paper1133/Official_Comment", "content": {"title": "Clarification about QBFEVAL and additional data sets", "comment": "Thank you for the detailed comments.\n\n>>> The authors note the difficulty of finding suitable benchmarks and restrict the set of instances\n>>> they use for evaluation to formulae where the proposed method is likely to achieve improvements. \n>>> This skews the evaluation in favor of the proposed method; in particular, the 90% improvement\n>>> figure mentioned in the abstract is not representative of the general case. Indeed, on \n>>> another set of instances the proposed method falls significantly short of the performance of\n>>> a state-of-the-art heuristic that does not employ learning.\n\nOur claim is that training a model on several hundred formulas greatly improves the performance of the logic solver on formulas from the same distribution. In our paper, we only support this claim by experiments on the Reductions benchmark. But, in fact, we have confirmed the same results on several datasets of artificially synthesized formulas (encoding random bit-level and word-level circuits). These additional datasets can be found with the published code, and we will provide more details about them in the appendix.\n\nIt is natural to ask how a model trained on one distribution performs on a different dataset. Since QBFEVAL is an important data set in the formal methods community, we used it to test the transferability of the heuristics with only partial success. (Training a model directly on QBFEVAL does not seem to be possible at the moment, because of the small size of the dataset, leading to overfitting.)\n\nLastly, we want to point out that most other works on ML for formulas only consider sets of random formulas (in particular, formulas synthesized by the authors themselves). In comparison, the Reduction benchmark is a well-known data set from the literature and generated independently from our work. In this way, we believe that we avoid skewing the results in our favor and set a higher bar than related work.\n\n>>> A drawback of the paper is that there is no comparison to related work. I\n>>> realize that this is difficult to achieve because other approaches are in\n>>> related, but different areas and may be difficult to adapt for this case, but a\n>>> general comparison to the improvements other approaches achieve would be\n>>> helpful.\n\nWe would love to learn about (and compare to) related work, but we are not aware of any we could meaningfully compare to. Could you point us to any works you are aware of?\n\nCompared to the typical improvements through progress in hand-written heuristics, the 1000x improvement in the number of steps needed is enormous.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1133/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1133/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1133/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Heuristics for Automated Reasoning through Reinforcement Learning", "abstract": "We demonstrate how to learn efficient heuristics for automated reasoning algorithms through deep reinforcement learning. We focus on backtracking search algorithms for quantified Boolean logics, which already can solve formulas of impressive size - up to 100s of thousands of variables. The main challenge is to find a representation of these formulas that lends itself to making predictions in a scalable way. For challenging problems, the heuristic learned through our approach reduces execution time by a factor of 10 compared to the existing handwritten heuristics.", "keywords": ["reinforcement learning", "deep learning", "logics", "formal methods", "automated reasoning", "backtracking search", "satisfiability", "quantified Boolean formulas"], "authorids": ["gilled@berkeley.edu", "markus.norman.rabe@gmail.com", "eal@berkeley.edu", "sseshia@eecs.berkeley.edu"], "authors": ["Gil Lederman", "Markus N. Rabe", "Edward A. Lee", "Sanjit A. Seshia"], "TL;DR": "RL finds better heuristics for automated reasoning algorithms.", "pdf": "/pdf/0cd39db88fadedf14c8a841fd359f62a53297a39.pdf", "paperhash": "lederman|learning_heuristics_for_automated_reasoning_through_reinforcement_learning", "_bibtex": "@misc{\nlederman2019learning,\ntitle={Learning Heuristics for Automated Reasoning through Reinforcement Learning},\nauthor={Gil Lederman and Markus N. Rabe and Edward A. Lee and Sanjit A. Seshia},\nyear={2019},\nurl={https://openreview.net/forum?id=HkeyZhC9F7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1133/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621607142, "tddate": null, "super": null, "final": null, "reply": {"forum": "HkeyZhC9F7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1133/Authors", "ICLR.cc/2019/Conference/Paper1133/Reviewers", "ICLR.cc/2019/Conference/Paper1133/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1133/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1133/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1133/Authors|ICLR.cc/2019/Conference/Paper1133/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1133/Reviewers", "ICLR.cc/2019/Conference/Paper1133/Authors", "ICLR.cc/2019/Conference/Paper1133/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621607142}}}, {"id": "r1ggEpNKaX", "original": null, "number": 2, "cdate": 1542176039735, "ddate": null, "tcdate": 1542176039735, "tmdate": 1542176039735, "tddate": null, "forum": "HkeyZhC9F7", "replyto": "BklL0lZq3X", "invitation": "ICLR.cc/2019/Conference/-/Paper1133/Official_Comment", "content": {"title": "Some remarks about the concerns raised", "comment": "We thank the reviewer for the detailed feedback.\n\n>>> No theoretical justification about why this heuristic should work better than the existing ones.\n\nThis is a very interesting question, but surprisingly hard to answer. Even for the simpler question of why CDCL for SAT solvers is so unreasonably effective for a wide range of applications, there is no concrete theoretical explanation - despite two decades of research! When there is no satisfactory theoretical explanation, we suggest that it is better to learn the heuristics based on the data itself.\n\n>>> Doesn't solve QBF formulas in general, but only 2QBF.\n\nOur approach could be easily applied to general QBF as well. The limitation to 2QBF is also due to the underlying tool. But keep in mind that most applications of QBF, e.g. in verification and program synthesis, can be encoded with just one quantifier alternation, so we believe that we captured the most interesting cases of QBF.\n\n>>> It is not clear whether the range of formulas that can be solved using this approach is \n>>> greater than that of existing solvers.\n\nOur experiments demonstrate that we can solve significantly more formulas when given enough formulas from a single source (=distribution). We do not claim that the learned models generalize to formulas far away from that distribution. The question whether it is possible to learn models that apply to a wide \u201crange of formulas\u201d is indeed an open one.\n\n>>> Having a substantial amount of formulas that produce incomplete episodes, as it might be\n>>> the case in real world scenarios, hinders learning, so the dataset has to be manually\n>>> adjusted.\n\nWe believe that this the inherent challenge of problem solving: how can we learn to solve problems that we have never solved? The assumption underlying this paper is that learning how to solve simpler problems faster, helps us to solve harder problems, too. Our experiments demonstrate that this is indeed possible for problems sets containing many related formulas of different hardness levels.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1133/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1133/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1133/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Heuristics for Automated Reasoning through Reinforcement Learning", "abstract": "We demonstrate how to learn efficient heuristics for automated reasoning algorithms through deep reinforcement learning. We focus on backtracking search algorithms for quantified Boolean logics, which already can solve formulas of impressive size - up to 100s of thousands of variables. The main challenge is to find a representation of these formulas that lends itself to making predictions in a scalable way. For challenging problems, the heuristic learned through our approach reduces execution time by a factor of 10 compared to the existing handwritten heuristics.", "keywords": ["reinforcement learning", "deep learning", "logics", "formal methods", "automated reasoning", "backtracking search", "satisfiability", "quantified Boolean formulas"], "authorids": ["gilled@berkeley.edu", "markus.norman.rabe@gmail.com", "eal@berkeley.edu", "sseshia@eecs.berkeley.edu"], "authors": ["Gil Lederman", "Markus N. Rabe", "Edward A. Lee", "Sanjit A. Seshia"], "TL;DR": "RL finds better heuristics for automated reasoning algorithms.", "pdf": "/pdf/0cd39db88fadedf14c8a841fd359f62a53297a39.pdf", "paperhash": "lederman|learning_heuristics_for_automated_reasoning_through_reinforcement_learning", "_bibtex": "@misc{\nlederman2019learning,\ntitle={Learning Heuristics for Automated Reasoning through Reinforcement Learning},\nauthor={Gil Lederman and Markus N. Rabe and Edward A. Lee and Sanjit A. Seshia},\nyear={2019},\nurl={https://openreview.net/forum?id=HkeyZhC9F7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1133/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621607142, "tddate": null, "super": null, "final": null, "reply": {"forum": "HkeyZhC9F7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1133/Authors", "ICLR.cc/2019/Conference/Paper1133/Reviewers", "ICLR.cc/2019/Conference/Paper1133/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1133/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1133/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1133/Authors|ICLR.cc/2019/Conference/Paper1133/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1133/Reviewers", "ICLR.cc/2019/Conference/Paper1133/Authors", "ICLR.cc/2019/Conference/Paper1133/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621607142}}}, {"id": "Bye14hEKpQ", "original": null, "number": 1, "cdate": 1542175783456, "ddate": null, "tcdate": 1542175783456, "tmdate": 1542175783456, "tddate": null, "forum": "HkeyZhC9F7", "replyto": "SklWGYIq2X", "invitation": "ICLR.cc/2019/Conference/-/Paper1133/Official_Comment", "content": {"title": "We believe the work contains insights for the ML community, too.", "comment": "We thank the reviewer for the insightful comments.\n\n>>> [...] the novelty from a ML and RL point of view remains limited [...]\n\nWe see contributions to two lines of work published in ICLR and related conferences: The first concerns the representation of formulas to facilitate learning [1, 2, 3], and the second concerns leveraging reinforcement learning in combinatorial search algorithms [5, 6].\n\nCompared to [1, 2, 3], we show how to address the problem of scale. Previous works suggested tree-encoders [2], possible worlds [1], and top-down tree encoders [3]. These approaches seem to be limited to formulas with tens of variables, which would be considered tiny in the verification/formal methods community. To scale up to realistic formulas, orders of magnitude larger of what has been considered before, we suggest to exploit the graph representation of formulas in conjunctive normal form and apply GNNs. While GNNs generally scale well, this is also a conceptual shift: Previous works needed to learn a fixed embedding for variables \u201ca\u201d, \u201cb\u201d, \u201cc\u201d, etc., even though variable \u201ca\u201d has no shared meaning across different formulas. GNNs enable us to embed variables based only on the context of their occurrences in the current formula.\n\nCompared to [5, 6], our work represents a big step towards practicality. While interesting from a learning perspective, their methods do not come even close to the state-of-the-art in specialized algorithms. We demonstrate that the tight integration of deep learning and combinatorial search algorithms can actually improve the performance of complex and (relatively) large-scale applications of combinatorial search. The main challenge here was the significant performance cost of neural networks. Our work shows that this cost can be outweighed by the dramatically better decisions neural networks suggest (1000x fewer steps needed to solve hard formulas).\n\nWe acknowledge that we need to state these points more clearly, and will improve the paper accordingly.\n\n[1] \"Can Neural Networks Understand Logical Entailment?\", in ICLR 2018\n[2] \"Learning Continuous Semantic Representations of Symbolic Expressions\", in ICML 2017\n[3] \"Top-down neural model for formulae\", under submission to ICLR 2019\n[4] \"Learning a SAT Solver from Single-Bit Supervision\", under submission to ICLR 2019\n[5] \"Learning Combinatorial Optimization Algorithms over Graphs\", in NIPS 2017\n[6] \"Neural Combinatorial Optimization with Reinforcement Learning\", in ICLR 2017\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1133/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1133/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1133/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Heuristics for Automated Reasoning through Reinforcement Learning", "abstract": "We demonstrate how to learn efficient heuristics for automated reasoning algorithms through deep reinforcement learning. We focus on backtracking search algorithms for quantified Boolean logics, which already can solve formulas of impressive size - up to 100s of thousands of variables. The main challenge is to find a representation of these formulas that lends itself to making predictions in a scalable way. For challenging problems, the heuristic learned through our approach reduces execution time by a factor of 10 compared to the existing handwritten heuristics.", "keywords": ["reinforcement learning", "deep learning", "logics", "formal methods", "automated reasoning", "backtracking search", "satisfiability", "quantified Boolean formulas"], "authorids": ["gilled@berkeley.edu", "markus.norman.rabe@gmail.com", "eal@berkeley.edu", "sseshia@eecs.berkeley.edu"], "authors": ["Gil Lederman", "Markus N. Rabe", "Edward A. Lee", "Sanjit A. Seshia"], "TL;DR": "RL finds better heuristics for automated reasoning algorithms.", "pdf": "/pdf/0cd39db88fadedf14c8a841fd359f62a53297a39.pdf", "paperhash": "lederman|learning_heuristics_for_automated_reasoning_through_reinforcement_learning", "_bibtex": "@misc{\nlederman2019learning,\ntitle={Learning Heuristics for Automated Reasoning through Reinforcement Learning},\nauthor={Gil Lederman and Markus N. Rabe and Edward A. Lee and Sanjit A. Seshia},\nyear={2019},\nurl={https://openreview.net/forum?id=HkeyZhC9F7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1133/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621607142, "tddate": null, "super": null, "final": null, "reply": {"forum": "HkeyZhC9F7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1133/Authors", "ICLR.cc/2019/Conference/Paper1133/Reviewers", "ICLR.cc/2019/Conference/Paper1133/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1133/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1133/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1133/Authors|ICLR.cc/2019/Conference/Paper1133/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1133/Reviewers", "ICLR.cc/2019/Conference/Paper1133/Authors", "ICLR.cc/2019/Conference/Paper1133/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621607142}}}, {"id": "SklWGYIq2X", "original": null, "number": 3, "cdate": 1541200137333, "ddate": null, "tcdate": 1541200137333, "tmdate": 1541533393966, "tddate": null, "forum": "HkeyZhC9F7", "replyto": "HkeyZhC9F7", "invitation": "ICLR.cc/2019/Conference/-/Paper1133/Official_Review", "content": {"title": "Interesting application of reinforcement learning and GNN over a specific decision problem", "review": "The paper is proposing to use reinforcement learning as a method for implementing heuristics of a backtracking search algorithm or Boolean Logic. While I'm not familiar with this specific topic, Section 2 is didactic and clear. The challenges of the tackle problem are clearly explained in this section.\n\nThe Graph neural network architecture proposed in Section 4 to compute literals of the formula is an original idea. The experimental results look convincing and suggest this approach should be more deeply investigated.\n\nMy main concern is that the novelty from a machine learning and reinforcement learning point of view remains limited while the application seems original and promising. So I will not be strongly opposed to the publication if this work in ICLR venue while I remain unsure it is the best one.", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1133/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Heuristics for Automated Reasoning through Reinforcement Learning", "abstract": "We demonstrate how to learn efficient heuristics for automated reasoning algorithms through deep reinforcement learning. We focus on backtracking search algorithms for quantified Boolean logics, which already can solve formulas of impressive size - up to 100s of thousands of variables. The main challenge is to find a representation of these formulas that lends itself to making predictions in a scalable way. For challenging problems, the heuristic learned through our approach reduces execution time by a factor of 10 compared to the existing handwritten heuristics.", "keywords": ["reinforcement learning", "deep learning", "logics", "formal methods", "automated reasoning", "backtracking search", "satisfiability", "quantified Boolean formulas"], "authorids": ["gilled@berkeley.edu", "markus.norman.rabe@gmail.com", "eal@berkeley.edu", "sseshia@eecs.berkeley.edu"], "authors": ["Gil Lederman", "Markus N. Rabe", "Edward A. Lee", "Sanjit A. Seshia"], "TL;DR": "RL finds better heuristics for automated reasoning algorithms.", "pdf": "/pdf/0cd39db88fadedf14c8a841fd359f62a53297a39.pdf", "paperhash": "lederman|learning_heuristics_for_automated_reasoning_through_reinforcement_learning", "_bibtex": "@misc{\nlederman2019learning,\ntitle={Learning Heuristics for Automated Reasoning through Reinforcement Learning},\nauthor={Gil Lederman and Markus N. Rabe and Edward A. Lee and Sanjit A. Seshia},\nyear={2019},\nurl={https://openreview.net/forum?id=HkeyZhC9F7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1133/Official_Review", "cdate": 1542234298589, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HkeyZhC9F7", "replyto": "HkeyZhC9F7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1133/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335880059, "tmdate": 1552335880059, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1133/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "BklL0lZq3X", "original": null, "number": 2, "cdate": 1541177549687, "ddate": null, "tcdate": 1541177549687, "tmdate": 1541533393767, "tddate": null, "forum": "HkeyZhC9F7", "replyto": "HkeyZhC9F7", "invitation": "ICLR.cc/2019/Conference/-/Paper1133/Official_Review", "content": {"title": "needs some improvement ", "review": "The aim of this paper is to learn a heuristic for a backtracking search algorithm utilizing Reinforcement learning. The proposed model makes use of Graphical Neural Networks to produce literal and clauses embeddings, and use them to predict the quality of each literal, through a NN, which in turn decides the probability of each action.\n\nPositives\nA new approach on how to employ Machine learning techniques to Automated reasoning problems. Works with any 2QBF solver.\nThe learned heuristic seems to perform better than the state of the art in the presented experiments.\n\nNegatives\nNo theoretical justification about why this heuristic should work better than the existing ones.\nDoesn't solve QBF formulas in general, but only 2QBF.\nIt is not clear whether the range of formulas that can be solved using this approach is greater than that of existing solvers.\nHaving a substantial amount of formulas that produce incomplete episodes, as it might be the case in real world scenarios, hinders learning, so the dataset has to be manually adjusted.\n\nConclusion\nThe proposed framework is an interesting addition to existing techniques in the field and the idea is suitable for further exploration and refinement. The experimental results are promising, so the direction of the work is worth pursuing. However, some of the foundations and overall nature of the work needs some improvement and maturity. ", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1133/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Heuristics for Automated Reasoning through Reinforcement Learning", "abstract": "We demonstrate how to learn efficient heuristics for automated reasoning algorithms through deep reinforcement learning. We focus on backtracking search algorithms for quantified Boolean logics, which already can solve formulas of impressive size - up to 100s of thousands of variables. The main challenge is to find a representation of these formulas that lends itself to making predictions in a scalable way. For challenging problems, the heuristic learned through our approach reduces execution time by a factor of 10 compared to the existing handwritten heuristics.", "keywords": ["reinforcement learning", "deep learning", "logics", "formal methods", "automated reasoning", "backtracking search", "satisfiability", "quantified Boolean formulas"], "authorids": ["gilled@berkeley.edu", "markus.norman.rabe@gmail.com", "eal@berkeley.edu", "sseshia@eecs.berkeley.edu"], "authors": ["Gil Lederman", "Markus N. Rabe", "Edward A. Lee", "Sanjit A. Seshia"], "TL;DR": "RL finds better heuristics for automated reasoning algorithms.", "pdf": "/pdf/0cd39db88fadedf14c8a841fd359f62a53297a39.pdf", "paperhash": "lederman|learning_heuristics_for_automated_reasoning_through_reinforcement_learning", "_bibtex": "@misc{\nlederman2019learning,\ntitle={Learning Heuristics for Automated Reasoning through Reinforcement Learning},\nauthor={Gil Lederman and Markus N. Rabe and Edward A. Lee and Sanjit A. Seshia},\nyear={2019},\nurl={https://openreview.net/forum?id=HkeyZhC9F7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1133/Official_Review", "cdate": 1542234298589, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HkeyZhC9F7", "replyto": "HkeyZhC9F7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1133/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335880059, "tmdate": 1552335880059, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1133/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "ryxJA8gF2Q", "original": null, "number": 1, "cdate": 1541109447297, "ddate": null, "tcdate": 1541109447297, "tmdate": 1541533393560, "tddate": null, "forum": "HkeyZhC9F7", "replyto": "HkeyZhC9F7", "invitation": "ICLR.cc/2019/Conference/-/Paper1133/Official_Review", "content": {"title": "Interesting application of deep learning with interesting results.", "review": "The paper proposes an approach to automatically learning variable selection\nheuristics for QBF using deep learning. The evaluation presented by the authors\nshows the promise of the method and demonstrates significant performance\nimprovements over a variable selection heuristic that does not use machine\nlearning.\n\nIn practice, the overhead of the proposed method is likely to be a major\nobstacle in its adoption. The authors note the difficulty of finding suitable\nbenchmarks and restrict the set of instances they use for evaluation to formulae\nwhere the proposed method is likely to achieve improvements. This skews the\nevaluation in favor of the proposed method; in particular, the 90% improvement\nfigure mentioned in the abstract is not representative of the general case.\nIndeed, on another set of instances the proposed method falls significantly\nshort of the performance of a state-of-the-art heuristic that does not employ\nlearning.\n\nA drawback of the paper is that there is no comparison to related work. I\nrealize that this is difficult to achieve because other approaches are in\nrelated, but different areas and may be difficult to adapt for this case, but a\ngeneral comparison to the improvements other approaches achieve would be\nhelpful.\n\nNevertheless, the work is interesting and presents a new angle on using machine\nlearning to speed up combinatorial problem solving. While several issues hinder\npractical adoption, this is likely to lead to interesting follow-up work that\nwill improve problem solving in practice.\n\nThe description of the method (Section 4.1) is short and not detailed enough to\nreproduce the approach the authors are proposing. However, the code is\navailable.\n\nIn summary, I feel that the paper can be accepted.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1133/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Heuristics for Automated Reasoning through Reinforcement Learning", "abstract": "We demonstrate how to learn efficient heuristics for automated reasoning algorithms through deep reinforcement learning. We focus on backtracking search algorithms for quantified Boolean logics, which already can solve formulas of impressive size - up to 100s of thousands of variables. The main challenge is to find a representation of these formulas that lends itself to making predictions in a scalable way. For challenging problems, the heuristic learned through our approach reduces execution time by a factor of 10 compared to the existing handwritten heuristics.", "keywords": ["reinforcement learning", "deep learning", "logics", "formal methods", "automated reasoning", "backtracking search", "satisfiability", "quantified Boolean formulas"], "authorids": ["gilled@berkeley.edu", "markus.norman.rabe@gmail.com", "eal@berkeley.edu", "sseshia@eecs.berkeley.edu"], "authors": ["Gil Lederman", "Markus N. Rabe", "Edward A. Lee", "Sanjit A. Seshia"], "TL;DR": "RL finds better heuristics for automated reasoning algorithms.", "pdf": "/pdf/0cd39db88fadedf14c8a841fd359f62a53297a39.pdf", "paperhash": "lederman|learning_heuristics_for_automated_reasoning_through_reinforcement_learning", "_bibtex": "@misc{\nlederman2019learning,\ntitle={Learning Heuristics for Automated Reasoning through Reinforcement Learning},\nauthor={Gil Lederman and Markus N. Rabe and Edward A. Lee and Sanjit A. Seshia},\nyear={2019},\nurl={https://openreview.net/forum?id=HkeyZhC9F7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1133/Official_Review", "cdate": 1542234298589, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HkeyZhC9F7", "replyto": "HkeyZhC9F7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1133/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335880059, "tmdate": 1552335880059, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1133/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 11}