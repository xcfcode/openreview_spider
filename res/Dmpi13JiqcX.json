{"notes": [{"id": "Dmpi13JiqcX", "original": "uLp3Vzc6FkW", "number": 3631, "cdate": 1601308404041, "ddate": null, "tcdate": 1601308404041, "tmdate": 1614985681604, "tddate": null, "forum": "Dmpi13JiqcX", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Disentangling Representations of Text by Masking Transformers", "authorids": ["~Xiongyi_Zhang2", "~Jan-Willem_van_de_Meent1", "~Byron_C_Wallace1"], "authors": ["Xiongyi Zhang", "Jan-Willem van de Meent", "Byron C Wallace"], "keywords": ["disentanglement", "model pruning", "representation learning", "transformers"], "abstract": "Representations in large language models such as BERT encode a range of features into a single vector, which are predictive in the context of a multitude of downstream tasks. In this paper, we explore whether it is possible to learn disentangled representations  by identifying subnetworks in pre-trained models that encode distinct, complementary aspects of the representation. Concretely, we learn binary masks over transformer weights or hidden units to uncover the subset of features that correlate with a specific factor of variation. This sidesteps the need to train a disentangled model from scratch within a particular domain. We evaluate the ability of this method to disentangle representations of syntax and semantics, and sentiment from genre in the context of movie reviews. By combining this method with magnitude pruning we find that we can identify quite sparse subnetworks. Moreover, we find that this disentanglement-via-masking approach performs as well as or better than previously proposed methods based on variational autoencoders and adversarial training. ", "one-sentence_summary": "Learning disentangled representations by identifying subnetworks in pre-trained transformer models.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|disentangling_representations_of_text_by_masking_transformers", "pdf": "/pdf/f8a8cf9435778ae4038e813fe7d1fa3e1a22826b.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=_3CoaHN2lu", "_bibtex": "@misc{\nzhang2021disentangling,\ntitle={Disentangling Representations of Text by Masking Transformers},\nauthor={Xiongyi Zhang and Jan-Willem van de Meent and Byron C Wallace},\nyear={2021},\nurl={https://openreview.net/forum?id=Dmpi13JiqcX}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 13, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "PdPAfR3uwCE", "original": null, "number": 1, "cdate": 1610040471988, "ddate": null, "tcdate": 1610040471988, "tmdate": 1610474076081, "tddate": null, "forum": "Dmpi13JiqcX", "replyto": "Dmpi13JiqcX", "invitation": "ICLR.cc/2021/Conference/Paper3631/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "This paper explores a methodology for learning disentangled representations using a triplet loss to find subnetworks within a transformer.  The authors compare against several other methods and find that their method performs well without needing to train from scratch. The reviewers thought this paper was well written and the authors were very responsive during the review period.  However, there were some questions about the experimental setup and empirical performance of the paper, leaving the reviewers wondering if the performance was convincing.  We agree that there is value in exploring disentangled representations even if they do not necessarily improve performance (as the authors point out), but clearly explaining the reasoning behind all analyses (e.g. specifically choosing domains to introduce a spurious correlation), and justifying differences in performance is particularly important in these cases."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Disentangling Representations of Text by Masking Transformers", "authorids": ["~Xiongyi_Zhang2", "~Jan-Willem_van_de_Meent1", "~Byron_C_Wallace1"], "authors": ["Xiongyi Zhang", "Jan-Willem van de Meent", "Byron C Wallace"], "keywords": ["disentanglement", "model pruning", "representation learning", "transformers"], "abstract": "Representations in large language models such as BERT encode a range of features into a single vector, which are predictive in the context of a multitude of downstream tasks. In this paper, we explore whether it is possible to learn disentangled representations  by identifying subnetworks in pre-trained models that encode distinct, complementary aspects of the representation. Concretely, we learn binary masks over transformer weights or hidden units to uncover the subset of features that correlate with a specific factor of variation. This sidesteps the need to train a disentangled model from scratch within a particular domain. We evaluate the ability of this method to disentangle representations of syntax and semantics, and sentiment from genre in the context of movie reviews. By combining this method with magnitude pruning we find that we can identify quite sparse subnetworks. Moreover, we find that this disentanglement-via-masking approach performs as well as or better than previously proposed methods based on variational autoencoders and adversarial training. ", "one-sentence_summary": "Learning disentangled representations by identifying subnetworks in pre-trained transformer models.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|disentangling_representations_of_text_by_masking_transformers", "pdf": "/pdf/f8a8cf9435778ae4038e813fe7d1fa3e1a22826b.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=_3CoaHN2lu", "_bibtex": "@misc{\nzhang2021disentangling,\ntitle={Disentangling Representations of Text by Masking Transformers},\nauthor={Xiongyi Zhang and Jan-Willem van de Meent and Byron C Wallace},\nyear={2021},\nurl={https://openreview.net/forum?id=Dmpi13JiqcX}\n}"}, "tags": [], "invitation": {"reply": {"forum": "Dmpi13JiqcX", "replyto": "Dmpi13JiqcX", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040471975, "tmdate": 1610474076065, "id": "ICLR.cc/2021/Conference/Paper3631/-/Decision"}}}, {"id": "-hkkYOtrT8i", "original": null, "number": 9, "cdate": 1605449376684, "ddate": null, "tcdate": 1605449376684, "tmdate": 1605821416692, "tddate": null, "forum": "Dmpi13JiqcX", "replyto": "8J1uz_8GIM7", "invitation": "ICLR.cc/2021/Conference/Paper3631/-/Official_Comment", "content": {"title": "Reply to Reviewer#3's response", "comment": "The purpose of the work is to devise a method for disentangling representations of text, and one important potential benefit of such methods is making models more robust, i.e., less reliant on spurious correlations. The method is therefore general in that is appropriate for any application in which robustness is a concern, which in practice is most cases."}, "signatures": ["ICLR.cc/2021/Conference/Paper3631/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3631/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Disentangling Representations of Text by Masking Transformers", "authorids": ["~Xiongyi_Zhang2", "~Jan-Willem_van_de_Meent1", "~Byron_C_Wallace1"], "authors": ["Xiongyi Zhang", "Jan-Willem van de Meent", "Byron C Wallace"], "keywords": ["disentanglement", "model pruning", "representation learning", "transformers"], "abstract": "Representations in large language models such as BERT encode a range of features into a single vector, which are predictive in the context of a multitude of downstream tasks. In this paper, we explore whether it is possible to learn disentangled representations  by identifying subnetworks in pre-trained models that encode distinct, complementary aspects of the representation. Concretely, we learn binary masks over transformer weights or hidden units to uncover the subset of features that correlate with a specific factor of variation. This sidesteps the need to train a disentangled model from scratch within a particular domain. We evaluate the ability of this method to disentangle representations of syntax and semantics, and sentiment from genre in the context of movie reviews. By combining this method with magnitude pruning we find that we can identify quite sparse subnetworks. Moreover, we find that this disentanglement-via-masking approach performs as well as or better than previously proposed methods based on variational autoencoders and adversarial training. ", "one-sentence_summary": "Learning disentangled representations by identifying subnetworks in pre-trained transformer models.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|disentangling_representations_of_text_by_masking_transformers", "pdf": "/pdf/f8a8cf9435778ae4038e813fe7d1fa3e1a22826b.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=_3CoaHN2lu", "_bibtex": "@misc{\nzhang2021disentangling,\ntitle={Disentangling Representations of Text by Masking Transformers},\nauthor={Xiongyi Zhang and Jan-Willem van de Meent and Byron C Wallace},\nyear={2021},\nurl={https://openreview.net/forum?id=Dmpi13JiqcX}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Dmpi13JiqcX", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3631/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3631/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3631/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3631/Authors|ICLR.cc/2021/Conference/Paper3631/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3631/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923835510, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3631/-/Official_Comment"}}}, {"id": "kUKcooJZWHv", "original": null, "number": 11, "cdate": 1605821401335, "ddate": null, "tcdate": 1605821401335, "tmdate": 1605821401335, "tddate": null, "forum": "Dmpi13JiqcX", "replyto": "wg2g-2jMsly", "invitation": "ICLR.cc/2021/Conference/Paper3631/-/Official_Comment", "content": {"title": "References added", "comment": "We have updated our paper and included HUBERT in our related work section. "}, "signatures": ["ICLR.cc/2021/Conference/Paper3631/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3631/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Disentangling Representations of Text by Masking Transformers", "authorids": ["~Xiongyi_Zhang2", "~Jan-Willem_van_de_Meent1", "~Byron_C_Wallace1"], "authors": ["Xiongyi Zhang", "Jan-Willem van de Meent", "Byron C Wallace"], "keywords": ["disentanglement", "model pruning", "representation learning", "transformers"], "abstract": "Representations in large language models such as BERT encode a range of features into a single vector, which are predictive in the context of a multitude of downstream tasks. In this paper, we explore whether it is possible to learn disentangled representations  by identifying subnetworks in pre-trained models that encode distinct, complementary aspects of the representation. Concretely, we learn binary masks over transformer weights or hidden units to uncover the subset of features that correlate with a specific factor of variation. This sidesteps the need to train a disentangled model from scratch within a particular domain. We evaluate the ability of this method to disentangle representations of syntax and semantics, and sentiment from genre in the context of movie reviews. By combining this method with magnitude pruning we find that we can identify quite sparse subnetworks. Moreover, we find that this disentanglement-via-masking approach performs as well as or better than previously proposed methods based on variational autoencoders and adversarial training. ", "one-sentence_summary": "Learning disentangled representations by identifying subnetworks in pre-trained transformer models.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|disentangling_representations_of_text_by_masking_transformers", "pdf": "/pdf/f8a8cf9435778ae4038e813fe7d1fa3e1a22826b.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=_3CoaHN2lu", "_bibtex": "@misc{\nzhang2021disentangling,\ntitle={Disentangling Representations of Text by Masking Transformers},\nauthor={Xiongyi Zhang and Jan-Willem van de Meent and Byron C Wallace},\nyear={2021},\nurl={https://openreview.net/forum?id=Dmpi13JiqcX}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Dmpi13JiqcX", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3631/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3631/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3631/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3631/Authors|ICLR.cc/2021/Conference/Paper3631/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3631/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923835510, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3631/-/Official_Comment"}}}, {"id": "-oVoSwrX5S5", "original": null, "number": 10, "cdate": 1605821344723, "ddate": null, "tcdate": 1605821344723, "tmdate": 1605821344723, "tddate": null, "forum": "Dmpi13JiqcX", "replyto": "pZXSvm1NEw", "invitation": "ICLR.cc/2021/Conference/Paper3631/-/Official_Comment", "content": {"title": "VAE baselines added", "comment": "We have updated the sentiment/genre experiment (Section 3.1) to include two VAE baselines. As shown in figure 2 and table 2, our methods outperform both baselines. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3631/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3631/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Disentangling Representations of Text by Masking Transformers", "authorids": ["~Xiongyi_Zhang2", "~Jan-Willem_van_de_Meent1", "~Byron_C_Wallace1"], "authors": ["Xiongyi Zhang", "Jan-Willem van de Meent", "Byron C Wallace"], "keywords": ["disentanglement", "model pruning", "representation learning", "transformers"], "abstract": "Representations in large language models such as BERT encode a range of features into a single vector, which are predictive in the context of a multitude of downstream tasks. In this paper, we explore whether it is possible to learn disentangled representations  by identifying subnetworks in pre-trained models that encode distinct, complementary aspects of the representation. Concretely, we learn binary masks over transformer weights or hidden units to uncover the subset of features that correlate with a specific factor of variation. This sidesteps the need to train a disentangled model from scratch within a particular domain. We evaluate the ability of this method to disentangle representations of syntax and semantics, and sentiment from genre in the context of movie reviews. By combining this method with magnitude pruning we find that we can identify quite sparse subnetworks. Moreover, we find that this disentanglement-via-masking approach performs as well as or better than previously proposed methods based on variational autoencoders and adversarial training. ", "one-sentence_summary": "Learning disentangled representations by identifying subnetworks in pre-trained transformer models.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|disentangling_representations_of_text_by_masking_transformers", "pdf": "/pdf/f8a8cf9435778ae4038e813fe7d1fa3e1a22826b.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=_3CoaHN2lu", "_bibtex": "@misc{\nzhang2021disentangling,\ntitle={Disentangling Representations of Text by Masking Transformers},\nauthor={Xiongyi Zhang and Jan-Willem van de Meent and Byron C Wallace},\nyear={2021},\nurl={https://openreview.net/forum?id=Dmpi13JiqcX}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Dmpi13JiqcX", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3631/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3631/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3631/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3631/Authors|ICLR.cc/2021/Conference/Paper3631/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3631/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923835510, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3631/-/Official_Comment"}}}, {"id": "8J1uz_8GIM7", "original": null, "number": 8, "cdate": 1605445577017, "ddate": null, "tcdate": 1605445577017, "tmdate": 1605445577017, "tddate": null, "forum": "Dmpi13JiqcX", "replyto": "mBKjmCFkSl6", "invitation": "ICLR.cc/2021/Conference/Paper3631/-/Official_Comment", "content": {"title": "About the response from authors", "comment": "The response seems reasonable. However, it raises a new question. Since the authors carefully selected the data and  designed specific downstrean tasks, How to ensure or reflect the generality of the proposed method.  That is, what's the meaning of the proposed work.  It is just for specific cases or for general cases."}, "signatures": ["ICLR.cc/2021/Conference/Paper3631/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3631/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Disentangling Representations of Text by Masking Transformers", "authorids": ["~Xiongyi_Zhang2", "~Jan-Willem_van_de_Meent1", "~Byron_C_Wallace1"], "authors": ["Xiongyi Zhang", "Jan-Willem van de Meent", "Byron C Wallace"], "keywords": ["disentanglement", "model pruning", "representation learning", "transformers"], "abstract": "Representations in large language models such as BERT encode a range of features into a single vector, which are predictive in the context of a multitude of downstream tasks. In this paper, we explore whether it is possible to learn disentangled representations  by identifying subnetworks in pre-trained models that encode distinct, complementary aspects of the representation. Concretely, we learn binary masks over transformer weights or hidden units to uncover the subset of features that correlate with a specific factor of variation. This sidesteps the need to train a disentangled model from scratch within a particular domain. We evaluate the ability of this method to disentangle representations of syntax and semantics, and sentiment from genre in the context of movie reviews. By combining this method with magnitude pruning we find that we can identify quite sparse subnetworks. Moreover, we find that this disentanglement-via-masking approach performs as well as or better than previously proposed methods based on variational autoencoders and adversarial training. ", "one-sentence_summary": "Learning disentangled representations by identifying subnetworks in pre-trained transformer models.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|disentangling_representations_of_text_by_masking_transformers", "pdf": "/pdf/f8a8cf9435778ae4038e813fe7d1fa3e1a22826b.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=_3CoaHN2lu", "_bibtex": "@misc{\nzhang2021disentangling,\ntitle={Disentangling Representations of Text by Masking Transformers},\nauthor={Xiongyi Zhang and Jan-Willem van de Meent and Byron C Wallace},\nyear={2021},\nurl={https://openreview.net/forum?id=Dmpi13JiqcX}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Dmpi13JiqcX", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3631/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3631/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3631/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3631/Authors|ICLR.cc/2021/Conference/Paper3631/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3631/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923835510, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3631/-/Official_Comment"}}}, {"id": "mBKjmCFkSl6", "original": null, "number": 7, "cdate": 1605300709368, "ddate": null, "tcdate": 1605300709368, "tmdate": 1605300750264, "tddate": null, "forum": "Dmpi13JiqcX", "replyto": "r-ps45B8POr", "invitation": "ICLR.cc/2021/Conference/Paper3631/-/Official_Comment", "content": {"title": "Response to reviewer #3", "comment": "We thank the reviewer for their comments and provide clarifications and responses to specific questions below.\n\n**The experimental setup is not convincing. Why only pick the two genres of drama and horror?**\n\nPerhaps we were not clear enough in motivating our experimental setup here: We are interested in examining the degree to which disentanglement via different methods affords robustness to reliance on spurious correlations, such as (in this example) associating a particular genre with a specific sentiment (here, e.g., horror with negative sentiment). In many cases, it is reasonable to assume that either we do not want to rely on certain correlations like this for reasons of fairness, and/or the conditional distributions \u2014 p(sentiment|genre) \u2014 may shift in the test distribution. With this framing in mind, we selected Drama and Horror because among all the major genres, these two genres of reviews have the most correlation between genre and sentiment: Drama reviews are more likely positive and Horror negative. This is to create a spurious correlation between the genre and sentiment, so that we can probe for robustness to the same.\n\n\n\n**Figure 3 does not show that the proposed method achieves better results than the finetuned baseline**\n\nWe feel there is a misreading of the figure here (perhaps we could improve the presentation and description). Figure 3 does show our models outperforming the finetuned baseline, with respect to the representations that it induces. When trained on sentiment (upper row), the representations from the finetuned model are still clustered with respect to genre (marker shape); this clustering is not observed using the proposed masking approaches. When trained on genre (bottom row), the two genres are not well separated; whereas the representations from our two models, although still imperfect, are clearly better separated than the finetuned model. This also aligns with the quantitative results in Figure 2 and Table 2.\n\n\n\t\n**How to use these disentangling representations in downstream tasks, such as text classification, natural language inference, and semantic similarity? It is better to discuss and conduct experiment to show the advantages of their disentangling representations in downstream tasks.**\n\nWe did report the STS-Spearman correlation in Sec 3.2, which is a semantic similarity benchmark. And in Sec 3.1, we designed a specific text classification task with two correlating attributes, which we believe demonstrates the advantage of our model over other baselines with respect to robustness, i.e.,  performing well in situations where other methods result in overreliance on artifacts (spurious correlations) in the training set. We feel these experiments do show the important robustness advantages of this approach in downstream tasks. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3631/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3631/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Disentangling Representations of Text by Masking Transformers", "authorids": ["~Xiongyi_Zhang2", "~Jan-Willem_van_de_Meent1", "~Byron_C_Wallace1"], "authors": ["Xiongyi Zhang", "Jan-Willem van de Meent", "Byron C Wallace"], "keywords": ["disentanglement", "model pruning", "representation learning", "transformers"], "abstract": "Representations in large language models such as BERT encode a range of features into a single vector, which are predictive in the context of a multitude of downstream tasks. In this paper, we explore whether it is possible to learn disentangled representations  by identifying subnetworks in pre-trained models that encode distinct, complementary aspects of the representation. Concretely, we learn binary masks over transformer weights or hidden units to uncover the subset of features that correlate with a specific factor of variation. This sidesteps the need to train a disentangled model from scratch within a particular domain. We evaluate the ability of this method to disentangle representations of syntax and semantics, and sentiment from genre in the context of movie reviews. By combining this method with magnitude pruning we find that we can identify quite sparse subnetworks. Moreover, we find that this disentanglement-via-masking approach performs as well as or better than previously proposed methods based on variational autoencoders and adversarial training. ", "one-sentence_summary": "Learning disentangled representations by identifying subnetworks in pre-trained transformer models.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|disentangling_representations_of_text_by_masking_transformers", "pdf": "/pdf/f8a8cf9435778ae4038e813fe7d1fa3e1a22826b.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=_3CoaHN2lu", "_bibtex": "@misc{\nzhang2021disentangling,\ntitle={Disentangling Representations of Text by Masking Transformers},\nauthor={Xiongyi Zhang and Jan-Willem van de Meent and Byron C Wallace},\nyear={2021},\nurl={https://openreview.net/forum?id=Dmpi13JiqcX}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Dmpi13JiqcX", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3631/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3631/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3631/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3631/Authors|ICLR.cc/2021/Conference/Paper3631/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3631/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923835510, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3631/-/Official_Comment"}}}, {"id": "wg2g-2jMsly", "original": null, "number": 6, "cdate": 1605300418434, "ddate": null, "tcdate": 1605300418434, "tmdate": 1605300418434, "tddate": null, "forum": "Dmpi13JiqcX", "replyto": "0DZtJZeDaDY", "invitation": "ICLR.cc/2021/Conference/Paper3631/-/Official_Comment", "content": {"title": "Response to Review #1", "comment": "We thank the reviewer for their detailed comments and suggestions, and respond to all concerns below. \n\n**I wish the authors performed their first experiment on more domains: books, music, etc. and consider more than two labels. From current results, it's hard to confidently conclude that this approach is generalizable.**\n\nWe note that we did perform experiments on two different types of datasets, corresponding to (quite) different tasks; we thought this would be more compelling than a suite of experiments on sentiment tasks. We would argue that the fact that our model does well on these two considerably distinct datasets/tasks indicates that it has reasonably good generalizability. We will consider including additional experiments on more datasets for a camera-ready version, however.\n\n**Judging based on Figure 4 results I'm not convinced that the proposed approach does better than the finetuned (which I believe has a trained classifier on top of BERT) approach especially for Semantic tasks. Perhaps a discussion/ error analysis would be appropriate given better results on Syntax tasks.**\n\t\nThe reviewers\u2019  observation is correct: our model performs roughly on par with the fine-tuning method in Figure 4 (arguably a bit better, but the objective is multivariate so it is hard to say). But we would highlight that the main purpose of this paper is to provide a new way of looking at the problem of learning disentangled representations; in Figure 3 we can see that the representations learned using the finetuned approach fail to achieve the level of disentanglement enjoyed by the proposed approach. And again, ours are learned without modifying the BERT weights, which we think is an interesting finding. Furthermore, in the robustness experiments (Figure 2) we show that the fine-tuned approach fares considerably worse than the proposed approach. \n\n**Also a discussion on the results for masking weights vs. masking hidden units is missing. If I'm not mistaken, mathematically, hidden unit masking is a subset of weight masking, where masking an item in hidden activation is equivalent to masking an entire column in the weight matrix?**\n\t\nIn principle, the reviewer is correct: masking hidden units is technically a subset of weight masking. Effectively masking hidden representations is a strategy by which to select grouped sets of weights to mask simultaneously (i.e., all associated with individual nodes), whereas weight masking has more flexibility and no such grouping of masked weights. We therefore think it is intuitive conceptually to think about these as distinct strategies. And from an optimization point of view, masking hidden units may be easier to optimize.\n\n**Reply to comments:**\nWe thank the reviewer for the references and will add them to the related work section in the updated version. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3631/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3631/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Disentangling Representations of Text by Masking Transformers", "authorids": ["~Xiongyi_Zhang2", "~Jan-Willem_van_de_Meent1", "~Byron_C_Wallace1"], "authors": ["Xiongyi Zhang", "Jan-Willem van de Meent", "Byron C Wallace"], "keywords": ["disentanglement", "model pruning", "representation learning", "transformers"], "abstract": "Representations in large language models such as BERT encode a range of features into a single vector, which are predictive in the context of a multitude of downstream tasks. In this paper, we explore whether it is possible to learn disentangled representations  by identifying subnetworks in pre-trained models that encode distinct, complementary aspects of the representation. Concretely, we learn binary masks over transformer weights or hidden units to uncover the subset of features that correlate with a specific factor of variation. This sidesteps the need to train a disentangled model from scratch within a particular domain. We evaluate the ability of this method to disentangle representations of syntax and semantics, and sentiment from genre in the context of movie reviews. By combining this method with magnitude pruning we find that we can identify quite sparse subnetworks. Moreover, we find that this disentanglement-via-masking approach performs as well as or better than previously proposed methods based on variational autoencoders and adversarial training. ", "one-sentence_summary": "Learning disentangled representations by identifying subnetworks in pre-trained transformer models.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|disentangling_representations_of_text_by_masking_transformers", "pdf": "/pdf/f8a8cf9435778ae4038e813fe7d1fa3e1a22826b.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=_3CoaHN2lu", "_bibtex": "@misc{\nzhang2021disentangling,\ntitle={Disentangling Representations of Text by Masking Transformers},\nauthor={Xiongyi Zhang and Jan-Willem van de Meent and Byron C Wallace},\nyear={2021},\nurl={https://openreview.net/forum?id=Dmpi13JiqcX}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Dmpi13JiqcX", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3631/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3631/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3631/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3631/Authors|ICLR.cc/2021/Conference/Paper3631/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3631/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923835510, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3631/-/Official_Comment"}}}, {"id": "BawWjLEgF5k", "original": null, "number": 5, "cdate": 1605298858239, "ddate": null, "tcdate": 1605298858239, "tmdate": 1605298883919, "tddate": null, "forum": "Dmpi13JiqcX", "replyto": "iAbUhllI7EC", "invitation": "ICLR.cc/2021/Conference/Paper3631/-/Official_Comment", "content": {"title": "Response to Review #2", "comment": "We thank the reviewer for their detailed and insightful comments.  First, we would like to make one clarification on the reviewer's comment:\n\n **The triplet loss as formulated in this work seems to make it possible to disentangle only two factors of variation (a) and (b).** \n\nIt is true we have only shown the approach for two factors, but the method is sufficiently general to be amenable to additional factors, though one would have to construct triplets for all-pairs, which would not scale well to a large number of factors. We view extensions to such cases as an interesting direction for future work.\n\nWe respond to all questions and comments below.\n\n**Response to Questions & Comments**\n1. **What would performance look like if masks were trained after fine-tuning on sentiment/genre classification? Rather than training masks directly on top of BERT-base. It would be interesting to see if the model is stable to recover from fine-tuning on data with spurious correlations and still produce disentangled representations.**\n\nThis is an interesting question, and something we did not try. We would conduct more experiments to verify this and update our paper with the results as soon as possible. We are thankful for the suggestion!\n\n2. **Is every single weight/activation masked at every transformer layer? The paper seems to lack some specifics about exactly what layers/weights are masked. Along these lines, did you experiment with masking only the last few layers? This could save time & parameters**\n\nThe reviewer is correct to point out that we should have been more explicit about this; We only mask the last 9 layers of the model (which we found in preliminary experiments on dev data to work well). It appears we omitted this implementation detail and we will clarify this. \n\n3. **In Figure 3 is the model training with L_{cls} corresponding to sentiment and then visualized for sentiment and genre? Or is the top trained with the supervised sentiment loss and the bottom for supervised genre loss?**\n\nThe latter; the top is trained with sentiment loss and the bottom genre loss. \n\n4.**It would be interesting to explore an L1 penalty on the masks for increasing sparsity, possibly in conjunction with magnitude pruning as well.**\n\nThank you for this suggestion. We did have an L1-penalty that discourages the mask for different attributes to be on (equal to one) in the same position. The idea there is to encourage mutually exclusive masks. But it would be interesting to also explore an L1 penalty on the masks to improve overall sparsity. We plan to do more experiments on that for the camera-ready version. More generally, we hope this approach suggests a line of alternative sparsity-inducing methods for disentanglement via masking.\n\n5. **The WC task doesn't feel very representative of sentence \"semantics\"**\n\nWe agree that the degree to which a representation captures \u201csemantics\u201d is hard to measure (or even define). We here follow the setting established in prior work (Conneau et al., 2018) regarding the \u201csemantic\u201d probing tasks. It captures a lexical level of \u201csemantics\u201d, which is often used as a substitute for the real \u201csemantics\u201d. Note that we also use a semantic-similarity task (STS) to better capture the sentence level semantics.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3631/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3631/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Disentangling Representations of Text by Masking Transformers", "authorids": ["~Xiongyi_Zhang2", "~Jan-Willem_van_de_Meent1", "~Byron_C_Wallace1"], "authors": ["Xiongyi Zhang", "Jan-Willem van de Meent", "Byron C Wallace"], "keywords": ["disentanglement", "model pruning", "representation learning", "transformers"], "abstract": "Representations in large language models such as BERT encode a range of features into a single vector, which are predictive in the context of a multitude of downstream tasks. In this paper, we explore whether it is possible to learn disentangled representations  by identifying subnetworks in pre-trained models that encode distinct, complementary aspects of the representation. Concretely, we learn binary masks over transformer weights or hidden units to uncover the subset of features that correlate with a specific factor of variation. This sidesteps the need to train a disentangled model from scratch within a particular domain. We evaluate the ability of this method to disentangle representations of syntax and semantics, and sentiment from genre in the context of movie reviews. By combining this method with magnitude pruning we find that we can identify quite sparse subnetworks. Moreover, we find that this disentanglement-via-masking approach performs as well as or better than previously proposed methods based on variational autoencoders and adversarial training. ", "one-sentence_summary": "Learning disentangled representations by identifying subnetworks in pre-trained transformer models.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|disentangling_representations_of_text_by_masking_transformers", "pdf": "/pdf/f8a8cf9435778ae4038e813fe7d1fa3e1a22826b.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=_3CoaHN2lu", "_bibtex": "@misc{\nzhang2021disentangling,\ntitle={Disentangling Representations of Text by Masking Transformers},\nauthor={Xiongyi Zhang and Jan-Willem van de Meent and Byron C Wallace},\nyear={2021},\nurl={https://openreview.net/forum?id=Dmpi13JiqcX}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Dmpi13JiqcX", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3631/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3631/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3631/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3631/Authors|ICLR.cc/2021/Conference/Paper3631/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3631/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923835510, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3631/-/Official_Comment"}}}, {"id": "pZXSvm1NEw", "original": null, "number": 4, "cdate": 1605297321858, "ddate": null, "tcdate": 1605297321858, "tmdate": 1605297321858, "tddate": null, "forum": "Dmpi13JiqcX", "replyto": "dWdl7GTuvUv", "invitation": "ICLR.cc/2021/Conference/Paper3631/-/Official_Comment", "content": {"title": "Response to Review #4", "comment": "We thank the reviewer for their comments, and we are glad the reviewer found this new direction to be interesting.\nWe would like to address two of the main concerns raised in this review, before responding to specific questions: \n\nLack of comparison to variational auto-encoders: We believe this may be a misunderstanding on the part of the reviewer.  We in fact compare with VGVAE, which is a VAE model, in our experiments involving disentangling semantics and syntax. We show results in Figure 4, which demonstrates that the proposed method outperforms VGVAE.  Because this VGVAE model was specifically designed for disentangling semantics and syntax, we did not include comparisons to it in the sentiment/genre experiment. We agree that a comparison to a (different) VAE-based baseline in the sentiment/genre experiment would strengthen the work, and plan to update the paper with the results from this as soon as possible.\n\nThe reviewer correctly points out that we do not achieve \u201cstate of the art\u201d (SOTA) on any particular standard benchmark dataset. However this is not the primary aim of this paper. Our interest here is to learn disentangled representations, and there is no existing benchmark for disentanglement in NLP. We aim to achieve this in service of robustness, e.g., to make models less sensitive to spurious correlations (as mentioned by R2). Our experiments are therefore designed to probe the degree to which the proposed approach achieves disentanglement (and robustness); we think (as does R1) that the results are convincing in this respect. \n\nMore generally, while achieving SOTA on benchmark datasets is one means of showing the value of particular methods or approaches, we argue that we should not, as a community, require all research to be focussed on topping leaderboards. For example, this would largely preclude any work on robustness and interpretability, which are key open problems in NLP and ML more broadly. (See also Ethayarajh and Jurafsky, 2020: https://arxiv.org/abs/2009.13888).\n\nResponse to your other questions:\n\nQ1: Would training binary masks be a speedup over fine-tuning?\nThere is no reason to believe that training binary masks will be faster than fine-tuning the model. However, binary masks do have the advantage of requiring less memory, which is often at a premium in GPU-based computations. .\n\nQ2:Does using the pretrained model (vs. one trained from scratch) help?\n\nIn our approach we do not fine-tune BERT, so if we randomly initialized this the method would not work. The insight here is to uncover existing subnetworks that yield disentangled representations from pretrained models, so training BERT from scratch would not be a viable approach here.\n\nQ3:Have you considered masking a subset of the weights/activations (e.g. only in the last layer)?\nGreat question. We only mask the last 9 layers of the model (which we found in preliminary experiments on dev data to work well). It appears we omitted this implementation detail and we will clarify this. \n\nQ4: Do you have any intuition about the learned masks? E.g. are most weights/activations being removed? How much overlap is there between the masks learned for each attribute?\nFor Sec 3.1 and Sec 3.2, only a small percentage of the weights/activations are masked (~1%) at convergence. We note this is an interesting finding in its own right; apparently masking out a small fraction of weights can substantially affect the degree of disentanglement. The overlap is very small between the two attributes (close to 0), which is intuitive. \n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3631/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3631/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Disentangling Representations of Text by Masking Transformers", "authorids": ["~Xiongyi_Zhang2", "~Jan-Willem_van_de_Meent1", "~Byron_C_Wallace1"], "authors": ["Xiongyi Zhang", "Jan-Willem van de Meent", "Byron C Wallace"], "keywords": ["disentanglement", "model pruning", "representation learning", "transformers"], "abstract": "Representations in large language models such as BERT encode a range of features into a single vector, which are predictive in the context of a multitude of downstream tasks. In this paper, we explore whether it is possible to learn disentangled representations  by identifying subnetworks in pre-trained models that encode distinct, complementary aspects of the representation. Concretely, we learn binary masks over transformer weights or hidden units to uncover the subset of features that correlate with a specific factor of variation. This sidesteps the need to train a disentangled model from scratch within a particular domain. We evaluate the ability of this method to disentangle representations of syntax and semantics, and sentiment from genre in the context of movie reviews. By combining this method with magnitude pruning we find that we can identify quite sparse subnetworks. Moreover, we find that this disentanglement-via-masking approach performs as well as or better than previously proposed methods based on variational autoencoders and adversarial training. ", "one-sentence_summary": "Learning disentangled representations by identifying subnetworks in pre-trained transformer models.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|disentangling_representations_of_text_by_masking_transformers", "pdf": "/pdf/f8a8cf9435778ae4038e813fe7d1fa3e1a22826b.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=_3CoaHN2lu", "_bibtex": "@misc{\nzhang2021disentangling,\ntitle={Disentangling Representations of Text by Masking Transformers},\nauthor={Xiongyi Zhang and Jan-Willem van de Meent and Byron C Wallace},\nyear={2021},\nurl={https://openreview.net/forum?id=Dmpi13JiqcX}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Dmpi13JiqcX", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3631/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3631/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3631/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3631/Authors|ICLR.cc/2021/Conference/Paper3631/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3631/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923835510, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3631/-/Official_Comment"}}}, {"id": "0DZtJZeDaDY", "original": null, "number": 2, "cdate": 1603946376942, "ddate": null, "tcdate": 1603946376942, "tmdate": 1605023965894, "tddate": null, "forum": "Dmpi13JiqcX", "replyto": "Dmpi13JiqcX", "invitation": "ICLR.cc/2021/Conference/Paper3631/-/Official_Review", "content": {"title": "Light-weight approach to untangle language model representations", "review": "This paper proposes a masking strategy to identify subnetworks within language models responsible for predicting different text features. This approach requires no fine-tuning of model parameters and still achieves better results compared to previous approaches. Their experimental results on the movie domain show some level of disentanglement is achieved between sentiment and genre. Disentanglement capabilities of their model between sentence semantics and structure, are also tested on four tasks. \n\nPros:\n- Paper is well-written and the idea is explained well.\n- Experiment results are convincing and support the claims.\n- Achieving comparable results to SOTA without the need to train or finetune models is interesting especially from a computational point of view.\n\nCons:\n- I wish the authors performed their first experiment on more domains: books, music, etc. and consider more than two labels.\nFrom current results, it's hard to confidently conclude that this approach is generalizable.\n- Judging based on Figure 4 results I'm not convinced that the proposed approach does better than the *finetuned* (which I believe has a trained classifier on top of BERT) approach especially for Semantic tasks. Perhaps a discussion/ error analysis would be appropriate given better results on Syntax tasks.\n- Also a discussion on the results for masking weights vs. masking hidden units is missing. If I'm not mistaken, mathematically, hidden unit masking is a subset of weight masking, where masking an item in hidden activation is equivalent to masking an entire column in the weight matrix?\n\n\nComments:\n- Although the idea of masking model parameters to achieve untanglment is new, there has been [previous work](https://www.aclweb.org/anthology/P18-1069.pdf) on using dropout to identify sub-parts of the network that contribute more/ less to model predictions framed as a confidence modeling task. Authors may consider adding it to related work.\n- Another missed citation under related work is [HUBERT](https://arxiv.org/pdf/1910.12647.pdf) which examines untanglement of semantics and structure across a wide range of NLP tasks.\n\n\nMinor typos:\n- \"we *measure* evaluate them on four tasks ...\" on page 7 \n- \"Technically, in the Pruned + Masked Weights method, *the* refining the masks ...\" ", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3631/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3631/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Disentangling Representations of Text by Masking Transformers", "authorids": ["~Xiongyi_Zhang2", "~Jan-Willem_van_de_Meent1", "~Byron_C_Wallace1"], "authors": ["Xiongyi Zhang", "Jan-Willem van de Meent", "Byron C Wallace"], "keywords": ["disentanglement", "model pruning", "representation learning", "transformers"], "abstract": "Representations in large language models such as BERT encode a range of features into a single vector, which are predictive in the context of a multitude of downstream tasks. In this paper, we explore whether it is possible to learn disentangled representations  by identifying subnetworks in pre-trained models that encode distinct, complementary aspects of the representation. Concretely, we learn binary masks over transformer weights or hidden units to uncover the subset of features that correlate with a specific factor of variation. This sidesteps the need to train a disentangled model from scratch within a particular domain. We evaluate the ability of this method to disentangle representations of syntax and semantics, and sentiment from genre in the context of movie reviews. By combining this method with magnitude pruning we find that we can identify quite sparse subnetworks. Moreover, we find that this disentanglement-via-masking approach performs as well as or better than previously proposed methods based on variational autoencoders and adversarial training. ", "one-sentence_summary": "Learning disentangled representations by identifying subnetworks in pre-trained transformer models.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|disentangling_representations_of_text_by_masking_transformers", "pdf": "/pdf/f8a8cf9435778ae4038e813fe7d1fa3e1a22826b.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=_3CoaHN2lu", "_bibtex": "@misc{\nzhang2021disentangling,\ntitle={Disentangling Representations of Text by Masking Transformers},\nauthor={Xiongyi Zhang and Jan-Willem van de Meent and Byron C Wallace},\nyear={2021},\nurl={https://openreview.net/forum?id=Dmpi13JiqcX}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Dmpi13JiqcX", "replyto": "Dmpi13JiqcX", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3631/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538072439, "tmdate": 1606915792556, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3631/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3631/-/Official_Review"}}}, {"id": "iAbUhllI7EC", "original": null, "number": 3, "cdate": 1603955665812, "ddate": null, "tcdate": 1603955665812, "tmdate": 1605023965827, "tddate": null, "forum": "Dmpi13JiqcX", "replyto": "Dmpi13JiqcX", "invitation": "ICLR.cc/2021/Conference/Paper3631/-/Official_Review", "content": {"title": "Official Review - AnonReviewer2", "review": "The paper presents a way to learn disentangled representations with respect to target attributes of interest by learning to mask weights or activations. A particular piece of text is encoded into distinct vectors that capture different factors of variation in the data. The method involves learning masks for each factor of variation while keeping the pre-trained model parameters fixed. The masks for every layer are trained using a combination of a triplet-loss, attribute classification loss, and one that encourages masks for different factors to be different across all layers. The triplet loss forces representations of examples that are similar with respect to a particular attribute to be closer than one that are similar based on another attribute.\n\nModels are evaluated on a sentiment/genre classification on a dataset sampled in such a way that introduces spurious correlations between genre and sentiment but evaluated on data that does not have any such correlation. The approach is also evaluated on disentangling syntax and semantics.\n\nStrengths\n\nBuilding models that are robust to spurious correlations in data is important for a variety of reasons and learning disentangled representations is a promising way to achieve that. This paper shows good generalization performance on datasets with such characteristics.\n\nThe overall approach is simple and only requires training masks over weights/activations at each layer. The masks are trained with a fairly straightforward choice of training objectives.\n\nThe paper is well written and the overall approach is easy to understand.\n\nWeaknesses\n\nThe triplet loss as formulated in this work seems to make it possible to disentangle only two factors of variation (a) and (b).\n\nThere is still a fair amount of attribute leakage and the probe designed to measure this leak is only a single layer MLP, there might be more leakage with stronger probes.\n\nThe weight masking strategy significantly increases the number of parameters (although the masks are binary, so it just requires a single bit as opposed to 16/32 bit floating point numbers). In this particular work, the number of parameters triples, and it scales linearly with the number of attributes as well.\n\nIt requires running the model forward multiple times to get representations that encode different factors of variation.\n\n\nQuestions & Comments\n\nWhat would performance look like if masks were trained after fine-tuning on sentiment/genre classification? Rather than training masks directly on top of BERT-base. It would be interesting to see if the model is stable to recover from fine-tuning on data with spurious correlations and still produce disentangled representations.\n\nIs every single weight/activation masked at every transformer layer? The paper seems to lack some specifics about exactly what layers/weights are masked. Along these lines, did you experiment with masking only the last few layers? This could save time & parameters\n\nIn Figure 3 is the model training with L_{cls} corresponding to sentiment and then visualized for sentiment and genre? Or is the top trained with the supervised sentiment loss and the bottom for supervised genre loss?\n\nIt would be interesting to explore an L1 penalty on the masks for increasing sparsity, possibly in conjunction with magnitude pruning as well.\n\nThe WC task doesn't feel very representative of sentence \"semantics\"", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3631/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3631/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Disentangling Representations of Text by Masking Transformers", "authorids": ["~Xiongyi_Zhang2", "~Jan-Willem_van_de_Meent1", "~Byron_C_Wallace1"], "authors": ["Xiongyi Zhang", "Jan-Willem van de Meent", "Byron C Wallace"], "keywords": ["disentanglement", "model pruning", "representation learning", "transformers"], "abstract": "Representations in large language models such as BERT encode a range of features into a single vector, which are predictive in the context of a multitude of downstream tasks. In this paper, we explore whether it is possible to learn disentangled representations  by identifying subnetworks in pre-trained models that encode distinct, complementary aspects of the representation. Concretely, we learn binary masks over transformer weights or hidden units to uncover the subset of features that correlate with a specific factor of variation. This sidesteps the need to train a disentangled model from scratch within a particular domain. We evaluate the ability of this method to disentangle representations of syntax and semantics, and sentiment from genre in the context of movie reviews. By combining this method with magnitude pruning we find that we can identify quite sparse subnetworks. Moreover, we find that this disentanglement-via-masking approach performs as well as or better than previously proposed methods based on variational autoencoders and adversarial training. ", "one-sentence_summary": "Learning disentangled representations by identifying subnetworks in pre-trained transformer models.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|disentangling_representations_of_text_by_masking_transformers", "pdf": "/pdf/f8a8cf9435778ae4038e813fe7d1fa3e1a22826b.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=_3CoaHN2lu", "_bibtex": "@misc{\nzhang2021disentangling,\ntitle={Disentangling Representations of Text by Masking Transformers},\nauthor={Xiongyi Zhang and Jan-Willem van de Meent and Byron C Wallace},\nyear={2021},\nurl={https://openreview.net/forum?id=Dmpi13JiqcX}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Dmpi13JiqcX", "replyto": "Dmpi13JiqcX", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3631/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538072439, "tmdate": 1606915792556, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3631/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3631/-/Official_Review"}}}, {"id": "r-ps45B8POr", "original": null, "number": 1, "cdate": 1603806785285, "ddate": null, "tcdate": 1603806785285, "tmdate": 1605023965759, "tddate": null, "forum": "Dmpi13JiqcX", "replyto": "Dmpi13JiqcX", "invitation": "ICLR.cc/2021/Conference/Paper3631/-/Official_Review", "content": {"title": "an interesting work on disentangling representations of text", "review": "The paper proposes a problem of disentangling representations generated in pretraining models, such as BERT. That is, it is possible to learn disentangled representations that encode distinct, complementary aspect representations. To this end, the authors proposes a method that employs the mask technique on transformer weights or hidden units to find the subset of features correlating with a specific task. The experimental results show that the proposed method can encode particular aspects while weakly encoding others. The main contributions of the paper is the introduction of binary masks to identifying some subnetworks, which may correlate with specific tasks, within pretrained models. Overall, the paper is well written and is easy to follow. \n\nConcerns:\n1. The experimental setup is not convincing. The authors just consider movie reviews corresponding to Drama and Horror from IMDB and exclude reviews corresponding to other genres. It is obvious that considering only two genres is not convincing and more genres should be considered in the experiments. So, the authors should answer the following questions: (1) Why do the authors just selected these two specific genres to conduct the experiments? (2) Do the authors conduct similar experiments on other genres and what about the experimental results?\n2. Figure 3 does not show that proposed method achieves better results than do the two baselines. In fact, the finetuned baseline performs very well according to Figure 3. I suggest that the author adopts some quantitative measures to accurately reflect the differences.\n3. In addition, how to use these disentangling representations in downstream tasks, such as text classification, natural language inference, and semantic similarity? It is better to discuss and conduct experiment to show the advantages of their disentangling representations in downstream tasks.\n\nMinor comments:\n1. In Formula (9), the parentheses are redundant.\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3631/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3631/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Disentangling Representations of Text by Masking Transformers", "authorids": ["~Xiongyi_Zhang2", "~Jan-Willem_van_de_Meent1", "~Byron_C_Wallace1"], "authors": ["Xiongyi Zhang", "Jan-Willem van de Meent", "Byron C Wallace"], "keywords": ["disentanglement", "model pruning", "representation learning", "transformers"], "abstract": "Representations in large language models such as BERT encode a range of features into a single vector, which are predictive in the context of a multitude of downstream tasks. In this paper, we explore whether it is possible to learn disentangled representations  by identifying subnetworks in pre-trained models that encode distinct, complementary aspects of the representation. Concretely, we learn binary masks over transformer weights or hidden units to uncover the subset of features that correlate with a specific factor of variation. This sidesteps the need to train a disentangled model from scratch within a particular domain. We evaluate the ability of this method to disentangle representations of syntax and semantics, and sentiment from genre in the context of movie reviews. By combining this method with magnitude pruning we find that we can identify quite sparse subnetworks. Moreover, we find that this disentanglement-via-masking approach performs as well as or better than previously proposed methods based on variational autoencoders and adversarial training. ", "one-sentence_summary": "Learning disentangled representations by identifying subnetworks in pre-trained transformer models.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|disentangling_representations_of_text_by_masking_transformers", "pdf": "/pdf/f8a8cf9435778ae4038e813fe7d1fa3e1a22826b.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=_3CoaHN2lu", "_bibtex": "@misc{\nzhang2021disentangling,\ntitle={Disentangling Representations of Text by Masking Transformers},\nauthor={Xiongyi Zhang and Jan-Willem van de Meent and Byron C Wallace},\nyear={2021},\nurl={https://openreview.net/forum?id=Dmpi13JiqcX}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Dmpi13JiqcX", "replyto": "Dmpi13JiqcX", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3631/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538072439, "tmdate": 1606915792556, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3631/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3631/-/Official_Review"}}}, {"id": "dWdl7GTuvUv", "original": null, "number": 4, "cdate": 1604272463593, "ddate": null, "tcdate": 1604272463593, "tmdate": 1605023965690, "tddate": null, "forum": "Dmpi13JiqcX", "replyto": "Dmpi13JiqcX", "invitation": "ICLR.cc/2021/Conference/Paper3631/-/Official_Review", "content": {"title": "New approach to disentangling representations", "review": "**Summary**:\n\nThe paper proposes a procedure to extract disentangled representations from pretrained BERT models. In particular, the paper proposes learning binary masks over BERT weights (or, as an alternative, over BERT activations) such that the resulting representations correspond to the desired aspect representations. The model requires additional supervision (binary labels or example triplets) and training (for the masks but not the BERT weights). The experiments aim to perform disentangling to ensure that (1) the learned representation does not \u201cleak\u201d a potentially sensitive attribute, and (2) the downstream classifier\u2019s performance is good across all subgroups formed by the attributes. The experiments show that the proposed method outperforms baselines such as unmasked BERT, unmasked-but-finetuned BERT, and unmasked-but-adversarially-finetuned BERT.\n\n**Concerns**:\n\nThe fact that one can uncover disentangled representations from BERT models by masking weights/activations is a nice result and I'm not aware of similar approaches for BERT. However, it's unclear from the paper that this approach outperforms previous alternatives:\n* First, the abstract mentions that the approach is the same or better than variational auto-encoder approaches, and I don't see it mentioned elsewhere in the main text. Am I missing something?\n* Second, the paper does not show improved results on any benchmarks.\nAs a result, I'm not sure whether the paper will be impactful enough for the community.\n\n**Other Questions**:\n\n* The proposed approach involves training binary masks rather than fine-tuning the BERT weights. Given that the mask has the same shape as the weights, it's unclear whether this is a major speedup. Could you discuss this more?\n* Does using the pretrained model (vs. one trained from scratch) help?\n* Have you considered masking a subset of the weights/activations (e.g. only in the last layer)?\n* Do you have any intuition about the learned masks? E.g. are most weights/activations being removed? How much overlap is there between the masks learned for each attribute?\n\nOverall, I like this research direction, but I think it requires more work to be accepted.", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3631/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3631/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Disentangling Representations of Text by Masking Transformers", "authorids": ["~Xiongyi_Zhang2", "~Jan-Willem_van_de_Meent1", "~Byron_C_Wallace1"], "authors": ["Xiongyi Zhang", "Jan-Willem van de Meent", "Byron C Wallace"], "keywords": ["disentanglement", "model pruning", "representation learning", "transformers"], "abstract": "Representations in large language models such as BERT encode a range of features into a single vector, which are predictive in the context of a multitude of downstream tasks. In this paper, we explore whether it is possible to learn disentangled representations  by identifying subnetworks in pre-trained models that encode distinct, complementary aspects of the representation. Concretely, we learn binary masks over transformer weights or hidden units to uncover the subset of features that correlate with a specific factor of variation. This sidesteps the need to train a disentangled model from scratch within a particular domain. We evaluate the ability of this method to disentangle representations of syntax and semantics, and sentiment from genre in the context of movie reviews. By combining this method with magnitude pruning we find that we can identify quite sparse subnetworks. Moreover, we find that this disentanglement-via-masking approach performs as well as or better than previously proposed methods based on variational autoencoders and adversarial training. ", "one-sentence_summary": "Learning disentangled representations by identifying subnetworks in pre-trained transformer models.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|disentangling_representations_of_text_by_masking_transformers", "pdf": "/pdf/f8a8cf9435778ae4038e813fe7d1fa3e1a22826b.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=_3CoaHN2lu", "_bibtex": "@misc{\nzhang2021disentangling,\ntitle={Disentangling Representations of Text by Masking Transformers},\nauthor={Xiongyi Zhang and Jan-Willem van de Meent and Byron C Wallace},\nyear={2021},\nurl={https://openreview.net/forum?id=Dmpi13JiqcX}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Dmpi13JiqcX", "replyto": "Dmpi13JiqcX", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3631/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538072439, "tmdate": 1606915792556, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3631/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3631/-/Official_Review"}}}], "count": 14}