{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1528124429951, "tcdate": 1518473790125, "number": 359, "cdate": 1518473790125, "id": "BJLSGcywG", "invitation": "ICLR.cc/2018/Workshop/-/Submission", "forum": "BJLSGcywG", "signatures": ["~Super_User1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop"], "content": {"title": "An Analysis of the Delayed Gradients Problem in Asynchronous SGD", "abstract": "Stochastic Gradient Descent can be effectively paralellized to many workers via the use of minibatches. Yet, parameter synchronization requires that each batch waits for the slowest worker to finish. There is a fully asynchronous method, detailed by Dean et. al (2012), called Downpour SGD, or ASGD, which minimizes worker idle time by allowing gradients which are computed on stale parameters to be sent to the parameter server. In practice, direct usage of ASGD is not recommended due to the added noise from stale gradients (referred as the \"delayed gradient problem\"), and therefore some form of delay compensation is required, as is detailed in Zheng et al (2017). In this paper, we present a detailed analysis of the failure modes of asynchronous SGD due to delayed gradients under various hyperparameter selections in order to better inform in what cases ASGD is best applied. On the MNIST digit recognition task with the LeNet5 model we find that delayed gradients significantly reduce test accuracy with large batch sizes and large learning rates. This limits the applicability of asynchronous gradient methods (without delay compensation) in cases where the learning rate is scaled linearly with the batch size, or with adaptive methods that may select large learning rates at times. Finally, we provide discussion on possible delay compensation methods.", "paperhash": "srinivasan|an_analysis_of_the_delayed_gradients_problem_in_asynchronous_sgd", "keywords": ["asynchronous", "SGD", "gradient descent", "parallel training", "workers", "reduction", "delayed gradient", "convolutional neural network"], "_bibtex": "@misc{\n  srinivasan2018an,\n  title={An Analysis of the Delayed Gradients Problem in Asynchronous SGD},\n  author={Anand Srinivasan and Ajay Jain and Parnian Barekatain},\n  year={2018},\n  url={https://openreview.net/forum?id=BJLSGcywG}\n}", "authorids": ["anandtech1532@gmail.com", "ajayjain@mit.edu", "parni@ideapad.io"], "authors": ["Anand Srinivasan", "Ajay Jain", "Parnian Barekatain"], "TL;DR": "A causal analysis of test accuracy reduction when using asynchronous SGD, and a recommendation of hyperparameter bounds in which to use delayed gradients.", "pdf": "/pdf/16a1a9da05f5ae5e24ace9d3e2b2fa37c61ec260.pdf"}, "nonreaders": [], "details": {"replyCount": 5, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1518472800000, "tmdate": 1518474081690, "id": "ICLR.cc/2018/Workshop/-/Submission", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Workshop"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2018/Workshop", "description": "Your authorized identity to be associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 9, "value-regex": "upload", "description": "Upload a PDF file that ends with .pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 8, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names. Please provide real names; identities will be anonymized."}, "keywords": {"order": 6, "values-regex": "(^$)|[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of keywords."}, "TL;DR": {"required": false, "order": 7, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,500}"}, "authorids": {"required": true, "order": 3, "values-regex": "([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,},){0,}([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,})", "description": "Comma separated list of author email addresses, lowercased, in the same order as above. For authors with existing OpenReview accounts, please make sure that the provided email address(es) match those listed in the author's profile. Please provide real emails; identities will be anonymized."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1526248800000, "cdate": 1518474081690}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582848411, "tcdate": 1520581901114, "number": 1, "cdate": 1520581901114, "id": "ByHGp2JFz", "invitation": "ICLR.cc/2018/Workshop/-/Paper359/Official_Review", "forum": "BJLSGcywG", "replyto": "BJLSGcywG", "signatures": ["ICLR.cc/2018/Workshop/Paper359/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper359/AnonReviewer3"], "content": {"title": "need deeper analysis on learning curves", "rating": "3: Clear rejection", "review": "The quality of writing needs big improvements. Most of claims made in Section 3 are made without reference to specific data points, making it very difficult to judge the validity of the claim. For example, authors mention that 'We observe that regardless of delay amount, test accuracy scales smoothly upward with lr, then sharply drops off', it is unclear which figure authors are referring to. None of the figures report results with different learning rate parameters. Another example: We find that optimizers which employ per-parameter adaptive learning rates, such as Adam (Kingma & Ba, 2014), increase resiliency to learning rate and batch size selections, achieving test accuracy comparable to baseline up to 256 delay, in comparison to SGD, which cannot handle more than 32 delay'. Again, the claim is made without reference to actual data point. In this case, I can guess that authors are referring to Figure 3, left. But it is unclear to me what range of learning rates were tried, and how authors chose specific learning rate and batch size each delay setting. The claim that 'SGD cannot handle more than 32 delay' should be refined to be more specific: When learning rates are very small, I believe SGD will be able to converge to a good solution, although it will take a long time. \n\nAlso, many of the claims authors make are applicable to synchronous SGD, and it is unclear how much of the effect they observe is due to the asynchronous aspect of optimization. Even in synchronous SGD, it is very well known that using smaller learning rates and smaller batch sizes is more stable than using larger learning rates and larger batch sizes. ", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "An Analysis of the Delayed Gradients Problem in Asynchronous SGD", "abstract": "Stochastic Gradient Descent can be effectively paralellized to many workers via the use of minibatches. Yet, parameter synchronization requires that each batch waits for the slowest worker to finish. There is a fully asynchronous method, detailed by Dean et. al (2012), called Downpour SGD, or ASGD, which minimizes worker idle time by allowing gradients which are computed on stale parameters to be sent to the parameter server. In practice, direct usage of ASGD is not recommended due to the added noise from stale gradients (referred as the \"delayed gradient problem\"), and therefore some form of delay compensation is required, as is detailed in Zheng et al (2017). In this paper, we present a detailed analysis of the failure modes of asynchronous SGD due to delayed gradients under various hyperparameter selections in order to better inform in what cases ASGD is best applied. On the MNIST digit recognition task with the LeNet5 model we find that delayed gradients significantly reduce test accuracy with large batch sizes and large learning rates. This limits the applicability of asynchronous gradient methods (without delay compensation) in cases where the learning rate is scaled linearly with the batch size, or with adaptive methods that may select large learning rates at times. Finally, we provide discussion on possible delay compensation methods.", "paperhash": "srinivasan|an_analysis_of_the_delayed_gradients_problem_in_asynchronous_sgd", "keywords": ["asynchronous", "SGD", "gradient descent", "parallel training", "workers", "reduction", "delayed gradient", "convolutional neural network"], "_bibtex": "@misc{\n  srinivasan2018an,\n  title={An Analysis of the Delayed Gradients Problem in Asynchronous SGD},\n  author={Anand Srinivasan and Ajay Jain and Parnian Barekatain},\n  year={2018},\n  url={https://openreview.net/forum?id=BJLSGcywG}\n}", "authorids": ["anandtech1532@gmail.com", "ajayjain@mit.edu", "parni@ideapad.io"], "authors": ["Anand Srinivasan", "Ajay Jain", "Parnian Barekatain"], "TL;DR": "A causal analysis of test accuracy reduction when using asynchronous SGD, and a recommendation of hyperparameter bounds in which to use delayed gradients.", "pdf": "/pdf/16a1a9da05f5ae5e24ace9d3e2b2fa37c61ec260.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582848186, "id": "ICLR.cc/2018/Workshop/-/Paper359/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper359/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper359/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper359/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper359/AnonReviewer1"], "reply": {"forum": "BJLSGcywG", "replyto": "BJLSGcywG", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper359/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper359/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582848186}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582760782, "tcdate": 1520644089425, "number": 2, "cdate": 1520644089425, "id": "SkZZe2lYG", "invitation": "ICLR.cc/2018/Workshop/-/Paper359/Official_Review", "forum": "BJLSGcywG", "replyto": "BJLSGcywG", "signatures": ["ICLR.cc/2018/Workshop/Paper359/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper359/AnonReviewer2"], "content": {"title": "Useful to community, but not much novelty.", "rating": "6: Marginally above acceptance threshold", "review": "Summary: \nThe paper provides an analysis of the impact of delayed gradients in Asynchronous SGD. The authors create a simple model of delays and use that for the analysis. The focus is on impact of learning rate and batch sizes across two optimizers (ASGD and Adam). Adaptive optimizers do better than ASGD and are less sensitive to these two parameters. All experiments are performed for the LeNet-5 model on the MNIST dataset\n\nPros:\n- Analysis shows the significant sensitivity to delayed gradients and the rapid reduction of the hyper-parameter space that yields good results  with increasing delays.\n- This is consistent with what many papers over the years have shown but not analyzed earlier in this much detail.\n- Adam is more resilient to delays and allows for a much larger range of learning rates and batch sizes even with longer delays\n\nCons\n- Experiments are limited to 1 model on MNIST, it is unclear if they will hold for different kinds of models or larger datasets\n- Not much novelty since the experimental results mostly agree with prior work\n\nMay be worth accepting primarily to share the details of the analysis with the community and to encourage more analysis in this area.", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "An Analysis of the Delayed Gradients Problem in Asynchronous SGD", "abstract": "Stochastic Gradient Descent can be effectively paralellized to many workers via the use of minibatches. Yet, parameter synchronization requires that each batch waits for the slowest worker to finish. There is a fully asynchronous method, detailed by Dean et. al (2012), called Downpour SGD, or ASGD, which minimizes worker idle time by allowing gradients which are computed on stale parameters to be sent to the parameter server. In practice, direct usage of ASGD is not recommended due to the added noise from stale gradients (referred as the \"delayed gradient problem\"), and therefore some form of delay compensation is required, as is detailed in Zheng et al (2017). In this paper, we present a detailed analysis of the failure modes of asynchronous SGD due to delayed gradients under various hyperparameter selections in order to better inform in what cases ASGD is best applied. On the MNIST digit recognition task with the LeNet5 model we find that delayed gradients significantly reduce test accuracy with large batch sizes and large learning rates. This limits the applicability of asynchronous gradient methods (without delay compensation) in cases where the learning rate is scaled linearly with the batch size, or with adaptive methods that may select large learning rates at times. Finally, we provide discussion on possible delay compensation methods.", "paperhash": "srinivasan|an_analysis_of_the_delayed_gradients_problem_in_asynchronous_sgd", "keywords": ["asynchronous", "SGD", "gradient descent", "parallel training", "workers", "reduction", "delayed gradient", "convolutional neural network"], "_bibtex": "@misc{\n  srinivasan2018an,\n  title={An Analysis of the Delayed Gradients Problem in Asynchronous SGD},\n  author={Anand Srinivasan and Ajay Jain and Parnian Barekatain},\n  year={2018},\n  url={https://openreview.net/forum?id=BJLSGcywG}\n}", "authorids": ["anandtech1532@gmail.com", "ajayjain@mit.edu", "parni@ideapad.io"], "authors": ["Anand Srinivasan", "Ajay Jain", "Parnian Barekatain"], "TL;DR": "A causal analysis of test accuracy reduction when using asynchronous SGD, and a recommendation of hyperparameter bounds in which to use delayed gradients.", "pdf": "/pdf/16a1a9da05f5ae5e24ace9d3e2b2fa37c61ec260.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582848186, "id": "ICLR.cc/2018/Workshop/-/Paper359/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper359/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper359/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper359/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper359/AnonReviewer1"], "reply": {"forum": "BJLSGcywG", "replyto": "BJLSGcywG", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper359/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper359/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582848186}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582747130, "tcdate": 1520657873509, "number": 3, "cdate": 1520657873509, "id": "SyYRrJbtz", "invitation": "ICLR.cc/2018/Workshop/-/Paper359/Official_Review", "forum": "BJLSGcywG", "replyto": "BJLSGcywG", "signatures": ["ICLR.cc/2018/Workshop/Paper359/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper359/AnonReviewer1"], "content": {"title": "Empirical evaluation of the performance of ASGD on various configurations of learning rate and batch size", "rating": "4: Ok but not good enough - rejection", "review": "The authors empirically evaluate the performance of ASGD on various configurations of learning rate and batch size. This suggests a guidance to the choice of these hyperparameters when using ASGD. Although it is useful practical information, I'm not sure suggesting a good hyperparameter range is enough contribution for publication. Also, the conclusion is drawn only from MNIST experiments. It is required to do more experiments on a broader range of tasks/datasets to make the claim more persuasive. ", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "An Analysis of the Delayed Gradients Problem in Asynchronous SGD", "abstract": "Stochastic Gradient Descent can be effectively paralellized to many workers via the use of minibatches. Yet, parameter synchronization requires that each batch waits for the slowest worker to finish. There is a fully asynchronous method, detailed by Dean et. al (2012), called Downpour SGD, or ASGD, which minimizes worker idle time by allowing gradients which are computed on stale parameters to be sent to the parameter server. In practice, direct usage of ASGD is not recommended due to the added noise from stale gradients (referred as the \"delayed gradient problem\"), and therefore some form of delay compensation is required, as is detailed in Zheng et al (2017). In this paper, we present a detailed analysis of the failure modes of asynchronous SGD due to delayed gradients under various hyperparameter selections in order to better inform in what cases ASGD is best applied. On the MNIST digit recognition task with the LeNet5 model we find that delayed gradients significantly reduce test accuracy with large batch sizes and large learning rates. This limits the applicability of asynchronous gradient methods (without delay compensation) in cases where the learning rate is scaled linearly with the batch size, or with adaptive methods that may select large learning rates at times. Finally, we provide discussion on possible delay compensation methods.", "paperhash": "srinivasan|an_analysis_of_the_delayed_gradients_problem_in_asynchronous_sgd", "keywords": ["asynchronous", "SGD", "gradient descent", "parallel training", "workers", "reduction", "delayed gradient", "convolutional neural network"], "_bibtex": "@misc{\n  srinivasan2018an,\n  title={An Analysis of the Delayed Gradients Problem in Asynchronous SGD},\n  author={Anand Srinivasan and Ajay Jain and Parnian Barekatain},\n  year={2018},\n  url={https://openreview.net/forum?id=BJLSGcywG}\n}", "authorids": ["anandtech1532@gmail.com", "ajayjain@mit.edu", "parni@ideapad.io"], "authors": ["Anand Srinivasan", "Ajay Jain", "Parnian Barekatain"], "TL;DR": "A causal analysis of test accuracy reduction when using asynchronous SGD, and a recommendation of hyperparameter bounds in which to use delayed gradients.", "pdf": "/pdf/16a1a9da05f5ae5e24ace9d3e2b2fa37c61ec260.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582848186, "id": "ICLR.cc/2018/Workshop/-/Paper359/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper359/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper359/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper359/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper359/AnonReviewer1"], "reply": {"forum": "BJLSGcywG", "replyto": "BJLSGcywG", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper359/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper359/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582848186}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521573599397, "tcdate": 1521573599397, "number": 241, "cdate": 1521573599063, "id": "HJP1Jk15f", "invitation": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "forum": "BJLSGcywG", "replyto": "BJLSGcywG", "signatures": ["ICLR.cc/2018/Workshop/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Program_Chairs"], "content": {"decision": "Reject", "title": "ICLR 2018 Workshop Acceptance Decision", "comment": "Based on the reviews, this paper has not been accepted for presentation at the ICLR workshop. However, the conversation and updates can continue to appear here on OpenReview."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "An Analysis of the Delayed Gradients Problem in Asynchronous SGD", "abstract": "Stochastic Gradient Descent can be effectively paralellized to many workers via the use of minibatches. Yet, parameter synchronization requires that each batch waits for the slowest worker to finish. There is a fully asynchronous method, detailed by Dean et. al (2012), called Downpour SGD, or ASGD, which minimizes worker idle time by allowing gradients which are computed on stale parameters to be sent to the parameter server. In practice, direct usage of ASGD is not recommended due to the added noise from stale gradients (referred as the \"delayed gradient problem\"), and therefore some form of delay compensation is required, as is detailed in Zheng et al (2017). In this paper, we present a detailed analysis of the failure modes of asynchronous SGD due to delayed gradients under various hyperparameter selections in order to better inform in what cases ASGD is best applied. On the MNIST digit recognition task with the LeNet5 model we find that delayed gradients significantly reduce test accuracy with large batch sizes and large learning rates. This limits the applicability of asynchronous gradient methods (without delay compensation) in cases where the learning rate is scaled linearly with the batch size, or with adaptive methods that may select large learning rates at times. Finally, we provide discussion on possible delay compensation methods.", "paperhash": "srinivasan|an_analysis_of_the_delayed_gradients_problem_in_asynchronous_sgd", "keywords": ["asynchronous", "SGD", "gradient descent", "parallel training", "workers", "reduction", "delayed gradient", "convolutional neural network"], "_bibtex": "@misc{\n  srinivasan2018an,\n  title={An Analysis of the Delayed Gradients Problem in Asynchronous SGD},\n  author={Anand Srinivasan and Ajay Jain and Parnian Barekatain},\n  year={2018},\n  url={https://openreview.net/forum?id=BJLSGcywG}\n}", "authorids": ["anandtech1532@gmail.com", "ajayjain@mit.edu", "parni@ideapad.io"], "authors": ["Anand Srinivasan", "Ajay Jain", "Parnian Barekatain"], "TL;DR": "A causal analysis of test accuracy reduction when using asynchronous SGD, and a recommendation of hyperparameter bounds in which to use delayed gradients.", "pdf": "/pdf/16a1a9da05f5ae5e24ace9d3e2b2fa37c61ec260.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518629844880, "id": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Program_Chairs"], "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Workshop/-/Submission", "writers": {"values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Workshop/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Workshop Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept", "Reject"]}}}, "nonreaders": [], "noninvitees": [], "cdate": 1518629844880}}}, {"tddate": null, "ddate": null, "tmdate": 1519080985359, "tcdate": 1519080985359, "number": 1, "cdate": 1519080985359, "id": "BkbmLAdvz", "invitation": "ICLR.cc/2018/Workshop/-/Paper359/Public_Comment", "forum": "BJLSGcywG", "replyto": "BJLSGcywG", "signatures": ["~Anand_Srinivasan1"], "readers": ["everyone"], "writers": ["~Anand_Srinivasan1"], "content": {"title": "Experimental code and data", "comment": "Code used to run experiments and data used to generate the plots in this paper can be accessed at https://github.com/ajayjain/outofsync-optim . There are instructions for getting started as well. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "An Analysis of the Delayed Gradients Problem in Asynchronous SGD", "abstract": "Stochastic Gradient Descent can be effectively paralellized to many workers via the use of minibatches. Yet, parameter synchronization requires that each batch waits for the slowest worker to finish. There is a fully asynchronous method, detailed by Dean et. al (2012), called Downpour SGD, or ASGD, which minimizes worker idle time by allowing gradients which are computed on stale parameters to be sent to the parameter server. In practice, direct usage of ASGD is not recommended due to the added noise from stale gradients (referred as the \"delayed gradient problem\"), and therefore some form of delay compensation is required, as is detailed in Zheng et al (2017). In this paper, we present a detailed analysis of the failure modes of asynchronous SGD due to delayed gradients under various hyperparameter selections in order to better inform in what cases ASGD is best applied. On the MNIST digit recognition task with the LeNet5 model we find that delayed gradients significantly reduce test accuracy with large batch sizes and large learning rates. This limits the applicability of asynchronous gradient methods (without delay compensation) in cases where the learning rate is scaled linearly with the batch size, or with adaptive methods that may select large learning rates at times. Finally, we provide discussion on possible delay compensation methods.", "paperhash": "srinivasan|an_analysis_of_the_delayed_gradients_problem_in_asynchronous_sgd", "keywords": ["asynchronous", "SGD", "gradient descent", "parallel training", "workers", "reduction", "delayed gradient", "convolutional neural network"], "_bibtex": "@misc{\n  srinivasan2018an,\n  title={An Analysis of the Delayed Gradients Problem in Asynchronous SGD},\n  author={Anand Srinivasan and Ajay Jain and Parnian Barekatain},\n  year={2018},\n  url={https://openreview.net/forum?id=BJLSGcywG}\n}", "authorids": ["anandtech1532@gmail.com", "ajayjain@mit.edu", "parni@ideapad.io"], "authors": ["Anand Srinivasan", "Ajay Jain", "Parnian Barekatain"], "TL;DR": "A causal analysis of test accuracy reduction when using asynchronous SGD, and a recommendation of hyperparameter bounds in which to use delayed gradients.", "pdf": "/pdf/16a1a9da05f5ae5e24ace9d3e2b2fa37c61ec260.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518712622406, "id": "ICLR.cc/2018/Workshop/-/Paper359/Public_Comment", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper359/Reviewers"], "reply": {"replyto": null, "forum": "BJLSGcywG", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1518712622406}}}], "count": 6}