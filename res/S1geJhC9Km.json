{"notes": [{"id": "S1geJhC9Km", "original": "Syxe6C6ctm", "number": 953, "cdate": 1538087895772, "ddate": null, "tcdate": 1538087895772, "tmdate": 1545355391742, "tddate": null, "forum": "S1geJhC9Km", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Feature quantization for parsimonious and interpretable predictive models", "abstract": "For regulatory and interpretability reasons, the logistic regression is still widely used by financial institutions to learn the refunding probability of a loan from applicant's historical data. To improve prediction accuracy and interpretability, a preprocessing step quantizing both continuous and categorical data is usually performed: continuous features are discretized by assigning factor levels to intervals and, if numerous, levels of categorical features are grouped. However, a better predictive accuracy can be reached by embedding this quantization estimation step directly into the predictive estimation step itself. By doing so, the predictive loss has to be optimized on a huge and untractable discontinuous quantization set. To overcome this difficulty, we introduce a specific two-step optimization strategy: first, the optimization problem is relaxed by approximating discontinuous quantization functions by smooth functions; second, the resulting relaxed optimization problem is solved via a particular neural network and stochastic gradient descent. The strategy gives then access to good candidates for the original optimization problem after a straightforward maximum a posteriori procedure to obtain cutpoints. The good performances of this approach, which we call glmdisc, are illustrated on simulated and real data from the UCI library and Cr\u00e9dit Agricole Consumer Finance (a major European historic player in the consumer credit market). The results show that practitioners finally have an automatic all-in-one tool that answers their recurring needs of quantization for predictive tasks.", "keywords": ["discretization", "grouping", "interpretability", "shallow neural networks"], "authorids": ["adrien.ehrhardt@inria.fr", "vincent.vandewalle@inria.fr", "christophe.biernacki@inria.fr", "philippe.heinrich@univ-lille.fr"], "authors": ["Adrien EHRHARDT", "Vincent VANDEWALLE", "Christophe BIERNACKI", "Philippe HEINRICH"], "TL;DR": "We tackle discretization of continuous features and grouping of factor levels as a representation learning problem and provide a rigorous way of estimating the best quantization to yield good performance and interpretability.", "pdf": "/pdf/46adc31ebde8befed7e4a2ed8920146fbc104ffb.pdf", "paperhash": "ehrhardt|feature_quantization_for_parsimonious_and_interpretable_predictive_models", "_bibtex": "@misc{\nehrhardt2019feature,\ntitle={Feature quantization for parsimonious and interpretable predictive models},\nauthor={Adrien EHRHARDT and Vincent VANDEWALLE and Christophe BIERNACKI and Philippe HEINRICH},\nyear={2019},\nurl={https://openreview.net/forum?id=S1geJhC9Km},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "rkgrGZnggN", "original": null, "number": 1, "cdate": 1544761612975, "ddate": null, "tcdate": 1544761612975, "tmdate": 1545354519261, "tddate": null, "forum": "S1geJhC9Km", "replyto": "S1geJhC9Km", "invitation": "ICLR.cc/2019/Conference/-/Paper953/Meta_Review", "content": {"metareview": "All reviewers agree to reject. While there were many positive points to this work, reviewers believed that it was not yet ready for acceptance.", "confidence": "5: The area chair is absolutely certain", "recommendation": "Reject", "title": "Meta-Review for interpretable predictive models paper"}, "signatures": ["ICLR.cc/2019/Conference/Paper953/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper953/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Feature quantization for parsimonious and interpretable predictive models", "abstract": "For regulatory and interpretability reasons, the logistic regression is still widely used by financial institutions to learn the refunding probability of a loan from applicant's historical data. To improve prediction accuracy and interpretability, a preprocessing step quantizing both continuous and categorical data is usually performed: continuous features are discretized by assigning factor levels to intervals and, if numerous, levels of categorical features are grouped. However, a better predictive accuracy can be reached by embedding this quantization estimation step directly into the predictive estimation step itself. By doing so, the predictive loss has to be optimized on a huge and untractable discontinuous quantization set. To overcome this difficulty, we introduce a specific two-step optimization strategy: first, the optimization problem is relaxed by approximating discontinuous quantization functions by smooth functions; second, the resulting relaxed optimization problem is solved via a particular neural network and stochastic gradient descent. The strategy gives then access to good candidates for the original optimization problem after a straightforward maximum a posteriori procedure to obtain cutpoints. The good performances of this approach, which we call glmdisc, are illustrated on simulated and real data from the UCI library and Cr\u00e9dit Agricole Consumer Finance (a major European historic player in the consumer credit market). The results show that practitioners finally have an automatic all-in-one tool that answers their recurring needs of quantization for predictive tasks.", "keywords": ["discretization", "grouping", "interpretability", "shallow neural networks"], "authorids": ["adrien.ehrhardt@inria.fr", "vincent.vandewalle@inria.fr", "christophe.biernacki@inria.fr", "philippe.heinrich@univ-lille.fr"], "authors": ["Adrien EHRHARDT", "Vincent VANDEWALLE", "Christophe BIERNACKI", "Philippe HEINRICH"], "TL;DR": "We tackle discretization of continuous features and grouping of factor levels as a representation learning problem and provide a rigorous way of estimating the best quantization to yield good performance and interpretability.", "pdf": "/pdf/46adc31ebde8befed7e4a2ed8920146fbc104ffb.pdf", "paperhash": "ehrhardt|feature_quantization_for_parsimonious_and_interpretable_predictive_models", "_bibtex": "@misc{\nehrhardt2019feature,\ntitle={Feature quantization for parsimonious and interpretable predictive models},\nauthor={Adrien EHRHARDT and Vincent VANDEWALLE and Christophe BIERNACKI and Philippe HEINRICH},\nyear={2019},\nurl={https://openreview.net/forum?id=S1geJhC9Km},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper953/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353021996, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1geJhC9Km", "replyto": "S1geJhC9Km", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper953/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper953/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper953/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353021996}}}, {"id": "B1ehjp9SC7", "original": null, "number": 4, "cdate": 1542987172229, "ddate": null, "tcdate": 1542987172229, "tmdate": 1542987172229, "tddate": null, "forum": "S1geJhC9Km", "replyto": "Bkl6ds5G2m", "invitation": "ICLR.cc/2019/Conference/-/Paper953/Official_Comment", "content": {"title": "The paper had typos and was updated accordingly. Comparison to the state of the art should appear more clearly.", "comment": "Thank you for your review. We are aware that the first submitted version has typos and mistakes and have updated the paper accordingly. We apologize for the inconvenience and hope you might see the revised paper with new eyes. Moreover, we have addressed your following remarks:\n\n- The state of the art consists in three baselines in Table 2 and 3:\n* A standard logistic regression (first column)\n* The current performance (\u2018manual\u2019 in-house approach - second column)\n* Ad hoc methods from the literature (MDLP (Fayyad & Irani (1993)) and Chi2 independence tests to group factor levels - third column)\n\nWe developed the literature review in Section 2.1 and in the experiments on real data in Section 4.2 to clarify this point.\nAs shown in Table 2 and 3 it outperforms the first two baselines on all datasets, performs better than ad hoc methods on UCI, and on par / sometimes worse on Credit Scoring data.\n\nAlthough we do not provide theoretical guarantees, experiments on simulated data (Table 1) seem to show consistency of the proposed approach (see the figures in the Appendix)."}, "signatures": ["ICLR.cc/2019/Conference/Paper953/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper953/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper953/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Feature quantization for parsimonious and interpretable predictive models", "abstract": "For regulatory and interpretability reasons, the logistic regression is still widely used by financial institutions to learn the refunding probability of a loan from applicant's historical data. To improve prediction accuracy and interpretability, a preprocessing step quantizing both continuous and categorical data is usually performed: continuous features are discretized by assigning factor levels to intervals and, if numerous, levels of categorical features are grouped. However, a better predictive accuracy can be reached by embedding this quantization estimation step directly into the predictive estimation step itself. By doing so, the predictive loss has to be optimized on a huge and untractable discontinuous quantization set. To overcome this difficulty, we introduce a specific two-step optimization strategy: first, the optimization problem is relaxed by approximating discontinuous quantization functions by smooth functions; second, the resulting relaxed optimization problem is solved via a particular neural network and stochastic gradient descent. The strategy gives then access to good candidates for the original optimization problem after a straightforward maximum a posteriori procedure to obtain cutpoints. The good performances of this approach, which we call glmdisc, are illustrated on simulated and real data from the UCI library and Cr\u00e9dit Agricole Consumer Finance (a major European historic player in the consumer credit market). The results show that practitioners finally have an automatic all-in-one tool that answers their recurring needs of quantization for predictive tasks.", "keywords": ["discretization", "grouping", "interpretability", "shallow neural networks"], "authorids": ["adrien.ehrhardt@inria.fr", "vincent.vandewalle@inria.fr", "christophe.biernacki@inria.fr", "philippe.heinrich@univ-lille.fr"], "authors": ["Adrien EHRHARDT", "Vincent VANDEWALLE", "Christophe BIERNACKI", "Philippe HEINRICH"], "TL;DR": "We tackle discretization of continuous features and grouping of factor levels as a representation learning problem and provide a rigorous way of estimating the best quantization to yield good performance and interpretability.", "pdf": "/pdf/46adc31ebde8befed7e4a2ed8920146fbc104ffb.pdf", "paperhash": "ehrhardt|feature_quantization_for_parsimonious_and_interpretable_predictive_models", "_bibtex": "@misc{\nehrhardt2019feature,\ntitle={Feature quantization for parsimonious and interpretable predictive models},\nauthor={Adrien EHRHARDT and Vincent VANDEWALLE and Christophe BIERNACKI and Philippe HEINRICH},\nyear={2019},\nurl={https://openreview.net/forum?id=S1geJhC9Km},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper953/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621614873, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1geJhC9Km", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper953/Authors", "ICLR.cc/2019/Conference/Paper953/Reviewers", "ICLR.cc/2019/Conference/Paper953/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper953/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper953/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper953/Authors|ICLR.cc/2019/Conference/Paper953/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper953/Reviewers", "ICLR.cc/2019/Conference/Paper953/Authors", "ICLR.cc/2019/Conference/Paper953/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621614873}}}, {"id": "rJe3snqBRQ", "original": null, "number": 3, "cdate": 1542986916465, "ddate": null, "tcdate": 1542986916465, "tmdate": 1542986916465, "tddate": null, "forum": "S1geJhC9Km", "replyto": "S1eWiCYE3m", "invitation": "ICLR.cc/2019/Conference/-/Paper953/Official_Comment", "content": {"title": "The paper was admittedly unclear and was updated accordingly. Experiments have been enhanced.", "comment": "Thank you for your review. We are aware that the first submitted version has typos and mistakes and have updated the paper accordingly. We apologize for the inconvenience and hope you might see the revised paper with new eyes. Moreover, we have addressed your following remarks:\n\n- The motivation behind the relaxation is that it can approximate the discrete quantization functions as now shown on Figure 2. The parameters of this relaxation are easily obtained via a very simple neural network, contrary to a greedy intractable search of the best quantization functions as in Equation (6). In a way, we use the proposed neural network as a computational graph to get the best quantization through the optimization of the \\alpha parameters and the use of fast and standard libraries for deep learning.\n\n- Regarding the experiments, the simulated data are here only to to show empirically the consistency of the proposed method in various settings. As for real data, we understand the number of datasets on which we compare our proposed method to standard methods was not enough, so that we added 3 portfolios from Cr\u00e9dit Agricole Consumer Finance as well as 6 datasets from the UCI library, some of which also in the field of Credit Scoring.\n\nIn your question, we suppose that by \u201cfeatures\u201d, you meant categorical factor levels (say we have a categorical features with 5 levels of which only the 4 \u2018first\u2019 are seen during training). If our interpretation of your question is correct, recall that the neural network is only a proxy: in the end, our method yields a logistic regression, so that we need to estimate a coefficient for each factor level (as in Equation (4)) and p(y|x=5) cannot be calculated. This is the case for any \u201cstandard\u201d supervised classification method to our knowledge. Missing values in categorical or continuous features can be addressed if there was missing values in these features in the training phase, thus forming a level labelled as \u201cmissing\u201d that can be grouped with other levels / discretization intervals.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper953/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper953/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper953/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Feature quantization for parsimonious and interpretable predictive models", "abstract": "For regulatory and interpretability reasons, the logistic regression is still widely used by financial institutions to learn the refunding probability of a loan from applicant's historical data. To improve prediction accuracy and interpretability, a preprocessing step quantizing both continuous and categorical data is usually performed: continuous features are discretized by assigning factor levels to intervals and, if numerous, levels of categorical features are grouped. However, a better predictive accuracy can be reached by embedding this quantization estimation step directly into the predictive estimation step itself. By doing so, the predictive loss has to be optimized on a huge and untractable discontinuous quantization set. To overcome this difficulty, we introduce a specific two-step optimization strategy: first, the optimization problem is relaxed by approximating discontinuous quantization functions by smooth functions; second, the resulting relaxed optimization problem is solved via a particular neural network and stochastic gradient descent. The strategy gives then access to good candidates for the original optimization problem after a straightforward maximum a posteriori procedure to obtain cutpoints. The good performances of this approach, which we call glmdisc, are illustrated on simulated and real data from the UCI library and Cr\u00e9dit Agricole Consumer Finance (a major European historic player in the consumer credit market). The results show that practitioners finally have an automatic all-in-one tool that answers their recurring needs of quantization for predictive tasks.", "keywords": ["discretization", "grouping", "interpretability", "shallow neural networks"], "authorids": ["adrien.ehrhardt@inria.fr", "vincent.vandewalle@inria.fr", "christophe.biernacki@inria.fr", "philippe.heinrich@univ-lille.fr"], "authors": ["Adrien EHRHARDT", "Vincent VANDEWALLE", "Christophe BIERNACKI", "Philippe HEINRICH"], "TL;DR": "We tackle discretization of continuous features and grouping of factor levels as a representation learning problem and provide a rigorous way of estimating the best quantization to yield good performance and interpretability.", "pdf": "/pdf/46adc31ebde8befed7e4a2ed8920146fbc104ffb.pdf", "paperhash": "ehrhardt|feature_quantization_for_parsimonious_and_interpretable_predictive_models", "_bibtex": "@misc{\nehrhardt2019feature,\ntitle={Feature quantization for parsimonious and interpretable predictive models},\nauthor={Adrien EHRHARDT and Vincent VANDEWALLE and Christophe BIERNACKI and Philippe HEINRICH},\nyear={2019},\nurl={https://openreview.net/forum?id=S1geJhC9Km},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper953/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621614873, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1geJhC9Km", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper953/Authors", "ICLR.cc/2019/Conference/Paper953/Reviewers", "ICLR.cc/2019/Conference/Paper953/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper953/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper953/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper953/Authors|ICLR.cc/2019/Conference/Paper953/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper953/Reviewers", "ICLR.cc/2019/Conference/Paper953/Authors", "ICLR.cc/2019/Conference/Paper953/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621614873}}}, {"id": "Sygx9ccS0Q", "original": null, "number": 1, "cdate": 1542986375699, "ddate": null, "tcdate": 1542986375699, "tmdate": 1542986783644, "tddate": null, "forum": "S1geJhC9Km", "replyto": "S1geJhC9Km", "invitation": "ICLR.cc/2019/Conference/-/Paper953/Official_Comment", "content": {"title": "Revised version - short \"cover letter\"", "comment": "A revised version of the paper has been uploaded which, independently from the reviews, aims at:\n\n- Correcting the typos present in the first version.\n\n- Changing the notation of the quantization functions from f_j to q_j (which stands for quantization).\n\n- Changing the result of the quantization, which was previously a single attribute with integer levels, for a dummy-encoding scheme (see Equation 1).\n\n- Re-running the experiments on the simulated data, following the fact that we found RMSprop to work better than standard SGD.\n\nFollowing the reviews, we also incorporated the following modifications:\n\n- Adding several real data experiments by the addition of 3 portfolios of Cr\u00e9dit Agricole Consumer Finance (Table 3) and 6 datasets from the UCI library (Table 2).\n\n- Enriching the literature review by adding some detail about the already cited taxonomy (Ramirez-Gallego et al. (2016)).\n\n- Providing some detail on the ad hoc methods used in Table 2 (MDLP (Fayyad & Irani (1993)) and Chi2 independence tests to group factor levels).\n\n- Clarifying Figure 1 (now Figure 2) to show that the smooth approximation can indeed approximate a \u201chard\u201d quantization.\n\n- Adding a Jupyter Notebook in the Appendix to witness the empirical consistence of the proposed approach.\n\nThe first version of the paper was admittedly hard to read due to typos, diverse mistakes and some missing references. We misunderstood the submission deadline hour. Nevertheless, we hope to be reread with new eyes as we think our proposal can be impactful for practitioners relying on discretization and grouping of factor levels, as we witness in the Credit Scoring industry where it is already successfully used by Cr\u00e9dit Agricole Consumer Finance."}, "signatures": ["ICLR.cc/2019/Conference/Paper953/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper953/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper953/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Feature quantization for parsimonious and interpretable predictive models", "abstract": "For regulatory and interpretability reasons, the logistic regression is still widely used by financial institutions to learn the refunding probability of a loan from applicant's historical data. To improve prediction accuracy and interpretability, a preprocessing step quantizing both continuous and categorical data is usually performed: continuous features are discretized by assigning factor levels to intervals and, if numerous, levels of categorical features are grouped. However, a better predictive accuracy can be reached by embedding this quantization estimation step directly into the predictive estimation step itself. By doing so, the predictive loss has to be optimized on a huge and untractable discontinuous quantization set. To overcome this difficulty, we introduce a specific two-step optimization strategy: first, the optimization problem is relaxed by approximating discontinuous quantization functions by smooth functions; second, the resulting relaxed optimization problem is solved via a particular neural network and stochastic gradient descent. The strategy gives then access to good candidates for the original optimization problem after a straightforward maximum a posteriori procedure to obtain cutpoints. The good performances of this approach, which we call glmdisc, are illustrated on simulated and real data from the UCI library and Cr\u00e9dit Agricole Consumer Finance (a major European historic player in the consumer credit market). The results show that practitioners finally have an automatic all-in-one tool that answers their recurring needs of quantization for predictive tasks.", "keywords": ["discretization", "grouping", "interpretability", "shallow neural networks"], "authorids": ["adrien.ehrhardt@inria.fr", "vincent.vandewalle@inria.fr", "christophe.biernacki@inria.fr", "philippe.heinrich@univ-lille.fr"], "authors": ["Adrien EHRHARDT", "Vincent VANDEWALLE", "Christophe BIERNACKI", "Philippe HEINRICH"], "TL;DR": "We tackle discretization of continuous features and grouping of factor levels as a representation learning problem and provide a rigorous way of estimating the best quantization to yield good performance and interpretability.", "pdf": "/pdf/46adc31ebde8befed7e4a2ed8920146fbc104ffb.pdf", "paperhash": "ehrhardt|feature_quantization_for_parsimonious_and_interpretable_predictive_models", "_bibtex": "@misc{\nehrhardt2019feature,\ntitle={Feature quantization for parsimonious and interpretable predictive models},\nauthor={Adrien EHRHARDT and Vincent VANDEWALLE and Christophe BIERNACKI and Philippe HEINRICH},\nyear={2019},\nurl={https://openreview.net/forum?id=S1geJhC9Km},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper953/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621614873, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1geJhC9Km", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper953/Authors", "ICLR.cc/2019/Conference/Paper953/Reviewers", "ICLR.cc/2019/Conference/Paper953/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper953/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper953/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper953/Authors|ICLR.cc/2019/Conference/Paper953/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper953/Reviewers", "ICLR.cc/2019/Conference/Paper953/Authors", "ICLR.cc/2019/Conference/Paper953/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621614873}}}, {"id": "Byl9x2qB0Q", "original": null, "number": 2, "cdate": 1542986737670, "ddate": null, "tcdate": 1542986737670, "tmdate": 1542986737670, "tddate": null, "forum": "S1geJhC9Km", "replyto": "HyxFMmvtn7", "invitation": "ICLR.cc/2019/Conference/-/Paper953/Official_Comment", "content": {"title": "It does - softmax functions can approximate gate functions and provide an automatic way of selecting the number of cutpoints", "comment": "Thank you for your review. We are aware that the first submitted version has typos and mistakes and have updated the paper accordingly. We apologize for the inconvenience and hope you might see the revised paper with new eyes. Moreover, we have addressed your following remarks:\n\n- The proposed relaxation is proportional to exp(a+bx) through, as was/is stated in Section 3.2, a softmax layer. As a consequence, the normalization also depends on x. For example, a discretization in 3 levels would be relaxed as (exp(a_1 + b_1 x) / sum_{h=1}^3 exp(a_h + b_h x), exp(a_2 + b_2 x) / sum_{h=1}^3 exp(a_h + b_h x), exp(a_3 + b_3 x) / sum_{h=1}^3 exp(a_h + b_h x)) that sum to 1 for each observation x. We updated the paper (Section 3.1) so that this normalization should now clearly appear.\n\n- Figure 1 and Figure 2 have been swapped for clarity. The Figure that you refer to as misleading, now Figure 2, has been replaced by the actual best discretization chosen by the proposed approach in experiment (a), where we see that the proposed relaxation is able to approximate the true data generating mechanism (i.e. the true quantization gate functions). In the appendix, we illustrate how, after some epochs, the proposed relaxation is able to converge to the gate functions and, in experiment (b), how it is able to \u201cshut off\u201d 2 neurons among 5 over the training data, thus approximating again the true data generating mechanism although we provided it with much more capacity than needed.\n\n- For grouping levels of categorical features, as you have rightfully stated, the exponential has no effect. It is here only to emphasize the straightforward equivalence of this parametrization and a softmax layer. As a consequence of this normalization, q_alpha can be interpreted as a probability of each of the (possibly numerous) factor levels of belonging to each group.\n\n- Concerning ordinal features, it is not in this paper\u2019s scope. Moreover, in the logistic regression setting, it is actually much easier (because much less combinatorial) to solve by e.g. a Fused-Lasso penalization between all pairs of consecutive factor levels.\n\n- Concerning the selection of the number of cutpoints, as was stated in Section 3.2 (now Section 3.1) and in particular in Equation (10), it is done automatically by starting with a high number m_j of hidden neurons per feature which correspond to the maximum number of cutpoints plus one. Being able, by learning appropriate weights alpha, to \u201cshut off\u201d some neurons, the proposed neural network can explore quantizations \\hat{q}_j from 1 to m bins, optimizing indirectly in each case the location of the cutpoints. This phenomenon is/was illustrated on Table 2, experiment (b), and is now as well illustrated in the appendix, as stated above.\n\n- Concerning the literature review, we added some comments about the already cited paper (Ramirez-Gallego et al. (2016)), in Section 2.2 (now Section 2.1). Moreover, we developed the references to two procedures in Section 4, namely MDLP and Chi2 tests of independence for grouping factor levels.\n\n- Concerning \u201coptimization\u201d, we guess you might refer to the gradient descent algorithm(s) and its / their hyper-parameters. In the first version of the paper, we used standard SGD with standard hyperparameters from Keras (lr=0.01, momentum=0.0, decay=0.0, nesterov=False). In this second version, we rely on RMSProp with a higher than default learning rate (lr=0.5, rho=0.9, epsilon=None, decay=0.0), as can be seen in the Appendix. Way better results could probably be obtained by changing the optimizer and / or its hyperparameters.\n\n- Concerning experiments, simulated data are here only to show empirically the consistency of the proposed method in various settings. As for real data, we understand the number of datasets on which we compare our proposed method to standard methods was not enough, so that we added 3 portfolios from Cr\u00e9dit Agricole Consumer Finance as well as 6 datasets from the UCI library, some of which also in the field of Credit Scoring."}, "signatures": ["ICLR.cc/2019/Conference/Paper953/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper953/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper953/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Feature quantization for parsimonious and interpretable predictive models", "abstract": "For regulatory and interpretability reasons, the logistic regression is still widely used by financial institutions to learn the refunding probability of a loan from applicant's historical data. To improve prediction accuracy and interpretability, a preprocessing step quantizing both continuous and categorical data is usually performed: continuous features are discretized by assigning factor levels to intervals and, if numerous, levels of categorical features are grouped. However, a better predictive accuracy can be reached by embedding this quantization estimation step directly into the predictive estimation step itself. By doing so, the predictive loss has to be optimized on a huge and untractable discontinuous quantization set. To overcome this difficulty, we introduce a specific two-step optimization strategy: first, the optimization problem is relaxed by approximating discontinuous quantization functions by smooth functions; second, the resulting relaxed optimization problem is solved via a particular neural network and stochastic gradient descent. The strategy gives then access to good candidates for the original optimization problem after a straightforward maximum a posteriori procedure to obtain cutpoints. The good performances of this approach, which we call glmdisc, are illustrated on simulated and real data from the UCI library and Cr\u00e9dit Agricole Consumer Finance (a major European historic player in the consumer credit market). The results show that practitioners finally have an automatic all-in-one tool that answers their recurring needs of quantization for predictive tasks.", "keywords": ["discretization", "grouping", "interpretability", "shallow neural networks"], "authorids": ["adrien.ehrhardt@inria.fr", "vincent.vandewalle@inria.fr", "christophe.biernacki@inria.fr", "philippe.heinrich@univ-lille.fr"], "authors": ["Adrien EHRHARDT", "Vincent VANDEWALLE", "Christophe BIERNACKI", "Philippe HEINRICH"], "TL;DR": "We tackle discretization of continuous features and grouping of factor levels as a representation learning problem and provide a rigorous way of estimating the best quantization to yield good performance and interpretability.", "pdf": "/pdf/46adc31ebde8befed7e4a2ed8920146fbc104ffb.pdf", "paperhash": "ehrhardt|feature_quantization_for_parsimonious_and_interpretable_predictive_models", "_bibtex": "@misc{\nehrhardt2019feature,\ntitle={Feature quantization for parsimonious and interpretable predictive models},\nauthor={Adrien EHRHARDT and Vincent VANDEWALLE and Christophe BIERNACKI and Philippe HEINRICH},\nyear={2019},\nurl={https://openreview.net/forum?id=S1geJhC9Km},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper953/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621614873, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1geJhC9Km", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper953/Authors", "ICLR.cc/2019/Conference/Paper953/Reviewers", "ICLR.cc/2019/Conference/Paper953/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper953/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper953/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper953/Authors|ICLR.cc/2019/Conference/Paper953/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper953/Reviewers", "ICLR.cc/2019/Conference/Paper953/Authors", "ICLR.cc/2019/Conference/Paper953/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621614873}}}, {"id": "HyxFMmvtn7", "original": null, "number": 3, "cdate": 1541137168544, "ddate": null, "tcdate": 1541137168544, "tmdate": 1541533550316, "tddate": null, "forum": "S1geJhC9Km", "replyto": "S1geJhC9Km", "invitation": "ICLR.cc/2019/Conference/-/Paper953/Official_Review", "content": {"title": "The paper does not solve the question it proposed", "review": "The paper describes a question about discretize continuous features or group discrete features in the preprocessing step, which they call feature quantification. It considers that a joint training of feature quantification and a discriminative model can lead to a better performance than treating feature quantification as a preprocessing step. \n\nThis paper has many typos, grammar mistakes and question marks, which make it hard to follow. The question proposed is simple and easy to understand. However, I don't convinced by the solution in this paper. Since it is a hard optimization question, the authors proposed a relaxation approach in section 3.1. I do not think that exp(a+bx) is able to approach step functions since exp(a+bx) is monotone. I think Figure 1 is misleading. For grouping discrete features, the author propose to use exp(\\alpha_{x_j, j}^h) and hoping that some \\alpha parameters can be optimized to be equal, which is too simple. The exponential transformation here does not have an effect. It is more interesting to consider how to add some constraints. For example, if the discrete feature is ordinal, how one can assure that the grouped discrete feature is still ordinal. The relaxation in this paper is too much without handling any interesting constraints and the proposed exp(a+bx) can not approach step functions. The authors do not provide a good way to select number of cut points, which I think is a hard but interesting question.\n\nThe work also lacks value in literature review, optimization and experiments.", "rating": "2: Strong rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper953/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Feature quantization for parsimonious and interpretable predictive models", "abstract": "For regulatory and interpretability reasons, the logistic regression is still widely used by financial institutions to learn the refunding probability of a loan from applicant's historical data. To improve prediction accuracy and interpretability, a preprocessing step quantizing both continuous and categorical data is usually performed: continuous features are discretized by assigning factor levels to intervals and, if numerous, levels of categorical features are grouped. However, a better predictive accuracy can be reached by embedding this quantization estimation step directly into the predictive estimation step itself. By doing so, the predictive loss has to be optimized on a huge and untractable discontinuous quantization set. To overcome this difficulty, we introduce a specific two-step optimization strategy: first, the optimization problem is relaxed by approximating discontinuous quantization functions by smooth functions; second, the resulting relaxed optimization problem is solved via a particular neural network and stochastic gradient descent. The strategy gives then access to good candidates for the original optimization problem after a straightforward maximum a posteriori procedure to obtain cutpoints. The good performances of this approach, which we call glmdisc, are illustrated on simulated and real data from the UCI library and Cr\u00e9dit Agricole Consumer Finance (a major European historic player in the consumer credit market). The results show that practitioners finally have an automatic all-in-one tool that answers their recurring needs of quantization for predictive tasks.", "keywords": ["discretization", "grouping", "interpretability", "shallow neural networks"], "authorids": ["adrien.ehrhardt@inria.fr", "vincent.vandewalle@inria.fr", "christophe.biernacki@inria.fr", "philippe.heinrich@univ-lille.fr"], "authors": ["Adrien EHRHARDT", "Vincent VANDEWALLE", "Christophe BIERNACKI", "Philippe HEINRICH"], "TL;DR": "We tackle discretization of continuous features and grouping of factor levels as a representation learning problem and provide a rigorous way of estimating the best quantization to yield good performance and interpretability.", "pdf": "/pdf/46adc31ebde8befed7e4a2ed8920146fbc104ffb.pdf", "paperhash": "ehrhardt|feature_quantization_for_parsimonious_and_interpretable_predictive_models", "_bibtex": "@misc{\nehrhardt2019feature,\ntitle={Feature quantization for parsimonious and interpretable predictive models},\nauthor={Adrien EHRHARDT and Vincent VANDEWALLE and Christophe BIERNACKI and Philippe HEINRICH},\nyear={2019},\nurl={https://openreview.net/forum?id=S1geJhC9Km},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper953/Official_Review", "cdate": 1542234339420, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "S1geJhC9Km", "replyto": "S1geJhC9Km", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper953/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335840324, "tmdate": 1552335840324, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper953/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "S1eWiCYE3m", "original": null, "number": 2, "cdate": 1540820633323, "ddate": null, "tcdate": 1540820633323, "tmdate": 1541533550106, "tddate": null, "forum": "S1geJhC9Km", "replyto": "S1geJhC9Km", "invitation": "ICLR.cc/2019/Conference/-/Paper953/Official_Review", "content": {"title": "The presentation is unclear ", "review": "This paper presents a feature quantization technique for logistic regression, which has already been a common practice in \n many finance applications.  The text feels rushed. From the current presentation, I find it difficult to understand what is the motivation of adopting the proposed relaxation of the optimization method, and how is the neural network-based estimation strategy connected to the logistic regression model. It seems the difference lies in the parameterized nonlinear transformation such that the cutting points can be somehow optimized.  The quality of the experiments performed is way below the expectation for ICLR. Although numerical experiments are performed on both simulated data and credit scoring data, it is still unclear whether the proposed method has superiority over competitors.  \n\nQuestion: In the test phase, how would the proposed method handle features that are not seen in the training phase? ", "rating": "3: Clear rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper953/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Feature quantization for parsimonious and interpretable predictive models", "abstract": "For regulatory and interpretability reasons, the logistic regression is still widely used by financial institutions to learn the refunding probability of a loan from applicant's historical data. To improve prediction accuracy and interpretability, a preprocessing step quantizing both continuous and categorical data is usually performed: continuous features are discretized by assigning factor levels to intervals and, if numerous, levels of categorical features are grouped. However, a better predictive accuracy can be reached by embedding this quantization estimation step directly into the predictive estimation step itself. By doing so, the predictive loss has to be optimized on a huge and untractable discontinuous quantization set. To overcome this difficulty, we introduce a specific two-step optimization strategy: first, the optimization problem is relaxed by approximating discontinuous quantization functions by smooth functions; second, the resulting relaxed optimization problem is solved via a particular neural network and stochastic gradient descent. The strategy gives then access to good candidates for the original optimization problem after a straightforward maximum a posteriori procedure to obtain cutpoints. The good performances of this approach, which we call glmdisc, are illustrated on simulated and real data from the UCI library and Cr\u00e9dit Agricole Consumer Finance (a major European historic player in the consumer credit market). The results show that practitioners finally have an automatic all-in-one tool that answers their recurring needs of quantization for predictive tasks.", "keywords": ["discretization", "grouping", "interpretability", "shallow neural networks"], "authorids": ["adrien.ehrhardt@inria.fr", "vincent.vandewalle@inria.fr", "christophe.biernacki@inria.fr", "philippe.heinrich@univ-lille.fr"], "authors": ["Adrien EHRHARDT", "Vincent VANDEWALLE", "Christophe BIERNACKI", "Philippe HEINRICH"], "TL;DR": "We tackle discretization of continuous features and grouping of factor levels as a representation learning problem and provide a rigorous way of estimating the best quantization to yield good performance and interpretability.", "pdf": "/pdf/46adc31ebde8befed7e4a2ed8920146fbc104ffb.pdf", "paperhash": "ehrhardt|feature_quantization_for_parsimonious_and_interpretable_predictive_models", "_bibtex": "@misc{\nehrhardt2019feature,\ntitle={Feature quantization for parsimonious and interpretable predictive models},\nauthor={Adrien EHRHARDT and Vincent VANDEWALLE and Christophe BIERNACKI and Philippe HEINRICH},\nyear={2019},\nurl={https://openreview.net/forum?id=S1geJhC9Km},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper953/Official_Review", "cdate": 1542234339420, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "S1geJhC9Km", "replyto": "S1geJhC9Km", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper953/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335840324, "tmdate": 1552335840324, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper953/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "Bkl6ds5G2m", "original": null, "number": 1, "cdate": 1540692852802, "ddate": null, "tcdate": 1540692852802, "tmdate": 1541533549890, "tddate": null, "forum": "S1geJhC9Km", "replyto": "S1geJhC9Km", "invitation": "ICLR.cc/2019/Conference/-/Paper953/Official_Review", "content": {"title": "many typos", "review": "i take reviewing very seriously, and it often takes hours per paper. this paper, however, has many typos, grammatical errors, and seems to have been submitted last minute.  therefore, i have read the paper quickly.\nthat said, i do not understand the results.\nclearly, many discretization methods have previously been described, as alluded to by citing the taxonomy paper on the subject.  the authors state they have developed a better approach.  however, i do not see a comparison to the state of the art in the simulations, and i do not follow the results of Table 2, which columns correspond to which particular algorithms? in either case, the proposed approach does not seem to improve the empirical results, nor have theoretical guarantees, so i am not particularly impressed with the results either.", "rating": "4: Ok but not good enough - rejection", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2019/Conference/Paper953/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Feature quantization for parsimonious and interpretable predictive models", "abstract": "For regulatory and interpretability reasons, the logistic regression is still widely used by financial institutions to learn the refunding probability of a loan from applicant's historical data. To improve prediction accuracy and interpretability, a preprocessing step quantizing both continuous and categorical data is usually performed: continuous features are discretized by assigning factor levels to intervals and, if numerous, levels of categorical features are grouped. However, a better predictive accuracy can be reached by embedding this quantization estimation step directly into the predictive estimation step itself. By doing so, the predictive loss has to be optimized on a huge and untractable discontinuous quantization set. To overcome this difficulty, we introduce a specific two-step optimization strategy: first, the optimization problem is relaxed by approximating discontinuous quantization functions by smooth functions; second, the resulting relaxed optimization problem is solved via a particular neural network and stochastic gradient descent. The strategy gives then access to good candidates for the original optimization problem after a straightforward maximum a posteriori procedure to obtain cutpoints. The good performances of this approach, which we call glmdisc, are illustrated on simulated and real data from the UCI library and Cr\u00e9dit Agricole Consumer Finance (a major European historic player in the consumer credit market). The results show that practitioners finally have an automatic all-in-one tool that answers their recurring needs of quantization for predictive tasks.", "keywords": ["discretization", "grouping", "interpretability", "shallow neural networks"], "authorids": ["adrien.ehrhardt@inria.fr", "vincent.vandewalle@inria.fr", "christophe.biernacki@inria.fr", "philippe.heinrich@univ-lille.fr"], "authors": ["Adrien EHRHARDT", "Vincent VANDEWALLE", "Christophe BIERNACKI", "Philippe HEINRICH"], "TL;DR": "We tackle discretization of continuous features and grouping of factor levels as a representation learning problem and provide a rigorous way of estimating the best quantization to yield good performance and interpretability.", "pdf": "/pdf/46adc31ebde8befed7e4a2ed8920146fbc104ffb.pdf", "paperhash": "ehrhardt|feature_quantization_for_parsimonious_and_interpretable_predictive_models", "_bibtex": "@misc{\nehrhardt2019feature,\ntitle={Feature quantization for parsimonious and interpretable predictive models},\nauthor={Adrien EHRHARDT and Vincent VANDEWALLE and Christophe BIERNACKI and Philippe HEINRICH},\nyear={2019},\nurl={https://openreview.net/forum?id=S1geJhC9Km},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper953/Official_Review", "cdate": 1542234339420, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "S1geJhC9Km", "replyto": "S1geJhC9Km", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper953/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335840324, "tmdate": 1552335840324, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper953/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 9}