{"notes": [{"id": "H1x-3xSKDr", "original": "Skxybx-tvS", "number": 2525, "cdate": 1569439912521, "ddate": null, "tcdate": 1569439912521, "tmdate": 1577168281258, "tddate": null, "forum": "H1x-3xSKDr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["gallowaa@uoguelph.ca", "agolubeva@perimeterinstitute.ca", "thomas.tanay.13@ucl.ac.uk", "mmoussa@uoguelph.ca", "gwtaylor@uoguelph.ca"], "title": "Batch Normalization is a Cause of Adversarial Vulnerability", "authors": ["Angus Galloway", "Anna Golubeva", "Thomas Tanay", "Medhat Moussa", "Graham W. Taylor"], "pdf": "/pdf/b4194c0ee567812e4a29e27d0deebb65dc0242e3.pdf", "TL;DR": "Batch normalization reduces robustness at test-time to common corruptions and adversarial examples.", "abstract": "Batch normalization (BN) is often used in an attempt to stabilize and accelerate training in deep neural networks. In many cases it indeed decreases the number of parameter updates required to achieve low training error. However, it also reduces robustness to small adversarial input perturbations and common corruptions by double-digit percentages, as we show on five standard datasets. Furthermore, we find that substituting weight decay for BN is sufficient to nullify a relationship between adversarial vulnerability and the input dimension. A recent mean-field analysis found that BN induces gradient explosion when used on multiple layers, but this cannot fully explain the vulnerability we observe, given that it occurs already for a single BN layer. We argue that the actual cause is the tilting of the decision boundary with respect to the nearest-centroid classifier along input dimensions of low variance. As a result, the constant introduced for numerical stability in the BN step acts as an important hyperparameter that can be tuned to recover some robustness at the cost of standard test accuracy. We explain this mechanism explicitly on a linear ``toy model and show in experiments that it still holds for nonlinear ``real-world models.", "keywords": ["batch normalization", "adversarial examples", "robustness"], "paperhash": "galloway|batch_normalization_is_a_cause_of_adversarial_vulnerability", "original_pdf": "/attachment/0d17fb5927362e54d58bec28099a64e132d8b968.pdf", "_bibtex": "@misc{\ngalloway2020batch,\ntitle={Batch Normalization is a Cause of Adversarial Vulnerability},\nauthor={Angus Galloway and Anna Golubeva and Thomas Tanay and Medhat Moussa and Graham W. Taylor},\nyear={2020},\nurl={https://openreview.net/forum?id=H1x-3xSKDr}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 20, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "42owlipPkB", "original": null, "number": 1, "cdate": 1576798751232, "ddate": null, "tcdate": 1576798751232, "tmdate": 1576800884465, "tddate": null, "forum": "H1x-3xSKDr", "replyto": "H1x-3xSKDr", "invitation": "ICLR.cc/2020/Conference/Paper2525/-/Decision", "content": {"decision": "Reject", "comment": "This article studies the effects of BN on robustness. The article presents a series of experiments on various datasets with noise, PGD adversarial attacks, and various corruption benchmarks, that show a drop in robustness when using BN. It is suggested that a main cause of vulnerability is the tiling angle of the decision boundary, which is illustrated in a toy example. \nThe reviewers found the contribution interesting and that the effect will impact many DNNs. However, they the did not find the arguments for the tiling explanation convincing enough, and suggested more theory and experimental illustration of this explanation would be important. In the rebuttal the authors maintain that the main contribution is to link BN and adversarial vulnerability and consider their explanation reasonable. In the initial discussion the reviewers also mentioned that the experiments were not convincing enough and that the phenomenon could be an effect of gradient masking, and that more experiments with other attack strategies would be important to clarify this. In response, the revision included various experiments, including some with various initial learning schedules. The revision clarified some of these issues. However, the reviewers still found that the reason behind the effect requires more explanations. In summary, this article makes an important observation that is already generating a vivid discussion and will likely have an impact, but the reviewers were not convinced by the explanations provided for these observations. \n", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["gallowaa@uoguelph.ca", "agolubeva@perimeterinstitute.ca", "thomas.tanay.13@ucl.ac.uk", "mmoussa@uoguelph.ca", "gwtaylor@uoguelph.ca"], "title": "Batch Normalization is a Cause of Adversarial Vulnerability", "authors": ["Angus Galloway", "Anna Golubeva", "Thomas Tanay", "Medhat Moussa", "Graham W. Taylor"], "pdf": "/pdf/b4194c0ee567812e4a29e27d0deebb65dc0242e3.pdf", "TL;DR": "Batch normalization reduces robustness at test-time to common corruptions and adversarial examples.", "abstract": "Batch normalization (BN) is often used in an attempt to stabilize and accelerate training in deep neural networks. In many cases it indeed decreases the number of parameter updates required to achieve low training error. However, it also reduces robustness to small adversarial input perturbations and common corruptions by double-digit percentages, as we show on five standard datasets. Furthermore, we find that substituting weight decay for BN is sufficient to nullify a relationship between adversarial vulnerability and the input dimension. A recent mean-field analysis found that BN induces gradient explosion when used on multiple layers, but this cannot fully explain the vulnerability we observe, given that it occurs already for a single BN layer. We argue that the actual cause is the tilting of the decision boundary with respect to the nearest-centroid classifier along input dimensions of low variance. As a result, the constant introduced for numerical stability in the BN step acts as an important hyperparameter that can be tuned to recover some robustness at the cost of standard test accuracy. We explain this mechanism explicitly on a linear ``toy model and show in experiments that it still holds for nonlinear ``real-world models.", "keywords": ["batch normalization", "adversarial examples", "robustness"], "paperhash": "galloway|batch_normalization_is_a_cause_of_adversarial_vulnerability", "original_pdf": "/attachment/0d17fb5927362e54d58bec28099a64e132d8b968.pdf", "_bibtex": "@misc{\ngalloway2020batch,\ntitle={Batch Normalization is a Cause of Adversarial Vulnerability},\nauthor={Angus Galloway and Anna Golubeva and Thomas Tanay and Medhat Moussa and Graham W. Taylor},\nyear={2020},\nurl={https://openreview.net/forum?id=H1x-3xSKDr}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "H1x-3xSKDr", "replyto": "H1x-3xSKDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795704476, "tmdate": 1576800252061, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2525/-/Decision"}}}, {"id": "rJxofaHa_r", "original": null, "number": 1, "cdate": 1570753810672, "ddate": null, "tcdate": 1570753810672, "tmdate": 1574236053784, "tddate": null, "forum": "H1x-3xSKDr", "replyto": "H1x-3xSKDr", "invitation": "ICLR.cc/2020/Conference/Paper2525/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #3", "review": "This paper identifies an important weakness of batch normalization: it increases adversarial vulnerability. It is very well written and the claims are theoretically sound. In the experiments, the authors demonstrated a significant difference in robustness between networks with or without batch normalization layers, in varies settings against both random input noise and adversarial noise. This weakness of batch norm was explained due to the \"decision boundary tilting\" effect caused by the normalization. Overall, this paper has done solid work to reveal an interesting phenomenon. If it is true, this finding will impact almost all DNN models. \n\nMy concern is that this phenomenon is just another effect of \"gradient masking \" (as pointed out by Athalye, et al.). Batch norm is a well-known technique to avoid overfitting, without batch norm the network can be easily trained to be saturated with almost zero gradients, demonstrating a false signal of \"robustness\" to noise. The random noise and real-world corruption experiments are definitely helpful to clear this doubt, but only partially. My concern remains because of two obvious signs of  gradient masking: \n1. The accuracy on PGD-li (epsilon=0.031) attacks are suspiciously too high (20% - 40% Table 3/4). For this level of attack, the acc should be nearly zero. This is likely caused by the gradient masking effect, considering the cifar-10 networks were trained for longer time with larger learning rate (150 epochs, fixed lr 0.01). Training on MNIST is much easier to get zero gradients.  \n2. The weight decay discussion is not helpful at all, on the contrary, it confirms my concern on the gradient masking effect. In Table 8, the robustness was increased ~40% by just using large weight decay. This is not the \"real robustness\", and can be easily evaded by adaptive attack (see Athalye's paper).\nWith the above two concerns in mind, I doubt the phenomenon revealed in this paper is just \"one can easily train a saturated model without batch norm\" or equivalently \"it's hard to train a saturated model with batch norm\". It is hard to say if this is a bad thing for batch norm.\n\nI am quite surprised that the authors ignore this completely. Here are a few things that can be done to rule out the possibility of gradient masking. The masked gradient can be identified by: 1) One-step attacks perform better than iterative attacks; 2) Unbounded attacks do not reach 100% success., etc (see Section 3.1 of Athalye's paper).\n1. Including FGSM in the experiments and show the same trends as PGD-li. \n2. Show two networks have similar gradient norms.\n3. Apply cw-l2 attack, and show batch norm has forced large perturbation.\n\nTwo other suggestions:\n1. Summarize the different angles/steps taken to verify the phenomenon, somewhere before the experiments.\n2. Cannot see why the input dimension discussion contribute to explanations of the batch norm weakness.\n\n============\nMy rating stays the same after rebuttal. \n\nMy original concerns are like the other reviewers: why BN, not other techniques such as structure of DNNs MLP vs CNN vs ResNet, activation functions, weight decay, learning rates, softmax etc. My initial suspect was that it is caused by gradient masking likely caused by the l2 weight regularization, so asked the authors to look at the gradient norms and run some testes to rule this out. Yes, the weight norm is directly related to the Lipschitz continuity of the function represented by the network, but it often becomes more complicated on complex nonlinear neural networks. \n\nAccording to the new experiment results, the vulnerability is indeed not an effect of gradient masking, thanks for the clarification. However, the new results also indicate that the finding is susceptible to both weight decay and learning rate: in Figure 16 (a): \"Un PGD\" < \"BN PGD\" before learning rate decay, andFigure 17 (a) vs (b), doubling the weight decay penalty to 1e-3 also increases the vulnerability of BN. Overall, I believe the phenomenon exists, but the reasons behind requires more explanations, at least not just the batch norm.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper2525/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2525/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["gallowaa@uoguelph.ca", "agolubeva@perimeterinstitute.ca", "thomas.tanay.13@ucl.ac.uk", "mmoussa@uoguelph.ca", "gwtaylor@uoguelph.ca"], "title": "Batch Normalization is a Cause of Adversarial Vulnerability", "authors": ["Angus Galloway", "Anna Golubeva", "Thomas Tanay", "Medhat Moussa", "Graham W. Taylor"], "pdf": "/pdf/b4194c0ee567812e4a29e27d0deebb65dc0242e3.pdf", "TL;DR": "Batch normalization reduces robustness at test-time to common corruptions and adversarial examples.", "abstract": "Batch normalization (BN) is often used in an attempt to stabilize and accelerate training in deep neural networks. In many cases it indeed decreases the number of parameter updates required to achieve low training error. However, it also reduces robustness to small adversarial input perturbations and common corruptions by double-digit percentages, as we show on five standard datasets. Furthermore, we find that substituting weight decay for BN is sufficient to nullify a relationship between adversarial vulnerability and the input dimension. A recent mean-field analysis found that BN induces gradient explosion when used on multiple layers, but this cannot fully explain the vulnerability we observe, given that it occurs already for a single BN layer. We argue that the actual cause is the tilting of the decision boundary with respect to the nearest-centroid classifier along input dimensions of low variance. As a result, the constant introduced for numerical stability in the BN step acts as an important hyperparameter that can be tuned to recover some robustness at the cost of standard test accuracy. We explain this mechanism explicitly on a linear ``toy model and show in experiments that it still holds for nonlinear ``real-world models.", "keywords": ["batch normalization", "adversarial examples", "robustness"], "paperhash": "galloway|batch_normalization_is_a_cause_of_adversarial_vulnerability", "original_pdf": "/attachment/0d17fb5927362e54d58bec28099a64e132d8b968.pdf", "_bibtex": "@misc{\ngalloway2020batch,\ntitle={Batch Normalization is a Cause of Adversarial Vulnerability},\nauthor={Angus Galloway and Anna Golubeva and Thomas Tanay and Medhat Moussa and Graham W. Taylor},\nyear={2020},\nurl={https://openreview.net/forum?id=H1x-3xSKDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "H1x-3xSKDr", "replyto": "H1x-3xSKDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2525/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2525/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575842317395, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2525/Reviewers"], "noninvitees": [], "tcdate": 1570237721620, "tmdate": 1575842317409, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2525/-/Official_Review"}}}, {"id": "H1xR93zcoB", "original": null, "number": 11, "cdate": 1573690518397, "ddate": null, "tcdate": 1573690518397, "tmdate": 1573854644092, "tddate": null, "forum": "H1x-3xSKDr", "replyto": "H1x-3xSKDr", "invitation": "ICLR.cc/2020/Conference/Paper2525/-/Official_Comment", "content": {"title": "Summary of changes", "comment": "Dear reviewers, \n\nThank you for your thought provoking reviews which have helped us improve the work. Here we briefly recap the main points expressed in the reviews, and where changes have been made to accommodate the concerns raised.\n\nReviews 2 and 3 are encouraged by the contribution and its significance, but with legitimate concerns about the \"gradient masking\" phenomenon (R3), and to what extent the results apply to adversarially trained models (R2). \n\nWe have demonstrated absence of gradient masking through the following best practices:\n- Additive white Gaussian noise (AWGN), and common corruption benchmarks that don't require gradients\n- Accuracy versus perturbation magnitude curves (Fig. 5, all go to zero accuracy)\n- Unbounded white-box attacks all reach 100% success (fooling images, PGD, CWL2)\n- Single step attacks perform worse than iterative attacks (wrt the attacker)\n- Black-box transferability analysis on ImageNet (Table 12)\n- Qualitative evidence, i.e., adversarial examples that contain semantic features\n\nRegarding adversarial training, we have added commentary and more detailed results showing the limitations of the PGD training approach on common corruptions.\n\nR1 was more sceptical, expressing concerns that the training procedure and effective learning rate may have confounded the results. R1 may not have seen some important details regarding the initial learning rate and schedules that were considered originally, but we can see how this could have been easily missed and have reorganized the section in question. Also, based on their feedback and after reading the linked Li et al., 19 work, we made some interesting discoveries while evaluating robustness on CIFAR-10 every ten epochs during training for different learning rate schedules, which shows more precisely at which times the baselines outperform their BN equivalent.\n\nTo more easily parse through the changes, we provide a detailed changelog below:\n\n- Re-organize Section 4 \"Empirical Results\" (per R1, R3), defer implementation details to Appendix.\n\n- Section 5, clarify relevance of input dimension experiments (per R3)\n\n- Discussed the work of Labatie 2019 where relevant in the Introduction and Section 3.\n\n- Appendix B \"PGD Implementation Details\" centralize and clarify all implementation details related to PGD and preprocessing (per R2).\n\n- Appendix D \"PGD Training Yields Unfortunate Robustness Trade-Offs\". Supplementary explanations and full breakdown of MNIST-C results with multiple runs (Table 9), showing that a reasonable baseline with similar clean test accuracy outperforms: BN, PGD, and PGD + BN.\n\n- Appendix I \"Alternative Explanations of the Vulnerability\". We place the results of a concurrent submission in the context of our work and discussion of BN's numerical stability constant. Qualitatively and quantitatively shows what happens when we \"fold\" the BN statistics into the weights post-training.\n\n- Appendix J \"Adversarial Examples\". We craft fooling images and Carlini & Wagner L2 examples on MNIST and SVHN, providing qualitative and quantitative evidence that BN degrades robustness.\n\n- Appendix K \"On The Initial Learning Rate\". We place our results in the context of Li et al., showing that *at no time during training* does the *best robustness of BN* exceed that obtainable by an equivalent unnormalized model. The BN model's PGD (40-step) accuracy is consistently below 40%, whereas the baseline achieves 52%. We support these results with two standard tests of gradient masking by comparison with AWGN, and FGSM (per R3 concern). Perhaps most interesting is that the PGD accuracy of the BN models declines during training, whereas that of the baseline increases monotonically or plateaus. The use of early stopping is therefore much more important when BN is used, but this still does not recover the robustness of the unnormalized model.\n\nWe sincerely hope to have the opportunity to engage in discussions with the reviewers prior to the platform closing in case anything remains unclear, and we thank them for their effort."}, "signatures": ["ICLR.cc/2020/Conference/Paper2525/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2525/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["gallowaa@uoguelph.ca", "agolubeva@perimeterinstitute.ca", "thomas.tanay.13@ucl.ac.uk", "mmoussa@uoguelph.ca", "gwtaylor@uoguelph.ca"], "title": "Batch Normalization is a Cause of Adversarial Vulnerability", "authors": ["Angus Galloway", "Anna Golubeva", "Thomas Tanay", "Medhat Moussa", "Graham W. Taylor"], "pdf": "/pdf/b4194c0ee567812e4a29e27d0deebb65dc0242e3.pdf", "TL;DR": "Batch normalization reduces robustness at test-time to common corruptions and adversarial examples.", "abstract": "Batch normalization (BN) is often used in an attempt to stabilize and accelerate training in deep neural networks. In many cases it indeed decreases the number of parameter updates required to achieve low training error. However, it also reduces robustness to small adversarial input perturbations and common corruptions by double-digit percentages, as we show on five standard datasets. Furthermore, we find that substituting weight decay for BN is sufficient to nullify a relationship between adversarial vulnerability and the input dimension. A recent mean-field analysis found that BN induces gradient explosion when used on multiple layers, but this cannot fully explain the vulnerability we observe, given that it occurs already for a single BN layer. We argue that the actual cause is the tilting of the decision boundary with respect to the nearest-centroid classifier along input dimensions of low variance. As a result, the constant introduced for numerical stability in the BN step acts as an important hyperparameter that can be tuned to recover some robustness at the cost of standard test accuracy. We explain this mechanism explicitly on a linear ``toy model and show in experiments that it still holds for nonlinear ``real-world models.", "keywords": ["batch normalization", "adversarial examples", "robustness"], "paperhash": "galloway|batch_normalization_is_a_cause_of_adversarial_vulnerability", "original_pdf": "/attachment/0d17fb5927362e54d58bec28099a64e132d8b968.pdf", "_bibtex": "@misc{\ngalloway2020batch,\ntitle={Batch Normalization is a Cause of Adversarial Vulnerability},\nauthor={Angus Galloway and Anna Golubeva and Thomas Tanay and Medhat Moussa and Graham W. Taylor},\nyear={2020},\nurl={https://openreview.net/forum?id=H1x-3xSKDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1x-3xSKDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2525/Authors", "ICLR.cc/2020/Conference/Paper2525/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2525/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2525/Reviewers", "ICLR.cc/2020/Conference/Paper2525/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2525/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2525/Authors|ICLR.cc/2020/Conference/Paper2525/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504140068, "tmdate": 1576860534953, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2525/Authors", "ICLR.cc/2020/Conference/Paper2525/Reviewers", "ICLR.cc/2020/Conference/Paper2525/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2525/-/Official_Comment"}}}, {"id": "SJeMYKKyiS", "original": null, "number": 4, "cdate": 1572997497793, "ddate": null, "tcdate": 1572997497793, "tmdate": 1573681775159, "tddate": null, "forum": "H1x-3xSKDr", "replyto": "rJxofaHa_r", "invitation": "ICLR.cc/2020/Conference/Paper2525/-/Official_Comment", "content": {"title": "Clarification about gradient masking", "comment": "We thank the reviewer for recognising the positive aspects of this work, and for stating that they believe the work to be theoretically sound. We agree that it's imperative to ensure results are not tainted by gradient masking; we have taken care to ensure this did not play a role here and wish to alleviate this natural concern.\n\n-- \n\nRe: 1. \u201cThe accuracy on PGD-li (epsilon=0.031) are suspiciously too high (20% - 40% in Tables 3, 4), the accuracy should be nearly zero\u201d. \n\nFor the experiments of Section 4, the input was normalized to zero mean and unit variance using per-channel statistics computed per dataset. Thus, epsilon=0.03 represents about 4/255 rather than 8/255 on the [0, 1] scale. In Figure 5, epsilon is increased until zero accuracy is reached for a vanilla ResNet, and accuracy is indeed reduced to zero by epsilon=6 for all models under standard training. The difference in accuracy at epsilon 4/255 compared to Tables 3/4 is explained by the difference in architecture, WideResNet and VGG. In particular, VGG is more robust than the residual networks we tested under standard training. \n\n--\n\nRe: 2. \u201cThe weight decay discussion is not helpful at all, on the contrary, it confirms my concern on the gradient masking effect. In Table 8, the robustness was increased ~40% by just using large weight decay. This is not the \"real robustness\", and can be easily evaded by adaptive attack (see Athalye's paper).\n\n1. The claim you mention (40% increase in robustness) was evaluated using an adaptive method.\n\n2. We interpret this comment as an affirmation that our findings are non-obvious, rather than as a limitation. \n\n3. We would really appreciate if you can clarify why you believe the weight decay discussion is not helpful, and the robustness \u201cnot real\u201d. There are strong theoretical connections between penalizing the parameter norms, robust optimization, and decision boundary tilting, see e.g., Xu & Manor, (JMLR 2009), Tanay & Griffin, (2016). We also showed that this is essential for mitigating an increase in vulnerability as the input dimension increases. L2 weight decay minimizes an upper bound on the Frobenius norm of the linear operators in the network, thus bounding the Lipschitz constant of the network. The Lipschitz constant taken together with the mean prediction margin is well known to govern adversarial perturbation robustness (Tsuzuku et al., in NeurIPS 2018).\n\n4. Can you recommend a specific attack you would like us to evaluate against that would increase your confidence in our results? We are familiar with the work of Athalye et al and do not believe that the attack evaluation is in any way related to the \"defense\", all attacks are unseen to all models as of training time. Furthermore, a concurrent submission and Reviewer 2 appear to have successfully reproduced our main result. \n\n--\n\nRe: \"One-step attacks perform better than iterative attacks\". \n\nAre you referring to an instance of this rule being violated in our work (if you believe you saw this, it would help us if you could point to a specific instance), or are you suggesting that we show results for one step attacks alongside those of iterative attacks? It's generally agreed that one step attacks aren't as meaningful for deep/nonlinear models, which is why we only use FGSM for linear models and PGD for deep models. Nonetheless, we have added this result to a new section of the Appendix titled \u201cOn the Initial Learning Rate\u201d where we plot the test accuracy under various perturbations vs training epochs. The FGSM curves lie above the 40-step PGD curve for a given model in all cases.\n\nAlso, Appendix C deals with unbounded attacks which reach 100% success in all cases. \n\n--\n\nRe: \u201cApply cw-l2 attack, and show batch norm has forced large perturbation.\u201d\n\nThank you for this suggestion. We have added adversarial examples crafted by the CWL2 method in Figure 13 of an Appendix I titled \u201cAdversarial Examples\u201d. The L2 distortion required to achieve a fixed misclassification confidence threshold is 0.95 in the case of batch norm, and 2.89 for the baseline, which represents a three fold improvement on the relevant performance metric for this attack.\n\nThe white-box procedures would not work without access to a clean gradient signal, yet all reach 100% success, or confidence, in all cases. \n\nPlease let us know if these clarifications address your concerns. We agree that our work would be far less impactful if our result could be reduced to gradient masking. We hope that the additional experiments and analysis we have performed lays this concern to rest."}, "signatures": ["ICLR.cc/2020/Conference/Paper2525/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2525/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["gallowaa@uoguelph.ca", "agolubeva@perimeterinstitute.ca", "thomas.tanay.13@ucl.ac.uk", "mmoussa@uoguelph.ca", "gwtaylor@uoguelph.ca"], "title": "Batch Normalization is a Cause of Adversarial Vulnerability", "authors": ["Angus Galloway", "Anna Golubeva", "Thomas Tanay", "Medhat Moussa", "Graham W. Taylor"], "pdf": "/pdf/b4194c0ee567812e4a29e27d0deebb65dc0242e3.pdf", "TL;DR": "Batch normalization reduces robustness at test-time to common corruptions and adversarial examples.", "abstract": "Batch normalization (BN) is often used in an attempt to stabilize and accelerate training in deep neural networks. In many cases it indeed decreases the number of parameter updates required to achieve low training error. However, it also reduces robustness to small adversarial input perturbations and common corruptions by double-digit percentages, as we show on five standard datasets. Furthermore, we find that substituting weight decay for BN is sufficient to nullify a relationship between adversarial vulnerability and the input dimension. A recent mean-field analysis found that BN induces gradient explosion when used on multiple layers, but this cannot fully explain the vulnerability we observe, given that it occurs already for a single BN layer. We argue that the actual cause is the tilting of the decision boundary with respect to the nearest-centroid classifier along input dimensions of low variance. As a result, the constant introduced for numerical stability in the BN step acts as an important hyperparameter that can be tuned to recover some robustness at the cost of standard test accuracy. We explain this mechanism explicitly on a linear ``toy model and show in experiments that it still holds for nonlinear ``real-world models.", "keywords": ["batch normalization", "adversarial examples", "robustness"], "paperhash": "galloway|batch_normalization_is_a_cause_of_adversarial_vulnerability", "original_pdf": "/attachment/0d17fb5927362e54d58bec28099a64e132d8b968.pdf", "_bibtex": "@misc{\ngalloway2020batch,\ntitle={Batch Normalization is a Cause of Adversarial Vulnerability},\nauthor={Angus Galloway and Anna Golubeva and Thomas Tanay and Medhat Moussa and Graham W. Taylor},\nyear={2020},\nurl={https://openreview.net/forum?id=H1x-3xSKDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1x-3xSKDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2525/Authors", "ICLR.cc/2020/Conference/Paper2525/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2525/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2525/Reviewers", "ICLR.cc/2020/Conference/Paper2525/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2525/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2525/Authors|ICLR.cc/2020/Conference/Paper2525/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504140068, "tmdate": 1576860534953, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2525/Authors", "ICLR.cc/2020/Conference/Paper2525/Reviewers", "ICLR.cc/2020/Conference/Paper2525/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2525/-/Official_Comment"}}}, {"id": "S1xIO7xciH", "original": null, "number": 10, "cdate": 1573679981670, "ddate": null, "tcdate": 1573679981670, "tmdate": 1573680604096, "tddate": null, "forum": "H1x-3xSKDr", "replyto": "HkxOtjiG5r", "invitation": "ICLR.cc/2020/Conference/Paper2525/-/Official_Comment", "content": {"title": "Robustness generally decreases with many BN layers; effect is slower in residual networks", "comment": "Thank you for the question and kind words. \n\nThe experiment depicted in Figure 2 examines the impact of increasing the number of layers (where every layer is batch-normalized) from 10-60 in a vanilla feedforward network. There, we see consistency on train-ability with the upper bound from Yang et al., (2019), however in terms of robustness to perturbations/noise there appears to be a sweet spot around the point where accuracy begins to degrade @ ~20-30 layers. \n\nIn a separate experiment (not reported in the paper), we train constant width ($n=384$) ReLU networks of 1-10 layers ($L$) with vanilla GD (LR=0.1) on the 3 vs 7 testbed from Section 3. For BNGD over five random seeds, at $L=1$ we have for Clean, AWGN, PGD test sets respectively, absolute test accuracies of $98.4 \\pm 0.1\\%$, $69 \\pm 1\\%$, $15.1 \\pm 0.8\\%$.  For $L=10$, this is reduced to $93.1 \\pm 0.2\\%$, $52.2 \\pm 0.8\\%$, and $0.05 \\pm 0.01\\%$.  Without BN, at $L=1$ we have $98.0 \\pm 0.1\\%$, $84.7 \\pm 0.2\\%$, $50.7 \\pm 0.7\\%$, and for $L=10$, $98.0 \\pm 0.1$, $82 \\pm 2$, $40 \\pm 5\\%$.\n\nFor BN + SGD (mini batch size 50), the trend is similar but accuracy decays more slowly than for BNGD over the range of 1-10 layers. E.g. for the AWGN test accuracy, instead of dropping from $69 \\pm 1\\%$ to $52.2 \\pm 0.8\\%$, dropping from $70 \\pm 2\\%$ to $66 \\pm 4\\%$. The biggest factor over this range is between using BN or not, the unnormalized model consistently yields higher AWGN and PGD robustness by a difference of high 10s, and high 20s absolute percent, respectively.\n\nFor residual networks, Figure 4 of the paper evaluates the robustness of ResNet{20, 32, 44, 56, 110}, which shows that the robustness gap generally widens with depth between BN and fixed-update initialization (Fixup - Zhang et al., ICLR 2019) variants. Section 7 of Labatie (ICML 2019) sheds theoretical insight as to how the combined action of BN and skip-connections slows down the exploding gradients/sensitivity. "}, "signatures": ["ICLR.cc/2020/Conference/Paper2525/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2525/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["gallowaa@uoguelph.ca", "agolubeva@perimeterinstitute.ca", "thomas.tanay.13@ucl.ac.uk", "mmoussa@uoguelph.ca", "gwtaylor@uoguelph.ca"], "title": "Batch Normalization is a Cause of Adversarial Vulnerability", "authors": ["Angus Galloway", "Anna Golubeva", "Thomas Tanay", "Medhat Moussa", "Graham W. Taylor"], "pdf": "/pdf/b4194c0ee567812e4a29e27d0deebb65dc0242e3.pdf", "TL;DR": "Batch normalization reduces robustness at test-time to common corruptions and adversarial examples.", "abstract": "Batch normalization (BN) is often used in an attempt to stabilize and accelerate training in deep neural networks. In many cases it indeed decreases the number of parameter updates required to achieve low training error. However, it also reduces robustness to small adversarial input perturbations and common corruptions by double-digit percentages, as we show on five standard datasets. Furthermore, we find that substituting weight decay for BN is sufficient to nullify a relationship between adversarial vulnerability and the input dimension. A recent mean-field analysis found that BN induces gradient explosion when used on multiple layers, but this cannot fully explain the vulnerability we observe, given that it occurs already for a single BN layer. We argue that the actual cause is the tilting of the decision boundary with respect to the nearest-centroid classifier along input dimensions of low variance. As a result, the constant introduced for numerical stability in the BN step acts as an important hyperparameter that can be tuned to recover some robustness at the cost of standard test accuracy. We explain this mechanism explicitly on a linear ``toy model and show in experiments that it still holds for nonlinear ``real-world models.", "keywords": ["batch normalization", "adversarial examples", "robustness"], "paperhash": "galloway|batch_normalization_is_a_cause_of_adversarial_vulnerability", "original_pdf": "/attachment/0d17fb5927362e54d58bec28099a64e132d8b968.pdf", "_bibtex": "@misc{\ngalloway2020batch,\ntitle={Batch Normalization is a Cause of Adversarial Vulnerability},\nauthor={Angus Galloway and Anna Golubeva and Thomas Tanay and Medhat Moussa and Graham W. Taylor},\nyear={2020},\nurl={https://openreview.net/forum?id=H1x-3xSKDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1x-3xSKDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2525/Authors", "ICLR.cc/2020/Conference/Paper2525/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2525/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2525/Reviewers", "ICLR.cc/2020/Conference/Paper2525/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2525/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2525/Authors|ICLR.cc/2020/Conference/Paper2525/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504140068, "tmdate": 1576860534953, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2525/Authors", "ICLR.cc/2020/Conference/Paper2525/Reviewers", "ICLR.cc/2020/Conference/Paper2525/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2525/-/Official_Comment"}}}, {"id": "B1xtz9k9sB", "original": null, "number": 9, "cdate": 1573677584730, "ddate": null, "tcdate": 1573677584730, "tmdate": 1573677720989, "tddate": null, "forum": "H1x-3xSKDr", "replyto": "SyxyABk9ir", "invitation": "ICLR.cc/2020/Conference/Paper2525/-/Official_Comment", "content": {"title": "Response (part 2) - Implementation details and references", "comment": "\"Could you provide more implementation details about the adversarial training and attacker?\"\n\nThe meta-parameters used for the PGD training of the WideResNet 28-10 on CIFAR-10-C were  $\\epsilon_{\\max} = 4/255$, 5 iterations, step size of 1. This was the model for which accuracy on the contrast CIFAR-10-C corruption is degraded by over 20% absolute for Fixup and BN variants. These details were also in a Figure caption, but we have made the connection more explicit in text. We also PGD train normal ResNets {32, 110} using $\\epsilon_{\\max}=8/255$, 7 iterations, and a step size of 2 (as in Madry et al., 18) and observe similar trends.\n\nFor PGD training on MNIST we use 20 iterations with a step size $\\epsilon / 10$.\n\n--\n\nRe: Question about 20 or 40 iterations of PGD for eval\n\nWe agree this wasn\u2019t clear from our language. We report 20 iterations in most cases, but confirmed that 40 iterations did not significantly improve upon this, i.e., degrade the accuracy much further to within the measurement random error. For example, for VGG16 on CIFAR-10 evaluated using 40 iterations of PGD with a step size of $\\epsilon_\\infty / 20$, instead of 20 iterations with $\\epsilon_\\infty / 10$, reduced accuracy from $28.9 \\pm 0.2\\%$ to $28.5 \\pm 0.3\\%$, a difference of $0.4 \\pm 0.5\\%$.\n\nAlso, to err on the side of caution, we use 40 iterations of PGD for evaluation on CIFAR-10 in the new Appendix \"On The Initial Learning Rate\".\n\n--\n\nLastly, as suggested we have discussed the work of Labatie 2019 in several places where relevant. \n\nReferences\n\nYash Sharma and Pin-Yu Chen. Attacking the Madry Defense Model with $L_1$-based Adversarial Examples. In ICLR Workshop, 2018.\n\nLukas Schott, Jonas Rauber, Matthias Bethge, and Wieland Brendel. Towards the first adversarially robust neural network model on mnist. International Conference for Learning Representations, 2019.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2525/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2525/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["gallowaa@uoguelph.ca", "agolubeva@perimeterinstitute.ca", "thomas.tanay.13@ucl.ac.uk", "mmoussa@uoguelph.ca", "gwtaylor@uoguelph.ca"], "title": "Batch Normalization is a Cause of Adversarial Vulnerability", "authors": ["Angus Galloway", "Anna Golubeva", "Thomas Tanay", "Medhat Moussa", "Graham W. Taylor"], "pdf": "/pdf/b4194c0ee567812e4a29e27d0deebb65dc0242e3.pdf", "TL;DR": "Batch normalization reduces robustness at test-time to common corruptions and adversarial examples.", "abstract": "Batch normalization (BN) is often used in an attempt to stabilize and accelerate training in deep neural networks. In many cases it indeed decreases the number of parameter updates required to achieve low training error. However, it also reduces robustness to small adversarial input perturbations and common corruptions by double-digit percentages, as we show on five standard datasets. Furthermore, we find that substituting weight decay for BN is sufficient to nullify a relationship between adversarial vulnerability and the input dimension. A recent mean-field analysis found that BN induces gradient explosion when used on multiple layers, but this cannot fully explain the vulnerability we observe, given that it occurs already for a single BN layer. We argue that the actual cause is the tilting of the decision boundary with respect to the nearest-centroid classifier along input dimensions of low variance. As a result, the constant introduced for numerical stability in the BN step acts as an important hyperparameter that can be tuned to recover some robustness at the cost of standard test accuracy. We explain this mechanism explicitly on a linear ``toy model and show in experiments that it still holds for nonlinear ``real-world models.", "keywords": ["batch normalization", "adversarial examples", "robustness"], "paperhash": "galloway|batch_normalization_is_a_cause_of_adversarial_vulnerability", "original_pdf": "/attachment/0d17fb5927362e54d58bec28099a64e132d8b968.pdf", "_bibtex": "@misc{\ngalloway2020batch,\ntitle={Batch Normalization is a Cause of Adversarial Vulnerability},\nauthor={Angus Galloway and Anna Golubeva and Thomas Tanay and Medhat Moussa and Graham W. Taylor},\nyear={2020},\nurl={https://openreview.net/forum?id=H1x-3xSKDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1x-3xSKDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2525/Authors", "ICLR.cc/2020/Conference/Paper2525/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2525/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2525/Reviewers", "ICLR.cc/2020/Conference/Paper2525/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2525/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2525/Authors|ICLR.cc/2020/Conference/Paper2525/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504140068, "tmdate": 1576860534953, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2525/Authors", "ICLR.cc/2020/Conference/Paper2525/Reviewers", "ICLR.cc/2020/Conference/Paper2525/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2525/-/Official_Comment"}}}, {"id": "SyxyABk9ir", "original": null, "number": 8, "cdate": 1573676486593, "ddate": null, "tcdate": 1573676486593, "tmdate": 1573676486593, "tddate": null, "forum": "H1x-3xSKDr", "replyto": "S1lNkzJ2YB", "invitation": "ICLR.cc/2020/Conference/Paper2525/-/Official_Comment", "content": {"title": "Response (part 1) - on visualizations and PGD training", "comment": "Thank you for your feedback, and for taking the time to reproduce the main result on several architectures, this effort goes above and beyond, we appreciate it. We respond to each concern below.\n\n--\n\n\"Why do not visualize the decision boundary of networks (used in this work) to valid the boundary tilting \"theory\". The toy example is clear but not convincing enough. There exist several techniques may be helpful to the visualization. (i.e. Robustness via curvature regularization, and vice versa). I think it is one of the important parts of this work.\"\n\nAside from space limitations, we wanted to restrict visualizations to scenarios where they are faithful to the underlying model. Although such visualizations would be nice to have for further intuition, we believe the experiments provide sufficient evidence to support the main hypothesis. Also, beyond the toy model, we computed the boundary tilting angle of linear models w.r.t. the nearest centroid classifier on MNIST. Any dimensionality reduction technique used to visualize a high dimensional decision boundary introduces trade-offs that we didn\u2019t feel were necessary to support the main message. We may opt to add such visualizations in a subsequent blog article as you suggest.\n\n--\n\n\"The observation of BN causes adversarial vulnerability is interesting, but the main focus should be offering more convincing explanations.\"\n\nThank you for stating that you believe the main finding of this work is interesting. We believe we have provided a reasonably accessible explanation, but wish to emphasize that our main contribution is that, to the best of our knowledge, this is the first work to explicitly link batch norm and adversarial vulnerability. This has major implications for state-of-the-art networks, and we believe this connection was previously unknown in the adversarial examples / robustness to distribution shift literature. Many laws of physics that we now take for granted were initially discovered via systematic experiment, and subsequently formalized by others. Other works, e.g., Yang et al., ICLR 2019, Labatie, ICML 2019 examine batch norm at length from a theoretical perspective which supports our conclusions, and a concurrent submission provides further insight as to why the vulnerability we discovered occurs. The official reviews of Yang et al., (https://openreview.net/forum?id=SyMDXnCcF7) expressed concern that their approach was not particularly intuitive or reflecting of popular practice, hence we made an effort to cast the limitations of BN in a practical light for practitioners and researchers of broad backgrounds to understand.\n\n--\n\n\"I do run experiments \u2026 There exist ~20 ATA performance gaps between networks with BN and networks without BN. But for adversarial trained models, the gaps don't exist anymore, at least for VGG11,13,16,19 on cifar10 with PGD 3 attack. (The performance gap is less than 0.5). In other words, without the BN layers, the robustness of adversarially trained models will not increase in my experiments. I see you report some results in Appendix C. But it is not enough to convince me. If adversarial training can fix the vulnerability by BN and BN can give a TA boost, there is no reason we need to remove BN in our adversarial training setting.\"\n\nWe agree that the adversarial training results originally presented in Appendix C were perhaps a bit too informal. We have tightened up this section, including a full breakdown showing test accuracy and variance for each corruption, instead of simply stating the mean test accuracy over all corruptions.\n\nAs we motivated in the main text, it is imperative to consider robustness to unseen adversaries. Thus, it is unfair to benchmark the robustness of natural and adversarially trained networks using the same procedure, when one approach directly optimizes performance w.r.t. one of the evaluations. As you found, in some circumstances the performance degradation of BN seems small if we train on PGD and evaluate on the same, but this no longer holds if we consider other more realistic threat models and common corruptions. \n\nTo be more clear, we have renamed Appendix C to \u201cPGD Training Yields Unfortunate Robustness Trade-offs\u201d, as PGD can fail to yield a model with semantically meaningful gradients or convincingly broad robustness even for MNIST, due to the thresholding operation that is learned to favor $\\ell_{\\infty}$ robustness (see the discussion of Appendix E \u201cMNIST Inspection\u201d of Madry et al., (ICLR 2018)). These limitations have been widely discussed, e.g., Sharma & Chen., (ICLR Workshop 2018), Schott et al., (ICLR 2019), Jacobsen et al., (ICLR 2019, ICLR Workshop 2019), Mu & Gilmer., (ICML Workshop 2019) and are not too surprising given Goodhart\u2019s law: \u201cWhen a measure becomes a target, it ceases to be a good measure\u201d. The \u201cmeasure\u201d in this case being the $\\ell_{\\infty}$ norm of the perturbations, which has become, somewhat arbitrarily, a focal point in the adversarial examples literature.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2525/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2525/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["gallowaa@uoguelph.ca", "agolubeva@perimeterinstitute.ca", "thomas.tanay.13@ucl.ac.uk", "mmoussa@uoguelph.ca", "gwtaylor@uoguelph.ca"], "title": "Batch Normalization is a Cause of Adversarial Vulnerability", "authors": ["Angus Galloway", "Anna Golubeva", "Thomas Tanay", "Medhat Moussa", "Graham W. Taylor"], "pdf": "/pdf/b4194c0ee567812e4a29e27d0deebb65dc0242e3.pdf", "TL;DR": "Batch normalization reduces robustness at test-time to common corruptions and adversarial examples.", "abstract": "Batch normalization (BN) is often used in an attempt to stabilize and accelerate training in deep neural networks. In many cases it indeed decreases the number of parameter updates required to achieve low training error. However, it also reduces robustness to small adversarial input perturbations and common corruptions by double-digit percentages, as we show on five standard datasets. Furthermore, we find that substituting weight decay for BN is sufficient to nullify a relationship between adversarial vulnerability and the input dimension. A recent mean-field analysis found that BN induces gradient explosion when used on multiple layers, but this cannot fully explain the vulnerability we observe, given that it occurs already for a single BN layer. We argue that the actual cause is the tilting of the decision boundary with respect to the nearest-centroid classifier along input dimensions of low variance. As a result, the constant introduced for numerical stability in the BN step acts as an important hyperparameter that can be tuned to recover some robustness at the cost of standard test accuracy. We explain this mechanism explicitly on a linear ``toy model and show in experiments that it still holds for nonlinear ``real-world models.", "keywords": ["batch normalization", "adversarial examples", "robustness"], "paperhash": "galloway|batch_normalization_is_a_cause_of_adversarial_vulnerability", "original_pdf": "/attachment/0d17fb5927362e54d58bec28099a64e132d8b968.pdf", "_bibtex": "@misc{\ngalloway2020batch,\ntitle={Batch Normalization is a Cause of Adversarial Vulnerability},\nauthor={Angus Galloway and Anna Golubeva and Thomas Tanay and Medhat Moussa and Graham W. Taylor},\nyear={2020},\nurl={https://openreview.net/forum?id=H1x-3xSKDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1x-3xSKDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2525/Authors", "ICLR.cc/2020/Conference/Paper2525/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2525/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2525/Reviewers", "ICLR.cc/2020/Conference/Paper2525/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2525/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2525/Authors|ICLR.cc/2020/Conference/Paper2525/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504140068, "tmdate": 1576860534953, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2525/Authors", "ICLR.cc/2020/Conference/Paper2525/Reviewers", "ICLR.cc/2020/Conference/Paper2525/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2525/-/Official_Comment"}}}, {"id": "rkgmZfkciB", "original": null, "number": 7, "cdate": 1573675515327, "ddate": null, "tcdate": 1573675515327, "tmdate": 1573675515327, "tddate": null, "forum": "H1x-3xSKDr", "replyto": "Sygo0J1ciH", "invitation": "ICLR.cc/2020/Conference/Paper2525/-/Official_Comment", "content": {"title": "Response (part 2)", "comment": "\u201cthere is nothing \"special\" nor unique about BN-networks; instead, the BN factorization merely permits accelerated training efficiency. Consider the fact that a BN model may be re-expressed by merely folding in the parameters (i.e. applying the matrix multiplications) into the MLP weights or CNN filters. Thus, the numerical function approximated by the BN and the \"folded\" non-BN model is identical.\u201d\n\nAs outlined by existing theoretical work, training with BN is unique in several respects. To start, it imposes a hard upper limit on maximum trainable depth that is solely a function of the mini-batch size due to gradient explosion (Thms 3.9 & 3.10, Yang et al., 2019), and leads to exploding sensitivity wrt the input (S6 & Fig 4, Labatie et al., 2019). \n\nNote: in response to a concurrent submission which suggests that the use of tracked statistics are the source of adversarial vulnerability, we\u2019ve added an Appendix H \u201cAlternative Explanations of the Vulnerability\u201d in which we isolate the effect of folding the BN statistics into the weights post-training on MNIST. The weights learned under BN, either with or without plugging in the tracked statistics at test time, differ substantially both qualitatively, and quantitatively in terms of boundary tilting angle wrt unnormalized weights. \n\n--\n\n\u201cadversarial vulnerability predates BN. Likewise, non-BN models exhibit adversarial vulnerability. Thus, this title is not a great reflection of the findings of the paper. I would strongly suggest replacing \"is a cause\" with \"increases\" or \"exacerbates\". \n\nWe are happy to discuss the title of this work. The publication of adversarial examples predating that of batch normalization does not imply that batch normalization cannot act as one of many contributory causes of adversarial vulnerability. Qualitatively, we demonstrated that for MNIST, where a reasonably robust model can be obtained via per-image normalization and L2 regularization (see the adversarial examples of Appendix I which support this), the addition of BN in this case is sufficient to induce adversarial vulnerability. In terms of an apparent relationship between input dimension and vulnerability, which intuitively should not affect robustness as it does not affect the signal-to-noise ratio (Shafahi et al., ICLR 2019), the introduction of batch norm is again sufficient for this adverse relationship to exist. Ultimately, given that robustness for natural datasets is usually a trade-off, e.g., with accuracy (Tsipras et al., 2019), or between minimum and mean margin (Wu & Yu, 2019), we can accept your suggestion to replace \"is a cause\" with \"increases\" or \"exacerbates\".\n\nReferences\n\nDimitris Tsipras and Shibani Santurkar and Logan Engstrom and Alexander Turner and Aleksander Madry. Robustness May Be at Odds with Accuracy. In International Conference on Learning Representations, 2019.\n\nKaiwen Wu and Yaoliang Yu. Understanding Adversarial Robustness: The Trade-off between Minimum and Average Margin. In NeurIPS 19 Workshop on Machine Learning with Guarantees.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2525/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2525/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["gallowaa@uoguelph.ca", "agolubeva@perimeterinstitute.ca", "thomas.tanay.13@ucl.ac.uk", "mmoussa@uoguelph.ca", "gwtaylor@uoguelph.ca"], "title": "Batch Normalization is a Cause of Adversarial Vulnerability", "authors": ["Angus Galloway", "Anna Golubeva", "Thomas Tanay", "Medhat Moussa", "Graham W. Taylor"], "pdf": "/pdf/b4194c0ee567812e4a29e27d0deebb65dc0242e3.pdf", "TL;DR": "Batch normalization reduces robustness at test-time to common corruptions and adversarial examples.", "abstract": "Batch normalization (BN) is often used in an attempt to stabilize and accelerate training in deep neural networks. In many cases it indeed decreases the number of parameter updates required to achieve low training error. However, it also reduces robustness to small adversarial input perturbations and common corruptions by double-digit percentages, as we show on five standard datasets. Furthermore, we find that substituting weight decay for BN is sufficient to nullify a relationship between adversarial vulnerability and the input dimension. A recent mean-field analysis found that BN induces gradient explosion when used on multiple layers, but this cannot fully explain the vulnerability we observe, given that it occurs already for a single BN layer. We argue that the actual cause is the tilting of the decision boundary with respect to the nearest-centroid classifier along input dimensions of low variance. As a result, the constant introduced for numerical stability in the BN step acts as an important hyperparameter that can be tuned to recover some robustness at the cost of standard test accuracy. We explain this mechanism explicitly on a linear ``toy model and show in experiments that it still holds for nonlinear ``real-world models.", "keywords": ["batch normalization", "adversarial examples", "robustness"], "paperhash": "galloway|batch_normalization_is_a_cause_of_adversarial_vulnerability", "original_pdf": "/attachment/0d17fb5927362e54d58bec28099a64e132d8b968.pdf", "_bibtex": "@misc{\ngalloway2020batch,\ntitle={Batch Normalization is a Cause of Adversarial Vulnerability},\nauthor={Angus Galloway and Anna Golubeva and Thomas Tanay and Medhat Moussa and Graham W. Taylor},\nyear={2020},\nurl={https://openreview.net/forum?id=H1x-3xSKDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1x-3xSKDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2525/Authors", "ICLR.cc/2020/Conference/Paper2525/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2525/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2525/Reviewers", "ICLR.cc/2020/Conference/Paper2525/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2525/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2525/Authors|ICLR.cc/2020/Conference/Paper2525/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504140068, "tmdate": 1576860534953, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2525/Authors", "ICLR.cc/2020/Conference/Paper2525/Reviewers", "ICLR.cc/2020/Conference/Paper2525/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2525/-/Official_Comment"}}}, {"id": "Sygo0J1ciH", "original": null, "number": 6, "cdate": 1573674963332, "ddate": null, "tcdate": 1573674963332, "tmdate": 1573675213301, "tddate": null, "forum": "H1x-3xSKDr", "replyto": "rJg7I5MztH", "invitation": "ICLR.cc/2020/Conference/Paper2525/-/Official_Comment", "content": {"title": "Response (part 1) - misc clarifications, new experiments on initial learning rate and folding BN statistics into weights", "comment": "We thank the reviewer for their frank comments and pointing us to the work on explaining the effect of using a large initial learning rate. Responding to this review has helped us improve the work, as well as our own understanding. We aim to satisfy the reviewer\u2019s concerns with a new section titled \u201cOn The Initial Learning Rate\u201d in which we evaluate robustness on CIFAR-10 during training under various initial learning rates and schedules. In-line responses/clarifications to the review follow:\n\n--\n\n\"The authors demonstrate their results on SVHN, CIFAR-10, CIFAR-100 CIFAR-10.1\"\n\nTo clarify, we evaluated on CIFAR-{10, 10.1, 10-C}, but not on CIFAR-100. We also evaluated pre-trained models on ImageNet, as well as the Adversarial Spheres dataset (Gilmer et al., ICLR Workshop 2018) (albeit in an Appendix).\n\n--\n\n\"I do not know how much of the changes in adversarial vulnerability are due to batch normalization as opposed to other facets of the training procedure that may have changed in their BN vs no-BN experiments.\"\n\nNo aspect of the training procedure was changed for BN vs no-BN, unless explicitly stated otherwise, e.g., to allow BN a larger initial learning rate, see next bullet.\n\n--\n\n\u201cbatch normalization usually accommodates higher learning rates and it is not clear if the authors adjusted the initial learning rate\u201d \n\nQuoting from the paper at the time of submission: \n(In ref Table 4): \u201cIt has been suggested that one of the benefits of BN is that it facilitates training with a larger learning rate (Ioffe & Szegedy, 2015; Bjorck et al., 2018). We test this from a robustness perspective in an experiment summarized in Table 4, where the initial learning rate is increased to 0.1 when BN is used.\u201d\n\nTo prevent this from being missed and improve the clarity of our work, we can summarize the scenarios considered at the beginning of Section 4, e.g., \u201cCase 1) both models get small LR\u201d, \u201cCase 2) BN gets high initial LR\u201d. If the reviewer previously read this and still believes it to be unclear, please let us know.\n\nNote that we also provide a counterexample to the conventional wisdom that BN allows a higher learning rate in Appendix G on the Adversarial Spheres dataset.\n\n--\n\n\"By swapping in batch normalization, the authors may just be altering the norm of the weight change in the (re-parameterized) weights. In this scenario, the gains of removing batch normalization could just as well be explained by the effective change in the learning rate, and not about batch normalization itself, c.f. (Li et al., 2019).\"\n\nAs indicated by the two cases (on the synthetic and MNIST dataset) where we compute the boundary tilting angle, BN affects not only the norm (invariance) of the weights, but also the angle. Consider the first thing that happens when we feed forward a batch of inputs after random initialization: the weights are rescaled by the inverse of the standard deviation (and numerical stability const) along each dimension. Thus, the weights corresponding to low variance features increase in value, while those corresponding to high variance features shrink, and the resulting batch-normalized weight vectors can be nearly orthogonal to those without BN. Thus, the angle is a critical difference between batch-normalized and unnormalized weights. Although we motivated and explicitly characterized this for linear models, Section 6 of Labatie, (ICML 2019) shows that deep batch-normalized networks accordingly suffer from increased sensitivity w.r.t. the input as a result, and similarly remark: \"[under BN] directions of high signal variance are dampened, while directions of low signal variance are amplified. This preferential exploration of low signal directions naturally deteriorates the signal-to-noise ratio and amplifies $\\chi^l$\". Where $\\chi^l$ is defined as the normalized sensitivity from layer 0 to $l$, such that $\\chi^l > 1$ degrades the signal-to-noise ratio. \n\nWe found Li et al., 2019 and the learning order concept interesting. We agree that in the context of this work, it is imperative to consider the case where a higher initial learning rate is used with BN given that it does usually facilitate this. Please see Table 4, and the new section \u201cOn The Initial Learning Rate\u201d regarding this point which shows that over the course of 150 epochs of training using several learning rate schedules, BN obtains at most 40% accuracy to 40-step PGD while the baseline exceeds 50% accuracy on the same.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2525/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2525/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["gallowaa@uoguelph.ca", "agolubeva@perimeterinstitute.ca", "thomas.tanay.13@ucl.ac.uk", "mmoussa@uoguelph.ca", "gwtaylor@uoguelph.ca"], "title": "Batch Normalization is a Cause of Adversarial Vulnerability", "authors": ["Angus Galloway", "Anna Golubeva", "Thomas Tanay", "Medhat Moussa", "Graham W. Taylor"], "pdf": "/pdf/b4194c0ee567812e4a29e27d0deebb65dc0242e3.pdf", "TL;DR": "Batch normalization reduces robustness at test-time to common corruptions and adversarial examples.", "abstract": "Batch normalization (BN) is often used in an attempt to stabilize and accelerate training in deep neural networks. In many cases it indeed decreases the number of parameter updates required to achieve low training error. However, it also reduces robustness to small adversarial input perturbations and common corruptions by double-digit percentages, as we show on five standard datasets. Furthermore, we find that substituting weight decay for BN is sufficient to nullify a relationship between adversarial vulnerability and the input dimension. A recent mean-field analysis found that BN induces gradient explosion when used on multiple layers, but this cannot fully explain the vulnerability we observe, given that it occurs already for a single BN layer. We argue that the actual cause is the tilting of the decision boundary with respect to the nearest-centroid classifier along input dimensions of low variance. As a result, the constant introduced for numerical stability in the BN step acts as an important hyperparameter that can be tuned to recover some robustness at the cost of standard test accuracy. We explain this mechanism explicitly on a linear ``toy model and show in experiments that it still holds for nonlinear ``real-world models.", "keywords": ["batch normalization", "adversarial examples", "robustness"], "paperhash": "galloway|batch_normalization_is_a_cause_of_adversarial_vulnerability", "original_pdf": "/attachment/0d17fb5927362e54d58bec28099a64e132d8b968.pdf", "_bibtex": "@misc{\ngalloway2020batch,\ntitle={Batch Normalization is a Cause of Adversarial Vulnerability},\nauthor={Angus Galloway and Anna Golubeva and Thomas Tanay and Medhat Moussa and Graham W. Taylor},\nyear={2020},\nurl={https://openreview.net/forum?id=H1x-3xSKDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1x-3xSKDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2525/Authors", "ICLR.cc/2020/Conference/Paper2525/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2525/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2525/Reviewers", "ICLR.cc/2020/Conference/Paper2525/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2525/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2525/Authors|ICLR.cc/2020/Conference/Paper2525/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504140068, "tmdate": 1576860534953, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2525/Authors", "ICLR.cc/2020/Conference/Paper2525/Reviewers", "ICLR.cc/2020/Conference/Paper2525/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2525/-/Official_Comment"}}}, {"id": "rkx6F6AFoS", "original": null, "number": 5, "cdate": 1573674372879, "ddate": null, "tcdate": 1573674372879, "tmdate": 1573674402468, "tddate": null, "forum": "H1x-3xSKDr", "replyto": "SJeMYKKyiS", "invitation": "ICLR.cc/2020/Conference/Paper2525/-/Official_Comment", "content": {"title": "References", "comment": "References \n\nHuan Xu, Constantine Caramanis, and Shie Mannor. Robustness and Regularization of Support VectorMachines. In Journal of Machine Learning Research, 10:1485\u20131510, 2009\n\nAnh Nguyen, Jason Yosinki, Jeff Clune. Deep Neural Networks are Easily Fooled: High Confidence Predictions for Unrecognizable Images. In IEEE Conference on Computer Vision and Pattern Recognition, pp. 427-436, 2015.\n\nYusuke Tsuzuku, Issei Sato, and Masashi Sugiyama. Lipschitz-Margin Training: Scalable Certification of Perturbation Invariance for Deep Neural Networks. In Advances in Neural Information Processing Systems 31, pp. 6541-6650, 2018."}, "signatures": ["ICLR.cc/2020/Conference/Paper2525/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2525/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["gallowaa@uoguelph.ca", "agolubeva@perimeterinstitute.ca", "thomas.tanay.13@ucl.ac.uk", "mmoussa@uoguelph.ca", "gwtaylor@uoguelph.ca"], "title": "Batch Normalization is a Cause of Adversarial Vulnerability", "authors": ["Angus Galloway", "Anna Golubeva", "Thomas Tanay", "Medhat Moussa", "Graham W. Taylor"], "pdf": "/pdf/b4194c0ee567812e4a29e27d0deebb65dc0242e3.pdf", "TL;DR": "Batch normalization reduces robustness at test-time to common corruptions and adversarial examples.", "abstract": "Batch normalization (BN) is often used in an attempt to stabilize and accelerate training in deep neural networks. In many cases it indeed decreases the number of parameter updates required to achieve low training error. However, it also reduces robustness to small adversarial input perturbations and common corruptions by double-digit percentages, as we show on five standard datasets. Furthermore, we find that substituting weight decay for BN is sufficient to nullify a relationship between adversarial vulnerability and the input dimension. A recent mean-field analysis found that BN induces gradient explosion when used on multiple layers, but this cannot fully explain the vulnerability we observe, given that it occurs already for a single BN layer. We argue that the actual cause is the tilting of the decision boundary with respect to the nearest-centroid classifier along input dimensions of low variance. As a result, the constant introduced for numerical stability in the BN step acts as an important hyperparameter that can be tuned to recover some robustness at the cost of standard test accuracy. We explain this mechanism explicitly on a linear ``toy model and show in experiments that it still holds for nonlinear ``real-world models.", "keywords": ["batch normalization", "adversarial examples", "robustness"], "paperhash": "galloway|batch_normalization_is_a_cause_of_adversarial_vulnerability", "original_pdf": "/attachment/0d17fb5927362e54d58bec28099a64e132d8b968.pdf", "_bibtex": "@misc{\ngalloway2020batch,\ntitle={Batch Normalization is a Cause of Adversarial Vulnerability},\nauthor={Angus Galloway and Anna Golubeva and Thomas Tanay and Medhat Moussa and Graham W. Taylor},\nyear={2020},\nurl={https://openreview.net/forum?id=H1x-3xSKDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1x-3xSKDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2525/Authors", "ICLR.cc/2020/Conference/Paper2525/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2525/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2525/Reviewers", "ICLR.cc/2020/Conference/Paper2525/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2525/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2525/Authors|ICLR.cc/2020/Conference/Paper2525/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504140068, "tmdate": 1576860534953, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2525/Authors", "ICLR.cc/2020/Conference/Paper2525/Reviewers", "ICLR.cc/2020/Conference/Paper2525/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2525/-/Official_Comment"}}}, {"id": "rJg7I5MztH", "original": null, "number": 2, "cdate": 1571068491488, "ddate": null, "tcdate": 1571068491488, "tmdate": 1572972327181, "tddate": null, "forum": "H1x-3xSKDr", "replyto": "H1x-3xSKDr", "invitation": "ICLR.cc/2020/Conference/Paper2525/-/Official_Review", "content": {"rating": "1: Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "Summary:\nIn this empirical study, the authors identify that batch normalization -- a common technique for accelerating training -- leads to brittle representations that exhibit a lack of robustness and are more susceptible to adversarial attacks. The authors demonstrate their results on SVHN, CIFAR-10, CIFAR-100 CIFAR-10.1 using a variety of network architectures including VGG, BagNet, WideResNet, AlexNet, etc.\n\nMajor Concerns:\n\n1. As presented, the experiments are not convincing.\n\nI do not know how much of the changes in adversarial vulnerability are due to batch normalization as opposed to other facets of the training procedure that may have changed in their BN vs no-BN experiments.\n\nFor instance, batch normalization usually accommodates higher learning rates and it is not clear if the authors adjusted the initial learning rate, learning rate schedule or training schedule accordingly. If so, it would be important to run a set of experiments with these parameters fixed as per the baseline no-BN models. \n\nThat said, even if the authors did run these experiments, it is still not clear if the cause of adversarial vulnerability is due to BN. Consider that what is truly important in model training is not the learning rate (i.e. step size), but rather the magnitude of the changes in each weight (or the ratio of weight change to the weight). By swapping in batch normalization, the authors may just be altering the norm of the weight change in the (re-parameterized) weights. In this scenario, the gains of removing batch normalization could just as well be explained by the effective change in the learning rate, and not about batch normalization itself, c.f.\n  Towards Explaining the Regularization Effect of Initial Large Learning Rate in Training Neural Networks\n  Yuanzhi Li, Colin Wei, Tengyu Ma\n  https://arxiv.org/abs/1907.04595\nIf the differences in the adversarial vulnerability could be ascribed to effective changes in gradient updates, then this would change the interpretation of these results notably.\n\n2. The underlying hypothesis is specious.\n\nI have several reservations about the underlying hypothesis that requires stronger evidence to overcome. In particular, I have reservations in believing that BN itself is a cause of adversarial vulnerability because BN is just a factorization of a network's weights. That is, there is nothing \"special\" nor unique about BN-networks; instead, the BN factorization merely permits accelerated training efficiency.\n\nConsider the fact that a BN model may be re-expressed by merely folding in the parameters (i.e. applying the matrix multiplications) into the MLP weights or CNN filters. Thus, the numerical function approximated by the BN and the \"folded\" non-BN model is identical. What would it mean to say that the BN is \"causing\" adversarial vulnerability in the BN model given that both the BN and non-BN model perform the identical function?\n\nAnother way to say this is to pretend we train a non-BN MLP or CNN model. After training the model, we could apply a BN factorization of the weights. Thus, the non-BN model may be factorized into a BN model. If the resulting BN model were adversarial vulnerable (which I suspect is the case), it would seem very hard to believe that BN was the cause of the vulnerability given it was a post-hoc factorization of the weights.\n\nThat said, I could definitely imagine that the training procedure itself could lead to adversarial vulnerability (e.g. citation above) and by employing a BN factorization, one may be encouraged to use a training procedure which leads to increased vulnerability. I would encourage the authors to consider this line of attack and thus, re-orient their analysis and discussion accordingly.\n\n3. The title is poorly worded.\n\nNot withstanding the point above, adversarial vulnerability predates BN. Likewise, non-BN models exhibit adversarial vulnerability. Thus, this title is not a great reflection of the findings of the paper. I would strongly suggest replacing \"is a cause\" with \"increases\" or \"exacerbates\"."}, "signatures": ["ICLR.cc/2020/Conference/Paper2525/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2525/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["gallowaa@uoguelph.ca", "agolubeva@perimeterinstitute.ca", "thomas.tanay.13@ucl.ac.uk", "mmoussa@uoguelph.ca", "gwtaylor@uoguelph.ca"], "title": "Batch Normalization is a Cause of Adversarial Vulnerability", "authors": ["Angus Galloway", "Anna Golubeva", "Thomas Tanay", "Medhat Moussa", "Graham W. Taylor"], "pdf": "/pdf/b4194c0ee567812e4a29e27d0deebb65dc0242e3.pdf", "TL;DR": "Batch normalization reduces robustness at test-time to common corruptions and adversarial examples.", "abstract": "Batch normalization (BN) is often used in an attempt to stabilize and accelerate training in deep neural networks. In many cases it indeed decreases the number of parameter updates required to achieve low training error. However, it also reduces robustness to small adversarial input perturbations and common corruptions by double-digit percentages, as we show on five standard datasets. Furthermore, we find that substituting weight decay for BN is sufficient to nullify a relationship between adversarial vulnerability and the input dimension. A recent mean-field analysis found that BN induces gradient explosion when used on multiple layers, but this cannot fully explain the vulnerability we observe, given that it occurs already for a single BN layer. We argue that the actual cause is the tilting of the decision boundary with respect to the nearest-centroid classifier along input dimensions of low variance. As a result, the constant introduced for numerical stability in the BN step acts as an important hyperparameter that can be tuned to recover some robustness at the cost of standard test accuracy. We explain this mechanism explicitly on a linear ``toy model and show in experiments that it still holds for nonlinear ``real-world models.", "keywords": ["batch normalization", "adversarial examples", "robustness"], "paperhash": "galloway|batch_normalization_is_a_cause_of_adversarial_vulnerability", "original_pdf": "/attachment/0d17fb5927362e54d58bec28099a64e132d8b968.pdf", "_bibtex": "@misc{\ngalloway2020batch,\ntitle={Batch Normalization is a Cause of Adversarial Vulnerability},\nauthor={Angus Galloway and Anna Golubeva and Thomas Tanay and Medhat Moussa and Graham W. Taylor},\nyear={2020},\nurl={https://openreview.net/forum?id=H1x-3xSKDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "H1x-3xSKDr", "replyto": "H1x-3xSKDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2525/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2525/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575842317395, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2525/Reviewers"], "noninvitees": [], "tcdate": 1570237721620, "tmdate": 1575842317409, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2525/-/Official_Review"}}}, {"id": "S1lNkzJ2YB", "original": null, "number": 3, "cdate": 1571709403655, "ddate": null, "tcdate": 1571709403655, "tmdate": 1572972327139, "tddate": null, "forum": "H1x-3xSKDr", "replyto": "H1x-3xSKDr", "invitation": "ICLR.cc/2020/Conference/Paper2525/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "Overview:\nThis is an interesting work. The paper is dedicated to studying the effect of BN to network robustness. The author shows that BN can reduce network robustness to small adversarial input perturbations and common corruptions by double-digit percentages. Then, they use a linear \"toy model\" to explain the mechanism that the actual cause is the tilting of the decision boundary. Moreover, the author conducts extensive experiments on popular datasets to show the robustness margin with or without the BN module. Finally, the author finds that substituting weight decay for BN is good enough to nullify a relationship between adversarial vulnerability and the input resolution.\n\nStrength Bullets:\n1. I like the linear toy example. For that binary classification example, the author explicitly explains the boundary tilting, which increases the adversarial vulnerability of the model. It is clear.\n2. The paper conducts extensive experiment on SVHN, MNIST, CIFAR10 (C) datasets. And they show performance margin with or without the BN module. And for the attacker setting, they do use the popular setting (i.e. Mardy's PGD setting) in this field which makes the results more convincing.\n\nWeakness Bullets:\n1. Why do not visualize the decision boundary of networks (used in this work) to valid the boundary tilting \"theory\". The toy example is clear but not convincing enough. There exist several techniques may be helpful to the visualization. (i.e. Robustness via curvature regularization, and vice versa). I think it is one of the important parts of this work. The observation of BN causes adversarial vulnerability is interesting but the main focus should be offering more convincing explanations.\n2. I do run experiments for VGG11,13,16,19 on cifar10 with PGD 3 attack (Mardy's setting). There exist ~20 ATA performance gaps between networks with BN and BN networks without BN. But for adversarial trained models, the gaps don't exist anymore, at least for VGG11,13,16,19 on cifar10 with PGD 3 attack. (The performance gap is less than 0.5). In other words, without the BN layers, the robustness of adversarially trained models will not increase in my experiments. I see you report some results in Appendix C. But it is not enough to convince me. Could you provide more implementation details about the adversarial training and attacker? And more experiment results about this point are needed. If adversarial training can fix the vulnerability by BN and BN can give a TA boost, there is no reason we need to remove BN in our adversarial training setting. I see there are similar concerns in the OpenReview.\n3. [Minior] The experiments need to be organized better. Especially for section 3, it will be better to divide different experiments or observations into the different subsections.\n\n\nRecommendations:\nFor the above weakness bullets, this is a week reject.\n\nSuggestions:\n1. To solve the weakness bullets;\n2. minor suggestion: add the reference mention in the OpenReview, they are related to this work.\n\nQuestions:\n1. You mention that you run PGD for 20-40 iterations in the experiment at the bottom of page three. But at each table, you only report one number. So my question is for that accuracy number, you run how many iterations for PGD?"}, "signatures": ["ICLR.cc/2020/Conference/Paper2525/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2525/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["gallowaa@uoguelph.ca", "agolubeva@perimeterinstitute.ca", "thomas.tanay.13@ucl.ac.uk", "mmoussa@uoguelph.ca", "gwtaylor@uoguelph.ca"], "title": "Batch Normalization is a Cause of Adversarial Vulnerability", "authors": ["Angus Galloway", "Anna Golubeva", "Thomas Tanay", "Medhat Moussa", "Graham W. Taylor"], "pdf": "/pdf/b4194c0ee567812e4a29e27d0deebb65dc0242e3.pdf", "TL;DR": "Batch normalization reduces robustness at test-time to common corruptions and adversarial examples.", "abstract": "Batch normalization (BN) is often used in an attempt to stabilize and accelerate training in deep neural networks. In many cases it indeed decreases the number of parameter updates required to achieve low training error. However, it also reduces robustness to small adversarial input perturbations and common corruptions by double-digit percentages, as we show on five standard datasets. Furthermore, we find that substituting weight decay for BN is sufficient to nullify a relationship between adversarial vulnerability and the input dimension. A recent mean-field analysis found that BN induces gradient explosion when used on multiple layers, but this cannot fully explain the vulnerability we observe, given that it occurs already for a single BN layer. We argue that the actual cause is the tilting of the decision boundary with respect to the nearest-centroid classifier along input dimensions of low variance. As a result, the constant introduced for numerical stability in the BN step acts as an important hyperparameter that can be tuned to recover some robustness at the cost of standard test accuracy. We explain this mechanism explicitly on a linear ``toy model and show in experiments that it still holds for nonlinear ``real-world models.", "keywords": ["batch normalization", "adversarial examples", "robustness"], "paperhash": "galloway|batch_normalization_is_a_cause_of_adversarial_vulnerability", "original_pdf": "/attachment/0d17fb5927362e54d58bec28099a64e132d8b968.pdf", "_bibtex": "@misc{\ngalloway2020batch,\ntitle={Batch Normalization is a Cause of Adversarial Vulnerability},\nauthor={Angus Galloway and Anna Golubeva and Thomas Tanay and Medhat Moussa and Graham W. Taylor},\nyear={2020},\nurl={https://openreview.net/forum?id=H1x-3xSKDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "H1x-3xSKDr", "replyto": "H1x-3xSKDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2525/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2525/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575842317395, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2525/Reviewers"], "noninvitees": [], "tcdate": 1570237721620, "tmdate": 1575842317409, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2525/-/Official_Review"}}}, {"id": "HkxOtjiG5r", "original": null, "number": 5, "cdate": 1572154240346, "ddate": null, "tcdate": 1572154240346, "tmdate": 1572154240346, "tddate": null, "forum": "H1x-3xSKDr", "replyto": "H1x-3xSKDr", "invitation": "ICLR.cc/2020/Conference/Paper2525/-/Public_Comment", "content": {"title": "The number of batchnorm layers applied", "comment": "Really enjoyed reading your paper, inspired a lot! One question though.\n\nThere are models employed not only one but more layers of batch normalizations, have you considered making some evaluation on the relationship between the number of BNs used and the degradation (if any) seen in the same setting as you have demonstrated in the paper?"}, "signatures": ["~Rui_Wang1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Rui_Wang1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["gallowaa@uoguelph.ca", "agolubeva@perimeterinstitute.ca", "thomas.tanay.13@ucl.ac.uk", "mmoussa@uoguelph.ca", "gwtaylor@uoguelph.ca"], "title": "Batch Normalization is a Cause of Adversarial Vulnerability", "authors": ["Angus Galloway", "Anna Golubeva", "Thomas Tanay", "Medhat Moussa", "Graham W. Taylor"], "pdf": "/pdf/b4194c0ee567812e4a29e27d0deebb65dc0242e3.pdf", "TL;DR": "Batch normalization reduces robustness at test-time to common corruptions and adversarial examples.", "abstract": "Batch normalization (BN) is often used in an attempt to stabilize and accelerate training in deep neural networks. In many cases it indeed decreases the number of parameter updates required to achieve low training error. However, it also reduces robustness to small adversarial input perturbations and common corruptions by double-digit percentages, as we show on five standard datasets. Furthermore, we find that substituting weight decay for BN is sufficient to nullify a relationship between adversarial vulnerability and the input dimension. A recent mean-field analysis found that BN induces gradient explosion when used on multiple layers, but this cannot fully explain the vulnerability we observe, given that it occurs already for a single BN layer. We argue that the actual cause is the tilting of the decision boundary with respect to the nearest-centroid classifier along input dimensions of low variance. As a result, the constant introduced for numerical stability in the BN step acts as an important hyperparameter that can be tuned to recover some robustness at the cost of standard test accuracy. We explain this mechanism explicitly on a linear ``toy model and show in experiments that it still holds for nonlinear ``real-world models.", "keywords": ["batch normalization", "adversarial examples", "robustness"], "paperhash": "galloway|batch_normalization_is_a_cause_of_adversarial_vulnerability", "original_pdf": "/attachment/0d17fb5927362e54d58bec28099a64e132d8b968.pdf", "_bibtex": "@misc{\ngalloway2020batch,\ntitle={Batch Normalization is a Cause of Adversarial Vulnerability},\nauthor={Angus Galloway and Anna Golubeva and Thomas Tanay and Medhat Moussa and Graham W. Taylor},\nyear={2020},\nurl={https://openreview.net/forum?id=H1x-3xSKDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1x-3xSKDr", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504179044, "tmdate": 1576860568668, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper2525/Authors", "ICLR.cc/2020/Conference/Paper2525/Reviewers", "ICLR.cc/2020/Conference/Paper2525/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2525/-/Public_Comment"}}}, {"id": "HygzLHdjuS", "original": null, "number": 3, "cdate": 1570633034141, "ddate": null, "tcdate": 1570633034141, "tmdate": 1570633034141, "tddate": null, "forum": "H1x-3xSKDr", "replyto": "rygmCcRcOH", "invitation": "ICLR.cc/2020/Conference/Paper2525/-/Official_Comment", "content": {"comment": "Thanks for catching this, VGG8 is indeed a non-standard architecture. To reduce capacity, we remove layers from VGG11 such that there is only one convolution layer between each 2x2 max pooling (indicated 'M').\n\nVGG8:   [64, 'M', 128, 'M', 256, 'M', 512, 'M', 512, 'M']\nVGG11: [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M']", "title": "Architecture"}, "signatures": ["ICLR.cc/2020/Conference/Paper2525/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2525/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["gallowaa@uoguelph.ca", "agolubeva@perimeterinstitute.ca", "thomas.tanay.13@ucl.ac.uk", "mmoussa@uoguelph.ca", "gwtaylor@uoguelph.ca"], "title": "Batch Normalization is a Cause of Adversarial Vulnerability", "authors": ["Angus Galloway", "Anna Golubeva", "Thomas Tanay", "Medhat Moussa", "Graham W. Taylor"], "pdf": "/pdf/b4194c0ee567812e4a29e27d0deebb65dc0242e3.pdf", "TL;DR": "Batch normalization reduces robustness at test-time to common corruptions and adversarial examples.", "abstract": "Batch normalization (BN) is often used in an attempt to stabilize and accelerate training in deep neural networks. In many cases it indeed decreases the number of parameter updates required to achieve low training error. However, it also reduces robustness to small adversarial input perturbations and common corruptions by double-digit percentages, as we show on five standard datasets. Furthermore, we find that substituting weight decay for BN is sufficient to nullify a relationship between adversarial vulnerability and the input dimension. A recent mean-field analysis found that BN induces gradient explosion when used on multiple layers, but this cannot fully explain the vulnerability we observe, given that it occurs already for a single BN layer. We argue that the actual cause is the tilting of the decision boundary with respect to the nearest-centroid classifier along input dimensions of low variance. As a result, the constant introduced for numerical stability in the BN step acts as an important hyperparameter that can be tuned to recover some robustness at the cost of standard test accuracy. We explain this mechanism explicitly on a linear ``toy model and show in experiments that it still holds for nonlinear ``real-world models.", "keywords": ["batch normalization", "adversarial examples", "robustness"], "paperhash": "galloway|batch_normalization_is_a_cause_of_adversarial_vulnerability", "original_pdf": "/attachment/0d17fb5927362e54d58bec28099a64e132d8b968.pdf", "_bibtex": "@misc{\ngalloway2020batch,\ntitle={Batch Normalization is a Cause of Adversarial Vulnerability},\nauthor={Angus Galloway and Anna Golubeva and Thomas Tanay and Medhat Moussa and Graham W. Taylor},\nyear={2020},\nurl={https://openreview.net/forum?id=H1x-3xSKDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1x-3xSKDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2525/Authors", "ICLR.cc/2020/Conference/Paper2525/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2525/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2525/Reviewers", "ICLR.cc/2020/Conference/Paper2525/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2525/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2525/Authors|ICLR.cc/2020/Conference/Paper2525/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504140068, "tmdate": 1576860534953, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2525/Authors", "ICLR.cc/2020/Conference/Paper2525/Reviewers", "ICLR.cc/2020/Conference/Paper2525/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2525/-/Official_Comment"}}}, {"id": "rygmCcRcOH", "original": null, "number": 4, "cdate": 1570593482935, "ddate": null, "tcdate": 1570593482935, "tmdate": 1570593530748, "tddate": null, "forum": "H1x-3xSKDr", "replyto": "H1x-3xSKDr", "invitation": "ICLR.cc/2020/Conference/Paper2525/-/Public_Comment", "content": {"comment": "What is VGG8? \n\nI did not find any clue about the VGG version with 8 weight layers in the paper of VGG.\n", "title": "Minor question"}, "signatures": ["~Anthony_Wittmer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Anthony_Wittmer1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["gallowaa@uoguelph.ca", "agolubeva@perimeterinstitute.ca", "thomas.tanay.13@ucl.ac.uk", "mmoussa@uoguelph.ca", "gwtaylor@uoguelph.ca"], "title": "Batch Normalization is a Cause of Adversarial Vulnerability", "authors": ["Angus Galloway", "Anna Golubeva", "Thomas Tanay", "Medhat Moussa", "Graham W. Taylor"], "pdf": "/pdf/b4194c0ee567812e4a29e27d0deebb65dc0242e3.pdf", "TL;DR": "Batch normalization reduces robustness at test-time to common corruptions and adversarial examples.", "abstract": "Batch normalization (BN) is often used in an attempt to stabilize and accelerate training in deep neural networks. In many cases it indeed decreases the number of parameter updates required to achieve low training error. However, it also reduces robustness to small adversarial input perturbations and common corruptions by double-digit percentages, as we show on five standard datasets. Furthermore, we find that substituting weight decay for BN is sufficient to nullify a relationship between adversarial vulnerability and the input dimension. A recent mean-field analysis found that BN induces gradient explosion when used on multiple layers, but this cannot fully explain the vulnerability we observe, given that it occurs already for a single BN layer. We argue that the actual cause is the tilting of the decision boundary with respect to the nearest-centroid classifier along input dimensions of low variance. As a result, the constant introduced for numerical stability in the BN step acts as an important hyperparameter that can be tuned to recover some robustness at the cost of standard test accuracy. We explain this mechanism explicitly on a linear ``toy model and show in experiments that it still holds for nonlinear ``real-world models.", "keywords": ["batch normalization", "adversarial examples", "robustness"], "paperhash": "galloway|batch_normalization_is_a_cause_of_adversarial_vulnerability", "original_pdf": "/attachment/0d17fb5927362e54d58bec28099a64e132d8b968.pdf", "_bibtex": "@misc{\ngalloway2020batch,\ntitle={Batch Normalization is a Cause of Adversarial Vulnerability},\nauthor={Angus Galloway and Anna Golubeva and Thomas Tanay and Medhat Moussa and Graham W. Taylor},\nyear={2020},\nurl={https://openreview.net/forum?id=H1x-3xSKDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1x-3xSKDr", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504179044, "tmdate": 1576860568668, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper2525/Authors", "ICLR.cc/2020/Conference/Paper2525/Reviewers", "ICLR.cc/2020/Conference/Paper2525/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2525/-/Public_Comment"}}}, {"id": "BylPxYLmuH", "original": null, "number": 3, "cdate": 1570101486663, "ddate": null, "tcdate": 1570101486663, "tmdate": 1570101486663, "tddate": null, "forum": "H1x-3xSKDr", "replyto": "rkloiiNGuH", "invitation": "ICLR.cc/2020/Conference/Paper2525/-/Public_Comment", "content": {"comment": "I also believe that the discussion from the ICML 2019 paper on the effect of BN is very relevant. Thank you very much for your quick response.\u00a0", "title": "Thank you for your response"}, "signatures": ["~Antoine_Labatie1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Antoine_Labatie1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["gallowaa@uoguelph.ca", "agolubeva@perimeterinstitute.ca", "thomas.tanay.13@ucl.ac.uk", "mmoussa@uoguelph.ca", "gwtaylor@uoguelph.ca"], "title": "Batch Normalization is a Cause of Adversarial Vulnerability", "authors": ["Angus Galloway", "Anna Golubeva", "Thomas Tanay", "Medhat Moussa", "Graham W. Taylor"], "pdf": "/pdf/b4194c0ee567812e4a29e27d0deebb65dc0242e3.pdf", "TL;DR": "Batch normalization reduces robustness at test-time to common corruptions and adversarial examples.", "abstract": "Batch normalization (BN) is often used in an attempt to stabilize and accelerate training in deep neural networks. In many cases it indeed decreases the number of parameter updates required to achieve low training error. However, it also reduces robustness to small adversarial input perturbations and common corruptions by double-digit percentages, as we show on five standard datasets. Furthermore, we find that substituting weight decay for BN is sufficient to nullify a relationship between adversarial vulnerability and the input dimension. A recent mean-field analysis found that BN induces gradient explosion when used on multiple layers, but this cannot fully explain the vulnerability we observe, given that it occurs already for a single BN layer. We argue that the actual cause is the tilting of the decision boundary with respect to the nearest-centroid classifier along input dimensions of low variance. As a result, the constant introduced for numerical stability in the BN step acts as an important hyperparameter that can be tuned to recover some robustness at the cost of standard test accuracy. We explain this mechanism explicitly on a linear ``toy model and show in experiments that it still holds for nonlinear ``real-world models.", "keywords": ["batch normalization", "adversarial examples", "robustness"], "paperhash": "galloway|batch_normalization_is_a_cause_of_adversarial_vulnerability", "original_pdf": "/attachment/0d17fb5927362e54d58bec28099a64e132d8b968.pdf", "_bibtex": "@misc{\ngalloway2020batch,\ntitle={Batch Normalization is a Cause of Adversarial Vulnerability},\nauthor={Angus Galloway and Anna Golubeva and Thomas Tanay and Medhat Moussa and Graham W. Taylor},\nyear={2020},\nurl={https://openreview.net/forum?id=H1x-3xSKDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1x-3xSKDr", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504179044, "tmdate": 1576860568668, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper2525/Authors", "ICLR.cc/2020/Conference/Paper2525/Reviewers", "ICLR.cc/2020/Conference/Paper2525/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2525/-/Public_Comment"}}}, {"id": "rkloiiNGuH", "original": null, "number": 2, "cdate": 1570028451026, "ddate": null, "tcdate": 1570028451026, "tmdate": 1570028451026, "tddate": null, "forum": "H1x-3xSKDr", "replyto": "SygA8UybOr", "invitation": "ICLR.cc/2020/Conference/Paper2525/-/Official_Comment", "content": {"comment": "Yes, your interpretation seems valid, although we do not exactly make this argument in the text. This is worth formalizing and could make our study that considers the input dimension and vulnerability more precise for the case with BN.\n\nWe're sorry to have missed the reference you mention from ICML 2019. On first pass, it is indeed highly relevant to our discussion in Section 3, particularly how BN reduces signal-to-noise in Section 6, where you mention \"directions of high signal variance are dampened, while directions of low signal variance are amplified\". We're looking forward to discussing theses connections in our next revision.", "title": "Thank you for pointing us to this work"}, "signatures": ["ICLR.cc/2020/Conference/Paper2525/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2525/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["gallowaa@uoguelph.ca", "agolubeva@perimeterinstitute.ca", "thomas.tanay.13@ucl.ac.uk", "mmoussa@uoguelph.ca", "gwtaylor@uoguelph.ca"], "title": "Batch Normalization is a Cause of Adversarial Vulnerability", "authors": ["Angus Galloway", "Anna Golubeva", "Thomas Tanay", "Medhat Moussa", "Graham W. Taylor"], "pdf": "/pdf/b4194c0ee567812e4a29e27d0deebb65dc0242e3.pdf", "TL;DR": "Batch normalization reduces robustness at test-time to common corruptions and adversarial examples.", "abstract": "Batch normalization (BN) is often used in an attempt to stabilize and accelerate training in deep neural networks. In many cases it indeed decreases the number of parameter updates required to achieve low training error. However, it also reduces robustness to small adversarial input perturbations and common corruptions by double-digit percentages, as we show on five standard datasets. Furthermore, we find that substituting weight decay for BN is sufficient to nullify a relationship between adversarial vulnerability and the input dimension. A recent mean-field analysis found that BN induces gradient explosion when used on multiple layers, but this cannot fully explain the vulnerability we observe, given that it occurs already for a single BN layer. We argue that the actual cause is the tilting of the decision boundary with respect to the nearest-centroid classifier along input dimensions of low variance. As a result, the constant introduced for numerical stability in the BN step acts as an important hyperparameter that can be tuned to recover some robustness at the cost of standard test accuracy. We explain this mechanism explicitly on a linear ``toy model and show in experiments that it still holds for nonlinear ``real-world models.", "keywords": ["batch normalization", "adversarial examples", "robustness"], "paperhash": "galloway|batch_normalization_is_a_cause_of_adversarial_vulnerability", "original_pdf": "/attachment/0d17fb5927362e54d58bec28099a64e132d8b968.pdf", "_bibtex": "@misc{\ngalloway2020batch,\ntitle={Batch Normalization is a Cause of Adversarial Vulnerability},\nauthor={Angus Galloway and Anna Golubeva and Thomas Tanay and Medhat Moussa and Graham W. Taylor},\nyear={2020},\nurl={https://openreview.net/forum?id=H1x-3xSKDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1x-3xSKDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2525/Authors", "ICLR.cc/2020/Conference/Paper2525/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2525/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2525/Reviewers", "ICLR.cc/2020/Conference/Paper2525/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2525/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2525/Authors|ICLR.cc/2020/Conference/Paper2525/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504140068, "tmdate": 1576860534953, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2525/Authors", "ICLR.cc/2020/Conference/Paper2525/Reviewers", "ICLR.cc/2020/Conference/Paper2525/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2525/-/Official_Comment"}}}, {"id": "HkeHazWxdr", "original": null, "number": 1, "cdate": 1569882813262, "ddate": null, "tcdate": 1569882813262, "tmdate": 1569941114280, "tddate": null, "forum": "H1x-3xSKDr", "replyto": "SklNaNu2vr", "invitation": "ICLR.cc/2020/Conference/Paper2525/-/Official_Comment", "content": {"comment": "1. Thanks for pointing us to these relevant concurrent submissions. More discussion follows.\n\n2. Regarding \"Adversarially trained models also have the adversarial vulnerability for strong attack\", I'm sorry but I don't quite understand the connection to our claim about BatchNorm (BN). All models are vulnerable to strong attacks if \"strong\" means large perturbation budget. Our claim is simply that using BN makes models more vulnerable. We report results for PGD trained models on MNIST and CIFAR-10 in Appendix C. We take a broad view of robustness, but considering the limited \"max-norm\" attack model we observe a non-trivial improvement in robustness using Fixup in place of BN for the same WideResNet as in Madry et al. One of the concurrent submissions you reference that aims to fix the vulnerability induced by BN reports an even larger 14% gap in terms of PGD test accuracy for PGD trained vanilla ResNets.\n\nAs an aside, I stress that the current form of PGD training with a one-size-fits all epsilon (i.e. the same constant is applied to all examples regardless of their intrinsic noise) can yield excessive invariance that reduces robustness more broadly, see e.g., Jacobsen et al., (2019), Mu & Gilmer, (2019). In the case where epsilon is adapted to each training example, the procedure is equivalent to penalizing the parameters' dual norm for linear models (Xu et al., (2009)). We perform several experiments on the interaction of BN with parameter norm regularization in the text.\n\n3. We believe it would be valuable to study the effect of these other normalization layers on robustness and would be excited if someone else does so. Due to widespread use of BN and space constraints, these are out of scope for the present submission. \n\nWe tested the hypothesis that the vulnerability of BN could arise from using the tracked mean and variance of the training data at test time. This is an insightful observation, indeed not using these statistics at test time does increases robustness, but the same can be achieved through increasing the numerical stability constant (while preserving the ability to test with arbitrary batch sizes). Neither of these mechanisms fully account for the vulnerability, due to decision boundary tilting inherent in the normalization procedure itself. We've added an Appendix to discuss this alternate hypothesis which will become available when updates to all papers are enabled.", "title": "Thank you for the comments"}, "signatures": ["ICLR.cc/2020/Conference/Paper2525/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2525/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["gallowaa@uoguelph.ca", "agolubeva@perimeterinstitute.ca", "thomas.tanay.13@ucl.ac.uk", "mmoussa@uoguelph.ca", "gwtaylor@uoguelph.ca"], "title": "Batch Normalization is a Cause of Adversarial Vulnerability", "authors": ["Angus Galloway", "Anna Golubeva", "Thomas Tanay", "Medhat Moussa", "Graham W. Taylor"], "pdf": "/pdf/b4194c0ee567812e4a29e27d0deebb65dc0242e3.pdf", "TL;DR": "Batch normalization reduces robustness at test-time to common corruptions and adversarial examples.", "abstract": "Batch normalization (BN) is often used in an attempt to stabilize and accelerate training in deep neural networks. In many cases it indeed decreases the number of parameter updates required to achieve low training error. However, it also reduces robustness to small adversarial input perturbations and common corruptions by double-digit percentages, as we show on five standard datasets. Furthermore, we find that substituting weight decay for BN is sufficient to nullify a relationship between adversarial vulnerability and the input dimension. A recent mean-field analysis found that BN induces gradient explosion when used on multiple layers, but this cannot fully explain the vulnerability we observe, given that it occurs already for a single BN layer. We argue that the actual cause is the tilting of the decision boundary with respect to the nearest-centroid classifier along input dimensions of low variance. As a result, the constant introduced for numerical stability in the BN step acts as an important hyperparameter that can be tuned to recover some robustness at the cost of standard test accuracy. We explain this mechanism explicitly on a linear ``toy model and show in experiments that it still holds for nonlinear ``real-world models.", "keywords": ["batch normalization", "adversarial examples", "robustness"], "paperhash": "galloway|batch_normalization_is_a_cause_of_adversarial_vulnerability", "original_pdf": "/attachment/0d17fb5927362e54d58bec28099a64e132d8b968.pdf", "_bibtex": "@misc{\ngalloway2020batch,\ntitle={Batch Normalization is a Cause of Adversarial Vulnerability},\nauthor={Angus Galloway and Anna Golubeva and Thomas Tanay and Medhat Moussa and Graham W. Taylor},\nyear={2020},\nurl={https://openreview.net/forum?id=H1x-3xSKDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1x-3xSKDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2525/Authors", "ICLR.cc/2020/Conference/Paper2525/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2525/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2525/Reviewers", "ICLR.cc/2020/Conference/Paper2525/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2525/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2525/Authors|ICLR.cc/2020/Conference/Paper2525/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504140068, "tmdate": 1576860534953, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2525/Authors", "ICLR.cc/2020/Conference/Paper2525/Reviewers", "ICLR.cc/2020/Conference/Paper2525/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2525/-/Official_Comment"}}}, {"id": "SygA8UybOr", "original": null, "number": 2, "cdate": 1569941078094, "ddate": null, "tcdate": 1569941078094, "tmdate": 1569941078094, "tddate": null, "forum": "H1x-3xSKDr", "replyto": "H1x-3xSKDr", "invitation": "ICLR.cc/2020/Conference/Paper2525/-/Public_Comment", "content": {"comment": "Hi,\n\nI enjoyed reading your paper. I just have two comments.\n\nFirst, the discussion and the experiments of Section 3 are very interesting. Do you think that the following interpretation would be valid:\n- Since unnormalized MNIST images \"live\" in a subspace of small dimensionality, many directions do not play any role in the max-margin solution\n- On the other hand, normalized MNIST images \"live\" in a subspace of larger dimensionality, so that many more dimensions play a role in the\u00a0max-margin solution\n\nSecond, I wanted to mention a closely related paper from ICML 2019 precisely showing that\u00a0batch normalization causes exploding sensitivity to input perturbations at initialization [1].\n\n[1] Characterizing Well-Behaved vs. Pathological Deep Neural Networks. ICML 2019.\n", "title": "Two comments"}, "signatures": ["~Antoine_Labatie1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Antoine_Labatie1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["gallowaa@uoguelph.ca", "agolubeva@perimeterinstitute.ca", "thomas.tanay.13@ucl.ac.uk", "mmoussa@uoguelph.ca", "gwtaylor@uoguelph.ca"], "title": "Batch Normalization is a Cause of Adversarial Vulnerability", "authors": ["Angus Galloway", "Anna Golubeva", "Thomas Tanay", "Medhat Moussa", "Graham W. Taylor"], "pdf": "/pdf/b4194c0ee567812e4a29e27d0deebb65dc0242e3.pdf", "TL;DR": "Batch normalization reduces robustness at test-time to common corruptions and adversarial examples.", "abstract": "Batch normalization (BN) is often used in an attempt to stabilize and accelerate training in deep neural networks. In many cases it indeed decreases the number of parameter updates required to achieve low training error. However, it also reduces robustness to small adversarial input perturbations and common corruptions by double-digit percentages, as we show on five standard datasets. Furthermore, we find that substituting weight decay for BN is sufficient to nullify a relationship between adversarial vulnerability and the input dimension. A recent mean-field analysis found that BN induces gradient explosion when used on multiple layers, but this cannot fully explain the vulnerability we observe, given that it occurs already for a single BN layer. We argue that the actual cause is the tilting of the decision boundary with respect to the nearest-centroid classifier along input dimensions of low variance. As a result, the constant introduced for numerical stability in the BN step acts as an important hyperparameter that can be tuned to recover some robustness at the cost of standard test accuracy. We explain this mechanism explicitly on a linear ``toy model and show in experiments that it still holds for nonlinear ``real-world models.", "keywords": ["batch normalization", "adversarial examples", "robustness"], "paperhash": "galloway|batch_normalization_is_a_cause_of_adversarial_vulnerability", "original_pdf": "/attachment/0d17fb5927362e54d58bec28099a64e132d8b968.pdf", "_bibtex": "@misc{\ngalloway2020batch,\ntitle={Batch Normalization is a Cause of Adversarial Vulnerability},\nauthor={Angus Galloway and Anna Golubeva and Thomas Tanay and Medhat Moussa and Graham W. Taylor},\nyear={2020},\nurl={https://openreview.net/forum?id=H1x-3xSKDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1x-3xSKDr", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504179044, "tmdate": 1576860568668, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper2525/Authors", "ICLR.cc/2020/Conference/Paper2525/Reviewers", "ICLR.cc/2020/Conference/Paper2525/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2525/-/Public_Comment"}}}, {"id": "SklNaNu2vr", "original": null, "number": 1, "cdate": 1569649852225, "ddate": null, "tcdate": 1569649852225, "tmdate": 1569655622348, "tddate": null, "forum": "H1x-3xSKDr", "replyto": "H1x-3xSKDr", "invitation": "ICLR.cc/2020/Conference/Paper2525/-/Public_Comment", "content": {"comment": "Hi, it is an interesting work. \n\n1. In intuition, the mean and the variance in the BN layer are strongly related with the clean training data, which leads to the unsuitable normalization to the adversarial testing data. I see two related submission, namely https://openreview.net/forum?id=HyxJhCEFDS and https://openreview.net/forum?id=BJlEEaEFDS&noteId=BylDR1y3Pr , where the authors propose MBN to disentangle the distribution for clean and adversarial data to estimate normalization statistics.\n\n2. Adversarially trained models also have the adversarial vulnerability for strong attack. So I have a little doubt whether batch normalization is a cause of adversarial vulnerability for adversarially trained models. In other words, without the BN layers, the robustness of adversarially trained models will increase? In my opinion, the answer is \"no\", because a stronger neural network architecture can help to increase the robustness of the adversarially trained model.\n\n3. Have the authors tried other normalization layers, which do not need to store the mean and the variance of training data, such as instance normalization, layer normalization and so on?", "title": "Nice work and some questions"}, "signatures": ["~Anthony_Wittmer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Anthony_Wittmer1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["gallowaa@uoguelph.ca", "agolubeva@perimeterinstitute.ca", "thomas.tanay.13@ucl.ac.uk", "mmoussa@uoguelph.ca", "gwtaylor@uoguelph.ca"], "title": "Batch Normalization is a Cause of Adversarial Vulnerability", "authors": ["Angus Galloway", "Anna Golubeva", "Thomas Tanay", "Medhat Moussa", "Graham W. Taylor"], "pdf": "/pdf/b4194c0ee567812e4a29e27d0deebb65dc0242e3.pdf", "TL;DR": "Batch normalization reduces robustness at test-time to common corruptions and adversarial examples.", "abstract": "Batch normalization (BN) is often used in an attempt to stabilize and accelerate training in deep neural networks. In many cases it indeed decreases the number of parameter updates required to achieve low training error. However, it also reduces robustness to small adversarial input perturbations and common corruptions by double-digit percentages, as we show on five standard datasets. Furthermore, we find that substituting weight decay for BN is sufficient to nullify a relationship between adversarial vulnerability and the input dimension. A recent mean-field analysis found that BN induces gradient explosion when used on multiple layers, but this cannot fully explain the vulnerability we observe, given that it occurs already for a single BN layer. We argue that the actual cause is the tilting of the decision boundary with respect to the nearest-centroid classifier along input dimensions of low variance. As a result, the constant introduced for numerical stability in the BN step acts as an important hyperparameter that can be tuned to recover some robustness at the cost of standard test accuracy. We explain this mechanism explicitly on a linear ``toy model and show in experiments that it still holds for nonlinear ``real-world models.", "keywords": ["batch normalization", "adversarial examples", "robustness"], "paperhash": "galloway|batch_normalization_is_a_cause_of_adversarial_vulnerability", "original_pdf": "/attachment/0d17fb5927362e54d58bec28099a64e132d8b968.pdf", "_bibtex": "@misc{\ngalloway2020batch,\ntitle={Batch Normalization is a Cause of Adversarial Vulnerability},\nauthor={Angus Galloway and Anna Golubeva and Thomas Tanay and Medhat Moussa and Graham W. Taylor},\nyear={2020},\nurl={https://openreview.net/forum?id=H1x-3xSKDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1x-3xSKDr", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504179044, "tmdate": 1576860568668, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper2525/Authors", "ICLR.cc/2020/Conference/Paper2525/Reviewers", "ICLR.cc/2020/Conference/Paper2525/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2525/-/Public_Comment"}}}], "count": 21}