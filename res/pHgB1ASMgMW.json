{"notes": [{"id": "pHgB1ASMgMW", "original": "3WgzAIUh2a", "number": 1565, "cdate": 1601308173599, "ddate": null, "tcdate": 1601308173599, "tmdate": 1614985692733, "tddate": null, "forum": "pHgB1ASMgMW", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Rethinking Uncertainty in Deep Learning: Whether and How it Improves Robustness", "authorids": ["~Yilun_Jin1", "~Lixin_Fan1", "jinhewu@webank.com", "~Ce_Ju1", "~Qiang_Yang1"], "authors": ["Yilun Jin", "Lixin Fan", "Kam Woh Ng", "Ce Ju", "Qiang Yang"], "keywords": ["Adversarial Robustness", "Uncertainty Promotion", "Adversarial Training"], "abstract": "Deep neural networks (DNNs) are known to be prone to adversarial attacks, for which many remedies are proposed. While adversarial training (AT) is regarded as the most robust defense, it suffers from poor performance both on clean examples and under other types of attacks, e.g. attacks with larger perturbations. Meanwhile, regularizers that encourage uncertain outputs, such as entropy maximization (EntM) and label smoothing (LS) can maintain accuracy on clean examples and improve performance under weak attacks, yet their ability to defend against strong attacks is still in doubt. In this paper, we revisit uncertainty promotion regularizers, including EntM and LS, in the field of adversarial learning. We show that EntM and LS alone provide robustness only under small perturbations. Contrarily, we show that uncertainty promotion regularizers complement AT in a principled manner, consistently improving performance on both clean examples and under various attacks, especially attacks with large perturbations. We further analyze how uncertainty promotion regularizers enhance the performance of AT from the perspective of Jacobian matrices $\\nabla_X f(X;\\theta)$, and find out that EntM effectively shrinks the norm of Jacobian matrices and hence promotes robustness. ", "one-sentence_summary": "We show that uncertainty promotion regularizers complement adversarial training consistently, while uncertainty promotion alone does not provide consistent robustness. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "jin|rethinking_uncertainty_in_deep_learning_whether_and_how_it_improves_robustness", "pdf": "/pdf/6b95747025734e0cad1854ffa90ce2afc0404636.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=X-J3BZ5Z93", "_bibtex": "@misc{\njin2021rethinking,\ntitle={Rethinking Uncertainty in Deep Learning: Whether and How it Improves Robustness},\nauthor={Yilun Jin and Lixin Fan and Kam Woh Ng and Ce Ju and Qiang Yang},\nyear={2021},\nurl={https://openreview.net/forum?id=pHgB1ASMgMW}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 11, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "njL3h_TnEHY", "original": null, "number": 1, "cdate": 1610040455709, "ddate": null, "tcdate": 1610040455709, "tmdate": 1610474058347, "tddate": null, "forum": "pHgB1ASMgMW", "replyto": "pHgB1ASMgMW", "invitation": "ICLR.cc/2021/Conference/Paper1565/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "The reviews were a bit mixed, with some concerns on the novelty and experimental evaluation. While the authors' efforts during rebuttable were appreciated, the overall sentiment is that this work, in its current form, cannot be accepted to ICLR yet. Please consider revising your work based on the excellent reviews. Some more comments from the AC's independent assessment:\n \n(a) Further elaboration on the novelty is needed. Currently the main message appears to be that if we combine two existing approaches (AT and EntM or LS) then we get better results. This is perhaps not too surprising and more elaboration on the significance would be appreciated. \n\n(b) More comparisons in the experiments, including the SOTA performances and alternative defenses (some below).\n \n(c) The analysis in Section 6 adds more confusion than clarification. It is clear that EntM and LS would largely decrease M_f, but why would they also decrease the Lipschitz constant even more sharply? If this explanation is useful, why not directly regularize the Lipschitz constant and maximize the margin M_f? There is in fact a large body of work on this, see for example:\n\n1. Formal Guarantees on the Robustness of a Classifier against Adversarial Manipulation\n\n2. Parseval Networks: Improving Robustness to Adversarial Examples\n\n3. L2-Nonexpansive Neural Networks\n\n4. and the many references since.\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Rethinking Uncertainty in Deep Learning: Whether and How it Improves Robustness", "authorids": ["~Yilun_Jin1", "~Lixin_Fan1", "jinhewu@webank.com", "~Ce_Ju1", "~Qiang_Yang1"], "authors": ["Yilun Jin", "Lixin Fan", "Kam Woh Ng", "Ce Ju", "Qiang Yang"], "keywords": ["Adversarial Robustness", "Uncertainty Promotion", "Adversarial Training"], "abstract": "Deep neural networks (DNNs) are known to be prone to adversarial attacks, for which many remedies are proposed. While adversarial training (AT) is regarded as the most robust defense, it suffers from poor performance both on clean examples and under other types of attacks, e.g. attacks with larger perturbations. Meanwhile, regularizers that encourage uncertain outputs, such as entropy maximization (EntM) and label smoothing (LS) can maintain accuracy on clean examples and improve performance under weak attacks, yet their ability to defend against strong attacks is still in doubt. In this paper, we revisit uncertainty promotion regularizers, including EntM and LS, in the field of adversarial learning. We show that EntM and LS alone provide robustness only under small perturbations. Contrarily, we show that uncertainty promotion regularizers complement AT in a principled manner, consistently improving performance on both clean examples and under various attacks, especially attacks with large perturbations. We further analyze how uncertainty promotion regularizers enhance the performance of AT from the perspective of Jacobian matrices $\\nabla_X f(X;\\theta)$, and find out that EntM effectively shrinks the norm of Jacobian matrices and hence promotes robustness. ", "one-sentence_summary": "We show that uncertainty promotion regularizers complement adversarial training consistently, while uncertainty promotion alone does not provide consistent robustness. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "jin|rethinking_uncertainty_in_deep_learning_whether_and_how_it_improves_robustness", "pdf": "/pdf/6b95747025734e0cad1854ffa90ce2afc0404636.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=X-J3BZ5Z93", "_bibtex": "@misc{\njin2021rethinking,\ntitle={Rethinking Uncertainty in Deep Learning: Whether and How it Improves Robustness},\nauthor={Yilun Jin and Lixin Fan and Kam Woh Ng and Ce Ju and Qiang Yang},\nyear={2021},\nurl={https://openreview.net/forum?id=pHgB1ASMgMW}\n}"}, "tags": [], "invitation": {"reply": {"forum": "pHgB1ASMgMW", "replyto": "pHgB1ASMgMW", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040455696, "tmdate": 1610474058332, "id": "ICLR.cc/2021/Conference/Paper1565/-/Decision"}}}, {"id": "bkGALOX1Z6b", "original": null, "number": 1, "cdate": 1602794067932, "ddate": null, "tcdate": 1602794067932, "tmdate": 1606769604930, "tddate": null, "forum": "pHgB1ASMgMW", "replyto": "pHgB1ASMgMW", "invitation": "ICLR.cc/2021/Conference/Paper1565/-/Official_Review", "content": {"title": "Limited novelty and insufficient experiments.", "review": "1) First, this work studies how entropy maximization and label smoothing combined with adversarial training can improve adversarial robustness. Although these two techniques have been shown to prevent model from being over-confident, I still think it is an over-claim that \"rethinking uncertainty in deep learning\" as this work does not truly study uncertainty in deep learning or the correlation between uncertainty and adversarial robustness.\n2) Label smoothing alone does not provide stronger adversarial robustness, this is not a surprising result as pointed out by many existing work. As entropy maximization is similar as label smoothing, it is also in the expectation that they have similar performance without providing stronger adversarial robustness.\n3) \"Combining label smoothing/EntM with adversarial training can further provide stronger adversarial robustness\", this is the main conclusion of this work. However, I am not fully convinced this is novel enough as this has been observed by existing work. In addition, this work only tested the model on the PGD attacks, the same type of attack (although with smaller l infinity norm bound) is used during training. It is very necessary to test the model against different types of attacks, especially decision-boundary based attacks, to support this conclusion as this is the main contribution of this work. \nIn all, I vote for a rejection for this work.\n\n******** After Rebuttal ************\nI carefully read the authors' response and unfortunately they do not address my concerns. Based on my research background in adversarial robustness and uncertainty estimates, I would keep my original rating unchanged as this work has very limited contribution to these two areas.", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper1565/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1565/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Rethinking Uncertainty in Deep Learning: Whether and How it Improves Robustness", "authorids": ["~Yilun_Jin1", "~Lixin_Fan1", "jinhewu@webank.com", "~Ce_Ju1", "~Qiang_Yang1"], "authors": ["Yilun Jin", "Lixin Fan", "Kam Woh Ng", "Ce Ju", "Qiang Yang"], "keywords": ["Adversarial Robustness", "Uncertainty Promotion", "Adversarial Training"], "abstract": "Deep neural networks (DNNs) are known to be prone to adversarial attacks, for which many remedies are proposed. While adversarial training (AT) is regarded as the most robust defense, it suffers from poor performance both on clean examples and under other types of attacks, e.g. attacks with larger perturbations. Meanwhile, regularizers that encourage uncertain outputs, such as entropy maximization (EntM) and label smoothing (LS) can maintain accuracy on clean examples and improve performance under weak attacks, yet their ability to defend against strong attacks is still in doubt. In this paper, we revisit uncertainty promotion regularizers, including EntM and LS, in the field of adversarial learning. We show that EntM and LS alone provide robustness only under small perturbations. Contrarily, we show that uncertainty promotion regularizers complement AT in a principled manner, consistently improving performance on both clean examples and under various attacks, especially attacks with large perturbations. We further analyze how uncertainty promotion regularizers enhance the performance of AT from the perspective of Jacobian matrices $\\nabla_X f(X;\\theta)$, and find out that EntM effectively shrinks the norm of Jacobian matrices and hence promotes robustness. ", "one-sentence_summary": "We show that uncertainty promotion regularizers complement adversarial training consistently, while uncertainty promotion alone does not provide consistent robustness. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "jin|rethinking_uncertainty_in_deep_learning_whether_and_how_it_improves_robustness", "pdf": "/pdf/6b95747025734e0cad1854ffa90ce2afc0404636.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=X-J3BZ5Z93", "_bibtex": "@misc{\njin2021rethinking,\ntitle={Rethinking Uncertainty in Deep Learning: Whether and How it Improves Robustness},\nauthor={Yilun Jin and Lixin Fan and Kam Woh Ng and Ce Ju and Qiang Yang},\nyear={2021},\nurl={https://openreview.net/forum?id=pHgB1ASMgMW}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "pHgB1ASMgMW", "replyto": "pHgB1ASMgMW", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1565/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538115827, "tmdate": 1606915788278, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1565/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1565/-/Official_Review"}}}, {"id": "obbpodQfdb1", "original": null, "number": 4, "cdate": 1604040883651, "ddate": null, "tcdate": 1604040883651, "tmdate": 1606676261628, "tddate": null, "forum": "pHgB1ASMgMW", "replyto": "pHgB1ASMgMW", "invitation": "ICLR.cc/2021/Conference/Paper1565/-/Official_Review", "content": {"title": "Interesting combination of ideas to increase adversarial robustness but gaps in the experimental evaluation", "review": "The authors combine adversarial training with two methods that increase the entropy of the output distribution of neural networks (label smoothing and entropy maximization). The authors find that this combination of ideas increases adversarial robustness on standard benchmarks, especially in the regime of large perturbation budgets (e.g., 16/255 on CIFAR-10). The authors also investigate the effect of their methods on the classification margin to understand the increase in adversarial accuracy.\n\nWhile the authors provide a detailed experimental evaluation of their defense method, this part of the paper is still my main concern. In particular, I see the following issues:\n\n* It is not clear if the authors choose a sufficiently large steps size for the PGD attacks. Typical values are 2 * eps / k to 10 * eps / k, where eps is the l_inf perturbation budget and k is the step size. However, the authors choose smaller step sizes in at least some cases, e.g., eps / k for the perturbation accuracy curves in Figure 1. This is a concern particularly because Table 1 shows that the model accuracies still decrease substantially when going from 10 to 40 PGD steps.\n\n* Again on the note of step sizes, why did the authors choose max(1 / 510, eps / k) in the attacks with more iterations?\n\n* It would be good to present a single table with the smallest known adversarial accuracies for the various defenses. E.g., Table 1 does not seem to include the results from more adaptive attacks where TRADES is sometimes comparable to the proposed methods.\n\n* How did the authors choose random restarts? They mention sigma = 0.005 - is this for a Gaussian distribution? What happens if the authors pick a point from the appropriately scaled l_inf ball instead?\n\n* It would be good to see attacks with the Carlini-Wagner (margin) loss function.\n\nConsidering the well-known difficulties with evaluating defenses against adversarial attacks, I currently cannot recommend accepting the paper.\n\nAdditional comments:\n\n- The authors attribue Szegedy et al., 2013 with adversarial examples. The authors may be interested in https://arxiv.org/abs/1712.03141 , in particular Figure 8, for a more detailed history of this research direction.\n\n- Trade-offs between adversarial and standard accuracy have been studied before Rice et al., 2020. For instance, see https://arxiv.org/abs/1805.12152 and https://arxiv.org/abs/2002.10716 .\n\n- Could scaling the softmax temperature also work for increasing the entropy of the softmax distribution in a way that leads to increased robustness?\n\n- Could the proposed technique be combined with TRADES or training on additional unlabeled data? (e.g., see https://arxiv.org/abs/1905.13736 and https://arxiv.org/abs/1905.13725 ).\n\n- Is there a \"max\" missing in Equation 5?\n\n- Beginning of Section 6: \"deeper into the how\"\n\n- Equation 12: what is X + delta?\n\n---------------------------------------------------------------------------------------------\n\nThank you for the detailed response. I have updated my score from 4 to 5.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1565/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1565/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Rethinking Uncertainty in Deep Learning: Whether and How it Improves Robustness", "authorids": ["~Yilun_Jin1", "~Lixin_Fan1", "jinhewu@webank.com", "~Ce_Ju1", "~Qiang_Yang1"], "authors": ["Yilun Jin", "Lixin Fan", "Kam Woh Ng", "Ce Ju", "Qiang Yang"], "keywords": ["Adversarial Robustness", "Uncertainty Promotion", "Adversarial Training"], "abstract": "Deep neural networks (DNNs) are known to be prone to adversarial attacks, for which many remedies are proposed. While adversarial training (AT) is regarded as the most robust defense, it suffers from poor performance both on clean examples and under other types of attacks, e.g. attacks with larger perturbations. Meanwhile, regularizers that encourage uncertain outputs, such as entropy maximization (EntM) and label smoothing (LS) can maintain accuracy on clean examples and improve performance under weak attacks, yet their ability to defend against strong attacks is still in doubt. In this paper, we revisit uncertainty promotion regularizers, including EntM and LS, in the field of adversarial learning. We show that EntM and LS alone provide robustness only under small perturbations. Contrarily, we show that uncertainty promotion regularizers complement AT in a principled manner, consistently improving performance on both clean examples and under various attacks, especially attacks with large perturbations. We further analyze how uncertainty promotion regularizers enhance the performance of AT from the perspective of Jacobian matrices $\\nabla_X f(X;\\theta)$, and find out that EntM effectively shrinks the norm of Jacobian matrices and hence promotes robustness. ", "one-sentence_summary": "We show that uncertainty promotion regularizers complement adversarial training consistently, while uncertainty promotion alone does not provide consistent robustness. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "jin|rethinking_uncertainty_in_deep_learning_whether_and_how_it_improves_robustness", "pdf": "/pdf/6b95747025734e0cad1854ffa90ce2afc0404636.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=X-J3BZ5Z93", "_bibtex": "@misc{\njin2021rethinking,\ntitle={Rethinking Uncertainty in Deep Learning: Whether and How it Improves Robustness},\nauthor={Yilun Jin and Lixin Fan and Kam Woh Ng and Ce Ju and Qiang Yang},\nyear={2021},\nurl={https://openreview.net/forum?id=pHgB1ASMgMW}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "pHgB1ASMgMW", "replyto": "pHgB1ASMgMW", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1565/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538115827, "tmdate": 1606915788278, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1565/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1565/-/Official_Review"}}}, {"id": "e6jFM6b_bh9", "original": null, "number": 7, "cdate": 1605967882685, "ddate": null, "tcdate": 1605967882685, "tmdate": 1605967882685, "tddate": null, "forum": "pHgB1ASMgMW", "replyto": "pHgB1ASMgMW", "invitation": "ICLR.cc/2021/Conference/Paper1565/-/Official_Comment", "content": {"title": "Revision Submittes. Add experiments of WideResNet34-10. ", "comment": "Dear reviewers: \n\nWe upload another rebuttal revision. The changes are: \n1. We add experiments on WideResNet34-10 (which is also commonly used in adversarial learning literature, and used in TRADES). The results are shown in Appendix D. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1565/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1565/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Rethinking Uncertainty in Deep Learning: Whether and How it Improves Robustness", "authorids": ["~Yilun_Jin1", "~Lixin_Fan1", "jinhewu@webank.com", "~Ce_Ju1", "~Qiang_Yang1"], "authors": ["Yilun Jin", "Lixin Fan", "Kam Woh Ng", "Ce Ju", "Qiang Yang"], "keywords": ["Adversarial Robustness", "Uncertainty Promotion", "Adversarial Training"], "abstract": "Deep neural networks (DNNs) are known to be prone to adversarial attacks, for which many remedies are proposed. While adversarial training (AT) is regarded as the most robust defense, it suffers from poor performance both on clean examples and under other types of attacks, e.g. attacks with larger perturbations. Meanwhile, regularizers that encourage uncertain outputs, such as entropy maximization (EntM) and label smoothing (LS) can maintain accuracy on clean examples and improve performance under weak attacks, yet their ability to defend against strong attacks is still in doubt. In this paper, we revisit uncertainty promotion regularizers, including EntM and LS, in the field of adversarial learning. We show that EntM and LS alone provide robustness only under small perturbations. Contrarily, we show that uncertainty promotion regularizers complement AT in a principled manner, consistently improving performance on both clean examples and under various attacks, especially attacks with large perturbations. We further analyze how uncertainty promotion regularizers enhance the performance of AT from the perspective of Jacobian matrices $\\nabla_X f(X;\\theta)$, and find out that EntM effectively shrinks the norm of Jacobian matrices and hence promotes robustness. ", "one-sentence_summary": "We show that uncertainty promotion regularizers complement adversarial training consistently, while uncertainty promotion alone does not provide consistent robustness. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "jin|rethinking_uncertainty_in_deep_learning_whether_and_how_it_improves_robustness", "pdf": "/pdf/6b95747025734e0cad1854ffa90ce2afc0404636.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=X-J3BZ5Z93", "_bibtex": "@misc{\njin2021rethinking,\ntitle={Rethinking Uncertainty in Deep Learning: Whether and How it Improves Robustness},\nauthor={Yilun Jin and Lixin Fan and Kam Woh Ng and Ce Ju and Qiang Yang},\nyear={2021},\nurl={https://openreview.net/forum?id=pHgB1ASMgMW}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "pHgB1ASMgMW", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1565/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1565/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1565/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1565/Authors|ICLR.cc/2021/Conference/Paper1565/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1565/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923858290, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1565/-/Official_Comment"}}}, {"id": "JCcoMXmU-lx", "original": null, "number": 6, "cdate": 1605346234519, "ddate": null, "tcdate": 1605346234519, "tmdate": 1605346234519, "tddate": null, "forum": "pHgB1ASMgMW", "replyto": "pHgB1ASMgMW", "invitation": "ICLR.cc/2021/Conference/Paper1565/-/Official_Comment", "content": {"title": "Revision Submitted. New experiments added. Restructure of analysis section. ", "comment": "Dear reviewers: \n\nWe uploaded a revision to our paper according to your comments. The changes are: \n1. Addition of CW attack results in Table 1 and 2. Under CW attacks, the results are similar to those under PGD, and do not contradict our claims. \n2. Addition of hyperparameter analysis $\\lambda$ in Sect. 5.3 and Fig. 3. By choosing appropriate $\\lambda$, uncertainty level, we can see both accuracy and robustness improvements. \n3. Restructure of the Sect. 6, analysis. We add necessary discussions and descriptions. \n4. Discussion of KD methods in Sect. 2 Related Works. \n5. Fix minor issues and typos. \n\nThank you all for your helpful comments. "}, "signatures": ["ICLR.cc/2021/Conference/Paper1565/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1565/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Rethinking Uncertainty in Deep Learning: Whether and How it Improves Robustness", "authorids": ["~Yilun_Jin1", "~Lixin_Fan1", "jinhewu@webank.com", "~Ce_Ju1", "~Qiang_Yang1"], "authors": ["Yilun Jin", "Lixin Fan", "Kam Woh Ng", "Ce Ju", "Qiang Yang"], "keywords": ["Adversarial Robustness", "Uncertainty Promotion", "Adversarial Training"], "abstract": "Deep neural networks (DNNs) are known to be prone to adversarial attacks, for which many remedies are proposed. While adversarial training (AT) is regarded as the most robust defense, it suffers from poor performance both on clean examples and under other types of attacks, e.g. attacks with larger perturbations. Meanwhile, regularizers that encourage uncertain outputs, such as entropy maximization (EntM) and label smoothing (LS) can maintain accuracy on clean examples and improve performance under weak attacks, yet their ability to defend against strong attacks is still in doubt. In this paper, we revisit uncertainty promotion regularizers, including EntM and LS, in the field of adversarial learning. We show that EntM and LS alone provide robustness only under small perturbations. Contrarily, we show that uncertainty promotion regularizers complement AT in a principled manner, consistently improving performance on both clean examples and under various attacks, especially attacks with large perturbations. We further analyze how uncertainty promotion regularizers enhance the performance of AT from the perspective of Jacobian matrices $\\nabla_X f(X;\\theta)$, and find out that EntM effectively shrinks the norm of Jacobian matrices and hence promotes robustness. ", "one-sentence_summary": "We show that uncertainty promotion regularizers complement adversarial training consistently, while uncertainty promotion alone does not provide consistent robustness. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "jin|rethinking_uncertainty_in_deep_learning_whether_and_how_it_improves_robustness", "pdf": "/pdf/6b95747025734e0cad1854ffa90ce2afc0404636.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=X-J3BZ5Z93", "_bibtex": "@misc{\njin2021rethinking,\ntitle={Rethinking Uncertainty in Deep Learning: Whether and How it Improves Robustness},\nauthor={Yilun Jin and Lixin Fan and Kam Woh Ng and Ce Ju and Qiang Yang},\nyear={2021},\nurl={https://openreview.net/forum?id=pHgB1ASMgMW}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "pHgB1ASMgMW", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1565/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1565/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1565/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1565/Authors|ICLR.cc/2021/Conference/Paper1565/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1565/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923858290, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1565/-/Official_Comment"}}}, {"id": "mUZjPfP1CTZ", "original": null, "number": 3, "cdate": 1605252650493, "ddate": null, "tcdate": 1605252650493, "tmdate": 1605341542625, "tddate": null, "forum": "pHgB1ASMgMW", "replyto": "FmeoZcVHs5H", "invitation": "ICLR.cc/2021/Conference/Paper1565/-/Official_Comment", "content": {"title": "Thanks for your comments. Reply. ", "comment": "Thanks for your comments. We quickly reply to your doubts on the paper. \n\n1. **Novelty**. Although the method of entropy maximization and label smoothing is not new, we believe that our paper still provides contributions that are different from existing works. First, we show that EntM and LS alone can only provide partial and conditional robustness (under weak attacks, or under attacks with low perturbation), and that EntM and LS alone cause gradient obfuscation. Previously, as far as we know, the question of how robust EntM and LS alone are is not settled. Please refer to 2. in the reply to Reviewer 4 for details. \n\nAlso, we demonstrate through extensive experiments that EntM and LS consistently improves PAT over all perturbations and a wide range of attacks, which is contrary to the case of EntM and LS alone. We also provide an analysis of why this happens in Sect. 6, which, as far as we know is not done by previous works. Therefore, although we agree that the method is not new, we still make adequate contributions that are not solved by existing works. \n\n2. **The analysis part is difficult to follow.** Including Eq 11 and Norm of Jacobian Matrix. \nWe agree that the analysis part is not described clearly, and we have added related discussions in the updated paper. \nEq 11 is a direct outcome of Eq 10, which basically divides a positive term $\\|X'-X\\|_2$ on both sides and re-arranging the LHS and the RHS. \nFor a matrix $M$, in general we use $\\|M\\|_2$ to denote its spectral norm, which is defined to be its largest singular vector. (See https://en.wikipedia.org/wiki/Matrix_norm). \nAlso we will modify the descriptions to make them clearer. \n\n3. **Regarding hyperparameter experiments and the tradeoff**\nWe have done hyper parameter experiments in Sect. 5.3 and Fig. 3. The results show that EntM can achieve improvements on both accuracy and robustness, thus partially alleviating the tradeoff. \n\n4. Eq 7, 8, **C** stands for the number of classes, which is defined in 3 Preliminaries. \n\n5. **Targeted attacks.** Since our defense does not focus on a specific \"victim\" class to strengthen, and untargeted attacks are more difficult to defend than targeted ones, we think that untargeted attacks would be sufficient to demonstrate the effectiveness. \n\n6. **Attacks not SOTA.** We added results using CW attacks to attack our defenses (See Table 1 and 2). The results are consistent with using PGD and do not contradict our claims. \n\n7. **Distillation Training**. We note that a work in AAAI 2020, **Adversarially robust distillation** (ARD, https://arxiv.org/abs/1905.09747), mentioned the problem of distilling a teacher model into a student model while maintaining robustness. We summarize several findings made by that paper to answer your questions. \n- First, ARD aims to build small but robust models from large models without adversarial training, which is different from our focus. Our focus and contribution do not rely on existing large models, and aim to build better adversarially robust models from scratch. \n- Second, ARD shows that non-robust teachers lead to non-robust students (See Table 2 of ARD). Therefore, in order to obtain a robust student model, a robust teacher should be obtained, probably via AT. In this way, in terms of time consumption, ARD is no more efficient than ours. \n- Third, ARD shows that through ordinary distillation procedures, student models distilled from teacher models show performance drops in terms of both accuracy and robustness. See Table 3 of ARD. \n- Last, ARD shows that using the proposed ARD technique, robust teachers can be distilled to students with robustness preserved. However, ARD requires another procedure of PGD-AT, which takes more time than our approach. \n\nWe will add the above discussions to the Related Work section of our paper. "}, "signatures": ["ICLR.cc/2021/Conference/Paper1565/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1565/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Rethinking Uncertainty in Deep Learning: Whether and How it Improves Robustness", "authorids": ["~Yilun_Jin1", "~Lixin_Fan1", "jinhewu@webank.com", "~Ce_Ju1", "~Qiang_Yang1"], "authors": ["Yilun Jin", "Lixin Fan", "Kam Woh Ng", "Ce Ju", "Qiang Yang"], "keywords": ["Adversarial Robustness", "Uncertainty Promotion", "Adversarial Training"], "abstract": "Deep neural networks (DNNs) are known to be prone to adversarial attacks, for which many remedies are proposed. While adversarial training (AT) is regarded as the most robust defense, it suffers from poor performance both on clean examples and under other types of attacks, e.g. attacks with larger perturbations. Meanwhile, regularizers that encourage uncertain outputs, such as entropy maximization (EntM) and label smoothing (LS) can maintain accuracy on clean examples and improve performance under weak attacks, yet their ability to defend against strong attacks is still in doubt. In this paper, we revisit uncertainty promotion regularizers, including EntM and LS, in the field of adversarial learning. We show that EntM and LS alone provide robustness only under small perturbations. Contrarily, we show that uncertainty promotion regularizers complement AT in a principled manner, consistently improving performance on both clean examples and under various attacks, especially attacks with large perturbations. We further analyze how uncertainty promotion regularizers enhance the performance of AT from the perspective of Jacobian matrices $\\nabla_X f(X;\\theta)$, and find out that EntM effectively shrinks the norm of Jacobian matrices and hence promotes robustness. ", "one-sentence_summary": "We show that uncertainty promotion regularizers complement adversarial training consistently, while uncertainty promotion alone does not provide consistent robustness. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "jin|rethinking_uncertainty_in_deep_learning_whether_and_how_it_improves_robustness", "pdf": "/pdf/6b95747025734e0cad1854ffa90ce2afc0404636.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=X-J3BZ5Z93", "_bibtex": "@misc{\njin2021rethinking,\ntitle={Rethinking Uncertainty in Deep Learning: Whether and How it Improves Robustness},\nauthor={Yilun Jin and Lixin Fan and Kam Woh Ng and Ce Ju and Qiang Yang},\nyear={2021},\nurl={https://openreview.net/forum?id=pHgB1ASMgMW}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "pHgB1ASMgMW", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1565/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1565/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1565/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1565/Authors|ICLR.cc/2021/Conference/Paper1565/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1565/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923858290, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1565/-/Official_Comment"}}}, {"id": "Ud6MgY110OO", "original": null, "number": 5, "cdate": 1605268965070, "ddate": null, "tcdate": 1605268965070, "tmdate": 1605269463043, "tddate": null, "forum": "pHgB1ASMgMW", "replyto": "bitzfW4zgM_", "invitation": "ICLR.cc/2021/Conference/Paper1565/-/Official_Comment", "content": {"title": "Thanks for your comments. Response. ", "comment": "We are grateful to your helpful comments to our paper. \n\nWe address some of your comments below. \n1. **Regarding larger datasets (ImageNet) and larger models**. \nRegarding dataset, in fact, many papers in the field of adversarial robustness fail to carry out experiments on Imagenet. The reason is probably that Imagenet is too resource consuming. As pointed out by https://arxiv.org/pdf/1904.12843.pdf, footnote 1, adversarial training on imagenet may take hundreds of GPUs like Tesla V100 and weeks of time. \n\nRegarding larger models, we point out that ResNet18s are commonly used in the field of adversarial training, such as TRADES (Zhang et al. 2019), and the paper \"Overfitting in adversarially robust deep learning\" (Wong et al. 2020). Therefore we consider ResNet18 to be also appropriate. To be aligned with the SOTAs, we are running experiments on Wide-resnet32s, which are also commonly used in the field of adversarial robustness and bigger than ResNet18. We will update our paper by then. \n\n2.**Regarding point clouds of Normal**. \nWe once added the Normal into the point cloud, but the points of Normal are highly cluttered around the 0 point and not clearly visible. Therefore we omit the points by \"Normal\". We think that the Normal points will not affect our claim and our results. \n\n3. **Regarding terms strong adversary and true robustness**. \nWe admit failing to clearly define them. The term \"Strong adversary\" in general refers to attacks with more effort, but under the same budget. For example, we can say that PGD10-4 is stronger than FGSM4, and PGD40-4 is stronger than PGD10-4, but we generally do not say that PGD10-8 is stronger than PGD10-4 (because they are given different budgets). However, the term \"strong attack\" is a relative term, and is hard to be rigorously defined. \n\nThe term \"true robustness\" represents the same meaning as the term \"adversarial risk\" defined in (Uesato et al. 2018). We will revise our paper and replace all \"true robustness\" by \"adversarial risk\". In short, adversarial risk is a term that measures the \"existence\" of adversarial examples around a data point, regardless of specific attacks. It is a term that upper bounds the success rate of all attacks, and cannot be measured but can only be approximated by stronger and stronger attacks. \n\n4. **KL divergence**\nThe KL divergence is used in our paper to show the connection between entropy maximization and label smoothing. See Eqn. 7-8. However, since KL(p|q) = H(q) + CE(p|q), the entropy term is taken over q, i.e. labels, which is not directly related to the outputs of networks. We leave further investigation as future work. \n\n5. **Fig. 1, 2 labels**: We have updated them in the latest revision. \n\n6. **Dropping $\\theta$**. The suggestion is highly helpful and we have dropped it. \n\n7. **$\\delta$ variable**. We admit failing to state Theorem 1 very clearly. $\\delta$ is an arbitrary noise, and as long as the condition in Eqn. 12 holds, the $\\delta$ direction can be arbitrary. We updated the descriptions of Theorem 1 to make things clearer. \n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1565/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1565/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Rethinking Uncertainty in Deep Learning: Whether and How it Improves Robustness", "authorids": ["~Yilun_Jin1", "~Lixin_Fan1", "jinhewu@webank.com", "~Ce_Ju1", "~Qiang_Yang1"], "authors": ["Yilun Jin", "Lixin Fan", "Kam Woh Ng", "Ce Ju", "Qiang Yang"], "keywords": ["Adversarial Robustness", "Uncertainty Promotion", "Adversarial Training"], "abstract": "Deep neural networks (DNNs) are known to be prone to adversarial attacks, for which many remedies are proposed. While adversarial training (AT) is regarded as the most robust defense, it suffers from poor performance both on clean examples and under other types of attacks, e.g. attacks with larger perturbations. Meanwhile, regularizers that encourage uncertain outputs, such as entropy maximization (EntM) and label smoothing (LS) can maintain accuracy on clean examples and improve performance under weak attacks, yet their ability to defend against strong attacks is still in doubt. In this paper, we revisit uncertainty promotion regularizers, including EntM and LS, in the field of adversarial learning. We show that EntM and LS alone provide robustness only under small perturbations. Contrarily, we show that uncertainty promotion regularizers complement AT in a principled manner, consistently improving performance on both clean examples and under various attacks, especially attacks with large perturbations. We further analyze how uncertainty promotion regularizers enhance the performance of AT from the perspective of Jacobian matrices $\\nabla_X f(X;\\theta)$, and find out that EntM effectively shrinks the norm of Jacobian matrices and hence promotes robustness. ", "one-sentence_summary": "We show that uncertainty promotion regularizers complement adversarial training consistently, while uncertainty promotion alone does not provide consistent robustness. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "jin|rethinking_uncertainty_in_deep_learning_whether_and_how_it_improves_robustness", "pdf": "/pdf/6b95747025734e0cad1854ffa90ce2afc0404636.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=X-J3BZ5Z93", "_bibtex": "@misc{\njin2021rethinking,\ntitle={Rethinking Uncertainty in Deep Learning: Whether and How it Improves Robustness},\nauthor={Yilun Jin and Lixin Fan and Kam Woh Ng and Ce Ju and Qiang Yang},\nyear={2021},\nurl={https://openreview.net/forum?id=pHgB1ASMgMW}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "pHgB1ASMgMW", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1565/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1565/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1565/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1565/Authors|ICLR.cc/2021/Conference/Paper1565/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1565/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923858290, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1565/-/Official_Comment"}}}, {"id": "AIVcKmHFM5y", "original": null, "number": 4, "cdate": 1605255436619, "ddate": null, "tcdate": 1605255436619, "tmdate": 1605255909720, "tddate": null, "forum": "pHgB1ASMgMW", "replyto": "bkGALOX1Z6b", "invitation": "ICLR.cc/2021/Conference/Paper1565/-/Official_Comment", "content": {"title": "Thanks for your comments. We provide our evidence and look forward to additional clarifications from your side.  ", "comment": "We are grateful to you for your suggestions on this paper, and we provide some of our evidence in support of our claims. We are also looking forward to some additional clarifications from your side.  \n\n1. Regarding your opinion on **over-claiming**. \nYou point out that \"this work does not **truly** study uncertainty in deep learning or the correlation between uncertainty and adversarial robustness\". We would like to point out that entropy, \"play a central role in information theory as measures of information, choice and **uncertainty**\" (Shannon, 1948), and therefore, from our perspective, studying entropy in supervised learning should be considered as studying \"uncertainty\".  \n\nAlso, through our experiments, we find out that adding uncertainty to adversarial training is helpful in improving both accuracy and robustness. In addition, we also add another experiment in Sect 5.3 regarding the relationship between hyperparameter $\\lambda$, which controls the level of uncertainty, and accuracy/robustness. We consider the above experiments to be studying the correlation between uncertainty and robustness. \n\nSince different people may have different personal viewpoints, we are looking forward to hearing from you about your opinion on what is a **true** study of uncertainty in deep learning, and what is a **true** study of uncertainty and robustness. \n\n2. You pointed out that **\"Label smoothing alone does not provide stronger adversarial robustness\" is pointed out by many existing works.**\nWe would like to show our evidence that, whether label smoothing provides stronger robustness is still an unsettled question. \nAn ICLR 2019 submission (**Label Smoothing and Logit Squeezing: A Replacement for Adversarial Training?**, https://openreview.net/forum?id=BJlr0j0ctX), which claims that label smoothing can achieve robustness better than adversarial training, received 25 replies, most of which on experiments. The authors provide experiments in the replies on # steps, # restarts, varying step sizes, and unbounded eps, but still, the doubts are not settled, and the authors eventually withdrew their submission. \n\nAlso, whether entropy maximization can provide stronger adversarial robustness is also an unsettled problem. The paper **Improving adversarial robustness via promoting ensemble diversity**, ICML 2019 uses the technique entropy maximization, and claims that it achieves stronger robustness. However, their experiments are only done on PGD10, which is not strong enough and far from conclusive. \n\nWe, therefore, consider whether entropy maximization and label smoothing alone contribute to stronger robustness as unsettled problems. Since you said that \"LS cannot provide stronger robustness, which is pointed out by **many** existing works\", but did not provide **any** references, we look forward to your clarification on your opinion. \n\n3. You pointed out that \"Combining label smoothing/EntM with adversarial training can further provide stronger adversarial robustness\", this is the main conclusion of this work. However, I am not fully convinced this is novel enough as this has been observed by existing work. \" However, as far as we know, there are no previous works that study the combination of EntM and AT, while our paper carries out an extensive experimental evaluation on the method of EntM. Also, we provide a margin/gradient norm analysis in Section 6, which explains the result and cannot be seen in previous works. Therefore, we think that our contribution is adequate and distinct from existing works. \n\n4. **Regarding your concerns about different types of attacks,** we added experiments using CW (margin) attacks in the new revision of this paper, see Table 1 and 2. Under the CW (margin) attacks, PAT-EntM also outperforms PAT, and is comparable or more robust than TRADES. Therefore, we consider that our claim and contribution is valid. \n\nWe look forward to clarifications from you. "}, "signatures": ["ICLR.cc/2021/Conference/Paper1565/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1565/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Rethinking Uncertainty in Deep Learning: Whether and How it Improves Robustness", "authorids": ["~Yilun_Jin1", "~Lixin_Fan1", "jinhewu@webank.com", "~Ce_Ju1", "~Qiang_Yang1"], "authors": ["Yilun Jin", "Lixin Fan", "Kam Woh Ng", "Ce Ju", "Qiang Yang"], "keywords": ["Adversarial Robustness", "Uncertainty Promotion", "Adversarial Training"], "abstract": "Deep neural networks (DNNs) are known to be prone to adversarial attacks, for which many remedies are proposed. While adversarial training (AT) is regarded as the most robust defense, it suffers from poor performance both on clean examples and under other types of attacks, e.g. attacks with larger perturbations. Meanwhile, regularizers that encourage uncertain outputs, such as entropy maximization (EntM) and label smoothing (LS) can maintain accuracy on clean examples and improve performance under weak attacks, yet their ability to defend against strong attacks is still in doubt. In this paper, we revisit uncertainty promotion regularizers, including EntM and LS, in the field of adversarial learning. We show that EntM and LS alone provide robustness only under small perturbations. Contrarily, we show that uncertainty promotion regularizers complement AT in a principled manner, consistently improving performance on both clean examples and under various attacks, especially attacks with large perturbations. We further analyze how uncertainty promotion regularizers enhance the performance of AT from the perspective of Jacobian matrices $\\nabla_X f(X;\\theta)$, and find out that EntM effectively shrinks the norm of Jacobian matrices and hence promotes robustness. ", "one-sentence_summary": "We show that uncertainty promotion regularizers complement adversarial training consistently, while uncertainty promotion alone does not provide consistent robustness. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "jin|rethinking_uncertainty_in_deep_learning_whether_and_how_it_improves_robustness", "pdf": "/pdf/6b95747025734e0cad1854ffa90ce2afc0404636.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=X-J3BZ5Z93", "_bibtex": "@misc{\njin2021rethinking,\ntitle={Rethinking Uncertainty in Deep Learning: Whether and How it Improves Robustness},\nauthor={Yilun Jin and Lixin Fan and Kam Woh Ng and Ce Ju and Qiang Yang},\nyear={2021},\nurl={https://openreview.net/forum?id=pHgB1ASMgMW}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "pHgB1ASMgMW", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1565/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1565/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1565/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1565/Authors|ICLR.cc/2021/Conference/Paper1565/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1565/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923858290, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1565/-/Official_Comment"}}}, {"id": "_JGvhAMgyZF", "original": null, "number": 2, "cdate": 1605175501855, "ddate": null, "tcdate": 1605175501855, "tmdate": 1605251992475, "tddate": null, "forum": "pHgB1ASMgMW", "replyto": "obbpodQfdb1", "invitation": "ICLR.cc/2021/Conference/Paper1565/-/Official_Comment", "content": {"title": "Thanks for your helpful comments regarding experimental evaluations. New experiments done. ", "comment": "We are grateful for your comments regarding deficiencies in experiments. We do additional experiments to address your concerns. Revisions of the paper has been uploaded. \n\n1. **Regarding small step size.** Upon your comments, we redo the experiments in Fig. 1 with step size 2eps / #steps, or in the experiments, stepsize = 2/255, with eps k/255 and k steps. The curves are roughly the same as using eps / #steps, but generally lower accuracies are observed. \nAlso, for Table 1, 2, 3 experiments, we use eps = k/255, but take step size **k/2550 regardless of # steps**. This means that for 40-step attacks, step size = 4eps / # steps, which should be sufficient. Therefore, the results in Table 1, 2, 3 under PGD40 should be credible. \n\n2. **Also regarding step size choice of max(1/510, eps/k).**\nUpon your comments, we redo the experiments in Fig. 2 (attacks with more steps) with step size 2eps / k. We find out that by using 2eps/k, **attack success rates converge earlier than using max(eps/k, 1/510), and to a lower success rate**. We therefore use a constant step size (1/510) when k is very large (when max(2eps/k, 1/510) takes 1/510), which leads to more powerful attacks. This result shows the importance of using a **constant (rather than decaying with k)** step size when k is large. \n\n3. **Regarding worst-case robustness**. \nWe made a revision to the paper, which explicitly states the worst-case robustness of PAT-EntM under CIFAR-10, eps = 8 (See Sect 5.2.2 Adaptive Attacks). The result is 0.4505 compared to 0.4521 for TRADES, which is still a comparable number. For other scenarios, PAT-EntM is consistently and clearly over TRADES (by at least 2%), and thus we omit another table. \n\n4. **Regarding random restarts**. \nWe use 5 random restarts for each attack, each initialized with Gaussian noise with a standard deviation 0.005. \nUpon your comments, we do some experiments regarding randomness. Gauss 0.005 denotes Gaussian noise with stddev 0.005, and Unif 0.01 denotes uniform noise with range -0.01 to 0.01. \n\n|Initialization    | PAT-EntM, PGD40-8, CIFAR10|\n|--------------------|----------------------------------------|\n|Gauss 0.005     | 0.4618|\n|Gauss 0.001     | 0.4619|\n|Unif 0.005        | 0.4615|\n|Unif 0.01          | 0.4619|\n|Unif 0.02          | 0.4630|\n\n|Num Restarts | PAT-EntM, PGD40-8, CIFAR10 (Gauss 0.005)|\n|------------------- |--------------------------------------------------------     |\n|1                        | 0.4619                                                                 |\n|2                        | 0.4612                                                                 |\n|5                        | 0.4598                                                                 |\n|10                      | 0.4592                                                                 |\n|20                      | 0.4591                                                                 |\n\nTherefore we claim that randomness does not significantly influence our conclusion. Also, 5 random restarts are sufficient to obtain reliable robustness, and increasing # restarts cannot lead to a much higher success rate. \n\n5. **CW attacks**\nUpon your comments we did CW margin attacks on CIFAR10 and CIFAR100. The implementation is taken from https://github.com/zjfheart/Friendly-Adversarial-Training/. We list related results in Table 1, 2. \nThese results are still consistent with our claim, that PAT-EntM achieves consistent improvement over PAT, and comparable or more robust than TRADES. \n\n- Additional Comments: \n1. Review Papers: We will add them as more comprehensive surveys than Szegedy 2013 and Goodfellow 2014. \n2. Adv-Std Accuracy Tradeoff: We have added the papers you mentioned to Sect 5.3, where we discuss one additional experiment done for the revision, and the accuracy-robustness tradeoff. \n3. Temperature Scaling: We have not done it but it seems unlikely. The reason is that, if we scale the output of a network $f(X)$ by 10, $f_1(X) = f(X)/10$, then both the decision margin $M_{f, X}$ and the gradient norm $\\nabla_X f(X;\\theta)$ will shrink by 10 times, leading to no change in the normalized margin (Theorem 1 and related discussions). However, in Sect. 6 we can see that EntM effectively enlarges the normalized margin, which marks its difference. \n4. We listed results of TRADE-EntM in the Appendix. In short, TRADES-EntM is better than TRADES, but not better than PAT-EntM. We consider unlabeled data as future work. \n5. There should be a \"max\". We fix all typos.\n6. $\\delta$ is an arbitrary noise. We have added descriptions. "}, "signatures": ["ICLR.cc/2021/Conference/Paper1565/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1565/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Rethinking Uncertainty in Deep Learning: Whether and How it Improves Robustness", "authorids": ["~Yilun_Jin1", "~Lixin_Fan1", "jinhewu@webank.com", "~Ce_Ju1", "~Qiang_Yang1"], "authors": ["Yilun Jin", "Lixin Fan", "Kam Woh Ng", "Ce Ju", "Qiang Yang"], "keywords": ["Adversarial Robustness", "Uncertainty Promotion", "Adversarial Training"], "abstract": "Deep neural networks (DNNs) are known to be prone to adversarial attacks, for which many remedies are proposed. While adversarial training (AT) is regarded as the most robust defense, it suffers from poor performance both on clean examples and under other types of attacks, e.g. attacks with larger perturbations. Meanwhile, regularizers that encourage uncertain outputs, such as entropy maximization (EntM) and label smoothing (LS) can maintain accuracy on clean examples and improve performance under weak attacks, yet their ability to defend against strong attacks is still in doubt. In this paper, we revisit uncertainty promotion regularizers, including EntM and LS, in the field of adversarial learning. We show that EntM and LS alone provide robustness only under small perturbations. Contrarily, we show that uncertainty promotion regularizers complement AT in a principled manner, consistently improving performance on both clean examples and under various attacks, especially attacks with large perturbations. We further analyze how uncertainty promotion regularizers enhance the performance of AT from the perspective of Jacobian matrices $\\nabla_X f(X;\\theta)$, and find out that EntM effectively shrinks the norm of Jacobian matrices and hence promotes robustness. ", "one-sentence_summary": "We show that uncertainty promotion regularizers complement adversarial training consistently, while uncertainty promotion alone does not provide consistent robustness. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "jin|rethinking_uncertainty_in_deep_learning_whether_and_how_it_improves_robustness", "pdf": "/pdf/6b95747025734e0cad1854ffa90ce2afc0404636.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=X-J3BZ5Z93", "_bibtex": "@misc{\njin2021rethinking,\ntitle={Rethinking Uncertainty in Deep Learning: Whether and How it Improves Robustness},\nauthor={Yilun Jin and Lixin Fan and Kam Woh Ng and Ce Ju and Qiang Yang},\nyear={2021},\nurl={https://openreview.net/forum?id=pHgB1ASMgMW}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "pHgB1ASMgMW", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1565/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1565/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1565/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1565/Authors|ICLR.cc/2021/Conference/Paper1565/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1565/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923858290, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1565/-/Official_Comment"}}}, {"id": "bitzfW4zgM_", "original": null, "number": 2, "cdate": 1603803037259, "ddate": null, "tcdate": 1603803037259, "tmdate": 1605024413462, "tddate": null, "forum": "pHgB1ASMgMW", "replyto": "pHgB1ASMgMW", "invitation": "ICLR.cc/2021/Conference/Paper1565/-/Official_Review", "content": {"title": "Review of \"Rethinking Uncertainty in Deep Learning: Whether and How it Improves Robustness \"", "review": "# Summary\nThis paper investigates the complementary mechanisms of adversarial training and uncertainty promoting regularizers. In the field of adversarial machine learning, adversarial training as proposed by Madry et al. 2017 has been the common method. In the field of uncertainty regularization, maximum entropy and label smoothing have been the accepted methods. However, the combination of both has not been investigated before. The paper provides extensive experiments on these methods. The final section gives insights in the theory behind adversarial training and uncertainty promoting regularization, where they show that the combined method increases a notion of normalized margin and a notion of adversarial robustness. \n\n# Strong & Weak points\n\n## Strong points\n\n  * The related work section contains an extensive overview of related and contemporary literature, where both literature in adversarial ML and uncertainty promotion is being discussed.\n  * Ablation experiments in figure 1 and 2 show that combining adversarial training and uncertainty promoting regularizers have better accuracy under a range of attack settings. \n  * Theoretical insights as to why Entropy Maximization would help improve adversarial robustness, complementary to adversarial training is provided in Section 6\n\n## Weak points\n\n  * The method is studied on small datasets (CIFAR, MNIST, SVHN) using small models (ResNet18). It remains to be seen how these insights translate to larger datasets (ImageNet) and larger models (ResNet50).\n  * In the abstract and introduction, the text speaks of \u201ctrue\u201d robustness, against \u201cstrong\u201d attacks, but these terms are never defined.\n  * The increase in (approximate) normalized margin when using entropy maximization as shown in table 4 provides an important argument for the claim of this paper. However, the numbers were obtained using the CIFAR10 dataset and a ResNet18 model. A stronger case could be made with a larger dataset and a larger model.\n\n# Statement\n\nRecommendation: 6\n\nReasons\n   * The claims for a complementary benefit of adversarial training and uncertainty promoting regularization are backed up with both extensive experiments and theoretical insights.\n   * The fields of adversarial training and uncertainty regularizers have been evolving separately and this paper provides initial insights how these two lines of research can be combined. [Comment: I am not 100% up to date with the related literature, so I'll be looking to other reviewers if they are aware of existing work combining uncertainty regularization and adversarial training.]\n   * Ablation experiments in Figure 3 shows substrates for the complementary actions of Entropy Maximization and Adversarial training. Entropy maximization can be shown to increase a notion of \u201cnormalized margin width\u201d, adversarial training can be shown to increase a notion of  adversarial robustness, and when combining the methods, both metrics increase. \n\n# Additional questions:\n\nTable 3a) misses the point cloud for normal training. How do the normalized margins and adversarial distances of normal training compare in this plot? Table 4 already shows that the normalized margin (0.19) is smaller than the three methods, but I miss the numbers for adversarial distances during normal training.\n\n# Minor feedback\n\nThese points are minor feedback and not part of the assessment.\n\n  * KL divergence can be decomposed as $KL(p|q) = H[q] + CE(p|q)$. Could this decomposition explain the differences between TRADES, PAT, and entropy maximization?\n\n  * Figure 1 & 2: please provide labels for the x axes.\n  * None of the derivations on eqn. 9 to 13 depend on $\\theta$. Consider dropping the $\\theta$ variables everywhere to focus the analysis on what really matters: the derivatives w.r.t. $x$. \n  * Equation 12: the $\\delta$ variable is only implicitly defined. It is not clear to me if its direction is parallel or orthogonal to the decision boundary. \n\n", "rating": "6: Marginally above acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2021/Conference/Paper1565/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1565/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Rethinking Uncertainty in Deep Learning: Whether and How it Improves Robustness", "authorids": ["~Yilun_Jin1", "~Lixin_Fan1", "jinhewu@webank.com", "~Ce_Ju1", "~Qiang_Yang1"], "authors": ["Yilun Jin", "Lixin Fan", "Kam Woh Ng", "Ce Ju", "Qiang Yang"], "keywords": ["Adversarial Robustness", "Uncertainty Promotion", "Adversarial Training"], "abstract": "Deep neural networks (DNNs) are known to be prone to adversarial attacks, for which many remedies are proposed. While adversarial training (AT) is regarded as the most robust defense, it suffers from poor performance both on clean examples and under other types of attacks, e.g. attacks with larger perturbations. Meanwhile, regularizers that encourage uncertain outputs, such as entropy maximization (EntM) and label smoothing (LS) can maintain accuracy on clean examples and improve performance under weak attacks, yet their ability to defend against strong attacks is still in doubt. In this paper, we revisit uncertainty promotion regularizers, including EntM and LS, in the field of adversarial learning. We show that EntM and LS alone provide robustness only under small perturbations. Contrarily, we show that uncertainty promotion regularizers complement AT in a principled manner, consistently improving performance on both clean examples and under various attacks, especially attacks with large perturbations. We further analyze how uncertainty promotion regularizers enhance the performance of AT from the perspective of Jacobian matrices $\\nabla_X f(X;\\theta)$, and find out that EntM effectively shrinks the norm of Jacobian matrices and hence promotes robustness. ", "one-sentence_summary": "We show that uncertainty promotion regularizers complement adversarial training consistently, while uncertainty promotion alone does not provide consistent robustness. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "jin|rethinking_uncertainty_in_deep_learning_whether_and_how_it_improves_robustness", "pdf": "/pdf/6b95747025734e0cad1854ffa90ce2afc0404636.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=X-J3BZ5Z93", "_bibtex": "@misc{\njin2021rethinking,\ntitle={Rethinking Uncertainty in Deep Learning: Whether and How it Improves Robustness},\nauthor={Yilun Jin and Lixin Fan and Kam Woh Ng and Ce Ju and Qiang Yang},\nyear={2021},\nurl={https://openreview.net/forum?id=pHgB1ASMgMW}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "pHgB1ASMgMW", "replyto": "pHgB1ASMgMW", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1565/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538115827, "tmdate": 1606915788278, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1565/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1565/-/Official_Review"}}}, {"id": "FmeoZcVHs5H", "original": null, "number": 3, "cdate": 1603803284944, "ddate": null, "tcdate": 1603803284944, "tmdate": 1605024413398, "tddate": null, "forum": "pHgB1ASMgMW", "replyto": "pHgB1ASMgMW", "invitation": "ICLR.cc/2021/Conference/Paper1565/-/Official_Review", "content": {"title": "Need Novelty Clarification", "review": "Summary:\n\nThis paper tries to improve the model robustness by modifying the loss function with the EntM or LS term. Also, they give a further analysis to identify how uncertainty promotion works.\n\n\nStrength:\n\n-- The methods seem rational. And the experiments also demonstrate the effectiveness.\n\n\nWeakness:\n\n--The novelty is limited. Training the model without the one-hot label is not new. Also, it seems that the label smoothing is the only contribution proposed by this work, but it is pretty naive.\n\n--The analysis part is difficult to follow.\n\n\nComments:\n--There are many works that claim the distillation training can improve the robustness of the model, which also adopt the soft label in loss function. Please compare with relative methods.\n\n-- For uncertainty analysis, the explanation is not straightforward. First, I am not sure why Eq. (11) holds. Second, why the L2 norm of the Jacobian matrix is its largest singular value? Also, I found the analysis is finally conducted through empirical studies. I think maybe the equation descriptions can be modified easier to understand and make the empirical study more clearly if possible.\n\n-- I think the performance drop with adversarial training is still high, although the results show the accuracy is better than TRADE. I suggest the authors to provide some experiments that adjust the hyper parameters to identify the trade-off between the clean accuracy and the robustness of the model.\n\n--For Eqs. (7)-(8), what does the term \u2018logC\u2019 stand for?\n\n--This work only considers untarget attack. How about the performance under target attack?\n\n--The attack algorithms seem not state-of-the-art. Please try more algorithms if possible.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1565/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1565/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Rethinking Uncertainty in Deep Learning: Whether and How it Improves Robustness", "authorids": ["~Yilun_Jin1", "~Lixin_Fan1", "jinhewu@webank.com", "~Ce_Ju1", "~Qiang_Yang1"], "authors": ["Yilun Jin", "Lixin Fan", "Kam Woh Ng", "Ce Ju", "Qiang Yang"], "keywords": ["Adversarial Robustness", "Uncertainty Promotion", "Adversarial Training"], "abstract": "Deep neural networks (DNNs) are known to be prone to adversarial attacks, for which many remedies are proposed. While adversarial training (AT) is regarded as the most robust defense, it suffers from poor performance both on clean examples and under other types of attacks, e.g. attacks with larger perturbations. Meanwhile, regularizers that encourage uncertain outputs, such as entropy maximization (EntM) and label smoothing (LS) can maintain accuracy on clean examples and improve performance under weak attacks, yet their ability to defend against strong attacks is still in doubt. In this paper, we revisit uncertainty promotion regularizers, including EntM and LS, in the field of adversarial learning. We show that EntM and LS alone provide robustness only under small perturbations. Contrarily, we show that uncertainty promotion regularizers complement AT in a principled manner, consistently improving performance on both clean examples and under various attacks, especially attacks with large perturbations. We further analyze how uncertainty promotion regularizers enhance the performance of AT from the perspective of Jacobian matrices $\\nabla_X f(X;\\theta)$, and find out that EntM effectively shrinks the norm of Jacobian matrices and hence promotes robustness. ", "one-sentence_summary": "We show that uncertainty promotion regularizers complement adversarial training consistently, while uncertainty promotion alone does not provide consistent robustness. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "jin|rethinking_uncertainty_in_deep_learning_whether_and_how_it_improves_robustness", "pdf": "/pdf/6b95747025734e0cad1854ffa90ce2afc0404636.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=X-J3BZ5Z93", "_bibtex": "@misc{\njin2021rethinking,\ntitle={Rethinking Uncertainty in Deep Learning: Whether and How it Improves Robustness},\nauthor={Yilun Jin and Lixin Fan and Kam Woh Ng and Ce Ju and Qiang Yang},\nyear={2021},\nurl={https://openreview.net/forum?id=pHgB1ASMgMW}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "pHgB1ASMgMW", "replyto": "pHgB1ASMgMW", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1565/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538115827, "tmdate": 1606915788278, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1565/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1565/-/Official_Review"}}}], "count": 12}