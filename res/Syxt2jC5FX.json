{"notes": [{"id": "Syxt2jC5FX", "original": "rJeidVj9tX", "number": 732, "cdate": 1538087857307, "ddate": null, "tcdate": 1538087857307, "tmdate": 1550368628503, "tddate": null, "forum": "Syxt2jC5FX", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "From Hard to Soft: Understanding Deep Network Nonlinearities via Vector Quantization and Statistical Inference", "abstract": "Nonlinearity is crucial to the performance of a deep (neural) network (DN).\nTo date there has been little progress understanding the menagerie of available  nonlinearities, but recently progress has been made on understanding the r\\^{o}le played by piecewise affine and convex nonlinearities like the ReLU and absolute value activation functions and max-pooling.\nIn particular, DN layers constructed from these operations can be interpreted as {\\em max-affine spline operators} (MASOs) that have an elegant link to vector quantization (VQ) and $K$-means.\nWhile this is good theoretical progress, the entire MASO approach is predicated on the requirement that the nonlinearities be piecewise affine and convex, which precludes important activation functions like the sigmoid, hyperbolic tangent, and softmax.\n{\\em This paper extends the MASO framework to these and an infinitely large class of new nonlinearities by linking deterministic MASOs with probabilistic Gaussian Mixture Models (GMMs).}\nWe show that, under a GMM, piecewise affine, convex nonlinearities like ReLU, absolute value, and max-pooling can be interpreted as solutions to certain natural ``hard'' VQ inference problems, while sigmoid, hyperbolic tangent, and softmax can be interpreted as solutions to corresponding ``soft'' VQ inference problems.\nWe further extend the framework by hybridizing the hard and soft VQ optimizations to create a $\\beta$-VQ inference that interpolates between hard, soft, and linear VQ inference.\nA prime example of a $\\beta$-VQ DN nonlinearity is the {\\em swish} nonlinearity, which offers state-of-the-art performance in a range of computer vision tasks but was developed ad hoc by experimentation.\nFinally, we validate with experiments an important assertion of our theory, namely that DN performance can be significantly improved by enforcing orthogonality in its linear filters.\n", "keywords": ["Spline", "Vector Quantization", "Inference", "Nonlinearities", "Deep Network"], "authorids": ["randallbalestriero@gmail.com", "richb@rice.edu"], "authors": ["Randall Balestriero", "Richard Baraniuk"], "TL;DR": "Reformulate deep networks nonlinearities from a vector quantization scope and bridge most known nonlinearities together.", "pdf": "/pdf/586d2cb1a01d67a4a95dc22e4ebf511b0aa52404.pdf", "paperhash": "balestriero|from_hard_to_soft_understanding_deep_network_nonlinearities_via_vector_quantization_and_statistical_inference", "_bibtex": "@inproceedings{\nbalestriero2018from,\ntitle={From Hard to Soft: Understanding Deep Network Nonlinearities via Vector Quantization and Statistical Inference},\nauthor={Randall Balestriero and Richard Baraniuk},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Syxt2jC5FX},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "B1xoBMnkl4", "original": null, "number": 1, "cdate": 1544696386755, "ddate": null, "tcdate": 1544696386755, "tmdate": 1545354506940, "tddate": null, "forum": "Syxt2jC5FX", "replyto": "Syxt2jC5FX", "invitation": "ICLR.cc/2019/Conference/-/Paper732/Meta_Review", "content": {"metareview": "Dear authors,\n\nAll reviewers liked your work. However, they also noted that the paper was hard to read, whether because of the notation or the lack of visualization.\n\nI strongly encourage you to spend the extra effort making your work more accessible for the final version.", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Accept (Poster)", "title": "Nice piece of work"}, "signatures": ["ICLR.cc/2019/Conference/Paper732/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper732/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "From Hard to Soft: Understanding Deep Network Nonlinearities via Vector Quantization and Statistical Inference", "abstract": "Nonlinearity is crucial to the performance of a deep (neural) network (DN).\nTo date there has been little progress understanding the menagerie of available  nonlinearities, but recently progress has been made on understanding the r\\^{o}le played by piecewise affine and convex nonlinearities like the ReLU and absolute value activation functions and max-pooling.\nIn particular, DN layers constructed from these operations can be interpreted as {\\em max-affine spline operators} (MASOs) that have an elegant link to vector quantization (VQ) and $K$-means.\nWhile this is good theoretical progress, the entire MASO approach is predicated on the requirement that the nonlinearities be piecewise affine and convex, which precludes important activation functions like the sigmoid, hyperbolic tangent, and softmax.\n{\\em This paper extends the MASO framework to these and an infinitely large class of new nonlinearities by linking deterministic MASOs with probabilistic Gaussian Mixture Models (GMMs).}\nWe show that, under a GMM, piecewise affine, convex nonlinearities like ReLU, absolute value, and max-pooling can be interpreted as solutions to certain natural ``hard'' VQ inference problems, while sigmoid, hyperbolic tangent, and softmax can be interpreted as solutions to corresponding ``soft'' VQ inference problems.\nWe further extend the framework by hybridizing the hard and soft VQ optimizations to create a $\\beta$-VQ inference that interpolates between hard, soft, and linear VQ inference.\nA prime example of a $\\beta$-VQ DN nonlinearity is the {\\em swish} nonlinearity, which offers state-of-the-art performance in a range of computer vision tasks but was developed ad hoc by experimentation.\nFinally, we validate with experiments an important assertion of our theory, namely that DN performance can be significantly improved by enforcing orthogonality in its linear filters.\n", "keywords": ["Spline", "Vector Quantization", "Inference", "Nonlinearities", "Deep Network"], "authorids": ["randallbalestriero@gmail.com", "richb@rice.edu"], "authors": ["Randall Balestriero", "Richard Baraniuk"], "TL;DR": "Reformulate deep networks nonlinearities from a vector quantization scope and bridge most known nonlinearities together.", "pdf": "/pdf/586d2cb1a01d67a4a95dc22e4ebf511b0aa52404.pdf", "paperhash": "balestriero|from_hard_to_soft_understanding_deep_network_nonlinearities_via_vector_quantization_and_statistical_inference", "_bibtex": "@inproceedings{\nbalestriero2018from,\ntitle={From Hard to Soft: Understanding Deep Network Nonlinearities via Vector Quantization and Statistical Inference},\nauthor={Randall Balestriero and Richard Baraniuk},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Syxt2jC5FX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper732/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353104677, "tddate": null, "super": null, "final": null, "reply": {"forum": "Syxt2jC5FX", "replyto": "Syxt2jC5FX", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper732/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper732/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper732/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353104677}}}, {"id": "HylVtNN767", "original": null, "number": 4, "cdate": 1541780604088, "ddate": null, "tcdate": 1541780604088, "tmdate": 1541780604088, "tddate": null, "forum": "Syxt2jC5FX", "replyto": "SJetq0GQ67", "invitation": "ICLR.cc/2019/Conference/-/Paper732/Official_Comment", "content": {"title": "Answer to Reviewer1", "comment": "We thank the reviewer for their constructive comments. We agree that our soft-VQ extension is an important piece of the puzzle that is necessary to ensure a solid foundation of the 'MASO-view' of deep neural networks. \n\nRegarding the clarity of presentation, we agree that our streamlined treatment of the MASO background, while self-contained, is quite terse. The reason is very short page limit allowed for the submission. We hope that the reader will find our new results compelling enough that they will refer to [1] for additional background information and insights.\n\nRegarding the experiments and visualization, we also had to make hard choices due to space limitations. We decided that repeating visualizations from [1] using a Soft-VQ partitioning would be less useful than a detailed derivation and explanation of the internal Hard/Soft/Beta-VQ processes and how they lead to new nonlinearities. We certainly plan to include many more visualizations in our conference presentation, should the paper be accepted.\n\nFinally, we feel that our extension of the deterministic MASO framework is more than incremental, since it opens the door to a range of new applications, improvements, and theoretical questions that go far beyond the scope of [1]. For some examples, please see our reply to Reviewer 3.\n\n[1] Mad Max: Affine Spline Insights into Deep Learning https://arxiv.org/abs/1805.06576"}, "signatures": ["ICLR.cc/2019/Conference/Paper732/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper732/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper732/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "From Hard to Soft: Understanding Deep Network Nonlinearities via Vector Quantization and Statistical Inference", "abstract": "Nonlinearity is crucial to the performance of a deep (neural) network (DN).\nTo date there has been little progress understanding the menagerie of available  nonlinearities, but recently progress has been made on understanding the r\\^{o}le played by piecewise affine and convex nonlinearities like the ReLU and absolute value activation functions and max-pooling.\nIn particular, DN layers constructed from these operations can be interpreted as {\\em max-affine spline operators} (MASOs) that have an elegant link to vector quantization (VQ) and $K$-means.\nWhile this is good theoretical progress, the entire MASO approach is predicated on the requirement that the nonlinearities be piecewise affine and convex, which precludes important activation functions like the sigmoid, hyperbolic tangent, and softmax.\n{\\em This paper extends the MASO framework to these and an infinitely large class of new nonlinearities by linking deterministic MASOs with probabilistic Gaussian Mixture Models (GMMs).}\nWe show that, under a GMM, piecewise affine, convex nonlinearities like ReLU, absolute value, and max-pooling can be interpreted as solutions to certain natural ``hard'' VQ inference problems, while sigmoid, hyperbolic tangent, and softmax can be interpreted as solutions to corresponding ``soft'' VQ inference problems.\nWe further extend the framework by hybridizing the hard and soft VQ optimizations to create a $\\beta$-VQ inference that interpolates between hard, soft, and linear VQ inference.\nA prime example of a $\\beta$-VQ DN nonlinearity is the {\\em swish} nonlinearity, which offers state-of-the-art performance in a range of computer vision tasks but was developed ad hoc by experimentation.\nFinally, we validate with experiments an important assertion of our theory, namely that DN performance can be significantly improved by enforcing orthogonality in its linear filters.\n", "keywords": ["Spline", "Vector Quantization", "Inference", "Nonlinearities", "Deep Network"], "authorids": ["randallbalestriero@gmail.com", "richb@rice.edu"], "authors": ["Randall Balestriero", "Richard Baraniuk"], "TL;DR": "Reformulate deep networks nonlinearities from a vector quantization scope and bridge most known nonlinearities together.", "pdf": "/pdf/586d2cb1a01d67a4a95dc22e4ebf511b0aa52404.pdf", "paperhash": "balestriero|from_hard_to_soft_understanding_deep_network_nonlinearities_via_vector_quantization_and_statistical_inference", "_bibtex": "@inproceedings{\nbalestriero2018from,\ntitle={From Hard to Soft: Understanding Deep Network Nonlinearities via Vector Quantization and Statistical Inference},\nauthor={Randall Balestriero and Richard Baraniuk},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Syxt2jC5FX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper732/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621620348, "tddate": null, "super": null, "final": null, "reply": {"forum": "Syxt2jC5FX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper732/Authors", "ICLR.cc/2019/Conference/Paper732/Reviewers", "ICLR.cc/2019/Conference/Paper732/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper732/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper732/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper732/Authors|ICLR.cc/2019/Conference/Paper732/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper732/Reviewers", "ICLR.cc/2019/Conference/Paper732/Authors", "ICLR.cc/2019/Conference/Paper732/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621620348}}}, {"id": "HkxSzkX767", "original": null, "number": 3, "cdate": 1541775116824, "ddate": null, "tcdate": 1541775116824, "tmdate": 1541775116824, "tddate": null, "forum": "Syxt2jC5FX", "replyto": "SJetq0GQ67", "invitation": "ICLR.cc/2019/Conference/-/Paper732/Official_Comment", "content": {"title": "Adding cite for [1]", "comment": "[1] Mad Max: Affine Spline Insights into Deep Learning https://arxiv.org/abs/1805.06576"}, "signatures": ["ICLR.cc/2019/Conference/Paper732/AnonReviewer1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper732/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper732/AnonReviewer1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "From Hard to Soft: Understanding Deep Network Nonlinearities via Vector Quantization and Statistical Inference", "abstract": "Nonlinearity is crucial to the performance of a deep (neural) network (DN).\nTo date there has been little progress understanding the menagerie of available  nonlinearities, but recently progress has been made on understanding the r\\^{o}le played by piecewise affine and convex nonlinearities like the ReLU and absolute value activation functions and max-pooling.\nIn particular, DN layers constructed from these operations can be interpreted as {\\em max-affine spline operators} (MASOs) that have an elegant link to vector quantization (VQ) and $K$-means.\nWhile this is good theoretical progress, the entire MASO approach is predicated on the requirement that the nonlinearities be piecewise affine and convex, which precludes important activation functions like the sigmoid, hyperbolic tangent, and softmax.\n{\\em This paper extends the MASO framework to these and an infinitely large class of new nonlinearities by linking deterministic MASOs with probabilistic Gaussian Mixture Models (GMMs).}\nWe show that, under a GMM, piecewise affine, convex nonlinearities like ReLU, absolute value, and max-pooling can be interpreted as solutions to certain natural ``hard'' VQ inference problems, while sigmoid, hyperbolic tangent, and softmax can be interpreted as solutions to corresponding ``soft'' VQ inference problems.\nWe further extend the framework by hybridizing the hard and soft VQ optimizations to create a $\\beta$-VQ inference that interpolates between hard, soft, and linear VQ inference.\nA prime example of a $\\beta$-VQ DN nonlinearity is the {\\em swish} nonlinearity, which offers state-of-the-art performance in a range of computer vision tasks but was developed ad hoc by experimentation.\nFinally, we validate with experiments an important assertion of our theory, namely that DN performance can be significantly improved by enforcing orthogonality in its linear filters.\n", "keywords": ["Spline", "Vector Quantization", "Inference", "Nonlinearities", "Deep Network"], "authorids": ["randallbalestriero@gmail.com", "richb@rice.edu"], "authors": ["Randall Balestriero", "Richard Baraniuk"], "TL;DR": "Reformulate deep networks nonlinearities from a vector quantization scope and bridge most known nonlinearities together.", "pdf": "/pdf/586d2cb1a01d67a4a95dc22e4ebf511b0aa52404.pdf", "paperhash": "balestriero|from_hard_to_soft_understanding_deep_network_nonlinearities_via_vector_quantization_and_statistical_inference", "_bibtex": "@inproceedings{\nbalestriero2018from,\ntitle={From Hard to Soft: Understanding Deep Network Nonlinearities via Vector Quantization and Statistical Inference},\nauthor={Randall Balestriero and Richard Baraniuk},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Syxt2jC5FX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper732/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621620348, "tddate": null, "super": null, "final": null, "reply": {"forum": "Syxt2jC5FX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper732/Authors", "ICLR.cc/2019/Conference/Paper732/Reviewers", "ICLR.cc/2019/Conference/Paper732/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper732/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper732/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper732/Authors|ICLR.cc/2019/Conference/Paper732/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper732/Reviewers", "ICLR.cc/2019/Conference/Paper732/Authors", "ICLR.cc/2019/Conference/Paper732/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621620348}}}, {"id": "SJetq0GQ67", "original": null, "number": 3, "cdate": 1541774992721, "ddate": null, "tcdate": 1541774992721, "tmdate": 1541774992721, "tddate": null, "forum": "Syxt2jC5FX", "replyto": "Syxt2jC5FX", "invitation": "ICLR.cc/2019/Conference/-/Paper732/Official_Review", "content": {"title": "Logical continuation of existing work", "review": "At the core of this paper is the insight from [1] that a neural network layer constructed from a combination of linear, piecewise affine and convex operators can be interpreted as a max-affine spline operator (MASO). MASOs are directly connected to vector quantization (VQ) and K-means clustering, which means that a deep network implicitly constructs a hierarchical clustering of the training data during learning. This paper now substitutes VQ with probabilistic clustering models (GMMs) and extends the MASO interpretation of a wider range of possible operations in deep neural networks (sigmoidal activation functions, etc.).\n\nGiven the detailed treatment of MASOs in [1], this paper is a logical continuation of this approach. As such, it may seem only incremental, but I would consider it as an important piece to ensure a solid foundation of the 'MASO-view' on deep neural networks.\n\nMy main criticism is with respect to the quality and clarity of the presentation. Without reading in detail [1] it is very difficult to understand the presented work here. Moreover, compared to [1], a lot of explanatory content is missing, e.g. [1] had nice visualisations of the resulting partitioning on toy data.\n\nClearly, this work and [1] belong together in a larger form (e.g. a journal article), I hope that this is considered by the authors.", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper732/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "From Hard to Soft: Understanding Deep Network Nonlinearities via Vector Quantization and Statistical Inference", "abstract": "Nonlinearity is crucial to the performance of a deep (neural) network (DN).\nTo date there has been little progress understanding the menagerie of available  nonlinearities, but recently progress has been made on understanding the r\\^{o}le played by piecewise affine and convex nonlinearities like the ReLU and absolute value activation functions and max-pooling.\nIn particular, DN layers constructed from these operations can be interpreted as {\\em max-affine spline operators} (MASOs) that have an elegant link to vector quantization (VQ) and $K$-means.\nWhile this is good theoretical progress, the entire MASO approach is predicated on the requirement that the nonlinearities be piecewise affine and convex, which precludes important activation functions like the sigmoid, hyperbolic tangent, and softmax.\n{\\em This paper extends the MASO framework to these and an infinitely large class of new nonlinearities by linking deterministic MASOs with probabilistic Gaussian Mixture Models (GMMs).}\nWe show that, under a GMM, piecewise affine, convex nonlinearities like ReLU, absolute value, and max-pooling can be interpreted as solutions to certain natural ``hard'' VQ inference problems, while sigmoid, hyperbolic tangent, and softmax can be interpreted as solutions to corresponding ``soft'' VQ inference problems.\nWe further extend the framework by hybridizing the hard and soft VQ optimizations to create a $\\beta$-VQ inference that interpolates between hard, soft, and linear VQ inference.\nA prime example of a $\\beta$-VQ DN nonlinearity is the {\\em swish} nonlinearity, which offers state-of-the-art performance in a range of computer vision tasks but was developed ad hoc by experimentation.\nFinally, we validate with experiments an important assertion of our theory, namely that DN performance can be significantly improved by enforcing orthogonality in its linear filters.\n", "keywords": ["Spline", "Vector Quantization", "Inference", "Nonlinearities", "Deep Network"], "authorids": ["randallbalestriero@gmail.com", "richb@rice.edu"], "authors": ["Randall Balestriero", "Richard Baraniuk"], "TL;DR": "Reformulate deep networks nonlinearities from a vector quantization scope and bridge most known nonlinearities together.", "pdf": "/pdf/586d2cb1a01d67a4a95dc22e4ebf511b0aa52404.pdf", "paperhash": "balestriero|from_hard_to_soft_understanding_deep_network_nonlinearities_via_vector_quantization_and_statistical_inference", "_bibtex": "@inproceedings{\nbalestriero2018from,\ntitle={From Hard to Soft: Understanding Deep Network Nonlinearities via Vector Quantization and Statistical Inference},\nauthor={Randall Balestriero and Richard Baraniuk},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Syxt2jC5FX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper732/Official_Review", "cdate": 1542234388754, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "Syxt2jC5FX", "replyto": "Syxt2jC5FX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper732/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335790935, "tmdate": 1552335790935, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper732/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "HygJDAvzp7", "original": null, "number": 2, "cdate": 1541729878595, "ddate": null, "tcdate": 1541729878595, "tmdate": 1541729878595, "tddate": null, "forum": "Syxt2jC5FX", "replyto": "SylBlmHqhX", "invitation": "ICLR.cc/2019/Conference/-/Paper732/Official_Comment", "content": {"title": "Answer to Reviewer2", "comment": "We thank the reviewer for their constructive comments. We have revised the manuscript accordingly.\n\nWe have simplified the background (Section 2) by removing the superfluous l (layer) superscript.  This reworking clarifies the definition and operation of the MASO.\n\nWe have defined  $[\\pi^{(l)}]_{k,t}$ in the early part of Theorem 2 and then derived (5).\n\nThe assumption on the bias value needed for Proposition 1 is indeed only needed for that particular result.  We have highlighted this (i) in the second paragraph following Proposition 1 and (ii) in the sentence immediately after Theorem 2.\n\nWe have highlighted that Proposition 2 is a standard result (and added references) and motivated its presence in order to unify all the different VQs under a single optimization problem. Adding an Entropy regularization to the original optimization problem then enables us to interpolate between hard,soft and linear VQ.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper732/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper732/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper732/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "From Hard to Soft: Understanding Deep Network Nonlinearities via Vector Quantization and Statistical Inference", "abstract": "Nonlinearity is crucial to the performance of a deep (neural) network (DN).\nTo date there has been little progress understanding the menagerie of available  nonlinearities, but recently progress has been made on understanding the r\\^{o}le played by piecewise affine and convex nonlinearities like the ReLU and absolute value activation functions and max-pooling.\nIn particular, DN layers constructed from these operations can be interpreted as {\\em max-affine spline operators} (MASOs) that have an elegant link to vector quantization (VQ) and $K$-means.\nWhile this is good theoretical progress, the entire MASO approach is predicated on the requirement that the nonlinearities be piecewise affine and convex, which precludes important activation functions like the sigmoid, hyperbolic tangent, and softmax.\n{\\em This paper extends the MASO framework to these and an infinitely large class of new nonlinearities by linking deterministic MASOs with probabilistic Gaussian Mixture Models (GMMs).}\nWe show that, under a GMM, piecewise affine, convex nonlinearities like ReLU, absolute value, and max-pooling can be interpreted as solutions to certain natural ``hard'' VQ inference problems, while sigmoid, hyperbolic tangent, and softmax can be interpreted as solutions to corresponding ``soft'' VQ inference problems.\nWe further extend the framework by hybridizing the hard and soft VQ optimizations to create a $\\beta$-VQ inference that interpolates between hard, soft, and linear VQ inference.\nA prime example of a $\\beta$-VQ DN nonlinearity is the {\\em swish} nonlinearity, which offers state-of-the-art performance in a range of computer vision tasks but was developed ad hoc by experimentation.\nFinally, we validate with experiments an important assertion of our theory, namely that DN performance can be significantly improved by enforcing orthogonality in its linear filters.\n", "keywords": ["Spline", "Vector Quantization", "Inference", "Nonlinearities", "Deep Network"], "authorids": ["randallbalestriero@gmail.com", "richb@rice.edu"], "authors": ["Randall Balestriero", "Richard Baraniuk"], "TL;DR": "Reformulate deep networks nonlinearities from a vector quantization scope and bridge most known nonlinearities together.", "pdf": "/pdf/586d2cb1a01d67a4a95dc22e4ebf511b0aa52404.pdf", "paperhash": "balestriero|from_hard_to_soft_understanding_deep_network_nonlinearities_via_vector_quantization_and_statistical_inference", "_bibtex": "@inproceedings{\nbalestriero2018from,\ntitle={From Hard to Soft: Understanding Deep Network Nonlinearities via Vector Quantization and Statistical Inference},\nauthor={Randall Balestriero and Richard Baraniuk},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Syxt2jC5FX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper732/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621620348, "tddate": null, "super": null, "final": null, "reply": {"forum": "Syxt2jC5FX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper732/Authors", "ICLR.cc/2019/Conference/Paper732/Reviewers", "ICLR.cc/2019/Conference/Paper732/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper732/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper732/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper732/Authors|ICLR.cc/2019/Conference/Paper732/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper732/Reviewers", "ICLR.cc/2019/Conference/Paper732/Authors", "ICLR.cc/2019/Conference/Paper732/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621620348}}}, {"id": "SJxaGCDMTX", "original": null, "number": 1, "cdate": 1541729812727, "ddate": null, "tcdate": 1541729812727, "tmdate": 1541729812727, "tddate": null, "forum": "Syxt2jC5FX", "replyto": "HyebQg_c2m", "invitation": "ICLR.cc/2019/Conference/-/Paper732/Official_Comment", "content": {"title": "Answer to Reviewer3", "comment": "We thank the reviewer for their constructive comments. We respond to each below in detail.\n\nTECHNICAL CONTRIBUTIONS\nWe briefly review our four primary technical contributions.\n[C1] We extend the deterministic max-affine spline operator (MASO) framework for deep networks (DNs) developed in (Balestriero & Baraniuk, ICML2018) to a probabilistic Gaussian mixture model (GMM). \n[C2] We extend the deterministic vector quantization (VQ) spline partition of the MASO framework to a probabilistic, soft VQ that enables us to derive from first principles and unify most of the known DN nonlinearities, including nonlinear and nonconvex ones such as the softmax and sigmoid gated linear unit.\n[C3] By interpolating between hard and soft inference, we derive a new class of beta-VQ activation functions. In particular, a beta-VQ version of the hard ReLU activation is the \u201cSwish\u201d nonlinearity, which offers state-of-the-art performance in a range of computer vision tasks but was proposed ad hoc through experimentation.\n[C4] We rigorously prove that orthogonal filters endow a DN with an attractive inference capability. Orthogonal filters enable a DN to perform efficient, tractable, jointly optimal VQ inference across all units in a layer. This is in contrast to non-orthogonal DNs, which support optimal VQ only on a per-unit basis. Previous works have studied orthogonality only empirically.\n\nORTHOGONALIZATION\nAs noted in contribution [4] above, orthogonalization has already been applied in deep learning, but it has typically been applied ad hoc with little to no theoretical justification. In our paper, we have justified orthogonalization from the novel point of view of inferring the VQ partition of each of the unit outputs in a DN layer. In a standard DN, each unit output computation is performed independently from the other units. This absence of \u2018\u2019lateral connections\u2019\u2019 can lead to two problematic situations: on the one hand redundant information in a feature map or on the other hand incomplete representation of the input. We demonstrate that an elegant solution to both problems is to enforce orthogonality. We have added a statement after Theorem 4 regarding how orthogonalization has potential applications of independent interest outside of deep learning, for example in factorial GMMs and HMMs.\n\nFUTURE DIRECTIONS AND DISCUSSIONS\nWe agree with the reviewer that our hard/soft VQ perspective opens up many new directions to both understand and improve DNs. Here are several new directions that we could discuss further in the revised paper or at the conference:\n[F1] VQ penalization: Given our explicit (and differentiable) formulas for the soft VQ, we can derive new kinds of penalties to apply during learning.  For example, we could penalize an overconfident-VQ (as measured by the joint likelihood of the unit VQ representation of the layer input), which is symptomatic of over-fitting.\n[F2] Leaning new activation functions: The state-of-the-art Swish nonlinearity has learnable parameter that enables it to range from ReLU to sigmoid gated linear unit to linear. We can further augment this parametrization to enable us to reach the sigmoid unit as well. This will enable us to use learning experiments to investigate the conjecture that ReLU like nonlinearities are best for early DN layers while sigmoid-like nonlinearities are best for later layers.\n[F3] We can use the VQ and the per-unit VQ-based likelihood to create DNs that detect outliers and perform model selection.\n[F4] Alternative soft-VQ regularization: Replacing the Shannon Entropy regularization in (7) with a different penalty could yield new classes of nonlinear activation functions."}, "signatures": ["ICLR.cc/2019/Conference/Paper732/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper732/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper732/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "From Hard to Soft: Understanding Deep Network Nonlinearities via Vector Quantization and Statistical Inference", "abstract": "Nonlinearity is crucial to the performance of a deep (neural) network (DN).\nTo date there has been little progress understanding the menagerie of available  nonlinearities, but recently progress has been made on understanding the r\\^{o}le played by piecewise affine and convex nonlinearities like the ReLU and absolute value activation functions and max-pooling.\nIn particular, DN layers constructed from these operations can be interpreted as {\\em max-affine spline operators} (MASOs) that have an elegant link to vector quantization (VQ) and $K$-means.\nWhile this is good theoretical progress, the entire MASO approach is predicated on the requirement that the nonlinearities be piecewise affine and convex, which precludes important activation functions like the sigmoid, hyperbolic tangent, and softmax.\n{\\em This paper extends the MASO framework to these and an infinitely large class of new nonlinearities by linking deterministic MASOs with probabilistic Gaussian Mixture Models (GMMs).}\nWe show that, under a GMM, piecewise affine, convex nonlinearities like ReLU, absolute value, and max-pooling can be interpreted as solutions to certain natural ``hard'' VQ inference problems, while sigmoid, hyperbolic tangent, and softmax can be interpreted as solutions to corresponding ``soft'' VQ inference problems.\nWe further extend the framework by hybridizing the hard and soft VQ optimizations to create a $\\beta$-VQ inference that interpolates between hard, soft, and linear VQ inference.\nA prime example of a $\\beta$-VQ DN nonlinearity is the {\\em swish} nonlinearity, which offers state-of-the-art performance in a range of computer vision tasks but was developed ad hoc by experimentation.\nFinally, we validate with experiments an important assertion of our theory, namely that DN performance can be significantly improved by enforcing orthogonality in its linear filters.\n", "keywords": ["Spline", "Vector Quantization", "Inference", "Nonlinearities", "Deep Network"], "authorids": ["randallbalestriero@gmail.com", "richb@rice.edu"], "authors": ["Randall Balestriero", "Richard Baraniuk"], "TL;DR": "Reformulate deep networks nonlinearities from a vector quantization scope and bridge most known nonlinearities together.", "pdf": "/pdf/586d2cb1a01d67a4a95dc22e4ebf511b0aa52404.pdf", "paperhash": "balestriero|from_hard_to_soft_understanding_deep_network_nonlinearities_via_vector_quantization_and_statistical_inference", "_bibtex": "@inproceedings{\nbalestriero2018from,\ntitle={From Hard to Soft: Understanding Deep Network Nonlinearities via Vector Quantization and Statistical Inference},\nauthor={Randall Balestriero and Richard Baraniuk},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Syxt2jC5FX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper732/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621620348, "tddate": null, "super": null, "final": null, "reply": {"forum": "Syxt2jC5FX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper732/Authors", "ICLR.cc/2019/Conference/Paper732/Reviewers", "ICLR.cc/2019/Conference/Paper732/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper732/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper732/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper732/Authors|ICLR.cc/2019/Conference/Paper732/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper732/Reviewers", "ICLR.cc/2019/Conference/Paper732/Authors", "ICLR.cc/2019/Conference/Paper732/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621620348}}}, {"id": "HyebQg_c2m", "original": null, "number": 2, "cdate": 1541206040825, "ddate": null, "tcdate": 1541206040825, "tmdate": 1541533733451, "tddate": null, "forum": "Syxt2jC5FX", "replyto": "Syxt2jC5FX", "invitation": "ICLR.cc/2019/Conference/-/Paper732/Official_Review", "content": {"title": "Somewhat incremental work, but well posited and written.", "review": "This work extends the applicability of the spline theory of deep networks explored in previous works of Balestriero/ Baraniuk. The previous works setup DNs as layer-wise max-affine spline operators (MASOs) and recovers several non-linearities practically used as special cases of these MASOs. The previous works already recover RELU variants and some downsampling operators that the current submission characterizes as \"hard\" quantization.\n\nThe major contribution of this work is extending the application to \"soft\" quantization that recovers several new non-linear activations such as soft-max. It is well-known that the k-means algorithm can be considered as a run of an EM algorithm to recover the mean parameters of a gaussian mixture model. The \"hard\" to \"soft\" transformation, and any interpolation in between follows from combining this insight with the previous works. As such there isnt a major technical contribution imho in this work. Furthermore, the presented orthogonalization for easier inference has been used before in many works, some of which this submission also cites, most importantly in the previous work of Balestriero/ Baraniuk that this submission extends. \n\nNevertheless there is value in novel results that may follow from previous works in a straightforward but non-trivial fashion, as long as it is well-presented and thoroughly researched and implication well-highlighted. This paper does that adequately, so I will suggest weak accept. Furthermore, this work could spark interesting future works and fruitful discussions at the ICLR. It is well-written and the experimental evaluation is adequate.\n\nI would suggest a couple of ways to possibly improve the exposition. The paper is somewhat notation heavy. When considering single layers, the superscript for the layer could be dropped in favor of clarity. I would suggest moving the definition of MASOs to the main text, and present Proposition 8 in some form in the main text as well. To a reader not familiar with previous works, or with splines, this could be helpful. Use of orthogonalization could be highlighted not just a tool for tractability but also regularization. For inference on GMMs, it corresponds to a type of variational inference, which could be mentioned. \n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper732/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "From Hard to Soft: Understanding Deep Network Nonlinearities via Vector Quantization and Statistical Inference", "abstract": "Nonlinearity is crucial to the performance of a deep (neural) network (DN).\nTo date there has been little progress understanding the menagerie of available  nonlinearities, but recently progress has been made on understanding the r\\^{o}le played by piecewise affine and convex nonlinearities like the ReLU and absolute value activation functions and max-pooling.\nIn particular, DN layers constructed from these operations can be interpreted as {\\em max-affine spline operators} (MASOs) that have an elegant link to vector quantization (VQ) and $K$-means.\nWhile this is good theoretical progress, the entire MASO approach is predicated on the requirement that the nonlinearities be piecewise affine and convex, which precludes important activation functions like the sigmoid, hyperbolic tangent, and softmax.\n{\\em This paper extends the MASO framework to these and an infinitely large class of new nonlinearities by linking deterministic MASOs with probabilistic Gaussian Mixture Models (GMMs).}\nWe show that, under a GMM, piecewise affine, convex nonlinearities like ReLU, absolute value, and max-pooling can be interpreted as solutions to certain natural ``hard'' VQ inference problems, while sigmoid, hyperbolic tangent, and softmax can be interpreted as solutions to corresponding ``soft'' VQ inference problems.\nWe further extend the framework by hybridizing the hard and soft VQ optimizations to create a $\\beta$-VQ inference that interpolates between hard, soft, and linear VQ inference.\nA prime example of a $\\beta$-VQ DN nonlinearity is the {\\em swish} nonlinearity, which offers state-of-the-art performance in a range of computer vision tasks but was developed ad hoc by experimentation.\nFinally, we validate with experiments an important assertion of our theory, namely that DN performance can be significantly improved by enforcing orthogonality in its linear filters.\n", "keywords": ["Spline", "Vector Quantization", "Inference", "Nonlinearities", "Deep Network"], "authorids": ["randallbalestriero@gmail.com", "richb@rice.edu"], "authors": ["Randall Balestriero", "Richard Baraniuk"], "TL;DR": "Reformulate deep networks nonlinearities from a vector quantization scope and bridge most known nonlinearities together.", "pdf": "/pdf/586d2cb1a01d67a4a95dc22e4ebf511b0aa52404.pdf", "paperhash": "balestriero|from_hard_to_soft_understanding_deep_network_nonlinearities_via_vector_quantization_and_statistical_inference", "_bibtex": "@inproceedings{\nbalestriero2018from,\ntitle={From Hard to Soft: Understanding Deep Network Nonlinearities via Vector Quantization and Statistical Inference},\nauthor={Randall Balestriero and Richard Baraniuk},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Syxt2jC5FX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper732/Official_Review", "cdate": 1542234388754, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "Syxt2jC5FX", "replyto": "Syxt2jC5FX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper732/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335790935, "tmdate": 1552335790935, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper732/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "SylBlmHqhX", "original": null, "number": 1, "cdate": 1541194476727, "ddate": null, "tcdate": 1541194476727, "tmdate": 1541533733244, "tddate": null, "forum": "Syxt2jC5FX", "replyto": "Syxt2jC5FX", "invitation": "ICLR.cc/2019/Conference/-/Paper732/Official_Review", "content": {"title": "This paper extends the max-affine spline operator (MISO) interpretation of a class of deep neural networks to cover a wider class of activation functions, namely the sigmoid, hyperbolic tangent and softmax. The authors also use the formulation to create a family of models that interpolates between hard and soft non-linearities. ", "review": "Interesting work, extending previous work by Balestriero and Baraniuk in a relevant and non-trivial direction. The presentation could be cleaner and clearer, \n\nThe paper contains solid work and contributes to an interesting perspective/interpretation of deep networks. The presentation is reasonably clear, although somewhat cluttered by a large number of subscripts and superscripts, which could be avoided by using a more modular formulation; e.g., in equation (1), when referring to a specific layer l, the superscript l can be dropped as it adds no useful information. By the way, when l is first used, just before equation (1), it is undefined, although the reader can guess what it stands for.\n\nIt is not clear why $[\\pi^{(l)}]_{k,t}$ is defined after equation (5), as these quantities are not mentioned in Theorem 2. Another confusion issue is that it is not clear if the assumption made in Proposition 1 concerning is only valid there of if it is assued to hold elsewhere in the paper.\n\nProposition 2 is simply a statement of the well-known relationship between between soft-max (a.k.a. logistic regression) and the maximum entropy principle (see, for example, http://www.win-vector.com/dfiles/LogisticRegressionMaxEnt.pdf).\n\n", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper732/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "From Hard to Soft: Understanding Deep Network Nonlinearities via Vector Quantization and Statistical Inference", "abstract": "Nonlinearity is crucial to the performance of a deep (neural) network (DN).\nTo date there has been little progress understanding the menagerie of available  nonlinearities, but recently progress has been made on understanding the r\\^{o}le played by piecewise affine and convex nonlinearities like the ReLU and absolute value activation functions and max-pooling.\nIn particular, DN layers constructed from these operations can be interpreted as {\\em max-affine spline operators} (MASOs) that have an elegant link to vector quantization (VQ) and $K$-means.\nWhile this is good theoretical progress, the entire MASO approach is predicated on the requirement that the nonlinearities be piecewise affine and convex, which precludes important activation functions like the sigmoid, hyperbolic tangent, and softmax.\n{\\em This paper extends the MASO framework to these and an infinitely large class of new nonlinearities by linking deterministic MASOs with probabilistic Gaussian Mixture Models (GMMs).}\nWe show that, under a GMM, piecewise affine, convex nonlinearities like ReLU, absolute value, and max-pooling can be interpreted as solutions to certain natural ``hard'' VQ inference problems, while sigmoid, hyperbolic tangent, and softmax can be interpreted as solutions to corresponding ``soft'' VQ inference problems.\nWe further extend the framework by hybridizing the hard and soft VQ optimizations to create a $\\beta$-VQ inference that interpolates between hard, soft, and linear VQ inference.\nA prime example of a $\\beta$-VQ DN nonlinearity is the {\\em swish} nonlinearity, which offers state-of-the-art performance in a range of computer vision tasks but was developed ad hoc by experimentation.\nFinally, we validate with experiments an important assertion of our theory, namely that DN performance can be significantly improved by enforcing orthogonality in its linear filters.\n", "keywords": ["Spline", "Vector Quantization", "Inference", "Nonlinearities", "Deep Network"], "authorids": ["randallbalestriero@gmail.com", "richb@rice.edu"], "authors": ["Randall Balestriero", "Richard Baraniuk"], "TL;DR": "Reformulate deep networks nonlinearities from a vector quantization scope and bridge most known nonlinearities together.", "pdf": "/pdf/586d2cb1a01d67a4a95dc22e4ebf511b0aa52404.pdf", "paperhash": "balestriero|from_hard_to_soft_understanding_deep_network_nonlinearities_via_vector_quantization_and_statistical_inference", "_bibtex": "@inproceedings{\nbalestriero2018from,\ntitle={From Hard to Soft: Understanding Deep Network Nonlinearities via Vector Quantization and Statistical Inference},\nauthor={Randall Balestriero and Richard Baraniuk},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Syxt2jC5FX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper732/Official_Review", "cdate": 1542234388754, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "Syxt2jC5FX", "replyto": "Syxt2jC5FX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper732/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335790935, "tmdate": 1552335790935, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper732/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 9}