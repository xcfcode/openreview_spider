{"notes": [{"id": "H1edEyBKDS", "original": "HyxSmT3uPB", "number": 1659, "cdate": 1569439535703, "ddate": null, "tcdate": 1569439535703, "tmdate": 1587603366427, "tddate": null, "forum": "H1edEyBKDS", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Plug and Play Language Models: A Simple Approach to Controlled Text Generation", "authors": ["Sumanth Dathathri", "Andrea Madotto", "Janice Lan", "Jane Hung", "Eric Frank", "Piero Molino", "Jason Yosinski", "Rosanne Liu"], "authorids": ["dathathris@gmail.com", "amadotto@connect.ust.hk", "lan.janice.j@gmail.com", "jane.hung@uber.com", "mysterefrank@uber.com", "piero@uber.com", "yosinski@uber.com", "rosanne@uber.com"], "keywords": ["controlled text generation", "generative models", "conditional generative models", "language modeling", "transformer"], "TL;DR": "We control the topic and sentiment of text generation (almost) without any training. ", "abstract": "Large transformer-based language models (LMs) trained on huge text corpora have shown unparalleled generation capabilities. However, controlling attributes of the generated language (e.g. switching topic or sentiment) is difficult without modifying the model architecture or fine-tuning on attribute-specific data and entailing the significant cost of retraining. We propose a simple alternative: the Plug and Play Language Model (PPLM) for controllable language generation, which combines a pretrained LM with one or more simple attribute classifiers that guide text generation without any further training of the LM. In the canonical scenario we present, the attribute models are simple classifiers consisting of a user-specified bag of words or a single learned layer with 100,000 times fewer parameters than the LM. Sampling entails a forward and backward pass in which gradients from the attribute model push the LM's hidden activations and thus guide the generation. Model samples demonstrate control over a range of topics and sentiment styles, and extensive automated and human annotated evaluations show attribute alignment and fluency. PPLMs are flexible in that any combination of differentiable attribute models may be used to steer text generation, which will allow for diverse and creative applications beyond the examples given in this paper.", "pdf": "/pdf/e5bf5d6bd5aa1cbd71c4bb0632cdee0bd9e2b130.pdf", "code": "https://github.com/uber-research/PPLM", "paperhash": "dathathri|plug_and_play_language_models_a_simple_approach_to_controlled_text_generation", "_bibtex": "@inproceedings{\nDathathri2020Plug,\ntitle={Plug and Play Language Models: A Simple Approach to Controlled Text Generation},\nauthor={Sumanth Dathathri and Andrea Madotto and Janice Lan and Jane Hung and Eric Frank and Piero Molino and Jason Yosinski and Rosanne Liu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=H1edEyBKDS}\n}", "original_pdf": "/attachment/e783532ae84827007d75d2865206ef14fcc544db.pdf"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 27, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "yMNRSl3cei", "original": null, "number": 22, "cdate": 1581453486162, "ddate": null, "tcdate": 1581453486162, "tmdate": 1581494777699, "tddate": null, "forum": "H1edEyBKDS", "replyto": "7bAtaBWJGZ", "invitation": "ICLR.cc/2020/Conference/Paper1659/-/Official_Comment", "content": {"title": "PPLM vs Fine-tuning", "comment": "Hi, \n\nThank you for the insightful questions. We clarify below. \n\n1. K and V are activations not weights of the model. Usually, with backprop you compute the gradient with respect to the weights of the model because you want to update. We don\u2019t update the weights but we update the activations, which are dynamically determined at each step by encoding the input.\n\n2. Fine-tuning implies having a pre-trained model that you update. Here the pre-trained LM is untouched, and the updated components are initialized at 0 (no pre-training).  \n\n3. For conventional fine-tuning, you increase the likelihood of a given set of sequences by updating your model weights. PPLM does not update the model or increase the likelihood of a set of sequences directly. "}, "signatures": ["ICLR.cc/2020/Conference/Paper1659/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1659/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Plug and Play Language Models: A Simple Approach to Controlled Text Generation", "authors": ["Sumanth Dathathri", "Andrea Madotto", "Janice Lan", "Jane Hung", "Eric Frank", "Piero Molino", "Jason Yosinski", "Rosanne Liu"], "authorids": ["dathathris@gmail.com", "amadotto@connect.ust.hk", "lan.janice.j@gmail.com", "jane.hung@uber.com", "mysterefrank@uber.com", "piero@uber.com", "yosinski@uber.com", "rosanne@uber.com"], "keywords": ["controlled text generation", "generative models", "conditional generative models", "language modeling", "transformer"], "TL;DR": "We control the topic and sentiment of text generation (almost) without any training. ", "abstract": "Large transformer-based language models (LMs) trained on huge text corpora have shown unparalleled generation capabilities. However, controlling attributes of the generated language (e.g. switching topic or sentiment) is difficult without modifying the model architecture or fine-tuning on attribute-specific data and entailing the significant cost of retraining. We propose a simple alternative: the Plug and Play Language Model (PPLM) for controllable language generation, which combines a pretrained LM with one or more simple attribute classifiers that guide text generation without any further training of the LM. In the canonical scenario we present, the attribute models are simple classifiers consisting of a user-specified bag of words or a single learned layer with 100,000 times fewer parameters than the LM. Sampling entails a forward and backward pass in which gradients from the attribute model push the LM's hidden activations and thus guide the generation. Model samples demonstrate control over a range of topics and sentiment styles, and extensive automated and human annotated evaluations show attribute alignment and fluency. PPLMs are flexible in that any combination of differentiable attribute models may be used to steer text generation, which will allow for diverse and creative applications beyond the examples given in this paper.", "pdf": "/pdf/e5bf5d6bd5aa1cbd71c4bb0632cdee0bd9e2b130.pdf", "code": "https://github.com/uber-research/PPLM", "paperhash": "dathathri|plug_and_play_language_models_a_simple_approach_to_controlled_text_generation", "_bibtex": "@inproceedings{\nDathathri2020Plug,\ntitle={Plug and Play Language Models: A Simple Approach to Controlled Text Generation},\nauthor={Sumanth Dathathri and Andrea Madotto and Janice Lan and Jane Hung and Eric Frank and Piero Molino and Jason Yosinski and Rosanne Liu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=H1edEyBKDS}\n}", "original_pdf": "/attachment/e783532ae84827007d75d2865206ef14fcc544db.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1edEyBKDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1659/Authors", "ICLR.cc/2020/Conference/Paper1659/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1659/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1659/Reviewers", "ICLR.cc/2020/Conference/Paper1659/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1659/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1659/Authors|ICLR.cc/2020/Conference/Paper1659/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504152760, "tmdate": 1576860532109, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1659/Authors", "ICLR.cc/2020/Conference/Paper1659/Reviewers", "ICLR.cc/2020/Conference/Paper1659/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1659/-/Official_Comment"}}}, {"id": "7bAtaBWJGZ", "original": null, "number": 9, "cdate": 1581437231682, "ddate": null, "tcdate": 1581437231682, "tmdate": 1581437231682, "tddate": null, "forum": "H1edEyBKDS", "replyto": "H1edEyBKDS", "invitation": "ICLR.cc/2020/Conference/Paper1659/-/Public_Comment", "content": {"title": "What is the difference of this method vs. back-propagation?", "comment": "I'm wondering since K and V of the transformer network is the target and \"shift the history H_t in the direction of the sum of two gradients\", what is the difference between actually back-propagating this gradient in the backward phase (step 2 in the illustration) and the method described? By backprop into the model, isn't that fine-tuning? "}, "signatures": ["~Zhiyu_Lin1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Zhiyu_Lin1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Plug and Play Language Models: A Simple Approach to Controlled Text Generation", "authors": ["Sumanth Dathathri", "Andrea Madotto", "Janice Lan", "Jane Hung", "Eric Frank", "Piero Molino", "Jason Yosinski", "Rosanne Liu"], "authorids": ["dathathris@gmail.com", "amadotto@connect.ust.hk", "lan.janice.j@gmail.com", "jane.hung@uber.com", "mysterefrank@uber.com", "piero@uber.com", "yosinski@uber.com", "rosanne@uber.com"], "keywords": ["controlled text generation", "generative models", "conditional generative models", "language modeling", "transformer"], "TL;DR": "We control the topic and sentiment of text generation (almost) without any training. ", "abstract": "Large transformer-based language models (LMs) trained on huge text corpora have shown unparalleled generation capabilities. However, controlling attributes of the generated language (e.g. switching topic or sentiment) is difficult without modifying the model architecture or fine-tuning on attribute-specific data and entailing the significant cost of retraining. We propose a simple alternative: the Plug and Play Language Model (PPLM) for controllable language generation, which combines a pretrained LM with one or more simple attribute classifiers that guide text generation without any further training of the LM. In the canonical scenario we present, the attribute models are simple classifiers consisting of a user-specified bag of words or a single learned layer with 100,000 times fewer parameters than the LM. Sampling entails a forward and backward pass in which gradients from the attribute model push the LM's hidden activations and thus guide the generation. Model samples demonstrate control over a range of topics and sentiment styles, and extensive automated and human annotated evaluations show attribute alignment and fluency. PPLMs are flexible in that any combination of differentiable attribute models may be used to steer text generation, which will allow for diverse and creative applications beyond the examples given in this paper.", "pdf": "/pdf/e5bf5d6bd5aa1cbd71c4bb0632cdee0bd9e2b130.pdf", "code": "https://github.com/uber-research/PPLM", "paperhash": "dathathri|plug_and_play_language_models_a_simple_approach_to_controlled_text_generation", "_bibtex": "@inproceedings{\nDathathri2020Plug,\ntitle={Plug and Play Language Models: A Simple Approach to Controlled Text Generation},\nauthor={Sumanth Dathathri and Andrea Madotto and Janice Lan and Jane Hung and Eric Frank and Piero Molino and Jason Yosinski and Rosanne Liu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=H1edEyBKDS}\n}", "original_pdf": "/attachment/e783532ae84827007d75d2865206ef14fcc544db.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1edEyBKDS", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504191620, "tmdate": 1576860565737, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper1659/Authors", "ICLR.cc/2020/Conference/Paper1659/Reviewers", "ICLR.cc/2020/Conference/Paper1659/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1659/-/Public_Comment"}}}, {"id": "FmVXgU2BYJ", "original": null, "number": 1, "cdate": 1576798729092, "ddate": null, "tcdate": 1576798729092, "tmdate": 1576800907441, "tddate": null, "forum": "H1edEyBKDS", "replyto": "H1edEyBKDS", "invitation": "ICLR.cc/2020/Conference/Paper1659/-/Decision", "content": {"decision": "Accept (Poster)", "comment": "This paper proposes a simple plug-and-play language model approach to the problem of controlled language generation. The problem is important and timely, and the approach is simple yet effective. Reviewers had some discussions whether  1) there is enough novelty, 2) evaluation task really shows effectiveness, and 3) this paper will inspire future research directions. \n\nAfter discussions of the above points, reviewers are leaning more positive, and I reflect their positive sentiment by recommending it to be accepted. I look forward to seeing this work presented at ICLR.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Plug and Play Language Models: A Simple Approach to Controlled Text Generation", "authors": ["Sumanth Dathathri", "Andrea Madotto", "Janice Lan", "Jane Hung", "Eric Frank", "Piero Molino", "Jason Yosinski", "Rosanne Liu"], "authorids": ["dathathris@gmail.com", "amadotto@connect.ust.hk", "lan.janice.j@gmail.com", "jane.hung@uber.com", "mysterefrank@uber.com", "piero@uber.com", "yosinski@uber.com", "rosanne@uber.com"], "keywords": ["controlled text generation", "generative models", "conditional generative models", "language modeling", "transformer"], "TL;DR": "We control the topic and sentiment of text generation (almost) without any training. ", "abstract": "Large transformer-based language models (LMs) trained on huge text corpora have shown unparalleled generation capabilities. However, controlling attributes of the generated language (e.g. switching topic or sentiment) is difficult without modifying the model architecture or fine-tuning on attribute-specific data and entailing the significant cost of retraining. We propose a simple alternative: the Plug and Play Language Model (PPLM) for controllable language generation, which combines a pretrained LM with one or more simple attribute classifiers that guide text generation without any further training of the LM. In the canonical scenario we present, the attribute models are simple classifiers consisting of a user-specified bag of words or a single learned layer with 100,000 times fewer parameters than the LM. Sampling entails a forward and backward pass in which gradients from the attribute model push the LM's hidden activations and thus guide the generation. Model samples demonstrate control over a range of topics and sentiment styles, and extensive automated and human annotated evaluations show attribute alignment and fluency. PPLMs are flexible in that any combination of differentiable attribute models may be used to steer text generation, which will allow for diverse and creative applications beyond the examples given in this paper.", "pdf": "/pdf/e5bf5d6bd5aa1cbd71c4bb0632cdee0bd9e2b130.pdf", "code": "https://github.com/uber-research/PPLM", "paperhash": "dathathri|plug_and_play_language_models_a_simple_approach_to_controlled_text_generation", "_bibtex": "@inproceedings{\nDathathri2020Plug,\ntitle={Plug and Play Language Models: A Simple Approach to Controlled Text Generation},\nauthor={Sumanth Dathathri and Andrea Madotto and Janice Lan and Jane Hung and Eric Frank and Piero Molino and Jason Yosinski and Rosanne Liu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=H1edEyBKDS}\n}", "original_pdf": "/attachment/e783532ae84827007d75d2865206ef14fcc544db.pdf"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "H1edEyBKDS", "replyto": "H1edEyBKDS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795711244, "tmdate": 1576800260410, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1659/-/Decision"}}}, {"id": "H1x4jhziFS", "original": null, "number": 1, "cdate": 1571658908107, "ddate": null, "tcdate": 1571658908107, "tmdate": 1574068275433, "tddate": null, "forum": "H1edEyBKDS", "replyto": "H1edEyBKDS", "invitation": "ICLR.cc/2020/Conference/Paper1659/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #2", "review": "The authors describe a method for training plug and play language models, a way to incorporate control elements into pre-trained LMs. In contrast to existing work, which often trains conditioned upon the control element, the authors emphasize that their method does not require re-training the initial LM. This is exciting and a great research direction. It is evaluated in a number of different settings.\n\n1. The authors claim that this method is a baseline for controlled text generation (see e.g. the title). However, there does not appear to be any evaluation with any existing work that performs controlled text generation. I don't see how this can be proposed as a baseline for controlled text generation is there is no comparison to other methods. I imagine the authors will emphasize that that's not fair - because their method doesn't require retraining the language model - but it is relevant to demonstrate if there is a gap in performance or not. As is, there is only one baseline- unconditional language model - and to me this is mostly a way to calibrate the evaluators and not a way to compare their model against other models. \n\n2. Can the authors make a point or discuss the relationship of this work to neural style transfer? Compared to unsupervised style transfer approaches, which also use lists of words or attributes to learn to dis-entangle content and style, what are the benefits of the proposed approach and how would it compare?\n\n3. Can the authors discuss the effectiveness of their control mechanism for less logical control settings? For example, what if there was \"religion\" for \"the potato\" prompt? Does the model still respect these settings, or no? \n\n4. Can the authors add analysis on how much the model respects the control variables? This is quite common in existing controlled generation papers. If the model is updated to have the control variables and then is not provided with one at test time, what happens? Can you also control very easy to measure attributes, such as length?\n\nThis question ties in with a general point I am ambivalent to in this paper- that it is very long, but there is very little analysis done on what makes the method work, why it is better than other control methods or control baselines, where the proposed control mechanism is not effective, how the model scales if there are large quantities of topics rather than just a few of them, if the BoW and discriminator attribute models work well together or if certain attributes are easier to learn than others, so the model focuses more on those when there are conflicts, etc\n\n5. Missing citations: \n\nPrevious work has investigated controlling various attributes of text generation. Several of these works have also controlled multiple attributes simultaneously. For example, here's a list of a few of the works that were missed:\n\nKikuchi et al 2016\nFicler and Goldberg, 2017\nWang et al, 2017\nFan et al, 2018\nBaheti et al, 2018\nSee et al, 2019\nMartin et al, 2019\n\nThe related work section only focuses on very recent work, e.g. only one paper is discussed amongst a large body of existing work. I feel this is not an accurate reflection of how much previous work has investigated these techniques and analyzed how models deal with control variables. \n\nPlease also cite:\n- which dataset was used for story generation, appears to be missing\n- top-k sampling \n\n\nI have read the author response. Thanks for the details and additional analysis in the paper. ", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper1659/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1659/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Plug and Play Language Models: A Simple Approach to Controlled Text Generation", "authors": ["Sumanth Dathathri", "Andrea Madotto", "Janice Lan", "Jane Hung", "Eric Frank", "Piero Molino", "Jason Yosinski", "Rosanne Liu"], "authorids": ["dathathris@gmail.com", "amadotto@connect.ust.hk", "lan.janice.j@gmail.com", "jane.hung@uber.com", "mysterefrank@uber.com", "piero@uber.com", "yosinski@uber.com", "rosanne@uber.com"], "keywords": ["controlled text generation", "generative models", "conditional generative models", "language modeling", "transformer"], "TL;DR": "We control the topic and sentiment of text generation (almost) without any training. ", "abstract": "Large transformer-based language models (LMs) trained on huge text corpora have shown unparalleled generation capabilities. However, controlling attributes of the generated language (e.g. switching topic or sentiment) is difficult without modifying the model architecture or fine-tuning on attribute-specific data and entailing the significant cost of retraining. We propose a simple alternative: the Plug and Play Language Model (PPLM) for controllable language generation, which combines a pretrained LM with one or more simple attribute classifiers that guide text generation without any further training of the LM. In the canonical scenario we present, the attribute models are simple classifiers consisting of a user-specified bag of words or a single learned layer with 100,000 times fewer parameters than the LM. Sampling entails a forward and backward pass in which gradients from the attribute model push the LM's hidden activations and thus guide the generation. Model samples demonstrate control over a range of topics and sentiment styles, and extensive automated and human annotated evaluations show attribute alignment and fluency. PPLMs are flexible in that any combination of differentiable attribute models may be used to steer text generation, which will allow for diverse and creative applications beyond the examples given in this paper.", "pdf": "/pdf/e5bf5d6bd5aa1cbd71c4bb0632cdee0bd9e2b130.pdf", "code": "https://github.com/uber-research/PPLM", "paperhash": "dathathri|plug_and_play_language_models_a_simple_approach_to_controlled_text_generation", "_bibtex": "@inproceedings{\nDathathri2020Plug,\ntitle={Plug and Play Language Models: A Simple Approach to Controlled Text Generation},\nauthor={Sumanth Dathathri and Andrea Madotto and Janice Lan and Jane Hung and Eric Frank and Piero Molino and Jason Yosinski and Rosanne Liu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=H1edEyBKDS}\n}", "original_pdf": "/attachment/e783532ae84827007d75d2865206ef14fcc544db.pdf"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "H1edEyBKDS", "replyto": "H1edEyBKDS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1659/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1659/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575938240702, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1659/Reviewers"], "noninvitees": [], "tcdate": 1570237734157, "tmdate": 1575938240714, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1659/-/Official_Review"}}}, {"id": "B1xorXFnjS", "original": null, "number": 14, "cdate": 1573847875072, "ddate": null, "tcdate": 1573847875072, "tmdate": 1573847891203, "tddate": null, "forum": "H1edEyBKDS", "replyto": "BJeY0tInor", "invitation": "ICLR.cc/2020/Conference/Paper1659/-/Official_Comment", "content": {"title": "Thanks for the quick response!", "comment": "Thanks for your quick response!\n\nWe agree with you that controlling for consistency in context is very interesting and in general, the lack of metrics for measuring content shift. \n\nThe current objective of this piece of work is controlled generation as studied in [1,2, 3, 4], as opposed to style transfer where retaining content is more of a concern. We do hope to extend PPLM into style transfer in the future, and also to applications such as NMT ( e.g. this could allow transforming a German to English translation model and a Twitter vs. Wikipedia classifier to translate German phrases into their English Twitter equivalents) where retaining content is extremely important.\n\nHowever, in the context of this paper, the factors we are evaluating for 1) can we steer text generation towards desired attributes, and 2) can we do this without degrading language fluency and diversity. We have used 2 types of human annotation (with thousands of labels collected), 4 kinds of automated measures, and evaluations from an additional, separately trained classifier. We believe this presents the most comprehensive evaluation we have seen so far in the literature for the task of controlled generation. We have also included controlled generation baselines as mentioned in our general response. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1659/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1659/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Plug and Play Language Models: A Simple Approach to Controlled Text Generation", "authors": ["Sumanth Dathathri", "Andrea Madotto", "Janice Lan", "Jane Hung", "Eric Frank", "Piero Molino", "Jason Yosinski", "Rosanne Liu"], "authorids": ["dathathris@gmail.com", "amadotto@connect.ust.hk", "lan.janice.j@gmail.com", "jane.hung@uber.com", "mysterefrank@uber.com", "piero@uber.com", "yosinski@uber.com", "rosanne@uber.com"], "keywords": ["controlled text generation", "generative models", "conditional generative models", "language modeling", "transformer"], "TL;DR": "We control the topic and sentiment of text generation (almost) without any training. ", "abstract": "Large transformer-based language models (LMs) trained on huge text corpora have shown unparalleled generation capabilities. However, controlling attributes of the generated language (e.g. switching topic or sentiment) is difficult without modifying the model architecture or fine-tuning on attribute-specific data and entailing the significant cost of retraining. We propose a simple alternative: the Plug and Play Language Model (PPLM) for controllable language generation, which combines a pretrained LM with one or more simple attribute classifiers that guide text generation without any further training of the LM. In the canonical scenario we present, the attribute models are simple classifiers consisting of a user-specified bag of words or a single learned layer with 100,000 times fewer parameters than the LM. Sampling entails a forward and backward pass in which gradients from the attribute model push the LM's hidden activations and thus guide the generation. Model samples demonstrate control over a range of topics and sentiment styles, and extensive automated and human annotated evaluations show attribute alignment and fluency. PPLMs are flexible in that any combination of differentiable attribute models may be used to steer text generation, which will allow for diverse and creative applications beyond the examples given in this paper.", "pdf": "/pdf/e5bf5d6bd5aa1cbd71c4bb0632cdee0bd9e2b130.pdf", "code": "https://github.com/uber-research/PPLM", "paperhash": "dathathri|plug_and_play_language_models_a_simple_approach_to_controlled_text_generation", "_bibtex": "@inproceedings{\nDathathri2020Plug,\ntitle={Plug and Play Language Models: A Simple Approach to Controlled Text Generation},\nauthor={Sumanth Dathathri and Andrea Madotto and Janice Lan and Jane Hung and Eric Frank and Piero Molino and Jason Yosinski and Rosanne Liu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=H1edEyBKDS}\n}", "original_pdf": "/attachment/e783532ae84827007d75d2865206ef14fcc544db.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1edEyBKDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1659/Authors", "ICLR.cc/2020/Conference/Paper1659/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1659/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1659/Reviewers", "ICLR.cc/2020/Conference/Paper1659/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1659/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1659/Authors|ICLR.cc/2020/Conference/Paper1659/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504152760, "tmdate": 1576860532109, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1659/Authors", "ICLR.cc/2020/Conference/Paper1659/Reviewers", "ICLR.cc/2020/Conference/Paper1659/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1659/-/Official_Comment"}}}, {"id": "BJeY0tInor", "original": null, "number": 13, "cdate": 1573837265368, "ddate": null, "tcdate": 1573837265368, "tmdate": 1573837265368, "tddate": null, "forum": "H1edEyBKDS", "replyto": "Sye1kV9qjB", "invitation": "ICLR.cc/2020/Conference/Paper1659/-/Official_Comment", "content": {"title": "Thank you for your rebuttal", "comment": "Thank you very much for your feedback on my reviews; really appreciate that.\n\nRegarding \"If you have any suggestions for other automatic evaluation metrics, we would be happy to consider including them.\", unfortunately, I don't have them, but I do think that measuring the content shifting among the generated texts could be useful. In this sense, we would be able to control the consistency of the targeted context or conversation. "}, "signatures": ["ICLR.cc/2020/Conference/Paper1659/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1659/AnonReviewer1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Plug and Play Language Models: A Simple Approach to Controlled Text Generation", "authors": ["Sumanth Dathathri", "Andrea Madotto", "Janice Lan", "Jane Hung", "Eric Frank", "Piero Molino", "Jason Yosinski", "Rosanne Liu"], "authorids": ["dathathris@gmail.com", "amadotto@connect.ust.hk", "lan.janice.j@gmail.com", "jane.hung@uber.com", "mysterefrank@uber.com", "piero@uber.com", "yosinski@uber.com", "rosanne@uber.com"], "keywords": ["controlled text generation", "generative models", "conditional generative models", "language modeling", "transformer"], "TL;DR": "We control the topic and sentiment of text generation (almost) without any training. ", "abstract": "Large transformer-based language models (LMs) trained on huge text corpora have shown unparalleled generation capabilities. However, controlling attributes of the generated language (e.g. switching topic or sentiment) is difficult without modifying the model architecture or fine-tuning on attribute-specific data and entailing the significant cost of retraining. We propose a simple alternative: the Plug and Play Language Model (PPLM) for controllable language generation, which combines a pretrained LM with one or more simple attribute classifiers that guide text generation without any further training of the LM. In the canonical scenario we present, the attribute models are simple classifiers consisting of a user-specified bag of words or a single learned layer with 100,000 times fewer parameters than the LM. Sampling entails a forward and backward pass in which gradients from the attribute model push the LM's hidden activations and thus guide the generation. Model samples demonstrate control over a range of topics and sentiment styles, and extensive automated and human annotated evaluations show attribute alignment and fluency. PPLMs are flexible in that any combination of differentiable attribute models may be used to steer text generation, which will allow for diverse and creative applications beyond the examples given in this paper.", "pdf": "/pdf/e5bf5d6bd5aa1cbd71c4bb0632cdee0bd9e2b130.pdf", "code": "https://github.com/uber-research/PPLM", "paperhash": "dathathri|plug_and_play_language_models_a_simple_approach_to_controlled_text_generation", "_bibtex": "@inproceedings{\nDathathri2020Plug,\ntitle={Plug and Play Language Models: A Simple Approach to Controlled Text Generation},\nauthor={Sumanth Dathathri and Andrea Madotto and Janice Lan and Jane Hung and Eric Frank and Piero Molino and Jason Yosinski and Rosanne Liu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=H1edEyBKDS}\n}", "original_pdf": "/attachment/e783532ae84827007d75d2865206ef14fcc544db.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1edEyBKDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1659/Authors", "ICLR.cc/2020/Conference/Paper1659/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1659/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1659/Reviewers", "ICLR.cc/2020/Conference/Paper1659/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1659/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1659/Authors|ICLR.cc/2020/Conference/Paper1659/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504152760, "tmdate": 1576860532109, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1659/Authors", "ICLR.cc/2020/Conference/Paper1659/Reviewers", "ICLR.cc/2020/Conference/Paper1659/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1659/-/Official_Comment"}}}, {"id": "Bkxgl5Toir", "original": null, "number": 11, "cdate": 1573800424422, "ddate": null, "tcdate": 1573800424422, "tmdate": 1573800477737, "tddate": null, "forum": "H1edEyBKDS", "replyto": "H1edEyBKDS", "invitation": "ICLR.cc/2020/Conference/Paper1659/-/Official_Comment", "content": {"title": "General Response", "comment": "\nDear Reviewers, \n\nThank you for your comments helping us improve the paper! We appreciate the time/effort you all have taken in reviewing our paper carefully and giving insightful feedback. We have updated a great deal of our paper thanks to your feedback and to address the concerns raised. To summarize, the main changes are:\na) Added strong baselines: CTRL (a conditional language model with 1.6B parameters; https://arxiv.org/abs/1909.05858), and GPT2-FT-RL (a pre-trained 774M GPT-2 model fine-tuned for positivity; https://arxiv.org/abs/1909.08593), as well as Weighted Decoding, a more direct conditioning approach suggested by Reviewer #3. \nb) Performed full evaluations on all those baselines, including over 2000 additional human annotations. \nc) Found that PPLM outperforms GPT2-FT-RL and performs comparably with CTRL, even though both of them are trained specifically for conditioned generation, and are over 4 times and twice of our model size (and 5 orders of magnitude larger than our attribute models). Further, PPLM outperforms Weighted Decoding significantly on both topic and sentiment control. \nd) Extended related work section where neural style transfer, weighted decoding, and most recent work on controlled generation are included. We thank you for all the suggested references!\ne) Extended analysis for settings in which PPLM succeeds and fails. Thanks to Reviewer #2 for the suggestions! \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1659/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1659/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Plug and Play Language Models: A Simple Approach to Controlled Text Generation", "authors": ["Sumanth Dathathri", "Andrea Madotto", "Janice Lan", "Jane Hung", "Eric Frank", "Piero Molino", "Jason Yosinski", "Rosanne Liu"], "authorids": ["dathathris@gmail.com", "amadotto@connect.ust.hk", "lan.janice.j@gmail.com", "jane.hung@uber.com", "mysterefrank@uber.com", "piero@uber.com", "yosinski@uber.com", "rosanne@uber.com"], "keywords": ["controlled text generation", "generative models", "conditional generative models", "language modeling", "transformer"], "TL;DR": "We control the topic and sentiment of text generation (almost) without any training. ", "abstract": "Large transformer-based language models (LMs) trained on huge text corpora have shown unparalleled generation capabilities. However, controlling attributes of the generated language (e.g. switching topic or sentiment) is difficult without modifying the model architecture or fine-tuning on attribute-specific data and entailing the significant cost of retraining. We propose a simple alternative: the Plug and Play Language Model (PPLM) for controllable language generation, which combines a pretrained LM with one or more simple attribute classifiers that guide text generation without any further training of the LM. In the canonical scenario we present, the attribute models are simple classifiers consisting of a user-specified bag of words or a single learned layer with 100,000 times fewer parameters than the LM. Sampling entails a forward and backward pass in which gradients from the attribute model push the LM's hidden activations and thus guide the generation. Model samples demonstrate control over a range of topics and sentiment styles, and extensive automated and human annotated evaluations show attribute alignment and fluency. PPLMs are flexible in that any combination of differentiable attribute models may be used to steer text generation, which will allow for diverse and creative applications beyond the examples given in this paper.", "pdf": "/pdf/e5bf5d6bd5aa1cbd71c4bb0632cdee0bd9e2b130.pdf", "code": "https://github.com/uber-research/PPLM", "paperhash": "dathathri|plug_and_play_language_models_a_simple_approach_to_controlled_text_generation", "_bibtex": "@inproceedings{\nDathathri2020Plug,\ntitle={Plug and Play Language Models: A Simple Approach to Controlled Text Generation},\nauthor={Sumanth Dathathri and Andrea Madotto and Janice Lan and Jane Hung and Eric Frank and Piero Molino and Jason Yosinski and Rosanne Liu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=H1edEyBKDS}\n}", "original_pdf": "/attachment/e783532ae84827007d75d2865206ef14fcc544db.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1edEyBKDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1659/Authors", "ICLR.cc/2020/Conference/Paper1659/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1659/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1659/Reviewers", "ICLR.cc/2020/Conference/Paper1659/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1659/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1659/Authors|ICLR.cc/2020/Conference/Paper1659/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504152760, "tmdate": 1576860532109, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1659/Authors", "ICLR.cc/2020/Conference/Paper1659/Reviewers", "ICLR.cc/2020/Conference/Paper1659/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1659/-/Official_Comment"}}}, {"id": "BJgpGJ7ooH", "original": null, "number": 10, "cdate": 1573756692904, "ddate": null, "tcdate": 1573756692904, "tmdate": 1573756692904, "tddate": null, "forum": "H1edEyBKDS", "replyto": "H1lsWy7ssS", "invitation": "ICLR.cc/2020/Conference/Paper1659/-/Official_Comment", "content": {"title": "Response 2/2", "comment": ">> 4. Can the authors add analysis on how much the model respects the control variables? This is quite common in existing controlled generation papers. If the model is updated to have the control variables and then is not provided with one at test time, what happens? \n\nOur human/automatic evaluation in Tables 4, 6  (and Tables S8, S9 in Supplementary Information) reflect attribute relevance. We find that it is comparable with other state-of-the-art approaches that have been trained for the task of controlled generation. In Tables 4, 6, the entries corresponding to the method \u2018B\u2019 are those where samples are generated with an uncontrolled language model (original GPT-2) -- comparing \u2018B\u2019 with \u2018BR\u2019, \u2018BC\u2019 and \u2018BCR\u2019 provides an understanding of the extent of language control provided by our proposed method.\n\n\n>>Can you also control very easy to measure attributes, such as length?\n\nIt would definitely be possible to optimize for easily measurable attributes such as a length of sentence  by building an attribute model that can predict the probability of attribute p(a|x). Beyond that it would be a direct application of the methods developed in our paper.\n\n>> why it is better than other control methods or control baselines\n\nThe biggest difference is that PPLM do not train LM at all, in comparison to fine-tuning/training an LM with desired control attributes. The amount of compute is hence negligible. Therefore, we do not need domain specific annotated data -- for instance, CTRL uses data from specific subreddits to train a controlled language model. To finetune an LM to be positive, [5] had to first get human annotations on generated samples, and then fine-tune to the LM. In contrast, we can take a simple out of domain dataset such as a SST (which is about movie reviews) or a bag of words, and generate positive- or negative-controlled passages about arbitrary topics such as a chicken, potatoes, lakes, horses or paintings.\n\nLastly, both [4, 5] do not provide the fine-grained control PPLM does (See Table S17 for an illustration), or the flexibility (that PPLM can always turn the control to zero to fully recover the original LM).\n\n>> where the proposed control mechanism is not effective\n\nWe can gain some insight into this question from Tables S8, S9, and Figure S3, S4.  We look at controllability for different topics (Table S8) and we see that the controllability varies quite a bit by topic. It is significantly easier to control for commonly occurring topics in the training data for GPT-2 such as \u2018religion\u2019, \u2018politics\u2019, \u2018science\u2019 as opposed to rarer topics such as a \u2018legal\u2019 or \u2018space\u2019. Based on human annotations, by default, GPT-2 generates most sentences on \u2018politics\u2019 and \u2018science\u2019 in comparison to \u2018space\u2019 or \u2018legal\u2019. During our experiments, we found that rarer topics such as \u2018Fantasy\u2019 are far more difficult to control while retaining fluency as opposed to topics such as \u2018science\u2019. We have included this discussion (Section 4.2). See Tables S8 and S9 for details.\n\n\n>> 5. Missing citations\n\nThank you for these references. \n\nWe have restructured the paper to include the extended related work in the main text, and have updated the references to include the ones you suggested.\n\n>> Please also cite:\n- which dataset was used for story generation \u2026. missing\nWe use an improv sketch as the skeleton for story generation. Beyond that we do not use any dataset to train the model -- the underlying model is GPT-2 and the attribute models are the same as the ones in the other sections of the paper.\n\n- top-k sampling\n  We added the citation (Section 4). \n\n[1] Hafez: an Interactive Poetry Generation System, Ghazvininejad\u2019 et. al., ACL\u20192017\n[2] Multiple-Attribute Text Rewriting, Lample et. al., ICLR\u201919\n[3] What makes a good conversation? How controllable attributes affect human judgments, See et. al., NAACL\u201919\n[4]  CTRL: A Conditional Transformer Language Model for Controllable Generation, Keskar et. al., 2019\n[5] Fine-Tuning Language Models from Human Preferences, Ziegler et. al, 2019\n[6] Hierarchical Neural Story Generation, Fan et. al., ACL\u201918\n[7] Towards controlled text generation,Hu et. al., ICML\u201817\n[8] Controlling Linguistic Style Aspects in Neural Language Generation, Ficler et. al, 2017\n[9] Style Transfer from Non-Parallel Text by Cross-Alignment, Shen et.al., NeurIPS\u201917"}, "signatures": ["ICLR.cc/2020/Conference/Paper1659/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1659/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Plug and Play Language Models: A Simple Approach to Controlled Text Generation", "authors": ["Sumanth Dathathri", "Andrea Madotto", "Janice Lan", "Jane Hung", "Eric Frank", "Piero Molino", "Jason Yosinski", "Rosanne Liu"], "authorids": ["dathathris@gmail.com", "amadotto@connect.ust.hk", "lan.janice.j@gmail.com", "jane.hung@uber.com", "mysterefrank@uber.com", "piero@uber.com", "yosinski@uber.com", "rosanne@uber.com"], "keywords": ["controlled text generation", "generative models", "conditional generative models", "language modeling", "transformer"], "TL;DR": "We control the topic and sentiment of text generation (almost) without any training. ", "abstract": "Large transformer-based language models (LMs) trained on huge text corpora have shown unparalleled generation capabilities. However, controlling attributes of the generated language (e.g. switching topic or sentiment) is difficult without modifying the model architecture or fine-tuning on attribute-specific data and entailing the significant cost of retraining. We propose a simple alternative: the Plug and Play Language Model (PPLM) for controllable language generation, which combines a pretrained LM with one or more simple attribute classifiers that guide text generation without any further training of the LM. In the canonical scenario we present, the attribute models are simple classifiers consisting of a user-specified bag of words or a single learned layer with 100,000 times fewer parameters than the LM. Sampling entails a forward and backward pass in which gradients from the attribute model push the LM's hidden activations and thus guide the generation. Model samples demonstrate control over a range of topics and sentiment styles, and extensive automated and human annotated evaluations show attribute alignment and fluency. PPLMs are flexible in that any combination of differentiable attribute models may be used to steer text generation, which will allow for diverse and creative applications beyond the examples given in this paper.", "pdf": "/pdf/e5bf5d6bd5aa1cbd71c4bb0632cdee0bd9e2b130.pdf", "code": "https://github.com/uber-research/PPLM", "paperhash": "dathathri|plug_and_play_language_models_a_simple_approach_to_controlled_text_generation", "_bibtex": "@inproceedings{\nDathathri2020Plug,\ntitle={Plug and Play Language Models: A Simple Approach to Controlled Text Generation},\nauthor={Sumanth Dathathri and Andrea Madotto and Janice Lan and Jane Hung and Eric Frank and Piero Molino and Jason Yosinski and Rosanne Liu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=H1edEyBKDS}\n}", "original_pdf": "/attachment/e783532ae84827007d75d2865206ef14fcc544db.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1edEyBKDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1659/Authors", "ICLR.cc/2020/Conference/Paper1659/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1659/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1659/Reviewers", "ICLR.cc/2020/Conference/Paper1659/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1659/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1659/Authors|ICLR.cc/2020/Conference/Paper1659/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504152760, "tmdate": 1576860532109, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1659/Authors", "ICLR.cc/2020/Conference/Paper1659/Reviewers", "ICLR.cc/2020/Conference/Paper1659/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1659/-/Official_Comment"}}}, {"id": "H1lsWy7ssS", "original": null, "number": 9, "cdate": 1573756674570, "ddate": null, "tcdate": 1573756674570, "tmdate": 1573756674570, "tddate": null, "forum": "H1edEyBKDS", "replyto": "H1x4jhziFS", "invitation": "ICLR.cc/2020/Conference/Paper1659/-/Official_Comment", "content": {"title": "Response 1/2", "comment": ">>In contrast to existing work, which often trains conditioned upon the control element, the authors emphasize that their method does not require re-training the initial LM. This is exciting and a great research direction.\n\nWe are glad you like the research direction!\n\n>> 1. The authors \u2026. evaluation with any existing work that performs controlled text generation.\n\nThank you for the suggestions. We have included the following baselines: i) Weighted Decoding [1], ii) CTRL (a conditional language model trained for controlled text generation), and iii) a fine-tuned GPT-2 language model. Despite, CTRL being trained for the task (and with over 4 times as many parameters) and GPT-2 being fine-tuned for the task (and with over twice as many parameters), we perform comparably with CTRL and outperform the fine-tuned GPT-2 based on human-evaluation/automated evaluation. We also clearly outperform the more direct approach of weighted decoding proposed [1] (also, used in [3]). See  Tables 4, 5 for updated results and Section S7 for baseline details.\n\n>> 2. Can\u2026 discuss the relationship of this work to neural style transfer? Compared to unsupervised style transfer approaches \u2026 what are the benefits of the proposed approach and how would it compare?\n\nWe have moved the discussion of neural style transfer from supplementary information section to Section 2 Related Work. Thanks for your suggestion.\n\nBenefits of PPLM over style transfer: \n-- Most style transfer approaches [2] require training a seq2seq model from scratch and it is also not possible to plug in new attributes that were not considered during training.\n\n\n-- Further, there are many domains outside style transfer where it is useful to control style -- for example, story writing [6], dialogue systems [3], where approaches from current work on style transfer are not directly applicable. We believe PPLM would be directly applicable to any transformer based generative model in all these domains. \n\n-- In contrast to current approaches for unsupervised neural style transfer [2, 9], our approach allows for fine-grained control (e.g. How positive do we want our LM to be?).\n\n-- We also note that controlled/stylized generation itself is a well studied problem [4, 5, 6, 7, 8], and there are merits to generating text in a controlled manner outside of the style transfer setting.\n\n>> 3. Can the authors discuss the effectiveness of their control mechanism for less logical control settings? .. \"religion\" for \"the potato\" prompt? .. still respect these settings, or no? \n\nThis is a great idea! We\u2019ve added examples of how PPLM responds to the following odd or illogical topic-prefix combinations. The experiment is described in Section S9 and we list samples from various combinations in Tables S10-S16. The conclusion of this experiment is that PPLM can handle those odd settings as well. For example, a sample from \u201cThe potato\u201d + \u201cReligion\u201d is as follows: \n\n=== Sample 1 ====\nThe potato, an ancient food, is considered a sacred plant by many Hindus. However, some Hindus believe that the potatoes are the seed of a demon. ...\n\n\"In India we have the Hindu god Vishnu, Vish, the God. He has come to the world,\" said a woman in Mumbai.\n\n\n\"He came to the world because of God. God came to the world to save people from the curse of the devil God. God came to save us from the curse of the devil,\"\n=== end of Sample 1 ====\n\n=== Sample 2 ====\nThe potato salad that I have recently been making for our family is so good, I wanted to share it with you guys. This was my first attempt at a Potato Salad recipe, and I love it. It also reminds me why I love cooking. I love\n how good it tastes and how it reminds you why you love Cooking with God. I love how it is a great way to celebrate Thanksgiving and Christmas. It also reminds me why I am a Christian. I love how it reminds me why I love to\n=== end of Sample 2 ====\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1659/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1659/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Plug and Play Language Models: A Simple Approach to Controlled Text Generation", "authors": ["Sumanth Dathathri", "Andrea Madotto", "Janice Lan", "Jane Hung", "Eric Frank", "Piero Molino", "Jason Yosinski", "Rosanne Liu"], "authorids": ["dathathris@gmail.com", "amadotto@connect.ust.hk", "lan.janice.j@gmail.com", "jane.hung@uber.com", "mysterefrank@uber.com", "piero@uber.com", "yosinski@uber.com", "rosanne@uber.com"], "keywords": ["controlled text generation", "generative models", "conditional generative models", "language modeling", "transformer"], "TL;DR": "We control the topic and sentiment of text generation (almost) without any training. ", "abstract": "Large transformer-based language models (LMs) trained on huge text corpora have shown unparalleled generation capabilities. However, controlling attributes of the generated language (e.g. switching topic or sentiment) is difficult without modifying the model architecture or fine-tuning on attribute-specific data and entailing the significant cost of retraining. We propose a simple alternative: the Plug and Play Language Model (PPLM) for controllable language generation, which combines a pretrained LM with one or more simple attribute classifiers that guide text generation without any further training of the LM. In the canonical scenario we present, the attribute models are simple classifiers consisting of a user-specified bag of words or a single learned layer with 100,000 times fewer parameters than the LM. Sampling entails a forward and backward pass in which gradients from the attribute model push the LM's hidden activations and thus guide the generation. Model samples demonstrate control over a range of topics and sentiment styles, and extensive automated and human annotated evaluations show attribute alignment and fluency. PPLMs are flexible in that any combination of differentiable attribute models may be used to steer text generation, which will allow for diverse and creative applications beyond the examples given in this paper.", "pdf": "/pdf/e5bf5d6bd5aa1cbd71c4bb0632cdee0bd9e2b130.pdf", "code": "https://github.com/uber-research/PPLM", "paperhash": "dathathri|plug_and_play_language_models_a_simple_approach_to_controlled_text_generation", "_bibtex": "@inproceedings{\nDathathri2020Plug,\ntitle={Plug and Play Language Models: A Simple Approach to Controlled Text Generation},\nauthor={Sumanth Dathathri and Andrea Madotto and Janice Lan and Jane Hung and Eric Frank and Piero Molino and Jason Yosinski and Rosanne Liu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=H1edEyBKDS}\n}", "original_pdf": "/attachment/e783532ae84827007d75d2865206ef14fcc544db.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1edEyBKDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1659/Authors", "ICLR.cc/2020/Conference/Paper1659/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1659/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1659/Reviewers", "ICLR.cc/2020/Conference/Paper1659/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1659/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1659/Authors|ICLR.cc/2020/Conference/Paper1659/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504152760, "tmdate": 1576860532109, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1659/Authors", "ICLR.cc/2020/Conference/Paper1659/Reviewers", "ICLR.cc/2020/Conference/Paper1659/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1659/-/Official_Comment"}}}, {"id": "Sye1kV9qjB", "original": null, "number": 8, "cdate": 1573721047495, "ddate": null, "tcdate": 1573721047495, "tmdate": 1573721201379, "tddate": null, "forum": "H1edEyBKDS", "replyto": "HkegjQxaFH", "invitation": "ICLR.cc/2020/Conference/Paper1659/-/Official_Comment", "content": {"title": "Response", "comment": ">> The proposed method is simple and makes sense to me... is very neat here. However, I have two main concerns, as follows.\n\nWe thank you for your comments helping us improve the paper. We address your comments below.\n\n>> \"1. The main focuses of the generated text seem to be dramatically changed in an unpredictable way while tailoring the control attributes. In this sense, how useful these kinds of text generation techniques are not clear to me. .... Is there an automatic evaluation metric to subjectively evaluate the change of the focuses/ideas of two pieces of text?\"\n\nThis is certainly the case. In our work, two samples from either an LM distribution p(x) or a controlled LM p(x|a) are independent. The task being studied is controlled generation as opposed to style transfer, the latter scenario in which one aims to retain content but adjust style. Our goal is not to control the language so that the idea being conveyed is retained.\n\nAlthough it would be great if our model could accomplish both feats, we would like to note that controlled generation on its own is an actively studied problem in the language community. Recently several approaches have been proposed towards solving the problem of open-ended controlled generation where the goal is to only generate language with specific attributes without controlling for context, for example, the following papers: [1], [2], [3], [4], [5]. Another paper, [6], showed the benefits of language control (without directly controlling the idea being conveyed) on human judgement of the quality of engagement during interaction with a dialogue agent. We also note that the PPGN model in the paper inspiring this work does not control for deviation in context, but rather only controls for the generated image having the desired attribute (i.e. PPGN and PPLM both perform \u201ccontrolled generation\u201d but not \u201cstyle transfer\u201d).\n\nFor the open-ended controlled generation task (such as studied in [1,2,3,4]), we consider several possible automatic and human evaluation metrics, including perplexity, dist scores, human fluency and attribute relevance scores. If you have any suggestions for other automatic evaluation metrics, we would be happy to consider including them.\n\n>> \"2. The model is a straightforward adaptation of the Plug and Play Generative Networks from the vision community.\"\n\nWe respectfully disagree that the adaptation was straightforward. While we would have been happy to apply the PPGN approach directly to the language domain, the adaptation actually required several modifications, summarized as follows:\nPPGN:\n-- A graphical model depiction of the network looks like this: h -> x -> y, where h is a latent code, x is an image, and y is a class or attribute.\n-- A single h generates an entire, single image x.\nh and x are both continuous, and the gradient w.r.t. y passes through x to h.\n-- The Markov chain is run in h space, with a separate p(h) model being trained and used to ensure h does not drift too far from high probability regions.\n-- Multiple steps are taken in h space, corresponding to multiple entire images.\n-- Noise is added in h space to obtain the correct diversity of images.\nPPLM:\n-- A graphical model depiction of the network looks like this: [x1 -> (h1, x2) -> (h2, x3), \u2026 ] -> y, h_t and x_t are the latents and byte-pairs at time t and y is an attribute.\n-- A single h generates a distribution over sentences x.\nh is continuous and x is discrete, and gradient w.r.t. y passes directly to h, with discrete x skipped, except in the distribution propagation approach in Sec 4.3, which propagates through the single word x_t+1 (\u201cInstead, as in -- Dai et al. (2019a), we use the distribution\u2026\u201d).\n-- A complete Markov chain is not run, as this would require multiple full forward and backward passes through the transformer. Instead, we update only a sliding window of the recent past of h and sample only one word at a time. This is a compromise between speed and quality of the samples. The particular dependency structure of the transformer allows us to update only the past (key, value) pairs, which also allows for efficient sampling.\n-- Multiple steps are taken in h space as the sentence is constructed word by word. Multiple entire sentences are never produced.\n-- Noise is added via the sampling of each word in x space to obtain the correct diversity of sentences.\n\n[1] CTRL: A Conditional Transformer Language Model for Controllable Generation, Keskar et al., https://arxiv.org/abs/1909.05858\n[2] Fine-Tuning Language Models from Human Preferences, Ziegler et al., https://arxiv.org/abs/1909.08593\n[3] Towards controlled text generation, Hu et al., https://arxiv.org/abs/1703.00955\n[4] Controlling Linguistic Style Aspects in Neural Language Generation, Ficler et al.,  https://arxiv.org/abs/1707.02633\n[5] Towards Controllable Story Generation, Peng et al.\n[6] What makes a good conversation? How controllable attributes affect human judgments,See et al., NAACL\u201919"}, "signatures": ["ICLR.cc/2020/Conference/Paper1659/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1659/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Plug and Play Language Models: A Simple Approach to Controlled Text Generation", "authors": ["Sumanth Dathathri", "Andrea Madotto", "Janice Lan", "Jane Hung", "Eric Frank", "Piero Molino", "Jason Yosinski", "Rosanne Liu"], "authorids": ["dathathris@gmail.com", "amadotto@connect.ust.hk", "lan.janice.j@gmail.com", "jane.hung@uber.com", "mysterefrank@uber.com", "piero@uber.com", "yosinski@uber.com", "rosanne@uber.com"], "keywords": ["controlled text generation", "generative models", "conditional generative models", "language modeling", "transformer"], "TL;DR": "We control the topic and sentiment of text generation (almost) without any training. ", "abstract": "Large transformer-based language models (LMs) trained on huge text corpora have shown unparalleled generation capabilities. However, controlling attributes of the generated language (e.g. switching topic or sentiment) is difficult without modifying the model architecture or fine-tuning on attribute-specific data and entailing the significant cost of retraining. We propose a simple alternative: the Plug and Play Language Model (PPLM) for controllable language generation, which combines a pretrained LM with one or more simple attribute classifiers that guide text generation without any further training of the LM. In the canonical scenario we present, the attribute models are simple classifiers consisting of a user-specified bag of words or a single learned layer with 100,000 times fewer parameters than the LM. Sampling entails a forward and backward pass in which gradients from the attribute model push the LM's hidden activations and thus guide the generation. Model samples demonstrate control over a range of topics and sentiment styles, and extensive automated and human annotated evaluations show attribute alignment and fluency. PPLMs are flexible in that any combination of differentiable attribute models may be used to steer text generation, which will allow for diverse and creative applications beyond the examples given in this paper.", "pdf": "/pdf/e5bf5d6bd5aa1cbd71c4bb0632cdee0bd9e2b130.pdf", "code": "https://github.com/uber-research/PPLM", "paperhash": "dathathri|plug_and_play_language_models_a_simple_approach_to_controlled_text_generation", "_bibtex": "@inproceedings{\nDathathri2020Plug,\ntitle={Plug and Play Language Models: A Simple Approach to Controlled Text Generation},\nauthor={Sumanth Dathathri and Andrea Madotto and Janice Lan and Jane Hung and Eric Frank and Piero Molino and Jason Yosinski and Rosanne Liu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=H1edEyBKDS}\n}", "original_pdf": "/attachment/e783532ae84827007d75d2865206ef14fcc544db.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1edEyBKDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1659/Authors", "ICLR.cc/2020/Conference/Paper1659/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1659/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1659/Reviewers", "ICLR.cc/2020/Conference/Paper1659/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1659/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1659/Authors|ICLR.cc/2020/Conference/Paper1659/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504152760, "tmdate": 1576860532109, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1659/Authors", "ICLR.cc/2020/Conference/Paper1659/Reviewers", "ICLR.cc/2020/Conference/Paper1659/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1659/-/Official_Comment"}}}, {"id": "B1ljnf5csH", "original": null, "number": 7, "cdate": 1573720755446, "ddate": null, "tcdate": 1573720755446, "tmdate": 1573720835840, "tddate": null, "forum": "H1edEyBKDS", "replyto": "rklLgxopKB", "invitation": "ICLR.cc/2020/Conference/Paper1659/-/Official_Comment", "content": {"title": "Response to review", "comment": "Thank you for your comments helping us improve the paper!\n\n>> \u201cfollowing the framework known in NLP as noisy-channel modeling \u2026. this connection (they should!).\u201d\n\nThanks for pointing out references to noisy-channel modeling. We have added a mention of the noisy channel modeling approach to our related work section and have discussed how that approach compares to PPLM (Section 2).\n\n>> \u201cI find this approach interesting and like the paper overall.\u201d\nThanks!\n\n>> \u201cHowever \u2026 do not compare to more direct ways of integrating the conditional ... expect the proposed approach to work better (or at least differently) but it would be interesting to see it confirmed. ....  will be no increase in the probability of generating relevant words before the first seed word is generated.\u201d\n\nThank you for these great suggestions. We\u2019ve updated the paper to include this approach both for PPLM-BoW and PPLM-Discrim models:\nFor PPLM-BoW: this corresponds to an existing approach referred to in literature as \u201cWeighted Decoding\u201d (https://www.aclweb.org/anthology/P17-4008/, See et al., NAACL\u201919).\nFor PPLM-Discrim: for each token in the vocabulary we compute p(y=desired sentiment | x) and sample from the distribution p(x)*p(y=sentiment|x) with top-k=5. While the forward passes over the vocabulary are extremely expensive (e.g. 50000x), we get a sense of how well PPLM compares with a direct integration of the condition. We have included both sets of results in the paper (Table 3 and Table 6, row \u201cWD\u201d), where we find, as you presumed, that it does not work quite as well as PPLM.\n\nPPLM works better than directly integrating the conditioning into the decoding procedure. For the bag of words, we can further confirm the observation that the probability of generating relevant words before the first seed word from the bag does not increase. Another key difference is that the semantics of the bag of words are not captured, rather the decoder chooses to pick one of the words that fits context. For instance, a generated sample when conditioned on the prefix \u201cOnce upon a time\u201d with the \u201cSpace\u201d bag of words is \u201cI used to have a pretty good idea what a starfish was. I was a starfish biologist.\u201d.  See Section 4.2 for details.\n\nFor the sentiment control task, we find that this results in a lot of adversarial samples. Sequences often have a high attribute likelihood under the discriminator used during decoding but do not possess the attribute under human evaluation/external classifier evaluation. \n\n>> \u201cAnother limitation is the lack of comparison to standard controlled generation.... fine-tuning off-the-shelf pretrained decoders.\u201d\n\nThanks for the suggestion! We have updated the paper to include comparisons with a recent conditional LM (CTRL, https://arxiv.org/abs/1909.05858) and a GPT-2 LM fine-tuned for positivity with RL and human preferences (https://arxiv.org/abs/1909.08593). The details of the set-up can be found in the Section S7, particuarly, S7.1 and S7.2. We find PPLM performs comparably with CTRL on sentiment control (Table 6) and (perhaps surprisingly) outperforms CTRL on topic control (Table 3). PPLM also significantly outperforms the fine-tuned GPT-2 model on the sentiment task. In all of the above cases, PPLM is at least as fluent or more fluent than the baselines (CTRL & fine-tuned GPT-2). This is impressive considering that the fine-tuned GPT-2 model has over twice as many parameters, and the CTRL conditional language model has over 4 times as many parameters and are specifically tuned/trained for controlled gen.\n\n>>\u201dThere .. interesting relation to the NIPS 2019 paper \u2026 'steerability' rather ... controlled-generation model.\u201d\n\nThanks for the interesting connection! We have included this (Section 2, Page 3).\n\n>> \u201cGiven that style-controlled \u2026 can push this approach ... pretrained conditional LMs?\u201d\n\nWe believe the PPLM approach should scale well to any method with a Transformer based decoder, including potentially the application you describe, or NMT or Dialogue systems, where See et al.\u201919 showed the utility of being able to control the response in dialogue systems. Just as PPLM allows one to combine a p(x) model and p(a|x) model to generate samples from p(x|a), in the NMT scenario one could combine a base translation model p(x_target | x_source) with a p(a | x_target, x_source) to generate samples from p(x_target | a, x_source). E.g. this could allow transforming a German to English translation model and a Twitter vs. Wikipedia classifier to translate German phrases into their English Twitter equivalents!\n\nThese are some of the immediate next steps we plan on exploring!\n\n>>\u201dMinor: ...  Something seems off here.\n\nIndeed this was an error -- we had posted a comment mentioning a correction on openreview. Thanks for pointing out; we have fixed this in the revision now.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1659/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1659/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Plug and Play Language Models: A Simple Approach to Controlled Text Generation", "authors": ["Sumanth Dathathri", "Andrea Madotto", "Janice Lan", "Jane Hung", "Eric Frank", "Piero Molino", "Jason Yosinski", "Rosanne Liu"], "authorids": ["dathathris@gmail.com", "amadotto@connect.ust.hk", "lan.janice.j@gmail.com", "jane.hung@uber.com", "mysterefrank@uber.com", "piero@uber.com", "yosinski@uber.com", "rosanne@uber.com"], "keywords": ["controlled text generation", "generative models", "conditional generative models", "language modeling", "transformer"], "TL;DR": "We control the topic and sentiment of text generation (almost) without any training. ", "abstract": "Large transformer-based language models (LMs) trained on huge text corpora have shown unparalleled generation capabilities. However, controlling attributes of the generated language (e.g. switching topic or sentiment) is difficult without modifying the model architecture or fine-tuning on attribute-specific data and entailing the significant cost of retraining. We propose a simple alternative: the Plug and Play Language Model (PPLM) for controllable language generation, which combines a pretrained LM with one or more simple attribute classifiers that guide text generation without any further training of the LM. In the canonical scenario we present, the attribute models are simple classifiers consisting of a user-specified bag of words or a single learned layer with 100,000 times fewer parameters than the LM. Sampling entails a forward and backward pass in which gradients from the attribute model push the LM's hidden activations and thus guide the generation. Model samples demonstrate control over a range of topics and sentiment styles, and extensive automated and human annotated evaluations show attribute alignment and fluency. PPLMs are flexible in that any combination of differentiable attribute models may be used to steer text generation, which will allow for diverse and creative applications beyond the examples given in this paper.", "pdf": "/pdf/e5bf5d6bd5aa1cbd71c4bb0632cdee0bd9e2b130.pdf", "code": "https://github.com/uber-research/PPLM", "paperhash": "dathathri|plug_and_play_language_models_a_simple_approach_to_controlled_text_generation", "_bibtex": "@inproceedings{\nDathathri2020Plug,\ntitle={Plug and Play Language Models: A Simple Approach to Controlled Text Generation},\nauthor={Sumanth Dathathri and Andrea Madotto and Janice Lan and Jane Hung and Eric Frank and Piero Molino and Jason Yosinski and Rosanne Liu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=H1edEyBKDS}\n}", "original_pdf": "/attachment/e783532ae84827007d75d2865206ef14fcc544db.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1edEyBKDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1659/Authors", "ICLR.cc/2020/Conference/Paper1659/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1659/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1659/Reviewers", "ICLR.cc/2020/Conference/Paper1659/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1659/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1659/Authors|ICLR.cc/2020/Conference/Paper1659/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504152760, "tmdate": 1576860532109, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1659/Authors", "ICLR.cc/2020/Conference/Paper1659/Reviewers", "ICLR.cc/2020/Conference/Paper1659/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1659/-/Official_Comment"}}}, {"id": "HkegjQxaFH", "original": null, "number": 2, "cdate": 1571779479992, "ddate": null, "tcdate": 1571779479992, "tmdate": 1572972439842, "tddate": null, "forum": "H1edEyBKDS", "replyto": "H1edEyBKDS", "invitation": "ICLR.cc/2020/Conference/Paper1659/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper proposes a Plug and Play LM model for controlled natural language generation. Similar to the idea of the Plug and Play Generative Networks for vision, the model plugs in a discriminator, which is either a bag-of-words model or a single layer classifier. The added simple discriminator is then coupled with a pre-trained generative language model such as GPT-2, to obtain a conditional probability for generating controllable text. The authors evaluate the proposed model using human evaluation studies and quantitative perplexity metrics, aiming at measuring the relevance and fluency of the generated text. Their experimental results show that the text generated is fluent and aligned with the desired attributes.  \n\nThe proposed method is simple and makes sense to me. The idea of how one can make good use of large, pre-trained  generative language models is very neat here. However, I have two main concerns, as follows.\n\n1. The main focuses of the generated text seem to be dramatically changed in an unpredictable way while tailoring the control attributes. In this sense, how useful these kinds of text generation techniques are not clear to me. For example, the first two rows in Table 3 contain two paragraphs with very different main ideas to be conveyed. Similarly for sentences in Table 1. It seems that those sentences talk about very different topics/things to me, although they may reflect the desired control attributes.  Is there an automatic evaluation metric to subjectively evaluate the change of the focuses/ideas of two pieces of text?\n\n2. The model is a straightforward adaption of the Plug and Play Generative Networks from the vision community. \n\nIn short, the idea in the paper is simple and seems effective. On the other hand, the lack of a good evaluation metric makes me a bit uncertain about the contribution of the paper. I am willing to increase my evaluation score if I will be convinced by other reviews and comments.  \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1659/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1659/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Plug and Play Language Models: A Simple Approach to Controlled Text Generation", "authors": ["Sumanth Dathathri", "Andrea Madotto", "Janice Lan", "Jane Hung", "Eric Frank", "Piero Molino", "Jason Yosinski", "Rosanne Liu"], "authorids": ["dathathris@gmail.com", "amadotto@connect.ust.hk", "lan.janice.j@gmail.com", "jane.hung@uber.com", "mysterefrank@uber.com", "piero@uber.com", "yosinski@uber.com", "rosanne@uber.com"], "keywords": ["controlled text generation", "generative models", "conditional generative models", "language modeling", "transformer"], "TL;DR": "We control the topic and sentiment of text generation (almost) without any training. ", "abstract": "Large transformer-based language models (LMs) trained on huge text corpora have shown unparalleled generation capabilities. However, controlling attributes of the generated language (e.g. switching topic or sentiment) is difficult without modifying the model architecture or fine-tuning on attribute-specific data and entailing the significant cost of retraining. We propose a simple alternative: the Plug and Play Language Model (PPLM) for controllable language generation, which combines a pretrained LM with one or more simple attribute classifiers that guide text generation without any further training of the LM. In the canonical scenario we present, the attribute models are simple classifiers consisting of a user-specified bag of words or a single learned layer with 100,000 times fewer parameters than the LM. Sampling entails a forward and backward pass in which gradients from the attribute model push the LM's hidden activations and thus guide the generation. Model samples demonstrate control over a range of topics and sentiment styles, and extensive automated and human annotated evaluations show attribute alignment and fluency. PPLMs are flexible in that any combination of differentiable attribute models may be used to steer text generation, which will allow for diverse and creative applications beyond the examples given in this paper.", "pdf": "/pdf/e5bf5d6bd5aa1cbd71c4bb0632cdee0bd9e2b130.pdf", "code": "https://github.com/uber-research/PPLM", "paperhash": "dathathri|plug_and_play_language_models_a_simple_approach_to_controlled_text_generation", "_bibtex": "@inproceedings{\nDathathri2020Plug,\ntitle={Plug and Play Language Models: A Simple Approach to Controlled Text Generation},\nauthor={Sumanth Dathathri and Andrea Madotto and Janice Lan and Jane Hung and Eric Frank and Piero Molino and Jason Yosinski and Rosanne Liu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=H1edEyBKDS}\n}", "original_pdf": "/attachment/e783532ae84827007d75d2865206ef14fcc544db.pdf"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "H1edEyBKDS", "replyto": "H1edEyBKDS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1659/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1659/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575938240702, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1659/Reviewers"], "noninvitees": [], "tcdate": 1570237734157, "tmdate": 1575938240714, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1659/-/Official_Review"}}}, {"id": "rklLgxopKB", "original": null, "number": 3, "cdate": 1571823597576, "ddate": null, "tcdate": 1571823597576, "tmdate": 1572972439796, "tddate": null, "forum": "H1edEyBKDS", "replyto": "H1edEyBKDS", "invitation": "ICLR.cc/2020/Conference/Paper1659/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The paper introduces an approach to the conditional generation of text, relying on pre-trained decoders, without fine-tuning and, in certain cases, without any training at all. The approach they introduce is following the framework known in NLP as noisy-channel modeling, previously standard in machine translation (in its SMT days), but undergoing certain revival recently (https://arxiv.org/abs/1611.02554, https://arxiv.org/abs/1910.00553,https://arxiv.org/abs/1908.05731,https://arxiv.org/abs/1907.06616). The authors do not mention this connection (they should!).\nVery differently from these previous approaches attempting to integrate the two factors in the search process (e.g., using reranking), the authors instead rely on gradient descent in the latent space of their model (Transformer), similarly to plug-n-play generative networks in image generation.  \n\nI find this approach interesting and like the paper overall. However, I do not see why authors do not compare to more direct ways of integrating the conditional component into the model. This would have been tricky in the NMT papers mentioned above, as the entire source sentences need to be reconstructred, however, it should be quite straightforward in this work, with conditioning on single categorical control variables (or maybe a couple in the additional experiments in sect 4.4). Especially, given that the authors already make the predictions of the control variable independently per prediction (e.g., see eq. (5) in section 4.2) / greedily per prefix (bottom lines, page 7). I would actually expect the proposed approach to work better (or at least differently) but it would be interesting to see it confirmed. E.g., for the experiments defining topics as sets of seed words (section 4.2), when integrating factors directly (unlike the proposed approach, Table 3), there will be no increase in the probability of generating relevant words before the first seed word is generated. \n\nAnother limitation is the lack of comparison to standard controlled generation work, i.e. those requiring training a model or/and fine-tuning pretrained decoder. I understand that the proposed approach falls in a different category and, of course, do not expect it to beat a fine-tuned model, but I'd like to get some feel for how much one loses by using this simpler method. There has been a lot of work on controlled generation in recent ~3 years, and they can also be combined with intializing and fine-tuning off-the-shelf pretrained decoders.\n\nThere is an interesting relation to the NIPS 2019 paper: https://arxiv.org/abs/1907.04944  They also rely on gradient descent to steer a pretrained language model. Their goal is to assess the degree of 'steerability' rather than building a controlled-generation model.\n\nGiven that style-controlled but otherwise unconditional generation may not have that many applications, I am curious how far you can push this approach. E.g., can you make it scale to more complicated data-to-text generation tasks (https://www.aclweb.org/anthology/D17-1239/)? Or, will the only application in this context be integrating new conditioning variables into pretrained conditional LMs? \n\nMinor: I am confused with the notation in \"Post-norm Geometric Mean Fusion\" section.  It says that softmax is applied to the product of probabilities. Maybe to a linear interpolation of log-probs? Or maybe that's not softmax at all? Something seems off here.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1659/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1659/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Plug and Play Language Models: A Simple Approach to Controlled Text Generation", "authors": ["Sumanth Dathathri", "Andrea Madotto", "Janice Lan", "Jane Hung", "Eric Frank", "Piero Molino", "Jason Yosinski", "Rosanne Liu"], "authorids": ["dathathris@gmail.com", "amadotto@connect.ust.hk", "lan.janice.j@gmail.com", "jane.hung@uber.com", "mysterefrank@uber.com", "piero@uber.com", "yosinski@uber.com", "rosanne@uber.com"], "keywords": ["controlled text generation", "generative models", "conditional generative models", "language modeling", "transformer"], "TL;DR": "We control the topic and sentiment of text generation (almost) without any training. ", "abstract": "Large transformer-based language models (LMs) trained on huge text corpora have shown unparalleled generation capabilities. However, controlling attributes of the generated language (e.g. switching topic or sentiment) is difficult without modifying the model architecture or fine-tuning on attribute-specific data and entailing the significant cost of retraining. We propose a simple alternative: the Plug and Play Language Model (PPLM) for controllable language generation, which combines a pretrained LM with one or more simple attribute classifiers that guide text generation without any further training of the LM. In the canonical scenario we present, the attribute models are simple classifiers consisting of a user-specified bag of words or a single learned layer with 100,000 times fewer parameters than the LM. Sampling entails a forward and backward pass in which gradients from the attribute model push the LM's hidden activations and thus guide the generation. Model samples demonstrate control over a range of topics and sentiment styles, and extensive automated and human annotated evaluations show attribute alignment and fluency. PPLMs are flexible in that any combination of differentiable attribute models may be used to steer text generation, which will allow for diverse and creative applications beyond the examples given in this paper.", "pdf": "/pdf/e5bf5d6bd5aa1cbd71c4bb0632cdee0bd9e2b130.pdf", "code": "https://github.com/uber-research/PPLM", "paperhash": "dathathri|plug_and_play_language_models_a_simple_approach_to_controlled_text_generation", "_bibtex": "@inproceedings{\nDathathri2020Plug,\ntitle={Plug and Play Language Models: A Simple Approach to Controlled Text Generation},\nauthor={Sumanth Dathathri and Andrea Madotto and Janice Lan and Jane Hung and Eric Frank and Piero Molino and Jason Yosinski and Rosanne Liu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=H1edEyBKDS}\n}", "original_pdf": "/attachment/e783532ae84827007d75d2865206ef14fcc544db.pdf"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "H1edEyBKDS", "replyto": "H1edEyBKDS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1659/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1659/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575938240702, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1659/Reviewers"], "noninvitees": [], "tcdate": 1570237734157, "tmdate": 1575938240714, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1659/-/Official_Review"}}}, {"id": "BklhMpxp_r", "original": null, "number": 6, "cdate": 1570733331812, "ddate": null, "tcdate": 1570733331812, "tmdate": 1570733331812, "tddate": null, "forum": "H1edEyBKDS", "replyto": "ByeOsKdGOr", "invitation": "ICLR.cc/2020/Conference/Paper1659/-/Official_Comment", "content": {"comment": "Hi Eric,\nThe code is now in the dropbox folder. You should be able to check-out the implementation. \n", "title": "Code available"}, "signatures": ["ICLR.cc/2020/Conference/Paper1659/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1659/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Plug and Play Language Models: A Simple Approach to Controlled Text Generation", "authors": ["Sumanth Dathathri", "Andrea Madotto", "Janice Lan", "Jane Hung", "Eric Frank", "Piero Molino", "Jason Yosinski", "Rosanne Liu"], "authorids": ["dathathris@gmail.com", "amadotto@connect.ust.hk", "lan.janice.j@gmail.com", "jane.hung@uber.com", "mysterefrank@uber.com", "piero@uber.com", "yosinski@uber.com", "rosanne@uber.com"], "keywords": ["controlled text generation", "generative models", "conditional generative models", "language modeling", "transformer"], "TL;DR": "We control the topic and sentiment of text generation (almost) without any training. ", "abstract": "Large transformer-based language models (LMs) trained on huge text corpora have shown unparalleled generation capabilities. However, controlling attributes of the generated language (e.g. switching topic or sentiment) is difficult without modifying the model architecture or fine-tuning on attribute-specific data and entailing the significant cost of retraining. We propose a simple alternative: the Plug and Play Language Model (PPLM) for controllable language generation, which combines a pretrained LM with one or more simple attribute classifiers that guide text generation without any further training of the LM. In the canonical scenario we present, the attribute models are simple classifiers consisting of a user-specified bag of words or a single learned layer with 100,000 times fewer parameters than the LM. Sampling entails a forward and backward pass in which gradients from the attribute model push the LM's hidden activations and thus guide the generation. Model samples demonstrate control over a range of topics and sentiment styles, and extensive automated and human annotated evaluations show attribute alignment and fluency. PPLMs are flexible in that any combination of differentiable attribute models may be used to steer text generation, which will allow for diverse and creative applications beyond the examples given in this paper.", "pdf": "/pdf/e5bf5d6bd5aa1cbd71c4bb0632cdee0bd9e2b130.pdf", "code": "https://github.com/uber-research/PPLM", "paperhash": "dathathri|plug_and_play_language_models_a_simple_approach_to_controlled_text_generation", "_bibtex": "@inproceedings{\nDathathri2020Plug,\ntitle={Plug and Play Language Models: A Simple Approach to Controlled Text Generation},\nauthor={Sumanth Dathathri and Andrea Madotto and Janice Lan and Jane Hung and Eric Frank and Piero Molino and Jason Yosinski and Rosanne Liu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=H1edEyBKDS}\n}", "original_pdf": "/attachment/e783532ae84827007d75d2865206ef14fcc544db.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1edEyBKDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1659/Authors", "ICLR.cc/2020/Conference/Paper1659/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1659/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1659/Reviewers", "ICLR.cc/2020/Conference/Paper1659/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1659/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1659/Authors|ICLR.cc/2020/Conference/Paper1659/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504152760, "tmdate": 1576860532109, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1659/Authors", "ICLR.cc/2020/Conference/Paper1659/Reviewers", "ICLR.cc/2020/Conference/Paper1659/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1659/-/Official_Comment"}}}, {"id": "SJxBXUyT_B", "original": null, "number": 5, "cdate": 1570727453077, "ddate": null, "tcdate": 1570727453077, "tmdate": 1570727518434, "tddate": null, "forum": "H1edEyBKDS", "replyto": "H1edEyBKDS", "invitation": "ICLR.cc/2020/Conference/Paper1659/-/Official_Comment", "content": {"comment": "We would like to point the readers to 2 minor corrections in the paper -- we will fix this during revision:\n1. Instead of the Softmax normalization in Section 3.3, page 5, paragraph 3, we actually divide by a normalizing factor such that it forms a valid distribution.\n2. Table S1, Row corresponding to POSITIVE, NEGATIVE --> gamma_gm = 0.9 is incorrect, and should be gamma_gm = 0.95.", "title": "Minor Corrections"}, "signatures": ["ICLR.cc/2020/Conference/Paper1659/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1659/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Plug and Play Language Models: A Simple Approach to Controlled Text Generation", "authors": ["Sumanth Dathathri", "Andrea Madotto", "Janice Lan", "Jane Hung", "Eric Frank", "Piero Molino", "Jason Yosinski", "Rosanne Liu"], "authorids": ["dathathris@gmail.com", "amadotto@connect.ust.hk", "lan.janice.j@gmail.com", "jane.hung@uber.com", "mysterefrank@uber.com", "piero@uber.com", "yosinski@uber.com", "rosanne@uber.com"], "keywords": ["controlled text generation", "generative models", "conditional generative models", "language modeling", "transformer"], "TL;DR": "We control the topic and sentiment of text generation (almost) without any training. ", "abstract": "Large transformer-based language models (LMs) trained on huge text corpora have shown unparalleled generation capabilities. However, controlling attributes of the generated language (e.g. switching topic or sentiment) is difficult without modifying the model architecture or fine-tuning on attribute-specific data and entailing the significant cost of retraining. We propose a simple alternative: the Plug and Play Language Model (PPLM) for controllable language generation, which combines a pretrained LM with one or more simple attribute classifiers that guide text generation without any further training of the LM. In the canonical scenario we present, the attribute models are simple classifiers consisting of a user-specified bag of words or a single learned layer with 100,000 times fewer parameters than the LM. Sampling entails a forward and backward pass in which gradients from the attribute model push the LM's hidden activations and thus guide the generation. Model samples demonstrate control over a range of topics and sentiment styles, and extensive automated and human annotated evaluations show attribute alignment and fluency. PPLMs are flexible in that any combination of differentiable attribute models may be used to steer text generation, which will allow for diverse and creative applications beyond the examples given in this paper.", "pdf": "/pdf/e5bf5d6bd5aa1cbd71c4bb0632cdee0bd9e2b130.pdf", "code": "https://github.com/uber-research/PPLM", "paperhash": "dathathri|plug_and_play_language_models_a_simple_approach_to_controlled_text_generation", "_bibtex": "@inproceedings{\nDathathri2020Plug,\ntitle={Plug and Play Language Models: A Simple Approach to Controlled Text Generation},\nauthor={Sumanth Dathathri and Andrea Madotto and Janice Lan and Jane Hung and Eric Frank and Piero Molino and Jason Yosinski and Rosanne Liu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=H1edEyBKDS}\n}", "original_pdf": "/attachment/e783532ae84827007d75d2865206ef14fcc544db.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1edEyBKDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1659/Authors", "ICLR.cc/2020/Conference/Paper1659/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1659/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1659/Reviewers", "ICLR.cc/2020/Conference/Paper1659/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1659/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1659/Authors|ICLR.cc/2020/Conference/Paper1659/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504152760, "tmdate": 1576860532109, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1659/Authors", "ICLR.cc/2020/Conference/Paper1659/Reviewers", "ICLR.cc/2020/Conference/Paper1659/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1659/-/Official_Comment"}}}, {"id": "r1emraXVOr", "original": null, "number": 8, "cdate": 1570155834996, "ddate": null, "tcdate": 1570155834996, "tmdate": 1570155834996, "tddate": null, "forum": "H1edEyBKDS", "replyto": "H1edEyBKDS", "invitation": "ICLR.cc/2020/Conference/Paper1659/-/Public_Comment", "content": {"comment": "Thanks for your reply! The response is sound and makes sense in every way. I do hope to see this paper get accepted. \n\nGood luck,\nJason", "title": "Reply to Authors"}, "signatures": ["~Jason_Brett1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Jason_Brett1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Plug and Play Language Models: A Simple Approach to Controlled Text Generation", "authors": ["Sumanth Dathathri", "Andrea Madotto", "Janice Lan", "Jane Hung", "Eric Frank", "Piero Molino", "Jason Yosinski", "Rosanne Liu"], "authorids": ["dathathris@gmail.com", "amadotto@connect.ust.hk", "lan.janice.j@gmail.com", "jane.hung@uber.com", "mysterefrank@uber.com", "piero@uber.com", "yosinski@uber.com", "rosanne@uber.com"], "keywords": ["controlled text generation", "generative models", "conditional generative models", "language modeling", "transformer"], "TL;DR": "We control the topic and sentiment of text generation (almost) without any training. ", "abstract": "Large transformer-based language models (LMs) trained on huge text corpora have shown unparalleled generation capabilities. However, controlling attributes of the generated language (e.g. switching topic or sentiment) is difficult without modifying the model architecture or fine-tuning on attribute-specific data and entailing the significant cost of retraining. We propose a simple alternative: the Plug and Play Language Model (PPLM) for controllable language generation, which combines a pretrained LM with one or more simple attribute classifiers that guide text generation without any further training of the LM. In the canonical scenario we present, the attribute models are simple classifiers consisting of a user-specified bag of words or a single learned layer with 100,000 times fewer parameters than the LM. Sampling entails a forward and backward pass in which gradients from the attribute model push the LM's hidden activations and thus guide the generation. Model samples demonstrate control over a range of topics and sentiment styles, and extensive automated and human annotated evaluations show attribute alignment and fluency. PPLMs are flexible in that any combination of differentiable attribute models may be used to steer text generation, which will allow for diverse and creative applications beyond the examples given in this paper.", "pdf": "/pdf/e5bf5d6bd5aa1cbd71c4bb0632cdee0bd9e2b130.pdf", "code": "https://github.com/uber-research/PPLM", "paperhash": "dathathri|plug_and_play_language_models_a_simple_approach_to_controlled_text_generation", "_bibtex": "@inproceedings{\nDathathri2020Plug,\ntitle={Plug and Play Language Models: A Simple Approach to Controlled Text Generation},\nauthor={Sumanth Dathathri and Andrea Madotto and Janice Lan and Jane Hung and Eric Frank and Piero Molino and Jason Yosinski and Rosanne Liu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=H1edEyBKDS}\n}", "original_pdf": "/attachment/e783532ae84827007d75d2865206ef14fcc544db.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1edEyBKDS", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504191620, "tmdate": 1576860565737, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper1659/Authors", "ICLR.cc/2020/Conference/Paper1659/Reviewers", "ICLR.cc/2020/Conference/Paper1659/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1659/-/Public_Comment"}}}, {"id": "HJlzgOuG_B", "original": null, "number": 1, "cdate": 1570043882371, "ddate": null, "tcdate": 1570043882371, "tmdate": 1570066050338, "tddate": null, "forum": "H1edEyBKDS", "replyto": "SkezXrHiPr", "invitation": "ICLR.cc/2020/Conference/Paper1659/-/Official_Comment", "content": {"comment": "Dear Jason and Karl,\n\nThank you for your interest in our paper, including posting questions and comments that will help the community better understand and contextualize our work. You both make several valid points, and we thank you both for this engaging discussion. In addition to the points made by Karl (who we are in no way associated with) -- we want to emphasize that our proposed methodology is Plug-and-Play, thus no re-training or fine-tuning of the language model is needed. We address Jason's comments below, although several of them have already been directly addressed by Karl\u2019s responses. \n\n>>(1) This paper does not provide any comparison with other works. For example, for a publicly available conditional language model, CTRL is only mentioned in related work but not compared as a baseline. As a researcher myself, I completely understand that CTRL is recently released and the authors do not have sufficient time to run CTRL. But I wish to see the results as supplemental material later on OpenReview. On the other hand, there are already several conditional generation model, e.g., S-VAE, CTRL-GEN. The authors should at least compare with these works.\n\nBoth CTRL and CTRL-GEN train *conditional language models* -- in contrast, we demonstrate controlled generation with a *pre-trained unconditional language model*. We do not believe a meaningful comparison can be made with a \u2018plug-and-play\u2019 method and training a conditional LM (as done in S-GAN, CTRL-GEN and CTRL). That said, we could\u2019ve made the distinction more clear in the paper; thanks for the suggestions! \n\nIn the limit of infinite annotated data and compute for training, approaches such as CTRL that directly train p(x|a) will outperform our approach. The main objective of our work is to provide a simple inexpensive alternate approach to directly trained conditional models. We\u2019ve added a note to fully clarify this point in the next draft of the paper.\n\n\n\n>>(2) The comparison with the vanilla GPT-2 seems to be unreasonable. The GPT-2 baseline is not provided with any condition-specific information, which surely cannot generate conditional text. For example, if providing the GPT-2 with the CTRL-style prompt (e.g., Rating 5.0/5.0, or Topic: Military), I'd like to see how GPT-2 performs with this setting.\n\nOur main objective with considering unconditioned GPT-2 text (\u201cB\u201d) in the ablation study is to obtain a baseline for human judges. More precisely, it provides information about the bias induced by asking a human if a sentence is positive, or if it is about the military: do humans squint and try to interpret everything in a manner the question suggests? The data shows that they do, and it was important to measure exactly how much, e.g. see the high 13.0% of unconditioned GPT-2 sentences judged to be about any particular topic!\n\nA second key objective is to illustrate that we retain most of the fluency from vanilla GPT-2 generation.\n\nHowever, your suggestion of an additional baseline is a great one. We quickly tried it out, and preliminary results show that:\n-- Merely adding the \u201cTopic: xyz\u201d prompt doesn't work too well in general. We did obtain some topical relevance for some topics when using short prefixes, but for longer prefixes and more complex topics, we found it not to work well.\n\n\nFor example, with the prefix, \u201cThe little girl lived in the woods, and her father was a carpenter\u201d adding \u201cTopic: Military\u201d does not reliably generate military related samples. In contrast, PPLM generates military related samples very reliably with the same prefix.\n\n\n-- For sentiment control, we found that simply adding \u201cRating x/y\u201d does not work reliably. Whether we added a \u201cRating 1.0/5.0\u201d or \u201cRating 0.0/5.0\u201d or \u201cRating 5.0/5.0\u201d prompt before the actual prefix, we found that this always biased GPT-2 to towards producing positive passages often.\n\n\n-- For more complex tasks such as detoxification, it might not be simple to find an easy \u201cextra prefix\u201d that works.\nWe\u2019ve added running a more rigorous evaluation of this approach to our TODO list and hope to complete it and add the results to the paper before the end of the review period. Thanks very much for the idea.\n\nFinally, we also note that the two approaches are complementary -- our conditioning can simply be used as an add on to other forms of condition (such as used in the CTRL paper).\n\n\n\n\n\n\n\n", "title": "Response to Karl and Jason's discussion: 1/3"}, "signatures": ["ICLR.cc/2020/Conference/Paper1659/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1659/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Plug and Play Language Models: A Simple Approach to Controlled Text Generation", "authors": ["Sumanth Dathathri", "Andrea Madotto", "Janice Lan", "Jane Hung", "Eric Frank", "Piero Molino", "Jason Yosinski", "Rosanne Liu"], "authorids": ["dathathris@gmail.com", "amadotto@connect.ust.hk", "lan.janice.j@gmail.com", "jane.hung@uber.com", "mysterefrank@uber.com", "piero@uber.com", "yosinski@uber.com", "rosanne@uber.com"], "keywords": ["controlled text generation", "generative models", "conditional generative models", "language modeling", "transformer"], "TL;DR": "We control the topic and sentiment of text generation (almost) without any training. ", "abstract": "Large transformer-based language models (LMs) trained on huge text corpora have shown unparalleled generation capabilities. However, controlling attributes of the generated language (e.g. switching topic or sentiment) is difficult without modifying the model architecture or fine-tuning on attribute-specific data and entailing the significant cost of retraining. We propose a simple alternative: the Plug and Play Language Model (PPLM) for controllable language generation, which combines a pretrained LM with one or more simple attribute classifiers that guide text generation without any further training of the LM. In the canonical scenario we present, the attribute models are simple classifiers consisting of a user-specified bag of words or a single learned layer with 100,000 times fewer parameters than the LM. Sampling entails a forward and backward pass in which gradients from the attribute model push the LM's hidden activations and thus guide the generation. Model samples demonstrate control over a range of topics and sentiment styles, and extensive automated and human annotated evaluations show attribute alignment and fluency. PPLMs are flexible in that any combination of differentiable attribute models may be used to steer text generation, which will allow for diverse and creative applications beyond the examples given in this paper.", "pdf": "/pdf/e5bf5d6bd5aa1cbd71c4bb0632cdee0bd9e2b130.pdf", "code": "https://github.com/uber-research/PPLM", "paperhash": "dathathri|plug_and_play_language_models_a_simple_approach_to_controlled_text_generation", "_bibtex": "@inproceedings{\nDathathri2020Plug,\ntitle={Plug and Play Language Models: A Simple Approach to Controlled Text Generation},\nauthor={Sumanth Dathathri and Andrea Madotto and Janice Lan and Jane Hung and Eric Frank and Piero Molino and Jason Yosinski and Rosanne Liu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=H1edEyBKDS}\n}", "original_pdf": "/attachment/e783532ae84827007d75d2865206ef14fcc544db.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1edEyBKDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1659/Authors", "ICLR.cc/2020/Conference/Paper1659/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1659/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1659/Reviewers", "ICLR.cc/2020/Conference/Paper1659/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1659/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1659/Authors|ICLR.cc/2020/Conference/Paper1659/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504152760, "tmdate": 1576860532109, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1659/Authors", "ICLR.cc/2020/Conference/Paper1659/Reviewers", "ICLR.cc/2020/Conference/Paper1659/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1659/-/Official_Comment"}}}, {"id": "SklON_OGdr", "original": null, "number": 2, "cdate": 1570043952171, "ddate": null, "tcdate": 1570043952171, "tmdate": 1570054131544, "tddate": null, "forum": "H1edEyBKDS", "replyto": "HJlzgOuG_B", "invitation": "ICLR.cc/2020/Conference/Paper1659/-/Official_Comment", "content": {"comment": ">>(3) To what end is the generated text conditional? The results in Table 4 seem to be significantly lower than prior methods (e.g., CTRL-GEN).\n\nBoth human evaluation and evaluation via automated methods can be very useful, but unfortunately the percentages reported in each case are not comparable for several reasons:\na) Human judgements are often quite different from automated measurements of an attribute. For example, a recent work on controlled generation in the style transfer setting MTR [2] is able to achieve 85%+ with automated evaluation (Table 4, [2]) but in the human evaluation setting the number drops to 69.6% (Table 5, [2]). Further, in DAR [1] -- which also focuses on style transfer -- the human evaluation accuracy is 64% (Table 5, [2])  while the automated evaluation accuracy for DAR in [1]  is over 90%. We hope this is sufficient evidence that automated evaluation and human evaluation numbers are not directly comparable. Note that both DAR/MTR are more recent works that CTRL-GEN.\n\nHuman judges (both in our paper and [2]) are given the option of saying a sentence is neutral: neither positive/negative. Evaluation with an SST-2 discriminator will classify neutral sentences as positive or negative (binary classifier), resulting in an increased count for both positive/negative accuracies. This in turn results in exaggerated numbers for sentiment accuracy. Please note that human annotation in our paper (as in [2]) is not \u201cbinary\u201d but rather \u201cternary\u201d, as the human can mark if a sentence is positive or negative or neutral (if the human marks a sentence as not positive and not negative).\n\nb) Additionally, CTRL-GEN evaluated sequences of length<15. In contrast, our numbers are reported on sequences of length 80 (for topic) and sequences of length 50 (for sentiment). This difference further prohibits direct comparison. \n\n\nc) While, the premise for PPLM is entirely different from DAR/MTR -- PPLM is generation from an unconditional model, note that we perform comparably to the human evaluation numbers reported in [2] for both DAR/MTR (more recent works than CTRL-GEN). Further, [2] reports the perplexity for CTRL-GEN (Hu et al., 2017) as 232.0 (significantly higher than other competing approaches).  Also, note that conditioning can be increased at the cost of fluency (See Table S5, Supplementary Information) -- in that setting the topic/sentiment accuracy numbers would go up at the cost of fluency.  In this regard, it makes sense to compare both accuracy and fluency between different approaches as done in [2].\n\n\n\n\n>>(4) Will the fluency degenerate along with the conditionality? How to balance that? I didn't find an answer in the article.\n\nYes, fluency does degenerate with the strength of condition as shown in the paper. You can control that by decreasing the step-size alpha, increasing the coefficient for the kl-loss and also decreasing lambda_gm  (in the limit, as lambda_gm goes to 0, you recover the original LM). We found a good set of hyper-parameters that work well in practice (Table S1 in Supplementary Information). We will make this clearer during revision.\n\n\n>>(5) The BoW strategy seems to be confusing. If we simply multiply the probability of the keywords by a constant larger than one, will it also do the trick? For example, we manually multiply the probability of {best, excellent, wonderful, ...} by 2, can GPT-2 generate positive samples as well? If we measure the cosine similarity between the pre-trained word embeddings of predicted words and the given keywords, can we even get better results? (e.g., it will decrease the probability of words like \"terrible\", \"infamous\" and increase the probability of \"terrific\", \"gorgeous\"). It seems like the work in this paper is an over-sophisticated version of what I just mentioned.\n\nThe approach you describe with increasing the probabilities of the bag-of-word tokens (referenced to in prior work as \u2018weighted decoding\u2019 [3]) will work to some extent but will not result in samples with subtle changes of topics that result from promoting *related* words to the bag of words but not necessarily words directly from the bag. We provide more discussion in extended related work about weighted decoding [3], in S1 in Supplementary Information. For example, consider the sentences generated with topics \u2018Politics\u2019, \u2018Science\u2019, \u2018Fantasy\u2019 in Table 3, and topic \u2018Fantasy\u2019  in Table 7 -- here words closely related to the topic appear before words from the bag.\n\n", "title": "Response to Karl and Jason's comments: 2/3"}, "signatures": ["ICLR.cc/2020/Conference/Paper1659/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1659/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Plug and Play Language Models: A Simple Approach to Controlled Text Generation", "authors": ["Sumanth Dathathri", "Andrea Madotto", "Janice Lan", "Jane Hung", "Eric Frank", "Piero Molino", "Jason Yosinski", "Rosanne Liu"], "authorids": ["dathathris@gmail.com", "amadotto@connect.ust.hk", "lan.janice.j@gmail.com", "jane.hung@uber.com", "mysterefrank@uber.com", "piero@uber.com", "yosinski@uber.com", "rosanne@uber.com"], "keywords": ["controlled text generation", "generative models", "conditional generative models", "language modeling", "transformer"], "TL;DR": "We control the topic and sentiment of text generation (almost) without any training. ", "abstract": "Large transformer-based language models (LMs) trained on huge text corpora have shown unparalleled generation capabilities. However, controlling attributes of the generated language (e.g. switching topic or sentiment) is difficult without modifying the model architecture or fine-tuning on attribute-specific data and entailing the significant cost of retraining. We propose a simple alternative: the Plug and Play Language Model (PPLM) for controllable language generation, which combines a pretrained LM with one or more simple attribute classifiers that guide text generation without any further training of the LM. In the canonical scenario we present, the attribute models are simple classifiers consisting of a user-specified bag of words or a single learned layer with 100,000 times fewer parameters than the LM. Sampling entails a forward and backward pass in which gradients from the attribute model push the LM's hidden activations and thus guide the generation. Model samples demonstrate control over a range of topics and sentiment styles, and extensive automated and human annotated evaluations show attribute alignment and fluency. PPLMs are flexible in that any combination of differentiable attribute models may be used to steer text generation, which will allow for diverse and creative applications beyond the examples given in this paper.", "pdf": "/pdf/e5bf5d6bd5aa1cbd71c4bb0632cdee0bd9e2b130.pdf", "code": "https://github.com/uber-research/PPLM", "paperhash": "dathathri|plug_and_play_language_models_a_simple_approach_to_controlled_text_generation", "_bibtex": "@inproceedings{\nDathathri2020Plug,\ntitle={Plug and Play Language Models: A Simple Approach to Controlled Text Generation},\nauthor={Sumanth Dathathri and Andrea Madotto and Janice Lan and Jane Hung and Eric Frank and Piero Molino and Jason Yosinski and Rosanne Liu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=H1edEyBKDS}\n}", "original_pdf": "/attachment/e783532ae84827007d75d2865206ef14fcc544db.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1edEyBKDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1659/Authors", "ICLR.cc/2020/Conference/Paper1659/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1659/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1659/Reviewers", "ICLR.cc/2020/Conference/Paper1659/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1659/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1659/Authors|ICLR.cc/2020/Conference/Paper1659/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504152760, "tmdate": 1576860532109, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1659/Authors", "ICLR.cc/2020/Conference/Paper1659/Reviewers", "ICLR.cc/2020/Conference/Paper1659/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1659/-/Official_Comment"}}}, {"id": "ByeOsKdGOr", "original": null, "number": 4, "cdate": 1570044319737, "ddate": null, "tcdate": 1570044319737, "tmdate": 1570051662042, "tddate": null, "forum": "H1edEyBKDS", "replyto": "BklATnxydS", "invitation": "ICLR.cc/2020/Conference/Paper1659/-/Official_Comment", "content": {"comment": "Hi Eric,\n\nThanks for the interest in our paper! We are actively working on institute approval, we hope to have it available in the next few days (~ 1-2 weeks). We will keep you updated about this! ", "title": "Code Release"}, "signatures": ["ICLR.cc/2020/Conference/Paper1659/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1659/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Plug and Play Language Models: A Simple Approach to Controlled Text Generation", "authors": ["Sumanth Dathathri", "Andrea Madotto", "Janice Lan", "Jane Hung", "Eric Frank", "Piero Molino", "Jason Yosinski", "Rosanne Liu"], "authorids": ["dathathris@gmail.com", "amadotto@connect.ust.hk", "lan.janice.j@gmail.com", "jane.hung@uber.com", "mysterefrank@uber.com", "piero@uber.com", "yosinski@uber.com", "rosanne@uber.com"], "keywords": ["controlled text generation", "generative models", "conditional generative models", "language modeling", "transformer"], "TL;DR": "We control the topic and sentiment of text generation (almost) without any training. ", "abstract": "Large transformer-based language models (LMs) trained on huge text corpora have shown unparalleled generation capabilities. However, controlling attributes of the generated language (e.g. switching topic or sentiment) is difficult without modifying the model architecture or fine-tuning on attribute-specific data and entailing the significant cost of retraining. We propose a simple alternative: the Plug and Play Language Model (PPLM) for controllable language generation, which combines a pretrained LM with one or more simple attribute classifiers that guide text generation without any further training of the LM. In the canonical scenario we present, the attribute models are simple classifiers consisting of a user-specified bag of words or a single learned layer with 100,000 times fewer parameters than the LM. Sampling entails a forward and backward pass in which gradients from the attribute model push the LM's hidden activations and thus guide the generation. Model samples demonstrate control over a range of topics and sentiment styles, and extensive automated and human annotated evaluations show attribute alignment and fluency. PPLMs are flexible in that any combination of differentiable attribute models may be used to steer text generation, which will allow for diverse and creative applications beyond the examples given in this paper.", "pdf": "/pdf/e5bf5d6bd5aa1cbd71c4bb0632cdee0bd9e2b130.pdf", "code": "https://github.com/uber-research/PPLM", "paperhash": "dathathri|plug_and_play_language_models_a_simple_approach_to_controlled_text_generation", "_bibtex": "@inproceedings{\nDathathri2020Plug,\ntitle={Plug and Play Language Models: A Simple Approach to Controlled Text Generation},\nauthor={Sumanth Dathathri and Andrea Madotto and Janice Lan and Jane Hung and Eric Frank and Piero Molino and Jason Yosinski and Rosanne Liu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=H1edEyBKDS}\n}", "original_pdf": "/attachment/e783532ae84827007d75d2865206ef14fcc544db.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1edEyBKDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1659/Authors", "ICLR.cc/2020/Conference/Paper1659/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1659/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1659/Reviewers", "ICLR.cc/2020/Conference/Paper1659/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1659/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1659/Authors|ICLR.cc/2020/Conference/Paper1659/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504152760, "tmdate": 1576860532109, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1659/Authors", "ICLR.cc/2020/Conference/Paper1659/Reviewers", "ICLR.cc/2020/Conference/Paper1659/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1659/-/Official_Comment"}}}, {"id": "BJgScu_zdr", "original": null, "number": 3, "cdate": 1570044045483, "ddate": null, "tcdate": 1570044045483, "tmdate": 1570044045483, "tddate": null, "forum": "H1edEyBKDS", "replyto": "SklON_OGdr", "invitation": "ICLR.cc/2020/Conference/Paper1659/-/Official_Comment", "content": {"comment": ">>(6) A question about the setting: I wonder whether the setting for sentiment-conditioned generation is too elaborate? Why choose the SST-5 dataset and only use \"Very Positive\" and \"Very Negative\" instead of SST-2 dataset with a binary classification or SST-5 with a three-fold classification? The setting in the paper clearly exaggerates the real performance of the proposed method.\n\nIt was a simple design choice to use SST-5 over SST-2. We do not measure with SST-2 (or SST-5) and we use humans and an external dataset (IMDB reviews) to measure sentiment controllability. In this regard, the results are meaningful and fair. \n\n>>Nitpick:\nThe Dist-1 metric in Table S4 is labeled as smaller better, which is in conflict with Table 4.\n\nThanks for the catch on the Dist-1 \u201clower is better\u201d typo! We have corrected this and it will appear in the next posted draft.\n\n\nWe once again thank you both for your interest, and we\u2019d be happy to continue the discussion.\n\n[1] Delete, Retrieve, Generate: A Simple Approach to Sentiment and Style Transfer\nhttps://arxiv.org/abs/1804.06437\n[2] Multiple-Attribute Text Rewriting, https://openreview.net/forum?id=H1g2NhC5KQ\n[3] What makes a good conversation? How controllable attributes affect human judgements. https://www.aclweb.org/anthology/N19-1170/\n[4] http://www.abigailsee.com/2019/08/13/what-makes-a-good-conversation.html;\n\n", "title": "Response to Karl and Jason's discussion 3/3"}, "signatures": ["ICLR.cc/2020/Conference/Paper1659/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1659/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Plug and Play Language Models: A Simple Approach to Controlled Text Generation", "authors": ["Sumanth Dathathri", "Andrea Madotto", "Janice Lan", "Jane Hung", "Eric Frank", "Piero Molino", "Jason Yosinski", "Rosanne Liu"], "authorids": ["dathathris@gmail.com", "amadotto@connect.ust.hk", "lan.janice.j@gmail.com", "jane.hung@uber.com", "mysterefrank@uber.com", "piero@uber.com", "yosinski@uber.com", "rosanne@uber.com"], "keywords": ["controlled text generation", "generative models", "conditional generative models", "language modeling", "transformer"], "TL;DR": "We control the topic and sentiment of text generation (almost) without any training. ", "abstract": "Large transformer-based language models (LMs) trained on huge text corpora have shown unparalleled generation capabilities. However, controlling attributes of the generated language (e.g. switching topic or sentiment) is difficult without modifying the model architecture or fine-tuning on attribute-specific data and entailing the significant cost of retraining. We propose a simple alternative: the Plug and Play Language Model (PPLM) for controllable language generation, which combines a pretrained LM with one or more simple attribute classifiers that guide text generation without any further training of the LM. In the canonical scenario we present, the attribute models are simple classifiers consisting of a user-specified bag of words or a single learned layer with 100,000 times fewer parameters than the LM. Sampling entails a forward and backward pass in which gradients from the attribute model push the LM's hidden activations and thus guide the generation. Model samples demonstrate control over a range of topics and sentiment styles, and extensive automated and human annotated evaluations show attribute alignment and fluency. PPLMs are flexible in that any combination of differentiable attribute models may be used to steer text generation, which will allow for diverse and creative applications beyond the examples given in this paper.", "pdf": "/pdf/e5bf5d6bd5aa1cbd71c4bb0632cdee0bd9e2b130.pdf", "code": "https://github.com/uber-research/PPLM", "paperhash": "dathathri|plug_and_play_language_models_a_simple_approach_to_controlled_text_generation", "_bibtex": "@inproceedings{\nDathathri2020Plug,\ntitle={Plug and Play Language Models: A Simple Approach to Controlled Text Generation},\nauthor={Sumanth Dathathri and Andrea Madotto and Janice Lan and Jane Hung and Eric Frank and Piero Molino and Jason Yosinski and Rosanne Liu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=H1edEyBKDS}\n}", "original_pdf": "/attachment/e783532ae84827007d75d2865206ef14fcc544db.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1edEyBKDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1659/Authors", "ICLR.cc/2020/Conference/Paper1659/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1659/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1659/Reviewers", "ICLR.cc/2020/Conference/Paper1659/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1659/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1659/Authors|ICLR.cc/2020/Conference/Paper1659/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504152760, "tmdate": 1576860532109, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1659/Authors", "ICLR.cc/2020/Conference/Paper1659/Reviewers", "ICLR.cc/2020/Conference/Paper1659/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1659/-/Official_Comment"}}}, {"id": "BklATnxydS", "original": null, "number": 7, "cdate": 1569815749693, "ddate": null, "tcdate": 1569815749693, "tmdate": 1569815749693, "tddate": null, "forum": "H1edEyBKDS", "replyto": "H1edEyBKDS", "invitation": "ICLR.cc/2020/Conference/Paper1659/-/Public_Comment", "content": {"comment": "Hi, thanks for the paper! I was interested to take a closer look at the implementation of your method. I see you are working on getting approval to release your code, do you have any estimate on when it will be available? If its a while (I know getting company approval can be slow/annoying), is it possible to share it privately? ", "title": "Any Estimated Time of Code Release?"}, "signatures": ["~Eric_Wallace1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Eric_Wallace1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Plug and Play Language Models: A Simple Approach to Controlled Text Generation", "authors": ["Sumanth Dathathri", "Andrea Madotto", "Janice Lan", "Jane Hung", "Eric Frank", "Piero Molino", "Jason Yosinski", "Rosanne Liu"], "authorids": ["dathathris@gmail.com", "amadotto@connect.ust.hk", "lan.janice.j@gmail.com", "jane.hung@uber.com", "mysterefrank@uber.com", "piero@uber.com", "yosinski@uber.com", "rosanne@uber.com"], "keywords": ["controlled text generation", "generative models", "conditional generative models", "language modeling", "transformer"], "TL;DR": "We control the topic and sentiment of text generation (almost) without any training. ", "abstract": "Large transformer-based language models (LMs) trained on huge text corpora have shown unparalleled generation capabilities. However, controlling attributes of the generated language (e.g. switching topic or sentiment) is difficult without modifying the model architecture or fine-tuning on attribute-specific data and entailing the significant cost of retraining. We propose a simple alternative: the Plug and Play Language Model (PPLM) for controllable language generation, which combines a pretrained LM with one or more simple attribute classifiers that guide text generation without any further training of the LM. In the canonical scenario we present, the attribute models are simple classifiers consisting of a user-specified bag of words or a single learned layer with 100,000 times fewer parameters than the LM. Sampling entails a forward and backward pass in which gradients from the attribute model push the LM's hidden activations and thus guide the generation. Model samples demonstrate control over a range of topics and sentiment styles, and extensive automated and human annotated evaluations show attribute alignment and fluency. PPLMs are flexible in that any combination of differentiable attribute models may be used to steer text generation, which will allow for diverse and creative applications beyond the examples given in this paper.", "pdf": "/pdf/e5bf5d6bd5aa1cbd71c4bb0632cdee0bd9e2b130.pdf", "code": "https://github.com/uber-research/PPLM", "paperhash": "dathathri|plug_and_play_language_models_a_simple_approach_to_controlled_text_generation", "_bibtex": "@inproceedings{\nDathathri2020Plug,\ntitle={Plug and Play Language Models: A Simple Approach to Controlled Text Generation},\nauthor={Sumanth Dathathri and Andrea Madotto and Janice Lan and Jane Hung and Eric Frank and Piero Molino and Jason Yosinski and Rosanne Liu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=H1edEyBKDS}\n}", "original_pdf": "/attachment/e783532ae84827007d75d2865206ef14fcc544db.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1edEyBKDS", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504191620, "tmdate": 1576860565737, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper1659/Authors", "ICLR.cc/2020/Conference/Paper1659/Reviewers", "ICLR.cc/2020/Conference/Paper1659/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1659/-/Public_Comment"}}}, {"id": "H1lCPj1pwH", "original": null, "number": 6, "cdate": 1569680229617, "ddate": null, "tcdate": 1569680229617, "tmdate": 1569680229617, "tddate": null, "forum": "H1edEyBKDS", "replyto": "SJlN9SciDS", "invitation": "ICLR.cc/2020/Conference/Paper1659/-/Public_Comment", "content": {"comment": "I actually hope reviewers are not influenced by this discussion, as some of the requests in the original comment and considerations in it are incorrect and may mislead judgment, but hopefully the authors can actually draw something useful from this.\n\n1) I believe this paper is the best conditioned language generation we have seen so far that doesn't require retraining or finetuning, both really expensive to perform given the size of the original GPT-2 model, so, in this specific setting, I would consider it SOTA.\n\n2) different annotations of the same inputs make for different datasets. The difference could even be 50%, but if the two numbers are obtained in totally different settings like in this case (human evaluation vs automatic evaluation with a specific classifier trained on a specific dataset) they are absolutely non comparable. You say you don't believe the difference is only because of the evaluation, I'm saying  the human judges have a completely different standard than the automatic system, thus the difference could have been smaller or bigger or reversed in sign, it would have still meant nothing at all.\n\n6) We agree on being curious to see the same automatic evaluation in CTRL-GEN applied here, to actually have a realistic reference. That said, I do believe that the human evaluation provided in the paper is much stronger evidence of the quality of the results than an automatic evaluation would be.", "title": "Reply"}, "signatures": ["~Karl_William_McMara1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Karl_William_McMara1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Plug and Play Language Models: A Simple Approach to Controlled Text Generation", "authors": ["Sumanth Dathathri", "Andrea Madotto", "Janice Lan", "Jane Hung", "Eric Frank", "Piero Molino", "Jason Yosinski", "Rosanne Liu"], "authorids": ["dathathris@gmail.com", "amadotto@connect.ust.hk", "lan.janice.j@gmail.com", "jane.hung@uber.com", "mysterefrank@uber.com", "piero@uber.com", "yosinski@uber.com", "rosanne@uber.com"], "keywords": ["controlled text generation", "generative models", "conditional generative models", "language modeling", "transformer"], "TL;DR": "We control the topic and sentiment of text generation (almost) without any training. ", "abstract": "Large transformer-based language models (LMs) trained on huge text corpora have shown unparalleled generation capabilities. However, controlling attributes of the generated language (e.g. switching topic or sentiment) is difficult without modifying the model architecture or fine-tuning on attribute-specific data and entailing the significant cost of retraining. We propose a simple alternative: the Plug and Play Language Model (PPLM) for controllable language generation, which combines a pretrained LM with one or more simple attribute classifiers that guide text generation without any further training of the LM. In the canonical scenario we present, the attribute models are simple classifiers consisting of a user-specified bag of words or a single learned layer with 100,000 times fewer parameters than the LM. Sampling entails a forward and backward pass in which gradients from the attribute model push the LM's hidden activations and thus guide the generation. Model samples demonstrate control over a range of topics and sentiment styles, and extensive automated and human annotated evaluations show attribute alignment and fluency. PPLMs are flexible in that any combination of differentiable attribute models may be used to steer text generation, which will allow for diverse and creative applications beyond the examples given in this paper.", "pdf": "/pdf/e5bf5d6bd5aa1cbd71c4bb0632cdee0bd9e2b130.pdf", "code": "https://github.com/uber-research/PPLM", "paperhash": "dathathri|plug_and_play_language_models_a_simple_approach_to_controlled_text_generation", "_bibtex": "@inproceedings{\nDathathri2020Plug,\ntitle={Plug and Play Language Models: A Simple Approach to Controlled Text Generation},\nauthor={Sumanth Dathathri and Andrea Madotto and Janice Lan and Jane Hung and Eric Frank and Piero Molino and Jason Yosinski and Rosanne Liu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=H1edEyBKDS}\n}", "original_pdf": "/attachment/e783532ae84827007d75d2865206ef14fcc544db.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1edEyBKDS", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504191620, "tmdate": 1576860565737, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper1659/Authors", "ICLR.cc/2020/Conference/Paper1659/Reviewers", "ICLR.cc/2020/Conference/Paper1659/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1659/-/Public_Comment"}}}, {"id": "rJxpps_swS", "original": null, "number": 3, "cdate": 1569586116583, "ddate": null, "tcdate": 1569586116583, "tmdate": 1569593400412, "tddate": null, "forum": "H1edEyBKDS", "replyto": "B1eC5MPoDH", "invitation": "ICLR.cc/2020/Conference/Paper1659/-/Public_Comment", "content": {"comment": "Hi Karl,\n\nThanks for your attempt to try to explain these to me. I believe you are one of the authors? If so, I think replying in the role of authors may be more appropriate. However, I'd like to point out something in addition to my comment above. \n\nFirst, I never neglect the interesting and smart dynamic conditioning method proposed in the paper. However, as a normal reader instead of a reviewer, I thought it is not mandatory to also compliment the strengths in the paper.\nSecond, I need to say the CTRL (proposed by Salesforce) and CTRL-GEN ( https://arxiv.org/pdf/1703.00955.pdf ) mentioned in my comment are not the same at all. I should have made it more clear. My bad.\nThen, I'd like to provide some more explanation for my comment and response to your questions. \n\n1) As I already said in my comment, it was completely fine not to provide a comparison with CTRL. I totally understand that. However, what I kindly ask is if the authors can provide the following experiment results on OpenReview, and this is the spirit of an open review, right?\n\n2) Yes! It is a weak but meaningful baseline. I would certainly add this work as a baseline if I have future work on controlled generation.\n\n3) In Table 1 of CTRL-GEN (NOT CTRL!), the result of CTRL-GEN and S-VAE on SST are both higher than the result shown in the second column of Table 6 in this paper. I know there may be some nuance on experimental settings but it cannot result in an absolute 20% performance drop, right? On the other hand, Table 1,3,5,7 and appendix S2, S5 and S6 which you mentioned are all qualitative results (which may be cherrypicked, more or less). However, I was talking about a quantitive result instead.\n\n4) It is indeed a pro of this paper! I may have missed something in the appendix.\n\n5) Yes. I just wonder if the method proposed in this paper is better than my proposed over-simplified version? Also, I'm not suggesting the authors need to compare my naive simplification. It is just proposed for discussion.\n\n6) However, in the CTRL-GEN paper (https://arxiv.org/pdf/1703.00955.pdf , NOT CTRL!), the setting is more reasonable (as I mentioned in the comment, using SST-2 instead of SST-5) and it yields even better performance (0.851) than Table 6 of this paper (0.696). I believe the experiment in this paper is also a binary generation (+, -) instead of a ternary one (+, 0, -). If it is not, please point out. I am not very sure about that. ", "title": "Some issues to be clear"}, "signatures": ["~Jason_Brett1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Jason_Brett1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Plug and Play Language Models: A Simple Approach to Controlled Text Generation", "authors": ["Sumanth Dathathri", "Andrea Madotto", "Janice Lan", "Jane Hung", "Eric Frank", "Piero Molino", "Jason Yosinski", "Rosanne Liu"], "authorids": ["dathathris@gmail.com", "amadotto@connect.ust.hk", "lan.janice.j@gmail.com", "jane.hung@uber.com", "mysterefrank@uber.com", "piero@uber.com", "yosinski@uber.com", "rosanne@uber.com"], "keywords": ["controlled text generation", "generative models", "conditional generative models", "language modeling", "transformer"], "TL;DR": "We control the topic and sentiment of text generation (almost) without any training. ", "abstract": "Large transformer-based language models (LMs) trained on huge text corpora have shown unparalleled generation capabilities. However, controlling attributes of the generated language (e.g. switching topic or sentiment) is difficult without modifying the model architecture or fine-tuning on attribute-specific data and entailing the significant cost of retraining. We propose a simple alternative: the Plug and Play Language Model (PPLM) for controllable language generation, which combines a pretrained LM with one or more simple attribute classifiers that guide text generation without any further training of the LM. In the canonical scenario we present, the attribute models are simple classifiers consisting of a user-specified bag of words or a single learned layer with 100,000 times fewer parameters than the LM. Sampling entails a forward and backward pass in which gradients from the attribute model push the LM's hidden activations and thus guide the generation. Model samples demonstrate control over a range of topics and sentiment styles, and extensive automated and human annotated evaluations show attribute alignment and fluency. PPLMs are flexible in that any combination of differentiable attribute models may be used to steer text generation, which will allow for diverse and creative applications beyond the examples given in this paper.", "pdf": "/pdf/e5bf5d6bd5aa1cbd71c4bb0632cdee0bd9e2b130.pdf", "code": "https://github.com/uber-research/PPLM", "paperhash": "dathathri|plug_and_play_language_models_a_simple_approach_to_controlled_text_generation", "_bibtex": "@inproceedings{\nDathathri2020Plug,\ntitle={Plug and Play Language Models: A Simple Approach to Controlled Text Generation},\nauthor={Sumanth Dathathri and Andrea Madotto and Janice Lan and Jane Hung and Eric Frank and Piero Molino and Jason Yosinski and Rosanne Liu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=H1edEyBKDS}\n}", "original_pdf": "/attachment/e783532ae84827007d75d2865206ef14fcc544db.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1edEyBKDS", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504191620, "tmdate": 1576860565737, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper1659/Authors", "ICLR.cc/2020/Conference/Paper1659/Reviewers", "ICLR.cc/2020/Conference/Paper1659/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1659/-/Public_Comment"}}}, {"id": "SJlN9SciDS", "original": null, "number": 5, "cdate": 1569592715740, "ddate": null, "tcdate": 1569592715740, "tmdate": 1569593186706, "tddate": null, "forum": "H1edEyBKDS", "replyto": "SyebpLFjPB", "invitation": "ICLR.cc/2020/Conference/Paper1659/-/Public_Comment", "content": {"comment": "Hi Karl,\n\nThanks for the discussion. I hope our discussion is beneficial for both the authors and reviewers!\n\n1) Yes! Of course, CTRL will have much better result, but it does not compromise this paper. We all agree that the method is a weak baseline method instead of a competitive SOTA. However, a fair comparison with CTRL, the possible SOTA will benefit the NLG community. This part is missing in CTRL due to there was no comparable controlled language model. But here comes the chance. \n\n3) I'd like to kindly remind you that SST-2 and SST-5 are actually the same dataset. They only differ on labels. Thus, I don't think training discriminator on the other SST is fairer. However, I believe you'd also agree that using only \"Very positive\" and \"Very negetive\" samples will make both automatic and human evaluation higher than using SST-2 (which only has \"Pos\" and \"Neg\"). It is a pity that these two papers use different evaluation, which indeed cannot be compared directly. I also acknowledged that in my second comment, but adding that I do not believe -20% is only because of the different evaluation. On the other hand, if the authors provide comparison under the same settings, I guess it will be more convincing.\n\n6) My point is if both CTRL-GEN and this paper use binary generation, they should be at least somehow comparable. That's to say, I won't compare a binary result to a ternary one. On the other hand, as I mentioned above, the different evaluation methods are not directly comparable but that's the only thing I can do since I really want to know to what end can GPT-2 be conditional with this proposed method. A direct comparison will surely help answer that. I sincerely hope to see the results provided by the authors.", "title": "Reply #2"}, "signatures": ["~Jason_Brett1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Jason_Brett1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Plug and Play Language Models: A Simple Approach to Controlled Text Generation", "authors": ["Sumanth Dathathri", "Andrea Madotto", "Janice Lan", "Jane Hung", "Eric Frank", "Piero Molino", "Jason Yosinski", "Rosanne Liu"], "authorids": ["dathathris@gmail.com", "amadotto@connect.ust.hk", "lan.janice.j@gmail.com", "jane.hung@uber.com", "mysterefrank@uber.com", "piero@uber.com", "yosinski@uber.com", "rosanne@uber.com"], "keywords": ["controlled text generation", "generative models", "conditional generative models", "language modeling", "transformer"], "TL;DR": "We control the topic and sentiment of text generation (almost) without any training. ", "abstract": "Large transformer-based language models (LMs) trained on huge text corpora have shown unparalleled generation capabilities. However, controlling attributes of the generated language (e.g. switching topic or sentiment) is difficult without modifying the model architecture or fine-tuning on attribute-specific data and entailing the significant cost of retraining. We propose a simple alternative: the Plug and Play Language Model (PPLM) for controllable language generation, which combines a pretrained LM with one or more simple attribute classifiers that guide text generation without any further training of the LM. In the canonical scenario we present, the attribute models are simple classifiers consisting of a user-specified bag of words or a single learned layer with 100,000 times fewer parameters than the LM. Sampling entails a forward and backward pass in which gradients from the attribute model push the LM's hidden activations and thus guide the generation. Model samples demonstrate control over a range of topics and sentiment styles, and extensive automated and human annotated evaluations show attribute alignment and fluency. PPLMs are flexible in that any combination of differentiable attribute models may be used to steer text generation, which will allow for diverse and creative applications beyond the examples given in this paper.", "pdf": "/pdf/e5bf5d6bd5aa1cbd71c4bb0632cdee0bd9e2b130.pdf", "code": "https://github.com/uber-research/PPLM", "paperhash": "dathathri|plug_and_play_language_models_a_simple_approach_to_controlled_text_generation", "_bibtex": "@inproceedings{\nDathathri2020Plug,\ntitle={Plug and Play Language Models: A Simple Approach to Controlled Text Generation},\nauthor={Sumanth Dathathri and Andrea Madotto and Janice Lan and Jane Hung and Eric Frank and Piero Molino and Jason Yosinski and Rosanne Liu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=H1edEyBKDS}\n}", "original_pdf": "/attachment/e783532ae84827007d75d2865206ef14fcc544db.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1edEyBKDS", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504191620, "tmdate": 1576860565737, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper1659/Authors", "ICLR.cc/2020/Conference/Paper1659/Reviewers", "ICLR.cc/2020/Conference/Paper1659/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1659/-/Public_Comment"}}}, {"id": "SyebpLFjPB", "original": null, "number": 4, "cdate": 1569588920570, "ddate": null, "tcdate": 1569588920570, "tmdate": 1569588920570, "tddate": null, "forum": "H1edEyBKDS", "replyto": "rJxpps_swS", "invitation": "ICLR.cc/2020/Conference/Paper1659/-/Public_Comment", "content": {"comment": "Hi Jason,\n\nI'm not an author of the paper, I guess authors answering will still be anonymous and still marked as authors, like previous years.\n\nI confused CTRL and CTRL-GEN (of which I was not aware of), sorry about that! My fault.\n\n1) I still think they are techniques with very different premises, in particular one is trained and the other is not. A comparison, although being a reasonable request, would be akin to comparing the machine translation capabilities in the GPT-2 paper with a fully supervise MT system, not super informative.\n\n3) My bad here for confusing the papers. Looking at CTRL-GEN I guess you are referring to Table 1 and Figure 3. If that is the case, I believe the topicality evaluated by human judges and the automatic accuracy obtained by a pretrained classifier are not comparable at all. But I think that type of evaluation could be requested to the authors of this paper, using the same independently trained classifier. Moreover, being their classifier being trained on a different SST, it would be an even more fair comparison than using the same classifier as discriminator too.\n\n5) My guess is that a reweighting and renormalization could have a similar effect of increasing the probability of the terms, although I believe if would not increase the probability of related words that are not in the bow, the words highlighted in dark red in the samples. But I agree with you, it would be interesting to see if that is actually the case.\n\n6) That accuracy 0.851 automatically computed score is not comparable with the 0.696 obtained from human evaluation. That said, I think most of the discriminator used in this paper are binary, so using a ternary alternative would be interesting too, although I'm not sure it would add a lot to the paper.", "title": "answering the clarification"}, "signatures": ["~Karl_William_McMara1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Karl_William_McMara1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Plug and Play Language Models: A Simple Approach to Controlled Text Generation", "authors": ["Sumanth Dathathri", "Andrea Madotto", "Janice Lan", "Jane Hung", "Eric Frank", "Piero Molino", "Jason Yosinski", "Rosanne Liu"], "authorids": ["dathathris@gmail.com", "amadotto@connect.ust.hk", "lan.janice.j@gmail.com", "jane.hung@uber.com", "mysterefrank@uber.com", "piero@uber.com", "yosinski@uber.com", "rosanne@uber.com"], "keywords": ["controlled text generation", "generative models", "conditional generative models", "language modeling", "transformer"], "TL;DR": "We control the topic and sentiment of text generation (almost) without any training. ", "abstract": "Large transformer-based language models (LMs) trained on huge text corpora have shown unparalleled generation capabilities. However, controlling attributes of the generated language (e.g. switching topic or sentiment) is difficult without modifying the model architecture or fine-tuning on attribute-specific data and entailing the significant cost of retraining. We propose a simple alternative: the Plug and Play Language Model (PPLM) for controllable language generation, which combines a pretrained LM with one or more simple attribute classifiers that guide text generation without any further training of the LM. In the canonical scenario we present, the attribute models are simple classifiers consisting of a user-specified bag of words or a single learned layer with 100,000 times fewer parameters than the LM. Sampling entails a forward and backward pass in which gradients from the attribute model push the LM's hidden activations and thus guide the generation. Model samples demonstrate control over a range of topics and sentiment styles, and extensive automated and human annotated evaluations show attribute alignment and fluency. PPLMs are flexible in that any combination of differentiable attribute models may be used to steer text generation, which will allow for diverse and creative applications beyond the examples given in this paper.", "pdf": "/pdf/e5bf5d6bd5aa1cbd71c4bb0632cdee0bd9e2b130.pdf", "code": "https://github.com/uber-research/PPLM", "paperhash": "dathathri|plug_and_play_language_models_a_simple_approach_to_controlled_text_generation", "_bibtex": "@inproceedings{\nDathathri2020Plug,\ntitle={Plug and Play Language Models: A Simple Approach to Controlled Text Generation},\nauthor={Sumanth Dathathri and Andrea Madotto and Janice Lan and Jane Hung and Eric Frank and Piero Molino and Jason Yosinski and Rosanne Liu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=H1edEyBKDS}\n}", "original_pdf": "/attachment/e783532ae84827007d75d2865206ef14fcc544db.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1edEyBKDS", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504191620, "tmdate": 1576860565737, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper1659/Authors", "ICLR.cc/2020/Conference/Paper1659/Reviewers", "ICLR.cc/2020/Conference/Paper1659/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1659/-/Public_Comment"}}}, {"id": "B1eC5MPoDH", "original": null, "number": 2, "cdate": 1569579669673, "ddate": null, "tcdate": 1569579669673, "tmdate": 1569579669673, "tddate": null, "forum": "H1edEyBKDS", "replyto": "SkezXrHiPr", "invitation": "ICLR.cc/2020/Conference/Paper1659/-/Public_Comment", "content": {"comment": "This paper is really smart in how it performs conditioning of the generated text.\nUnlike conditional generation and finetuning, which is a huge win, as the authors explain in the text, this method allows plugging any classifier and any bag of words as a way to condition the text, and also to change the intensity and the type of the conditioning along the way and also add additional conditioning aspects after the original LM model is trained, something that conditional methods cannot do.\nFor this reason I believe this work is in a different class with respect to other works and some of your comments are unfair:\n\n1) CTRL was released publically less than a week before the ICLR deadline, which means it was entirely impossible for the authors to add anything about it other than citing it. Moreover, because this PPLM approach can do things that CTRL (and other conditional models) can't do and doesn't require any training, they are on different planes. Comparing them would be like comparing an unsupervised model with a supervised model. In the CTRL paper, moreover, they don't compare with anything neither they run a user study, while in this paper there's both a user study and an automatic evaluation, that together make results really convincing.\n\n2) the comparison with vanilla looks to me like a.way to provide a baseline for fluency, and it's clearly a useful one as the PPLM slightly decreases the fluency in the human evaluation, which is something the reader would want to know. Moreover the authors also have the BC and BR ablations to compare against, which makes the results totally fair.\n\n3) the extent of the conditioning is clearly shown in table 1, 3, 5, 7 and in the appendix S2, S5 and S6. I guess you are referring to table 4 for the first column, the topical one, where PPLM performs better than vanilla (obviously) and the ablations. In that case, the CTRL paper does not report any such human evaluation numbers, so I'm not sure how can you say from those numbers they they seem significantly lower (lower to what?).\n\n4) in the article they both talk about a couple hyperparameters ( the amount of gradient and the number of gradient updates) and they report some parameters that work well in practice, plus they also show in S2 and S5 how much both parameters influence the generation topicality and the fluency degeneration.\n\n5) they KL approach is more principled than the hack you are proposing and could be expanded in the future to non bag of words scenarios, like topic obtained from LDA or other methods that return distributions of words per each topic. I wouldn't call their oversophisticated, I would call yours oversimplified.\n\n6) I guess that was intended to show the strength of the conditioning. I don't believe it exaggerates the real performance of the model at all, it's just a specific choice about what to condition on to obtain a specific result, I don't see anything bad or unfair about it.", "title": "Great paper with really smart dynamic conditioning"}, "signatures": ["~Karl_William_McMara1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Karl_William_McMara1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Plug and Play Language Models: A Simple Approach to Controlled Text Generation", "authors": ["Sumanth Dathathri", "Andrea Madotto", "Janice Lan", "Jane Hung", "Eric Frank", "Piero Molino", "Jason Yosinski", "Rosanne Liu"], "authorids": ["dathathris@gmail.com", "amadotto@connect.ust.hk", "lan.janice.j@gmail.com", "jane.hung@uber.com", "mysterefrank@uber.com", "piero@uber.com", "yosinski@uber.com", "rosanne@uber.com"], "keywords": ["controlled text generation", "generative models", "conditional generative models", "language modeling", "transformer"], "TL;DR": "We control the topic and sentiment of text generation (almost) without any training. ", "abstract": "Large transformer-based language models (LMs) trained on huge text corpora have shown unparalleled generation capabilities. However, controlling attributes of the generated language (e.g. switching topic or sentiment) is difficult without modifying the model architecture or fine-tuning on attribute-specific data and entailing the significant cost of retraining. We propose a simple alternative: the Plug and Play Language Model (PPLM) for controllable language generation, which combines a pretrained LM with one or more simple attribute classifiers that guide text generation without any further training of the LM. In the canonical scenario we present, the attribute models are simple classifiers consisting of a user-specified bag of words or a single learned layer with 100,000 times fewer parameters than the LM. Sampling entails a forward and backward pass in which gradients from the attribute model push the LM's hidden activations and thus guide the generation. Model samples demonstrate control over a range of topics and sentiment styles, and extensive automated and human annotated evaluations show attribute alignment and fluency. PPLMs are flexible in that any combination of differentiable attribute models may be used to steer text generation, which will allow for diverse and creative applications beyond the examples given in this paper.", "pdf": "/pdf/e5bf5d6bd5aa1cbd71c4bb0632cdee0bd9e2b130.pdf", "code": "https://github.com/uber-research/PPLM", "paperhash": "dathathri|plug_and_play_language_models_a_simple_approach_to_controlled_text_generation", "_bibtex": "@inproceedings{\nDathathri2020Plug,\ntitle={Plug and Play Language Models: A Simple Approach to Controlled Text Generation},\nauthor={Sumanth Dathathri and Andrea Madotto and Janice Lan and Jane Hung and Eric Frank and Piero Molino and Jason Yosinski and Rosanne Liu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=H1edEyBKDS}\n}", "original_pdf": "/attachment/e783532ae84827007d75d2865206ef14fcc544db.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1edEyBKDS", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504191620, "tmdate": 1576860565737, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper1659/Authors", "ICLR.cc/2020/Conference/Paper1659/Reviewers", "ICLR.cc/2020/Conference/Paper1659/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1659/-/Public_Comment"}}}, {"id": "SkezXrHiPr", "original": null, "number": 1, "cdate": 1569572121771, "ddate": null, "tcdate": 1569572121771, "tmdate": 1569572645476, "tddate": null, "forum": "H1edEyBKDS", "replyto": "H1edEyBKDS", "invitation": "ICLR.cc/2020/Conference/Paper1659/-/Public_Comment", "content": {"comment": "The idea behind this paper is simple but interesting. However, I personally have some questions about this work and hope to get replies from the authors.\n\n(1) This paper does not provide any comparison with other works. For example, for a publicly available conditional language model, CTRL is only mentioned in related work but not compared as a baseline. As a researcher myself, I completely understand that CTRL is recently released and the authors do not have sufficient time to run CTRL. But I wish to see the results as supplemental material later on OpenReview. On the other hand, there are already several conditional generation model, e.g., S-VAE, CTRL-GEN. The authors should at least compare with these works.\n\n(2) The comparison with the vanilla GPT-2 seems to be unreasonable. The GPT-2 baseline is not provided with any condition-specific information, which surely cannot generate conditional text. For example, if providing the GPT-2 with the CTRL-style prompt (e.g., Rating 5.0/5.0, or Topic: Military), I'd like to see how GPT-2 performs with this setting.\n\n(3) To what end is the generated text conditional? The results in Table 4 seem to be significantly lower than prior methods (e.g., CTRL-GEN).\n\n(4) Will the fluency degenerate along with the increase of conditionality? How to balance that? I didn't find an answer in the article.\n\n(5) The BoW strategy seems to be confusing. If we simply multiply the probability of the keywords by a constant larger than one, will it also do the trick? For example, we manually multiply the probability of {best, excellent, wonderful, ...} by 2, can GPT-2 generate positive samples as well? If we measure the cosine similarity between the pretrained word embeddings of predicted words and the given keywords, can we even get better results? (e.g., it will decrease the probability of words like \"terrible\", \"infamous\" and increase the probability of \"terrific\", \"gorgeous\"). It seems like the work in this paper is an over-sophisticated version of what I just mentioned.\n\n(6) A question about the setting: I wonder whether the setting for sentiment-conditioned generation is too elaborate? Why choose the SST-5 dataset and only use \"Very Positive\" and \"Very Negative\" instead of SST-2 dataset with a binary classification or SST-5 with a three-fold classification (i.e., Positive, Neutral, Negative)? The setting in the paper clearly exaggerates the real performance of the proposed method.\n\nNitpick:\nThe Dist-1 metric in Table S4 is labeled as smaller better, which is in conflict with Table 4.", "title": "Neat tricks but may have some critical defects"}, "signatures": ["~Jason_Brett1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Jason_Brett1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Plug and Play Language Models: A Simple Approach to Controlled Text Generation", "authors": ["Sumanth Dathathri", "Andrea Madotto", "Janice Lan", "Jane Hung", "Eric Frank", "Piero Molino", "Jason Yosinski", "Rosanne Liu"], "authorids": ["dathathris@gmail.com", "amadotto@connect.ust.hk", "lan.janice.j@gmail.com", "jane.hung@uber.com", "mysterefrank@uber.com", "piero@uber.com", "yosinski@uber.com", "rosanne@uber.com"], "keywords": ["controlled text generation", "generative models", "conditional generative models", "language modeling", "transformer"], "TL;DR": "We control the topic and sentiment of text generation (almost) without any training. ", "abstract": "Large transformer-based language models (LMs) trained on huge text corpora have shown unparalleled generation capabilities. However, controlling attributes of the generated language (e.g. switching topic or sentiment) is difficult without modifying the model architecture or fine-tuning on attribute-specific data and entailing the significant cost of retraining. We propose a simple alternative: the Plug and Play Language Model (PPLM) for controllable language generation, which combines a pretrained LM with one or more simple attribute classifiers that guide text generation without any further training of the LM. In the canonical scenario we present, the attribute models are simple classifiers consisting of a user-specified bag of words or a single learned layer with 100,000 times fewer parameters than the LM. Sampling entails a forward and backward pass in which gradients from the attribute model push the LM's hidden activations and thus guide the generation. Model samples demonstrate control over a range of topics and sentiment styles, and extensive automated and human annotated evaluations show attribute alignment and fluency. PPLMs are flexible in that any combination of differentiable attribute models may be used to steer text generation, which will allow for diverse and creative applications beyond the examples given in this paper.", "pdf": "/pdf/e5bf5d6bd5aa1cbd71c4bb0632cdee0bd9e2b130.pdf", "code": "https://github.com/uber-research/PPLM", "paperhash": "dathathri|plug_and_play_language_models_a_simple_approach_to_controlled_text_generation", "_bibtex": "@inproceedings{\nDathathri2020Plug,\ntitle={Plug and Play Language Models: A Simple Approach to Controlled Text Generation},\nauthor={Sumanth Dathathri and Andrea Madotto and Janice Lan and Jane Hung and Eric Frank and Piero Molino and Jason Yosinski and Rosanne Liu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=H1edEyBKDS}\n}", "original_pdf": "/attachment/e783532ae84827007d75d2865206ef14fcc544db.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1edEyBKDS", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504191620, "tmdate": 1576860565737, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper1659/Authors", "ICLR.cc/2020/Conference/Paper1659/Reviewers", "ICLR.cc/2020/Conference/Paper1659/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1659/-/Public_Comment"}}}], "count": 28}