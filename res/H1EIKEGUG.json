{"notes": [{"tddate": null, "ddate": null, "original": null, "tmdate": 1521582761981, "tcdate": 1520643136806, "number": 1, "cdate": 1520643136806, "id": "BJKBnoxYf", "invitation": "ICLR.cc/2018/Workshop/-/Paper16/Official_Review", "forum": "H1EIKEGUG", "replyto": "H1EIKEGUG", "signatures": ["ICLR.cc/2018/Workshop/Paper16/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper16/AnonReviewer2"], "content": {"title": "Agglomerative clustering to prune cnn filters for efficient evaluation. Preliminary work well behind the state-of-the-art. Reject.", "rating": "4: Ok but not good enough - rejection", "review": "The authors propose the use of agglomerative clustering to group and then subsequently sample cnn filters to remove redundancy and speed up the test-time execution of convolutional networks. They show that their approach can be used to speed up VGG and ResNets trained on on CiFar10 by ~25-40% with small degradation (about 5% relative), outperforming a technique recently proposed by Li et al., which prunes cnn filters based on the sum of their absolute weights.\n\nTheir technique, while intuitely appealing and realitively simple to implement, is not established as state-of-the-art. Several important, highly related, and more sophisticated recent papers on efficient evalution of cnns using filter clustering & subspace analysis are not cited or compared to qualitatively or quantitatively (see below). These existing techniques can speed up evalutions by 200% or more with negligible degradation (e.g. [4r] below).\n\nOverall, the paper does not contain any new, ground-breaking ideas or results, and is in a preliminary state. As such, the paper falls well below the acceptance threshold for ICLR. Reject.\n\nSome important & highly related recent papers on the efficient evaluation of cnns: \n\n[1r] Denton, Emily L., Wojciech Zaremba, Joan Bruna, Yann LeCun, and Rob Fergus. \"Exploiting linear structure within convolutional networks for efficient evaluation.\" In Advances in neural information processing systems, pp. 1269-1277. 2014.\n-not cited\n\n[2r] Han, Song, Huizi Mao, and William J. Dally. \"Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding.\" arXiv preprint arXiv:1510.00149 (2015). (ICLR 2016)\n-in bib but not cited\n\n[3r] Han, Song, Jeff Pool, John Tran, and William Dally. \"Learning both weights and connections for efficient neural network.\" In Advances in neural information processing systems, pp. 1135-1143. 2015.\n-in bib but not cited\n\n[4r] Jaderberg, Max, Andrea Vedaldi, and Andrew Zisserman. \"Speeding up convolutional neural networks with low rank expansions.\" arXiv preprint arXiv:1405.3866 (2014).\nhttps://arxiv.org/pdf/1405.3866.pdf", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Building Efficient ConvNets using Redundant Feature Pruning", "abstract": "This paper presents an efficient technique to prune deep and/or wide convolutional neural network models by eliminating redundant features (or filters). Previous studies have shown that over-sized deep neural network models tend to produce\na lot of redundant features that are either shifted version of one another or are very similar and show little or no variations; thus resulting in filtering redundancy. We propose to prune these redundant features along with their connecting feature\nmaps according to their differentiation and based on their relative cosine distances in the feature space, thus yielding smaller network size with reduced inference costs and competitive performance. We empirically show on select models and\nCIFAR-10 dataset that inference costs can be reduced by 40% for VGG-16, 27% for ResNet-56, and 39% for ResNet-110.", "pdf": "/pdf/189aba018f0b30965edbfe6c5b1f02e010ecabc1.pdf", "paperhash": "ayinde|building_efficient_convnets_using_redundant_feature_pruning", "keywords": ["Network compression", "redundant feature pruning", "filter pruning"], "authors": ["Babajide O. Ayinde", "Jacek M. Zurada"], "authorids": ["babajide.ayinde@louisville.edu", "jacek.zuradag@louisville.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582761759, "id": "ICLR.cc/2018/Workshop/-/Paper16/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper16/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper16/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper16/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper16/AnonReviewer1"], "reply": {"forum": "H1EIKEGUG", "replyto": "H1EIKEGUG", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper16/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper16/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582761759}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582735418, "tcdate": 1520667406532, "number": 2, "cdate": 1520667406532, "id": "SyUMobbFM", "invitation": "ICLR.cc/2018/Workshop/-/Paper16/Official_Review", "forum": "H1EIKEGUG", "replyto": "H1EIKEGUG", "signatures": ["ICLR.cc/2018/Workshop/Paper16/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper16/AnonReviewer3"], "content": {"title": "Pruning redundant filters via clustering is not a new idea", "rating": "6: Marginally above acceptance threshold", "review": "Pruning CNN models has been heavily studied in the recent years.   This work presents a new method for pruning based redundancy/similarity. The results show that it compares favorably to Li et al's structural pruning method based on L1 norm, which is encouraging. \nHowever, it should be noted that the idea of using filter similarity for pruning has been previously explored by RoyChowdhury et al. in their paper \"Reducing Duplicate Filters in Deep Neural Networks\". The only difference is that here hierarchical clustering is used to decide redundancy whereas RoyChowdhury et al.'s work uses a simple threshold of similarity to decide what are considered to be \"duplicate\".  This particular work was missing from the paper's discussion and comparison. \n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Building Efficient ConvNets using Redundant Feature Pruning", "abstract": "This paper presents an efficient technique to prune deep and/or wide convolutional neural network models by eliminating redundant features (or filters). Previous studies have shown that over-sized deep neural network models tend to produce\na lot of redundant features that are either shifted version of one another or are very similar and show little or no variations; thus resulting in filtering redundancy. We propose to prune these redundant features along with their connecting feature\nmaps according to their differentiation and based on their relative cosine distances in the feature space, thus yielding smaller network size with reduced inference costs and competitive performance. We empirically show on select models and\nCIFAR-10 dataset that inference costs can be reduced by 40% for VGG-16, 27% for ResNet-56, and 39% for ResNet-110.", "pdf": "/pdf/189aba018f0b30965edbfe6c5b1f02e010ecabc1.pdf", "paperhash": "ayinde|building_efficient_convnets_using_redundant_feature_pruning", "keywords": ["Network compression", "redundant feature pruning", "filter pruning"], "authors": ["Babajide O. Ayinde", "Jacek M. Zurada"], "authorids": ["babajide.ayinde@louisville.edu", "jacek.zuradag@louisville.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582761759, "id": "ICLR.cc/2018/Workshop/-/Paper16/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper16/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper16/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper16/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper16/AnonReviewer1"], "reply": {"forum": "H1EIKEGUG", "replyto": "H1EIKEGUG", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper16/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper16/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582761759}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582669636, "tcdate": 1520744973642, "number": 3, "cdate": 1520744973642, "id": "Bk8M54MFG", "invitation": "ICLR.cc/2018/Workshop/-/Paper16/Official_Review", "forum": "H1EIKEGUG", "replyto": "H1EIKEGUG", "signatures": ["ICLR.cc/2018/Workshop/Paper16/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper16/AnonReviewer1"], "content": {"title": "clustering and pruning redundant filters in neural networks for faster inference", "rating": "6: Marginally above acceptance threshold", "review": "The paper proposes to cluster the filters in each layer of a neural network based on cosine similarities and eliminate redundant filters, i.e., filters that belong to the same cluster. They show their approach outperforms alternative filter pruning methods based on magnitude if weight kernels.\n\nQ: Could the authors clarify what they mean by: \"A new kernel matrix is defined for both lth and (l + 1)th layer of a new smaller model\"\nIf we remove feature channels from a set of feature maps at layer l, since the kernels in the layer above expect all of them present, how do we handle that? Do you mean that we use the cluster representative everywhere instead? I believe I am missing something obvious, but please do clarify. Figure 1 was not very useful either for me to understand.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Building Efficient ConvNets using Redundant Feature Pruning", "abstract": "This paper presents an efficient technique to prune deep and/or wide convolutional neural network models by eliminating redundant features (or filters). Previous studies have shown that over-sized deep neural network models tend to produce\na lot of redundant features that are either shifted version of one another or are very similar and show little or no variations; thus resulting in filtering redundancy. We propose to prune these redundant features along with their connecting feature\nmaps according to their differentiation and based on their relative cosine distances in the feature space, thus yielding smaller network size with reduced inference costs and competitive performance. We empirically show on select models and\nCIFAR-10 dataset that inference costs can be reduced by 40% for VGG-16, 27% for ResNet-56, and 39% for ResNet-110.", "pdf": "/pdf/189aba018f0b30965edbfe6c5b1f02e010ecabc1.pdf", "paperhash": "ayinde|building_efficient_convnets_using_redundant_feature_pruning", "keywords": ["Network compression", "redundant feature pruning", "filter pruning"], "authors": ["Babajide O. Ayinde", "Jacek M. Zurada"], "authorids": ["babajide.ayinde@louisville.edu", "jacek.zuradag@louisville.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582761759, "id": "ICLR.cc/2018/Workshop/-/Paper16/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper16/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper16/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper16/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper16/AnonReviewer1"], "reply": {"forum": "H1EIKEGUG", "replyto": "H1EIKEGUG", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper16/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper16/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582761759}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521573577294, "tcdate": 1521573577294, "number": 145, "cdate": 1521573576957, "id": "rJZCRRRKf", "invitation": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "forum": "H1EIKEGUG", "replyto": "H1EIKEGUG", "signatures": ["ICLR.cc/2018/Workshop/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Program_Chairs"], "content": {"decision": "Reject", "title": "ICLR 2018 Workshop Acceptance Decision", "comment": "Based on the reviews, this paper has not been accepted for presentation at the ICLR workshop. However, the conversation and updates can continue to appear here on OpenReview."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Building Efficient ConvNets using Redundant Feature Pruning", "abstract": "This paper presents an efficient technique to prune deep and/or wide convolutional neural network models by eliminating redundant features (or filters). Previous studies have shown that over-sized deep neural network models tend to produce\na lot of redundant features that are either shifted version of one another or are very similar and show little or no variations; thus resulting in filtering redundancy. We propose to prune these redundant features along with their connecting feature\nmaps according to their differentiation and based on their relative cosine distances in the feature space, thus yielding smaller network size with reduced inference costs and competitive performance. We empirically show on select models and\nCIFAR-10 dataset that inference costs can be reduced by 40% for VGG-16, 27% for ResNet-56, and 39% for ResNet-110.", "pdf": "/pdf/189aba018f0b30965edbfe6c5b1f02e010ecabc1.pdf", "paperhash": "ayinde|building_efficient_convnets_using_redundant_feature_pruning", "keywords": ["Network compression", "redundant feature pruning", "filter pruning"], "authors": ["Babajide O. Ayinde", "Jacek M. Zurada"], "authorids": ["babajide.ayinde@louisville.edu", "jacek.zuradag@louisville.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518629844880, "id": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Program_Chairs"], "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Workshop/-/Submission", "writers": {"values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Workshop/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Workshop Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept", "Reject"]}}}, "nonreaders": [], "noninvitees": [], "cdate": 1518629844880}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1517599933713, "tcdate": 1517599051750, "number": 16, "cdate": 1517599051750, "id": "H1EIKEGUG", "invitation": "ICLR.cc/2018/Workshop/-/Submission", "forum": "H1EIKEGUG", "signatures": ["~Babajide_O_Ayinde1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop"], "content": {"title": "Building Efficient ConvNets using Redundant Feature Pruning", "abstract": "This paper presents an efficient technique to prune deep and/or wide convolutional neural network models by eliminating redundant features (or filters). Previous studies have shown that over-sized deep neural network models tend to produce\na lot of redundant features that are either shifted version of one another or are very similar and show little or no variations; thus resulting in filtering redundancy. We propose to prune these redundant features along with their connecting feature\nmaps according to their differentiation and based on their relative cosine distances in the feature space, thus yielding smaller network size with reduced inference costs and competitive performance. We empirically show on select models and\nCIFAR-10 dataset that inference costs can be reduced by 40% for VGG-16, 27% for ResNet-56, and 39% for ResNet-110.", "pdf": "/pdf/189aba018f0b30965edbfe6c5b1f02e010ecabc1.pdf", "paperhash": "ayinde|building_efficient_convnets_using_redundant_feature_pruning", "keywords": ["Network compression", "redundant feature pruning", "filter pruning"], "authors": ["Babajide O. Ayinde", "Jacek M. Zurada"], "authorids": ["babajide.ayinde@louisville.edu", "jacek.zuradag@louisville.edu"]}, "nonreaders": [], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1518472800000, "tmdate": 1518474081690, "id": "ICLR.cc/2018/Workshop/-/Submission", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Workshop"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2018/Workshop", "description": "Your authorized identity to be associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 9, "value-regex": "upload", "description": "Upload a PDF file that ends with .pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 8, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names. Please provide real names; identities will be anonymized."}, "keywords": {"order": 6, "values-regex": "(^$)|[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of keywords."}, "TL;DR": {"required": false, "order": 7, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,500}"}, "authorids": {"required": true, "order": 3, "values-regex": "([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,},){0,}([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,})", "description": "Comma separated list of author email addresses, lowercased, in the same order as above. For authors with existing OpenReview accounts, please make sure that the provided email address(es) match those listed in the author's profile. Please provide real emails; identities will be anonymized."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1526248800000, "cdate": 1518474081690}}}], "count": 5}