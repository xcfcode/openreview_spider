{"notes": [{"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396440804, "tcdate": 1486396440804, "number": 1, "id": "r1ZmnfIug", "invitation": "ICLR.cc/2017/conference/-/paper219/acceptance", "forum": "BysZhEqee", "replyto": "BysZhEqee", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision", "comment": "The reviewers unanimously recommend rejection."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Marginal Deep Architectures: Deep learning for Small and Middle Scale Applications", "abstract": "In recent years, many deep architectures have been proposed in different fields. However, to obtain good results, most of the previous deep models need a large number of training data. In this paper, for small and middle scale applications, we\npropose a novel deep learning framework based on stacked feature learning models. Particularly, we stack marginal Fisher analysis (MFA) layer by layer for the initialization of the deep architecture and call it \u201cMarginal Deep Architectures\u201d (MDA). In the implementation of MDA, the weight matrices of MFA are first learned layer by layer, and then we exploit some deep learning techniques, such as back propagation, dropout and denoising to fine tune the network. To evaluate the effectiveness of MDA, we have compared it with some feature learning methods and deep learning models on 7 small and middle scale real-world applications, including handwritten digits recognition, speech recognition, historical document understanding, image classification, action recognition and so on. Extensive experiments demonstrate that MDA performs not only better than shallow feature learning models, but also state-of-the-art deep learning models in these applications.", "pdf": "/pdf/e505cfc12d175bb6f600b7348110b72431813073.pdf", "paperhash": "zheng|marginal_deep_architectures_deep_learning_for_small_and_middle_scale_applications", "conflicts": ["ouc.edu.cn"], "keywords": [], "authors": ["Yuchen Zheng", "Guoqiang Zhong", "Junyu Dong"], "authorids": ["ouczyc@outlook.com", "gqzhong@ouc.edu.cn", "dongjunyu@ouc.edu.cn"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396441371, "id": "ICLR.cc/2017/conference/-/paper219/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "BysZhEqee", "replyto": "BysZhEqee", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396441371}}}, {"tddate": null, "tmdate": 1484956044406, "tcdate": 1484956044406, "number": 6, "id": "HJ45ZmeDe", "invitation": "ICLR.cc/2017/conference/-/paper219/public/comment", "forum": "BysZhEqee", "replyto": "SkyEzB_Ee", "signatures": ["~Guoqiang_Zhong1"], "readers": ["everyone"], "writers": ["~Guoqiang_Zhong1"], "content": {"title": "Answer", "comment": "Many thanks for your review!\nFirstly, thanks for pointing out the writing problems. We will revise this paper accordingly.\nSecondly, if large scale of labeled data are available, it's intuitive that it's a good idea to used supervised layer-wise pre-training to learn the deep nets. This is a main difference between our work with other related approaches.\nThirdly, marginal Fisher analysis (MFA) is an algorithm published in TPAMI and it's well known in the area of dimensionality reduction. Hence, we didn't introduce it in very detail. For the alluded denoising, as presented in the paper, it's just for input corruption.\nFourthly, ReLu may leads to sparse representations during the network training. For low dimensional data, it may not work well. Hence, we use sigmoid (logistic) activation function.\nFifthly, for the parameters in the deep networks including the compared ones, it's not possible to tune them one by one, even we have a validation set. However, using fixed parameters, the advantages and disadvantages are shared by all the compared methods. Hence, in our work, we didn't try to greatly optimize the network, but used some default or fixed values for the parameters.\nSixthly, We think that we cannot only focus on large scale data. There are many applications with only small or middle size data. How do we deal with these problems? In this case, MDA shows its advantages as shown in the experiments.\nFinally, thanks for your suggestions about visualization. We will add it in the next version of our paper. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Marginal Deep Architectures: Deep learning for Small and Middle Scale Applications", "abstract": "In recent years, many deep architectures have been proposed in different fields. However, to obtain good results, most of the previous deep models need a large number of training data. In this paper, for small and middle scale applications, we\npropose a novel deep learning framework based on stacked feature learning models. Particularly, we stack marginal Fisher analysis (MFA) layer by layer for the initialization of the deep architecture and call it \u201cMarginal Deep Architectures\u201d (MDA). In the implementation of MDA, the weight matrices of MFA are first learned layer by layer, and then we exploit some deep learning techniques, such as back propagation, dropout and denoising to fine tune the network. To evaluate the effectiveness of MDA, we have compared it with some feature learning methods and deep learning models on 7 small and middle scale real-world applications, including handwritten digits recognition, speech recognition, historical document understanding, image classification, action recognition and so on. Extensive experiments demonstrate that MDA performs not only better than shallow feature learning models, but also state-of-the-art deep learning models in these applications.", "pdf": "/pdf/e505cfc12d175bb6f600b7348110b72431813073.pdf", "paperhash": "zheng|marginal_deep_architectures_deep_learning_for_small_and_middle_scale_applications", "conflicts": ["ouc.edu.cn"], "keywords": [], "authors": ["Yuchen Zheng", "Guoqiang Zhong", "Junyu Dong"], "authorids": ["ouczyc@outlook.com", "gqzhong@ouc.edu.cn", "dongjunyu@ouc.edu.cn"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287679100, "id": "ICLR.cc/2017/conference/-/paper219/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BysZhEqee", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper219/reviewers", "ICLR.cc/2017/conference/paper219/areachairs"], "cdate": 1485287679100}}}, {"tddate": null, "tmdate": 1484954400248, "tcdate": 1484954400248, "number": 5, "id": "ryumiMgPe", "invitation": "ICLR.cc/2017/conference/-/paper219/public/comment", "forum": "BysZhEqee", "replyto": "HyjC1ZHVl", "signatures": ["~Guoqiang_Zhong1"], "readers": ["everyone"], "writers": ["~Guoqiang_Zhong1"], "content": {"title": "Answer", "comment": "Many thanks for your comments!\nFirstly, for sure, the compared approaches, deep autoencoder (DA), stacked denoising autoencoders (SDA) and others, have been fine-tuned by back propagation. It's obvious that, they are pre-trained layer by layer. But, the fine-tuning is definitely by back propagation. \nSecondly, random initialized deep models such as DBN (somewhat equivalent to DA) performed worse than the proposed method, MDA. And moreover, since we consider inputs of the vector form in this paper, CNN cannot perform well also.\nThirdly, like DA and SDA, to use MDA, one can define the number of layers and number of neurons in each layer. For the hyper-parameters, like that in other deep models, they can be pre-defined with cross validation or learned using back propagation.\nFourthly, dropout is widely in the deep learning area. It is used here in the common way of the deep learning models. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Marginal Deep Architectures: Deep learning for Small and Middle Scale Applications", "abstract": "In recent years, many deep architectures have been proposed in different fields. However, to obtain good results, most of the previous deep models need a large number of training data. In this paper, for small and middle scale applications, we\npropose a novel deep learning framework based on stacked feature learning models. Particularly, we stack marginal Fisher analysis (MFA) layer by layer for the initialization of the deep architecture and call it \u201cMarginal Deep Architectures\u201d (MDA). In the implementation of MDA, the weight matrices of MFA are first learned layer by layer, and then we exploit some deep learning techniques, such as back propagation, dropout and denoising to fine tune the network. To evaluate the effectiveness of MDA, we have compared it with some feature learning methods and deep learning models on 7 small and middle scale real-world applications, including handwritten digits recognition, speech recognition, historical document understanding, image classification, action recognition and so on. Extensive experiments demonstrate that MDA performs not only better than shallow feature learning models, but also state-of-the-art deep learning models in these applications.", "pdf": "/pdf/e505cfc12d175bb6f600b7348110b72431813073.pdf", "paperhash": "zheng|marginal_deep_architectures_deep_learning_for_small_and_middle_scale_applications", "conflicts": ["ouc.edu.cn"], "keywords": [], "authors": ["Yuchen Zheng", "Guoqiang Zhong", "Junyu Dong"], "authorids": ["ouczyc@outlook.com", "gqzhong@ouc.edu.cn", "dongjunyu@ouc.edu.cn"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287679100, "id": "ICLR.cc/2017/conference/-/paper219/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BysZhEqee", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper219/reviewers", "ICLR.cc/2017/conference/paper219/areachairs"], "cdate": 1485287679100}}}, {"tddate": null, "tmdate": 1484953527058, "tcdate": 1484953527058, "number": 4, "id": "SkJ6wMlPg", "invitation": "ICLR.cc/2017/conference/-/paper219/public/comment", "forum": "BysZhEqee", "replyto": "Skrw_lBVe", "signatures": ["~Guoqiang_Zhong1"], "readers": ["everyone"], "writers": ["~Guoqiang_Zhong1"], "content": {"title": "Answer", "comment": "Many thanks for your review!\nFirstly, please notice that we have compared the proposed method with related work on the CIFAR-10 data set in Section 4.4. The results are shown in Table 6, where deep autoencoders (DA), stacked denoising autoencoders (SDA) and other deep learning models, as baseline approaches, have been compared with. For sure, these compared approaches are trained using supervised back propagation.\nSecondly, in this paper we propose a new deep learning algorithm that is initialized based on supervised layer-wise pre-training. The contribution is not focused on the marginal Fisher analysis (MFA) algorithm. Moreover, the association matrix A is computed during model training. It's totally offline and can be approximated in the case of large scale applications. \nThirdly, yes, there are some similar work using stacked modules to build deep architectures, such as DA, SDA and SPCANet. However, none of them is initialized using supervised layer-wise pre-training, which is a main contribution of our paper.\nFourthly, please notice that there is no a deep network which can solve any problems. For example, conv net, which may perform well on large 2D images, will not perform well on the inputs of the vector form. For the applications with vector inputs, do we still need to compare the proposed method with conv net? In contrast, we compared the proposed method with more related approaches, such as DA, SDA and others.   \nFifthly, from the experimental results, we can see that MDA performs much better than DA, SDA and other deep networks, and it's well known from many deep learning literatures that these deep learning methods perform well compared to random initialized networks. Why should we compare MDA with random weight matrices?\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Marginal Deep Architectures: Deep learning for Small and Middle Scale Applications", "abstract": "In recent years, many deep architectures have been proposed in different fields. However, to obtain good results, most of the previous deep models need a large number of training data. In this paper, for small and middle scale applications, we\npropose a novel deep learning framework based on stacked feature learning models. Particularly, we stack marginal Fisher analysis (MFA) layer by layer for the initialization of the deep architecture and call it \u201cMarginal Deep Architectures\u201d (MDA). In the implementation of MDA, the weight matrices of MFA are first learned layer by layer, and then we exploit some deep learning techniques, such as back propagation, dropout and denoising to fine tune the network. To evaluate the effectiveness of MDA, we have compared it with some feature learning methods and deep learning models on 7 small and middle scale real-world applications, including handwritten digits recognition, speech recognition, historical document understanding, image classification, action recognition and so on. Extensive experiments demonstrate that MDA performs not only better than shallow feature learning models, but also state-of-the-art deep learning models in these applications.", "pdf": "/pdf/e505cfc12d175bb6f600b7348110b72431813073.pdf", "paperhash": "zheng|marginal_deep_architectures_deep_learning_for_small_and_middle_scale_applications", "conflicts": ["ouc.edu.cn"], "keywords": [], "authors": ["Yuchen Zheng", "Guoqiang Zhong", "Junyu Dong"], "authorids": ["ouczyc@outlook.com", "gqzhong@ouc.edu.cn", "dongjunyu@ouc.edu.cn"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287679100, "id": "ICLR.cc/2017/conference/-/paper219/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BysZhEqee", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper219/reviewers", "ICLR.cc/2017/conference/paper219/areachairs"], "cdate": 1485287679100}}}, {"tddate": null, "tmdate": 1482342951528, "tcdate": 1482342951528, "number": 3, "id": "SkyEzB_Ee", "invitation": "ICLR.cc/2017/conference/-/paper219/official/review", "forum": "BysZhEqee", "replyto": "BysZhEqee", "signatures": ["ICLR.cc/2017/conference/paper219/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper219/AnonReviewer3"], "content": {"title": "Review", "rating": "3: Clear rejection", "review": "The proposed approach consists in a greedy layer wise initialization strategy for a deep MLP model, which is followed by global gradient-descent with dropout for fine-tuning. The initialization strategy uses a first randomly initialized sigmoid layer for dimensionality expansion followed by 2 sigmoid layers whose weights are initialized by Marginal Fisher Analysis (MFA) which learns a linear dimensionality reduction based on a neighborhood graph constructed using class label information (i.e. supervised dimensionality reduction). Output layer is a standard softmax layer.\n\nThe approach is thus to be added to a growing list of heuristic layer-wise initialization schemes.\nThe particular choice of initialization strategy, while reasonable, is not sufficiently well motivated in the paper relative to alternatives, and thus feels rather arbitrary.\nThe paper lacks clarity in the description of the approach:  MFA is poorly explained with undefined notations (in Eq. 4, what is A? It has not been properly defined); the precise use of alluded denoising in the model is also unclear (is there really training of an additional denoting objective, or just input corruption?).\n\nThe question of the (arguably mild) inconsistency of applying a linear dimensionality reduction algorithm, that is trained without any sigmoid, and then passing its learned representation through a sigmoid is not even raised. This, in addition to the fact that sigmoid hidden layers are no longer commonly used (why did you not also consider using RELUs?).\n\nMore importantly I suspect methodological problems with the experimental comparisons: the paper mentions using *default* values for learning-rate and momentum, and having (arbitrarily?) fixed epoch to 400 (no early stopping?) and L2 regularization to 1e-4 for some models. \n*All* hyper parameters should always be properly hyper-optimized using a validation set (or cross-validation) including early-stopping, and this separately for each model under comparison (ideally also including layer sizes). This is all the more important since you are considering smallish datasets, so that the various initialization strategies act mainly as different indirect regularization schemes: they thus need to be carefully tuned. This casts serious doubts as to the amount of hyper-parameter tuning (close to none?) that went into training the alternative models used for comparison. \n\nThe Marginal Fisher Analysis dimensionality reduction initialization strategy may well offer advantages, but as it currently stands this paper doesn\u2019t yet make a sufficiently convincing case for it, nor provide useful insights into the nature of the expected advantages.\n\nI would also suggest, for image inputs such as CIFAR10, to use the qualitative tool of showing the filters (back projected to input space) learned by the different initialization schemes under consideration, as this could help visually gain insight as to what sets methods apart. \n\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Marginal Deep Architectures: Deep learning for Small and Middle Scale Applications", "abstract": "In recent years, many deep architectures have been proposed in different fields. However, to obtain good results, most of the previous deep models need a large number of training data. In this paper, for small and middle scale applications, we\npropose a novel deep learning framework based on stacked feature learning models. Particularly, we stack marginal Fisher analysis (MFA) layer by layer for the initialization of the deep architecture and call it \u201cMarginal Deep Architectures\u201d (MDA). In the implementation of MDA, the weight matrices of MFA are first learned layer by layer, and then we exploit some deep learning techniques, such as back propagation, dropout and denoising to fine tune the network. To evaluate the effectiveness of MDA, we have compared it with some feature learning methods and deep learning models on 7 small and middle scale real-world applications, including handwritten digits recognition, speech recognition, historical document understanding, image classification, action recognition and so on. Extensive experiments demonstrate that MDA performs not only better than shallow feature learning models, but also state-of-the-art deep learning models in these applications.", "pdf": "/pdf/e505cfc12d175bb6f600b7348110b72431813073.pdf", "paperhash": "zheng|marginal_deep_architectures_deep_learning_for_small_and_middle_scale_applications", "conflicts": ["ouc.edu.cn"], "keywords": [], "authors": ["Yuchen Zheng", "Guoqiang Zhong", "Junyu Dong"], "authorids": ["ouczyc@outlook.com", "gqzhong@ouc.edu.cn", "dongjunyu@ouc.edu.cn"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512660020, "id": "ICLR.cc/2017/conference/-/paper219/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper219/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper219/AnonReviewer2", "ICLR.cc/2017/conference/paper219/AnonReviewer1", "ICLR.cc/2017/conference/paper219/AnonReviewer3"], "reply": {"forum": "BysZhEqee", "replyto": "BysZhEqee", "writers": {"values-regex": "ICLR.cc/2017/conference/paper219/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper219/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512660020}}}, {"tddate": null, "tmdate": 1482129362645, "tcdate": 1482129362645, "number": 2, "id": "HyjC1ZHVl", "invitation": "ICLR.cc/2017/conference/-/paper219/official/review", "forum": "BysZhEqee", "replyto": "BysZhEqee", "signatures": ["ICLR.cc/2017/conference/paper219/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper219/AnonReviewer1"], "content": {"title": "Interesting motivation but fail to justify the proposed method", "rating": "4: Ok but not good enough - rejection", "review": "The authors pointed out some limitations of existing deep architectures, in particular hard to optimize on small or mid size datasets, and proposed to stack marginal fisher analysis (MFA) to build deep models. The proposed method is tested on several small to mid size datasets and compared with several feature learning methods. The authors also applied some existing techniques in deep learning, such as backprop, denoising and dropout to improve performance. \n\nThe new contribution of the paper is limited. MFA has long been proposed. The authors fail to theoretically or empirically justify the stacking of MFAs. The authors did not include any deep architectures that requires backprop over multiple layers in the comparison, which the authors set out to address, instead all the methods compared were learned layer by layer. Will a randomly initialized deep model such as DBN or CNN perform poorly on these datasets? It is also not clear how the authors came up with each particular model architecture and hyper-parameters used in the different datasets. The writing of the paper needs to be significantly improved. A lot of details were omitted, for example, how is dropout applied in the MFA. ", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Marginal Deep Architectures: Deep learning for Small and Middle Scale Applications", "abstract": "In recent years, many deep architectures have been proposed in different fields. However, to obtain good results, most of the previous deep models need a large number of training data. In this paper, for small and middle scale applications, we\npropose a novel deep learning framework based on stacked feature learning models. Particularly, we stack marginal Fisher analysis (MFA) layer by layer for the initialization of the deep architecture and call it \u201cMarginal Deep Architectures\u201d (MDA). In the implementation of MDA, the weight matrices of MFA are first learned layer by layer, and then we exploit some deep learning techniques, such as back propagation, dropout and denoising to fine tune the network. To evaluate the effectiveness of MDA, we have compared it with some feature learning methods and deep learning models on 7 small and middle scale real-world applications, including handwritten digits recognition, speech recognition, historical document understanding, image classification, action recognition and so on. Extensive experiments demonstrate that MDA performs not only better than shallow feature learning models, but also state-of-the-art deep learning models in these applications.", "pdf": "/pdf/e505cfc12d175bb6f600b7348110b72431813073.pdf", "paperhash": "zheng|marginal_deep_architectures_deep_learning_for_small_and_middle_scale_applications", "conflicts": ["ouc.edu.cn"], "keywords": [], "authors": ["Yuchen Zheng", "Guoqiang Zhong", "Junyu Dong"], "authorids": ["ouczyc@outlook.com", "gqzhong@ouc.edu.cn", "dongjunyu@ouc.edu.cn"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512660020, "id": "ICLR.cc/2017/conference/-/paper219/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper219/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper219/AnonReviewer2", "ICLR.cc/2017/conference/paper219/AnonReviewer1", "ICLR.cc/2017/conference/paper219/AnonReviewer3"], "reply": {"forum": "BysZhEqee", "replyto": "BysZhEqee", "writers": {"values-regex": "ICLR.cc/2017/conference/paper219/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper219/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512660020}}}, {"tddate": null, "tmdate": 1482127452607, "tcdate": 1482127452607, "number": 1, "id": "Skrw_lBVe", "invitation": "ICLR.cc/2017/conference/-/paper219/official/review", "forum": "BysZhEqee", "replyto": "BysZhEqee", "signatures": ["ICLR.cc/2017/conference/paper219/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper219/AnonReviewer2"], "content": {"title": "A deep learning model is proposed where layerwise stacking of a new type of layer (Marginal Fisher Analysis) is used.", "rating": "4: Ok but not good enough - rejection", "review": "This paper proposes to initialize the weights of a deep neural network layer-wise with a marginal Fisher analysis model, making use of potentially the similarity metric.\n \nPros: \nThere are a lot of experiments, albeit small datasets, that the authors tested their proposed method on.\n\nCons:\nlacking baseline such as discriminatively trained convolutional network on standard dataset such as CIFAR-10.\nIt is also unclear how costly in computation to compute the association matrix A in equation 4.\n\nThis is an OK paper, where a new idea is proposed, and combined with other existing ideas such as greedy-layerwise stacking, dropout, and denoising auto-encoders.\nHowever, there have been many papers with similar ideas perhaps 3-5 years ago, e.g. SPCANet. \n\nTherefore, the main novelty is the use of marginal Fisher Analysis as a new layer. This would be ok, but the baselines to demonstrate that this approach works better is missing. In particular, I'd like to see a conv net or fully connected net trained from scratch with good initialization would do at these problems.\n\nTo improve the paper, the authors should try to demonstrate without doubt that initializing layers with MFA is better than just random weight matrices.\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Marginal Deep Architectures: Deep learning for Small and Middle Scale Applications", "abstract": "In recent years, many deep architectures have been proposed in different fields. However, to obtain good results, most of the previous deep models need a large number of training data. In this paper, for small and middle scale applications, we\npropose a novel deep learning framework based on stacked feature learning models. Particularly, we stack marginal Fisher analysis (MFA) layer by layer for the initialization of the deep architecture and call it \u201cMarginal Deep Architectures\u201d (MDA). In the implementation of MDA, the weight matrices of MFA are first learned layer by layer, and then we exploit some deep learning techniques, such as back propagation, dropout and denoising to fine tune the network. To evaluate the effectiveness of MDA, we have compared it with some feature learning methods and deep learning models on 7 small and middle scale real-world applications, including handwritten digits recognition, speech recognition, historical document understanding, image classification, action recognition and so on. Extensive experiments demonstrate that MDA performs not only better than shallow feature learning models, but also state-of-the-art deep learning models in these applications.", "pdf": "/pdf/e505cfc12d175bb6f600b7348110b72431813073.pdf", "paperhash": "zheng|marginal_deep_architectures_deep_learning_for_small_and_middle_scale_applications", "conflicts": ["ouc.edu.cn"], "keywords": [], "authors": ["Yuchen Zheng", "Guoqiang Zhong", "Junyu Dong"], "authorids": ["ouczyc@outlook.com", "gqzhong@ouc.edu.cn", "dongjunyu@ouc.edu.cn"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512660020, "id": "ICLR.cc/2017/conference/-/paper219/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper219/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper219/AnonReviewer2", "ICLR.cc/2017/conference/paper219/AnonReviewer1", "ICLR.cc/2017/conference/paper219/AnonReviewer3"], "reply": {"forum": "BysZhEqee", "replyto": "BysZhEqee", "writers": {"values-regex": "ICLR.cc/2017/conference/paper219/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper219/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512660020}}}, {"tddate": null, "tmdate": 1481187615545, "tcdate": 1481187615539, "number": 3, "id": "BkPmZj8me", "invitation": "ICLR.cc/2017/conference/-/paper219/public/comment", "forum": "BysZhEqee", "replyto": "SkiiN5LXg", "signatures": ["~Guoqiang_Zhong1"], "readers": ["everyone"], "writers": ["~Guoqiang_Zhong1"], "content": {"title": "Answer", "comment": "Thank you so much for your review!\n1. For an example of matrix A, it can be a graph Laplacian, which is widely used in manifold learning and semi-supervised learning. For more details about MFA, please refer to [Shuicheng Yan et al., TPAMI 2007].\n2. The random projection step is to map the learned new representations of data into a high-dimensional space. We can consider it as a trick in deep learning, as many deep learning models include such a layer to map the activation outputs of one layer into a higher dimensional space. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Marginal Deep Architectures: Deep learning for Small and Middle Scale Applications", "abstract": "In recent years, many deep architectures have been proposed in different fields. However, to obtain good results, most of the previous deep models need a large number of training data. In this paper, for small and middle scale applications, we\npropose a novel deep learning framework based on stacked feature learning models. Particularly, we stack marginal Fisher analysis (MFA) layer by layer for the initialization of the deep architecture and call it \u201cMarginal Deep Architectures\u201d (MDA). In the implementation of MDA, the weight matrices of MFA are first learned layer by layer, and then we exploit some deep learning techniques, such as back propagation, dropout and denoising to fine tune the network. To evaluate the effectiveness of MDA, we have compared it with some feature learning methods and deep learning models on 7 small and middle scale real-world applications, including handwritten digits recognition, speech recognition, historical document understanding, image classification, action recognition and so on. Extensive experiments demonstrate that MDA performs not only better than shallow feature learning models, but also state-of-the-art deep learning models in these applications.", "pdf": "/pdf/e505cfc12d175bb6f600b7348110b72431813073.pdf", "paperhash": "zheng|marginal_deep_architectures_deep_learning_for_small_and_middle_scale_applications", "conflicts": ["ouc.edu.cn"], "keywords": [], "authors": ["Yuchen Zheng", "Guoqiang Zhong", "Junyu Dong"], "authorids": ["ouczyc@outlook.com", "gqzhong@ouc.edu.cn", "dongjunyu@ouc.edu.cn"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287679100, "id": "ICLR.cc/2017/conference/-/paper219/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BysZhEqee", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper219/reviewers", "ICLR.cc/2017/conference/paper219/areachairs"], "cdate": 1485287679100}}}, {"tddate": null, "tmdate": 1481184418789, "tcdate": 1481184418783, "number": 3, "id": "SkiiN5LXg", "invitation": "ICLR.cc/2017/conference/-/paper219/pre-review/question", "forum": "BysZhEqee", "replyto": "BysZhEqee", "signatures": ["ICLR.cc/2017/conference/paper219/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper219/AnonReviewer1"], "content": {"title": "Clarification", "question": "1. Can you give an example on how the matrix A in equation (4) is constructed for your experiments? \n2. How important is the random projection step?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Marginal Deep Architectures: Deep learning for Small and Middle Scale Applications", "abstract": "In recent years, many deep architectures have been proposed in different fields. However, to obtain good results, most of the previous deep models need a large number of training data. In this paper, for small and middle scale applications, we\npropose a novel deep learning framework based on stacked feature learning models. Particularly, we stack marginal Fisher analysis (MFA) layer by layer for the initialization of the deep architecture and call it \u201cMarginal Deep Architectures\u201d (MDA). In the implementation of MDA, the weight matrices of MFA are first learned layer by layer, and then we exploit some deep learning techniques, such as back propagation, dropout and denoising to fine tune the network. To evaluate the effectiveness of MDA, we have compared it with some feature learning methods and deep learning models on 7 small and middle scale real-world applications, including handwritten digits recognition, speech recognition, historical document understanding, image classification, action recognition and so on. Extensive experiments demonstrate that MDA performs not only better than shallow feature learning models, but also state-of-the-art deep learning models in these applications.", "pdf": "/pdf/e505cfc12d175bb6f600b7348110b72431813073.pdf", "paperhash": "zheng|marginal_deep_architectures_deep_learning_for_small_and_middle_scale_applications", "conflicts": ["ouc.edu.cn"], "keywords": [], "authors": ["Yuchen Zheng", "Guoqiang Zhong", "Junyu Dong"], "authorids": ["ouczyc@outlook.com", "gqzhong@ouc.edu.cn", "dongjunyu@ouc.edu.cn"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481184419300, "id": "ICLR.cc/2017/conference/-/paper219/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper219/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper219/AnonReviewer2", "ICLR.cc/2017/conference/paper219/AnonReviewer3", "ICLR.cc/2017/conference/paper219/AnonReviewer1"], "reply": {"forum": "BysZhEqee", "replyto": "BysZhEqee", "writers": {"values-regex": "ICLR.cc/2017/conference/paper219/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper219/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481184419300}}}, {"tddate": null, "tmdate": 1481183483857, "tcdate": 1481183483851, "number": 2, "id": "SyNZbq8Xl", "invitation": "ICLR.cc/2017/conference/-/paper219/public/comment", "forum": "BysZhEqee", "replyto": "HkaZvXZmx", "signatures": ["~Guoqiang_Zhong1"], "readers": ["everyone"], "writers": ["~Guoqiang_Zhong1"], "content": {"title": "Answer", "comment": "Thank you so much for your review!\n1. The first one. The weights learned by MFA are used as initialization and the whole network is then fine-tuned by back-propagation.\n2. In our experiments, we implemented MFA on the whole dataset. On large scale of data, one can use a subset of the data for the learning of MFA.\n3. MFA is a supervised feature learning method, for the purpose of classification, which can learn more effective representations of data than PCA and random initialization. For the initialization of the whole nets, intuitively, stacked MFAs can deliver a starting point close to a better local minimum than what its counterparts can do. This is the reason why we think MFA can work better than PCA or DAE or other layerwise initialization strategy.\n4. We have compared MDA with the standard random initialization + dropout training, whose results are similar to that without dropout. Therefore, for the space limitation of the table, we didn't report them."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Marginal Deep Architectures: Deep learning for Small and Middle Scale Applications", "abstract": "In recent years, many deep architectures have been proposed in different fields. However, to obtain good results, most of the previous deep models need a large number of training data. In this paper, for small and middle scale applications, we\npropose a novel deep learning framework based on stacked feature learning models. Particularly, we stack marginal Fisher analysis (MFA) layer by layer for the initialization of the deep architecture and call it \u201cMarginal Deep Architectures\u201d (MDA). In the implementation of MDA, the weight matrices of MFA are first learned layer by layer, and then we exploit some deep learning techniques, such as back propagation, dropout and denoising to fine tune the network. To evaluate the effectiveness of MDA, we have compared it with some feature learning methods and deep learning models on 7 small and middle scale real-world applications, including handwritten digits recognition, speech recognition, historical document understanding, image classification, action recognition and so on. Extensive experiments demonstrate that MDA performs not only better than shallow feature learning models, but also state-of-the-art deep learning models in these applications.", "pdf": "/pdf/e505cfc12d175bb6f600b7348110b72431813073.pdf", "paperhash": "zheng|marginal_deep_architectures_deep_learning_for_small_and_middle_scale_applications", "conflicts": ["ouc.edu.cn"], "keywords": [], "authors": ["Yuchen Zheng", "Guoqiang Zhong", "Junyu Dong"], "authorids": ["ouczyc@outlook.com", "gqzhong@ouc.edu.cn", "dongjunyu@ouc.edu.cn"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287679100, "id": "ICLR.cc/2017/conference/-/paper219/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BysZhEqee", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper219/reviewers", "ICLR.cc/2017/conference/paper219/areachairs"], "cdate": 1485287679100}}}, {"tddate": null, "tmdate": 1480828821617, "tcdate": 1480828821611, "number": 1, "id": "HyCqvQ-Qe", "invitation": "ICLR.cc/2017/conference/-/paper219/public/comment", "forum": "BysZhEqee", "replyto": "SJpTpJZmx", "signatures": ["~Guoqiang_Zhong1"], "readers": ["everyone"], "writers": ["~Guoqiang_Zhong1"], "content": {"title": "Answer", "comment": "Many thanks for your comments!\n1. In equation 3, \"a^k\" is the activation output of the k-th hidden layer. \nBy the way, do you mean the matrix \"A\" in equation 4? It's the similarity matrix between data. Sorry for the missing of its introduction. \n2. Since the inputs of MDA are in the form of vectors, while the input of classic CNNs are gray or color images, the most suitable baseline of MDA should be deep neural networks that deal with vector inputs, like deep autoencoders or stacked denoising autoencoders. In this work, we focus on deep architectures with vector inputs is because there are many applications where data are represented in the form of vectors, such as that in the domain of finance. Moreover, in many deep models, there are some fully connected layers. Indeed, the data are represented in the form of vectors here, and need to be dealt with subsequently. For fair comparison with CNNs, we transform the color images of CIFAR-10 to gray images, and use 1024 dimensional vectors as inputs, which is just as same as MDA. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Marginal Deep Architectures: Deep learning for Small and Middle Scale Applications", "abstract": "In recent years, many deep architectures have been proposed in different fields. However, to obtain good results, most of the previous deep models need a large number of training data. In this paper, for small and middle scale applications, we\npropose a novel deep learning framework based on stacked feature learning models. Particularly, we stack marginal Fisher analysis (MFA) layer by layer for the initialization of the deep architecture and call it \u201cMarginal Deep Architectures\u201d (MDA). In the implementation of MDA, the weight matrices of MFA are first learned layer by layer, and then we exploit some deep learning techniques, such as back propagation, dropout and denoising to fine tune the network. To evaluate the effectiveness of MDA, we have compared it with some feature learning methods and deep learning models on 7 small and middle scale real-world applications, including handwritten digits recognition, speech recognition, historical document understanding, image classification, action recognition and so on. Extensive experiments demonstrate that MDA performs not only better than shallow feature learning models, but also state-of-the-art deep learning models in these applications.", "pdf": "/pdf/e505cfc12d175bb6f600b7348110b72431813073.pdf", "paperhash": "zheng|marginal_deep_architectures_deep_learning_for_small_and_middle_scale_applications", "conflicts": ["ouc.edu.cn"], "keywords": [], "authors": ["Yuchen Zheng", "Guoqiang Zhong", "Junyu Dong"], "authorids": ["ouczyc@outlook.com", "gqzhong@ouc.edu.cn", "dongjunyu@ouc.edu.cn"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287679100, "id": "ICLR.cc/2017/conference/-/paper219/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BysZhEqee", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper219/reviewers", "ICLR.cc/2017/conference/paper219/areachairs"], "cdate": 1485287679100}}}, {"tddate": null, "tmdate": 1480828677064, "tcdate": 1480828677058, "number": 2, "id": "HkaZvXZmx", "invitation": "ICLR.cc/2017/conference/-/paper219/pre-review/question", "forum": "BysZhEqee", "replyto": "BysZhEqee", "signatures": ["ICLR.cc/2017/conference/paper219/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper219/AnonReviewer3"], "content": {"title": "Clarifications", "question": "1. Could you clarify whether the weights learned by MFA are used as initialization and the whole network is then fine-tuned by back-propagation? Or if MFA is performed during training and only the last layer is then learned in a supervised fashion? \n2. Could you clarify whether MFA is performed for each mini-batch or on the whole dataset?\n3. Why do you think MFA should work better than PCA or DAE or other layerwise initialization strategy? \n4. Why did you not also compare with standard random initialization + dropout training (i.e. no fancy layerwise initialization strategy)?\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Marginal Deep Architectures: Deep learning for Small and Middle Scale Applications", "abstract": "In recent years, many deep architectures have been proposed in different fields. However, to obtain good results, most of the previous deep models need a large number of training data. In this paper, for small and middle scale applications, we\npropose a novel deep learning framework based on stacked feature learning models. Particularly, we stack marginal Fisher analysis (MFA) layer by layer for the initialization of the deep architecture and call it \u201cMarginal Deep Architectures\u201d (MDA). In the implementation of MDA, the weight matrices of MFA are first learned layer by layer, and then we exploit some deep learning techniques, such as back propagation, dropout and denoising to fine tune the network. To evaluate the effectiveness of MDA, we have compared it with some feature learning methods and deep learning models on 7 small and middle scale real-world applications, including handwritten digits recognition, speech recognition, historical document understanding, image classification, action recognition and so on. Extensive experiments demonstrate that MDA performs not only better than shallow feature learning models, but also state-of-the-art deep learning models in these applications.", "pdf": "/pdf/e505cfc12d175bb6f600b7348110b72431813073.pdf", "paperhash": "zheng|marginal_deep_architectures_deep_learning_for_small_and_middle_scale_applications", "conflicts": ["ouc.edu.cn"], "keywords": [], "authors": ["Yuchen Zheng", "Guoqiang Zhong", "Junyu Dong"], "authorids": ["ouczyc@outlook.com", "gqzhong@ouc.edu.cn", "dongjunyu@ouc.edu.cn"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481184419300, "id": "ICLR.cc/2017/conference/-/paper219/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper219/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper219/AnonReviewer2", "ICLR.cc/2017/conference/paper219/AnonReviewer3", "ICLR.cc/2017/conference/paper219/AnonReviewer1"], "reply": {"forum": "BysZhEqee", "replyto": "BysZhEqee", "writers": {"values-regex": "ICLR.cc/2017/conference/paper219/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper219/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481184419300}}}, {"tddate": null, "tmdate": 1480814020869, "tcdate": 1480814020865, "number": 1, "id": "SJpTpJZmx", "invitation": "ICLR.cc/2017/conference/-/paper219/pre-review/question", "forum": "BysZhEqee", "replyto": "BysZhEqee", "signatures": ["ICLR.cc/2017/conference/paper219/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper219/AnonReviewer2"], "content": {"title": "Clarifications on notation", "question": "What is the A matrix in equation 3?\n\nFor classification on Cifar-10, wouldn't the simplest baseline be to compare MFA to state-of-the-art CNNs trained from random weights?\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Marginal Deep Architectures: Deep learning for Small and Middle Scale Applications", "abstract": "In recent years, many deep architectures have been proposed in different fields. However, to obtain good results, most of the previous deep models need a large number of training data. In this paper, for small and middle scale applications, we\npropose a novel deep learning framework based on stacked feature learning models. Particularly, we stack marginal Fisher analysis (MFA) layer by layer for the initialization of the deep architecture and call it \u201cMarginal Deep Architectures\u201d (MDA). In the implementation of MDA, the weight matrices of MFA are first learned layer by layer, and then we exploit some deep learning techniques, such as back propagation, dropout and denoising to fine tune the network. To evaluate the effectiveness of MDA, we have compared it with some feature learning methods and deep learning models on 7 small and middle scale real-world applications, including handwritten digits recognition, speech recognition, historical document understanding, image classification, action recognition and so on. Extensive experiments demonstrate that MDA performs not only better than shallow feature learning models, but also state-of-the-art deep learning models in these applications.", "pdf": "/pdf/e505cfc12d175bb6f600b7348110b72431813073.pdf", "paperhash": "zheng|marginal_deep_architectures_deep_learning_for_small_and_middle_scale_applications", "conflicts": ["ouc.edu.cn"], "keywords": [], "authors": ["Yuchen Zheng", "Guoqiang Zhong", "Junyu Dong"], "authorids": ["ouczyc@outlook.com", "gqzhong@ouc.edu.cn", "dongjunyu@ouc.edu.cn"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481184419300, "id": "ICLR.cc/2017/conference/-/paper219/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper219/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper219/AnonReviewer2", "ICLR.cc/2017/conference/paper219/AnonReviewer3", "ICLR.cc/2017/conference/paper219/AnonReviewer1"], "reply": {"forum": "BysZhEqee", "replyto": "BysZhEqee", "writers": {"values-regex": "ICLR.cc/2017/conference/paper219/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper219/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481184419300}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1478278147519, "tcdate": 1478278147513, "number": 219, "id": "BysZhEqee", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "BysZhEqee", "signatures": ["~Yuchen_Zheng1"], "readers": ["everyone"], "content": {"TL;DR": "", "title": "Marginal Deep Architectures: Deep learning for Small and Middle Scale Applications", "abstract": "In recent years, many deep architectures have been proposed in different fields. However, to obtain good results, most of the previous deep models need a large number of training data. In this paper, for small and middle scale applications, we\npropose a novel deep learning framework based on stacked feature learning models. Particularly, we stack marginal Fisher analysis (MFA) layer by layer for the initialization of the deep architecture and call it \u201cMarginal Deep Architectures\u201d (MDA). In the implementation of MDA, the weight matrices of MFA are first learned layer by layer, and then we exploit some deep learning techniques, such as back propagation, dropout and denoising to fine tune the network. To evaluate the effectiveness of MDA, we have compared it with some feature learning methods and deep learning models on 7 small and middle scale real-world applications, including handwritten digits recognition, speech recognition, historical document understanding, image classification, action recognition and so on. Extensive experiments demonstrate that MDA performs not only better than shallow feature learning models, but also state-of-the-art deep learning models in these applications.", "pdf": "/pdf/e505cfc12d175bb6f600b7348110b72431813073.pdf", "paperhash": "zheng|marginal_deep_architectures_deep_learning_for_small_and_middle_scale_applications", "conflicts": ["ouc.edu.cn"], "keywords": [], "authors": ["Yuchen Zheng", "Guoqiang Zhong", "Junyu Dong"], "authorids": ["ouczyc@outlook.com", "gqzhong@ouc.edu.cn", "dongjunyu@ouc.edu.cn"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 13, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}], "count": 14}