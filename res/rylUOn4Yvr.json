{"notes": [{"id": "sWoHRYXgzQ", "original": null, "number": 26, "cdate": 1577362910664, "ddate": null, "tcdate": 1577362910664, "tmdate": 1577362910664, "tddate": null, "forum": "rylUOn4Yvr", "replyto": "lqKzFYDjK", "invitation": "ICLR.cc/2020/Conference/Paper42/-/Official_Comment", "content": {"title": "Theoretical justification, Work of Lee et al. 2019, Code releasing", "comment": "\nOn theoretical justification, current theorems conditioned on symmetric loss values are not applicable and new theorems need to be built. Please see our viewpoints: https://openreview.net/forum?id=rylUOn4Yvr&noteId=BkfpxlP2or\n\nOn recent SOTA methods (e.g., Lee et al. 2019), there is a difficulty, please see our provided information: https://openreview.net/forum?id=rylUOn4Yvr&noteId=BJerdkPhjB\n\nMany thanks to your helpful reviews. We will improve our work based on them. For your better reference, our key code is released now: https://github.com/XinshaoAmosWang/Emphasis-Regularisation-by-Gradient-Rescaling#1-code-is-available-now"}, "signatures": ["ICLR.cc/2020/Conference/Paper42/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper42/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ROBUST DISCRIMINATIVE REPRESENTATION LEARNING VIA GRADIENT RESCALING: AN EMPHASIS REGULARISATION PERSPECTIVE", "authors": ["Xinshao Wang", "Yang Hua", "Elyor Kodirov", "Neil M. Robertson"], "authorids": ["xwang39@qub.ac.uk", "y.hua@qub.ac.uk", "elyor@anyvision.co", "n.robertson@qub.ac.uk"], "keywords": ["examples weighting", "emphasis regularisation", "gradient scaling", "abnormal training examples"], "TL;DR": "ROBUST DISCRIMINATIVE REPRESENTATION LEARNING VIA GRADIENT RESCALING: AN EMPHASIS REGULARISATION PERSPECTIVE", "abstract": "It is fundamental and challenging to train robust and accurate Deep Neural Networks (DNNs) when semantically abnormal examples exist. Although great progress has been made, there is still one crucial research question which is not thoroughly explored yet: What training examples should be focused and how much more should they be emphasised to achieve robust learning? In this work, we study this question and propose gradient rescaling (GR) to solve it. GR modifies the magnitude of logit vector\u2019s gradient to emphasise on relatively easier training data points when noise becomes more severe, which functions as explicit emphasis regularisation to improve the generalisation performance of DNNs. Apart from regularisation, we connect GR to examples weighting and designing robust loss functions. We empirically demonstrate that GR is highly anomaly-robust and outperforms the state-of-the-art by a large margin, e.g., increasing 7% on CIFAR100 with 40% noisy labels. It is also significantly superior to standard regularisers in both clean and abnormal settings. Furthermore, we present comprehensive ablation studies to explore the behaviours of GR under different cases, which is informative for applying GR in real-world scenarios.", "pdf": "/pdf/5fecb7e054b65fe4a80d5ed507144c216fbf56fb.pdf", "paperhash": "wang|robust_discriminative_representation_learning_via_gradient_rescaling_an_emphasis_regularisation_perspective", "original_pdf": "/attachment/2a507591594cf79fae3cd6005cbe964b4d863dd9.pdf", "_bibtex": "@misc{\nwang2020robust,\ntitle={{\\{}ROBUST{\\}} {\\{}DISCRIMINATIVE{\\}} {\\{}REPRESENTATION{\\}} {\\{}LEARNING{\\}} {\\{}VIA{\\}} {\\{}GRADIENT{\\}} {\\{}RESCALING{\\}}: {\\{}AN{\\}} {\\{}EMPHASIS{\\}} {\\{}REGULARISATION{\\}} {\\{}PERSPECTIVE{\\}}},\nauthor={Xinshao Wang and Yang Hua and Elyor Kodirov and Neil M. Robertson},\nyear={2020},\nurl={https://openreview.net/forum?id=rylUOn4Yvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rylUOn4Yvr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper42/Authors", "ICLR.cc/2020/Conference/Paper42/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper42/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper42/Reviewers", "ICLR.cc/2020/Conference/Paper42/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper42/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper42/Authors|ICLR.cc/2020/Conference/Paper42/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504177233, "tmdate": 1576860541108, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper42/Authors", "ICLR.cc/2020/Conference/Paper42/Reviewers", "ICLR.cc/2020/Conference/Paper42/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper42/-/Official_Comment"}}}, {"id": "rylUOn4Yvr", "original": "rJl1mTMr8H", "number": 42, "cdate": 1569438829965, "ddate": null, "tcdate": 1569438829965, "tmdate": 1577168265011, "tddate": null, "forum": "rylUOn4Yvr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "ROBUST DISCRIMINATIVE REPRESENTATION LEARNING VIA GRADIENT RESCALING: AN EMPHASIS REGULARISATION PERSPECTIVE", "authors": ["Xinshao Wang", "Yang Hua", "Elyor Kodirov", "Neil M. Robertson"], "authorids": ["xwang39@qub.ac.uk", "y.hua@qub.ac.uk", "elyor@anyvision.co", "n.robertson@qub.ac.uk"], "keywords": ["examples weighting", "emphasis regularisation", "gradient scaling", "abnormal training examples"], "TL;DR": "ROBUST DISCRIMINATIVE REPRESENTATION LEARNING VIA GRADIENT RESCALING: AN EMPHASIS REGULARISATION PERSPECTIVE", "abstract": "It is fundamental and challenging to train robust and accurate Deep Neural Networks (DNNs) when semantically abnormal examples exist. Although great progress has been made, there is still one crucial research question which is not thoroughly explored yet: What training examples should be focused and how much more should they be emphasised to achieve robust learning? In this work, we study this question and propose gradient rescaling (GR) to solve it. GR modifies the magnitude of logit vector\u2019s gradient to emphasise on relatively easier training data points when noise becomes more severe, which functions as explicit emphasis regularisation to improve the generalisation performance of DNNs. Apart from regularisation, we connect GR to examples weighting and designing robust loss functions. We empirically demonstrate that GR is highly anomaly-robust and outperforms the state-of-the-art by a large margin, e.g., increasing 7% on CIFAR100 with 40% noisy labels. It is also significantly superior to standard regularisers in both clean and abnormal settings. Furthermore, we present comprehensive ablation studies to explore the behaviours of GR under different cases, which is informative for applying GR in real-world scenarios.", "pdf": "/pdf/5fecb7e054b65fe4a80d5ed507144c216fbf56fb.pdf", "paperhash": "wang|robust_discriminative_representation_learning_via_gradient_rescaling_an_emphasis_regularisation_perspective", "original_pdf": "/attachment/2a507591594cf79fae3cd6005cbe964b4d863dd9.pdf", "_bibtex": "@misc{\nwang2020robust,\ntitle={{\\{}ROBUST{\\}} {\\{}DISCRIMINATIVE{\\}} {\\{}REPRESENTATION{\\}} {\\{}LEARNING{\\}} {\\{}VIA{\\}} {\\{}GRADIENT{\\}} {\\{}RESCALING{\\}}: {\\{}AN{\\}} {\\{}EMPHASIS{\\}} {\\{}REGULARISATION{\\}} {\\{}PERSPECTIVE{\\}}},\nauthor={Xinshao Wang and Yang Hua and Elyor Kodirov and Neil M. Robertson},\nyear={2020},\nurl={https://openreview.net/forum?id=rylUOn4Yvr}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 12, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "lqKzFYDjK", "original": null, "number": 1, "cdate": 1576798685735, "ddate": null, "tcdate": 1576798685735, "tmdate": 1576800949209, "tddate": null, "forum": "rylUOn4Yvr", "replyto": "rylUOn4Yvr", "invitation": "ICLR.cc/2020/Conference/Paper42/-/Decision", "content": {"decision": "Reject", "comment": "The paper proposes a gradient rescaling method to make deep neural network training more robust to label noise. The intuition of focusing more on easier examples is not particularly new, but empirical results are promising. On the weak side, no theoretical justification is provided, and the method introduces extra hyperparameters that need to be tuned. Finally, more discussions on recent SOTA methods (e.g., Lee et al. 2019) as well as further comprehensive evaluations on various cases, such as asymmetric label noise, semantic label noise, and open-set label noise, would be needed to justify and demonstrate the effectiveness of the proposed method. ", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ROBUST DISCRIMINATIVE REPRESENTATION LEARNING VIA GRADIENT RESCALING: AN EMPHASIS REGULARISATION PERSPECTIVE", "authors": ["Xinshao Wang", "Yang Hua", "Elyor Kodirov", "Neil M. Robertson"], "authorids": ["xwang39@qub.ac.uk", "y.hua@qub.ac.uk", "elyor@anyvision.co", "n.robertson@qub.ac.uk"], "keywords": ["examples weighting", "emphasis regularisation", "gradient scaling", "abnormal training examples"], "TL;DR": "ROBUST DISCRIMINATIVE REPRESENTATION LEARNING VIA GRADIENT RESCALING: AN EMPHASIS REGULARISATION PERSPECTIVE", "abstract": "It is fundamental and challenging to train robust and accurate Deep Neural Networks (DNNs) when semantically abnormal examples exist. Although great progress has been made, there is still one crucial research question which is not thoroughly explored yet: What training examples should be focused and how much more should they be emphasised to achieve robust learning? In this work, we study this question and propose gradient rescaling (GR) to solve it. GR modifies the magnitude of logit vector\u2019s gradient to emphasise on relatively easier training data points when noise becomes more severe, which functions as explicit emphasis regularisation to improve the generalisation performance of DNNs. Apart from regularisation, we connect GR to examples weighting and designing robust loss functions. We empirically demonstrate that GR is highly anomaly-robust and outperforms the state-of-the-art by a large margin, e.g., increasing 7% on CIFAR100 with 40% noisy labels. It is also significantly superior to standard regularisers in both clean and abnormal settings. Furthermore, we present comprehensive ablation studies to explore the behaviours of GR under different cases, which is informative for applying GR in real-world scenarios.", "pdf": "/pdf/5fecb7e054b65fe4a80d5ed507144c216fbf56fb.pdf", "paperhash": "wang|robust_discriminative_representation_learning_via_gradient_rescaling_an_emphasis_regularisation_perspective", "original_pdf": "/attachment/2a507591594cf79fae3cd6005cbe964b4d863dd9.pdf", "_bibtex": "@misc{\nwang2020robust,\ntitle={{\\{}ROBUST{\\}} {\\{}DISCRIMINATIVE{\\}} {\\{}REPRESENTATION{\\}} {\\{}LEARNING{\\}} {\\{}VIA{\\}} {\\{}GRADIENT{\\}} {\\{}RESCALING{\\}}: {\\{}AN{\\}} {\\{}EMPHASIS{\\}} {\\{}REGULARISATION{\\}} {\\{}PERSPECTIVE{\\}}},\nauthor={Xinshao Wang and Yang Hua and Elyor Kodirov and Neil M. Robertson},\nyear={2020},\nurl={https://openreview.net/forum?id=rylUOn4Yvr}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "rylUOn4Yvr", "replyto": "rylUOn4Yvr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795705989, "tmdate": 1576800253904, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper42/-/Decision"}}}, {"id": "Skee-hIhiH", "original": null, "number": 18, "cdate": 1573837815991, "ddate": null, "tcdate": 1573837815991, "tmdate": 1573843977221, "tddate": null, "forum": "rylUOn4Yvr", "replyto": "H1xJEl_Z9r", "invitation": "ICLR.cc/2020/Conference/Paper42/-/Official_Comment", "content": {"title": "Questions", "comment": "\nThank you for your questions.\n\n1&2: Is it always the case that \u201cdifficult\u201d samples exhibit small logit values, and \u201ceasy\u201d samples high logit values? If not,.....\nGenerally, the answer is yes. As training goes, the premise that semantic anomalies have small classification confidences while normal examples tend to have large classification confidences is indeed our reasonable assumption. \nActually, a lot of prior work has demonstrated the reasonability of this premise. In addition, many algorithms have been proposed based on similar premises. Our empirical analysis also supports this premise well. More details are as follows:\n\n(1) Technically, we have also remarked that \u201cwe do not design the weighting scheme heuristically from scratch. Instead, it is naturally motivated by the gradient analysis of several loss functions\u201d, which makes GR principally and technically sound. More discussion is provided in Section 3.3. \n\n(2) In terms of prior work, self-paced learning, e.g., Self-paced (Kumar et al., 2010), and curriculum learning, e.g., MentorNet (Jiang et al., 2018) are practical algorithms based on this premise. In addition, as demonstrated in (Krueger et al., 2017; Arpit et al., 2017), when severe noise exists, DNNs learn simple meaningful patterns first before memorising abnormal examples. \n\n(3) Finally, in the \u201cemphasis focus\u201d paragraph of Section 1, we discussed that \u201cIt is a common practice to focus on harder instances when training DNNs (Shrivastava et al., 2016; Lin et al., 2017).\u201d \n\u201cWhen a dataset is clean, it achieves faster convergence and better performance to emphasise on harder examples because they own larger gradient magnitude, which means more information and a larger update step for model\u2019s parameters.\u201d\n\u201cHowever, when severe noise exists, as demonstrated in (Krueger et al., 2017; Arpit et al., 2017), DNNs learn simple meaningful patterns first before memorising abnormal ones.\u201d\n\u201cIn other words, anomalies are harder to fit and own larger gradient magnitude in the later stage. Consequently, if we use the default sample weighting in categorical cross entropy (CCE) where harder samples obtain higher weights, anomalies tend to be fitted well especially when a network has large enough capacity. That is why we need to move the emphasis focus towards relatively easier ones, which serves as emphasis regularisation.\u201d\n\n3. Can GR be used simultaneously with other noise-robust learning methods to further boost the performance?\nWe have tried combining GR with other standard regularisers in Section 4.5, please see Table 8. For example, $\\mathit{Dropout \\ is \\ demonstrated \\ to \\ be \\ a \\ great \\ regulariser \\ against \\ label \\ noise \\ in \\ Arpit \\ et \\ al., \\ 2017}$, \u201cA closer look at memorization in deep networks\u201d. This is also demonstrated in our Table 8, e.g., Dropout > Baseline, and Dropout+L2 > L2. \n\nCase 1: GR can help other regularisation techniques. \nGR consistently improves the generalisation performance after it is added:\nGR > Baseline; \nGR+L2 > L2;\nGR+Dropout > Dropout; \nGR+L2+Dropout > Dropout+L2.   \nCase 2: Other regularisation techniques may not help GR.\nWhen GR is already applied, adding another regulariser may not lead to better regularisation effect and generalisation performance. For example:   \nAdding Dropout hurts the performance: \nGR+Dropout < GR; \nGR+L2+Dropout < GR+L2.\nAdding L2 decay improves the performance: \nGR+L2 > GR;\nGR+L2+Dropout > GR+Dropout. \n\nTherefore, we are sorry that there is no deterministic answer for your question. The search and studying space is large when considering the diverse combination options of different regularisers.  \nHowever, it is worth noting that the interaction over multiple regularisers may not improve the generalisation performance in practice.\n\n5. Does GR still work well on small datasets(#points < 5000)?\nTo address your concern on this question, we add a Section  $C$ in our supplementary material in the new revised version. Please have a check. Thanks. \n(1). The problem of label noise we study on CIFAR-10 and CIFAR-100 in Section 4.2 is of similar scale. \n(2). We compare GR with other standard regularisers on a small-scale fine-grained visual categorisation problem in Table 9. The number of training data points is 5,000 in total.\nThis new experiment demonstrates that GR works on small datasets as well.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper42/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper42/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ROBUST DISCRIMINATIVE REPRESENTATION LEARNING VIA GRADIENT RESCALING: AN EMPHASIS REGULARISATION PERSPECTIVE", "authors": ["Xinshao Wang", "Yang Hua", "Elyor Kodirov", "Neil M. Robertson"], "authorids": ["xwang39@qub.ac.uk", "y.hua@qub.ac.uk", "elyor@anyvision.co", "n.robertson@qub.ac.uk"], "keywords": ["examples weighting", "emphasis regularisation", "gradient scaling", "abnormal training examples"], "TL;DR": "ROBUST DISCRIMINATIVE REPRESENTATION LEARNING VIA GRADIENT RESCALING: AN EMPHASIS REGULARISATION PERSPECTIVE", "abstract": "It is fundamental and challenging to train robust and accurate Deep Neural Networks (DNNs) when semantically abnormal examples exist. Although great progress has been made, there is still one crucial research question which is not thoroughly explored yet: What training examples should be focused and how much more should they be emphasised to achieve robust learning? In this work, we study this question and propose gradient rescaling (GR) to solve it. GR modifies the magnitude of logit vector\u2019s gradient to emphasise on relatively easier training data points when noise becomes more severe, which functions as explicit emphasis regularisation to improve the generalisation performance of DNNs. Apart from regularisation, we connect GR to examples weighting and designing robust loss functions. We empirically demonstrate that GR is highly anomaly-robust and outperforms the state-of-the-art by a large margin, e.g., increasing 7% on CIFAR100 with 40% noisy labels. It is also significantly superior to standard regularisers in both clean and abnormal settings. Furthermore, we present comprehensive ablation studies to explore the behaviours of GR under different cases, which is informative for applying GR in real-world scenarios.", "pdf": "/pdf/5fecb7e054b65fe4a80d5ed507144c216fbf56fb.pdf", "paperhash": "wang|robust_discriminative_representation_learning_via_gradient_rescaling_an_emphasis_regularisation_perspective", "original_pdf": "/attachment/2a507591594cf79fae3cd6005cbe964b4d863dd9.pdf", "_bibtex": "@misc{\nwang2020robust,\ntitle={{\\{}ROBUST{\\}} {\\{}DISCRIMINATIVE{\\}} {\\{}REPRESENTATION{\\}} {\\{}LEARNING{\\}} {\\{}VIA{\\}} {\\{}GRADIENT{\\}} {\\{}RESCALING{\\}}: {\\{}AN{\\}} {\\{}EMPHASIS{\\}} {\\{}REGULARISATION{\\}} {\\{}PERSPECTIVE{\\}}},\nauthor={Xinshao Wang and Yang Hua and Elyor Kodirov and Neil M. Robertson},\nyear={2020},\nurl={https://openreview.net/forum?id=rylUOn4Yvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rylUOn4Yvr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper42/Authors", "ICLR.cc/2020/Conference/Paper42/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper42/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper42/Reviewers", "ICLR.cc/2020/Conference/Paper42/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper42/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper42/Authors|ICLR.cc/2020/Conference/Paper42/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504177233, "tmdate": 1576860541108, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper42/Authors", "ICLR.cc/2020/Conference/Paper42/Reviewers", "ICLR.cc/2020/Conference/Paper42/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper42/-/Official_Comment"}}}, {"id": "H1eW7jugsr", "original": null, "number": 2, "cdate": 1573059353241, "ddate": null, "tcdate": 1573059353241, "tmdate": 1573843291597, "tddate": null, "forum": "rylUOn4Yvr", "replyto": "B1xnoAwm9B", "invitation": "ICLR.cc/2020/Conference/Paper42/-/Official_Comment", "content": {"title": "Comparing with ICCV19 baseline-SL and Why not evaluate on asymmetric noise?", "comment": "\nThanks so much for your helpful review. We are glad that you like our simple, effective and principled method.   \nRegarding the weakness points you mentioned, we clarify them as follows. We look forward to further discussion with you. \n\n1. Regarding the recent baseline \u201csymmetric cross entropy for robust learning with noisy labels, ICCV2019\u201d, we read it before submission but did not compare with it because it was not officially published at that time. Now, we add their results in our revised version. Please check our Tables 5 and 6, and you will find our method outperforms this recent baseline.  \n\nBeyond, please check our Section 2.1, where we discuss and present some remarks on robustness theorems conditioned on symmetric losses and label noise. Our work challenges those robustness theorems, which can promote new thinking. \n\n\n2. We explain our two reasons for without testing on asymmetric label noise: \n1). As we mentioned in Section 4.2, we follow the prior work \u201cMa et al. Dimensionality-Driven Learning with Noisy Labels, ICML 2019\u201d to test only on symmetric label noise as it has been demonstrated in Vahdat (2017) that it is more challenging than asymmetric noisy labels. Please check \u201cArash Vahdat. Toward robustness against label noise in training deep discriminative neural networks. NeurIPS, 2017.\u201d \n2). We spend more effort and space on experimental analysis and more complex and valuable real-world applications, e.g., image classification on Clothing 1M and video retrieval on MARS. $\\mathit{Those \\ problems \\ are \\ challenging \\ and \\ contain \\ diverse \\ semantic \\ anomalies \\ instead \\ of \\ only \\ \\ noisy \\ labels.}$ For example, in Figure 3 in the supplementary material: $\\mathit{1) \\ Out-of-distribution \\ anomalies}$: An image may contain only background or an object which does not belong to any training class; $\\mathit{2) \\ In-distribution \\ anomalies}$: An image of class $a$ may be annotated to class $b$ or an image may contain more than one semantic object. \n\n$ \\ \\ $ Those experimental results prove that GR can achieve state-of-the-art performance on different domain tasks.\n\nFinally, we add the results of asymmetric label noise in Section $D$ of the supplementary material in our new revised version. The results are displayed in Table 10. When GR is used, the performance is better than its counterpart without GR.\n\n3. The reference citation causes read difficulty. \nThanks so much for pointing it out. We have put the citations into parentheses in the revised version. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper42/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper42/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ROBUST DISCRIMINATIVE REPRESENTATION LEARNING VIA GRADIENT RESCALING: AN EMPHASIS REGULARISATION PERSPECTIVE", "authors": ["Xinshao Wang", "Yang Hua", "Elyor Kodirov", "Neil M. Robertson"], "authorids": ["xwang39@qub.ac.uk", "y.hua@qub.ac.uk", "elyor@anyvision.co", "n.robertson@qub.ac.uk"], "keywords": ["examples weighting", "emphasis regularisation", "gradient scaling", "abnormal training examples"], "TL;DR": "ROBUST DISCRIMINATIVE REPRESENTATION LEARNING VIA GRADIENT RESCALING: AN EMPHASIS REGULARISATION PERSPECTIVE", "abstract": "It is fundamental and challenging to train robust and accurate Deep Neural Networks (DNNs) when semantically abnormal examples exist. Although great progress has been made, there is still one crucial research question which is not thoroughly explored yet: What training examples should be focused and how much more should they be emphasised to achieve robust learning? In this work, we study this question and propose gradient rescaling (GR) to solve it. GR modifies the magnitude of logit vector\u2019s gradient to emphasise on relatively easier training data points when noise becomes more severe, which functions as explicit emphasis regularisation to improve the generalisation performance of DNNs. Apart from regularisation, we connect GR to examples weighting and designing robust loss functions. We empirically demonstrate that GR is highly anomaly-robust and outperforms the state-of-the-art by a large margin, e.g., increasing 7% on CIFAR100 with 40% noisy labels. It is also significantly superior to standard regularisers in both clean and abnormal settings. Furthermore, we present comprehensive ablation studies to explore the behaviours of GR under different cases, which is informative for applying GR in real-world scenarios.", "pdf": "/pdf/5fecb7e054b65fe4a80d5ed507144c216fbf56fb.pdf", "paperhash": "wang|robust_discriminative_representation_learning_via_gradient_rescaling_an_emphasis_regularisation_perspective", "original_pdf": "/attachment/2a507591594cf79fae3cd6005cbe964b4d863dd9.pdf", "_bibtex": "@misc{\nwang2020robust,\ntitle={{\\{}ROBUST{\\}} {\\{}DISCRIMINATIVE{\\}} {\\{}REPRESENTATION{\\}} {\\{}LEARNING{\\}} {\\{}VIA{\\}} {\\{}GRADIENT{\\}} {\\{}RESCALING{\\}}: {\\{}AN{\\}} {\\{}EMPHASIS{\\}} {\\{}REGULARISATION{\\}} {\\{}PERSPECTIVE{\\}}},\nauthor={Xinshao Wang and Yang Hua and Elyor Kodirov and Neil M. Robertson},\nyear={2020},\nurl={https://openreview.net/forum?id=rylUOn4Yvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rylUOn4Yvr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper42/Authors", "ICLR.cc/2020/Conference/Paper42/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper42/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper42/Reviewers", "ICLR.cc/2020/Conference/Paper42/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper42/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper42/Authors|ICLR.cc/2020/Conference/Paper42/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504177233, "tmdate": 1576860541108, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper42/Authors", "ICLR.cc/2020/Conference/Paper42/Reviewers", "ICLR.cc/2020/Conference/Paper42/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper42/-/Official_Comment"}}}, {"id": "B1gcrlv3oB", "original": null, "number": 21, "cdate": 1573838914333, "ddate": null, "tcdate": 1573838914333, "tmdate": 1573839597663, "tddate": null, "forum": "rylUOn4Yvr", "replyto": "H1xJEl_Z9r", "invitation": "ICLR.cc/2020/Conference/Paper42/-/Official_Comment", "content": {"title": "The Major Concern and Misc. Comments", "comment": "\n1. The major concern: How can we guarantee to have intact validation set? Can we get any improvement via GR even with corrupted validation set for tuning hyperparameters?\nFor those questions, we would like to clarify two core concepts: $Robust \\ Learning \\ when \\ abnormal \\ Training \\ examples \\ exist \\ and \\ Model \\ Selection \\ according \\ to \\ a  \\ Clean \\ Validation  \\ set$. \n\n$Robust \\ Learning$: In our context, we mean the robustness against diverse semantic anomalies in the training set. In other words, when there are anomalies in the training set, we aim to learn meaningful patterns without fitting those semantic anomalies. Fitting anomalies well will definitely hurt the robustness and generalisation performance because a model learns errors and non-meaningful patterns by fitting anomalies.   \n\n$Model \\ Selection$: Using a validation set to evaluate and decide which model is better is a common practice in machine learning. This is because we are only able to see the test data after deployment.  \nWhen we compare different models, a constructed validation set has to be clean so that it can serve as an oracle. We cannot evaluate/decide a model\u2019s performance according to a noisy validation set: \n(1) If an example has an incorrect label and its true label is unknown, it would be an error to evaluate whether its predicted label is the same as the incorrect label.  \n(2) Another intuitive and straightforward example: Given a validation dataset with an unknown noise rate, $A$ model with 80% accuracy may be worse than $B$ model with 70% accuracy because $A$ model may predict those anomalies better, which means $A$ model makes more wrong decisions. \n(3) If an out-of-distribution example exists, since it does not belong to any training class in fact, what we should do is to conduct out-of-distribution example detection and reject predicting it.\n\n\n2. Misc. Comments\nPage 3-> inside L1 norm, no differentiation sign in the denominator.\nAround eq 2 and 4: missing derivative symbol w.r.t. z\n\n\nThank you so much for your careful and helpful check. We have revised them in our revised version.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper42/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper42/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ROBUST DISCRIMINATIVE REPRESENTATION LEARNING VIA GRADIENT RESCALING: AN EMPHASIS REGULARISATION PERSPECTIVE", "authors": ["Xinshao Wang", "Yang Hua", "Elyor Kodirov", "Neil M. Robertson"], "authorids": ["xwang39@qub.ac.uk", "y.hua@qub.ac.uk", "elyor@anyvision.co", "n.robertson@qub.ac.uk"], "keywords": ["examples weighting", "emphasis regularisation", "gradient scaling", "abnormal training examples"], "TL;DR": "ROBUST DISCRIMINATIVE REPRESENTATION LEARNING VIA GRADIENT RESCALING: AN EMPHASIS REGULARISATION PERSPECTIVE", "abstract": "It is fundamental and challenging to train robust and accurate Deep Neural Networks (DNNs) when semantically abnormal examples exist. Although great progress has been made, there is still one crucial research question which is not thoroughly explored yet: What training examples should be focused and how much more should they be emphasised to achieve robust learning? In this work, we study this question and propose gradient rescaling (GR) to solve it. GR modifies the magnitude of logit vector\u2019s gradient to emphasise on relatively easier training data points when noise becomes more severe, which functions as explicit emphasis regularisation to improve the generalisation performance of DNNs. Apart from regularisation, we connect GR to examples weighting and designing robust loss functions. We empirically demonstrate that GR is highly anomaly-robust and outperforms the state-of-the-art by a large margin, e.g., increasing 7% on CIFAR100 with 40% noisy labels. It is also significantly superior to standard regularisers in both clean and abnormal settings. Furthermore, we present comprehensive ablation studies to explore the behaviours of GR under different cases, which is informative for applying GR in real-world scenarios.", "pdf": "/pdf/5fecb7e054b65fe4a80d5ed507144c216fbf56fb.pdf", "paperhash": "wang|robust_discriminative_representation_learning_via_gradient_rescaling_an_emphasis_regularisation_perspective", "original_pdf": "/attachment/2a507591594cf79fae3cd6005cbe964b4d863dd9.pdf", "_bibtex": "@misc{\nwang2020robust,\ntitle={{\\{}ROBUST{\\}} {\\{}DISCRIMINATIVE{\\}} {\\{}REPRESENTATION{\\}} {\\{}LEARNING{\\}} {\\{}VIA{\\}} {\\{}GRADIENT{\\}} {\\{}RESCALING{\\}}: {\\{}AN{\\}} {\\{}EMPHASIS{\\}} {\\{}REGULARISATION{\\}} {\\{}PERSPECTIVE{\\}}},\nauthor={Xinshao Wang and Yang Hua and Elyor Kodirov and Neil M. Robertson},\nyear={2020},\nurl={https://openreview.net/forum?id=rylUOn4Yvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rylUOn4Yvr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper42/Authors", "ICLR.cc/2020/Conference/Paper42/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper42/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper42/Reviewers", "ICLR.cc/2020/Conference/Paper42/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper42/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper42/Authors|ICLR.cc/2020/Conference/Paper42/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504177233, "tmdate": 1576860541108, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper42/Authors", "ICLR.cc/2020/Conference/Paper42/Reviewers", "ICLR.cc/2020/Conference/Paper42/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper42/-/Official_Comment"}}}, {"id": "BkfpxlP2or", "original": null, "number": 20, "cdate": 1573838837140, "ddate": null, "tcdate": 1573838837140, "tmdate": 1573838837140, "tddate": null, "forum": "rylUOn4Yvr", "replyto": "H1xJEl_Z9r", "invitation": "ICLR.cc/2020/Conference/Paper42/-/Official_Comment", "content": {"title": "Cons 2-4", "comment": "\n2. No justifications (both theoretical and experimental) are provided on the claim that controlling emphasis focus/spread will result in more robust learning.\nEmpirically, we have presented extensive experimental analysis in Section 4.2.1 and supplementary material. \n\nTechnically, we discussed some justifications in Section 2.1 and Section 3.3. We emphasize that: \n1). In this work, inspired by the analysis of CCE, MAE and GCE, which only differ in the gradient magnitude while perform quite differently, leading to a natural interpretation that gradient magnitude matters. That is why we explore rescaling the gradient magnitude as illustrated in Figure 1 and Table 1.\n2). GR is independent of empirical loss expressions as presented in Table 1. Therefore, one specific loss is merely an indicator of how far we are away from a specific minimisation objective. It has no direct impact on the robustness of the learning process since it has no direct influence on the gradient back-propagation. \nSimilar to the prior work of rethinking generalisation (Zhang et al., 2017), we need to rethink robust training under diverse anomalies, where the robustness theorems conditioned on symmetric losses and label noise are not directly applicable.\n\nIn summary, our work is more about new findings and analysis on robust learning against diverse semantic anomalies. Although it does not build new theorems, GR challenges existing robustness theorems conditioned on symmetric losses and label noise. New theorems need to be built in the future when it comes to diverse semantic anomalies.\n\n\n3. This algorithm introduces 2 additional hyperparameters that are correlated with each other. This introduces additional labor.\nFrankly, this is a difficult issue to address as hyperparameters generally exist when a new regulariser/method is proposed. We provide our thinking as follows:   \n(1) It is natural that emphasis focus and spread are correlated since the classification confidence is distributed in a bounded range [0, 1]. \n\n(2) Regarding the additional labour, we introduce our understanding: In our framework, there are two regularisation concepts, emphasis focus and spread. In deep learning, whenever a new regulariser is proposed, we have to consider its regularisation weight in practice. And when two or more than two regularisers are combined together, they are correlated naturally. \n\n(3) We are also thinking on how to reduce the labour of optimising a model\u2019s hyperparameters when training a deep model. One possible solution may be using AutoML techniques to optimize a model\u2019s hyperparameters automatically.\n\n\n4. By changing the loss function, the outputs of the network might lose its interpretation as a probability distribution.\nSorry, we find it is a factual misunderstanding. In the loss layer, we do not apply automatic gradient computation and back-propagation because GR makes the forward loss computation and backward gradient computation independent. \nWe do not change the loss computation as we merely regard it as an indicator of how far we are away from a specific minimisation objective. It has no direct impact on the robustness of the learning process since it has no direct influence on the gradient back-propagation. Please see Section 2.1 for more details.\n\nTherefore, you can choose any loss computation as you need. You can also compute and output multiple losses. All loss indicators displayed in Table 1, i.e., CCE, MAE and GR, are computed by taking a predicted probability distribution as input. The predicted probability distribution can be computed as normal even if the loss computation is changed.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper42/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper42/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ROBUST DISCRIMINATIVE REPRESENTATION LEARNING VIA GRADIENT RESCALING: AN EMPHASIS REGULARISATION PERSPECTIVE", "authors": ["Xinshao Wang", "Yang Hua", "Elyor Kodirov", "Neil M. Robertson"], "authorids": ["xwang39@qub.ac.uk", "y.hua@qub.ac.uk", "elyor@anyvision.co", "n.robertson@qub.ac.uk"], "keywords": ["examples weighting", "emphasis regularisation", "gradient scaling", "abnormal training examples"], "TL;DR": "ROBUST DISCRIMINATIVE REPRESENTATION LEARNING VIA GRADIENT RESCALING: AN EMPHASIS REGULARISATION PERSPECTIVE", "abstract": "It is fundamental and challenging to train robust and accurate Deep Neural Networks (DNNs) when semantically abnormal examples exist. Although great progress has been made, there is still one crucial research question which is not thoroughly explored yet: What training examples should be focused and how much more should they be emphasised to achieve robust learning? In this work, we study this question and propose gradient rescaling (GR) to solve it. GR modifies the magnitude of logit vector\u2019s gradient to emphasise on relatively easier training data points when noise becomes more severe, which functions as explicit emphasis regularisation to improve the generalisation performance of DNNs. Apart from regularisation, we connect GR to examples weighting and designing robust loss functions. We empirically demonstrate that GR is highly anomaly-robust and outperforms the state-of-the-art by a large margin, e.g., increasing 7% on CIFAR100 with 40% noisy labels. It is also significantly superior to standard regularisers in both clean and abnormal settings. Furthermore, we present comprehensive ablation studies to explore the behaviours of GR under different cases, which is informative for applying GR in real-world scenarios.", "pdf": "/pdf/5fecb7e054b65fe4a80d5ed507144c216fbf56fb.pdf", "paperhash": "wang|robust_discriminative_representation_learning_via_gradient_rescaling_an_emphasis_regularisation_perspective", "original_pdf": "/attachment/2a507591594cf79fae3cd6005cbe964b4d863dd9.pdf", "_bibtex": "@misc{\nwang2020robust,\ntitle={{\\{}ROBUST{\\}} {\\{}DISCRIMINATIVE{\\}} {\\{}REPRESENTATION{\\}} {\\{}LEARNING{\\}} {\\{}VIA{\\}} {\\{}GRADIENT{\\}} {\\{}RESCALING{\\}}: {\\{}AN{\\}} {\\{}EMPHASIS{\\}} {\\{}REGULARISATION{\\}} {\\{}PERSPECTIVE{\\}}},\nauthor={Xinshao Wang and Yang Hua and Elyor Kodirov and Neil M. Robertson},\nyear={2020},\nurl={https://openreview.net/forum?id=rylUOn4Yvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rylUOn4Yvr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper42/Authors", "ICLR.cc/2020/Conference/Paper42/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper42/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper42/Reviewers", "ICLR.cc/2020/Conference/Paper42/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper42/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper42/Authors|ICLR.cc/2020/Conference/Paper42/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504177233, "tmdate": 1576860541108, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper42/Authors", "ICLR.cc/2020/Conference/Paper42/Reviewers", "ICLR.cc/2020/Conference/Paper42/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper42/-/Official_Comment"}}}, {"id": "BJerdkPhjB", "original": null, "number": 19, "cdate": 1573838700559, "ddate": null, "tcdate": 1573838700559, "tmdate": 1573838700559, "tddate": null, "forum": "rylUOn4Yvr", "replyto": "H1xJEl_Z9r", "invitation": "ICLR.cc/2020/Conference/Paper42/-/Official_Comment", "content": {"title": "Cons 5-8", "comment": "\n5. No confidence intervals are shown except for the CIFAR-100 experiment.\nSorry, we do not understand this question, could you please explain more in detail?\n\n\n6. Experiments are only conducted on vision tasks.\nWe analyse and demonstrate GR\u2019s effectiveness on diverse computer vision tasks using different net architectures: \n1) Image classification with clean training data, e.g., CIFAR-10 and CIFAR-100 datasets; \n2) Image classification with synthetic symmetric label noise, which is more challenging than asymmetric noise evaluated by (Vahdat, 2017; Ma et al., 2018), e.g., CIFAR-10 and CIFAR-100 datasets; \n3) Image classification with real-world unknown anomalies, which may contain open-set noise (Wang et al., 2018), e.g., images with only background, or outliers, etc. We test on Clothing 1M dataset; \n4) Video person re-identification, a video retrieval task containing diverse anomalies, e.g., MARS dataset. \n5) We show that GR is notably better than other standard regularisers, e.g., L2 weight decay and dropout. Besides, to comprehensively understand GR\u2019s behaviours, we present extensive ablation studies.\n \nWe agree it is always more convincing to evaluate on more tasks. We are working on it.\n\n\n7. The baseline menagerie also changes when the authors change the target dataset.\n* Background on experimental settings: The research problem we work on is important and popular. Therefore, on different datasets, different baselines along with different net architectures have been evaluated in the literature. For example, some prior work reimplemented baselines with their custom-designed net architectures. The experimental settings, e.g., choice of net architectures, are not consistent and rigid in different prior work. On the one hand, it is nice since it provides diverse evaluation settings. On the other hand, it presents a challenge to keep the compared baselines consistent when you aim to compare fairly with different types of baselines on different datasets. Especially some prior work designed their own net architectures.    \n\n* In our experimental setup, we aim to test on different benchmarks using different publicly available net architectures for more comprehensive evaluation: \n1). We choose GoogLeNet V1 on CIFAR-10 following MentorNet (Jiang et al., 2018), where most baselines are example reweighting algorithms as shown in Table 4.  \n2). We use ResNet-44 on CIFAR-100 following D2L (Ma et al., 2018), where most competitors are specifically designed for addressing label noise as displayed in Table 5. \n3). The experiments on Clothing 1M using ResNet-50 are consistent in most prior work. We have displayed most related baselines in Table 6.  \n\n* We remark that on each dataset, we only report the results of those baselines using the same net architecture for a fair comparison.\n\n\n8. Additional benchmarks of most recent noise-robustness algos such as <Lee et al. 2019 ICML> are required.\nWe introduce the reasons for without benchmarking the Robust Inference method (Lee et al. 2019 ICML). They have a misstatement in their experimental section 4.3, making it hard to compare fairly. More details are as follows:\n \nIn the experimental section 4.3 of Lee et al., 2019, it is stated they followed the same experimental setup as D2L (Ma et al., 2018). However, we were surprised by its reported much better performance than the original results reported in D2L (Ma et al., 2018). \nTherefore, we emailed the authors of this paper and got the reply that they used different networks in their implementation. They did not provide us with what exact net architecture they used. \n\nFurthermore, Lee et al., 2019 is orthogonal to our work. We target at robust learning during training. On the contrary, Lee et al., 2019 proposed an inference method, Robust Generative classifier (RoG), which is applicable to a discriminative neural classifier pre-trained on noisy datasets (without retraining). \nTheir premise is that the softmax DNNs can learn meaningful feature patterns shared by multiple training examples even under datasets with noisy labels. However, this premise is not always true without proper regularisation as shown in our Table 3. A deep model trained by softmax and cross entropy is able to fit noisy datasets well and learn non-meaningful patterns. This is also demonstrated in prior work Rethinking generalisation (Zhang et al., 2017 ICLR).\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper42/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper42/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ROBUST DISCRIMINATIVE REPRESENTATION LEARNING VIA GRADIENT RESCALING: AN EMPHASIS REGULARISATION PERSPECTIVE", "authors": ["Xinshao Wang", "Yang Hua", "Elyor Kodirov", "Neil M. Robertson"], "authorids": ["xwang39@qub.ac.uk", "y.hua@qub.ac.uk", "elyor@anyvision.co", "n.robertson@qub.ac.uk"], "keywords": ["examples weighting", "emphasis regularisation", "gradient scaling", "abnormal training examples"], "TL;DR": "ROBUST DISCRIMINATIVE REPRESENTATION LEARNING VIA GRADIENT RESCALING: AN EMPHASIS REGULARISATION PERSPECTIVE", "abstract": "It is fundamental and challenging to train robust and accurate Deep Neural Networks (DNNs) when semantically abnormal examples exist. Although great progress has been made, there is still one crucial research question which is not thoroughly explored yet: What training examples should be focused and how much more should they be emphasised to achieve robust learning? In this work, we study this question and propose gradient rescaling (GR) to solve it. GR modifies the magnitude of logit vector\u2019s gradient to emphasise on relatively easier training data points when noise becomes more severe, which functions as explicit emphasis regularisation to improve the generalisation performance of DNNs. Apart from regularisation, we connect GR to examples weighting and designing robust loss functions. We empirically demonstrate that GR is highly anomaly-robust and outperforms the state-of-the-art by a large margin, e.g., increasing 7% on CIFAR100 with 40% noisy labels. It is also significantly superior to standard regularisers in both clean and abnormal settings. Furthermore, we present comprehensive ablation studies to explore the behaviours of GR under different cases, which is informative for applying GR in real-world scenarios.", "pdf": "/pdf/5fecb7e054b65fe4a80d5ed507144c216fbf56fb.pdf", "paperhash": "wang|robust_discriminative_representation_learning_via_gradient_rescaling_an_emphasis_regularisation_perspective", "original_pdf": "/attachment/2a507591594cf79fae3cd6005cbe964b4d863dd9.pdf", "_bibtex": "@misc{\nwang2020robust,\ntitle={{\\{}ROBUST{\\}} {\\{}DISCRIMINATIVE{\\}} {\\{}REPRESENTATION{\\}} {\\{}LEARNING{\\}} {\\{}VIA{\\}} {\\{}GRADIENT{\\}} {\\{}RESCALING{\\}}: {\\{}AN{\\}} {\\{}EMPHASIS{\\}} {\\{}REGULARISATION{\\}} {\\{}PERSPECTIVE{\\}}},\nauthor={Xinshao Wang and Yang Hua and Elyor Kodirov and Neil M. Robertson},\nyear={2020},\nurl={https://openreview.net/forum?id=rylUOn4Yvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rylUOn4Yvr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper42/Authors", "ICLR.cc/2020/Conference/Paper42/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper42/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper42/Reviewers", "ICLR.cc/2020/Conference/Paper42/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper42/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper42/Authors|ICLR.cc/2020/Conference/Paper42/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504177233, "tmdate": 1576860541108, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper42/Authors", "ICLR.cc/2020/Conference/Paper42/Reviewers", "ICLR.cc/2020/Conference/Paper42/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper42/-/Official_Comment"}}}, {"id": "BylMdxxsor", "original": null, "number": 16, "cdate": 1573744745738, "ddate": null, "tcdate": 1573744745738, "tmdate": 1573744745738, "tddate": null, "forum": "rylUOn4Yvr", "replyto": "SJgoxomGsr", "invitation": "ICLR.cc/2020/Conference/Paper42/-/Official_Comment", "content": {"title": "Thanks for your response", "comment": "Thanks for addressing my questions."}, "signatures": ["ICLR.cc/2020/Conference/Paper42/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper42/AnonReviewer1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ROBUST DISCRIMINATIVE REPRESENTATION LEARNING VIA GRADIENT RESCALING: AN EMPHASIS REGULARISATION PERSPECTIVE", "authors": ["Xinshao Wang", "Yang Hua", "Elyor Kodirov", "Neil M. Robertson"], "authorids": ["xwang39@qub.ac.uk", "y.hua@qub.ac.uk", "elyor@anyvision.co", "n.robertson@qub.ac.uk"], "keywords": ["examples weighting", "emphasis regularisation", "gradient scaling", "abnormal training examples"], "TL;DR": "ROBUST DISCRIMINATIVE REPRESENTATION LEARNING VIA GRADIENT RESCALING: AN EMPHASIS REGULARISATION PERSPECTIVE", "abstract": "It is fundamental and challenging to train robust and accurate Deep Neural Networks (DNNs) when semantically abnormal examples exist. Although great progress has been made, there is still one crucial research question which is not thoroughly explored yet: What training examples should be focused and how much more should they be emphasised to achieve robust learning? In this work, we study this question and propose gradient rescaling (GR) to solve it. GR modifies the magnitude of logit vector\u2019s gradient to emphasise on relatively easier training data points when noise becomes more severe, which functions as explicit emphasis regularisation to improve the generalisation performance of DNNs. Apart from regularisation, we connect GR to examples weighting and designing robust loss functions. We empirically demonstrate that GR is highly anomaly-robust and outperforms the state-of-the-art by a large margin, e.g., increasing 7% on CIFAR100 with 40% noisy labels. It is also significantly superior to standard regularisers in both clean and abnormal settings. Furthermore, we present comprehensive ablation studies to explore the behaviours of GR under different cases, which is informative for applying GR in real-world scenarios.", "pdf": "/pdf/5fecb7e054b65fe4a80d5ed507144c216fbf56fb.pdf", "paperhash": "wang|robust_discriminative_representation_learning_via_gradient_rescaling_an_emphasis_regularisation_perspective", "original_pdf": "/attachment/2a507591594cf79fae3cd6005cbe964b4d863dd9.pdf", "_bibtex": "@misc{\nwang2020robust,\ntitle={{\\{}ROBUST{\\}} {\\{}DISCRIMINATIVE{\\}} {\\{}REPRESENTATION{\\}} {\\{}LEARNING{\\}} {\\{}VIA{\\}} {\\{}GRADIENT{\\}} {\\{}RESCALING{\\}}: {\\{}AN{\\}} {\\{}EMPHASIS{\\}} {\\{}REGULARISATION{\\}} {\\{}PERSPECTIVE{\\}}},\nauthor={Xinshao Wang and Yang Hua and Elyor Kodirov and Neil M. Robertson},\nyear={2020},\nurl={https://openreview.net/forum?id=rylUOn4Yvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rylUOn4Yvr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper42/Authors", "ICLR.cc/2020/Conference/Paper42/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper42/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper42/Reviewers", "ICLR.cc/2020/Conference/Paper42/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper42/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper42/Authors|ICLR.cc/2020/Conference/Paper42/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504177233, "tmdate": 1576860541108, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper42/Authors", "ICLR.cc/2020/Conference/Paper42/Reviewers", "ICLR.cc/2020/Conference/Paper42/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper42/-/Official_Comment"}}}, {"id": "SJgoxomGsr", "original": null, "number": 14, "cdate": 1573169907281, "ddate": null, "tcdate": 1573169907281, "tmdate": 1573480275837, "tddate": null, "forum": "rylUOn4Yvr", "replyto": "rJgz97JRYB", "invitation": "ICLR.cc/2020/Conference/Paper42/-/Official_Comment", "content": {"title": "Procedure and Principle for choosing beta/lambda, Experimental setup and Others ", "comment": "\n1. The main drawback of the proposed approach is that there is no clear way of choosing alpha and beta, other than a hyper-parameter search, which is not very practical and can lead to overfitting the test set.\n2. The experiments are very thorough and the results are very good, but I have few clarifying questions:\n    2.1. The procedure for choosing beta/gamma is not clear, and I see that for every experiment those values change.\n    2.2. Can you be more specific what do you mean by \"with a little effort for optimizing beta and gamma\" in caption of Table 5?\n\nWe reply to questions 1 and 2 together as they are related. \n\n- Reply to 1 & 2.1 on \u201cthe procedure for choosing beta/lambda\u201d: \nWe have summarised the procedure and principle for choosing $\\lambda,\\beta$ in the empirical analysis part, i.e., Section 4.2.1. Details are as follows:  \n  (1)  Emphasis focus: When noise rate is higher, we can improve a model\u2019s robustness by moving emphasis focus towards relatively less difficult examples with a larger $\\lambda$, which is informative in practice.\n  (2)  When $\\lambda$ is larger, $\\beta$ should be larger as shown in Figure 1c. This is also demonstrated in Table 3, Table 5, and Table 10. \n\n- Reply to 2.1 on \u201cfor every experiment those values change\u201d: \nIn Table 4 and Table 5, we have displayed the results of GR with fixed parameters $(\\beta=8, \\lambda=0.5)$. Those results are better than the state-of-the-art.  \n\n- Reply to 2.2: \nThose empirical results above make it easy to optimise $\\lambda,\\beta$ in practice. Therefore, we wrote \u201cwith a little effort for optimizing $\\lambda,\\beta$\u201d.\n\n\n3. It would be nice if the CIFAR-10 Table 4 results are performed using the exact same setting to prior work to make sure the comparison is fair. For example, GCE results in (Zhang & Sabuncu 2018) are much that the reported ones. While it seems that you're using GoogLeNet V1 architecture similar to Jiang et al. 2018, it's not clear which experimental setting you are comparing against.\n\n- The research problem we work on is important and popular. Therefore, on different datasets, different baselines along with different net architectures have been evaluated in the literature. For example, some prior work reimplemented baselines with their custom-designed net architectures. The experimental settings, e.g., choice of net architectures, are not consistent and rigid in different prior work. On the one hand, it is nice since it provides diverse evaluation settings. On the other hand, it presents a challenge to keep the compared baselines consistent when you aim to compare fairly with different types of baselines on different datasets. Especially some prior work designed their own net architectures.    \n\n- In our experimental setup, we aim to test on different benchmarks using different publicly available net architectures for more comprehensive evaluation: \n  (1) We choose GoogLeNet V1 on CIFAR-10 following MentorNet (Jiang et al., 2018), where most baselines are example reweighting algorithms as shown in Table 4.  \n  (2) We use ResNet-44 on CIFAR-100 following D2L (Ma et al., 2018), where most competitors are specifically designed for addressing label noise as displayed in Table 5. \n  (3) The experiments on Clothing 1M using ResNet-50 are consistent in most prior work. We have displayed most related baselines in Table 6.  \n\n- We remark that on each dataset, we only report the results of those baselines using the same net architecture for a fair comparison.\n\n\n\n4. The paper in general is easy to follow, but the paper is not very rigorous or clear on some important concepts. For example: \n\n  4.1 It is not so clear what it means to \"babysit\" emphasis focus and spread.\n  It means that we need to choose emphasis focus and spread properly instead of using the built-in default settings in loss functions. \n\n  4.2 I don't understand what Eq. 6 is supposed to tell.\n  We use Eq. (6) to tell that GR can be independent of empirical loss formulations. Since gradient computation is independent of loss computation, different losses only indicate how far we are away from different minimisation objectives. The supervision information of GR can be controlled independently and straightforwardly. \n\n  4.3 & 4.4 No clear mathematical definition of emphasis focus and spread. What are the \\dots in equation? \n  We have provided mathematical definitions of emphasis focus and spread in our revised version. \n  We have removed those \\cdot in our revised version. \n\n\n5. Minor:\n     * Grammer mistake: \"what training examples...focused *on*...\"\n     * Citations should be done with parentheses\n\n  Thanks so much for your helpful and careful check. We have revised them in our revised version.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper42/Authors"], "readers": ["everyone", "ICLR.cc/2020/Conference/Paper42/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper42/Area_Chairs"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper42/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ROBUST DISCRIMINATIVE REPRESENTATION LEARNING VIA GRADIENT RESCALING: AN EMPHASIS REGULARISATION PERSPECTIVE", "authors": ["Xinshao Wang", "Yang Hua", "Elyor Kodirov", "Neil M. Robertson"], "authorids": ["xwang39@qub.ac.uk", "y.hua@qub.ac.uk", "elyor@anyvision.co", "n.robertson@qub.ac.uk"], "keywords": ["examples weighting", "emphasis regularisation", "gradient scaling", "abnormal training examples"], "TL;DR": "ROBUST DISCRIMINATIVE REPRESENTATION LEARNING VIA GRADIENT RESCALING: AN EMPHASIS REGULARISATION PERSPECTIVE", "abstract": "It is fundamental and challenging to train robust and accurate Deep Neural Networks (DNNs) when semantically abnormal examples exist. Although great progress has been made, there is still one crucial research question which is not thoroughly explored yet: What training examples should be focused and how much more should they be emphasised to achieve robust learning? In this work, we study this question and propose gradient rescaling (GR) to solve it. GR modifies the magnitude of logit vector\u2019s gradient to emphasise on relatively easier training data points when noise becomes more severe, which functions as explicit emphasis regularisation to improve the generalisation performance of DNNs. Apart from regularisation, we connect GR to examples weighting and designing robust loss functions. We empirically demonstrate that GR is highly anomaly-robust and outperforms the state-of-the-art by a large margin, e.g., increasing 7% on CIFAR100 with 40% noisy labels. It is also significantly superior to standard regularisers in both clean and abnormal settings. Furthermore, we present comprehensive ablation studies to explore the behaviours of GR under different cases, which is informative for applying GR in real-world scenarios.", "pdf": "/pdf/5fecb7e054b65fe4a80d5ed507144c216fbf56fb.pdf", "paperhash": "wang|robust_discriminative_representation_learning_via_gradient_rescaling_an_emphasis_regularisation_perspective", "original_pdf": "/attachment/2a507591594cf79fae3cd6005cbe964b4d863dd9.pdf", "_bibtex": "@misc{\nwang2020robust,\ntitle={{\\{}ROBUST{\\}} {\\{}DISCRIMINATIVE{\\}} {\\{}REPRESENTATION{\\}} {\\{}LEARNING{\\}} {\\{}VIA{\\}} {\\{}GRADIENT{\\}} {\\{}RESCALING{\\}}: {\\{}AN{\\}} {\\{}EMPHASIS{\\}} {\\{}REGULARISATION{\\}} {\\{}PERSPECTIVE{\\}}},\nauthor={Xinshao Wang and Yang Hua and Elyor Kodirov and Neil M. Robertson},\nyear={2020},\nurl={https://openreview.net/forum?id=rylUOn4Yvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rylUOn4Yvr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper42/Authors", "ICLR.cc/2020/Conference/Paper42/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper42/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper42/Reviewers", "ICLR.cc/2020/Conference/Paper42/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper42/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper42/Authors|ICLR.cc/2020/Conference/Paper42/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504177233, "tmdate": 1576860541108, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper42/Authors", "ICLR.cc/2020/Conference/Paper42/Reviewers", "ICLR.cc/2020/Conference/Paper42/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper42/-/Official_Comment"}}}, {"id": "rJgz97JRYB", "original": null, "number": 1, "cdate": 1571840906015, "ddate": null, "tcdate": 1571840906015, "tmdate": 1572972646183, "tddate": null, "forum": "rylUOn4Yvr", "replyto": "rylUOn4Yvr", "invitation": "ICLR.cc/2020/Conference/Paper42/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary:\nThe paper proposes a method for noise robustness based on scaling gradients of examples. By choosing the proper scaling parameters (alpha and beta), the method recovers standard losses such as CCE, MAE, and GCE, while also recovering other losses. The method is strongly related to reweighting training examples, where alpha and beta define the shape of this weighting as a function of the model's prediction (i.e., p_i). Experiments show that the proposed method achieves competitive results on several standard benchmarks for noisy-labelled data.\n\nComments:\n- The main drawback of the proposed approach is that there is no clear way of choosing alpha and beta, other than a hyper-parameter search, which is not very practical and can lead to overfitting the test set.\n- The paper in general is easy to follow, but the paper is not very rigorous or clear on some important concepts. For example: \n     * No clear mathematical definition of emphasis focus and spread\n     * The term \"semantically abnormal examples\" should be defined in the main text.\n     * It is not so clear what it means to \"babyset\" emphasis focus and spread.\n     * I don't understand what Eq. 6 is supposed to tell.\n     * What are the \\dots in equation \n- The experiments are very thorough and the results are very good, but I have few clarifying questions:\n     * The procedure for choosing beta/gamma is not clear, and I see that for every experiment those values change.\n     * It would be nice if the CIFAR-10 Table 4 results are performed using the exact same setting to prior work to make sure the comparison is fair. For example, GCE results in (Zhang & Sabuncu 2018) are much that the reported ones. While it seems that you're using GoogLeNet V1 architecture similar to Jiang et al. 2018, it's not clear which experimental setting you are comparing against.\n     * Can you be more specific what do you mean by \"with a little effort for optimizing beta and gamma\" in caption of Table 5?\n\nMinor:\n     * Grammer mistake: \"what training examples...focused *on*...\"\n     * Citations should be done with parentheses\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper42/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper42/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ROBUST DISCRIMINATIVE REPRESENTATION LEARNING VIA GRADIENT RESCALING: AN EMPHASIS REGULARISATION PERSPECTIVE", "authors": ["Xinshao Wang", "Yang Hua", "Elyor Kodirov", "Neil M. Robertson"], "authorids": ["xwang39@qub.ac.uk", "y.hua@qub.ac.uk", "elyor@anyvision.co", "n.robertson@qub.ac.uk"], "keywords": ["examples weighting", "emphasis regularisation", "gradient scaling", "abnormal training examples"], "TL;DR": "ROBUST DISCRIMINATIVE REPRESENTATION LEARNING VIA GRADIENT RESCALING: AN EMPHASIS REGULARISATION PERSPECTIVE", "abstract": "It is fundamental and challenging to train robust and accurate Deep Neural Networks (DNNs) when semantically abnormal examples exist. Although great progress has been made, there is still one crucial research question which is not thoroughly explored yet: What training examples should be focused and how much more should they be emphasised to achieve robust learning? In this work, we study this question and propose gradient rescaling (GR) to solve it. GR modifies the magnitude of logit vector\u2019s gradient to emphasise on relatively easier training data points when noise becomes more severe, which functions as explicit emphasis regularisation to improve the generalisation performance of DNNs. Apart from regularisation, we connect GR to examples weighting and designing robust loss functions. We empirically demonstrate that GR is highly anomaly-robust and outperforms the state-of-the-art by a large margin, e.g., increasing 7% on CIFAR100 with 40% noisy labels. It is also significantly superior to standard regularisers in both clean and abnormal settings. Furthermore, we present comprehensive ablation studies to explore the behaviours of GR under different cases, which is informative for applying GR in real-world scenarios.", "pdf": "/pdf/5fecb7e054b65fe4a80d5ed507144c216fbf56fb.pdf", "paperhash": "wang|robust_discriminative_representation_learning_via_gradient_rescaling_an_emphasis_regularisation_perspective", "original_pdf": "/attachment/2a507591594cf79fae3cd6005cbe964b4d863dd9.pdf", "_bibtex": "@misc{\nwang2020robust,\ntitle={{\\{}ROBUST{\\}} {\\{}DISCRIMINATIVE{\\}} {\\{}REPRESENTATION{\\}} {\\{}LEARNING{\\}} {\\{}VIA{\\}} {\\{}GRADIENT{\\}} {\\{}RESCALING{\\}}: {\\{}AN{\\}} {\\{}EMPHASIS{\\}} {\\{}REGULARISATION{\\}} {\\{}PERSPECTIVE{\\}}},\nauthor={Xinshao Wang and Yang Hua and Elyor Kodirov and Neil M. Robertson},\nyear={2020},\nurl={https://openreview.net/forum?id=rylUOn4Yvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rylUOn4Yvr", "replyto": "rylUOn4Yvr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper42/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper42/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576046951544, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper42/Reviewers"], "noninvitees": [], "tcdate": 1570237757994, "tmdate": 1576046951556, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper42/-/Official_Review"}}}, {"id": "H1xJEl_Z9r", "original": null, "number": 2, "cdate": 1572073510857, "ddate": null, "tcdate": 1572073510857, "tmdate": 1572972646127, "tddate": null, "forum": "rylUOn4Yvr", "replyto": "rylUOn4Yvr", "invitation": "ICLR.cc/2020/Conference/Paper42/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary\nThis paper presents Gradient Rescaling (GR) for robust learning to combat label noise. They propose to treat each data sample with different significance scores: some samples are important to learning, and some examples are insignificant (or even detrimental) to learning. So they desire to weight each samples according to their significance. They propose the notion of emphasis focus (When learning, whether we should put emphasis on learning \u201chard\u201d examples or \u201ceasy\u201d examples) and emphasis spread (the variance of these significance weights). The authors propose that this \u201cdifficulty\u201d of samples are proportional to their network output logit values.\nThe authors examine the analytical forms of the gradients of popular loss functions such as Categorical Cross Entropy, Mean Absolute Error and Generalized Cross Entropy. They find that the formulas for the gradient are of similar family with varying hyperparameters. Authors claim that tweaking these hyperparameters result in tuning the emphasis focus and spread.\nThe authors conduct Experiments on CIFAR10, CIFAR100 with simulated symmetric noise. Also, they conduct experiments on real-world noisy datasets: Clothing 1M dataset and MARS video dataset. The authors claim that the performance of GR exceeds various baselines.\n\n\nSignificance/Novelty/Clarity\nSignificance: Low-Medium. The performance increase exhibited in the experiments are a bit underwhelming (when considering the fact that benchmarks of most recent noise-robustness algos such as <Lee et al. 2019 ICML> are missing).\nNovelty: Medium. The paper is interesting in the sense that the authors integrated (and allegedly generalized) the gradient formulas for several losses into one family, and tried to integrate and tweak their postulation of  \u201cEmphasis focus\u201d and \u201cemphasis spread\u201d into the framework. However, the theoretical ground and convincing reasoning for their claim seems a bit lackluster.\nClarity: Low. The overall flow of the paper is a bit fuzzy - exhibiting a stream-of-consciousness style flow.\n\n\nPros and Cons in Detail\nPros:\n1.The authors try to unify the analytical forms of the gradients of various loss functions into a single family equipped with hyperparameters that control emphasis focus and spread.\n2.Conducted experiments show that GR achieved increased performance when compared to the baselines.\nCons:\nMy major concern is about tuning newly introduced hyperparameters in practical settings. How can we guarantee to have intact validation set?  Can we get any improvement via GR even with corrupted validation set for tuning hyperparameters? \n1. The arguments of the authors are grounded in the premise that \u201cdifficult\u201d samples will exhibit small logit values, and \u201ceasy\u201d samples high logit values.\n2. No justifications (both theoretical and experimental) are provided on the claim that controlling emphasis focus/spread will result in more robust learning. \n3. This algorithm introduces 2 additional hyperparameters that are correlated with each other. This introduces additional labor.\n4. By changing the loss function, the outputs of the network might lose its interpretation as a probability distribution.\n5. No confidence intervals are shown except for the CIFAR-100 experiment.\n6. Experiments are only conducted on vision tasks.\n7. The baseline menagerie also changes when the authors change the target dataset.\n8. Additional benchmarks of most recent noise-robustness algos such as <Lee et al. 2019 ICML> are required.\n\n\nQuestions\n1. Is it always the case that \u201cdifficult\u201d samples exhibit small logit values, and \u201ceasy\u201d samples high logit values?\n2. If not, GR\u2019s emphasis manipulation might result in neglecting samples containing valuable information.\n3. Can GR be used simultaneously with other noise-robust learning methods to further boost the performance?\n4. Technically, GR aims to rescale the gradients of the logits. How will it interact with optimizers  other than SGD such as Adam?\n5. Does GR still work well on small datasets(#points < 5000)?\n\n\nMisc. Comments\nPage 3-> inside L1 norm, no differentiation sign in the denominator.\nAround eq 2 and 4: missing derivative symbol w.r.t. z"}, "signatures": ["ICLR.cc/2020/Conference/Paper42/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper42/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ROBUST DISCRIMINATIVE REPRESENTATION LEARNING VIA GRADIENT RESCALING: AN EMPHASIS REGULARISATION PERSPECTIVE", "authors": ["Xinshao Wang", "Yang Hua", "Elyor Kodirov", "Neil M. Robertson"], "authorids": ["xwang39@qub.ac.uk", "y.hua@qub.ac.uk", "elyor@anyvision.co", "n.robertson@qub.ac.uk"], "keywords": ["examples weighting", "emphasis regularisation", "gradient scaling", "abnormal training examples"], "TL;DR": "ROBUST DISCRIMINATIVE REPRESENTATION LEARNING VIA GRADIENT RESCALING: AN EMPHASIS REGULARISATION PERSPECTIVE", "abstract": "It is fundamental and challenging to train robust and accurate Deep Neural Networks (DNNs) when semantically abnormal examples exist. Although great progress has been made, there is still one crucial research question which is not thoroughly explored yet: What training examples should be focused and how much more should they be emphasised to achieve robust learning? In this work, we study this question and propose gradient rescaling (GR) to solve it. GR modifies the magnitude of logit vector\u2019s gradient to emphasise on relatively easier training data points when noise becomes more severe, which functions as explicit emphasis regularisation to improve the generalisation performance of DNNs. Apart from regularisation, we connect GR to examples weighting and designing robust loss functions. We empirically demonstrate that GR is highly anomaly-robust and outperforms the state-of-the-art by a large margin, e.g., increasing 7% on CIFAR100 with 40% noisy labels. It is also significantly superior to standard regularisers in both clean and abnormal settings. Furthermore, we present comprehensive ablation studies to explore the behaviours of GR under different cases, which is informative for applying GR in real-world scenarios.", "pdf": "/pdf/5fecb7e054b65fe4a80d5ed507144c216fbf56fb.pdf", "paperhash": "wang|robust_discriminative_representation_learning_via_gradient_rescaling_an_emphasis_regularisation_perspective", "original_pdf": "/attachment/2a507591594cf79fae3cd6005cbe964b4d863dd9.pdf", "_bibtex": "@misc{\nwang2020robust,\ntitle={{\\{}ROBUST{\\}} {\\{}DISCRIMINATIVE{\\}} {\\{}REPRESENTATION{\\}} {\\{}LEARNING{\\}} {\\{}VIA{\\}} {\\{}GRADIENT{\\}} {\\{}RESCALING{\\}}: {\\{}AN{\\}} {\\{}EMPHASIS{\\}} {\\{}REGULARISATION{\\}} {\\{}PERSPECTIVE{\\}}},\nauthor={Xinshao Wang and Yang Hua and Elyor Kodirov and Neil M. Robertson},\nyear={2020},\nurl={https://openreview.net/forum?id=rylUOn4Yvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rylUOn4Yvr", "replyto": "rylUOn4Yvr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper42/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper42/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576046951544, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper42/Reviewers"], "noninvitees": [], "tcdate": 1570237757994, "tmdate": 1576046951556, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper42/-/Official_Review"}}}, {"id": "B1xnoAwm9B", "original": null, "number": 3, "cdate": 1572204195755, "ddate": null, "tcdate": 1572204195755, "tmdate": 1572972646080, "tddate": null, "forum": "rylUOn4Yvr", "replyto": "rylUOn4Yvr", "invitation": "ICLR.cc/2020/Conference/Paper42/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "Summary:\nThe authors first analyze and answer the question: What training examples should be focused and how large the emphasis spread should be? Then, they proposed the gradient rescaling framework serving as emphasis regularization. \n\nStrengths:\n1. The paper is well organized except the reference citation (read difficultly)\n2. The proposed method is very simple and effective. \n3. Experiments show the improvements over SOTA. \n\nWeakness:\n1. The experiments lack the recent important baseline \"symmetric cross entropy for robust learning with noisy labels, ICCV2019\", which are the current SOTA. Maybe the author should check the above paper and show the results.  \n2. The experiments are only conducted on symmetric noise. Actually, asymmetric noise is also important. The author should conduct at least some experiments on asymmetric noise. "}, "signatures": ["ICLR.cc/2020/Conference/Paper42/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper42/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ROBUST DISCRIMINATIVE REPRESENTATION LEARNING VIA GRADIENT RESCALING: AN EMPHASIS REGULARISATION PERSPECTIVE", "authors": ["Xinshao Wang", "Yang Hua", "Elyor Kodirov", "Neil M. Robertson"], "authorids": ["xwang39@qub.ac.uk", "y.hua@qub.ac.uk", "elyor@anyvision.co", "n.robertson@qub.ac.uk"], "keywords": ["examples weighting", "emphasis regularisation", "gradient scaling", "abnormal training examples"], "TL;DR": "ROBUST DISCRIMINATIVE REPRESENTATION LEARNING VIA GRADIENT RESCALING: AN EMPHASIS REGULARISATION PERSPECTIVE", "abstract": "It is fundamental and challenging to train robust and accurate Deep Neural Networks (DNNs) when semantically abnormal examples exist. Although great progress has been made, there is still one crucial research question which is not thoroughly explored yet: What training examples should be focused and how much more should they be emphasised to achieve robust learning? In this work, we study this question and propose gradient rescaling (GR) to solve it. GR modifies the magnitude of logit vector\u2019s gradient to emphasise on relatively easier training data points when noise becomes more severe, which functions as explicit emphasis regularisation to improve the generalisation performance of DNNs. Apart from regularisation, we connect GR to examples weighting and designing robust loss functions. We empirically demonstrate that GR is highly anomaly-robust and outperforms the state-of-the-art by a large margin, e.g., increasing 7% on CIFAR100 with 40% noisy labels. It is also significantly superior to standard regularisers in both clean and abnormal settings. Furthermore, we present comprehensive ablation studies to explore the behaviours of GR under different cases, which is informative for applying GR in real-world scenarios.", "pdf": "/pdf/5fecb7e054b65fe4a80d5ed507144c216fbf56fb.pdf", "paperhash": "wang|robust_discriminative_representation_learning_via_gradient_rescaling_an_emphasis_regularisation_perspective", "original_pdf": "/attachment/2a507591594cf79fae3cd6005cbe964b4d863dd9.pdf", "_bibtex": "@misc{\nwang2020robust,\ntitle={{\\{}ROBUST{\\}} {\\{}DISCRIMINATIVE{\\}} {\\{}REPRESENTATION{\\}} {\\{}LEARNING{\\}} {\\{}VIA{\\}} {\\{}GRADIENT{\\}} {\\{}RESCALING{\\}}: {\\{}AN{\\}} {\\{}EMPHASIS{\\}} {\\{}REGULARISATION{\\}} {\\{}PERSPECTIVE{\\}}},\nauthor={Xinshao Wang and Yang Hua and Elyor Kodirov and Neil M. Robertson},\nyear={2020},\nurl={https://openreview.net/forum?id=rylUOn4Yvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rylUOn4Yvr", "replyto": "rylUOn4Yvr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper42/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper42/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576046951544, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper42/Reviewers"], "noninvitees": [], "tcdate": 1570237757994, "tmdate": 1576046951556, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper42/-/Official_Review"}}}], "count": 13}