{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1528124473353, "tcdate": 1518453937602, "number": 156, "cdate": 1518453937602, "id": "SJ9nVSkvG", "invitation": "ICLR.cc/2018/Workshop/-/Submission", "forum": "SJ9nVSkvG", "signatures": ["~Arjun_Nitin_Bhagoji1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop"], "content": {"title": "Black-box Attacks on Deep Neural Networks via Gradient Estimation", "abstract": "In this paper, we propose novel Gradient Estimation black-box attacks to generate adversarial examples with query access to the target model's class probabilities, which do not rely on transferability. We also propose strategies to decouple the number of queries required to generate each adversarial example from the dimensionality of the input. An iterative variant of our attack achieves close to 100% attack success rates for both targeted and untargeted attacks on DNNs. We show that the proposed Gradient Estimation attacks outperform all other black-box attacks we tested on both MNIST and CIFAR-10 datasets, achieving attack success rates similar to well known, state-of-the-art white-box attacks. We also apply the Gradient Estimation attacks successfully against a real-world content moderation classifier hosted by Clarifai.", "paperhash": "bhagoji|blackbox_attacks_on_deep_neural_networks_via_gradient_estimation", "keywords": ["adversarial examples", "black-box", "real-world attacks"], "_bibtex": "@misc{\n  bhagoji2018black-box,\n  title={Black-box Attacks on Deep Neural Networks via Gradient Estimation},\n  author={Arjun Nitin Bhagoji and Warren He and Bo Li and Dawn Song},\n  year={2018},\n  url={https://openreview.net/forum?id=SJ9nVSkvG}\n}", "authorids": ["abhagoji@princeton.edu", "_w@eecs.berkeley.edu", "lxbosky@gmail.com", "dawnsong@gmail.com"], "authors": ["Arjun Nitin Bhagoji", "Warren He", "Bo Li", "Dawn Song"], "TL;DR": "Efficient query-based black-box attacks on neural networks", "pdf": "/pdf/1e1d859033a4929e56e4bd3116cdd216d68ee837.pdf"}, "nonreaders": [], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1518472800000, "tmdate": 1518474081690, "id": "ICLR.cc/2018/Workshop/-/Submission", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Workshop"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2018/Workshop", "description": "Your authorized identity to be associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 9, "value-regex": "upload", "description": "Upload a PDF file that ends with .pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 8, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names. Please provide real names; identities will be anonymized."}, "keywords": {"order": 6, "values-regex": "(^$)|[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of keywords."}, "TL;DR": {"required": false, "order": 7, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,500}"}, "authorids": {"required": true, "order": 3, "values-regex": "([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,},){0,}([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,})", "description": "Comma separated list of author email addresses, lowercased, in the same order as above. For authors with existing OpenReview accounts, please make sure that the provided email address(es) match those listed in the author's profile. Please provide real emails; identities will be anonymized."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1526248800000, "cdate": 1518474081690}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582936570, "tcdate": 1520292698193, "number": 1, "cdate": 1520292698193, "id": "SkGDm8iuz", "invitation": "ICLR.cc/2018/Workshop/-/Paper156/Official_Review", "forum": "SJ9nVSkvG", "replyto": "SJ9nVSkvG", "signatures": ["ICLR.cc/2018/Workshop/Paper156/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper156/AnonReviewer2"], "content": {"title": "An interesting paper", "rating": "7: Good paper, accept", "review": "The paper tackles adversarial example generation when the model that produces predictions is treated as a black box, with only information it provides is the class probabilities. Authors' approach also does not rely on building a surrogate model based on the predictions from the real model, and then finding the adversarial examples for this surrogate model.\nThe gist is that the adversary adds perturbations proportional to the estimated gradient, which can be obtained by querying the probabilities from the black box model 2d times, where d is the dimension of the input.  Authors also suggest two extensions that allow to reduce the number of queries needed\n\nOverall the paper is relatively easy to read, however some background information is missing. For example, targeted vs untargeted setting, single vs iterative. A couple of sentences explaining this would have gone the long way. \n\n- What is f (in the set of queries the adversary can make)\n- H (constraint set) is also not introduced - what does it represent. Is it the max change that can be introduced on a input instance as not to change it visually. Why and how does it participate in formula (2)\n- Random groupping is not well explained either.\n\nFor mnist and cifar experiments, how were those learning rates chosen (eg how many tries you had to perform to choose those params - these should be included into the \"number of iterations\")\n\n\n\n\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Black-box Attacks on Deep Neural Networks via Gradient Estimation", "abstract": "In this paper, we propose novel Gradient Estimation black-box attacks to generate adversarial examples with query access to the target model's class probabilities, which do not rely on transferability. We also propose strategies to decouple the number of queries required to generate each adversarial example from the dimensionality of the input. An iterative variant of our attack achieves close to 100% attack success rates for both targeted and untargeted attacks on DNNs. We show that the proposed Gradient Estimation attacks outperform all other black-box attacks we tested on both MNIST and CIFAR-10 datasets, achieving attack success rates similar to well known, state-of-the-art white-box attacks. We also apply the Gradient Estimation attacks successfully against a real-world content moderation classifier hosted by Clarifai.", "paperhash": "bhagoji|blackbox_attacks_on_deep_neural_networks_via_gradient_estimation", "keywords": ["adversarial examples", "black-box", "real-world attacks"], "_bibtex": "@misc{\n  bhagoji2018black-box,\n  title={Black-box Attacks on Deep Neural Networks via Gradient Estimation},\n  author={Arjun Nitin Bhagoji and Warren He and Bo Li and Dawn Song},\n  year={2018},\n  url={https://openreview.net/forum?id=SJ9nVSkvG}\n}", "authorids": ["abhagoji@princeton.edu", "_w@eecs.berkeley.edu", "lxbosky@gmail.com", "dawnsong@gmail.com"], "authors": ["Arjun Nitin Bhagoji", "Warren He", "Bo Li", "Dawn Song"], "TL;DR": "Efficient query-based black-box attacks on neural networks", "pdf": "/pdf/1e1d859033a4929e56e4bd3116cdd216d68ee837.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582936383, "id": "ICLR.cc/2018/Workshop/-/Paper156/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper156/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper156/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper156/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper156/AnonReviewer1"], "reply": {"forum": "SJ9nVSkvG", "replyto": "SJ9nVSkvG", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper156/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper156/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582936383}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582905398, "tcdate": 1520454012406, "number": 2, "cdate": 1520454012406, "id": "SyNYF6pdf", "invitation": "ICLR.cc/2018/Workshop/-/Paper156/Official_Review", "forum": "SJ9nVSkvG", "replyto": "SJ9nVSkvG", "signatures": ["ICLR.cc/2018/Workshop/Paper156/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper156/AnonReviewer3"], "content": {"title": "An interesting paper on black-box attack", "rating": "7: Good paper, accept", "review": "This paper proposes a black-box gradient estimation attacks on DNNs using limited query access. It achieves attack success rates close to that of white-box attacks and do not rely on transferability. This short paper is very well written and is easy to follow. It is an interesting paper.\n\nI have two comments.\n\n(1) In the experiments shown in Table 1, the proposed method is only compared with the white-box attacks. Given that the proposed method is highly related to another black-box attack method ZOO proposed in Chen et al. (2017), the authors are suggested to add ZOO method in their numerical comparison.\n\n(2) The authors mentioned that the tuning parameters were chosen as \\alpha = 0.01 and t = 40 for MNIST and \\alpha = 1.0 and t = 10 for CIFAR-10. Can the authors provide some explanations on how to select these parameters in practice? Is there any data-driven way to choose them? \n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Black-box Attacks on Deep Neural Networks via Gradient Estimation", "abstract": "In this paper, we propose novel Gradient Estimation black-box attacks to generate adversarial examples with query access to the target model's class probabilities, which do not rely on transferability. We also propose strategies to decouple the number of queries required to generate each adversarial example from the dimensionality of the input. An iterative variant of our attack achieves close to 100% attack success rates for both targeted and untargeted attacks on DNNs. We show that the proposed Gradient Estimation attacks outperform all other black-box attacks we tested on both MNIST and CIFAR-10 datasets, achieving attack success rates similar to well known, state-of-the-art white-box attacks. We also apply the Gradient Estimation attacks successfully against a real-world content moderation classifier hosted by Clarifai.", "paperhash": "bhagoji|blackbox_attacks_on_deep_neural_networks_via_gradient_estimation", "keywords": ["adversarial examples", "black-box", "real-world attacks"], "_bibtex": "@misc{\n  bhagoji2018black-box,\n  title={Black-box Attacks on Deep Neural Networks via Gradient Estimation},\n  author={Arjun Nitin Bhagoji and Warren He and Bo Li and Dawn Song},\n  year={2018},\n  url={https://openreview.net/forum?id=SJ9nVSkvG}\n}", "authorids": ["abhagoji@princeton.edu", "_w@eecs.berkeley.edu", "lxbosky@gmail.com", "dawnsong@gmail.com"], "authors": ["Arjun Nitin Bhagoji", "Warren He", "Bo Li", "Dawn Song"], "TL;DR": "Efficient query-based black-box attacks on neural networks", "pdf": "/pdf/1e1d859033a4929e56e4bd3116cdd216d68ee837.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582936383, "id": "ICLR.cc/2018/Workshop/-/Paper156/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper156/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper156/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper156/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper156/AnonReviewer1"], "reply": {"forum": "SJ9nVSkvG", "replyto": "SJ9nVSkvG", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper156/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper156/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582936383}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582766519, "tcdate": 1520639204786, "number": 3, "cdate": 1520639204786, "id": "BkTyTqgYG", "invitation": "ICLR.cc/2018/Workshop/-/Paper156/Official_Review", "forum": "SJ9nVSkvG", "replyto": "SJ9nVSkvG", "signatures": ["ICLR.cc/2018/Workshop/Paper156/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper156/AnonReviewer1"], "content": {"title": "Promising approach  to the generation of adversarial attacks through gradient estimation in a black box setting.", "rating": "6: Marginally above acceptance threshold", "review": "This work proposes a  black box approach  to the generation of adversarial attacks through gradient estimation.\nThe proposed method is designed in a rather realistic thus difficult setting:\n  - Black box (no attacked model prior knowledge),\n  - No surrogate model (transferability) used to develop the adversarial attacks.\n  - Query based attacks with 2 query reduction approaches (to alleviate black-box approaches' need for large numbers of queries).   \nit does require access to attacked model's:\n  - Outputs to supplied inputs and\n  - class probabilities.\nThe approach is clear and well presented, through the limitations of a short paper.\nThe experimental part presents interesting results when compared to white box approaches especially with query reduction.\nAppreciated is the fact that the evaluations were conducted on both MNIST handwritten digits and CIFAR-10 image datasets which provide for varied types of images.\n\n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Black-box Attacks on Deep Neural Networks via Gradient Estimation", "abstract": "In this paper, we propose novel Gradient Estimation black-box attacks to generate adversarial examples with query access to the target model's class probabilities, which do not rely on transferability. We also propose strategies to decouple the number of queries required to generate each adversarial example from the dimensionality of the input. An iterative variant of our attack achieves close to 100% attack success rates for both targeted and untargeted attacks on DNNs. We show that the proposed Gradient Estimation attacks outperform all other black-box attacks we tested on both MNIST and CIFAR-10 datasets, achieving attack success rates similar to well known, state-of-the-art white-box attacks. We also apply the Gradient Estimation attacks successfully against a real-world content moderation classifier hosted by Clarifai.", "paperhash": "bhagoji|blackbox_attacks_on_deep_neural_networks_via_gradient_estimation", "keywords": ["adversarial examples", "black-box", "real-world attacks"], "_bibtex": "@misc{\n  bhagoji2018black-box,\n  title={Black-box Attacks on Deep Neural Networks via Gradient Estimation},\n  author={Arjun Nitin Bhagoji and Warren He and Bo Li and Dawn Song},\n  year={2018},\n  url={https://openreview.net/forum?id=SJ9nVSkvG}\n}", "authorids": ["abhagoji@princeton.edu", "_w@eecs.berkeley.edu", "lxbosky@gmail.com", "dawnsong@gmail.com"], "authors": ["Arjun Nitin Bhagoji", "Warren He", "Bo Li", "Dawn Song"], "TL;DR": "Efficient query-based black-box attacks on neural networks", "pdf": "/pdf/1e1d859033a4929e56e4bd3116cdd216d68ee837.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582936383, "id": "ICLR.cc/2018/Workshop/-/Paper156/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper156/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper156/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper156/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper156/AnonReviewer1"], "reply": {"forum": "SJ9nVSkvG", "replyto": "SJ9nVSkvG", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper156/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper156/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582936383}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521573555850, "tcdate": 1521573555850, "number": 56, "cdate": 1521573555504, "id": "BJhhCRAYz", "invitation": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "forum": "SJ9nVSkvG", "replyto": "SJ9nVSkvG", "signatures": ["ICLR.cc/2018/Workshop/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Program_Chairs"], "content": {"decision": "Accept", "title": "ICLR 2018 Workshop Acceptance Decision", "comment": "Congratulations, your paper was accepted to the ICLR workshop."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Black-box Attacks on Deep Neural Networks via Gradient Estimation", "abstract": "In this paper, we propose novel Gradient Estimation black-box attacks to generate adversarial examples with query access to the target model's class probabilities, which do not rely on transferability. We also propose strategies to decouple the number of queries required to generate each adversarial example from the dimensionality of the input. An iterative variant of our attack achieves close to 100% attack success rates for both targeted and untargeted attacks on DNNs. We show that the proposed Gradient Estimation attacks outperform all other black-box attacks we tested on both MNIST and CIFAR-10 datasets, achieving attack success rates similar to well known, state-of-the-art white-box attacks. We also apply the Gradient Estimation attacks successfully against a real-world content moderation classifier hosted by Clarifai.", "paperhash": "bhagoji|blackbox_attacks_on_deep_neural_networks_via_gradient_estimation", "keywords": ["adversarial examples", "black-box", "real-world attacks"], "_bibtex": "@misc{\n  bhagoji2018black-box,\n  title={Black-box Attacks on Deep Neural Networks via Gradient Estimation},\n  author={Arjun Nitin Bhagoji and Warren He and Bo Li and Dawn Song},\n  year={2018},\n  url={https://openreview.net/forum?id=SJ9nVSkvG}\n}", "authorids": ["abhagoji@princeton.edu", "_w@eecs.berkeley.edu", "lxbosky@gmail.com", "dawnsong@gmail.com"], "authors": ["Arjun Nitin Bhagoji", "Warren He", "Bo Li", "Dawn Song"], "TL;DR": "Efficient query-based black-box attacks on neural networks", "pdf": "/pdf/1e1d859033a4929e56e4bd3116cdd216d68ee837.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518629844880, "id": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Program_Chairs"], "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Workshop/-/Submission", "writers": {"values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Workshop/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Workshop Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept", "Reject"]}}}, "nonreaders": [], "noninvitees": [], "cdate": 1518629844880}}}], "count": 5}