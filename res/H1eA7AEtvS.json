{"notes": [{"id": "H1eA7AEtvS", "original": "SJx97xUODH", "number": 1057, "cdate": 1569439269788, "ddate": null, "tcdate": 1569439269788, "tmdate": 1583912037396, "tddate": null, "forum": "H1eA7AEtvS", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations", "authors": ["Zhenzhong Lan", "Mingda Chen", "Sebastian Goodman", "Kevin Gimpel", "Piyush Sharma", "Radu Soricut"], "authorids": ["lanzhzh@google.com", "mchen@ttic.edu", "seabass@google.com", "kgimpel@ttic.edu", "piyushsharma@google.com", "rsoricut@google.com"], "keywords": ["Natural Language Processing", "BERT", "Representation Learning"], "TL;DR": "A new pretraining method that establishes new state-of-the-art results on the GLUE, RACE, and SQuAD benchmarks while having fewer parameters compared to BERT-large. ", "abstract": "Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations and longer training times. To address these problems,  we present two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT~\\citep{devlin2018bert}. Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the original BERT. We also use a self-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and \\squad benchmarks while having fewer parameters compared to BERT-large. The code and the pretrained models are available at https://github.com/google-research/ALBERT.", "pdf": "/pdf/ce1860d9372b46ab2700549349420cc19125d478.pdf", "paperhash": "lan|albert_a_lite_bert_for_selfsupervised_learning_of_language_representations", "code": "https://github.com/google-research/ALBERT", "_bibtex": "@inproceedings{\nLan2020ALBERT:,\ntitle={ALBERT: A Lite BERT for Self-supervised Learning of Language Representations},\nauthor={Zhenzhong Lan and Mingda Chen and Sebastian Goodman and Kevin Gimpel and Piyush Sharma and Radu Soricut},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=H1eA7AEtvS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/dfa54b8bfba46beb0d45f76b449d4e5eb744c546.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 32, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "lTD-xLRqmc", "original": null, "number": 1, "cdate": 1576798713430, "ddate": null, "tcdate": 1576798713430, "tmdate": 1576800923028, "tddate": null, "forum": "H1eA7AEtvS", "replyto": "H1eA7AEtvS", "invitation": "ICLR.cc/2020/Conference/Paper1057/-/Decision", "content": {"decision": "Accept (Spotlight)", "comment": "This paper proposes three modifications of BERT type models two of which is concerned with parameter sharing and one with a new auxiliary loss. New SOTA on downstream tasks are demonstrated. \n\nAll reviewers liked the paper and so did a lot of comments. \n\nAcceptance is recommended.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations", "authors": ["Zhenzhong Lan", "Mingda Chen", "Sebastian Goodman", "Kevin Gimpel", "Piyush Sharma", "Radu Soricut"], "authorids": ["lanzhzh@google.com", "mchen@ttic.edu", "seabass@google.com", "kgimpel@ttic.edu", "piyushsharma@google.com", "rsoricut@google.com"], "keywords": ["Natural Language Processing", "BERT", "Representation Learning"], "TL;DR": "A new pretraining method that establishes new state-of-the-art results on the GLUE, RACE, and SQuAD benchmarks while having fewer parameters compared to BERT-large. ", "abstract": "Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations and longer training times. To address these problems,  we present two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT~\\citep{devlin2018bert}. Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the original BERT. We also use a self-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and \\squad benchmarks while having fewer parameters compared to BERT-large. The code and the pretrained models are available at https://github.com/google-research/ALBERT.", "pdf": "/pdf/ce1860d9372b46ab2700549349420cc19125d478.pdf", "paperhash": "lan|albert_a_lite_bert_for_selfsupervised_learning_of_language_representations", "code": "https://github.com/google-research/ALBERT", "_bibtex": "@inproceedings{\nLan2020ALBERT:,\ntitle={ALBERT: A Lite BERT for Self-supervised Learning of Language Representations},\nauthor={Zhenzhong Lan and Mingda Chen and Sebastian Goodman and Kevin Gimpel and Piyush Sharma and Radu Soricut},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=H1eA7AEtvS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/dfa54b8bfba46beb0d45f76b449d4e5eb744c546.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "H1eA7AEtvS", "replyto": "H1eA7AEtvS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795715146, "tmdate": 1576800264996, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1057/-/Decision"}}}, {"id": "rygf22FniS", "original": null, "number": 16, "cdate": 1573850281838, "ddate": null, "tcdate": 1573850281838, "tmdate": 1573850281838, "tddate": null, "forum": "H1eA7AEtvS", "replyto": "ByxuKYW-jB", "invitation": "ICLR.cc/2020/Conference/Paper1057/-/Official_Comment", "content": {"title": "Reply to authors", "comment": "Okay thanks for the clarification!"}, "signatures": ["ICLR.cc/2020/Conference/Paper1057/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1057/AnonReviewer1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations", "authors": ["Zhenzhong Lan", "Mingda Chen", "Sebastian Goodman", "Kevin Gimpel", "Piyush Sharma", "Radu Soricut"], "authorids": ["lanzhzh@google.com", "mchen@ttic.edu", "seabass@google.com", "kgimpel@ttic.edu", "piyushsharma@google.com", "rsoricut@google.com"], "keywords": ["Natural Language Processing", "BERT", "Representation Learning"], "TL;DR": "A new pretraining method that establishes new state-of-the-art results on the GLUE, RACE, and SQuAD benchmarks while having fewer parameters compared to BERT-large. ", "abstract": "Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations and longer training times. To address these problems,  we present two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT~\\citep{devlin2018bert}. Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the original BERT. We also use a self-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and \\squad benchmarks while having fewer parameters compared to BERT-large. The code and the pretrained models are available at https://github.com/google-research/ALBERT.", "pdf": "/pdf/ce1860d9372b46ab2700549349420cc19125d478.pdf", "paperhash": "lan|albert_a_lite_bert_for_selfsupervised_learning_of_language_representations", "code": "https://github.com/google-research/ALBERT", "_bibtex": "@inproceedings{\nLan2020ALBERT:,\ntitle={ALBERT: A Lite BERT for Self-supervised Learning of Language Representations},\nauthor={Zhenzhong Lan and Mingda Chen and Sebastian Goodman and Kevin Gimpel and Piyush Sharma and Radu Soricut},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=H1eA7AEtvS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/dfa54b8bfba46beb0d45f76b449d4e5eb744c546.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1eA7AEtvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1057/Authors", "ICLR.cc/2020/Conference/Paper1057/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1057/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1057/Reviewers", "ICLR.cc/2020/Conference/Paper1057/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1057/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1057/Authors|ICLR.cc/2020/Conference/Paper1057/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504161921, "tmdate": 1576860558114, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1057/Authors", "ICLR.cc/2020/Conference/Paper1057/Reviewers", "ICLR.cc/2020/Conference/Paper1057/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1057/-/Official_Comment"}}}, {"id": "HkxZGWJisB", "original": null, "number": 15, "cdate": 1573740809342, "ddate": null, "tcdate": 1573740809342, "tmdate": 1573740809342, "tddate": null, "forum": "H1eA7AEtvS", "replyto": "H1eA7AEtvS", "invitation": "ICLR.cc/2020/Conference/Paper1057/-/Official_Comment", "content": {"title": "Paper updates ", "comment": "We want to thank the reviewers again for their suggestions! We have updated the paper with the following changes: \n  - Addressing the typo pointed out by Reviewer 2.\n  - Addressing Reviewer 3\u2019s concern on the overgeneralization problem of this sentence \u201cdropout can hurt performance in large Transformer-based models\u201d\n- Addressing Reviewer 3\u2019s suggestion of comparing embedding factorization with other related methods in the literature.  \n- Addressing Reviewer 3\u2019s suggestion clarifying the additional data usage.  \n- Addressing all other public comments. \n\nWe also move the section of comparing BERT-xxlarge to BERT-large with the amount of training time to appendix so that we can fit the 10 page limit.  \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1057/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1057/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations", "authors": ["Zhenzhong Lan", "Mingda Chen", "Sebastian Goodman", "Kevin Gimpel", "Piyush Sharma", "Radu Soricut"], "authorids": ["lanzhzh@google.com", "mchen@ttic.edu", "seabass@google.com", "kgimpel@ttic.edu", "piyushsharma@google.com", "rsoricut@google.com"], "keywords": ["Natural Language Processing", "BERT", "Representation Learning"], "TL;DR": "A new pretraining method that establishes new state-of-the-art results on the GLUE, RACE, and SQuAD benchmarks while having fewer parameters compared to BERT-large. ", "abstract": "Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations and longer training times. To address these problems,  we present two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT~\\citep{devlin2018bert}. Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the original BERT. We also use a self-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and \\squad benchmarks while having fewer parameters compared to BERT-large. The code and the pretrained models are available at https://github.com/google-research/ALBERT.", "pdf": "/pdf/ce1860d9372b46ab2700549349420cc19125d478.pdf", "paperhash": "lan|albert_a_lite_bert_for_selfsupervised_learning_of_language_representations", "code": "https://github.com/google-research/ALBERT", "_bibtex": "@inproceedings{\nLan2020ALBERT:,\ntitle={ALBERT: A Lite BERT for Self-supervised Learning of Language Representations},\nauthor={Zhenzhong Lan and Mingda Chen and Sebastian Goodman and Kevin Gimpel and Piyush Sharma and Radu Soricut},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=H1eA7AEtvS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/dfa54b8bfba46beb0d45f76b449d4e5eb744c546.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1eA7AEtvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1057/Authors", "ICLR.cc/2020/Conference/Paper1057/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1057/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1057/Reviewers", "ICLR.cc/2020/Conference/Paper1057/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1057/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1057/Authors|ICLR.cc/2020/Conference/Paper1057/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504161921, "tmdate": 1576860558114, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1057/Authors", "ICLR.cc/2020/Conference/Paper1057/Reviewers", "ICLR.cc/2020/Conference/Paper1057/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1057/-/Official_Comment"}}}, {"id": "B1x9O5W-jr", "original": null, "number": 13, "cdate": 1573096050449, "ddate": null, "tcdate": 1573096050449, "tmdate": 1573097304281, "tddate": null, "forum": "H1eA7AEtvS", "replyto": "rJlMUMIoFH", "invitation": "ICLR.cc/2020/Conference/Paper1057/-/Official_Comment", "content": {"title": "Thank you so much for going through the paper carefully and providing positive and useful feedback to our work", "comment": "Dear Reviewer #3, \n\nThank you so much for going through the paper carefully and providing positive and useful feedback to our work! \nPlease see our responses below:\n\nWe do mention that \u201cThe parameter reduction techniques also act as a form of regularization that stabilizes the training and helps with generalization.\u201d However, we did not want to interpret along this dimension too much, as we did not have rigorous proof that regularization can solve the model degradation problem. One of the major difficulties in doing experiments with the BERT-xlarge setup is that it needs to train on 1024 TPUs v3 units, which is an expensive resource.\n\nFor the overgeneralization problem in the sentence \u201cdropout can hurt performance in large Transformer-based models\u201d, we will make it clear by adding this sentence: \u201cHowever, the underlying network structure of ALBERT is a special case of the Transformer, and further experimentation is needed to see if this phenomenon appears with other Transformer-based variants.\u201d \n\nWe agree that a better discussion of related work would help people to better understand our work.  Currently, we have a public comment from Sachin Mehta asking to compare related works such as \u2018Adaptive input representations for neural language modeling.\u2019, \u2018transformer-xl\u2019, and \u2018Efficient softmax approximation for GPUs\u2019. Please let us know if you have any specific additional work that you want us to include in the comparison of our embedding factorization methods, and we will incorporate it in the final version of the paper. \n \nThe reason we did not report the inference time (latency) is because it is a platform-specific metric. We would need different strategies to optimize for TPUs and CPUs. However, we do have some TPU-based metrics for BERT-base and ALBERT-base. By looking at these numbers, we see that ALBERT-base is about 3x faster than BERT-base at inference time. Other people have also converted a Chinese version of ALBERT-tiny (we would like to thank brightmart for implementing this project and he got an amazing number (1.4k) of stars in such a short amount of time!) into tf-lite format and measured the inference time on mobile devices. Here is the quote from his website (https://github.com/brightmart/albert_zh): \u201cOn an Android phone w/ Qualcomm's SD845 SoC, via the above benchmark tool, as of 2019/11/01, the inference latency is ~120ms w/ this converted TFLite model using 4 threads on CPU, and the memory usage is ~60MB for the model during inference. Note the performance will improve further with future TFLite implementation optimizations.\u201d\n\nThat being said, we would say ALBERT could have a huge impact on inference speed because inference speed is usually memory-bandwidth bound and memory bandwidth is limited by memory capacity. For example, if you can keep the model weights used in matmuls in a smaller high-bandwidth memory, you can go 10x - 100x faster than if you need to read it out of a larger lower-bandwidth memory.\n\nIn terms of model size in MB, they are roughly 4x as large as the parameter size as the weights are mostly in float format. \n\nWe use all the XLNet data (126G) as well as the stories data (31G) of raw data. Our 1M step training went though the same number of iterations over the data as Roberta 500K, as they use 2x as large a batch size as ours.\n\nThanks for bringing up these points, they are certainly valid and relevant. We will incorporate more info along the discussion above in the next version of our paper. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1057/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1057/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations", "authors": ["Zhenzhong Lan", "Mingda Chen", "Sebastian Goodman", "Kevin Gimpel", "Piyush Sharma", "Radu Soricut"], "authorids": ["lanzhzh@google.com", "mchen@ttic.edu", "seabass@google.com", "kgimpel@ttic.edu", "piyushsharma@google.com", "rsoricut@google.com"], "keywords": ["Natural Language Processing", "BERT", "Representation Learning"], "TL;DR": "A new pretraining method that establishes new state-of-the-art results on the GLUE, RACE, and SQuAD benchmarks while having fewer parameters compared to BERT-large. ", "abstract": "Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations and longer training times. To address these problems,  we present two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT~\\citep{devlin2018bert}. Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the original BERT. We also use a self-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and \\squad benchmarks while having fewer parameters compared to BERT-large. The code and the pretrained models are available at https://github.com/google-research/ALBERT.", "pdf": "/pdf/ce1860d9372b46ab2700549349420cc19125d478.pdf", "paperhash": "lan|albert_a_lite_bert_for_selfsupervised_learning_of_language_representations", "code": "https://github.com/google-research/ALBERT", "_bibtex": "@inproceedings{\nLan2020ALBERT:,\ntitle={ALBERT: A Lite BERT for Self-supervised Learning of Language Representations},\nauthor={Zhenzhong Lan and Mingda Chen and Sebastian Goodman and Kevin Gimpel and Piyush Sharma and Radu Soricut},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=H1eA7AEtvS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/dfa54b8bfba46beb0d45f76b449d4e5eb744c546.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1eA7AEtvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1057/Authors", "ICLR.cc/2020/Conference/Paper1057/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1057/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1057/Reviewers", "ICLR.cc/2020/Conference/Paper1057/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1057/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1057/Authors|ICLR.cc/2020/Conference/Paper1057/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504161921, "tmdate": 1576860558114, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1057/Authors", "ICLR.cc/2020/Conference/Paper1057/Reviewers", "ICLR.cc/2020/Conference/Paper1057/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1057/-/Official_Comment"}}}, {"id": "B1gKTRWbjH", "original": null, "number": 14, "cdate": 1573097153148, "ddate": null, "tcdate": 1573097153148, "tmdate": 1573097153148, "tddate": null, "forum": "H1eA7AEtvS", "replyto": "BkxJzVhyoB", "invitation": "ICLR.cc/2020/Conference/Paper1057/-/Official_Comment", "content": {"title": "Thanks for helping us to explain the oscillating phenomenon", "comment": "Hi Hi Renjie! \n\nThanks for commenting on our paper! I enjoy reading your paper and hoped that I could come up with such an elegant solution!  \n\nWould address the name error problem and acknowledge that DEQ (you) also found that simple weight-sharing would result in oscillation. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1057/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1057/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations", "authors": ["Zhenzhong Lan", "Mingda Chen", "Sebastian Goodman", "Kevin Gimpel", "Piyush Sharma", "Radu Soricut"], "authorids": ["lanzhzh@google.com", "mchen@ttic.edu", "seabass@google.com", "kgimpel@ttic.edu", "piyushsharma@google.com", "rsoricut@google.com"], "keywords": ["Natural Language Processing", "BERT", "Representation Learning"], "TL;DR": "A new pretraining method that establishes new state-of-the-art results on the GLUE, RACE, and SQuAD benchmarks while having fewer parameters compared to BERT-large. ", "abstract": "Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations and longer training times. To address these problems,  we present two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT~\\citep{devlin2018bert}. Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the original BERT. We also use a self-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and \\squad benchmarks while having fewer parameters compared to BERT-large. The code and the pretrained models are available at https://github.com/google-research/ALBERT.", "pdf": "/pdf/ce1860d9372b46ab2700549349420cc19125d478.pdf", "paperhash": "lan|albert_a_lite_bert_for_selfsupervised_learning_of_language_representations", "code": "https://github.com/google-research/ALBERT", "_bibtex": "@inproceedings{\nLan2020ALBERT:,\ntitle={ALBERT: A Lite BERT for Self-supervised Learning of Language Representations},\nauthor={Zhenzhong Lan and Mingda Chen and Sebastian Goodman and Kevin Gimpel and Piyush Sharma and Radu Soricut},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=H1eA7AEtvS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/dfa54b8bfba46beb0d45f76b449d4e5eb744c546.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1eA7AEtvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1057/Authors", "ICLR.cc/2020/Conference/Paper1057/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1057/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1057/Reviewers", "ICLR.cc/2020/Conference/Paper1057/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1057/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1057/Authors|ICLR.cc/2020/Conference/Paper1057/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504161921, "tmdate": 1576860558114, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1057/Authors", "ICLR.cc/2020/Conference/Paper1057/Reviewers", "ICLR.cc/2020/Conference/Paper1057/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1057/-/Official_Comment"}}}, {"id": "ByxuKYW-jB", "original": null, "number": 12, "cdate": 1573095807657, "ddate": null, "tcdate": 1573095807657, "tmdate": 1573095807657, "tddate": null, "forum": "H1eA7AEtvS", "replyto": "SygC0QIhFB", "invitation": "ICLR.cc/2020/Conference/Paper1057/-/Official_Comment", "content": {"title": "Thank you so much for going through the paper carefully and providing such a positive feedback about our work", "comment": "Dear Reviewer # 1\n\nThank you so much for going through the paper carefully and providing such a positive feedback about our work! Please see our answers below:\n\nFor hyper-parameter tuning, we only explore those hyper-parameters that are related to model size. This is done for the following two reasons: 1) To keep the comparison as meaningful as possible (so we fixed all other parameters as in BERT, and always used LAMB optimizer) 2) To keep under control the number of experiments we need to run; we already have a lot of experiments to report on; if we were to tune other hyper-parameters like learning rate and optimizer, the number of experiments needed can easily run out of control. \nFor optimizer, we choose LAMB because it allows us to use large batch sizes. We haven\u2019t tested other optimizers yet. \nBecause large models that can cause degenerate solutions are extremely expensive to run, we did not explore that area very much. For example, in order to run with a batch size of 4096, BERT-xlarge requires 1024 TPUs v3 units, which is an expensive resource. However, we are working on this and hopefully can give a reasonable explanation/solution to this problem soon.  "}, "signatures": ["ICLR.cc/2020/Conference/Paper1057/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1057/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations", "authors": ["Zhenzhong Lan", "Mingda Chen", "Sebastian Goodman", "Kevin Gimpel", "Piyush Sharma", "Radu Soricut"], "authorids": ["lanzhzh@google.com", "mchen@ttic.edu", "seabass@google.com", "kgimpel@ttic.edu", "piyushsharma@google.com", "rsoricut@google.com"], "keywords": ["Natural Language Processing", "BERT", "Representation Learning"], "TL;DR": "A new pretraining method that establishes new state-of-the-art results on the GLUE, RACE, and SQuAD benchmarks while having fewer parameters compared to BERT-large. ", "abstract": "Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations and longer training times. To address these problems,  we present two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT~\\citep{devlin2018bert}. Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the original BERT. We also use a self-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and \\squad benchmarks while having fewer parameters compared to BERT-large. The code and the pretrained models are available at https://github.com/google-research/ALBERT.", "pdf": "/pdf/ce1860d9372b46ab2700549349420cc19125d478.pdf", "paperhash": "lan|albert_a_lite_bert_for_selfsupervised_learning_of_language_representations", "code": "https://github.com/google-research/ALBERT", "_bibtex": "@inproceedings{\nLan2020ALBERT:,\ntitle={ALBERT: A Lite BERT for Self-supervised Learning of Language Representations},\nauthor={Zhenzhong Lan and Mingda Chen and Sebastian Goodman and Kevin Gimpel and Piyush Sharma and Radu Soricut},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=H1eA7AEtvS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/dfa54b8bfba46beb0d45f76b449d4e5eb744c546.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1eA7AEtvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1057/Authors", "ICLR.cc/2020/Conference/Paper1057/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1057/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1057/Reviewers", "ICLR.cc/2020/Conference/Paper1057/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1057/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1057/Authors|ICLR.cc/2020/Conference/Paper1057/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504161921, "tmdate": 1576860558114, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1057/Authors", "ICLR.cc/2020/Conference/Paper1057/Reviewers", "ICLR.cc/2020/Conference/Paper1057/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1057/-/Official_Comment"}}}, {"id": "Bkx3GYb-iB", "original": null, "number": 11, "cdate": 1573095700291, "ddate": null, "tcdate": 1573095700291, "tmdate": 1573095700291, "tddate": null, "forum": "H1eA7AEtvS", "replyto": "Hkl2NgFecr", "invitation": "ICLR.cc/2020/Conference/Paper1057/-/Official_Comment", "content": {"title": "Thank you so much for going through the paper carefully and providing such a positive feedback", "comment": "Dear Reviewer #2, \n\nThank you so much for going through the paper carefully and providing such a positive feedback. Please see below our response to your comments:\n\nAbout all-sharing vs non-shared: yes, non-shared gives better results, but the number of parameters is increased dramatically. We tried other strategies of sharing the parameters across layers. For example, we divided the L layers into N groups of size M (L=N*M), and each size-M group shares parameters. Overall, our experimental results show that the smaller the group size M is, the better the performance we get. However, decreasing group size M also dramatically increase the number of overall parameters. We chose the all-shared strategy to maximize our parameter reduction. \nWe are glad that you like our SOP objective. We did try other changes to the objectives, such as multiword masking, but papers proposing these ideas were posted before our work, so for simplicity we adopted and cited the previous works. \nThank you so much for helping us to correct the typo, we will fix them in our next version. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1057/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1057/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations", "authors": ["Zhenzhong Lan", "Mingda Chen", "Sebastian Goodman", "Kevin Gimpel", "Piyush Sharma", "Radu Soricut"], "authorids": ["lanzhzh@google.com", "mchen@ttic.edu", "seabass@google.com", "kgimpel@ttic.edu", "piyushsharma@google.com", "rsoricut@google.com"], "keywords": ["Natural Language Processing", "BERT", "Representation Learning"], "TL;DR": "A new pretraining method that establishes new state-of-the-art results on the GLUE, RACE, and SQuAD benchmarks while having fewer parameters compared to BERT-large. ", "abstract": "Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations and longer training times. To address these problems,  we present two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT~\\citep{devlin2018bert}. Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the original BERT. We also use a self-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and \\squad benchmarks while having fewer parameters compared to BERT-large. The code and the pretrained models are available at https://github.com/google-research/ALBERT.", "pdf": "/pdf/ce1860d9372b46ab2700549349420cc19125d478.pdf", "paperhash": "lan|albert_a_lite_bert_for_selfsupervised_learning_of_language_representations", "code": "https://github.com/google-research/ALBERT", "_bibtex": "@inproceedings{\nLan2020ALBERT:,\ntitle={ALBERT: A Lite BERT for Self-supervised Learning of Language Representations},\nauthor={Zhenzhong Lan and Mingda Chen and Sebastian Goodman and Kevin Gimpel and Piyush Sharma and Radu Soricut},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=H1eA7AEtvS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/dfa54b8bfba46beb0d45f76b449d4e5eb744c546.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1eA7AEtvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1057/Authors", "ICLR.cc/2020/Conference/Paper1057/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1057/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1057/Reviewers", "ICLR.cc/2020/Conference/Paper1057/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1057/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1057/Authors|ICLR.cc/2020/Conference/Paper1057/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504161921, "tmdate": 1576860558114, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1057/Authors", "ICLR.cc/2020/Conference/Paper1057/Reviewers", "ICLR.cc/2020/Conference/Paper1057/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1057/-/Official_Comment"}}}, {"id": "BkxJzVhyoB", "original": null, "number": 12, "cdate": 1573008390529, "ddate": null, "tcdate": 1573008390529, "tmdate": 1573008390529, "tddate": null, "forum": "H1eA7AEtvS", "replyto": "H1eA7AEtvS", "invitation": "ICLR.cc/2020/Conference/Paper1057/-/Public_Comment", "content": {"title": "Some very minor points on ALBERT and the deep equilibrium models", "comment": "Hi,\n\nI'm one of the authors of the deep equilibrium models. Congrats on the great work! I'm extremely excited that there is yet another proof of the power of weight-tied (or in your word, cross-layer sharing) models. I believe further investigation in this direction is a promising avenue for future work :-)\n\nJust a few very very minor points:\n\n- It's actually DEQ (as in Deep EQuilibrium models) instead of DQE :P \n\n- In your work, you claim that \"our observations show that our embeddings are oscillating rather than converging\", which you imply \"shows that the solution space for ALBERT parameters is very different from the one found by DQE [DEQ]\". But in fact, we also found that weight-tied Transformers oscillate (see Figure 2, right, and Figure 4, right, of the DEQ paper [1]); so it's not contradictive with your finding. In fact, we empirically found that these weight-shared networks are exactly oscillating around a fixed point (with some cyclic frequency, which is why layer diff seems to \"oscillate\"). We hypothesize this phenomenon is a result of the limitations of simple fixed-point iterations (FPI), which is what a weight-shared deep network is essentially doing. (The figure in https://aimeemarie88.wordpress.com/fixed-point-iterations/ actually visualizes this phenomenon for FPI, when the operator norm of the Jacobian of the tied transformation approaches 1). What we generally found is that once you target explicitly at solving for this fixed point, you will still be able to find it at the center of such oscillations (which yields DEQ).\n\nBut anyway, as I mentioned, these are very minor points. I just feel that this may help clarify a better connection between ALBERT and DEQ. (Plus, of course, in ALBERT you didn't use things like input injection, which DEQ required to formally formulate the implicit-depth model). Again, I'm excited about ALBERT!\n\n[1] https://arxiv.org/pdf/1909.01377.pdf"}, "signatures": ["~Shaojie_Bai1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Shaojie_Bai1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations", "authors": ["Zhenzhong Lan", "Mingda Chen", "Sebastian Goodman", "Kevin Gimpel", "Piyush Sharma", "Radu Soricut"], "authorids": ["lanzhzh@google.com", "mchen@ttic.edu", "seabass@google.com", "kgimpel@ttic.edu", "piyushsharma@google.com", "rsoricut@google.com"], "keywords": ["Natural Language Processing", "BERT", "Representation Learning"], "TL;DR": "A new pretraining method that establishes new state-of-the-art results on the GLUE, RACE, and SQuAD benchmarks while having fewer parameters compared to BERT-large. ", "abstract": "Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations and longer training times. To address these problems,  we present two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT~\\citep{devlin2018bert}. Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the original BERT. We also use a self-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and \\squad benchmarks while having fewer parameters compared to BERT-large. The code and the pretrained models are available at https://github.com/google-research/ALBERT.", "pdf": "/pdf/ce1860d9372b46ab2700549349420cc19125d478.pdf", "paperhash": "lan|albert_a_lite_bert_for_selfsupervised_learning_of_language_representations", "code": "https://github.com/google-research/ALBERT", "_bibtex": "@inproceedings{\nLan2020ALBERT:,\ntitle={ALBERT: A Lite BERT for Self-supervised Learning of Language Representations},\nauthor={Zhenzhong Lan and Mingda Chen and Sebastian Goodman and Kevin Gimpel and Piyush Sharma and Radu Soricut},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=H1eA7AEtvS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/dfa54b8bfba46beb0d45f76b449d4e5eb744c546.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1eA7AEtvS", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504200267, "tmdate": 1576860591222, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper1057/Authors", "ICLR.cc/2020/Conference/Paper1057/Reviewers", "ICLR.cc/2020/Conference/Paper1057/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1057/-/Public_Comment"}}}, {"id": "rJlMUMIoFH", "original": null, "number": 1, "cdate": 1571672649975, "ddate": null, "tcdate": 1571672649975, "tmdate": 1572972518118, "tddate": null, "forum": "H1eA7AEtvS", "replyto": "H1eA7AEtvS", "invitation": "ICLR.cc/2020/Conference/Paper1057/-/Official_Review", "content": {"rating": "6: Weak Accept", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "The authors present ALBERT, a modification of the BERT architecture with substantially fewer parameters. They show that despite being much smaller, the performance is very strong and achieves state of the art on a variety of different tasks. There are several ideas proposed here: embedding factorization, sharing layers, and sentence ordering as a training objective. \n\n1. The point that naively increasing the size of the BERT architecture does not work is a good one, but the authors don't acknowledge that this is tied up in the effect of regularization. Cross layer parameter sharing has a regularization effect that simply scaling up BERT large to x-large or such sizes does not have. This is also an issue with the authors making the statement that they are the first to show that dropout is harmful for Transformers. This is a large generalization that seems to be a special case of not only the regularized architecture they propose but also the large quantity of data that the model still underfits to.\n\n2. The authors propose embedding factorization to reduce the number of parameters in the embedding dimension. This is very intuitive, but the authors do not cite or compare to related approaches. I understand these models are computationally intensive and thus do not expect large quantities of detailed ablations. However, this kind of dimensionality reduction has been explored with other techniques, for example for knowledge distillation, quantization, or even adaptive input/softmax (and with subword as well, not just whole word modeling). These techniques have also been applied to machine translation models, which do not use them to learn rare words. I believe a better discussion of these methods should be added to the paper, as this is not a novel proposition.\n\n3. A large takeaway I have from this paper is that parameter size is not a good metric. While ALBERT is substantially smaller, the authors do not make it clear that this model is very slow at inference time due to the large size. This raises several questions: is it better to have models that are deeper or more wide? Can the authors actually report the latency in a comparative table next to BERT? Can the authors provide a sense of how large this model is in MB - e.g. presumably a goal of less parameters would be to have a model with less memory, but then the decision between memory and latency that different models make should be made more clear.\n\n4. Section 4.8 is not clear. Exactly how much data, in terms of GB of uncompressed text, is used here? Is it the data of XLNet and RoBERTa, so larger than both of those settings individually? Further, the authors train for 1 million steps. This is larger than both XLNet and RoBERTa, is that correct? Or there is some detail about the size of the batch that actually makes it comparable? The many small tables where the changes are not clearly delineated makes it difficult to compare results. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1057/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1057/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations", "authors": ["Zhenzhong Lan", "Mingda Chen", "Sebastian Goodman", "Kevin Gimpel", "Piyush Sharma", "Radu Soricut"], "authorids": ["lanzhzh@google.com", "mchen@ttic.edu", "seabass@google.com", "kgimpel@ttic.edu", "piyushsharma@google.com", "rsoricut@google.com"], "keywords": ["Natural Language Processing", "BERT", "Representation Learning"], "TL;DR": "A new pretraining method that establishes new state-of-the-art results on the GLUE, RACE, and SQuAD benchmarks while having fewer parameters compared to BERT-large. ", "abstract": "Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations and longer training times. To address these problems,  we present two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT~\\citep{devlin2018bert}. Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the original BERT. We also use a self-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and \\squad benchmarks while having fewer parameters compared to BERT-large. The code and the pretrained models are available at https://github.com/google-research/ALBERT.", "pdf": "/pdf/ce1860d9372b46ab2700549349420cc19125d478.pdf", "paperhash": "lan|albert_a_lite_bert_for_selfsupervised_learning_of_language_representations", "code": "https://github.com/google-research/ALBERT", "_bibtex": "@inproceedings{\nLan2020ALBERT:,\ntitle={ALBERT: A Lite BERT for Self-supervised Learning of Language Representations},\nauthor={Zhenzhong Lan and Mingda Chen and Sebastian Goodman and Kevin Gimpel and Piyush Sharma and Radu Soricut},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=H1eA7AEtvS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/dfa54b8bfba46beb0d45f76b449d4e5eb744c546.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "H1eA7AEtvS", "replyto": "H1eA7AEtvS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1057/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1057/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574722376000, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1057/Reviewers"], "noninvitees": [], "tcdate": 1570237742986, "tmdate": 1574723088446, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1057/-/Official_Review"}}}, {"id": "SygC0QIhFB", "original": null, "number": 2, "cdate": 1571738582459, "ddate": null, "tcdate": 1571738582459, "tmdate": 1572972518083, "tddate": null, "forum": "H1eA7AEtvS", "replyto": "H1eA7AEtvS", "invitation": "ICLR.cc/2020/Conference/Paper1057/-/Official_Review", "content": {"rating": "8: Accept", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "Summary: This paper investigates improving upon BERT by reducing complexity in terms of free parameters and memory footprint as well as computation steps. They propose 2 strategies for doing this: 1) Splitting the embedding matrix into two smaller matrices (going from V x A to V x B + B x A where B <<<< A); 2) layer-wise parameter sharing. They also utilize sentence order prediction to help with training. These coupled with a bunch of other choices such as using the lamb optimizer, certain hyperparameters etc help show dramatic empirical gains across the board on a wide variety of NLP/NLU tasks.\n\nPositives: This paper has a dramatic, seemingly statistically significant reduction in error across a wide-variety of tasks. It provides a thorough experimental plan and approaches the few addendums to training (splitting the embedding matrix, the layer-wise parameter sharing, and the sentence order prediction).\n\nConcerns & Questions: There's a lot of experimentation here and a lot of seemingly deliberate choices after seeing empirical results during the research phase. How crucial are the choices of optimizer and other specific hyperparameters? Were there ones you observed that were more brittle than others? Any specific 'reasonable' configurations/settings that caused degenerate solutions?"}, "signatures": ["ICLR.cc/2020/Conference/Paper1057/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1057/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations", "authors": ["Zhenzhong Lan", "Mingda Chen", "Sebastian Goodman", "Kevin Gimpel", "Piyush Sharma", "Radu Soricut"], "authorids": ["lanzhzh@google.com", "mchen@ttic.edu", "seabass@google.com", "kgimpel@ttic.edu", "piyushsharma@google.com", "rsoricut@google.com"], "keywords": ["Natural Language Processing", "BERT", "Representation Learning"], "TL;DR": "A new pretraining method that establishes new state-of-the-art results on the GLUE, RACE, and SQuAD benchmarks while having fewer parameters compared to BERT-large. ", "abstract": "Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations and longer training times. To address these problems,  we present two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT~\\citep{devlin2018bert}. Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the original BERT. We also use a self-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and \\squad benchmarks while having fewer parameters compared to BERT-large. The code and the pretrained models are available at https://github.com/google-research/ALBERT.", "pdf": "/pdf/ce1860d9372b46ab2700549349420cc19125d478.pdf", "paperhash": "lan|albert_a_lite_bert_for_selfsupervised_learning_of_language_representations", "code": "https://github.com/google-research/ALBERT", "_bibtex": "@inproceedings{\nLan2020ALBERT:,\ntitle={ALBERT: A Lite BERT for Self-supervised Learning of Language Representations},\nauthor={Zhenzhong Lan and Mingda Chen and Sebastian Goodman and Kevin Gimpel and Piyush Sharma and Radu Soricut},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=H1eA7AEtvS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/dfa54b8bfba46beb0d45f76b449d4e5eb744c546.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "H1eA7AEtvS", "replyto": "H1eA7AEtvS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1057/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1057/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574722376000, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1057/Reviewers"], "noninvitees": [], "tcdate": 1570237742986, "tmdate": 1574723088446, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1057/-/Official_Review"}}}, {"id": "Hkl2NgFecr", "original": null, "number": 3, "cdate": 1572012083819, "ddate": null, "tcdate": 1572012083819, "tmdate": 1572972518046, "tddate": null, "forum": "H1eA7AEtvS", "replyto": "H1eA7AEtvS", "invitation": "ICLR.cc/2020/Conference/Paper1057/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper proposes a new pre-trained BERT-like model called ALBERT. The contributions are mainly 3-fold: factorized embedding parameterization, cross-layer parameter sharing, and intern-sentence coherence loss. The first two address the issue of model size and memory consumption in BERT; the third corresponds to a new auxiliary task in pre-train, sentence-order prediction (SOP), replacing the next sentence prediction (NSP) task in BERT. These modifications lead to a much leaner model and improved performance. As a result, ALBERT pushes the state of the art on GLUE, RACE, and SQuAD while having fewer parameters than BERT-large. \n\nThis is a well-written paper which is easy to follow even for readers without deep background knowledge. The proposed method is meaningful and effective. Its empirical results are impressive. \n\nOther comments:\n\n- Section 4.9. Why use the all-share condition for state-of-the-art ALBERT results (as indicated in Table 2)? Judging from Table 4 and 5, shouldn't the non-shared condition give better results? The number of parameters would be larger, of course. \n\n- I like the justification/motivation given for replacing NSP with SOP. I wonder if the authors have tried other objectives (but didn't work out). Such negative results are valuable to practitioners.\n\n- Typo in Sec. 4.1: x1,1, x1,2 should be x2,1, x2,2. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1057/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1057/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations", "authors": ["Zhenzhong Lan", "Mingda Chen", "Sebastian Goodman", "Kevin Gimpel", "Piyush Sharma", "Radu Soricut"], "authorids": ["lanzhzh@google.com", "mchen@ttic.edu", "seabass@google.com", "kgimpel@ttic.edu", "piyushsharma@google.com", "rsoricut@google.com"], "keywords": ["Natural Language Processing", "BERT", "Representation Learning"], "TL;DR": "A new pretraining method that establishes new state-of-the-art results on the GLUE, RACE, and SQuAD benchmarks while having fewer parameters compared to BERT-large. ", "abstract": "Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations and longer training times. To address these problems,  we present two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT~\\citep{devlin2018bert}. Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the original BERT. We also use a self-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and \\squad benchmarks while having fewer parameters compared to BERT-large. The code and the pretrained models are available at https://github.com/google-research/ALBERT.", "pdf": "/pdf/ce1860d9372b46ab2700549349420cc19125d478.pdf", "paperhash": "lan|albert_a_lite_bert_for_selfsupervised_learning_of_language_representations", "code": "https://github.com/google-research/ALBERT", "_bibtex": "@inproceedings{\nLan2020ALBERT:,\ntitle={ALBERT: A Lite BERT for Self-supervised Learning of Language Representations},\nauthor={Zhenzhong Lan and Mingda Chen and Sebastian Goodman and Kevin Gimpel and Piyush Sharma and Radu Soricut},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=H1eA7AEtvS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/dfa54b8bfba46beb0d45f76b449d4e5eb744c546.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "H1eA7AEtvS", "replyto": "H1eA7AEtvS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1057/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1057/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574722376000, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1057/Reviewers"], "noninvitees": [], "tcdate": 1570237742986, "tmdate": 1574723088446, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1057/-/Official_Review"}}}, {"id": "B1lKRMYeqH", "original": null, "number": 11, "cdate": 1572012752960, "ddate": null, "tcdate": 1572012752960, "tmdate": 1572012752960, "tddate": null, "forum": "H1eA7AEtvS", "replyto": "SklNxFQ6KB", "invitation": "ICLR.cc/2020/Conference/Paper1057/-/Public_Comment", "content": {"title": "Thanks", "comment": "Thanks, this explains it. I was referring to MLM accuracy for the same number of steps. "}, "signatures": ["~Erhan_Bilal1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Erhan_Bilal1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations", "authors": ["Zhenzhong Lan", "Mingda Chen", "Sebastian Goodman", "Kevin Gimpel", "Piyush Sharma", "Radu Soricut"], "authorids": ["lanzhzh@google.com", "mchen@ttic.edu", "seabass@google.com", "kgimpel@ttic.edu", "piyushsharma@google.com", "rsoricut@google.com"], "keywords": ["Natural Language Processing", "BERT", "Representation Learning"], "TL;DR": "A new pretraining method that establishes new state-of-the-art results on the GLUE, RACE, and SQuAD benchmarks while having fewer parameters compared to BERT-large. ", "abstract": "Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations and longer training times. To address these problems,  we present two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT~\\citep{devlin2018bert}. Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the original BERT. We also use a self-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and \\squad benchmarks while having fewer parameters compared to BERT-large. The code and the pretrained models are available at https://github.com/google-research/ALBERT.", "pdf": "/pdf/ce1860d9372b46ab2700549349420cc19125d478.pdf", "paperhash": "lan|albert_a_lite_bert_for_selfsupervised_learning_of_language_representations", "code": "https://github.com/google-research/ALBERT", "_bibtex": "@inproceedings{\nLan2020ALBERT:,\ntitle={ALBERT: A Lite BERT for Self-supervised Learning of Language Representations},\nauthor={Zhenzhong Lan and Mingda Chen and Sebastian Goodman and Kevin Gimpel and Piyush Sharma and Radu Soricut},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=H1eA7AEtvS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/dfa54b8bfba46beb0d45f76b449d4e5eb744c546.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1eA7AEtvS", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504200267, "tmdate": 1576860591222, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper1057/Authors", "ICLR.cc/2020/Conference/Paper1057/Reviewers", "ICLR.cc/2020/Conference/Paper1057/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1057/-/Public_Comment"}}}, {"id": "BJex-gQg5B", "original": null, "number": 10, "cdate": 1571987448437, "ddate": null, "tcdate": 1571987448437, "tmdate": 1571987448437, "tddate": null, "forum": "H1eA7AEtvS", "replyto": "rJe1YLXptS", "invitation": "ICLR.cc/2020/Conference/Paper1057/-/Public_Comment", "content": {"title": "my email", "comment": "My email is \"ylvwzj@1timl.com\".\nWould you send me the link?\n\nI look forward to hearing from you."}, "signatures": ["~Tomotaka_Sasaki1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Tomotaka_Sasaki1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations", "authors": ["Zhenzhong Lan", "Mingda Chen", "Sebastian Goodman", "Kevin Gimpel", "Piyush Sharma", "Radu Soricut"], "authorids": ["lanzhzh@google.com", "mchen@ttic.edu", "seabass@google.com", "kgimpel@ttic.edu", "piyushsharma@google.com", "rsoricut@google.com"], "keywords": ["Natural Language Processing", "BERT", "Representation Learning"], "TL;DR": "A new pretraining method that establishes new state-of-the-art results on the GLUE, RACE, and SQuAD benchmarks while having fewer parameters compared to BERT-large. ", "abstract": "Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations and longer training times. To address these problems,  we present two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT~\\citep{devlin2018bert}. Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the original BERT. We also use a self-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and \\squad benchmarks while having fewer parameters compared to BERT-large. The code and the pretrained models are available at https://github.com/google-research/ALBERT.", "pdf": "/pdf/ce1860d9372b46ab2700549349420cc19125d478.pdf", "paperhash": "lan|albert_a_lite_bert_for_selfsupervised_learning_of_language_representations", "code": "https://github.com/google-research/ALBERT", "_bibtex": "@inproceedings{\nLan2020ALBERT:,\ntitle={ALBERT: A Lite BERT for Self-supervised Learning of Language Representations},\nauthor={Zhenzhong Lan and Mingda Chen and Sebastian Goodman and Kevin Gimpel and Piyush Sharma and Radu Soricut},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=H1eA7AEtvS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/dfa54b8bfba46beb0d45f76b449d4e5eb744c546.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1eA7AEtvS", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504200267, "tmdate": 1576860591222, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper1057/Authors", "ICLR.cc/2020/Conference/Paper1057/Reviewers", "ICLR.cc/2020/Conference/Paper1057/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1057/-/Public_Comment"}}}, {"id": "SklNxFQ6KB", "original": null, "number": 10, "cdate": 1571793132250, "ddate": null, "tcdate": 1571793132250, "tmdate": 1571793132250, "tddate": null, "forum": "H1eA7AEtvS", "replyto": "SkgS1BMsKB", "invitation": "ICLR.cc/2020/Conference/Paper1057/-/Official_Comment", "content": {"comment": "Hi Erhan, \n\nThank you for your comment. Here are the answers to your questions. \n\n1. Is the large batch size/LAMB combo essential for the performance of Albert? Bert uses a smaller batch size/Adam. At least in my implementation Albert converges slower when pretraining on Wiki.\n\nNot sure how large is the batch size you mentioned, but I tested on the 512/1M step setting from this paper (https://arxiv.org/pdf/1904.00962.pdf) as well and they converge to a roughly similar MLM accuracy.  I also tried to use Adam and the results there are also similar. \n\nGiven that none of our models really converge, I am not sure how you define slower convergence. However, albert-base does perform worse than bert-base, so you will see lower MLM accuracy given the same number of iterations. \n\n2. Are the initial parameters shared between input and output projections? \nYes. There are shared as BERT does. ", "title": "Thanks for your comment"}, "signatures": ["ICLR.cc/2020/Conference/Paper1057/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1057/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations", "authors": ["Zhenzhong Lan", "Mingda Chen", "Sebastian Goodman", "Kevin Gimpel", "Piyush Sharma", "Radu Soricut"], "authorids": ["lanzhzh@google.com", "mchen@ttic.edu", "seabass@google.com", "kgimpel@ttic.edu", "piyushsharma@google.com", "rsoricut@google.com"], "keywords": ["Natural Language Processing", "BERT", "Representation Learning"], "TL;DR": "A new pretraining method that establishes new state-of-the-art results on the GLUE, RACE, and SQuAD benchmarks while having fewer parameters compared to BERT-large. ", "abstract": "Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations and longer training times. To address these problems,  we present two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT~\\citep{devlin2018bert}. Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the original BERT. We also use a self-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and \\squad benchmarks while having fewer parameters compared to BERT-large. The code and the pretrained models are available at https://github.com/google-research/ALBERT.", "pdf": "/pdf/ce1860d9372b46ab2700549349420cc19125d478.pdf", "paperhash": "lan|albert_a_lite_bert_for_selfsupervised_learning_of_language_representations", "code": "https://github.com/google-research/ALBERT", "_bibtex": "@inproceedings{\nLan2020ALBERT:,\ntitle={ALBERT: A Lite BERT for Self-supervised Learning of Language Representations},\nauthor={Zhenzhong Lan and Mingda Chen and Sebastian Goodman and Kevin Gimpel and Piyush Sharma and Radu Soricut},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=H1eA7AEtvS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/dfa54b8bfba46beb0d45f76b449d4e5eb744c546.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1eA7AEtvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1057/Authors", "ICLR.cc/2020/Conference/Paper1057/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1057/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1057/Reviewers", "ICLR.cc/2020/Conference/Paper1057/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1057/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1057/Authors|ICLR.cc/2020/Conference/Paper1057/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504161921, "tmdate": 1576860558114, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1057/Authors", "ICLR.cc/2020/Conference/Paper1057/Reviewers", "ICLR.cc/2020/Conference/Paper1057/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1057/-/Official_Comment"}}}, {"id": "rJe1YLXptS", "original": null, "number": 9, "cdate": 1571792503151, "ddate": null, "tcdate": 1571792503151, "tmdate": 1571792503151, "tddate": null, "forum": "H1eA7AEtvS", "replyto": "HkelX735YH", "invitation": "ICLR.cc/2020/Conference/Paper1057/-/Official_Comment", "content": {"comment": "Hi Tomotaka, \n\nThe code and models are out in the public. But I cannot put the URL here due to the anonymous policy. However, if you put your email here, I can send you the link directly. Another way is to search the code online. ", "title": "the url to the code and models"}, "signatures": ["ICLR.cc/2020/Conference/Paper1057/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1057/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations", "authors": ["Zhenzhong Lan", "Mingda Chen", "Sebastian Goodman", "Kevin Gimpel", "Piyush Sharma", "Radu Soricut"], "authorids": ["lanzhzh@google.com", "mchen@ttic.edu", "seabass@google.com", "kgimpel@ttic.edu", "piyushsharma@google.com", "rsoricut@google.com"], "keywords": ["Natural Language Processing", "BERT", "Representation Learning"], "TL;DR": "A new pretraining method that establishes new state-of-the-art results on the GLUE, RACE, and SQuAD benchmarks while having fewer parameters compared to BERT-large. ", "abstract": "Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations and longer training times. To address these problems,  we present two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT~\\citep{devlin2018bert}. Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the original BERT. We also use a self-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and \\squad benchmarks while having fewer parameters compared to BERT-large. The code and the pretrained models are available at https://github.com/google-research/ALBERT.", "pdf": "/pdf/ce1860d9372b46ab2700549349420cc19125d478.pdf", "paperhash": "lan|albert_a_lite_bert_for_selfsupervised_learning_of_language_representations", "code": "https://github.com/google-research/ALBERT", "_bibtex": "@inproceedings{\nLan2020ALBERT:,\ntitle={ALBERT: A Lite BERT for Self-supervised Learning of Language Representations},\nauthor={Zhenzhong Lan and Mingda Chen and Sebastian Goodman and Kevin Gimpel and Piyush Sharma and Radu Soricut},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=H1eA7AEtvS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/dfa54b8bfba46beb0d45f76b449d4e5eb744c546.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1eA7AEtvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1057/Authors", "ICLR.cc/2020/Conference/Paper1057/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1057/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1057/Reviewers", "ICLR.cc/2020/Conference/Paper1057/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1057/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1057/Authors|ICLR.cc/2020/Conference/Paper1057/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504161921, "tmdate": 1576860558114, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1057/Authors", "ICLR.cc/2020/Conference/Paper1057/Reviewers", "ICLR.cc/2020/Conference/Paper1057/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1057/-/Official_Comment"}}}, {"id": "SkgS1BMsKB", "original": null, "number": 9, "cdate": 1571656924552, "ddate": null, "tcdate": 1571656924552, "tmdate": 1571656924552, "tddate": null, "forum": "H1eA7AEtvS", "replyto": "H1eA7AEtvS", "invitation": "ICLR.cc/2020/Conference/Paper1057/-/Public_Comment", "content": {"comment": "Thanks for sharing your work. I have a couple of questions:\n\n1. Is the large batch size/LAMB combo essential for the performance of Albert? Bert uses a smaller batch size/Adam. At least in my implementation Albert converges slower when pretraining on Wiki.\n2. Are the initial parameters shared between input and output projections? ", "title": "Projection and batch size "}, "signatures": ["~Erhan_Bilal1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Erhan_Bilal1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations", "authors": ["Zhenzhong Lan", "Mingda Chen", "Sebastian Goodman", "Kevin Gimpel", "Piyush Sharma", "Radu Soricut"], "authorids": ["lanzhzh@google.com", "mchen@ttic.edu", "seabass@google.com", "kgimpel@ttic.edu", "piyushsharma@google.com", "rsoricut@google.com"], "keywords": ["Natural Language Processing", "BERT", "Representation Learning"], "TL;DR": "A new pretraining method that establishes new state-of-the-art results on the GLUE, RACE, and SQuAD benchmarks while having fewer parameters compared to BERT-large. ", "abstract": "Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations and longer training times. To address these problems,  we present two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT~\\citep{devlin2018bert}. Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the original BERT. We also use a self-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and \\squad benchmarks while having fewer parameters compared to BERT-large. The code and the pretrained models are available at https://github.com/google-research/ALBERT.", "pdf": "/pdf/ce1860d9372b46ab2700549349420cc19125d478.pdf", "paperhash": "lan|albert_a_lite_bert_for_selfsupervised_learning_of_language_representations", "code": "https://github.com/google-research/ALBERT", "_bibtex": "@inproceedings{\nLan2020ALBERT:,\ntitle={ALBERT: A Lite BERT for Self-supervised Learning of Language Representations},\nauthor={Zhenzhong Lan and Mingda Chen and Sebastian Goodman and Kevin Gimpel and Piyush Sharma and Radu Soricut},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=H1eA7AEtvS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/dfa54b8bfba46beb0d45f76b449d4e5eb744c546.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1eA7AEtvS", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504200267, "tmdate": 1576860591222, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper1057/Authors", "ICLR.cc/2020/Conference/Paper1057/Reviewers", "ICLR.cc/2020/Conference/Paper1057/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1057/-/Public_Comment"}}}, {"id": "HkelX735YH", "original": null, "number": 8, "cdate": 1571631896309, "ddate": null, "tcdate": 1571631896309, "tmdate": 1571631896309, "tddate": null, "forum": "H1eA7AEtvS", "replyto": "Hygq1uOBtS", "invitation": "ICLR.cc/2020/Conference/Paper1057/-/Public_Comment", "content": {"comment": "Thank you for your comment.\nI'm really looking forward to ALBERT coming soon.\n\nWould you tell me the URL when the code and the models are released?\n", "title": "Would you tell me the URL when the code and the models are released?"}, "signatures": ["~Tomotaka_Sasaki1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Tomotaka_Sasaki1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations", "authors": ["Zhenzhong Lan", "Mingda Chen", "Sebastian Goodman", "Kevin Gimpel", "Piyush Sharma", "Radu Soricut"], "authorids": ["lanzhzh@google.com", "mchen@ttic.edu", "seabass@google.com", "kgimpel@ttic.edu", "piyushsharma@google.com", "rsoricut@google.com"], "keywords": ["Natural Language Processing", "BERT", "Representation Learning"], "TL;DR": "A new pretraining method that establishes new state-of-the-art results on the GLUE, RACE, and SQuAD benchmarks while having fewer parameters compared to BERT-large. ", "abstract": "Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations and longer training times. To address these problems,  we present two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT~\\citep{devlin2018bert}. Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the original BERT. We also use a self-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and \\squad benchmarks while having fewer parameters compared to BERT-large. The code and the pretrained models are available at https://github.com/google-research/ALBERT.", "pdf": "/pdf/ce1860d9372b46ab2700549349420cc19125d478.pdf", "paperhash": "lan|albert_a_lite_bert_for_selfsupervised_learning_of_language_representations", "code": "https://github.com/google-research/ALBERT", "_bibtex": "@inproceedings{\nLan2020ALBERT:,\ntitle={ALBERT: A Lite BERT for Self-supervised Learning of Language Representations},\nauthor={Zhenzhong Lan and Mingda Chen and Sebastian Goodman and Kevin Gimpel and Piyush Sharma and Radu Soricut},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=H1eA7AEtvS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/dfa54b8bfba46beb0d45f76b449d4e5eb744c546.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1eA7AEtvS", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504200267, "tmdate": 1576860591222, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper1057/Authors", "ICLR.cc/2020/Conference/Paper1057/Reviewers", "ICLR.cc/2020/Conference/Paper1057/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1057/-/Public_Comment"}}}, {"id": "HkeS39ottH", "original": null, "number": 8, "cdate": 1571564205147, "ddate": null, "tcdate": 1571564205147, "tmdate": 1571564205147, "tddate": null, "forum": "H1eA7AEtvS", "replyto": "H1eA7AEtvS", "invitation": "ICLR.cc/2020/Conference/Paper1057/-/Official_Comment", "content": {"comment": "When we mention \"full network pre-training (Radford et al., 2018; Devlin et al., 2019) has led to a series of breakthroughs in language representation learning\", we miss the following two seminal work: \n1) Dai, Andrew M., and Quoc V. Le. \"Semi-supervised sequence learning.\" Advances in neural information processing systems. 2015. \n2) Howard, Jeremy, and Sebastian Ruder. \"Universal language model fine-tuning for text classification.\" arXiv preprint arXiv:1801.06146 (2018).  \n\nWe apologize to the authors of these two papers and will add them to our citation list in our revised version. Thanks for people who bring this to our attention. ", "title": "Missing two important citations "}, "signatures": ["ICLR.cc/2020/Conference/Paper1057/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1057/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations", "authors": ["Zhenzhong Lan", "Mingda Chen", "Sebastian Goodman", "Kevin Gimpel", "Piyush Sharma", "Radu Soricut"], "authorids": ["lanzhzh@google.com", "mchen@ttic.edu", "seabass@google.com", "kgimpel@ttic.edu", "piyushsharma@google.com", "rsoricut@google.com"], "keywords": ["Natural Language Processing", "BERT", "Representation Learning"], "TL;DR": "A new pretraining method that establishes new state-of-the-art results on the GLUE, RACE, and SQuAD benchmarks while having fewer parameters compared to BERT-large. ", "abstract": "Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations and longer training times. To address these problems,  we present two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT~\\citep{devlin2018bert}. Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the original BERT. We also use a self-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and \\squad benchmarks while having fewer parameters compared to BERT-large. The code and the pretrained models are available at https://github.com/google-research/ALBERT.", "pdf": "/pdf/ce1860d9372b46ab2700549349420cc19125d478.pdf", "paperhash": "lan|albert_a_lite_bert_for_selfsupervised_learning_of_language_representations", "code": "https://github.com/google-research/ALBERT", "_bibtex": "@inproceedings{\nLan2020ALBERT:,\ntitle={ALBERT: A Lite BERT for Self-supervised Learning of Language Representations},\nauthor={Zhenzhong Lan and Mingda Chen and Sebastian Goodman and Kevin Gimpel and Piyush Sharma and Radu Soricut},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=H1eA7AEtvS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/dfa54b8bfba46beb0d45f76b449d4e5eb744c546.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1eA7AEtvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1057/Authors", "ICLR.cc/2020/Conference/Paper1057/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1057/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1057/Reviewers", "ICLR.cc/2020/Conference/Paper1057/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1057/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1057/Authors|ICLR.cc/2020/Conference/Paper1057/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504161921, "tmdate": 1576860558114, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1057/Authors", "ICLR.cc/2020/Conference/Paper1057/Reviewers", "ICLR.cc/2020/Conference/Paper1057/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1057/-/Official_Comment"}}}, {"id": "SyxfFRUKYH", "original": null, "number": 7, "cdate": 1571544697607, "ddate": null, "tcdate": 1571544697607, "tmdate": 1571544697607, "tddate": null, "forum": "H1eA7AEtvS", "replyto": "ryeI2ojwKS", "invitation": "ICLR.cc/2020/Conference/Paper1057/-/Official_Comment", "content": {"comment": "Hi Qingqing, \n\nThanks for your comment and I am glad that you like our work. We haven't throughly tested the inference speed of ALBERT models yet but theoretically speaking, ALBERT won't improve the inference speed.  It is more of an improvement on memory consumption. As stated in the discussion section, we are currently working on improving the inference speed by applying sparse/block attention.", "title": "Thank you for your comment"}, "signatures": ["ICLR.cc/2020/Conference/Paper1057/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1057/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations", "authors": ["Zhenzhong Lan", "Mingda Chen", "Sebastian Goodman", "Kevin Gimpel", "Piyush Sharma", "Radu Soricut"], "authorids": ["lanzhzh@google.com", "mchen@ttic.edu", "seabass@google.com", "kgimpel@ttic.edu", "piyushsharma@google.com", "rsoricut@google.com"], "keywords": ["Natural Language Processing", "BERT", "Representation Learning"], "TL;DR": "A new pretraining method that establishes new state-of-the-art results on the GLUE, RACE, and SQuAD benchmarks while having fewer parameters compared to BERT-large. ", "abstract": "Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations and longer training times. To address these problems,  we present two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT~\\citep{devlin2018bert}. Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the original BERT. We also use a self-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and \\squad benchmarks while having fewer parameters compared to BERT-large. The code and the pretrained models are available at https://github.com/google-research/ALBERT.", "pdf": "/pdf/ce1860d9372b46ab2700549349420cc19125d478.pdf", "paperhash": "lan|albert_a_lite_bert_for_selfsupervised_learning_of_language_representations", "code": "https://github.com/google-research/ALBERT", "_bibtex": "@inproceedings{\nLan2020ALBERT:,\ntitle={ALBERT: A Lite BERT for Self-supervised Learning of Language Representations},\nauthor={Zhenzhong Lan and Mingda Chen and Sebastian Goodman and Kevin Gimpel and Piyush Sharma and Radu Soricut},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=H1eA7AEtvS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/dfa54b8bfba46beb0d45f76b449d4e5eb744c546.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1eA7AEtvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1057/Authors", "ICLR.cc/2020/Conference/Paper1057/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1057/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1057/Reviewers", "ICLR.cc/2020/Conference/Paper1057/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1057/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1057/Authors|ICLR.cc/2020/Conference/Paper1057/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504161921, "tmdate": 1576860558114, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1057/Authors", "ICLR.cc/2020/Conference/Paper1057/Reviewers", "ICLR.cc/2020/Conference/Paper1057/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1057/-/Official_Comment"}}}, {"id": "ryeI2ojwKS", "original": null, "number": 7, "cdate": 1571433390122, "ddate": null, "tcdate": 1571433390122, "tmdate": 1571433390122, "tddate": null, "forum": "H1eA7AEtvS", "replyto": "H1eA7AEtvS", "invitation": "ICLR.cc/2020/Conference/Paper1057/-/Public_Comment", "content": {"comment": "Hi, thanks for your excellent work! I'm curious about how much inference speed benefits ALBERT can bring compared to BERT. The paper seems only discussed the training gains; correct me if I'm wrong. Some inference speedup numbers will be good to know.", "title": "Curious about inference speed over BERT"}, "signatures": ["~Qingqing_Cao1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Qingqing_Cao1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations", "authors": ["Zhenzhong Lan", "Mingda Chen", "Sebastian Goodman", "Kevin Gimpel", "Piyush Sharma", "Radu Soricut"], "authorids": ["lanzhzh@google.com", "mchen@ttic.edu", "seabass@google.com", "kgimpel@ttic.edu", "piyushsharma@google.com", "rsoricut@google.com"], "keywords": ["Natural Language Processing", "BERT", "Representation Learning"], "TL;DR": "A new pretraining method that establishes new state-of-the-art results on the GLUE, RACE, and SQuAD benchmarks while having fewer parameters compared to BERT-large. ", "abstract": "Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations and longer training times. To address these problems,  we present two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT~\\citep{devlin2018bert}. Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the original BERT. We also use a self-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and \\squad benchmarks while having fewer parameters compared to BERT-large. The code and the pretrained models are available at https://github.com/google-research/ALBERT.", "pdf": "/pdf/ce1860d9372b46ab2700549349420cc19125d478.pdf", "paperhash": "lan|albert_a_lite_bert_for_selfsupervised_learning_of_language_representations", "code": "https://github.com/google-research/ALBERT", "_bibtex": "@inproceedings{\nLan2020ALBERT:,\ntitle={ALBERT: A Lite BERT for Self-supervised Learning of Language Representations},\nauthor={Zhenzhong Lan and Mingda Chen and Sebastian Goodman and Kevin Gimpel and Piyush Sharma and Radu Soricut},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=H1eA7AEtvS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/dfa54b8bfba46beb0d45f76b449d4e5eb744c546.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1eA7AEtvS", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504200267, "tmdate": 1576860591222, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper1057/Authors", "ICLR.cc/2020/Conference/Paper1057/Reviewers", "ICLR.cc/2020/Conference/Paper1057/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1057/-/Public_Comment"}}}, {"id": "Hygq1uOBtS", "original": null, "number": 6, "cdate": 1571289057879, "ddate": null, "tcdate": 1571289057879, "tmdate": 1571289057879, "tddate": null, "forum": "H1eA7AEtvS", "replyto": "rJgHaTrStr", "invitation": "ICLR.cc/2020/Conference/Paper1057/-/Official_Comment", "content": {"comment": "Hi Tomotaka, \n\nThanks for your comment. I am glad that you like our work. We are in the process of releasing the code and models. They are expected to be out late this week or early next week. Please stay tuned!", "title": "Thanks for your comment"}, "signatures": ["ICLR.cc/2020/Conference/Paper1057/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1057/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations", "authors": ["Zhenzhong Lan", "Mingda Chen", "Sebastian Goodman", "Kevin Gimpel", "Piyush Sharma", "Radu Soricut"], "authorids": ["lanzhzh@google.com", "mchen@ttic.edu", "seabass@google.com", "kgimpel@ttic.edu", "piyushsharma@google.com", "rsoricut@google.com"], "keywords": ["Natural Language Processing", "BERT", "Representation Learning"], "TL;DR": "A new pretraining method that establishes new state-of-the-art results on the GLUE, RACE, and SQuAD benchmarks while having fewer parameters compared to BERT-large. ", "abstract": "Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations and longer training times. To address these problems,  we present two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT~\\citep{devlin2018bert}. Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the original BERT. We also use a self-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and \\squad benchmarks while having fewer parameters compared to BERT-large. The code and the pretrained models are available at https://github.com/google-research/ALBERT.", "pdf": "/pdf/ce1860d9372b46ab2700549349420cc19125d478.pdf", "paperhash": "lan|albert_a_lite_bert_for_selfsupervised_learning_of_language_representations", "code": "https://github.com/google-research/ALBERT", "_bibtex": "@inproceedings{\nLan2020ALBERT:,\ntitle={ALBERT: A Lite BERT for Self-supervised Learning of Language Representations},\nauthor={Zhenzhong Lan and Mingda Chen and Sebastian Goodman and Kevin Gimpel and Piyush Sharma and Radu Soricut},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=H1eA7AEtvS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/dfa54b8bfba46beb0d45f76b449d4e5eb744c546.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1eA7AEtvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1057/Authors", "ICLR.cc/2020/Conference/Paper1057/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1057/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1057/Reviewers", "ICLR.cc/2020/Conference/Paper1057/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1057/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1057/Authors|ICLR.cc/2020/Conference/Paper1057/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504161921, "tmdate": 1576860558114, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1057/Authors", "ICLR.cc/2020/Conference/Paper1057/Reviewers", "ICLR.cc/2020/Conference/Paper1057/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1057/-/Official_Comment"}}}, {"id": "rJgHaTrStr", "original": null, "number": 6, "cdate": 1571278269129, "ddate": null, "tcdate": 1571278269129, "tmdate": 1571278269129, "tddate": null, "forum": "H1eA7AEtvS", "replyto": "rke69I1GYr", "invitation": "ICLR.cc/2020/Conference/Paper1057/-/Public_Comment", "content": {"comment": "Thanks for this nice paper.\nI'm interested in ALBERT, so I want to use it.\n\nWhen will the code and the models be released?\n\nI'm so looking forward to them.", "title": "When will the code and the models be released?"}, "signatures": ["~Tomotaka_Sasaki1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Tomotaka_Sasaki1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations", "authors": ["Zhenzhong Lan", "Mingda Chen", "Sebastian Goodman", "Kevin Gimpel", "Piyush Sharma", "Radu Soricut"], "authorids": ["lanzhzh@google.com", "mchen@ttic.edu", "seabass@google.com", "kgimpel@ttic.edu", "piyushsharma@google.com", "rsoricut@google.com"], "keywords": ["Natural Language Processing", "BERT", "Representation Learning"], "TL;DR": "A new pretraining method that establishes new state-of-the-art results on the GLUE, RACE, and SQuAD benchmarks while having fewer parameters compared to BERT-large. ", "abstract": "Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations and longer training times. To address these problems,  we present two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT~\\citep{devlin2018bert}. Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the original BERT. We also use a self-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and \\squad benchmarks while having fewer parameters compared to BERT-large. The code and the pretrained models are available at https://github.com/google-research/ALBERT.", "pdf": "/pdf/ce1860d9372b46ab2700549349420cc19125d478.pdf", "paperhash": "lan|albert_a_lite_bert_for_selfsupervised_learning_of_language_representations", "code": "https://github.com/google-research/ALBERT", "_bibtex": "@inproceedings{\nLan2020ALBERT:,\ntitle={ALBERT: A Lite BERT for Self-supervised Learning of Language Representations},\nauthor={Zhenzhong Lan and Mingda Chen and Sebastian Goodman and Kevin Gimpel and Piyush Sharma and Radu Soricut},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=H1eA7AEtvS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/dfa54b8bfba46beb0d45f76b449d4e5eb744c546.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1eA7AEtvS", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504200267, "tmdate": 1576860591222, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper1057/Authors", "ICLR.cc/2020/Conference/Paper1057/Reviewers", "ICLR.cc/2020/Conference/Paper1057/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1057/-/Public_Comment"}}}, {"id": "BJej4IRGtB", "original": null, "number": 5, "cdate": 1571116595440, "ddate": null, "tcdate": 1571116595440, "tmdate": 1571116595440, "tddate": null, "forum": "H1eA7AEtvS", "replyto": "rylDeWCGKH", "invitation": "ICLR.cc/2020/Conference/Paper1057/-/Official_Comment", "content": {"comment": "Hi Shuailiang, \n\nThanks for your comment. Will correct this typo in next version. ", "title": "Thanks for your comment "}, "signatures": ["ICLR.cc/2020/Conference/Paper1057/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1057/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations", "authors": ["Zhenzhong Lan", "Mingda Chen", "Sebastian Goodman", "Kevin Gimpel", "Piyush Sharma", "Radu Soricut"], "authorids": ["lanzhzh@google.com", "mchen@ttic.edu", "seabass@google.com", "kgimpel@ttic.edu", "piyushsharma@google.com", "rsoricut@google.com"], "keywords": ["Natural Language Processing", "BERT", "Representation Learning"], "TL;DR": "A new pretraining method that establishes new state-of-the-art results on the GLUE, RACE, and SQuAD benchmarks while having fewer parameters compared to BERT-large. ", "abstract": "Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations and longer training times. To address these problems,  we present two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT~\\citep{devlin2018bert}. Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the original BERT. We also use a self-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and \\squad benchmarks while having fewer parameters compared to BERT-large. The code and the pretrained models are available at https://github.com/google-research/ALBERT.", "pdf": "/pdf/ce1860d9372b46ab2700549349420cc19125d478.pdf", "paperhash": "lan|albert_a_lite_bert_for_selfsupervised_learning_of_language_representations", "code": "https://github.com/google-research/ALBERT", "_bibtex": "@inproceedings{\nLan2020ALBERT:,\ntitle={ALBERT: A Lite BERT for Self-supervised Learning of Language Representations},\nauthor={Zhenzhong Lan and Mingda Chen and Sebastian Goodman and Kevin Gimpel and Piyush Sharma and Radu Soricut},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=H1eA7AEtvS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/dfa54b8bfba46beb0d45f76b449d4e5eb744c546.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1eA7AEtvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1057/Authors", "ICLR.cc/2020/Conference/Paper1057/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1057/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1057/Reviewers", "ICLR.cc/2020/Conference/Paper1057/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1057/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1057/Authors|ICLR.cc/2020/Conference/Paper1057/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504161921, "tmdate": 1576860558114, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1057/Authors", "ICLR.cc/2020/Conference/Paper1057/Reviewers", "ICLR.cc/2020/Conference/Paper1057/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1057/-/Official_Comment"}}}, {"id": "rylDeWCGKH", "original": null, "number": 5, "cdate": 1571115246658, "ddate": null, "tcdate": 1571115246658, "tmdate": 1571115246658, "tddate": null, "forum": "H1eA7AEtvS", "replyto": "H1eA7AEtvS", "invitation": "ICLR.cc/2020/Conference/Paper1057/-/Public_Comment", "content": {"comment": "The model name DCMI should be DCMN in Table 11.", "title": "It is DCMN, not DCMI in Table 11."}, "signatures": ["~Shuailiang_Zhang1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Shuailiang_Zhang1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations", "authors": ["Zhenzhong Lan", "Mingda Chen", "Sebastian Goodman", "Kevin Gimpel", "Piyush Sharma", "Radu Soricut"], "authorids": ["lanzhzh@google.com", "mchen@ttic.edu", "seabass@google.com", "kgimpel@ttic.edu", "piyushsharma@google.com", "rsoricut@google.com"], "keywords": ["Natural Language Processing", "BERT", "Representation Learning"], "TL;DR": "A new pretraining method that establishes new state-of-the-art results on the GLUE, RACE, and SQuAD benchmarks while having fewer parameters compared to BERT-large. ", "abstract": "Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations and longer training times. To address these problems,  we present two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT~\\citep{devlin2018bert}. Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the original BERT. We also use a self-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and \\squad benchmarks while having fewer parameters compared to BERT-large. The code and the pretrained models are available at https://github.com/google-research/ALBERT.", "pdf": "/pdf/ce1860d9372b46ab2700549349420cc19125d478.pdf", "paperhash": "lan|albert_a_lite_bert_for_selfsupervised_learning_of_language_representations", "code": "https://github.com/google-research/ALBERT", "_bibtex": "@inproceedings{\nLan2020ALBERT:,\ntitle={ALBERT: A Lite BERT for Self-supervised Learning of Language Representations},\nauthor={Zhenzhong Lan and Mingda Chen and Sebastian Goodman and Kevin Gimpel and Piyush Sharma and Radu Soricut},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=H1eA7AEtvS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/dfa54b8bfba46beb0d45f76b449d4e5eb744c546.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1eA7AEtvS", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504200267, "tmdate": 1576860591222, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper1057/Authors", "ICLR.cc/2020/Conference/Paper1057/Reviewers", "ICLR.cc/2020/Conference/Paper1057/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1057/-/Public_Comment"}}}, {"id": "rke69I1GYr", "original": null, "number": 4, "cdate": 1571055252832, "ddate": null, "tcdate": 1571055252832, "tmdate": 1571055252832, "tddate": null, "forum": "H1eA7AEtvS", "replyto": "Bke6BA0WFB", "invitation": "ICLR.cc/2020/Conference/Paper1057/-/Official_Comment", "content": {"comment": "Hi Chen, \n\nThanks for your comment. I am glad that you find our work interesting. \n\nWe are working on releasing the code as well as the models. As the same time, please let us know if you have any question about where the improvements come from. We will try out best to make it clear to you. ", "title": "Thank you for your comments"}, "signatures": ["ICLR.cc/2020/Conference/Paper1057/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1057/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations", "authors": ["Zhenzhong Lan", "Mingda Chen", "Sebastian Goodman", "Kevin Gimpel", "Piyush Sharma", "Radu Soricut"], "authorids": ["lanzhzh@google.com", "mchen@ttic.edu", "seabass@google.com", "kgimpel@ttic.edu", "piyushsharma@google.com", "rsoricut@google.com"], "keywords": ["Natural Language Processing", "BERT", "Representation Learning"], "TL;DR": "A new pretraining method that establishes new state-of-the-art results on the GLUE, RACE, and SQuAD benchmarks while having fewer parameters compared to BERT-large. ", "abstract": "Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations and longer training times. To address these problems,  we present two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT~\\citep{devlin2018bert}. Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the original BERT. We also use a self-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and \\squad benchmarks while having fewer parameters compared to BERT-large. The code and the pretrained models are available at https://github.com/google-research/ALBERT.", "pdf": "/pdf/ce1860d9372b46ab2700549349420cc19125d478.pdf", "paperhash": "lan|albert_a_lite_bert_for_selfsupervised_learning_of_language_representations", "code": "https://github.com/google-research/ALBERT", "_bibtex": "@inproceedings{\nLan2020ALBERT:,\ntitle={ALBERT: A Lite BERT for Self-supervised Learning of Language Representations},\nauthor={Zhenzhong Lan and Mingda Chen and Sebastian Goodman and Kevin Gimpel and Piyush Sharma and Radu Soricut},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=H1eA7AEtvS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/dfa54b8bfba46beb0d45f76b449d4e5eb744c546.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1eA7AEtvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1057/Authors", "ICLR.cc/2020/Conference/Paper1057/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1057/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1057/Reviewers", "ICLR.cc/2020/Conference/Paper1057/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1057/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1057/Authors|ICLR.cc/2020/Conference/Paper1057/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504161921, "tmdate": 1576860558114, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1057/Authors", "ICLR.cc/2020/Conference/Paper1057/Reviewers", "ICLR.cc/2020/Conference/Paper1057/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1057/-/Official_Comment"}}}, {"id": "Bke6BA0WFB", "original": null, "number": 4, "cdate": 1571053124592, "ddate": null, "tcdate": 1571053124592, "tmdate": 1571053124592, "tddate": null, "forum": "H1eA7AEtvS", "replyto": "H1eA7AEtvS", "invitation": "ICLR.cc/2020/Conference/Paper1057/-/Public_Comment", "content": {"comment": "I read this nice paper and interested in it. But it seems like no official code of ALBERT on Github. And there\u2019re only some unofficial implementation like https://github.com/brightmart/albert_zh\n(which is trained on Chinese vocab)\n\nWhere can I find official implementation and pre-trained  models of ALBERT(as in paper)? It may help me to understand the improvement points of ALBERT.", "title": "Official implementation and pre-trained ALBERT?"}, "signatures": ["~Chen_Yan1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Chen_Yan1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations", "authors": ["Zhenzhong Lan", "Mingda Chen", "Sebastian Goodman", "Kevin Gimpel", "Piyush Sharma", "Radu Soricut"], "authorids": ["lanzhzh@google.com", "mchen@ttic.edu", "seabass@google.com", "kgimpel@ttic.edu", "piyushsharma@google.com", "rsoricut@google.com"], "keywords": ["Natural Language Processing", "BERT", "Representation Learning"], "TL;DR": "A new pretraining method that establishes new state-of-the-art results on the GLUE, RACE, and SQuAD benchmarks while having fewer parameters compared to BERT-large. ", "abstract": "Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations and longer training times. To address these problems,  we present two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT~\\citep{devlin2018bert}. Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the original BERT. We also use a self-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and \\squad benchmarks while having fewer parameters compared to BERT-large. The code and the pretrained models are available at https://github.com/google-research/ALBERT.", "pdf": "/pdf/ce1860d9372b46ab2700549349420cc19125d478.pdf", "paperhash": "lan|albert_a_lite_bert_for_selfsupervised_learning_of_language_representations", "code": "https://github.com/google-research/ALBERT", "_bibtex": "@inproceedings{\nLan2020ALBERT:,\ntitle={ALBERT: A Lite BERT for Self-supervised Learning of Language Representations},\nauthor={Zhenzhong Lan and Mingda Chen and Sebastian Goodman and Kevin Gimpel and Piyush Sharma and Radu Soricut},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=H1eA7AEtvS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/dfa54b8bfba46beb0d45f76b449d4e5eb744c546.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1eA7AEtvS", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504200267, "tmdate": 1576860591222, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper1057/Authors", "ICLR.cc/2020/Conference/Paper1057/Reviewers", "ICLR.cc/2020/Conference/Paper1057/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1057/-/Public_Comment"}}}, {"id": "rkedPXbJYB", "original": null, "number": 3, "cdate": 1570866016257, "ddate": null, "tcdate": 1570866016257, "tmdate": 1570866016257, "tddate": null, "forum": "H1eA7AEtvS", "replyto": "Syx5NANRuH", "invitation": "ICLR.cc/2020/Conference/Paper1057/-/Official_Comment", "content": {"comment": "Hi Brandon, \n\nThank you so much for helping us to check the parameter counts! your numbers are correct. The ALBERT-xxlarge discrepancy is due to a typo. We meant to put in 223M as in Table 14  (appendix) instead of 233M. We will update the ALBERT-xxlarge count to be 225M and ALBERT-xlarge count to be 60M. ", "title": "Thanks for the comment"}, "signatures": ["ICLR.cc/2020/Conference/Paper1057/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1057/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations", "authors": ["Zhenzhong Lan", "Mingda Chen", "Sebastian Goodman", "Kevin Gimpel", "Piyush Sharma", "Radu Soricut"], "authorids": ["lanzhzh@google.com", "mchen@ttic.edu", "seabass@google.com", "kgimpel@ttic.edu", "piyushsharma@google.com", "rsoricut@google.com"], "keywords": ["Natural Language Processing", "BERT", "Representation Learning"], "TL;DR": "A new pretraining method that establishes new state-of-the-art results on the GLUE, RACE, and SQuAD benchmarks while having fewer parameters compared to BERT-large. ", "abstract": "Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations and longer training times. To address these problems,  we present two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT~\\citep{devlin2018bert}. Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the original BERT. We also use a self-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and \\squad benchmarks while having fewer parameters compared to BERT-large. The code and the pretrained models are available at https://github.com/google-research/ALBERT.", "pdf": "/pdf/ce1860d9372b46ab2700549349420cc19125d478.pdf", "paperhash": "lan|albert_a_lite_bert_for_selfsupervised_learning_of_language_representations", "code": "https://github.com/google-research/ALBERT", "_bibtex": "@inproceedings{\nLan2020ALBERT:,\ntitle={ALBERT: A Lite BERT for Self-supervised Learning of Language Representations},\nauthor={Zhenzhong Lan and Mingda Chen and Sebastian Goodman and Kevin Gimpel and Piyush Sharma and Radu Soricut},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=H1eA7AEtvS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/dfa54b8bfba46beb0d45f76b449d4e5eb744c546.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1eA7AEtvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1057/Authors", "ICLR.cc/2020/Conference/Paper1057/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1057/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1057/Reviewers", "ICLR.cc/2020/Conference/Paper1057/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1057/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1057/Authors|ICLR.cc/2020/Conference/Paper1057/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504161921, "tmdate": 1576860558114, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1057/Authors", "ICLR.cc/2020/Conference/Paper1057/Reviewers", "ICLR.cc/2020/Conference/Paper1057/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1057/-/Official_Comment"}}}, {"id": "Syx5NANRuH", "original": null, "number": 3, "cdate": 1570815537849, "ddate": null, "tcdate": 1570815537849, "tmdate": 1570830769632, "tddate": null, "forum": "H1eA7AEtvS", "replyto": "H1eA7AEtvS", "invitation": "ICLR.cc/2020/Conference/Paper1057/-/Public_Comment", "content": {"comment": "I was investigating the parameter counts across the various ALBERT variants. It seems that 8M parameters appear in the xxlarge model that are unaccounted for.\n\nWhen calculating the number of parameters, the only differences from a single layer BERT model (with appropriate transformer dimensions) should be the change in embedding dimension and the additional embedding projection. This yields the following formula: $numparams = v*e + e*h + s*h + 2*h + h + h + h*h + h + h*h + h + h*h + h + h*h + h + h + h + h*4*h + h*4 + h * h*4 + h + h + h + h*h + h$, where $v$ is vocab size, $e$ is embedding size, $s$ is maximum sequence length, and $h$ is hidden size. I have used $v=30000$ and $s=512$, then varied the hidden and embedding sizes.\n\nFor the three smaller variants (base, large, xlarge) the parameter count matches the formula above, but for xxlarge there are about 8M missing parameters.\n\n$$\n\\begin{align*}\n\\textrm{ALBERT}&\\textrm{-base} &=& \\enspace 12,013,056 \\\\\n\\textrm{ALBERT}&\\textrm{-large} &=& \\enspace 18,145,280 \\\\\n\\textrm{ALBERT}&\\textrm{-xlarge} &=& \\enspace 59,713,536 \\\\\n\\textrm{ALBERT}&\\textrm{-xxlarge} &=& \\enspace 224,638,976\n\\end{align*}\n$$\nCan the authors explain this discrepancy?", "title": "Where are the parameters?"}, "signatures": ["~Brandon_Norick1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Brandon_Norick1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations", "authors": ["Zhenzhong Lan", "Mingda Chen", "Sebastian Goodman", "Kevin Gimpel", "Piyush Sharma", "Radu Soricut"], "authorids": ["lanzhzh@google.com", "mchen@ttic.edu", "seabass@google.com", "kgimpel@ttic.edu", "piyushsharma@google.com", "rsoricut@google.com"], "keywords": ["Natural Language Processing", "BERT", "Representation Learning"], "TL;DR": "A new pretraining method that establishes new state-of-the-art results on the GLUE, RACE, and SQuAD benchmarks while having fewer parameters compared to BERT-large. ", "abstract": "Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations and longer training times. To address these problems,  we present two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT~\\citep{devlin2018bert}. Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the original BERT. We also use a self-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and \\squad benchmarks while having fewer parameters compared to BERT-large. The code and the pretrained models are available at https://github.com/google-research/ALBERT.", "pdf": "/pdf/ce1860d9372b46ab2700549349420cc19125d478.pdf", "paperhash": "lan|albert_a_lite_bert_for_selfsupervised_learning_of_language_representations", "code": "https://github.com/google-research/ALBERT", "_bibtex": "@inproceedings{\nLan2020ALBERT:,\ntitle={ALBERT: A Lite BERT for Self-supervised Learning of Language Representations},\nauthor={Zhenzhong Lan and Mingda Chen and Sebastian Goodman and Kevin Gimpel and Piyush Sharma and Radu Soricut},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=H1eA7AEtvS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/dfa54b8bfba46beb0d45f76b449d4e5eb744c546.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1eA7AEtvS", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504200267, "tmdate": 1576860591222, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper1057/Authors", "ICLR.cc/2020/Conference/Paper1057/Reviewers", "ICLR.cc/2020/Conference/Paper1057/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1057/-/Public_Comment"}}}, {"id": "rJl4bouVOr", "original": null, "number": 1, "cdate": 1570175740388, "ddate": null, "tcdate": 1570175740388, "tmdate": 1570175916665, "tddate": null, "forum": "H1eA7AEtvS", "replyto": "HkxHtfRyOB", "invitation": "ICLR.cc/2020/Conference/Paper1057/-/Official_Comment", "content": {"comment": "Hi Sachin, \nThank you for your comments, I am glad that you enjoy our paper. \nAlthough Baevski and Auli [r1], Dai et al. [r2], and Grave et al. [r3] also try to address the large vocabulary problem, they have different settings from the one in our paper for the following reasons:\n1) Different problem settings:\n[r1][r2][r3] use whole words embeddings as they main setting while we use subword embeddings. Because of this difference, their vocabulary sizes are much larger than the one we have. They have a problem of how to learn good embeddings for those words that rarely occur while this problem is less of an issue for us. Our main problem is the large memory consumption because of the large number of parameters. \n2) Different main ideas of solutions:\nBecause of the different problems we face, we use different solutions. [r1] and [r2] have adaptive embedding size to make it easier for infrequent words to learn embeddings. We use same but smaller embedding size for all our subwords. One evidence that illustrates the differences between these two solutions is that In [r1], they have a comparison with subword embeddings, where they still tie the embedding size and hidden size. \n3) Different experimental settings: \n[r1][r2][r3] focused on standard language modeling tasks rather than the pretraining/finetuning setting we have. \nNonetheless, It is helpful to illustrate our idea by comparing embedding factorization methods with adaptive softmax [r2] [r3] and embeddings [r1][r2]. And we agree that these relevant papers are worth citing and including in our paper. Thanks for your useful suggestions!\n", "title": "Thank you for your comment"}, "signatures": ["ICLR.cc/2020/Conference/Paper1057/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1057/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations", "authors": ["Zhenzhong Lan", "Mingda Chen", "Sebastian Goodman", "Kevin Gimpel", "Piyush Sharma", "Radu Soricut"], "authorids": ["lanzhzh@google.com", "mchen@ttic.edu", "seabass@google.com", "kgimpel@ttic.edu", "piyushsharma@google.com", "rsoricut@google.com"], "keywords": ["Natural Language Processing", "BERT", "Representation Learning"], "TL;DR": "A new pretraining method that establishes new state-of-the-art results on the GLUE, RACE, and SQuAD benchmarks while having fewer parameters compared to BERT-large. ", "abstract": "Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations and longer training times. To address these problems,  we present two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT~\\citep{devlin2018bert}. Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the original BERT. We also use a self-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and \\squad benchmarks while having fewer parameters compared to BERT-large. The code and the pretrained models are available at https://github.com/google-research/ALBERT.", "pdf": "/pdf/ce1860d9372b46ab2700549349420cc19125d478.pdf", "paperhash": "lan|albert_a_lite_bert_for_selfsupervised_learning_of_language_representations", "code": "https://github.com/google-research/ALBERT", "_bibtex": "@inproceedings{\nLan2020ALBERT:,\ntitle={ALBERT: A Lite BERT for Self-supervised Learning of Language Representations},\nauthor={Zhenzhong Lan and Mingda Chen and Sebastian Goodman and Kevin Gimpel and Piyush Sharma and Radu Soricut},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=H1eA7AEtvS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/dfa54b8bfba46beb0d45f76b449d4e5eb744c546.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1eA7AEtvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1057/Authors", "ICLR.cc/2020/Conference/Paper1057/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1057/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1057/Reviewers", "ICLR.cc/2020/Conference/Paper1057/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1057/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1057/Authors|ICLR.cc/2020/Conference/Paper1057/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504161921, "tmdate": 1576860558114, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1057/Authors", "ICLR.cc/2020/Conference/Paper1057/Reviewers", "ICLR.cc/2020/Conference/Paper1057/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1057/-/Official_Comment"}}}, {"id": "S1lYooO4dS", "original": null, "number": 2, "cdate": 1570175905434, "ddate": null, "tcdate": 1570175905434, "tmdate": 1570175905434, "tddate": null, "forum": "H1eA7AEtvS", "replyto": "B1gxrqAsPr", "invitation": "ICLR.cc/2020/Conference/Paper1057/-/Official_Comment", "content": {"comment": "Hi Emma, \n\nThanks for your comment. I am glad that you like our work. \n\nFor the cross-layer parameters sharing technique in the final results, we use all-shared technique as stated in section 3.1: \"The default decision for ALBERT is to share all parameters across layers.\" We will make it more explicit by adding the following sentence: \"all our experiments use this default decision unless otherwise specified\".\n\nRegarding speedup comparisons: fair enough. The reason we treated BERT-xlarge as a Speedup of 1X in Table 3 is because it's the slowest. To compare to BERT-large instead, one can divide all Speedup numbers by 3.8. We also have direct speed comparison between ALBERT-xxlarge and BERT-large in Section 4.7 and Section 5. However, we take your point that the BERT-large Speedup numbers are more useful for the community and will modify this in the next version.  \n\nFor the overgeneralization problem in the sentence \u201cdropout can hurt performance in large Transformer-based models\u201d, we will make it clear by adding this sentence: \u201cHowever, the underlying network structure of ALBERT is a special case of the transformer and further experimentation is needed to see if this phenomenon appears with other transformer-based architectures or not.\u201d\n", "title": "Thank you for your comment"}, "signatures": ["ICLR.cc/2020/Conference/Paper1057/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1057/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations", "authors": ["Zhenzhong Lan", "Mingda Chen", "Sebastian Goodman", "Kevin Gimpel", "Piyush Sharma", "Radu Soricut"], "authorids": ["lanzhzh@google.com", "mchen@ttic.edu", "seabass@google.com", "kgimpel@ttic.edu", "piyushsharma@google.com", "rsoricut@google.com"], "keywords": ["Natural Language Processing", "BERT", "Representation Learning"], "TL;DR": "A new pretraining method that establishes new state-of-the-art results on the GLUE, RACE, and SQuAD benchmarks while having fewer parameters compared to BERT-large. ", "abstract": "Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations and longer training times. To address these problems,  we present two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT~\\citep{devlin2018bert}. Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the original BERT. We also use a self-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and \\squad benchmarks while having fewer parameters compared to BERT-large. The code and the pretrained models are available at https://github.com/google-research/ALBERT.", "pdf": "/pdf/ce1860d9372b46ab2700549349420cc19125d478.pdf", "paperhash": "lan|albert_a_lite_bert_for_selfsupervised_learning_of_language_representations", "code": "https://github.com/google-research/ALBERT", "_bibtex": "@inproceedings{\nLan2020ALBERT:,\ntitle={ALBERT: A Lite BERT for Self-supervised Learning of Language Representations},\nauthor={Zhenzhong Lan and Mingda Chen and Sebastian Goodman and Kevin Gimpel and Piyush Sharma and Radu Soricut},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=H1eA7AEtvS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/dfa54b8bfba46beb0d45f76b449d4e5eb744c546.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1eA7AEtvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1057/Authors", "ICLR.cc/2020/Conference/Paper1057/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1057/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1057/Reviewers", "ICLR.cc/2020/Conference/Paper1057/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1057/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1057/Authors|ICLR.cc/2020/Conference/Paper1057/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504161921, "tmdate": 1576860558114, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1057/Authors", "ICLR.cc/2020/Conference/Paper1057/Reviewers", "ICLR.cc/2020/Conference/Paper1057/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1057/-/Official_Comment"}}}, {"id": "HkxHtfRyOB", "original": null, "number": 2, "cdate": 1569870461079, "ddate": null, "tcdate": 1569870461079, "tmdate": 1569870461079, "tddate": null, "forum": "H1eA7AEtvS", "replyto": "H1eA7AEtvS", "invitation": "ICLR.cc/2020/Conference/Paper1057/-/Public_Comment", "content": {"comment": "Enjoyed reading the paper! Interesting results!\n\nI had a question about embedding factorization. It looks like the proposed embedding factorization technique has been proposed in previous related work [r1, r2] for similar computational benefits. It would be great if authors can describe the differences (along with citing these highly relevant papers) of the proposed embedding factorization technique with respect to existing methods. To me, the factorization approach introduced in this paper is a special case of [r1] where the number of clusters is equal to one and has the same size as vocabulary. Also, this factorization looks the same as the one used in [r2] for their base model. See code here:\n\nhttps://github.com/kimiyoung/transformer-xl/blob/44781ed21dbaec88b280f74d9ae2877f52b492a5/pytorch/mem_transformer.py#L452\n\n[r1] Baevski, Alexei, and Michael Auli. \"Adaptive input representations for neural language modeling.\" arXiv preprint arXiv:1809.10853 (2018).\n[r2] Dai, Zihang, et al. \"Transformer-xl: Attentive language models beyond a fixed-length context.\" arXiv preprint arXiv:1901.02860 (2019).\n[r3] Grave, Edouard, et al. \"Efficient softmax approximation for GPUs.\" Proceedings of the 34th International Conference on Machine Learning-Volume 70. JMLR. org, 2017.", "title": "Comparison with existing embedding factorization methods"}, "signatures": ["~Sachin_Mehta1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Sachin_Mehta1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations", "authors": ["Zhenzhong Lan", "Mingda Chen", "Sebastian Goodman", "Kevin Gimpel", "Piyush Sharma", "Radu Soricut"], "authorids": ["lanzhzh@google.com", "mchen@ttic.edu", "seabass@google.com", "kgimpel@ttic.edu", "piyushsharma@google.com", "rsoricut@google.com"], "keywords": ["Natural Language Processing", "BERT", "Representation Learning"], "TL;DR": "A new pretraining method that establishes new state-of-the-art results on the GLUE, RACE, and SQuAD benchmarks while having fewer parameters compared to BERT-large. ", "abstract": "Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations and longer training times. To address these problems,  we present two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT~\\citep{devlin2018bert}. Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the original BERT. We also use a self-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and \\squad benchmarks while having fewer parameters compared to BERT-large. The code and the pretrained models are available at https://github.com/google-research/ALBERT.", "pdf": "/pdf/ce1860d9372b46ab2700549349420cc19125d478.pdf", "paperhash": "lan|albert_a_lite_bert_for_selfsupervised_learning_of_language_representations", "code": "https://github.com/google-research/ALBERT", "_bibtex": "@inproceedings{\nLan2020ALBERT:,\ntitle={ALBERT: A Lite BERT for Self-supervised Learning of Language Representations},\nauthor={Zhenzhong Lan and Mingda Chen and Sebastian Goodman and Kevin Gimpel and Piyush Sharma and Radu Soricut},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=H1eA7AEtvS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/dfa54b8bfba46beb0d45f76b449d4e5eb744c546.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1eA7AEtvS", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504200267, "tmdate": 1576860591222, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper1057/Authors", "ICLR.cc/2020/Conference/Paper1057/Reviewers", "ICLR.cc/2020/Conference/Paper1057/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1057/-/Public_Comment"}}}, {"id": "B1gxrqAsPr", "original": null, "number": 1, "cdate": 1569610296422, "ddate": null, "tcdate": 1569610296422, "tmdate": 1569610296422, "tddate": null, "forum": "H1eA7AEtvS", "replyto": "H1eA7AEtvS", "invitation": "ICLR.cc/2020/Conference/Paper1057/-/Public_Comment", "content": {"comment": "Thanks for this nice paper, important work! It wasn't clear to me which cross-layer parameters are shared in the final results, it would be great if you could update the paper to make that more clear (perhaps in section 3.1). \n\nIn Table 3 I think you should compute speed-up relative to BERT-large, the model that performs well and is actually used, rather than BERT-xlarge, which is not used. This would provide a more practical sense of how your model performs compared to the existing state-of-the-art, rather than a sort of artificially inflated benchmark :)\n\nI also think you should be careful about generalizing your results on removing dropout to large Transformer-based architectures more generally. The cross-layer parameter sharing is a form of regularization that could negate the need for dropout in your architecture/training setup, but this sharing is not typical, and so it remains unclear whether dropout is needed in large Transformers.\n", "title": "compare speed to BERT-large not BERT-xlarge, and a comment on dropout"}, "signatures": ["~Emma_Strubell1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Emma_Strubell1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations", "authors": ["Zhenzhong Lan", "Mingda Chen", "Sebastian Goodman", "Kevin Gimpel", "Piyush Sharma", "Radu Soricut"], "authorids": ["lanzhzh@google.com", "mchen@ttic.edu", "seabass@google.com", "kgimpel@ttic.edu", "piyushsharma@google.com", "rsoricut@google.com"], "keywords": ["Natural Language Processing", "BERT", "Representation Learning"], "TL;DR": "A new pretraining method that establishes new state-of-the-art results on the GLUE, RACE, and SQuAD benchmarks while having fewer parameters compared to BERT-large. ", "abstract": "Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations and longer training times. To address these problems,  we present two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT~\\citep{devlin2018bert}. Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the original BERT. We also use a self-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and \\squad benchmarks while having fewer parameters compared to BERT-large. The code and the pretrained models are available at https://github.com/google-research/ALBERT.", "pdf": "/pdf/ce1860d9372b46ab2700549349420cc19125d478.pdf", "paperhash": "lan|albert_a_lite_bert_for_selfsupervised_learning_of_language_representations", "code": "https://github.com/google-research/ALBERT", "_bibtex": "@inproceedings{\nLan2020ALBERT:,\ntitle={ALBERT: A Lite BERT for Self-supervised Learning of Language Representations},\nauthor={Zhenzhong Lan and Mingda Chen and Sebastian Goodman and Kevin Gimpel and Piyush Sharma and Radu Soricut},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=H1eA7AEtvS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/dfa54b8bfba46beb0d45f76b449d4e5eb744c546.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1eA7AEtvS", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504200267, "tmdate": 1576860591222, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper1057/Authors", "ICLR.cc/2020/Conference/Paper1057/Reviewers", "ICLR.cc/2020/Conference/Paper1057/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1057/-/Public_Comment"}}}], "count": 33}