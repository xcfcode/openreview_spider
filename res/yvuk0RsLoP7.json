{"notes": [{"id": "yvuk0RsLoP7", "original": "qQxreTlt79w", "number": 3224, "cdate": 1601308358224, "ddate": null, "tcdate": 1601308358224, "tmdate": 1614985774544, "tddate": null, "forum": "yvuk0RsLoP7", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Improving Model Robustness with Latent Distribution Locally and Globally", "authorids": ["~Zhuang_QIAN1", "~Shufei_Zhang1", "~Kaizhu_Huang1", "~Qiufeng_Wang2", "~Rui_Zhang10", "xinping.yi@liverpool.ac.uk"], "authors": ["Zhuang QIAN", "Shufei Zhang", "Kaizhu Huang", "Qiufeng Wang", "Rui Zhang", "Xinping Yi"], "keywords": ["adversarial example", "robustness", "data manifold", "adversarial training"], "abstract": "We propose a novel adversarial training method which leverages both the local and global information to defend  adversarial attacks. Existing adversarial training methods usually generate adversarial perturbations  locally in a supervised manner and fail to consider the data manifold information in a global way. Consequently, the resulting  adversarial examples may corrupt the underlying data structure and are typically biased towards the decision boundary. In this work, we exploit both the local and global information of data manifold to generate adversarial examples in an unsupervised manner. Specifically, we design our novel framework  via an adversarial game between  a discriminator and a classifier: the discriminator is  learned to differentiate the latent distributions  of the natural data and the perturbed counterpart, while the classifier is trained to recognize accurately the perturbed examples as well as enforcing the invariance between the two latent distributions. We conduct a series of analysis on the model robustness and also verify the effectiveness of our proposed method empirically. Experimental results show that our method  substantially outperforms the recent state-of-the-art (i.e. Feature Scattering) in defending adversarial attacks  by a large accuracy margin  (e.g. $17.0\\%$ and $18.1\\%$ on SVHN dataset, $9.3\\%$ and $17.4\\%$ on CIFAR-10 dataset, $6.0\\%$ and $16.2\\%$ on CIFAR-100 dataset for defending PGD20 and CW20 attacks respectively).", "one-sentence_summary": "We propose a novel adversarial training method which leverages both the local and global information to defend  adversarial attacks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "qian|improving_model_robustness_with_latent_distribution_locally_and_globally", "supplementary_material": "/attachment/da9cf6d020cff31017c09ffce30d2883e395d6ba.zip", "pdf": "/pdf/4648d81b527d927139e1f5ecf6d37e8f597f441e.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=Hak4wSTLQa", "_bibtex": "@misc{\nqian2021improving,\ntitle={Improving Model Robustness with Latent Distribution Locally and Globally},\nauthor={Zhuang QIAN and Shufei Zhang and Kaizhu Huang and Qiufeng Wang and Rui Zhang and Xinping Yi},\nyear={2021},\nurl={https://openreview.net/forum?id=yvuk0RsLoP7}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 14, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "b-sCkxegnsX", "original": null, "number": 1, "cdate": 1610040356888, "ddate": null, "tcdate": 1610040356888, "tmdate": 1610473946548, "tddate": null, "forum": "yvuk0RsLoP7", "replyto": "yvuk0RsLoP7", "invitation": "ICLR.cc/2021/Conference/Paper3224/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "This paper presents a framework for adversarial robustness by incorporating local and global structures of the data manifold. In particular, the authors use a discriminator-classifier model, where the discriminator tries to differentiate between the original and adversarial spaces and the classifier aims to classify between them. The authors implement the proposed approach on several datasets and the experimental results demonstrate performance improvements. The idea of using the global data manifold into addressing robustness of the learning model is interesting. However, the technical contribution and novelty have not been explained very well."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improving Model Robustness with Latent Distribution Locally and Globally", "authorids": ["~Zhuang_QIAN1", "~Shufei_Zhang1", "~Kaizhu_Huang1", "~Qiufeng_Wang2", "~Rui_Zhang10", "xinping.yi@liverpool.ac.uk"], "authors": ["Zhuang QIAN", "Shufei Zhang", "Kaizhu Huang", "Qiufeng Wang", "Rui Zhang", "Xinping Yi"], "keywords": ["adversarial example", "robustness", "data manifold", "adversarial training"], "abstract": "We propose a novel adversarial training method which leverages both the local and global information to defend  adversarial attacks. Existing adversarial training methods usually generate adversarial perturbations  locally in a supervised manner and fail to consider the data manifold information in a global way. Consequently, the resulting  adversarial examples may corrupt the underlying data structure and are typically biased towards the decision boundary. In this work, we exploit both the local and global information of data manifold to generate adversarial examples in an unsupervised manner. Specifically, we design our novel framework  via an adversarial game between  a discriminator and a classifier: the discriminator is  learned to differentiate the latent distributions  of the natural data and the perturbed counterpart, while the classifier is trained to recognize accurately the perturbed examples as well as enforcing the invariance between the two latent distributions. We conduct a series of analysis on the model robustness and also verify the effectiveness of our proposed method empirically. Experimental results show that our method  substantially outperforms the recent state-of-the-art (i.e. Feature Scattering) in defending adversarial attacks  by a large accuracy margin  (e.g. $17.0\\%$ and $18.1\\%$ on SVHN dataset, $9.3\\%$ and $17.4\\%$ on CIFAR-10 dataset, $6.0\\%$ and $16.2\\%$ on CIFAR-100 dataset for defending PGD20 and CW20 attacks respectively).", "one-sentence_summary": "We propose a novel adversarial training method which leverages both the local and global information to defend  adversarial attacks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "qian|improving_model_robustness_with_latent_distribution_locally_and_globally", "supplementary_material": "/attachment/da9cf6d020cff31017c09ffce30d2883e395d6ba.zip", "pdf": "/pdf/4648d81b527d927139e1f5ecf6d37e8f597f441e.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=Hak4wSTLQa", "_bibtex": "@misc{\nqian2021improving,\ntitle={Improving Model Robustness with Latent Distribution Locally and Globally},\nauthor={Zhuang QIAN and Shufei Zhang and Kaizhu Huang and Qiufeng Wang and Rui Zhang and Xinping Yi},\nyear={2021},\nurl={https://openreview.net/forum?id=yvuk0RsLoP7}\n}"}, "tags": [], "invitation": {"reply": {"forum": "yvuk0RsLoP7", "replyto": "yvuk0RsLoP7", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040356873, "tmdate": 1610473946531, "id": "ICLR.cc/2021/Conference/Paper3224/-/Decision"}}}, {"id": "hwK5B9Ky7GH", "original": null, "number": 11, "cdate": 1606302821405, "ddate": null, "tcdate": 1606302821405, "tmdate": 1606302821405, "tddate": null, "forum": "yvuk0RsLoP7", "replyto": "Kq7ImLsK2OO", "invitation": "ICLR.cc/2021/Conference/Paper3224/-/Official_Comment", "content": {"title": "Response to AnonReviewer4 ", "comment": "1. Thanks for your further comments. \n\n  It is observed from Table 1 and Table 3 that this phenomenon can also be observed from the results of FS, i.e. the white-box attacks appear weaker than the black-box attacks. Note that the results of FS were obtained directly by running the source codes implemented by the authors of FS. Our source codes were also uploaded as complementary materials, which can be used for further verification.\n\n  We think the reason might be due to the unsupervised nature of FS and ATLD. Specifically, adversarial examples of both FS and our method are generated in an unsupervised fashion which can retain more structure information of data distribution. Therefore, capturing the manifold of adversarial examples, they could generalize better than the traditional AT methods on the white box attacks (as seen in Table 1). In the black-box setting (Table 3), the distributions of transferred adversarial examples may be inconsistent with the retained manifold information learned during the training of FS and ATLD. This may lead to an accuracy drop especially when compared with the white-box setting. Nonetheless, they still performed better than the other AT methods that do not consider the data manifold information. \n\n  We will leave this interesting topic for future investigations.\n\n\n2. Following your suggestions, we have reported the ATLD-IMT against AA and Rays in the revised paper. Again, the ATLD -IMT is still ahead of many the-state-of-art methods (which can be seen in the updated Appendix).  \n\n  It is noted that the results of ATLD -IMT in Table3 and Table6 of the original paper should be ATLD-IMT+ (which we have corrected in the updated version). While we feel sorry for just spotting such mistake, it actually won\u2019t affect the conclusion at all since they performed more or less comparable in the black-box setting. It should also be noted that we discussed even in the original paper that the IMT method has a negative impact on transfer-based black-box attacks no matter ATLD-IMT or ATLD-IMT+.\n\n\n3. Regarding the adaptive attack and feature attack as suggested, we don\u2019t mind to conduct further experiments. However, the very limited rebuttal period may not allow us to do so especially since the suggestion was just made before the rebuttal window will be closed within a few hours. We will consider to add these comparisons in the final version.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3224/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3224/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improving Model Robustness with Latent Distribution Locally and Globally", "authorids": ["~Zhuang_QIAN1", "~Shufei_Zhang1", "~Kaizhu_Huang1", "~Qiufeng_Wang2", "~Rui_Zhang10", "xinping.yi@liverpool.ac.uk"], "authors": ["Zhuang QIAN", "Shufei Zhang", "Kaizhu Huang", "Qiufeng Wang", "Rui Zhang", "Xinping Yi"], "keywords": ["adversarial example", "robustness", "data manifold", "adversarial training"], "abstract": "We propose a novel adversarial training method which leverages both the local and global information to defend  adversarial attacks. Existing adversarial training methods usually generate adversarial perturbations  locally in a supervised manner and fail to consider the data manifold information in a global way. Consequently, the resulting  adversarial examples may corrupt the underlying data structure and are typically biased towards the decision boundary. In this work, we exploit both the local and global information of data manifold to generate adversarial examples in an unsupervised manner. Specifically, we design our novel framework  via an adversarial game between  a discriminator and a classifier: the discriminator is  learned to differentiate the latent distributions  of the natural data and the perturbed counterpart, while the classifier is trained to recognize accurately the perturbed examples as well as enforcing the invariance between the two latent distributions. We conduct a series of analysis on the model robustness and also verify the effectiveness of our proposed method empirically. Experimental results show that our method  substantially outperforms the recent state-of-the-art (i.e. Feature Scattering) in defending adversarial attacks  by a large accuracy margin  (e.g. $17.0\\%$ and $18.1\\%$ on SVHN dataset, $9.3\\%$ and $17.4\\%$ on CIFAR-10 dataset, $6.0\\%$ and $16.2\\%$ on CIFAR-100 dataset for defending PGD20 and CW20 attacks respectively).", "one-sentence_summary": "We propose a novel adversarial training method which leverages both the local and global information to defend  adversarial attacks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "qian|improving_model_robustness_with_latent_distribution_locally_and_globally", "supplementary_material": "/attachment/da9cf6d020cff31017c09ffce30d2883e395d6ba.zip", "pdf": "/pdf/4648d81b527d927139e1f5ecf6d37e8f597f441e.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=Hak4wSTLQa", "_bibtex": "@misc{\nqian2021improving,\ntitle={Improving Model Robustness with Latent Distribution Locally and Globally},\nauthor={Zhuang QIAN and Shufei Zhang and Kaizhu Huang and Qiufeng Wang and Rui Zhang and Xinping Yi},\nyear={2021},\nurl={https://openreview.net/forum?id=yvuk0RsLoP7}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "yvuk0RsLoP7", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3224/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3224/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3224/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3224/Authors|ICLR.cc/2021/Conference/Paper3224/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3224/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923839756, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3224/-/Official_Comment"}}}, {"id": "Kq7ImLsK2OO", "original": null, "number": 10, "cdate": 1606292244877, "ddate": null, "tcdate": 1606292244877, "tmdate": 1606294742977, "tddate": null, "forum": "yvuk0RsLoP7", "replyto": "augRWyagFY", "invitation": "ICLR.cc/2021/Conference/Paper3224/-/Official_Comment", "content": {"title": "Thanks for the results.", "comment": "Thanks for the additional results. \nHowever, for me, the results are  still a bit strange and unconvincing. \n\nAs shown in Table 1 and Table 3, for ATLD-IMT, the white-box attacks (acc: 74.46% ) are weaker than the black-box attacks (acc: 65.07%), which is exhibiting obfuscated gradients[1]. Could the authors show the evaluation results of ATLD-IMT under the Auto-Attack and the evaluation results of ATLD-IMT+ on the transfer-based attacks? \n\nIn addition, could the authors evaluate the proposed methods with the adaptive attack, i.e., the proposed attack used in the training process? And how about the robustness under the  Feature Attack[2]?\n\n\n\n[1]  Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples. ICML 2018.\n\n[2] Feature Attack: https://openreview.net/forum?id=Syejj0NYvr&noteId=rkeBhuBMjS"}, "signatures": ["ICLR.cc/2021/Conference/Paper3224/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3224/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improving Model Robustness with Latent Distribution Locally and Globally", "authorids": ["~Zhuang_QIAN1", "~Shufei_Zhang1", "~Kaizhu_Huang1", "~Qiufeng_Wang2", "~Rui_Zhang10", "xinping.yi@liverpool.ac.uk"], "authors": ["Zhuang QIAN", "Shufei Zhang", "Kaizhu Huang", "Qiufeng Wang", "Rui Zhang", "Xinping Yi"], "keywords": ["adversarial example", "robustness", "data manifold", "adversarial training"], "abstract": "We propose a novel adversarial training method which leverages both the local and global information to defend  adversarial attacks. Existing adversarial training methods usually generate adversarial perturbations  locally in a supervised manner and fail to consider the data manifold information in a global way. Consequently, the resulting  adversarial examples may corrupt the underlying data structure and are typically biased towards the decision boundary. In this work, we exploit both the local and global information of data manifold to generate adversarial examples in an unsupervised manner. Specifically, we design our novel framework  via an adversarial game between  a discriminator and a classifier: the discriminator is  learned to differentiate the latent distributions  of the natural data and the perturbed counterpart, while the classifier is trained to recognize accurately the perturbed examples as well as enforcing the invariance between the two latent distributions. We conduct a series of analysis on the model robustness and also verify the effectiveness of our proposed method empirically. Experimental results show that our method  substantially outperforms the recent state-of-the-art (i.e. Feature Scattering) in defending adversarial attacks  by a large accuracy margin  (e.g. $17.0\\%$ and $18.1\\%$ on SVHN dataset, $9.3\\%$ and $17.4\\%$ on CIFAR-10 dataset, $6.0\\%$ and $16.2\\%$ on CIFAR-100 dataset for defending PGD20 and CW20 attacks respectively).", "one-sentence_summary": "We propose a novel adversarial training method which leverages both the local and global information to defend  adversarial attacks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "qian|improving_model_robustness_with_latent_distribution_locally_and_globally", "supplementary_material": "/attachment/da9cf6d020cff31017c09ffce30d2883e395d6ba.zip", "pdf": "/pdf/4648d81b527d927139e1f5ecf6d37e8f597f441e.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=Hak4wSTLQa", "_bibtex": "@misc{\nqian2021improving,\ntitle={Improving Model Robustness with Latent Distribution Locally and Globally},\nauthor={Zhuang QIAN and Shufei Zhang and Kaizhu Huang and Qiufeng Wang and Rui Zhang and Xinping Yi},\nyear={2021},\nurl={https://openreview.net/forum?id=yvuk0RsLoP7}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "yvuk0RsLoP7", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3224/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3224/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3224/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3224/Authors|ICLR.cc/2021/Conference/Paper3224/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3224/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923839756, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3224/-/Official_Comment"}}}, {"id": "PEioL3rjHTL", "original": null, "number": 8, "cdate": 1606288708753, "ddate": null, "tcdate": 1606288708753, "tmdate": 1606292072516, "tddate": null, "forum": "yvuk0RsLoP7", "replyto": "vKNSD7gK_U", "invitation": "ICLR.cc/2021/Conference/Paper3224/-/Official_Comment", "content": {"title": "Response to AnonReviewer1 (Part 2/3)", "comment": "2. Response to Cons2:\nWe highly appreciate this constructive suggestion. Following the reviewer's suggestion, we have re-organized Section 4.1 and split Eq.(6) in the previous version to two equations Eq. (6)-(7) in the new submission (in blue).\n\n  Specifically, we made a direct translation from Eq. (4)-(5) to Eq. (6) in the revised version with the term $L^{1:C}_d$ relegated to Eq. (7). \n\n  In the revised version, we explained how Eq. (6) comes from Eqs.(4)-(5) and described each term in Eq. (6). Specifically, we approximate Jensen-Shannon divergence between $P_\\theta^*$($P_\\theta$) and $Q_\\theta$ with $\\sup_W\\sum_{i=1}^NL_d$ and we minimize the classification loss on adversarial examples by minimizing $\\sum_{i=1}^NL_f$. The optimization problem (6) is solved by alternatively updating parameters $\\theta$ and $W$ and crafting adversarial examples $\\{x_i^{adv}\\}_{i=0}^N$. Although $D_w$ cannot measure the divergence between two latent distributions exactly at the first several training steps, when the parameters $W$ converge, $D_W$ can help evaluate the divergence between distributions induced by perturbed examples and clean ones. Then the worst perturbed examples can be crafted with the help of the discriminator $D_W$ as the constraint of (6). \n\n  Moreover, we detailed why the term $L^{1:C}_d$ comes into play in Eq. (7). It is a regularization term inspired by some existing works to deal with the failure of the discriminator networks. Several work [1][2][3] reveals high risk of failure in measuring only a fraction of components underlying different distributions with the discriminator networks, and shows that even if the discriminator is fully confused, there is no theoretical guarantee that two different distributions are identical. To alleviate such problem, we additionally train the discriminator $D_W$ to predict the class labels for latent features as [4][5] via adding the term $L^{1:C}_d$.\n\n  [1]Martin Arjovsky and L`eon Bottou. Towards principled methods for training generative\nadversarial networks. In Stat, volume 1050, 2017.\n\n  [2]Tong Che, Yanran Li, Athul Paul Jacob, Yoshua Bengio, andWenjie Li. Mode regularized\ngenerative adversarial networks. 2016.\n\n  [3]Tengyu Ma. Generalization and equilibrium in generative adversarial nets (gans) (invited\ntalk). In the 50th Annual ACM SIGACT Symposium, 2018.\n\n  [4]Augustus Odena, Christopher Olah, and Jonathon Shlens. Conditional image synthesis\nwith auxiliary classifier gans. In International conference on machine learning, pp.\n2642\u20132651, 2017.\n\n  [5]Mingsheng Long, Zhangjie Cao, Jianmin Wang, and Michael I Jordan. Conditional\nadversarial domain adaptation. In Advances in Neural Information Processing Systems,\npp. 1640\u20131650, 2018.\n\n\n3. Response to Cons3:\nWe thank the reviewer for raising this point. In general, we agree with the reviewer that the definition of adversarial examples here is different from that of the conventional one. It is because the adversary is introduced in an unsupervised manner (i.e., no label information is required to produce the adversarial examples), and thus the adversarial examples do not necessarily result in a mis-classification.\n\n   Following the reviewer's suggestions, we emphasize the difference of 'manifold' adversarial examples from the conventional ones in the revised paper. In particular, the 'manifold' adversarial examples in this paper are defined as the worst perturbed examples which induce the most different latent features (latent distribution) from the clean ones (measured by JS-divergence). When the different latent representations lead to different outputs or predictions, such adversarial examples can result in a mis-classification and agree with the definition of the conventional adversarial examples.\n\n  We agree with the reviewer that the solution to the optimisation problem in Eq. (6) could be sub-optimal due to not well-trained neural networks. However, we argue that the optimal solution might be unnecessary with respect to adversarial robustness. Our method aims to promote the distributional robustness by enforcing the invariance between the latent distributions of adversarial examples and clean ones. Even if our method may not generate the strongest adversarial examples (to fool the classifier) as the traditional supervised method, in practice our method can still generate adversarial examples near the classification boundary (as shown in Figure 1) to help boost the model robustness. Moreover, compared with traditional methods, our method can retain more structure information of the distribution as Figure 1 shows. It is also noted that although the discriminator $D_W$ cannot help generate the optimal adversarial examples at the first several training steps, when the parameters $W$ of the discriminator converge, it can help generate the near-optimal adversarial examples. "}, "signatures": ["ICLR.cc/2021/Conference/Paper3224/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3224/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improving Model Robustness with Latent Distribution Locally and Globally", "authorids": ["~Zhuang_QIAN1", "~Shufei_Zhang1", "~Kaizhu_Huang1", "~Qiufeng_Wang2", "~Rui_Zhang10", "xinping.yi@liverpool.ac.uk"], "authors": ["Zhuang QIAN", "Shufei Zhang", "Kaizhu Huang", "Qiufeng Wang", "Rui Zhang", "Xinping Yi"], "keywords": ["adversarial example", "robustness", "data manifold", "adversarial training"], "abstract": "We propose a novel adversarial training method which leverages both the local and global information to defend  adversarial attacks. Existing adversarial training methods usually generate adversarial perturbations  locally in a supervised manner and fail to consider the data manifold information in a global way. Consequently, the resulting  adversarial examples may corrupt the underlying data structure and are typically biased towards the decision boundary. In this work, we exploit both the local and global information of data manifold to generate adversarial examples in an unsupervised manner. Specifically, we design our novel framework  via an adversarial game between  a discriminator and a classifier: the discriminator is  learned to differentiate the latent distributions  of the natural data and the perturbed counterpart, while the classifier is trained to recognize accurately the perturbed examples as well as enforcing the invariance between the two latent distributions. We conduct a series of analysis on the model robustness and also verify the effectiveness of our proposed method empirically. Experimental results show that our method  substantially outperforms the recent state-of-the-art (i.e. Feature Scattering) in defending adversarial attacks  by a large accuracy margin  (e.g. $17.0\\%$ and $18.1\\%$ on SVHN dataset, $9.3\\%$ and $17.4\\%$ on CIFAR-10 dataset, $6.0\\%$ and $16.2\\%$ on CIFAR-100 dataset for defending PGD20 and CW20 attacks respectively).", "one-sentence_summary": "We propose a novel adversarial training method which leverages both the local and global information to defend  adversarial attacks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "qian|improving_model_robustness_with_latent_distribution_locally_and_globally", "supplementary_material": "/attachment/da9cf6d020cff31017c09ffce30d2883e395d6ba.zip", "pdf": "/pdf/4648d81b527d927139e1f5ecf6d37e8f597f441e.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=Hak4wSTLQa", "_bibtex": "@misc{\nqian2021improving,\ntitle={Improving Model Robustness with Latent Distribution Locally and Globally},\nauthor={Zhuang QIAN and Shufei Zhang and Kaizhu Huang and Qiufeng Wang and Rui Zhang and Xinping Yi},\nyear={2021},\nurl={https://openreview.net/forum?id=yvuk0RsLoP7}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "yvuk0RsLoP7", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3224/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3224/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3224/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3224/Authors|ICLR.cc/2021/Conference/Paper3224/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3224/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923839756, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3224/-/Official_Comment"}}}, {"id": "augRWyagFY", "original": null, "number": 5, "cdate": 1606286653280, "ddate": null, "tcdate": 1606286653280, "tmdate": 1606291993471, "tddate": null, "forum": "yvuk0RsLoP7", "replyto": "pEPBJlVSWN5", "invitation": "ICLR.cc/2021/Conference/Paper3224/-/Official_Comment", "content": {"title": "Response to AnonReviewer4", "comment": "Thank you for your efforts reviewing our paper and providing constructive comments\nand helpful suggestions.\n\n\n1. Response to Cons1: Sorry for the confusion, but we respectfully disagree with this.\nTo avoid possible confusion, we have revised the description of our method in the revised\npaper. Although Feature-scattering (FS) is one of our motivations to consider global information,\nour proposed ATDL method is fundamentally different from FS as well as the\ntraditional AT. Specifically, FS generates adversarial examples by computing the feature\nmatching distance between the batch of the original and perturbed samples, whilst our\nmethod leverages a discriminator to distinguish the whole latent manifolds resulted from\nthe original clean and perturbed samples. In other words, our \u2018manifold\u2019 adversarial examples\nare crafted to disturb the manifold of latent distributions induced by the original\nsamples as much as possible by leveraging the discriminator. We have revised the equation\nand added more descriptions to elaborate our proposed method more clearly.\n\n    It is worth noting that our method can be viewed as the game among three players: the\nclassifier, discriminator, and adversarial examples (as emphasized in the revised version).\nThe discriminator is learned to differentiate the latent distributions of the perturbed examples\nand clean data; the classifier is trained to (1) enforce the invariance between these two\ndistributions to confuse the discriminator, and (2) classify the adversarial examples as accurately\nas possible; adversarial examples are crafted to differentiate the adversarial latent\ndistribution from the natural one.\n\n\n2. Response to Cons2: Following the suggestions, we have conducted additional experiments\nagainst AutoAttack and RayS on CIFAR-10 and CIFAR-100, which show our proposed\nATDL-IMT+ can again outperform the existing state-of-the-art methods (as seen in Appendix\nB.2).\nSpecifically, on CIFAR-10, our proposed ATDL-IMT+ method outperforms the-state-of-art\nmethods by a large accuracy margin. Without exploiting additional data, our method can\neven perform better than all the other 9 algorithms: our method attains 70.60% accuracy\nagainst AutoAttack ($\\epsilon = 8/255$) and 81.68% accuracy against Rays ($\\epsilon = 8/255$), while the\nbest of the others are just 65.88% and 64.6% respectively.\nOn CIFAR-100, our ATDL-IMT+ could achieve 32.36% accuracy against AA ($\\epsilon = 8/255$),\nwhich also outperforms all the other competitive methods without using additional data\n(e.g. without exploiting unlabeled data and pretraining). Although Gowal et al. (2020)*\nachieves better performance of 36.88%, it requires more unlabeled data. Moreover, our\nmethod is still ahead of its normal version which leverages no additional data.\n\n\n3. Response to Cons3: There might be some misunderstandings about this. Due to the page\nlimitation, several illustrative experiments and analysis are reported in Appendix including 1) how our proposed method affects the decision boundary compared with PGD and FS, 2)\nfurther analysis about the proposed ATDL-IMT, and 3) the illustration of the vector field\nfor different perturbation schemes. Nonetheless, we will surely conduct more analysis in\nthe final version."}, "signatures": ["ICLR.cc/2021/Conference/Paper3224/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3224/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improving Model Robustness with Latent Distribution Locally and Globally", "authorids": ["~Zhuang_QIAN1", "~Shufei_Zhang1", "~Kaizhu_Huang1", "~Qiufeng_Wang2", "~Rui_Zhang10", "xinping.yi@liverpool.ac.uk"], "authors": ["Zhuang QIAN", "Shufei Zhang", "Kaizhu Huang", "Qiufeng Wang", "Rui Zhang", "Xinping Yi"], "keywords": ["adversarial example", "robustness", "data manifold", "adversarial training"], "abstract": "We propose a novel adversarial training method which leverages both the local and global information to defend  adversarial attacks. Existing adversarial training methods usually generate adversarial perturbations  locally in a supervised manner and fail to consider the data manifold information in a global way. Consequently, the resulting  adversarial examples may corrupt the underlying data structure and are typically biased towards the decision boundary. In this work, we exploit both the local and global information of data manifold to generate adversarial examples in an unsupervised manner. Specifically, we design our novel framework  via an adversarial game between  a discriminator and a classifier: the discriminator is  learned to differentiate the latent distributions  of the natural data and the perturbed counterpart, while the classifier is trained to recognize accurately the perturbed examples as well as enforcing the invariance between the two latent distributions. We conduct a series of analysis on the model robustness and also verify the effectiveness of our proposed method empirically. Experimental results show that our method  substantially outperforms the recent state-of-the-art (i.e. Feature Scattering) in defending adversarial attacks  by a large accuracy margin  (e.g. $17.0\\%$ and $18.1\\%$ on SVHN dataset, $9.3\\%$ and $17.4\\%$ on CIFAR-10 dataset, $6.0\\%$ and $16.2\\%$ on CIFAR-100 dataset for defending PGD20 and CW20 attacks respectively).", "one-sentence_summary": "We propose a novel adversarial training method which leverages both the local and global information to defend  adversarial attacks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "qian|improving_model_robustness_with_latent_distribution_locally_and_globally", "supplementary_material": "/attachment/da9cf6d020cff31017c09ffce30d2883e395d6ba.zip", "pdf": "/pdf/4648d81b527d927139e1f5ecf6d37e8f597f441e.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=Hak4wSTLQa", "_bibtex": "@misc{\nqian2021improving,\ntitle={Improving Model Robustness with Latent Distribution Locally and Globally},\nauthor={Zhuang QIAN and Shufei Zhang and Kaizhu Huang and Qiufeng Wang and Rui Zhang and Xinping Yi},\nyear={2021},\nurl={https://openreview.net/forum?id=yvuk0RsLoP7}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "yvuk0RsLoP7", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3224/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3224/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3224/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3224/Authors|ICLR.cc/2021/Conference/Paper3224/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3224/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923839756, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3224/-/Official_Comment"}}}, {"id": "UGYxXi0Lhx7", "original": null, "number": 7, "cdate": 1606287954747, "ddate": null, "tcdate": 1606287954747, "tmdate": 1606291898368, "tddate": null, "forum": "yvuk0RsLoP7", "replyto": "vKNSD7gK_U", "invitation": "ICLR.cc/2021/Conference/Paper3224/-/Official_Comment", "content": {"title": "Response to AnonReviewer1 (Part 1/3)", "comment": "We appreciate R1 for the constructive comments which truly help us improve the quality\nof the paper. We believe the main concerns from R1 lie at the confusing writing in the original paper,\nwhich however may not affect the correctness of the proposed theory as well as its significance.\nFollowing the valuable suggestions, we have updated our paper by making clarifications and revising\nthe descriptions of the equations and some texts to avoid potential confusion.\n1. Response to Cons1:\nAccording to R1\u2019s comments, we have made Eqs.(4)-(5) more explicitly and clarified the\nreviewer\u2019s questions in the revised version (in blue).\nFor clarity, we also list the major definitions in Appendix A of the updated paper. We\nprovide these definitions as follows for convenience.\n\n    * $X_{org}=$ {${x:x\\sim Q_0}$}: the set of clean data samples, where $Q_0$ is its underlying\ndistribution;\n\n    * $X_{p}=$ {${x': x'\\in B(x,\\epsilon), \\forall x\\sim Q_0}$}: the set of perturbed samples, the element $x'\\in X_{p}$ is in the $\\epsilon$-neighborhood of the clean example $x\\sim Q_0$;\n\n    * $f_\\theta$: the mapping function from input to the latent features of the last hidden layer (i.e., the layer before the softmax layer);\n\n    * $Q_\\theta$: the underlying distribution of the latent feature $f_\\theta (x)$ for all $x \\in X_{org}$;\n\n    * $P_\\theta$: the underlying distribution of the latent feature $f_\\theta (x')$ for all $x'\\in X_{p}$;\n\n    * $\\mathcal{P}$: the feasible region of the latent distribution $P_{\\theta}$, which is defined as $\\mathcal{P} \\triangleq $ {${P:f_\\theta(x')\\sim P \\text{ subject to } \\forall x\\sim Q_0, x'\\in B(x,\\epsilon)}$}.\n\n     * $X_{adv}$: the set of the worst perturbed samples or manifold adversarial examples, the element $x^{adv}\\in X_{adv}$ are in the $\\epsilon$-neighborhood of clean example $x\\sim Q_0$;\n\n    * $P_\\theta^*$: the worst latent distribution within the feasible region $\\mathcal{P}$ which leads to the largest divergence or the underlying distribution of the latent feature $f_\\theta (x^{adv})$ for all $x^{adv}\\in X_{adv}$;\n  \n\n   While the optimization problem in Eqs.(4)-(5) remains equivalent to the previous formulation, we introduce some parameters and definitions to explicitly indicate the relation of the clean examples $X_{org}$, the manifold adversarial examples $X_{adv}$, and the latent distributions $P_{\\theta}$ and $Q_{\\theta}$ in the last hidden layer.\n\n  First of all, we would clarify that our aim is to enhance the distributional robustness in an unsupervised fashion, and the resulting adversarial examples $X_{adv}$ are not the same as the conventional definition. We refer to them as the manifold adversarial examples, which are drawn from an underlying distribution perturbed from the underlying distribution of the input samples $X_{org}$.\n\n  For distributional robustness, the initial objective is to figure out the underlying distribution of the manifold adversarial examples, which is the worst perturbation of $Q_0$, so that latent distribution $P_{\\theta}$ is as apart from $Q_{\\theta}$ as possible through the $f$-divergence metric.\n\n  Since this is in general intractable, we instead aim to maximize the $f$-divergence between two latent distributions, subject to the constraint that $P_{\\theta} \\in \\mathcal{P}$. It is expected that the underlying distribution of $X_{adv}$ that yields the worst-case latent distribution $P_{\\theta}^{*}$ could be utilized  to enhance robustness through adversarial training.\n\n  To directly answer the reviewer's question, in Eqs.(4)-(5) of the revised version, the adversarial example $x^{adv}$ is related to $x$ and $P_\\theta$ through the feasible region $\\mathcal{P}$."}, "signatures": ["ICLR.cc/2021/Conference/Paper3224/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3224/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improving Model Robustness with Latent Distribution Locally and Globally", "authorids": ["~Zhuang_QIAN1", "~Shufei_Zhang1", "~Kaizhu_Huang1", "~Qiufeng_Wang2", "~Rui_Zhang10", "xinping.yi@liverpool.ac.uk"], "authors": ["Zhuang QIAN", "Shufei Zhang", "Kaizhu Huang", "Qiufeng Wang", "Rui Zhang", "Xinping Yi"], "keywords": ["adversarial example", "robustness", "data manifold", "adversarial training"], "abstract": "We propose a novel adversarial training method which leverages both the local and global information to defend  adversarial attacks. Existing adversarial training methods usually generate adversarial perturbations  locally in a supervised manner and fail to consider the data manifold information in a global way. Consequently, the resulting  adversarial examples may corrupt the underlying data structure and are typically biased towards the decision boundary. In this work, we exploit both the local and global information of data manifold to generate adversarial examples in an unsupervised manner. Specifically, we design our novel framework  via an adversarial game between  a discriminator and a classifier: the discriminator is  learned to differentiate the latent distributions  of the natural data and the perturbed counterpart, while the classifier is trained to recognize accurately the perturbed examples as well as enforcing the invariance between the two latent distributions. We conduct a series of analysis on the model robustness and also verify the effectiveness of our proposed method empirically. Experimental results show that our method  substantially outperforms the recent state-of-the-art (i.e. Feature Scattering) in defending adversarial attacks  by a large accuracy margin  (e.g. $17.0\\%$ and $18.1\\%$ on SVHN dataset, $9.3\\%$ and $17.4\\%$ on CIFAR-10 dataset, $6.0\\%$ and $16.2\\%$ on CIFAR-100 dataset for defending PGD20 and CW20 attacks respectively).", "one-sentence_summary": "We propose a novel adversarial training method which leverages both the local and global information to defend  adversarial attacks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "qian|improving_model_robustness_with_latent_distribution_locally_and_globally", "supplementary_material": "/attachment/da9cf6d020cff31017c09ffce30d2883e395d6ba.zip", "pdf": "/pdf/4648d81b527d927139e1f5ecf6d37e8f597f441e.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=Hak4wSTLQa", "_bibtex": "@misc{\nqian2021improving,\ntitle={Improving Model Robustness with Latent Distribution Locally and Globally},\nauthor={Zhuang QIAN and Shufei Zhang and Kaizhu Huang and Qiufeng Wang and Rui Zhang and Xinping Yi},\nyear={2021},\nurl={https://openreview.net/forum?id=yvuk0RsLoP7}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "yvuk0RsLoP7", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3224/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3224/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3224/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3224/Authors|ICLR.cc/2021/Conference/Paper3224/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3224/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923839756, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3224/-/Official_Comment"}}}, {"id": "NeaS-KvOm0t", "original": null, "number": 9, "cdate": 1606289455798, "ddate": null, "tcdate": 1606289455798, "tmdate": 1606289455798, "tddate": null, "forum": "yvuk0RsLoP7", "replyto": "vKNSD7gK_U", "invitation": "ICLR.cc/2021/Conference/Paper3224/-/Official_Comment", "content": {"title": "Response to AnonReviewer1 (Part 3/3)", "comment": "4. Response to Cons4:\nThe mentioned term is now in Eq. (7) in the revised version, which has both $x$ and $x_{adv}$ matched with the same class label. This is a regularization term to reduce the risk of failure of the discriminator networks. It has nothing to do with adversarial robustness. The aim is to help the discriminator to distinguish between the two distributions more accurately.\n\n  For the adversarial robustness, as mentioned in '`Response to Cons3', it can be achieved by the interplay between all terms in Eq. (7) that form a minimax game among the classifier, the discriminator and the adversarial examples. The adversarial examples are crafted to induce the most different latent features from the latent features of the clean data (with the help of the discriminator), while the classifier is trained to classify the adversarial examples generated by our proposed method as accurately as possible and enforce the invariance between the latent distributions of adversarial examples and clean ones.  By the outer minimization, the latent representations of such adversarial examples are enforced to be similar to clean ones. It means that the outputs or predictions of adversarial examples are similar to those of the cleaned ones. In other words, the DNN can classify such adversarial examples as accurately as clean ones, meaning that the adversarial robustness is guaranteed.\n\n\n5. Response to Minor comments:\n\n    (a) $\\tau$ is an arbitrary class of functions $T:\\mathcal{X}\\to \\mathbb{R}$, we have added the description in the revised version.\n\n    (b) $f_\\theta$ represents the latent features of the last hidden layer. We clarify this point in our revised version, for example, ''$f_\\theta(x')$ and $f_\\theta(x^{adv})$ represents the latent features of the perturbed example $x'$ and adversarial example $x^{adv}$ respectively''; ''$f_\\theta(x_i)$ is the latent feature of the clean sample $x_i$''.\n\n    (c) Thank you for pointing out this issue. We remark that our proposed 'adversarial examples' are in a general sense, which are crafted by an adversary in an unsupervised manner. This is shown in the constraint in Eq.(6)-(7) in our revised paper.  It is different from the conventional definition, as it unnecessarily results in a mis-classification due to the lack of label information. Note that the label information used in the discriminator is only for better guiding the discriminator to distinguish distributions more accurately, and is not used for generating adversarial perturbation. As Figure 2 shows in the submission, the adversarial examples are generated according to the gradient of $L_d^0$ with respect to $x$, for which no label information is required.\n\n    (d) Thank you for pointing out this. In the revised version, we clarified these points. The manifold label is a binary value which indicates whether the latent features are induced by the perturbed samples or the clean samples. It is shown as $L_d$ in Eq.(6) and $L^0_d$ in Eq.(7). We also added the explanation for it in Figure 2.\n\n    (e) Roughly speaking, data manifold can be seen as the shape of the underlying distribution of the input data that lives in the input space. The input data are sampled from such manifold, where the inter-sample relationship may not be maintained when each data is considered separately. In the conventional adversarial training, each adversarial example is separately generated from a single clean data sample. Such  relationship might be implicitly captured by the neural network if the number of clean data samples tends to infinity; however it is impossible in practice. Given a finite number of training data, explicit consideration of inter-sample relationship helps, as demonstrated in this paper."}, "signatures": ["ICLR.cc/2021/Conference/Paper3224/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3224/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improving Model Robustness with Latent Distribution Locally and Globally", "authorids": ["~Zhuang_QIAN1", "~Shufei_Zhang1", "~Kaizhu_Huang1", "~Qiufeng_Wang2", "~Rui_Zhang10", "xinping.yi@liverpool.ac.uk"], "authors": ["Zhuang QIAN", "Shufei Zhang", "Kaizhu Huang", "Qiufeng Wang", "Rui Zhang", "Xinping Yi"], "keywords": ["adversarial example", "robustness", "data manifold", "adversarial training"], "abstract": "We propose a novel adversarial training method which leverages both the local and global information to defend  adversarial attacks. Existing adversarial training methods usually generate adversarial perturbations  locally in a supervised manner and fail to consider the data manifold information in a global way. Consequently, the resulting  adversarial examples may corrupt the underlying data structure and are typically biased towards the decision boundary. In this work, we exploit both the local and global information of data manifold to generate adversarial examples in an unsupervised manner. Specifically, we design our novel framework  via an adversarial game between  a discriminator and a classifier: the discriminator is  learned to differentiate the latent distributions  of the natural data and the perturbed counterpart, while the classifier is trained to recognize accurately the perturbed examples as well as enforcing the invariance between the two latent distributions. We conduct a series of analysis on the model robustness and also verify the effectiveness of our proposed method empirically. Experimental results show that our method  substantially outperforms the recent state-of-the-art (i.e. Feature Scattering) in defending adversarial attacks  by a large accuracy margin  (e.g. $17.0\\%$ and $18.1\\%$ on SVHN dataset, $9.3\\%$ and $17.4\\%$ on CIFAR-10 dataset, $6.0\\%$ and $16.2\\%$ on CIFAR-100 dataset for defending PGD20 and CW20 attacks respectively).", "one-sentence_summary": "We propose a novel adversarial training method which leverages both the local and global information to defend  adversarial attacks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "qian|improving_model_robustness_with_latent_distribution_locally_and_globally", "supplementary_material": "/attachment/da9cf6d020cff31017c09ffce30d2883e395d6ba.zip", "pdf": "/pdf/4648d81b527d927139e1f5ecf6d37e8f597f441e.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=Hak4wSTLQa", "_bibtex": "@misc{\nqian2021improving,\ntitle={Improving Model Robustness with Latent Distribution Locally and Globally},\nauthor={Zhuang QIAN and Shufei Zhang and Kaizhu Huang and Qiufeng Wang and Rui Zhang and Xinping Yi},\nyear={2021},\nurl={https://openreview.net/forum?id=yvuk0RsLoP7}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "yvuk0RsLoP7", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3224/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3224/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3224/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3224/Authors|ICLR.cc/2021/Conference/Paper3224/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3224/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923839756, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3224/-/Official_Comment"}}}, {"id": "jbcDz14wAMt", "original": null, "number": 4, "cdate": 1606284839940, "ddate": null, "tcdate": 1606284839940, "tmdate": 1606288843367, "tddate": null, "forum": "yvuk0RsLoP7", "replyto": "EIlDln-C5Ne", "invitation": "ICLR.cc/2021/Conference/Paper3224/-/Official_Comment", "content": {"title": "Response to AnonReviewer3", "comment": "We appreciate the positive comments of the reviewer and the constructive feedback.\n1. Response to Cons1:\nThanks for raising this issue. To clarify how Eqs.(4)-(5) are realized using discriminator and classifier, we have split Eq. (6) to two equations Eqs. (6)-(7) in the revised version. Note that now Eq. (6) is directly translated from Eqs. (4)-(5), where a discriminator network is employed for optimization, and it can be further reformulated in Eq. (7) by adding a regularization term to avoid some issues of discriminator networks. Specifically, in Eq. (6), we approximate Jensen-Shannon divergence between $P_\\theta^*$($P_\\theta$) and $Q_\\theta$ with $\\sup_W\\sum_{i=1}^NL_d$, and minimize the classification loss on adversarial examples by minimizing$\\sum_{i=1}^NL_f$ . As mentioned in the revised paper, it is a challenging task to evaluate the divergence between two latent distributions. To make it more tractable, we leverage a discriminator network for estimating the Jensen-Shannon divergence between two distributions $P_\\theta^*/P_\\theta$ and $Q_\\theta$ according to Section 3.2.\n \n \n2. Response to Cons2:\nThanks for pointing out this issue. We use $L_\\infty$ perturbation in all experiments including training and testing. We have added more description in the revised version.\n\n\n3. Response to Cons3: The attacks which are unsupervised generated by our method may be\nweaker than the supervised ones such as PGD and CW, since our attacks did not leverage the\nlabel information and could not affect on gradients directly. Besides, since other methods\nhave no components which aim to estimate the data manifold, so we could not generate\nglobal data manifold attacks on other methods which makes the results non-comparative.\n\n\n4. Moreover, we have also conducted two more experiments to show that our proposed method could achieve much better robustness than the latest competitive models against more updated attacks such as AutoAttack and Rays as shown in Appendix B.2"}, "signatures": ["ICLR.cc/2021/Conference/Paper3224/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3224/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improving Model Robustness with Latent Distribution Locally and Globally", "authorids": ["~Zhuang_QIAN1", "~Shufei_Zhang1", "~Kaizhu_Huang1", "~Qiufeng_Wang2", "~Rui_Zhang10", "xinping.yi@liverpool.ac.uk"], "authors": ["Zhuang QIAN", "Shufei Zhang", "Kaizhu Huang", "Qiufeng Wang", "Rui Zhang", "Xinping Yi"], "keywords": ["adversarial example", "robustness", "data manifold", "adversarial training"], "abstract": "We propose a novel adversarial training method which leverages both the local and global information to defend  adversarial attacks. Existing adversarial training methods usually generate adversarial perturbations  locally in a supervised manner and fail to consider the data manifold information in a global way. Consequently, the resulting  adversarial examples may corrupt the underlying data structure and are typically biased towards the decision boundary. In this work, we exploit both the local and global information of data manifold to generate adversarial examples in an unsupervised manner. Specifically, we design our novel framework  via an adversarial game between  a discriminator and a classifier: the discriminator is  learned to differentiate the latent distributions  of the natural data and the perturbed counterpart, while the classifier is trained to recognize accurately the perturbed examples as well as enforcing the invariance between the two latent distributions. We conduct a series of analysis on the model robustness and also verify the effectiveness of our proposed method empirically. Experimental results show that our method  substantially outperforms the recent state-of-the-art (i.e. Feature Scattering) in defending adversarial attacks  by a large accuracy margin  (e.g. $17.0\\%$ and $18.1\\%$ on SVHN dataset, $9.3\\%$ and $17.4\\%$ on CIFAR-10 dataset, $6.0\\%$ and $16.2\\%$ on CIFAR-100 dataset for defending PGD20 and CW20 attacks respectively).", "one-sentence_summary": "We propose a novel adversarial training method which leverages both the local and global information to defend  adversarial attacks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "qian|improving_model_robustness_with_latent_distribution_locally_and_globally", "supplementary_material": "/attachment/da9cf6d020cff31017c09ffce30d2883e395d6ba.zip", "pdf": "/pdf/4648d81b527d927139e1f5ecf6d37e8f597f441e.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=Hak4wSTLQa", "_bibtex": "@misc{\nqian2021improving,\ntitle={Improving Model Robustness with Latent Distribution Locally and Globally},\nauthor={Zhuang QIAN and Shufei Zhang and Kaizhu Huang and Qiufeng Wang and Rui Zhang and Xinping Yi},\nyear={2021},\nurl={https://openreview.net/forum?id=yvuk0RsLoP7}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "yvuk0RsLoP7", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3224/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3224/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3224/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3224/Authors|ICLR.cc/2021/Conference/Paper3224/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3224/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923839756, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3224/-/Official_Comment"}}}, {"id": "lQavxP8XHuH", "original": null, "number": 6, "cdate": 1606287040357, "ddate": null, "tcdate": 1606287040357, "tmdate": 1606287040357, "tddate": null, "forum": "yvuk0RsLoP7", "replyto": "cIp99ASc2ES", "invitation": "ICLR.cc/2021/Conference/Paper3224/-/Official_Comment", "content": {"title": "Response to AnonReviewer2", "comment": "We highly appreciate the positive comments of the reviewer. We have revised the whole\npaper and corrected the typos and inappropriate expressions. Moreover, we have conducted two\nmore experiments to show that our proposed method could achieve much better robustness than the\nlatest competitive models against more updated attacks such as AutoAttack and Rays as shown in\nAppendix B.2"}, "signatures": ["ICLR.cc/2021/Conference/Paper3224/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3224/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improving Model Robustness with Latent Distribution Locally and Globally", "authorids": ["~Zhuang_QIAN1", "~Shufei_Zhang1", "~Kaizhu_Huang1", "~Qiufeng_Wang2", "~Rui_Zhang10", "xinping.yi@liverpool.ac.uk"], "authors": ["Zhuang QIAN", "Shufei Zhang", "Kaizhu Huang", "Qiufeng Wang", "Rui Zhang", "Xinping Yi"], "keywords": ["adversarial example", "robustness", "data manifold", "adversarial training"], "abstract": "We propose a novel adversarial training method which leverages both the local and global information to defend  adversarial attacks. Existing adversarial training methods usually generate adversarial perturbations  locally in a supervised manner and fail to consider the data manifold information in a global way. Consequently, the resulting  adversarial examples may corrupt the underlying data structure and are typically biased towards the decision boundary. In this work, we exploit both the local and global information of data manifold to generate adversarial examples in an unsupervised manner. Specifically, we design our novel framework  via an adversarial game between  a discriminator and a classifier: the discriminator is  learned to differentiate the latent distributions  of the natural data and the perturbed counterpart, while the classifier is trained to recognize accurately the perturbed examples as well as enforcing the invariance between the two latent distributions. We conduct a series of analysis on the model robustness and also verify the effectiveness of our proposed method empirically. Experimental results show that our method  substantially outperforms the recent state-of-the-art (i.e. Feature Scattering) in defending adversarial attacks  by a large accuracy margin  (e.g. $17.0\\%$ and $18.1\\%$ on SVHN dataset, $9.3\\%$ and $17.4\\%$ on CIFAR-10 dataset, $6.0\\%$ and $16.2\\%$ on CIFAR-100 dataset for defending PGD20 and CW20 attacks respectively).", "one-sentence_summary": "We propose a novel adversarial training method which leverages both the local and global information to defend  adversarial attacks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "qian|improving_model_robustness_with_latent_distribution_locally_and_globally", "supplementary_material": "/attachment/da9cf6d020cff31017c09ffce30d2883e395d6ba.zip", "pdf": "/pdf/4648d81b527d927139e1f5ecf6d37e8f597f441e.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=Hak4wSTLQa", "_bibtex": "@misc{\nqian2021improving,\ntitle={Improving Model Robustness with Latent Distribution Locally and Globally},\nauthor={Zhuang QIAN and Shufei Zhang and Kaizhu Huang and Qiufeng Wang and Rui Zhang and Xinping Yi},\nyear={2021},\nurl={https://openreview.net/forum?id=yvuk0RsLoP7}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "yvuk0RsLoP7", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3224/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3224/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3224/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3224/Authors|ICLR.cc/2021/Conference/Paper3224/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3224/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923839756, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3224/-/Official_Comment"}}}, {"id": "AIQRrsf5c0e", "original": null, "number": 3, "cdate": 1606284191244, "ddate": null, "tcdate": 1606284191244, "tmdate": 1606285602531, "tddate": null, "forum": "yvuk0RsLoP7", "replyto": "yvuk0RsLoP7", "invitation": "ICLR.cc/2021/Conference/Paper3224/-/Official_Comment", "content": {"title": "MAJOR REVISION", "comment": "We appreciate the constructive comments from the reviewers which indeed improve the quality of\nour paper. In the revision, we substantially revised the paper, i.e., we conducted additional experimental\ncomparisons with several state-of-art methods against AutoAttack and Rays, made further\nclarification to explain our technical contributions, proofread the paper, and corrected some inaccurate\nexpressions according to the comments from all the reviewers. For convenience, we have made\nin blue all the revised parts in the new version.\n1. Conducted more experiments. Overall, we have compared our ATLD-IMT+ with 9 other\ncompetitive approaches against AutoAttack (AA) and Rays on CIFAR-10. Also, We have\ncompared ours with 4 competitive methods against AA on CIFAR-100. Experiments have\nshown that our model significantly outperforms the other competitive models in these new\nexperiments (see Appendix for details).\n\n    (a) Compared the proposed method ATLD-IMT+ against AA and RayS additionally with\n9 other methods, i.e., WAR and RTS on CIFAR-10.\n\n    (b) Compared the proposed method ATLD-IMT+ against AA additionally with four other\nmethods, i.e., Robust-overfitting and Pretraining on CIFAR-100.\n\n\n2. Revised thoroughly to explain our technical contributions in equation (4), (5) and (6).\n\n\n3. Proofreaded the paper and corrected typos and inappropriate expressions."}, "signatures": ["ICLR.cc/2021/Conference/Paper3224/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3224/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improving Model Robustness with Latent Distribution Locally and Globally", "authorids": ["~Zhuang_QIAN1", "~Shufei_Zhang1", "~Kaizhu_Huang1", "~Qiufeng_Wang2", "~Rui_Zhang10", "xinping.yi@liverpool.ac.uk"], "authors": ["Zhuang QIAN", "Shufei Zhang", "Kaizhu Huang", "Qiufeng Wang", "Rui Zhang", "Xinping Yi"], "keywords": ["adversarial example", "robustness", "data manifold", "adversarial training"], "abstract": "We propose a novel adversarial training method which leverages both the local and global information to defend  adversarial attacks. Existing adversarial training methods usually generate adversarial perturbations  locally in a supervised manner and fail to consider the data manifold information in a global way. Consequently, the resulting  adversarial examples may corrupt the underlying data structure and are typically biased towards the decision boundary. In this work, we exploit both the local and global information of data manifold to generate adversarial examples in an unsupervised manner. Specifically, we design our novel framework  via an adversarial game between  a discriminator and a classifier: the discriminator is  learned to differentiate the latent distributions  of the natural data and the perturbed counterpart, while the classifier is trained to recognize accurately the perturbed examples as well as enforcing the invariance between the two latent distributions. We conduct a series of analysis on the model robustness and also verify the effectiveness of our proposed method empirically. Experimental results show that our method  substantially outperforms the recent state-of-the-art (i.e. Feature Scattering) in defending adversarial attacks  by a large accuracy margin  (e.g. $17.0\\%$ and $18.1\\%$ on SVHN dataset, $9.3\\%$ and $17.4\\%$ on CIFAR-10 dataset, $6.0\\%$ and $16.2\\%$ on CIFAR-100 dataset for defending PGD20 and CW20 attacks respectively).", "one-sentence_summary": "We propose a novel adversarial training method which leverages both the local and global information to defend  adversarial attacks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "qian|improving_model_robustness_with_latent_distribution_locally_and_globally", "supplementary_material": "/attachment/da9cf6d020cff31017c09ffce30d2883e395d6ba.zip", "pdf": "/pdf/4648d81b527d927139e1f5ecf6d37e8f597f441e.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=Hak4wSTLQa", "_bibtex": "@misc{\nqian2021improving,\ntitle={Improving Model Robustness with Latent Distribution Locally and Globally},\nauthor={Zhuang QIAN and Shufei Zhang and Kaizhu Huang and Qiufeng Wang and Rui Zhang and Xinping Yi},\nyear={2021},\nurl={https://openreview.net/forum?id=yvuk0RsLoP7}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "yvuk0RsLoP7", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3224/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3224/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3224/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3224/Authors|ICLR.cc/2021/Conference/Paper3224/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3224/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923839756, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3224/-/Official_Comment"}}}, {"id": "vKNSD7gK_U", "original": null, "number": 1, "cdate": 1603990972370, "ddate": null, "tcdate": 1603990972370, "tmdate": 1605024042840, "tddate": null, "forum": "yvuk0RsLoP7", "replyto": "yvuk0RsLoP7", "invitation": "ICLR.cc/2021/Conference/Paper3224/-/Official_Review", "content": {"title": "Needs better technical exposition", "review": "This paper presents a framework for adversarial robustness via incorporating local and global structure of the data manifold. Specifically, the key motivation is that standard adversarial methods typically use only sample specific perturbations for generating the adversarial examples, and thus using them for robustness of the learning model is limited. Instead the paper proposes to capture the global data manifold as well in the robustifying framework. To this end, an objective is presented (4,5) that uses latent data distributions, with the goal that the adversarial perturbations should maximize the f-divergence against the latent distribution of the clean samples. Experiments are provided on several datasets and demonstrate significant performance improvements.\n\nPros:\n1. The key idea of using the global data manifold into the robustifying framework is quite interesting.\n2. Experiments demonstrate good empirical benefits of the approach.\n\nCons: \n1. While, the paper seemed well organized in the beginning, I got totally lost with Eq. (4-5). As I see, this objective is inaccurate and needs significant refinement. Specifically, it is unclear how is x^{adv} is related to x, and how is x^{adv} related to P*_theta? The paper tries to explain this objective in the paragraph below, but the explanation is very confusing as well.  A few other things that could help here:\na) It is said that \"Q_theta and P_\\theta* are the latent distributions induced by the natural example x\". How can a single data point induce a distribution? Do you assume the feature map from a hidden layer of a network represents a distribution? If so, in what sense? \nb) \"The adversarial example is crafted to induce the worst case distribution P*\". How is it crafted and what is the relation between P* and x? This is the key connection that is missing from (4-5).\n\n2. Moving along, Section 4.1 is organized very poorly as well. I believe too many concepts are tied together into one formulation in (6), making it hard to decipher. For example, why to include the classifier D^{1:C} within this formulation? Why not talk about it elsewhere and focus on the meat of the objective, systematically? \n\n3. Further, as I understand, x^{adv} is the first step that happens in (6), however, there is no \"adversary\" in this case, instead is finding a perturbed sample x' that maximizes the f-divergence. In what sense is x^{adv} then an adversarial sample? Perhaps the paper should re-define what is the definition of an adversarial example that it is using, to clearly state what the idea is. Technically, there is no requirement that the point x^{adv} found by this step will promote any data misclassification; however can be any point that is within a B(x,\\eps) ball from x, and that happens to maximize this divergence loss. Note that none of the other components D_W, f_theta, etc. are well trained in doing this optimization. So they could also be sub-optimal (in the sense of what the paper argues in the beginning of Page 4).\n\n4. Why is the middle formula in (6) minimizing over W to have both x and x^adv matched with the same label? Again, where is the adversary here? Or for that matter, how will the proposed approach achieve adversarial robustness ? \n\nMinor comments:\na. What is \\tau and T in (3)? \nb. How is f_\\theta defined in (6)? \nc. The paper writes that back and forth that there is no use of label information in the setup, however has labels used in discriminator in (6). This is very confusing. \nd. There is also reference to data manifold and manifold label in Figure 2, but these are not clearly explained. What precisely is the data manifold? Is it the latent distribution for a specific label? \ne. Page 4, top para: \"without considering the inter-relationship between data samples\". Won't this relation be captured implicitly through the neural network parameters theta when perturbations on all the samples are used in the training process?\n\nOverall, I think this paper needs a thorough revision to explain well its technical contributions. \n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3224/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3224/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improving Model Robustness with Latent Distribution Locally and Globally", "authorids": ["~Zhuang_QIAN1", "~Shufei_Zhang1", "~Kaizhu_Huang1", "~Qiufeng_Wang2", "~Rui_Zhang10", "xinping.yi@liverpool.ac.uk"], "authors": ["Zhuang QIAN", "Shufei Zhang", "Kaizhu Huang", "Qiufeng Wang", "Rui Zhang", "Xinping Yi"], "keywords": ["adversarial example", "robustness", "data manifold", "adversarial training"], "abstract": "We propose a novel adversarial training method which leverages both the local and global information to defend  adversarial attacks. Existing adversarial training methods usually generate adversarial perturbations  locally in a supervised manner and fail to consider the data manifold information in a global way. Consequently, the resulting  adversarial examples may corrupt the underlying data structure and are typically biased towards the decision boundary. In this work, we exploit both the local and global information of data manifold to generate adversarial examples in an unsupervised manner. Specifically, we design our novel framework  via an adversarial game between  a discriminator and a classifier: the discriminator is  learned to differentiate the latent distributions  of the natural data and the perturbed counterpart, while the classifier is trained to recognize accurately the perturbed examples as well as enforcing the invariance between the two latent distributions. We conduct a series of analysis on the model robustness and also verify the effectiveness of our proposed method empirically. Experimental results show that our method  substantially outperforms the recent state-of-the-art (i.e. Feature Scattering) in defending adversarial attacks  by a large accuracy margin  (e.g. $17.0\\%$ and $18.1\\%$ on SVHN dataset, $9.3\\%$ and $17.4\\%$ on CIFAR-10 dataset, $6.0\\%$ and $16.2\\%$ on CIFAR-100 dataset for defending PGD20 and CW20 attacks respectively).", "one-sentence_summary": "We propose a novel adversarial training method which leverages both the local and global information to defend  adversarial attacks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "qian|improving_model_robustness_with_latent_distribution_locally_and_globally", "supplementary_material": "/attachment/da9cf6d020cff31017c09ffce30d2883e395d6ba.zip", "pdf": "/pdf/4648d81b527d927139e1f5ecf6d37e8f597f441e.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=Hak4wSTLQa", "_bibtex": "@misc{\nqian2021improving,\ntitle={Improving Model Robustness with Latent Distribution Locally and Globally},\nauthor={Zhuang QIAN and Shufei Zhang and Kaizhu Huang and Qiufeng Wang and Rui Zhang and Xinping Yi},\nyear={2021},\nurl={https://openreview.net/forum?id=yvuk0RsLoP7}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "yvuk0RsLoP7", "replyto": "yvuk0RsLoP7", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3224/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538079711, "tmdate": 1606915760322, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3224/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3224/-/Official_Review"}}}, {"id": "cIp99ASc2ES", "original": null, "number": 2, "cdate": 1604014226077, "ddate": null, "tcdate": 1604014226077, "tmdate": 1605024042778, "tddate": null, "forum": "yvuk0RsLoP7", "replyto": "yvuk0RsLoP7", "invitation": "ICLR.cc/2021/Conference/Paper3224/-/Official_Review", "content": {"title": "Use global latent distribution to improve model robustness", "review": "The paper proposes a new method of improving model robustness by generating adversarial samples that are regularized by their latent distribution through f-divergence, whereas existing literature only uses local manifold property such as smoothness. \n\nThe method is well-motivated and the clarity of the paper is good. The experimental results are compared with several competitive baselines and the improvement looks significant (Although I am not familiar with the state-of-the-art experimental results). \n\nProofread is needed for the sentence \"The adversarial examples are crafted by ... \" on page 2 and several other small typos. ", "rating": "7: Good paper, accept", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2021/Conference/Paper3224/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3224/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improving Model Robustness with Latent Distribution Locally and Globally", "authorids": ["~Zhuang_QIAN1", "~Shufei_Zhang1", "~Kaizhu_Huang1", "~Qiufeng_Wang2", "~Rui_Zhang10", "xinping.yi@liverpool.ac.uk"], "authors": ["Zhuang QIAN", "Shufei Zhang", "Kaizhu Huang", "Qiufeng Wang", "Rui Zhang", "Xinping Yi"], "keywords": ["adversarial example", "robustness", "data manifold", "adversarial training"], "abstract": "We propose a novel adversarial training method which leverages both the local and global information to defend  adversarial attacks. Existing adversarial training methods usually generate adversarial perturbations  locally in a supervised manner and fail to consider the data manifold information in a global way. Consequently, the resulting  adversarial examples may corrupt the underlying data structure and are typically biased towards the decision boundary. In this work, we exploit both the local and global information of data manifold to generate adversarial examples in an unsupervised manner. Specifically, we design our novel framework  via an adversarial game between  a discriminator and a classifier: the discriminator is  learned to differentiate the latent distributions  of the natural data and the perturbed counterpart, while the classifier is trained to recognize accurately the perturbed examples as well as enforcing the invariance between the two latent distributions. We conduct a series of analysis on the model robustness and also verify the effectiveness of our proposed method empirically. Experimental results show that our method  substantially outperforms the recent state-of-the-art (i.e. Feature Scattering) in defending adversarial attacks  by a large accuracy margin  (e.g. $17.0\\%$ and $18.1\\%$ on SVHN dataset, $9.3\\%$ and $17.4\\%$ on CIFAR-10 dataset, $6.0\\%$ and $16.2\\%$ on CIFAR-100 dataset for defending PGD20 and CW20 attacks respectively).", "one-sentence_summary": "We propose a novel adversarial training method which leverages both the local and global information to defend  adversarial attacks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "qian|improving_model_robustness_with_latent_distribution_locally_and_globally", "supplementary_material": "/attachment/da9cf6d020cff31017c09ffce30d2883e395d6ba.zip", "pdf": "/pdf/4648d81b527d927139e1f5ecf6d37e8f597f441e.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=Hak4wSTLQa", "_bibtex": "@misc{\nqian2021improving,\ntitle={Improving Model Robustness with Latent Distribution Locally and Globally},\nauthor={Zhuang QIAN and Shufei Zhang and Kaizhu Huang and Qiufeng Wang and Rui Zhang and Xinping Yi},\nyear={2021},\nurl={https://openreview.net/forum?id=yvuk0RsLoP7}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "yvuk0RsLoP7", "replyto": "yvuk0RsLoP7", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3224/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538079711, "tmdate": 1606915760322, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3224/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3224/-/Official_Review"}}}, {"id": "EIlDln-C5Ne", "original": null, "number": 4, "cdate": 1604123461979, "ddate": null, "tcdate": 1604123461979, "tmdate": 1605024042707, "tddate": null, "forum": "yvuk0RsLoP7", "replyto": "yvuk0RsLoP7", "invitation": "ICLR.cc/2021/Conference/Paper3224/-/Official_Review", "content": {"title": "The paper analyzes the property of local and global data manifold for adversarial training. ", "review": "The paper analyzes the property of local and global data manifold for adversarial training. In particular, they used a discriminator-classifier model, where the discriminator tries to differentiate between the natural and adversarial space, and the classifier aims to classify between them while maintaining the constraints between local and global distributions. The authors implemented the proposed method on several datasets and achieved good performance. They also compared with several whitebox and blackbox methods and proved superiority. \n\nThis paper was, in general, well written. The authors provided a good visualization of their analysis. Using local and global information for adversarial training is intuitive. The authors provided a good theoretical background to establish their method. The empirical evaluations show promising results. \n\nSome major concerns are listed as follows:\n1. It is not clear how equations 4 and 5 are realized using discriminator and classifier. \n2. What kind of perturbations are chosen? It looks like all the experiments are with L-infinity. Does this observation hold for other ones?\n3.  If the attackers leverage the global and local data manifold, can they bypass this attack? ", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3224/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3224/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improving Model Robustness with Latent Distribution Locally and Globally", "authorids": ["~Zhuang_QIAN1", "~Shufei_Zhang1", "~Kaizhu_Huang1", "~Qiufeng_Wang2", "~Rui_Zhang10", "xinping.yi@liverpool.ac.uk"], "authors": ["Zhuang QIAN", "Shufei Zhang", "Kaizhu Huang", "Qiufeng Wang", "Rui Zhang", "Xinping Yi"], "keywords": ["adversarial example", "robustness", "data manifold", "adversarial training"], "abstract": "We propose a novel adversarial training method which leverages both the local and global information to defend  adversarial attacks. Existing adversarial training methods usually generate adversarial perturbations  locally in a supervised manner and fail to consider the data manifold information in a global way. Consequently, the resulting  adversarial examples may corrupt the underlying data structure and are typically biased towards the decision boundary. In this work, we exploit both the local and global information of data manifold to generate adversarial examples in an unsupervised manner. Specifically, we design our novel framework  via an adversarial game between  a discriminator and a classifier: the discriminator is  learned to differentiate the latent distributions  of the natural data and the perturbed counterpart, while the classifier is trained to recognize accurately the perturbed examples as well as enforcing the invariance between the two latent distributions. We conduct a series of analysis on the model robustness and also verify the effectiveness of our proposed method empirically. Experimental results show that our method  substantially outperforms the recent state-of-the-art (i.e. Feature Scattering) in defending adversarial attacks  by a large accuracy margin  (e.g. $17.0\\%$ and $18.1\\%$ on SVHN dataset, $9.3\\%$ and $17.4\\%$ on CIFAR-10 dataset, $6.0\\%$ and $16.2\\%$ on CIFAR-100 dataset for defending PGD20 and CW20 attacks respectively).", "one-sentence_summary": "We propose a novel adversarial training method which leverages both the local and global information to defend  adversarial attacks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "qian|improving_model_robustness_with_latent_distribution_locally_and_globally", "supplementary_material": "/attachment/da9cf6d020cff31017c09ffce30d2883e395d6ba.zip", "pdf": "/pdf/4648d81b527d927139e1f5ecf6d37e8f597f441e.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=Hak4wSTLQa", "_bibtex": "@misc{\nqian2021improving,\ntitle={Improving Model Robustness with Latent Distribution Locally and Globally},\nauthor={Zhuang QIAN and Shufei Zhang and Kaizhu Huang and Qiufeng Wang and Rui Zhang and Xinping Yi},\nyear={2021},\nurl={https://openreview.net/forum?id=yvuk0RsLoP7}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "yvuk0RsLoP7", "replyto": "yvuk0RsLoP7", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3224/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538079711, "tmdate": 1606915760322, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3224/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3224/-/Official_Review"}}}, {"id": "pEPBJlVSWN5", "original": null, "number": 3, "cdate": 1604058060996, "ddate": null, "tcdate": 1604058060996, "tmdate": 1605024042622, "tddate": null, "forum": "yvuk0RsLoP7", "replyto": "yvuk0RsLoP7", "invitation": "ICLR.cc/2021/Conference/Paper3224/-/Official_Review", "content": {"title": "Official Blind Review # 4", "review": "Summary: This paper considers the local and global information in adversarial attacks for adversarial training, where the authors design an adversarial framework containing a discriminator and a classifier. The idea is interesting and the paper is easy to follow. \n\nHowever, I have still some concerns below: \n- The novelty of this work combines the idea of PGD (local information) and Feature-Scatter (global information) .\n- More importantly, the evaluation is no enough, even though Feature-Scatter considers the global information, but many attack methods have shown the robustness of Feature-Scatter was overestimated, such as [1][2][3] and so on. So I think evaluating on PGD and CW  is not enough.\n- There are few analysis experiments for the proposed method, more analysis experiments are needed besides the comparision.\n\n[1] Feature Attack: https://openreview.net/forum?id=Syejj0NYvr&noteId=rkeBhuBMjS\n\n[2] RayS: A Ray Searching Method for Hard-label Adversarial Attack. KDD 2020.\n\n[3] Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks. ICML 2020.", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3224/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3224/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improving Model Robustness with Latent Distribution Locally and Globally", "authorids": ["~Zhuang_QIAN1", "~Shufei_Zhang1", "~Kaizhu_Huang1", "~Qiufeng_Wang2", "~Rui_Zhang10", "xinping.yi@liverpool.ac.uk"], "authors": ["Zhuang QIAN", "Shufei Zhang", "Kaizhu Huang", "Qiufeng Wang", "Rui Zhang", "Xinping Yi"], "keywords": ["adversarial example", "robustness", "data manifold", "adversarial training"], "abstract": "We propose a novel adversarial training method which leverages both the local and global information to defend  adversarial attacks. Existing adversarial training methods usually generate adversarial perturbations  locally in a supervised manner and fail to consider the data manifold information in a global way. Consequently, the resulting  adversarial examples may corrupt the underlying data structure and are typically biased towards the decision boundary. In this work, we exploit both the local and global information of data manifold to generate adversarial examples in an unsupervised manner. Specifically, we design our novel framework  via an adversarial game between  a discriminator and a classifier: the discriminator is  learned to differentiate the latent distributions  of the natural data and the perturbed counterpart, while the classifier is trained to recognize accurately the perturbed examples as well as enforcing the invariance between the two latent distributions. We conduct a series of analysis on the model robustness and also verify the effectiveness of our proposed method empirically. Experimental results show that our method  substantially outperforms the recent state-of-the-art (i.e. Feature Scattering) in defending adversarial attacks  by a large accuracy margin  (e.g. $17.0\\%$ and $18.1\\%$ on SVHN dataset, $9.3\\%$ and $17.4\\%$ on CIFAR-10 dataset, $6.0\\%$ and $16.2\\%$ on CIFAR-100 dataset for defending PGD20 and CW20 attacks respectively).", "one-sentence_summary": "We propose a novel adversarial training method which leverages both the local and global information to defend  adversarial attacks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "qian|improving_model_robustness_with_latent_distribution_locally_and_globally", "supplementary_material": "/attachment/da9cf6d020cff31017c09ffce30d2883e395d6ba.zip", "pdf": "/pdf/4648d81b527d927139e1f5ecf6d37e8f597f441e.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=Hak4wSTLQa", "_bibtex": "@misc{\nqian2021improving,\ntitle={Improving Model Robustness with Latent Distribution Locally and Globally},\nauthor={Zhuang QIAN and Shufei Zhang and Kaizhu Huang and Qiufeng Wang and Rui Zhang and Xinping Yi},\nyear={2021},\nurl={https://openreview.net/forum?id=yvuk0RsLoP7}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "yvuk0RsLoP7", "replyto": "yvuk0RsLoP7", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3224/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538079711, "tmdate": 1606915760322, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3224/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3224/-/Official_Review"}}}], "count": 15}