{"notes": [{"id": "HyG1_j0cYQ", "original": "H1eog46PK7", "number": 317, "cdate": 1538087782990, "ddate": null, "tcdate": 1538087782990, "tmdate": 1545355394261, "tddate": null, "forum": "HyG1_j0cYQ", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Pumpout: A Meta Approach for Robustly Training Deep Neural Networks with Noisy Labels", "abstract": "It is challenging to train deep neural networks robustly on the industrial-level data, since labels of such data are heavily noisy, and their label generation processes are normally agnostic. To handle these issues, by using the memorization effects of deep neural networks, we may train deep neural networks on the whole dataset only the first few iterations. Then, we may employ early stopping or the small-loss trick to train them on selected instances. However, in such training procedures, deep neural networks inevitably memorize some noisy labels, which will degrade their generalization. In this paper, we propose a meta algorithm called Pumpout to overcome the problem of memorizing noisy labels. By using scaled stochastic gradient ascent, Pumpout actively squeezes out the negative effects of noisy labels from the training model, instead of passively forgetting these effects. We leverage Pumpout to upgrade two representative methods: MentorNet and Backward Correction. Empirical results on benchmark vision and text datasets demonstrate that Pumpout can significantly improve the robustness of representative methods.", "keywords": ["Noisy Labels", "Deep Learning", "Meta Approach"], "authorids": ["bo.han@riken.jp", "gang.niu@riken.jp", "jiangchao.yao@student.uts.edu.au", "xingrui.yu@student.uts.edu.au", "miao.xu@riken.jp", "ivor.tsang@uts.edu.au", "sugi@k.u-tokyo.ac.jp"], "authors": ["Bo Han", "Gang Niu", "Jiangchao Yao", "Xingrui Yu", "Miao Xu", "Ivor Tsang", "Masashi Sugiyama"], "TL;DR": "Starting from tomorrow, never worry about your DNNs memorizing noisy labels---forget bad labels by Pumpout in an active manner!", "pdf": "/pdf/527948693e0b46f29236bb6e1af3355962881a51.pdf", "paperhash": "han|pumpout_a_meta_approach_for_robustly_training_deep_neural_networks_with_noisy_labels", "_bibtex": "@misc{\nhan2019pumpout,\ntitle={Pumpout: A Meta Approach for Robustly Training Deep Neural Networks with Noisy Labels},\nauthor={Bo Han and Gang Niu and Jiangchao Yao and Xingrui Yu and Miao Xu and Ivor Tsang and Masashi Sugiyama},\nyear={2019},\nurl={https://openreview.net/forum?id=HyG1_j0cYQ},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 27, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "ryg8VaFelE", "original": null, "number": 1, "cdate": 1544752429709, "ddate": null, "tcdate": 1544752429709, "tmdate": 1545354517069, "tddate": null, "forum": "HyG1_j0cYQ", "replyto": "HyG1_j0cYQ", "invitation": "ICLR.cc/2019/Conference/-/Paper317/Meta_Review", "content": {"metareview": "The paper presents an approach to mitigate the presence of noisy labels during\ntraining by trying to forget wrong labels. Reviewers pointed out a few\nconcerns, including lack of novelty, lack of enough experimental support, and\nlack of theoretical support. Authors have added some experiments and details\nabout the experimental section, but reviewers still think it's not enough\nfor acceptance. I concur with the reviewers to reject the paper.", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Reject", "title": "meta-review"}, "signatures": ["ICLR.cc/2019/Conference/Paper317/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper317/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pumpout: A Meta Approach for Robustly Training Deep Neural Networks with Noisy Labels", "abstract": "It is challenging to train deep neural networks robustly on the industrial-level data, since labels of such data are heavily noisy, and their label generation processes are normally agnostic. To handle these issues, by using the memorization effects of deep neural networks, we may train deep neural networks on the whole dataset only the first few iterations. Then, we may employ early stopping or the small-loss trick to train them on selected instances. However, in such training procedures, deep neural networks inevitably memorize some noisy labels, which will degrade their generalization. In this paper, we propose a meta algorithm called Pumpout to overcome the problem of memorizing noisy labels. By using scaled stochastic gradient ascent, Pumpout actively squeezes out the negative effects of noisy labels from the training model, instead of passively forgetting these effects. We leverage Pumpout to upgrade two representative methods: MentorNet and Backward Correction. Empirical results on benchmark vision and text datasets demonstrate that Pumpout can significantly improve the robustness of representative methods.", "keywords": ["Noisy Labels", "Deep Learning", "Meta Approach"], "authorids": ["bo.han@riken.jp", "gang.niu@riken.jp", "jiangchao.yao@student.uts.edu.au", "xingrui.yu@student.uts.edu.au", "miao.xu@riken.jp", "ivor.tsang@uts.edu.au", "sugi@k.u-tokyo.ac.jp"], "authors": ["Bo Han", "Gang Niu", "Jiangchao Yao", "Xingrui Yu", "Miao Xu", "Ivor Tsang", "Masashi Sugiyama"], "TL;DR": "Starting from tomorrow, never worry about your DNNs memorizing noisy labels---forget bad labels by Pumpout in an active manner!", "pdf": "/pdf/527948693e0b46f29236bb6e1af3355962881a51.pdf", "paperhash": "han|pumpout_a_meta_approach_for_robustly_training_deep_neural_networks_with_noisy_labels", "_bibtex": "@misc{\nhan2019pumpout,\ntitle={Pumpout: A Meta Approach for Robustly Training Deep Neural Networks with Noisy Labels},\nauthor={Bo Han and Gang Niu and Jiangchao Yao and Xingrui Yu and Miao Xu and Ivor Tsang and Masashi Sugiyama},\nyear={2019},\nurl={https://openreview.net/forum?id=HyG1_j0cYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper317/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353259683, "tddate": null, "super": null, "final": null, "reply": {"forum": "HyG1_j0cYQ", "replyto": "HyG1_j0cYQ", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper317/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper317/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper317/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353259683}}}, {"id": "S1eMFCbt0m", "original": null, "number": 18, "cdate": 1543212666450, "ddate": null, "tcdate": 1543212666450, "tmdate": 1543212666450, "tddate": null, "forum": "HyG1_j0cYQ", "replyto": "HyG1_j0cYQ", "invitation": "ICLR.cc/2019/Conference/-/Paper317/Official_Comment", "content": {"title": "Summary of Changes", "comment": "Dear Area Chair and Anonymous Reviewers,\n\nOn behalf of all co-authors, we appreciate your great efforts in our paper review. Except our point to point response to each reviewer (see details in following posts), we hope to highlight several important points that we revised in the high level.\n\n1. To further justify our Pumpout idea empirically,  we add a text dataset called NEWS, and conduct the corresponding experiments on Pumpout_{SL} and Pumpout_{BC}. This dataset is very important to justify that our Pumpout can be leveraged not only in vision tasks but also in text tasks, and various datasets will be more convincing (AnonReviewer1 and AnonReviewer3).\n\n2. To remedy the concerns to our experiments, we add a lot of detailed explanations in Section 4. For example, 1) \"Note that, the focus of our paper is to explore the efficacy of Pumpout. Therefore, we use Adam optimizer in all experiments for fair comparison without using data augmentation trick (Zhang & Sabuncu, 2018; Ma et al., 2018).\"; 2) \"Note that, the choice of two baselines is to justify whether Pumpout can benefit representative state-of-the-art algorithms. The readers are encouraged to upgrade other methods, such as Reed et al. (2015), Goldberger & Ben-Reuven (2017), and Kiryo et al. (2017) by using Pumpout.\". We just showcase 2 points here, and more explanations have been merged into the revised paper.\n\n3. To make readers easily know our algorithms, we changed the structure of Section 3. We first introduce the background of MentorNet (Backward Correction). Then, we propose  our Pumpout_{SL} (Pumpout_{BC}). Lastly, we explain the relations between MentorNet (Backward Correction) and Pumpout_{SL} (Pumpout_{BC}).\n\nTo sum up, we try our best to revise the whole paper, and hope reviewer can feed us more suggestive comments in order to make our paper better. Many thanks for all your efforts!!!\n\nRegards,\nThe authors"}, "signatures": ["ICLR.cc/2019/Conference/Paper317/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper317/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper317/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pumpout: A Meta Approach for Robustly Training Deep Neural Networks with Noisy Labels", "abstract": "It is challenging to train deep neural networks robustly on the industrial-level data, since labels of such data are heavily noisy, and their label generation processes are normally agnostic. To handle these issues, by using the memorization effects of deep neural networks, we may train deep neural networks on the whole dataset only the first few iterations. Then, we may employ early stopping or the small-loss trick to train them on selected instances. However, in such training procedures, deep neural networks inevitably memorize some noisy labels, which will degrade their generalization. In this paper, we propose a meta algorithm called Pumpout to overcome the problem of memorizing noisy labels. By using scaled stochastic gradient ascent, Pumpout actively squeezes out the negative effects of noisy labels from the training model, instead of passively forgetting these effects. We leverage Pumpout to upgrade two representative methods: MentorNet and Backward Correction. Empirical results on benchmark vision and text datasets demonstrate that Pumpout can significantly improve the robustness of representative methods.", "keywords": ["Noisy Labels", "Deep Learning", "Meta Approach"], "authorids": ["bo.han@riken.jp", "gang.niu@riken.jp", "jiangchao.yao@student.uts.edu.au", "xingrui.yu@student.uts.edu.au", "miao.xu@riken.jp", "ivor.tsang@uts.edu.au", "sugi@k.u-tokyo.ac.jp"], "authors": ["Bo Han", "Gang Niu", "Jiangchao Yao", "Xingrui Yu", "Miao Xu", "Ivor Tsang", "Masashi Sugiyama"], "TL;DR": "Starting from tomorrow, never worry about your DNNs memorizing noisy labels---forget bad labels by Pumpout in an active manner!", "pdf": "/pdf/527948693e0b46f29236bb6e1af3355962881a51.pdf", "paperhash": "han|pumpout_a_meta_approach_for_robustly_training_deep_neural_networks_with_noisy_labels", "_bibtex": "@misc{\nhan2019pumpout,\ntitle={Pumpout: A Meta Approach for Robustly Training Deep Neural Networks with Noisy Labels},\nauthor={Bo Han and Gang Niu and Jiangchao Yao and Xingrui Yu and Miao Xu and Ivor Tsang and Masashi Sugiyama},\nyear={2019},\nurl={https://openreview.net/forum?id=HyG1_j0cYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper317/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621611871, "tddate": null, "super": null, "final": null, "reply": {"forum": "HyG1_j0cYQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper317/Authors", "ICLR.cc/2019/Conference/Paper317/Reviewers", "ICLR.cc/2019/Conference/Paper317/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper317/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper317/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper317/Authors|ICLR.cc/2019/Conference/Paper317/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper317/Reviewers", "ICLR.cc/2019/Conference/Paper317/Authors", "ICLR.cc/2019/Conference/Paper317/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621611871}}}, {"id": "r1l3TQxVAQ", "original": null, "number": 16, "cdate": 1542878148497, "ddate": null, "tcdate": 1542878148497, "tmdate": 1542881035528, "tddate": null, "forum": "HyG1_j0cYQ", "replyto": "SJxMUmdJnm", "invitation": "ICLR.cc/2019/Conference/-/Paper317/Official_Comment", "content": {"title": "My main concern is related to the experiment results. The results of the baseline method look strange. Why there is a strong decrease in the MNIST test accuracy after 20 epochs?", "comment": "We are sorry your concerns. However, the degradation of the test accuracy in the early stage of training is due to the memorization effects of deep networks. Namely, the model first learns the simple and general patterns of the real data before over-fitting and memorizing the noise (which results in decreasing test accuracy). This observation has been well demonstrated by a lot of researchers recently (e.g. [1-3]), and this observation has become a common sense. From Figure 7(b) of [2] and Figures 3, 5, 6 of [3], we can clearly see the same phenomenon of memorization effects of deep networks on noisy labels.\n\nActually, the degradation of Figure 2(c) is small. Please note that the lowest Y-axis is 0.88. The performance degradation of this case is about 0.1, which is similar to [2] and [3].\n\nBy the way, to the best of our knowledge, we are not aware of a top conference/journal paper in this area claiming that \"Standard training of neural network is very robust to label noise.\" Could you please point out a paper for our reference?\n\nReferences:\n[1] Zhang C, et al. Understanding deep learning requires rethinking generalization. In ICLR, 2017.\n[2] Arpit D, et al. A closer look at memorization in deep networks. In ICML, 2017.\n[3] Han B, et al. Co-teaching: Robust training of deep neural networks with extremely noisy labels. In NeurIPS, 2018."}, "signatures": ["ICLR.cc/2019/Conference/Paper317/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper317/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper317/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pumpout: A Meta Approach for Robustly Training Deep Neural Networks with Noisy Labels", "abstract": "It is challenging to train deep neural networks robustly on the industrial-level data, since labels of such data are heavily noisy, and their label generation processes are normally agnostic. To handle these issues, by using the memorization effects of deep neural networks, we may train deep neural networks on the whole dataset only the first few iterations. Then, we may employ early stopping or the small-loss trick to train them on selected instances. However, in such training procedures, deep neural networks inevitably memorize some noisy labels, which will degrade their generalization. In this paper, we propose a meta algorithm called Pumpout to overcome the problem of memorizing noisy labels. By using scaled stochastic gradient ascent, Pumpout actively squeezes out the negative effects of noisy labels from the training model, instead of passively forgetting these effects. We leverage Pumpout to upgrade two representative methods: MentorNet and Backward Correction. Empirical results on benchmark vision and text datasets demonstrate that Pumpout can significantly improve the robustness of representative methods.", "keywords": ["Noisy Labels", "Deep Learning", "Meta Approach"], "authorids": ["bo.han@riken.jp", "gang.niu@riken.jp", "jiangchao.yao@student.uts.edu.au", "xingrui.yu@student.uts.edu.au", "miao.xu@riken.jp", "ivor.tsang@uts.edu.au", "sugi@k.u-tokyo.ac.jp"], "authors": ["Bo Han", "Gang Niu", "Jiangchao Yao", "Xingrui Yu", "Miao Xu", "Ivor Tsang", "Masashi Sugiyama"], "TL;DR": "Starting from tomorrow, never worry about your DNNs memorizing noisy labels---forget bad labels by Pumpout in an active manner!", "pdf": "/pdf/527948693e0b46f29236bb6e1af3355962881a51.pdf", "paperhash": "han|pumpout_a_meta_approach_for_robustly_training_deep_neural_networks_with_noisy_labels", "_bibtex": "@misc{\nhan2019pumpout,\ntitle={Pumpout: A Meta Approach for Robustly Training Deep Neural Networks with Noisy Labels},\nauthor={Bo Han and Gang Niu and Jiangchao Yao and Xingrui Yu and Miao Xu and Ivor Tsang and Masashi Sugiyama},\nyear={2019},\nurl={https://openreview.net/forum?id=HyG1_j0cYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper317/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621611871, "tddate": null, "super": null, "final": null, "reply": {"forum": "HyG1_j0cYQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper317/Authors", "ICLR.cc/2019/Conference/Paper317/Reviewers", "ICLR.cc/2019/Conference/Paper317/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper317/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper317/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper317/Authors|ICLR.cc/2019/Conference/Paper317/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper317/Reviewers", "ICLR.cc/2019/Conference/Paper317/Authors", "ICLR.cc/2019/Conference/Paper317/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621611871}}}, {"id": "BJlAGblVRQ", "original": null, "number": 14, "cdate": 1542877461621, "ddate": null, "tcdate": 1542877461621, "tmdate": 1542880981784, "tddate": null, "forum": "HyG1_j0cYQ", "replyto": "SJeMcexNRm", "invitation": "ICLR.cc/2019/Conference/-/Paper317/Official_Comment", "content": {"title": "Regarding the presentation, in section 3, I suggest to move the explanation of MentorNet and BackwardCorrection before their upgrade by PumpOut.", "comment": "Thanks for your comments. The background of MentorNet and Backward Correction has been moved before their upgraded version via Pumpout. Please check the new organization of Section~3. We have modified the Table 1 by introducing the details of two vision datasets (\\textit{MNIST} and \\textit{CIFAR-10}) and one text dataset (\\textit{NEWS}), and added the description of the activation function \"Leaky ReLu (LReLU) activation function\"."}, "signatures": ["ICLR.cc/2019/Conference/Paper317/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper317/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper317/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pumpout: A Meta Approach for Robustly Training Deep Neural Networks with Noisy Labels", "abstract": "It is challenging to train deep neural networks robustly on the industrial-level data, since labels of such data are heavily noisy, and their label generation processes are normally agnostic. To handle these issues, by using the memorization effects of deep neural networks, we may train deep neural networks on the whole dataset only the first few iterations. Then, we may employ early stopping or the small-loss trick to train them on selected instances. However, in such training procedures, deep neural networks inevitably memorize some noisy labels, which will degrade their generalization. In this paper, we propose a meta algorithm called Pumpout to overcome the problem of memorizing noisy labels. By using scaled stochastic gradient ascent, Pumpout actively squeezes out the negative effects of noisy labels from the training model, instead of passively forgetting these effects. We leverage Pumpout to upgrade two representative methods: MentorNet and Backward Correction. Empirical results on benchmark vision and text datasets demonstrate that Pumpout can significantly improve the robustness of representative methods.", "keywords": ["Noisy Labels", "Deep Learning", "Meta Approach"], "authorids": ["bo.han@riken.jp", "gang.niu@riken.jp", "jiangchao.yao@student.uts.edu.au", "xingrui.yu@student.uts.edu.au", "miao.xu@riken.jp", "ivor.tsang@uts.edu.au", "sugi@k.u-tokyo.ac.jp"], "authors": ["Bo Han", "Gang Niu", "Jiangchao Yao", "Xingrui Yu", "Miao Xu", "Ivor Tsang", "Masashi Sugiyama"], "TL;DR": "Starting from tomorrow, never worry about your DNNs memorizing noisy labels---forget bad labels by Pumpout in an active manner!", "pdf": "/pdf/527948693e0b46f29236bb6e1af3355962881a51.pdf", "paperhash": "han|pumpout_a_meta_approach_for_robustly_training_deep_neural_networks_with_noisy_labels", "_bibtex": "@misc{\nhan2019pumpout,\ntitle={Pumpout: A Meta Approach for Robustly Training Deep Neural Networks with Noisy Labels},\nauthor={Bo Han and Gang Niu and Jiangchao Yao and Xingrui Yu and Miao Xu and Ivor Tsang and Masashi Sugiyama},\nyear={2019},\nurl={https://openreview.net/forum?id=HyG1_j0cYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper317/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621611871, "tddate": null, "super": null, "final": null, "reply": {"forum": "HyG1_j0cYQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper317/Authors", "ICLR.cc/2019/Conference/Paper317/Reviewers", "ICLR.cc/2019/Conference/Paper317/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper317/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper317/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper317/Authors|ICLR.cc/2019/Conference/Paper317/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper317/Reviewers", "ICLR.cc/2019/Conference/Paper317/Authors", "ICLR.cc/2019/Conference/Paper317/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621611871}}}, {"id": "ryltU0y4RX", "original": null, "number": 8, "cdate": 1542876752652, "ddate": null, "tcdate": 1542876752652, "tmdate": 1542880774220, "tddate": null, "forum": "HyG1_j0cYQ", "replyto": "ByxT2oFn2X", "invitation": "ICLR.cc/2019/Conference/-/Paper317/Official_Comment", "content": {"title": "The idea using instance selection is not new. The novelty could be improved.", "comment": "Thanks for your suggestion. Recently, in the area of deep learning with noisy labels, training on selected instances is a totally new direction, which attracts a lot of attention [1-3]. However, once some false-positive instances (real noisy data) are selected during training, DNNs will memorize them finally, which inevitably degrades the generalization performance (i.e., test accuracy) in the test phase. The key novelty of this paper is how to actively mitigate memorizing the negative effects of noisy labels, instead of following the path of training on selected instances.\n\nSpecifically, to address such an issue, we introduce a meta approach called Pumpout. Intuitively, it squeezes the negative effects of noise labels actively by scaled stochastic gradient ascent. We can leverage Pumpout to upgrade orthogonal methods, such as MentorNet [1] (training on selected instances) and Backward Correction [4] (estimating the noise transition matrix).\n\nTo the best of our knowledge, it is the first attempt in deep learning that studies \"how to forget memorized information in an active manner\", which is critical to improve the performance of some existing state-of-the-art methods. Nevertheless, we agree that some theoretical justification for Pumpout can be useful to understand and improve our method. We leave this in future works.\n\nReferences:\n[1] Jiang L, Zhou Z, Leung T, et al. MentorNet: Learning data-driven curriculum for very deep neural networks on corrupted labels. In ICML, 2018.\n[2] Ren M, Zeng W, Yang B, Urtasun R. Learning to reweight examples for robust deep learning. In ICML, 2018.\n[3] Han B, Yao Q, Yu X, Niu G, et al. Co-teaching: Robust training of deep Neural networks with extremely noisy labels. In NeurIPS, 2018.\n[4] Patrini G, Rozza A, Menon A, et al. Making deep neural networks robust to label noise: A loss correction approach. In CVPR, 2017."}, "signatures": ["ICLR.cc/2019/Conference/Paper317/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper317/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper317/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pumpout: A Meta Approach for Robustly Training Deep Neural Networks with Noisy Labels", "abstract": "It is challenging to train deep neural networks robustly on the industrial-level data, since labels of such data are heavily noisy, and their label generation processes are normally agnostic. To handle these issues, by using the memorization effects of deep neural networks, we may train deep neural networks on the whole dataset only the first few iterations. Then, we may employ early stopping or the small-loss trick to train them on selected instances. However, in such training procedures, deep neural networks inevitably memorize some noisy labels, which will degrade their generalization. In this paper, we propose a meta algorithm called Pumpout to overcome the problem of memorizing noisy labels. By using scaled stochastic gradient ascent, Pumpout actively squeezes out the negative effects of noisy labels from the training model, instead of passively forgetting these effects. We leverage Pumpout to upgrade two representative methods: MentorNet and Backward Correction. Empirical results on benchmark vision and text datasets demonstrate that Pumpout can significantly improve the robustness of representative methods.", "keywords": ["Noisy Labels", "Deep Learning", "Meta Approach"], "authorids": ["bo.han@riken.jp", "gang.niu@riken.jp", "jiangchao.yao@student.uts.edu.au", "xingrui.yu@student.uts.edu.au", "miao.xu@riken.jp", "ivor.tsang@uts.edu.au", "sugi@k.u-tokyo.ac.jp"], "authors": ["Bo Han", "Gang Niu", "Jiangchao Yao", "Xingrui Yu", "Miao Xu", "Ivor Tsang", "Masashi Sugiyama"], "TL;DR": "Starting from tomorrow, never worry about your DNNs memorizing noisy labels---forget bad labels by Pumpout in an active manner!", "pdf": "/pdf/527948693e0b46f29236bb6e1af3355962881a51.pdf", "paperhash": "han|pumpout_a_meta_approach_for_robustly_training_deep_neural_networks_with_noisy_labels", "_bibtex": "@misc{\nhan2019pumpout,\ntitle={Pumpout: A Meta Approach for Robustly Training Deep Neural Networks with Noisy Labels},\nauthor={Bo Han and Gang Niu and Jiangchao Yao and Xingrui Yu and Miao Xu and Ivor Tsang and Masashi Sugiyama},\nyear={2019},\nurl={https://openreview.net/forum?id=HyG1_j0cYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper317/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621611871, "tddate": null, "super": null, "final": null, "reply": {"forum": "HyG1_j0cYQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper317/Authors", "ICLR.cc/2019/Conference/Paper317/Reviewers", "ICLR.cc/2019/Conference/Paper317/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper317/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper317/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper317/Authors|ICLR.cc/2019/Conference/Paper317/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper317/Reviewers", "ICLR.cc/2019/Conference/Paper317/Authors", "ICLR.cc/2019/Conference/Paper317/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621611871}}}, {"id": "rJeM7llN0Q", "original": null, "number": 12, "cdate": 1542877210331, "ddate": null, "tcdate": 1542877210331, "tmdate": 1542880568555, "tddate": null, "forum": "HyG1_j0cYQ", "replyto": "HkeWAkx4Am", "invitation": "ICLR.cc/2019/Conference/-/Paper317/Official_Comment", "content": {"title": "The non-negative version of the Backward Correction, and its appeal to Kiryo et al 17 is interesting, but it sidesteps its justification. A loss that can be negative does not necessarily means that it not lower-bounded.", "comment": "We agree that a well-defined minimization problem usually has a lower bounded objective. However, a sufficient optimization on such a bounded objective does not mean a better generalization on the test set, due to the gap between the true Risk Minimization (RM) and Empirical Risk Minimization (ERM). Previous work [1] provides a counterexample that a negative but lower bounded objective function can still result in terrible over-fitting issue in PU learning. We follow a similar motivation and introduce nnBC, and present an aggressive version of nnBC ($\\rm Pumpout_{BC}$) by using Pumpout.\n\nFor the standard classification problem with cross-entropy loss, the training objective should be lower bounded by zero. However, when we conduct Backward Correction on cross-entropy loss for noisy labels, the corrected cross-entropy loss may become negative, which will arise the over-fitting issue. Therefore, we should bound the objective of corrected cross-entropy loss with a $\\max$ operator (Theorem 2), which can avoid over-fitting issue. To achieve better performance, we extend Theorem 2 to Algorithm 3 ($\\rm Pumpout_{BC}$) that actively forget negative effects of noisy labels.\n\nReferences:\n[1] Kiryo R, Niu G, du Plessis M C, et al. Positive-unlabeled learning with non-negative risk estimator. In NeurIPS, 2017."}, "signatures": ["ICLR.cc/2019/Conference/Paper317/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper317/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper317/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pumpout: A Meta Approach for Robustly Training Deep Neural Networks with Noisy Labels", "abstract": "It is challenging to train deep neural networks robustly on the industrial-level data, since labels of such data are heavily noisy, and their label generation processes are normally agnostic. To handle these issues, by using the memorization effects of deep neural networks, we may train deep neural networks on the whole dataset only the first few iterations. Then, we may employ early stopping or the small-loss trick to train them on selected instances. However, in such training procedures, deep neural networks inevitably memorize some noisy labels, which will degrade their generalization. In this paper, we propose a meta algorithm called Pumpout to overcome the problem of memorizing noisy labels. By using scaled stochastic gradient ascent, Pumpout actively squeezes out the negative effects of noisy labels from the training model, instead of passively forgetting these effects. We leverage Pumpout to upgrade two representative methods: MentorNet and Backward Correction. Empirical results on benchmark vision and text datasets demonstrate that Pumpout can significantly improve the robustness of representative methods.", "keywords": ["Noisy Labels", "Deep Learning", "Meta Approach"], "authorids": ["bo.han@riken.jp", "gang.niu@riken.jp", "jiangchao.yao@student.uts.edu.au", "xingrui.yu@student.uts.edu.au", "miao.xu@riken.jp", "ivor.tsang@uts.edu.au", "sugi@k.u-tokyo.ac.jp"], "authors": ["Bo Han", "Gang Niu", "Jiangchao Yao", "Xingrui Yu", "Miao Xu", "Ivor Tsang", "Masashi Sugiyama"], "TL;DR": "Starting from tomorrow, never worry about your DNNs memorizing noisy labels---forget bad labels by Pumpout in an active manner!", "pdf": "/pdf/527948693e0b46f29236bb6e1af3355962881a51.pdf", "paperhash": "han|pumpout_a_meta_approach_for_robustly_training_deep_neural_networks_with_noisy_labels", "_bibtex": "@misc{\nhan2019pumpout,\ntitle={Pumpout: A Meta Approach for Robustly Training Deep Neural Networks with Noisy Labels},\nauthor={Bo Han and Gang Niu and Jiangchao Yao and Xingrui Yu and Miao Xu and Ivor Tsang and Masashi Sugiyama},\nyear={2019},\nurl={https://openreview.net/forum?id=HyG1_j0cYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper317/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621611871, "tddate": null, "super": null, "final": null, "reply": {"forum": "HyG1_j0cYQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper317/Authors", "ICLR.cc/2019/Conference/Paper317/Reviewers", "ICLR.cc/2019/Conference/Paper317/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper317/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper317/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper317/Authors|ICLR.cc/2019/Conference/Paper317/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper317/Reviewers", "ICLR.cc/2019/Conference/Paper317/Authors", "ICLR.cc/2019/Conference/Paper317/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621611871}}}, {"id": "BJlSZVx4Am", "original": null, "number": 17, "cdate": 1542878204549, "ddate": null, "tcdate": 1542878204549, "tmdate": 1542878216441, "tddate": null, "forum": "HyG1_j0cYQ", "replyto": "r1l3TQxVAQ", "invitation": "ICLR.cc/2019/Conference/-/Paper317/Official_Comment", "content": {"title": "At the beginning of the experiment section you mentioned several algorithms for training with noisy labels. I expect to compare your results to at least one of them.", "comment": "Thanks for the comments. In this paper, we introduce a meta algorithm to actively squeeze out the negative effects of noisy labels from training model, instead of passively forgetting these effects. We consider how our meta algorithm can simultaneously benefit different orthogonal algorithms. Thus, in experimental section, we evaluate the efficacy of our meta algorithm Pumpout by applying it to the representative method of training on selected instances (MentorNet [1]), and the representative method of estimating the noise transition matrix (Backward Correction [2]). Note that, the choice of two baselines is to justify whether Pumpout can benefit representative state-of-the-art algorithms. The readers are encouraged to upgrade other methods by using Pumpout.\n\nReferences:\n[1] Jiang L, Zhou Z, Leung T, et al. MentorNet: Learning data-driven curriculum for very deep neural networks on corrupted labels. In ICML, 2018.\n[2] Patrini G, Rozza A, Menon A, et al. Making deep neural networks robust to label noise: A loss correction approach. In CVPR, 2017."}, "signatures": ["ICLR.cc/2019/Conference/Paper317/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper317/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper317/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pumpout: A Meta Approach for Robustly Training Deep Neural Networks with Noisy Labels", "abstract": "It is challenging to train deep neural networks robustly on the industrial-level data, since labels of such data are heavily noisy, and their label generation processes are normally agnostic. To handle these issues, by using the memorization effects of deep neural networks, we may train deep neural networks on the whole dataset only the first few iterations. Then, we may employ early stopping or the small-loss trick to train them on selected instances. However, in such training procedures, deep neural networks inevitably memorize some noisy labels, which will degrade their generalization. In this paper, we propose a meta algorithm called Pumpout to overcome the problem of memorizing noisy labels. By using scaled stochastic gradient ascent, Pumpout actively squeezes out the negative effects of noisy labels from the training model, instead of passively forgetting these effects. We leverage Pumpout to upgrade two representative methods: MentorNet and Backward Correction. Empirical results on benchmark vision and text datasets demonstrate that Pumpout can significantly improve the robustness of representative methods.", "keywords": ["Noisy Labels", "Deep Learning", "Meta Approach"], "authorids": ["bo.han@riken.jp", "gang.niu@riken.jp", "jiangchao.yao@student.uts.edu.au", "xingrui.yu@student.uts.edu.au", "miao.xu@riken.jp", "ivor.tsang@uts.edu.au", "sugi@k.u-tokyo.ac.jp"], "authors": ["Bo Han", "Gang Niu", "Jiangchao Yao", "Xingrui Yu", "Miao Xu", "Ivor Tsang", "Masashi Sugiyama"], "TL;DR": "Starting from tomorrow, never worry about your DNNs memorizing noisy labels---forget bad labels by Pumpout in an active manner!", "pdf": "/pdf/527948693e0b46f29236bb6e1af3355962881a51.pdf", "paperhash": "han|pumpout_a_meta_approach_for_robustly_training_deep_neural_networks_with_noisy_labels", "_bibtex": "@misc{\nhan2019pumpout,\ntitle={Pumpout: A Meta Approach for Robustly Training Deep Neural Networks with Noisy Labels},\nauthor={Bo Han and Gang Niu and Jiangchao Yao and Xingrui Yu and Miao Xu and Ivor Tsang and Masashi Sugiyama},\nyear={2019},\nurl={https://openreview.net/forum?id=HyG1_j0cYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper317/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621611871, "tddate": null, "super": null, "final": null, "reply": {"forum": "HyG1_j0cYQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper317/Authors", "ICLR.cc/2019/Conference/Paper317/Reviewers", "ICLR.cc/2019/Conference/Paper317/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper317/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper317/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper317/Authors|ICLR.cc/2019/Conference/Paper317/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper317/Reviewers", "ICLR.cc/2019/Conference/Paper317/Authors", "ICLR.cc/2019/Conference/Paper317/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621611871}}}, {"id": "ByxhtWe4CX", "original": null, "number": 15, "cdate": 1542877571865, "ddate": null, "tcdate": 1542877571865, "tmdate": 1542877571865, "tddate": null, "forum": "HyG1_j0cYQ", "replyto": "BJlAGblVRQ", "invitation": "ICLR.cc/2019/Conference/-/Paper317/Official_Comment", "content": {"title": "SET2 is only run on MNIST. Why not even on CIFAR10 which is used in SET1? Any future reader will wonder \"did it work on CIFAR10\"?", "comment": "Thanks for your comments. For SET2, we have added experimental results on another vision dataset \\textit{CIFAR-10} and a text dataset \\textit{NEWS}. Due to the limited time, we only focus on these standard benchmark datasets at current stage, and we will remain the evaluation on \\textit{Open Images} dataset or \\textit{Clothing1M} dataset as a future work. For $\\beta$, we directly set it to zero and $\\gamma$ is chosen among $\\{0, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1\\}$ via a validation set."}, "signatures": ["ICLR.cc/2019/Conference/Paper317/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper317/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper317/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pumpout: A Meta Approach for Robustly Training Deep Neural Networks with Noisy Labels", "abstract": "It is challenging to train deep neural networks robustly on the industrial-level data, since labels of such data are heavily noisy, and their label generation processes are normally agnostic. To handle these issues, by using the memorization effects of deep neural networks, we may train deep neural networks on the whole dataset only the first few iterations. Then, we may employ early stopping or the small-loss trick to train them on selected instances. However, in such training procedures, deep neural networks inevitably memorize some noisy labels, which will degrade their generalization. In this paper, we propose a meta algorithm called Pumpout to overcome the problem of memorizing noisy labels. By using scaled stochastic gradient ascent, Pumpout actively squeezes out the negative effects of noisy labels from the training model, instead of passively forgetting these effects. We leverage Pumpout to upgrade two representative methods: MentorNet and Backward Correction. Empirical results on benchmark vision and text datasets demonstrate that Pumpout can significantly improve the robustness of representative methods.", "keywords": ["Noisy Labels", "Deep Learning", "Meta Approach"], "authorids": ["bo.han@riken.jp", "gang.niu@riken.jp", "jiangchao.yao@student.uts.edu.au", "xingrui.yu@student.uts.edu.au", "miao.xu@riken.jp", "ivor.tsang@uts.edu.au", "sugi@k.u-tokyo.ac.jp"], "authors": ["Bo Han", "Gang Niu", "Jiangchao Yao", "Xingrui Yu", "Miao Xu", "Ivor Tsang", "Masashi Sugiyama"], "TL;DR": "Starting from tomorrow, never worry about your DNNs memorizing noisy labels---forget bad labels by Pumpout in an active manner!", "pdf": "/pdf/527948693e0b46f29236bb6e1af3355962881a51.pdf", "paperhash": "han|pumpout_a_meta_approach_for_robustly_training_deep_neural_networks_with_noisy_labels", "_bibtex": "@misc{\nhan2019pumpout,\ntitle={Pumpout: A Meta Approach for Robustly Training Deep Neural Networks with Noisy Labels},\nauthor={Bo Han and Gang Niu and Jiangchao Yao and Xingrui Yu and Miao Xu and Ivor Tsang and Masashi Sugiyama},\nyear={2019},\nurl={https://openreview.net/forum?id=HyG1_j0cYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper317/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621611871, "tddate": null, "super": null, "final": null, "reply": {"forum": "HyG1_j0cYQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper317/Authors", "ICLR.cc/2019/Conference/Paper317/Reviewers", "ICLR.cc/2019/Conference/Paper317/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper317/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper317/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper317/Authors|ICLR.cc/2019/Conference/Paper317/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper317/Reviewers", "ICLR.cc/2019/Conference/Paper317/Authors", "ICLR.cc/2019/Conference/Paper317/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621611871}}}, {"id": "SJeMcexNRm", "original": null, "number": 13, "cdate": 1542877322461, "ddate": null, "tcdate": 1542877322461, "tmdate": 1542877401533, "tddate": null, "forum": "HyG1_j0cYQ", "replyto": "rJeM7llN0Q", "invitation": "ICLR.cc/2019/Conference/-/Paper317/Official_Comment", "content": {"title": "The statement of Theorem 2 is trivial. In fact, no proof is given as it would be self-evident. Moreover, the Theorem is not used by PumpOut. Algorithm 3 uses a if-else conditional on the scale of the backward correction, without the max(0, .).", "comment": "Thanks for your comments. We have modified the statement of Theorem 2. Non-negative version of BC is closely related to BC and $\\rm Pumpout_{BC}$ (Algorithm 3). The corrected loss by Backward Correction can be negative, which yields over-fitting issue. Non-negative version of BC is a strategy to overcome the over-fitting issue caused by the negative loss. However, the non-negative version of BC is passive, since max operator means stopping gradients computation on negative-risk instances. $\\rm Pumpout_{BC}$ is an aggressive version of non-negative BC. $\\rm Pumpout_{BC}$ conducts not only stochastic gradient descent on non-negative-risk instances (when $ \\mathbf{1}^{\\top}\\mathbf{T}^{-1}\\ell(\\mathbf{x}, y; w_f)\\geq0$), but also scaled stochastic gradient ascent on negative-risk instances (when $\\mathbf{1}^{\\top}\\mathbf{T}^{-1}\\ell(\\mathbf{x}, y; w_f)\\leq0$). As non-negative BC is highly related to BC and our $\\rm Pumpout_{BC}$, we present it on the text and use it as a baseline. Note that, in Algorithm 3, we set a tuning \"safety valve\" $\\beta$, and use the fitting condition as $ \\mathbf{1}^{\\top}\\mathbf{T}^{-1}\\ell(\\mathbf{x}, y; w_f)\\geq \\beta$. However, in all experiments, we directly set $\\beta = 0$."}, "signatures": ["ICLR.cc/2019/Conference/Paper317/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper317/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper317/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pumpout: A Meta Approach for Robustly Training Deep Neural Networks with Noisy Labels", "abstract": "It is challenging to train deep neural networks robustly on the industrial-level data, since labels of such data are heavily noisy, and their label generation processes are normally agnostic. To handle these issues, by using the memorization effects of deep neural networks, we may train deep neural networks on the whole dataset only the first few iterations. Then, we may employ early stopping or the small-loss trick to train them on selected instances. However, in such training procedures, deep neural networks inevitably memorize some noisy labels, which will degrade their generalization. In this paper, we propose a meta algorithm called Pumpout to overcome the problem of memorizing noisy labels. By using scaled stochastic gradient ascent, Pumpout actively squeezes out the negative effects of noisy labels from the training model, instead of passively forgetting these effects. We leverage Pumpout to upgrade two representative methods: MentorNet and Backward Correction. Empirical results on benchmark vision and text datasets demonstrate that Pumpout can significantly improve the robustness of representative methods.", "keywords": ["Noisy Labels", "Deep Learning", "Meta Approach"], "authorids": ["bo.han@riken.jp", "gang.niu@riken.jp", "jiangchao.yao@student.uts.edu.au", "xingrui.yu@student.uts.edu.au", "miao.xu@riken.jp", "ivor.tsang@uts.edu.au", "sugi@k.u-tokyo.ac.jp"], "authors": ["Bo Han", "Gang Niu", "Jiangchao Yao", "Xingrui Yu", "Miao Xu", "Ivor Tsang", "Masashi Sugiyama"], "TL;DR": "Starting from tomorrow, never worry about your DNNs memorizing noisy labels---forget bad labels by Pumpout in an active manner!", "pdf": "/pdf/527948693e0b46f29236bb6e1af3355962881a51.pdf", "paperhash": "han|pumpout_a_meta_approach_for_robustly_training_deep_neural_networks_with_noisy_labels", "_bibtex": "@misc{\nhan2019pumpout,\ntitle={Pumpout: A Meta Approach for Robustly Training Deep Neural Networks with Noisy Labels},\nauthor={Bo Han and Gang Niu and Jiangchao Yao and Xingrui Yu and Miao Xu and Ivor Tsang and Masashi Sugiyama},\nyear={2019},\nurl={https://openreview.net/forum?id=HyG1_j0cYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper317/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621611871, "tddate": null, "super": null, "final": null, "reply": {"forum": "HyG1_j0cYQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper317/Authors", "ICLR.cc/2019/Conference/Paper317/Reviewers", "ICLR.cc/2019/Conference/Paper317/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper317/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper317/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper317/Authors|ICLR.cc/2019/Conference/Paper317/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper317/Reviewers", "ICLR.cc/2019/Conference/Paper317/Authors", "ICLR.cc/2019/Conference/Paper317/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621611871}}}, {"id": "HkeWAkx4Am", "original": null, "number": 11, "cdate": 1542877128656, "ddate": null, "tcdate": 1542877128656, "tmdate": 1542877128656, "tddate": null, "forum": "HyG1_j0cYQ", "replyto": "Bkgn5kxN0Q", "invitation": "ICLR.cc/2019/Conference/-/Paper317/Official_Comment", "content": {"title": "Why MentorNet + Pumpout does not suffer from the selection bias effect of CoTraining? This is unclear to me.", "comment": "We are sorry for such confusing. In the high level, sample selection bias means that we assign different weights to different samples, and such bias will prioritize some of samples during training. Thus, both MentorNet and $\\rm Pumpout_{SL}$ (MentorNet + Pumpout) have sample selection bias. We leverage sample selection bias to overcome the label noise issue."}, "signatures": ["ICLR.cc/2019/Conference/Paper317/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper317/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper317/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pumpout: A Meta Approach for Robustly Training Deep Neural Networks with Noisy Labels", "abstract": "It is challenging to train deep neural networks robustly on the industrial-level data, since labels of such data are heavily noisy, and their label generation processes are normally agnostic. To handle these issues, by using the memorization effects of deep neural networks, we may train deep neural networks on the whole dataset only the first few iterations. Then, we may employ early stopping or the small-loss trick to train them on selected instances. However, in such training procedures, deep neural networks inevitably memorize some noisy labels, which will degrade their generalization. In this paper, we propose a meta algorithm called Pumpout to overcome the problem of memorizing noisy labels. By using scaled stochastic gradient ascent, Pumpout actively squeezes out the negative effects of noisy labels from the training model, instead of passively forgetting these effects. We leverage Pumpout to upgrade two representative methods: MentorNet and Backward Correction. Empirical results on benchmark vision and text datasets demonstrate that Pumpout can significantly improve the robustness of representative methods.", "keywords": ["Noisy Labels", "Deep Learning", "Meta Approach"], "authorids": ["bo.han@riken.jp", "gang.niu@riken.jp", "jiangchao.yao@student.uts.edu.au", "xingrui.yu@student.uts.edu.au", "miao.xu@riken.jp", "ivor.tsang@uts.edu.au", "sugi@k.u-tokyo.ac.jp"], "authors": ["Bo Han", "Gang Niu", "Jiangchao Yao", "Xingrui Yu", "Miao Xu", "Ivor Tsang", "Masashi Sugiyama"], "TL;DR": "Starting from tomorrow, never worry about your DNNs memorizing noisy labels---forget bad labels by Pumpout in an active manner!", "pdf": "/pdf/527948693e0b46f29236bb6e1af3355962881a51.pdf", "paperhash": "han|pumpout_a_meta_approach_for_robustly_training_deep_neural_networks_with_noisy_labels", "_bibtex": "@misc{\nhan2019pumpout,\ntitle={Pumpout: A Meta Approach for Robustly Training Deep Neural Networks with Noisy Labels},\nauthor={Bo Han and Gang Niu and Jiangchao Yao and Xingrui Yu and Miao Xu and Ivor Tsang and Masashi Sugiyama},\nyear={2019},\nurl={https://openreview.net/forum?id=HyG1_j0cYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper317/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621611871, "tddate": null, "super": null, "final": null, "reply": {"forum": "HyG1_j0cYQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper317/Authors", "ICLR.cc/2019/Conference/Paper317/Reviewers", "ICLR.cc/2019/Conference/Paper317/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper317/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper317/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper317/Authors|ICLR.cc/2019/Conference/Paper317/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper317/Reviewers", "ICLR.cc/2019/Conference/Paper317/Authors", "ICLR.cc/2019/Conference/Paper317/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621611871}}}, {"id": "Bkgn5kxN0Q", "original": null, "number": 10, "cdate": 1542877076063, "ddate": null, "tcdate": 1542877076063, "tmdate": 1542877076063, "tddate": null, "forum": "HyG1_j0cYQ", "replyto": "rJlfPNcx2X", "invitation": "ICLR.cc/2019/Conference/-/Paper317/Official_Comment", "content": {"title": "Section 2: the scaling factor $\\gamma$. Why using $\\gamma$=1 is suboptimal?", "comment": "We conduct the experiments on several benchmark datasets, including vision and text. We empirically find that the best performance is usually chosen when $\\gamma$ is in-between 0 and 1 by using the validation set."}, "signatures": ["ICLR.cc/2019/Conference/Paper317/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper317/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper317/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pumpout: A Meta Approach for Robustly Training Deep Neural Networks with Noisy Labels", "abstract": "It is challenging to train deep neural networks robustly on the industrial-level data, since labels of such data are heavily noisy, and their label generation processes are normally agnostic. To handle these issues, by using the memorization effects of deep neural networks, we may train deep neural networks on the whole dataset only the first few iterations. Then, we may employ early stopping or the small-loss trick to train them on selected instances. However, in such training procedures, deep neural networks inevitably memorize some noisy labels, which will degrade their generalization. In this paper, we propose a meta algorithm called Pumpout to overcome the problem of memorizing noisy labels. By using scaled stochastic gradient ascent, Pumpout actively squeezes out the negative effects of noisy labels from the training model, instead of passively forgetting these effects. We leverage Pumpout to upgrade two representative methods: MentorNet and Backward Correction. Empirical results on benchmark vision and text datasets demonstrate that Pumpout can significantly improve the robustness of representative methods.", "keywords": ["Noisy Labels", "Deep Learning", "Meta Approach"], "authorids": ["bo.han@riken.jp", "gang.niu@riken.jp", "jiangchao.yao@student.uts.edu.au", "xingrui.yu@student.uts.edu.au", "miao.xu@riken.jp", "ivor.tsang@uts.edu.au", "sugi@k.u-tokyo.ac.jp"], "authors": ["Bo Han", "Gang Niu", "Jiangchao Yao", "Xingrui Yu", "Miao Xu", "Ivor Tsang", "Masashi Sugiyama"], "TL;DR": "Starting from tomorrow, never worry about your DNNs memorizing noisy labels---forget bad labels by Pumpout in an active manner!", "pdf": "/pdf/527948693e0b46f29236bb6e1af3355962881a51.pdf", "paperhash": "han|pumpout_a_meta_approach_for_robustly_training_deep_neural_networks_with_noisy_labels", "_bibtex": "@misc{\nhan2019pumpout,\ntitle={Pumpout: A Meta Approach for Robustly Training Deep Neural Networks with Noisy Labels},\nauthor={Bo Han and Gang Niu and Jiangchao Yao and Xingrui Yu and Miao Xu and Ivor Tsang and Masashi Sugiyama},\nyear={2019},\nurl={https://openreview.net/forum?id=HyG1_j0cYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper317/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621611871, "tddate": null, "super": null, "final": null, "reply": {"forum": "HyG1_j0cYQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper317/Authors", "ICLR.cc/2019/Conference/Paper317/Reviewers", "ICLR.cc/2019/Conference/Paper317/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper317/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper317/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper317/Authors|ICLR.cc/2019/Conference/Paper317/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper317/Reviewers", "ICLR.cc/2019/Conference/Paper317/Authors", "ICLR.cc/2019/Conference/Paper317/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621611871}}}, {"id": "H1l5aC1E0Q", "original": null, "number": 9, "cdate": 1542876866366, "ddate": null, "tcdate": 1542876866366, "tmdate": 1542876886612, "tddate": null, "forum": "HyG1_j0cYQ", "replyto": "ryltU0y4RX", "invitation": "ICLR.cc/2019/Conference/-/Paper317/Official_Comment", "content": {"title": "Experiments are too standard. More divers and various data sets would be more convincing.", "comment": "Thanks for your suggestion. We have added one text dataset called NEWS in updated experiments. As can be seen in Figure 4 and Figure 7, obvious improvements (similar to other figures) are achieved compared to baseline approaches, which confirms the benefits of Pumpout. Namely, Pumpout indeed can overcome the issue of memorizing noisy labels."}, "signatures": ["ICLR.cc/2019/Conference/Paper317/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper317/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper317/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pumpout: A Meta Approach for Robustly Training Deep Neural Networks with Noisy Labels", "abstract": "It is challenging to train deep neural networks robustly on the industrial-level data, since labels of such data are heavily noisy, and their label generation processes are normally agnostic. To handle these issues, by using the memorization effects of deep neural networks, we may train deep neural networks on the whole dataset only the first few iterations. Then, we may employ early stopping or the small-loss trick to train them on selected instances. However, in such training procedures, deep neural networks inevitably memorize some noisy labels, which will degrade their generalization. In this paper, we propose a meta algorithm called Pumpout to overcome the problem of memorizing noisy labels. By using scaled stochastic gradient ascent, Pumpout actively squeezes out the negative effects of noisy labels from the training model, instead of passively forgetting these effects. We leverage Pumpout to upgrade two representative methods: MentorNet and Backward Correction. Empirical results on benchmark vision and text datasets demonstrate that Pumpout can significantly improve the robustness of representative methods.", "keywords": ["Noisy Labels", "Deep Learning", "Meta Approach"], "authorids": ["bo.han@riken.jp", "gang.niu@riken.jp", "jiangchao.yao@student.uts.edu.au", "xingrui.yu@student.uts.edu.au", "miao.xu@riken.jp", "ivor.tsang@uts.edu.au", "sugi@k.u-tokyo.ac.jp"], "authors": ["Bo Han", "Gang Niu", "Jiangchao Yao", "Xingrui Yu", "Miao Xu", "Ivor Tsang", "Masashi Sugiyama"], "TL;DR": "Starting from tomorrow, never worry about your DNNs memorizing noisy labels---forget bad labels by Pumpout in an active manner!", "pdf": "/pdf/527948693e0b46f29236bb6e1af3355962881a51.pdf", "paperhash": "han|pumpout_a_meta_approach_for_robustly_training_deep_neural_networks_with_noisy_labels", "_bibtex": "@misc{\nhan2019pumpout,\ntitle={Pumpout: A Meta Approach for Robustly Training Deep Neural Networks with Noisy Labels},\nauthor={Bo Han and Gang Niu and Jiangchao Yao and Xingrui Yu and Miao Xu and Ivor Tsang and Masashi Sugiyama},\nyear={2019},\nurl={https://openreview.net/forum?id=HyG1_j0cYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper317/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621611871, "tddate": null, "super": null, "final": null, "reply": {"forum": "HyG1_j0cYQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper317/Authors", "ICLR.cc/2019/Conference/Paper317/Reviewers", "ICLR.cc/2019/Conference/Paper317/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper317/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper317/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper317/Authors|ICLR.cc/2019/Conference/Paper317/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper317/Reviewers", "ICLR.cc/2019/Conference/Paper317/Authors", "ICLR.cc/2019/Conference/Paper317/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621611871}}}, {"id": "ByxT2oFn2X", "original": null, "number": 3, "cdate": 1541344180756, "ddate": null, "tcdate": 1541344180756, "tmdate": 1541534098379, "tddate": null, "forum": "HyG1_j0cYQ", "replyto": "HyG1_j0cYQ", "invitation": "ICLR.cc/2019/Conference/-/Paper317/Official_Review", "content": {"title": "This paper presents a meta algorithm to improve the robustness of learning methods under noisy labels.", "review": "This paper presents a meta algorithm to improve the robustness of learning methods under noisy labels. The idea is to squeeze out the negative effects of noisy labels actively. The paper trains deep neural networks by stochastic gradient descent on \u201cfitting\u201d labels; while trains deep neural networks by scaled stochastic gradient ascent on \u201cnot-fitting\u201d labels. Experimental results show the improvement on robustness. \n\nThe good things of the paper are clear. \n1.\tTechnical sound with reasonable idea\n2.\tProblem is well motivated\n3.\tPaper is general well written.\n\nSome comments\n1.\tThe idea using instance selection is not new. The novelty could be improved. If the paper could make more insight from either theoretical or application value, would be more interesting.\n2.\tExperiments are too standard. More divers and various data sets would be more convincing. \n", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper317/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pumpout: A Meta Approach for Robustly Training Deep Neural Networks with Noisy Labels", "abstract": "It is challenging to train deep neural networks robustly on the industrial-level data, since labels of such data are heavily noisy, and their label generation processes are normally agnostic. To handle these issues, by using the memorization effects of deep neural networks, we may train deep neural networks on the whole dataset only the first few iterations. Then, we may employ early stopping or the small-loss trick to train them on selected instances. However, in such training procedures, deep neural networks inevitably memorize some noisy labels, which will degrade their generalization. In this paper, we propose a meta algorithm called Pumpout to overcome the problem of memorizing noisy labels. By using scaled stochastic gradient ascent, Pumpout actively squeezes out the negative effects of noisy labels from the training model, instead of passively forgetting these effects. We leverage Pumpout to upgrade two representative methods: MentorNet and Backward Correction. Empirical results on benchmark vision and text datasets demonstrate that Pumpout can significantly improve the robustness of representative methods.", "keywords": ["Noisy Labels", "Deep Learning", "Meta Approach"], "authorids": ["bo.han@riken.jp", "gang.niu@riken.jp", "jiangchao.yao@student.uts.edu.au", "xingrui.yu@student.uts.edu.au", "miao.xu@riken.jp", "ivor.tsang@uts.edu.au", "sugi@k.u-tokyo.ac.jp"], "authors": ["Bo Han", "Gang Niu", "Jiangchao Yao", "Xingrui Yu", "Miao Xu", "Ivor Tsang", "Masashi Sugiyama"], "TL;DR": "Starting from tomorrow, never worry about your DNNs memorizing noisy labels---forget bad labels by Pumpout in an active manner!", "pdf": "/pdf/527948693e0b46f29236bb6e1af3355962881a51.pdf", "paperhash": "han|pumpout_a_meta_approach_for_robustly_training_deep_neural_networks_with_noisy_labels", "_bibtex": "@misc{\nhan2019pumpout,\ntitle={Pumpout: A Meta Approach for Robustly Training Deep Neural Networks with Noisy Labels},\nauthor={Bo Han and Gang Niu and Jiangchao Yao and Xingrui Yu and Miao Xu and Ivor Tsang and Masashi Sugiyama},\nyear={2019},\nurl={https://openreview.net/forum?id=HyG1_j0cYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper317/Official_Review", "cdate": 1542234488970, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HyG1_j0cYQ", "replyto": "HyG1_j0cYQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper317/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335697193, "tmdate": 1552335697193, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper317/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "rJlfPNcx2X", "original": null, "number": 2, "cdate": 1540559962337, "ddate": null, "tcdate": 1540559962337, "tmdate": 1541534098163, "tddate": null, "forum": "HyG1_j0cYQ", "replyto": "HyG1_j0cYQ", "invitation": "ICLR.cc/2019/Conference/-/Paper317/Official_Review", "content": {"title": "Original idea with promising experimental results, but a limited contribution", "review": "A new method for defending against label noise during training of deep neural networks is presented. The main idea is to \u201cforget\u201d about wrongly labeled examples during training by an ascending step in the gradient direction. This is a meta-approach that can be combined with other methods for noise robustness; the detection of the noisy examples is specific of the base method utilised. Experimental results are promising.\n\nIn general, I find the idea original, and of potential practical use, but I believe the contribution of paper to be limited and not well supported by experiments. Moreover, I think that some claims made in this work are poorly justified.\n\n== Method\n\nThe paper would greatly benefit from some theoretical backing of the proposed optimization scheme, even on a simplified scenario. An idea would be to prove that, given a dataset with noisy labels, PumpOut converges close to the best model (= the one learned without noise), for certain hyperparameters. I think this would be new and interesting. A result of similar fashion was proven in [A].\n\nI found the following arguments not well or only heuristically supported:\n* section 2: the scaling factor \\gamma. Why using \\gamma=1 is suboptimal? One could claim that as much as you want to memorize the true image-label patterns, you also want to forget the image-noise ones.\n* Why MentorNet + PumpOut does not suffer from the selection bias effect of CoTraining? This is unclear to me\n* The non-negative version of the BackwardCorrection, and its appeal to Kiryo et al 17 is interesting, but it sidesteps its justification. A loss that can be negative does not necessarily means that it not lower-bounded. In fact, for a minimization problem to be well-defined, all you need is a lower bounded objective. Then, adding the lower bound makes your loss non-negative. Notice that Patrini et al 17 did not state that BC is unbounded, but only that it can be negative. Can you show more that that -- maybe, at least experimentally?\n\nThe statement of Theorem 2 is trivial. In fact, no proof is given as it would be self-evident. Moreover, the Theorem is not used by PumpOut. Algorithm 3 uses a if-else conditional on the scale of the backward correction, without the max(0, .). I suggest to remove this part. I have notice later that the non-negative version of BC is used as a baseline in the experiment, but I think that is the only use.\n\nRegarding the presentation, in section 3, I suggest to move the explanation of MentorNet and BackwardCorrection before their upgrade by PumpOut.\n\n== Experiments\n\nTable 1 can be removed as these are extremely common datasets.\n\nThe experimental results look very promising for applications. As a side effect of this analysis, I can also notice an improvement over BC given to the nnBC, which is nice per se. Although, I would have strengthen the empirics as follow.\n* SET2 is only run on MNIST. Why not even on CIFAR10 which is used in SET1? Any future reader will wonder \u201cdid it work on CIFAR10?\"\n* A much harder instance of noise, for instance open set [B] or from a real dataset [Xiao et al 15] would have more clearly supported the use of PumpOut for real applications.\n* Can the authors elaborate on \u201cthe choices of \\beta and \\gamma follows Kirkyo et al 2017\u201d ? And how assuming their knowledge gives a fair comparison to BC which does not require them? I believe this is a critical point for the validity of the experiments.\n\nMinor:\n* \u201cLRELU active function\u2019 -> activation function. What is a LRELU? LeakyReLU?\n\n[A] Malach, Eran, and Shai Shalev-Shwartz. \"Decoupling\" when to update\" from\" how to update\".\" Advances in Neural Information Processing Systems. 2017.\n[B] Veit, Andreas, et al. \"Learning From Noisy Large-Scale Datasets With Minimal Supervision.\" CVPR. 2017.", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper317/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pumpout: A Meta Approach for Robustly Training Deep Neural Networks with Noisy Labels", "abstract": "It is challenging to train deep neural networks robustly on the industrial-level data, since labels of such data are heavily noisy, and their label generation processes are normally agnostic. To handle these issues, by using the memorization effects of deep neural networks, we may train deep neural networks on the whole dataset only the first few iterations. Then, we may employ early stopping or the small-loss trick to train them on selected instances. However, in such training procedures, deep neural networks inevitably memorize some noisy labels, which will degrade their generalization. In this paper, we propose a meta algorithm called Pumpout to overcome the problem of memorizing noisy labels. By using scaled stochastic gradient ascent, Pumpout actively squeezes out the negative effects of noisy labels from the training model, instead of passively forgetting these effects. We leverage Pumpout to upgrade two representative methods: MentorNet and Backward Correction. Empirical results on benchmark vision and text datasets demonstrate that Pumpout can significantly improve the robustness of representative methods.", "keywords": ["Noisy Labels", "Deep Learning", "Meta Approach"], "authorids": ["bo.han@riken.jp", "gang.niu@riken.jp", "jiangchao.yao@student.uts.edu.au", "xingrui.yu@student.uts.edu.au", "miao.xu@riken.jp", "ivor.tsang@uts.edu.au", "sugi@k.u-tokyo.ac.jp"], "authors": ["Bo Han", "Gang Niu", "Jiangchao Yao", "Xingrui Yu", "Miao Xu", "Ivor Tsang", "Masashi Sugiyama"], "TL;DR": "Starting from tomorrow, never worry about your DNNs memorizing noisy labels---forget bad labels by Pumpout in an active manner!", "pdf": "/pdf/527948693e0b46f29236bb6e1af3355962881a51.pdf", "paperhash": "han|pumpout_a_meta_approach_for_robustly_training_deep_neural_networks_with_noisy_labels", "_bibtex": "@misc{\nhan2019pumpout,\ntitle={Pumpout: A Meta Approach for Robustly Training Deep Neural Networks with Noisy Labels},\nauthor={Bo Han and Gang Niu and Jiangchao Yao and Xingrui Yu and Miao Xu and Ivor Tsang and Masashi Sugiyama},\nyear={2019},\nurl={https://openreview.net/forum?id=HyG1_j0cYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper317/Official_Review", "cdate": 1542234488970, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HyG1_j0cYQ", "replyto": "HyG1_j0cYQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper317/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335697193, "tmdate": 1552335697193, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper317/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "SJxMUmdJnm", "original": null, "number": 1, "cdate": 1540485962094, "ddate": null, "tcdate": 1540485962094, "tmdate": 1541534097913, "tddate": null, "forum": "HyG1_j0cYQ", "replyto": "HyG1_j0cYQ", "invitation": "ICLR.cc/2019/Conference/-/Paper317/Official_Review", "content": {"title": "Non convincing experiments   ", "review": "The paper proposes a meta algorithm to train a network with noisy labels.\nIt is not a general algorithm but a simple modification of two proposed methods.  It is presented as a heuristics and it  would be helpful to derive a theoretical framework or motivation for the proposed algorithm. \n\nMy main concern is related to the experiment results. The results of the baseline method look strange. Why there is a strong decrease in the MNIST test accuracy after 20 epochs? Standard training of neural network is very robust to label noise.  In case of  20% symmetric error  (figure 2c)  the performance degradation using standard training should be very small. \nHence it is difficult to evaluate to performance of the proposed method.\nAt the beginning of the experiment section you mentioned several algorithms   for training with noisy labels.  I expect to compare your results to at least one of them. \n    \n", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper317/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pumpout: A Meta Approach for Robustly Training Deep Neural Networks with Noisy Labels", "abstract": "It is challenging to train deep neural networks robustly on the industrial-level data, since labels of such data are heavily noisy, and their label generation processes are normally agnostic. To handle these issues, by using the memorization effects of deep neural networks, we may train deep neural networks on the whole dataset only the first few iterations. Then, we may employ early stopping or the small-loss trick to train them on selected instances. However, in such training procedures, deep neural networks inevitably memorize some noisy labels, which will degrade their generalization. In this paper, we propose a meta algorithm called Pumpout to overcome the problem of memorizing noisy labels. By using scaled stochastic gradient ascent, Pumpout actively squeezes out the negative effects of noisy labels from the training model, instead of passively forgetting these effects. We leverage Pumpout to upgrade two representative methods: MentorNet and Backward Correction. Empirical results on benchmark vision and text datasets demonstrate that Pumpout can significantly improve the robustness of representative methods.", "keywords": ["Noisy Labels", "Deep Learning", "Meta Approach"], "authorids": ["bo.han@riken.jp", "gang.niu@riken.jp", "jiangchao.yao@student.uts.edu.au", "xingrui.yu@student.uts.edu.au", "miao.xu@riken.jp", "ivor.tsang@uts.edu.au", "sugi@k.u-tokyo.ac.jp"], "authors": ["Bo Han", "Gang Niu", "Jiangchao Yao", "Xingrui Yu", "Miao Xu", "Ivor Tsang", "Masashi Sugiyama"], "TL;DR": "Starting from tomorrow, never worry about your DNNs memorizing noisy labels---forget bad labels by Pumpout in an active manner!", "pdf": "/pdf/527948693e0b46f29236bb6e1af3355962881a51.pdf", "paperhash": "han|pumpout_a_meta_approach_for_robustly_training_deep_neural_networks_with_noisy_labels", "_bibtex": "@misc{\nhan2019pumpout,\ntitle={Pumpout: A Meta Approach for Robustly Training Deep Neural Networks with Noisy Labels},\nauthor={Bo Han and Gang Niu and Jiangchao Yao and Xingrui Yu and Miao Xu and Ivor Tsang and Masashi Sugiyama},\nyear={2019},\nurl={https://openreview.net/forum?id=HyG1_j0cYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper317/Official_Review", "cdate": 1542234488970, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HyG1_j0cYQ", "replyto": "HyG1_j0cYQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper317/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335697193, "tmdate": 1552335697193, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper317/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "BJlibzhU57", "original": null, "number": 18, "cdate": 1538863618560, "ddate": null, "tcdate": 1538863618560, "tmdate": 1538864091879, "tddate": null, "forum": "HyG1_j0cYQ", "replyto": "S1lJiNpNcQ", "invitation": "ICLR.cc/2019/Conference/-/Paper317/Public_Comment", "content": {"comment": "In [5], their experiments at Table 2 (https://openreview.net/pdf?id=r1Ddp1-Rb ) using dropout (droprate=0.7) achieves > 89% test accuracy with the same setting as Figure 3.c (20% symmetric noise) though different architecture. Still, I think dropout should be compared as a strong baseline, especially since you also used it in training your models.\n\n[5] Zhang, Hongyi, et al. \"mixup: Beyond Empirical Risk Minimization\" International Conference on Learning Representations (ICLR). (2018).", "title": "missing comparison with dropout"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper317/Reviewers/Unsubmitted"], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pumpout: A Meta Approach for Robustly Training Deep Neural Networks with Noisy Labels", "abstract": "It is challenging to train deep neural networks robustly on the industrial-level data, since labels of such data are heavily noisy, and their label generation processes are normally agnostic. To handle these issues, by using the memorization effects of deep neural networks, we may train deep neural networks on the whole dataset only the first few iterations. Then, we may employ early stopping or the small-loss trick to train them on selected instances. However, in such training procedures, deep neural networks inevitably memorize some noisy labels, which will degrade their generalization. In this paper, we propose a meta algorithm called Pumpout to overcome the problem of memorizing noisy labels. By using scaled stochastic gradient ascent, Pumpout actively squeezes out the negative effects of noisy labels from the training model, instead of passively forgetting these effects. We leverage Pumpout to upgrade two representative methods: MentorNet and Backward Correction. Empirical results on benchmark vision and text datasets demonstrate that Pumpout can significantly improve the robustness of representative methods.", "keywords": ["Noisy Labels", "Deep Learning", "Meta Approach"], "authorids": ["bo.han@riken.jp", "gang.niu@riken.jp", "jiangchao.yao@student.uts.edu.au", "xingrui.yu@student.uts.edu.au", "miao.xu@riken.jp", "ivor.tsang@uts.edu.au", "sugi@k.u-tokyo.ac.jp"], "authors": ["Bo Han", "Gang Niu", "Jiangchao Yao", "Xingrui Yu", "Miao Xu", "Ivor Tsang", "Masashi Sugiyama"], "TL;DR": "Starting from tomorrow, never worry about your DNNs memorizing noisy labels---forget bad labels by Pumpout in an active manner!", "pdf": "/pdf/527948693e0b46f29236bb6e1af3355962881a51.pdf", "paperhash": "han|pumpout_a_meta_approach_for_robustly_training_deep_neural_networks_with_noisy_labels", "_bibtex": "@misc{\nhan2019pumpout,\ntitle={Pumpout: A Meta Approach for Robustly Training Deep Neural Networks with Noisy Labels},\nauthor={Bo Han and Gang Niu and Jiangchao Yao and Xingrui Yu and Miao Xu and Ivor Tsang and Masashi Sugiyama},\nyear={2019},\nurl={https://openreview.net/forum?id=HyG1_j0cYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper317/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311868292, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "HyG1_j0cYQ", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper317/Authors", "ICLR.cc/2019/Conference/Paper317/Reviewers", "ICLR.cc/2019/Conference/Paper317/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper317/Authors", "ICLR.cc/2019/Conference/Paper317/Reviewers", "ICLR.cc/2019/Conference/Paper317/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311868292}}}, {"id": "Bygu6To19X", "original": null, "number": 5, "cdate": 1538403775523, "ddate": null, "tcdate": 1538403775523, "tmdate": 1538846178996, "tddate": null, "forum": "HyG1_j0cYQ", "replyto": "HyG1_j0cYQ", "invitation": "ICLR.cc/2019/Conference/-/Paper317/Public_Comment", "content": {"comment": "Interesting idea. It is expected to benefit the area of learning from noisy labels and it is also meaningful for real scenario applications where clean labels are not available. Hope to see the released code soon.", "title": "Interesting idea"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper317/Reviewers/Unsubmitted"], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pumpout: A Meta Approach for Robustly Training Deep Neural Networks with Noisy Labels", "abstract": "It is challenging to train deep neural networks robustly on the industrial-level data, since labels of such data are heavily noisy, and their label generation processes are normally agnostic. To handle these issues, by using the memorization effects of deep neural networks, we may train deep neural networks on the whole dataset only the first few iterations. Then, we may employ early stopping or the small-loss trick to train them on selected instances. However, in such training procedures, deep neural networks inevitably memorize some noisy labels, which will degrade their generalization. In this paper, we propose a meta algorithm called Pumpout to overcome the problem of memorizing noisy labels. By using scaled stochastic gradient ascent, Pumpout actively squeezes out the negative effects of noisy labels from the training model, instead of passively forgetting these effects. We leverage Pumpout to upgrade two representative methods: MentorNet and Backward Correction. Empirical results on benchmark vision and text datasets demonstrate that Pumpout can significantly improve the robustness of representative methods.", "keywords": ["Noisy Labels", "Deep Learning", "Meta Approach"], "authorids": ["bo.han@riken.jp", "gang.niu@riken.jp", "jiangchao.yao@student.uts.edu.au", "xingrui.yu@student.uts.edu.au", "miao.xu@riken.jp", "ivor.tsang@uts.edu.au", "sugi@k.u-tokyo.ac.jp"], "authors": ["Bo Han", "Gang Niu", "Jiangchao Yao", "Xingrui Yu", "Miao Xu", "Ivor Tsang", "Masashi Sugiyama"], "TL;DR": "Starting from tomorrow, never worry about your DNNs memorizing noisy labels---forget bad labels by Pumpout in an active manner!", "pdf": "/pdf/527948693e0b46f29236bb6e1af3355962881a51.pdf", "paperhash": "han|pumpout_a_meta_approach_for_robustly_training_deep_neural_networks_with_noisy_labels", "_bibtex": "@misc{\nhan2019pumpout,\ntitle={Pumpout: A Meta Approach for Robustly Training Deep Neural Networks with Noisy Labels},\nauthor={Bo Han and Gang Niu and Jiangchao Yao and Xingrui Yu and Miao Xu and Ivor Tsang and Masashi Sugiyama},\nyear={2019},\nurl={https://openreview.net/forum?id=HyG1_j0cYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper317/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311868292, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "HyG1_j0cYQ", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper317/Authors", "ICLR.cc/2019/Conference/Paper317/Reviewers", "ICLR.cc/2019/Conference/Paper317/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper317/Authors", "ICLR.cc/2019/Conference/Paper317/Reviewers", "ICLR.cc/2019/Conference/Paper317/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311868292}}}, {"id": "rJeNFlCVcQ", "original": null, "number": 6, "cdate": 1538740347857, "ddate": null, "tcdate": 1538740347857, "tmdate": 1538740347857, "tddate": null, "forum": "HyG1_j0cYQ", "replyto": "S1luH11e5X", "invitation": "ICLR.cc/2019/Conference/-/Paper317/Official_Comment", "content": {"title": "The experimental comparisons are fair", "comment": "Sorry, we should make our point clearer about how to estimate the noise level in practice. It's true the noise level can be estimated from clean validation data. However, note that clean data require domain experts to label, and thus a small set of clean data can be as costly as a huge set of noisy data. On the other hand, we could give a representative sub-sampling of our noisy data and directly ask the domain expert to estimate the noise level. In this way, we won't obtain a small set of clean data from the domain expert. As a result, we could pay much less to only obtain some key parameters in the underlying data generation/corruption process. This is the scenario of the current paper. Therefore, the experimental comparisons are fair."}, "signatures": ["ICLR.cc/2019/Conference/Paper317/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper317/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper317/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pumpout: A Meta Approach for Robustly Training Deep Neural Networks with Noisy Labels", "abstract": "It is challenging to train deep neural networks robustly on the industrial-level data, since labels of such data are heavily noisy, and their label generation processes are normally agnostic. To handle these issues, by using the memorization effects of deep neural networks, we may train deep neural networks on the whole dataset only the first few iterations. Then, we may employ early stopping or the small-loss trick to train them on selected instances. However, in such training procedures, deep neural networks inevitably memorize some noisy labels, which will degrade their generalization. In this paper, we propose a meta algorithm called Pumpout to overcome the problem of memorizing noisy labels. By using scaled stochastic gradient ascent, Pumpout actively squeezes out the negative effects of noisy labels from the training model, instead of passively forgetting these effects. We leverage Pumpout to upgrade two representative methods: MentorNet and Backward Correction. Empirical results on benchmark vision and text datasets demonstrate that Pumpout can significantly improve the robustness of representative methods.", "keywords": ["Noisy Labels", "Deep Learning", "Meta Approach"], "authorids": ["bo.han@riken.jp", "gang.niu@riken.jp", "jiangchao.yao@student.uts.edu.au", "xingrui.yu@student.uts.edu.au", "miao.xu@riken.jp", "ivor.tsang@uts.edu.au", "sugi@k.u-tokyo.ac.jp"], "authors": ["Bo Han", "Gang Niu", "Jiangchao Yao", "Xingrui Yu", "Miao Xu", "Ivor Tsang", "Masashi Sugiyama"], "TL;DR": "Starting from tomorrow, never worry about your DNNs memorizing noisy labels---forget bad labels by Pumpout in an active manner!", "pdf": "/pdf/527948693e0b46f29236bb6e1af3355962881a51.pdf", "paperhash": "han|pumpout_a_meta_approach_for_robustly_training_deep_neural_networks_with_noisy_labels", "_bibtex": "@misc{\nhan2019pumpout,\ntitle={Pumpout: A Meta Approach for Robustly Training Deep Neural Networks with Noisy Labels},\nauthor={Bo Han and Gang Niu and Jiangchao Yao and Xingrui Yu and Miao Xu and Ivor Tsang and Masashi Sugiyama},\nyear={2019},\nurl={https://openreview.net/forum?id=HyG1_j0cYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper317/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621611871, "tddate": null, "super": null, "final": null, "reply": {"forum": "HyG1_j0cYQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper317/Authors", "ICLR.cc/2019/Conference/Paper317/Reviewers", "ICLR.cc/2019/Conference/Paper317/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper317/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper317/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper317/Authors|ICLR.cc/2019/Conference/Paper317/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper317/Reviewers", "ICLR.cc/2019/Conference/Paper317/Authors", "ICLR.cc/2019/Conference/Paper317/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621611871}}}, {"id": "BkefgvT4qm", "original": null, "number": 5, "cdate": 1538737898216, "ddate": null, "tcdate": 1538737898216, "tmdate": 1538737898216, "tddate": null, "forum": "HyG1_j0cYQ", "replyto": "Ske2_4yxcm", "invitation": "ICLR.cc/2019/Conference/-/Paper317/Official_Comment", "content": {"title": "Both [2] and [3] are great papers", "comment": "Both [2] and [3] are great papers, we will definitely cite them in the suitable place later. When our experiments involve the \\epsilon estimation, we will compare them and conduct the analysis."}, "signatures": ["ICLR.cc/2019/Conference/Paper317/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper317/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper317/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pumpout: A Meta Approach for Robustly Training Deep Neural Networks with Noisy Labels", "abstract": "It is challenging to train deep neural networks robustly on the industrial-level data, since labels of such data are heavily noisy, and their label generation processes are normally agnostic. To handle these issues, by using the memorization effects of deep neural networks, we may train deep neural networks on the whole dataset only the first few iterations. Then, we may employ early stopping or the small-loss trick to train them on selected instances. However, in such training procedures, deep neural networks inevitably memorize some noisy labels, which will degrade their generalization. In this paper, we propose a meta algorithm called Pumpout to overcome the problem of memorizing noisy labels. By using scaled stochastic gradient ascent, Pumpout actively squeezes out the negative effects of noisy labels from the training model, instead of passively forgetting these effects. We leverage Pumpout to upgrade two representative methods: MentorNet and Backward Correction. Empirical results on benchmark vision and text datasets demonstrate that Pumpout can significantly improve the robustness of representative methods.", "keywords": ["Noisy Labels", "Deep Learning", "Meta Approach"], "authorids": ["bo.han@riken.jp", "gang.niu@riken.jp", "jiangchao.yao@student.uts.edu.au", "xingrui.yu@student.uts.edu.au", "miao.xu@riken.jp", "ivor.tsang@uts.edu.au", "sugi@k.u-tokyo.ac.jp"], "authors": ["Bo Han", "Gang Niu", "Jiangchao Yao", "Xingrui Yu", "Miao Xu", "Ivor Tsang", "Masashi Sugiyama"], "TL;DR": "Starting from tomorrow, never worry about your DNNs memorizing noisy labels---forget bad labels by Pumpout in an active manner!", "pdf": "/pdf/527948693e0b46f29236bb6e1af3355962881a51.pdf", "paperhash": "han|pumpout_a_meta_approach_for_robustly_training_deep_neural_networks_with_noisy_labels", "_bibtex": "@misc{\nhan2019pumpout,\ntitle={Pumpout: A Meta Approach for Robustly Training Deep Neural Networks with Noisy Labels},\nauthor={Bo Han and Gang Niu and Jiangchao Yao and Xingrui Yu and Miao Xu and Ivor Tsang and Masashi Sugiyama},\nyear={2019},\nurl={https://openreview.net/forum?id=HyG1_j0cYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper317/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621611871, "tddate": null, "super": null, "final": null, "reply": {"forum": "HyG1_j0cYQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper317/Authors", "ICLR.cc/2019/Conference/Paper317/Reviewers", "ICLR.cc/2019/Conference/Paper317/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper317/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper317/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper317/Authors|ICLR.cc/2019/Conference/Paper317/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper317/Reviewers", "ICLR.cc/2019/Conference/Paper317/Authors", "ICLR.cc/2019/Conference/Paper317/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621611871}}}, {"id": "S1lJiNpNcQ", "original": null, "number": 3, "cdate": 1538737302876, "ddate": null, "tcdate": 1538737302876, "tmdate": 1538737747943, "tddate": null, "forum": "HyG1_j0cYQ", "replyto": "HkeJt51eqX", "invitation": "ICLR.cc/2019/Conference/-/Paper317/Official_Comment", "content": {"title": "The reason comes from Data Augmentation", "comment": "The main reason is that training strategies are very different. All above works [1-4] use SGD with a momentum of 0.9, weight decay of 0.0001, and data augmentation. Note that, data augmentation has been widely used in computer vision community, which significantly improves the classification performance of a deep learning model. \n\nHowever,  in our machine learning paper, our focus is to explore the efficacy of Pumpout. Therefore, we use Adam optimizer in all experiments for fair comparison without using data augmentation trick. \n\nFor Figure 3 (c), we have also tested the performance of \"Normal\" baseline using ResNet32 model, and the training strategy follows above works [1-4] using data augmentation. We achieved a similar test accuracy of 81%."}, "signatures": ["ICLR.cc/2019/Conference/Paper317/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper317/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper317/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pumpout: A Meta Approach for Robustly Training Deep Neural Networks with Noisy Labels", "abstract": "It is challenging to train deep neural networks robustly on the industrial-level data, since labels of such data are heavily noisy, and their label generation processes are normally agnostic. To handle these issues, by using the memorization effects of deep neural networks, we may train deep neural networks on the whole dataset only the first few iterations. Then, we may employ early stopping or the small-loss trick to train them on selected instances. However, in such training procedures, deep neural networks inevitably memorize some noisy labels, which will degrade their generalization. In this paper, we propose a meta algorithm called Pumpout to overcome the problem of memorizing noisy labels. By using scaled stochastic gradient ascent, Pumpout actively squeezes out the negative effects of noisy labels from the training model, instead of passively forgetting these effects. We leverage Pumpout to upgrade two representative methods: MentorNet and Backward Correction. Empirical results on benchmark vision and text datasets demonstrate that Pumpout can significantly improve the robustness of representative methods.", "keywords": ["Noisy Labels", "Deep Learning", "Meta Approach"], "authorids": ["bo.han@riken.jp", "gang.niu@riken.jp", "jiangchao.yao@student.uts.edu.au", "xingrui.yu@student.uts.edu.au", "miao.xu@riken.jp", "ivor.tsang@uts.edu.au", "sugi@k.u-tokyo.ac.jp"], "authors": ["Bo Han", "Gang Niu", "Jiangchao Yao", "Xingrui Yu", "Miao Xu", "Ivor Tsang", "Masashi Sugiyama"], "TL;DR": "Starting from tomorrow, never worry about your DNNs memorizing noisy labels---forget bad labels by Pumpout in an active manner!", "pdf": "/pdf/527948693e0b46f29236bb6e1af3355962881a51.pdf", "paperhash": "han|pumpout_a_meta_approach_for_robustly_training_deep_neural_networks_with_noisy_labels", "_bibtex": "@misc{\nhan2019pumpout,\ntitle={Pumpout: A Meta Approach for Robustly Training Deep Neural Networks with Noisy Labels},\nauthor={Bo Han and Gang Niu and Jiangchao Yao and Xingrui Yu and Miao Xu and Ivor Tsang and Masashi Sugiyama},\nyear={2019},\nurl={https://openreview.net/forum?id=HyG1_j0cYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper317/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621611871, "tddate": null, "super": null, "final": null, "reply": {"forum": "HyG1_j0cYQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper317/Authors", "ICLR.cc/2019/Conference/Paper317/Reviewers", "ICLR.cc/2019/Conference/Paper317/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper317/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper317/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper317/Authors|ICLR.cc/2019/Conference/Paper317/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper317/Reviewers", "ICLR.cc/2019/Conference/Paper317/Authors", "ICLR.cc/2019/Conference/Paper317/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621611871}}}, {"id": "H1eGMrT49m", "original": null, "number": 4, "cdate": 1538737417986, "ddate": null, "tcdate": 1538737417986, "tmdate": 1538737594212, "tddate": null, "forum": "HyG1_j0cYQ", "replyto": "S1luH11e5X", "invitation": "ICLR.cc/2019/Conference/-/Paper317/Official_Comment", "content": {"title": "Pumpout and Co-teaching are totally different", "comment": "The differences between Pumpout and Co-teaching [1] is obvious and significant. The idea of Co-teaching [1] is to train two deep neural networks, and each network samples small-loss instances to update the parameters of its peer network. \n\nHowever, Pumpout is a meta approach, which aims to benefit orthogonal algorithms in deep learning with noisy labels (i.e., MentorNet, Co-teaching, Backward Correction etc.). The idea of Pumpout is to actively squeeze out the negative effects of noisy labels from the training model, instead of passively forgetting these effects. Specifically, Pumpout conducts stochastic gradient descent on \u201cfitting\u201d labels; and conducts scaled stochastic gradient ascent on \u2018not-fitting\u2019 labels instead of stopping gradient computation. The \u201cfitting\u201d labels are not limted to be the selected clean labels. \n\nTo verify the efficacy of Pumpout, we leverage Pumpout to upgrade two representative but orthogonal approaches: MentorNet and Backward Correction. Note that, Co-teaching shares the similar direction with MentorNet. Thus, we do not need to compare it with [1] here. The potential comparison in future should between \u201cCo-teaching\u201d and \u201cCo-teaching + Pumpout\u201d. \n\nMoreover, we will add citation to Figure 1 in the updated version. For [2] and [3], we will cite them in our updated version, when our experiments involve the \\epsilon estimation."}, "signatures": ["ICLR.cc/2019/Conference/Paper317/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper317/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper317/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pumpout: A Meta Approach for Robustly Training Deep Neural Networks with Noisy Labels", "abstract": "It is challenging to train deep neural networks robustly on the industrial-level data, since labels of such data are heavily noisy, and their label generation processes are normally agnostic. To handle these issues, by using the memorization effects of deep neural networks, we may train deep neural networks on the whole dataset only the first few iterations. Then, we may employ early stopping or the small-loss trick to train them on selected instances. However, in such training procedures, deep neural networks inevitably memorize some noisy labels, which will degrade their generalization. In this paper, we propose a meta algorithm called Pumpout to overcome the problem of memorizing noisy labels. By using scaled stochastic gradient ascent, Pumpout actively squeezes out the negative effects of noisy labels from the training model, instead of passively forgetting these effects. We leverage Pumpout to upgrade two representative methods: MentorNet and Backward Correction. Empirical results on benchmark vision and text datasets demonstrate that Pumpout can significantly improve the robustness of representative methods.", "keywords": ["Noisy Labels", "Deep Learning", "Meta Approach"], "authorids": ["bo.han@riken.jp", "gang.niu@riken.jp", "jiangchao.yao@student.uts.edu.au", "xingrui.yu@student.uts.edu.au", "miao.xu@riken.jp", "ivor.tsang@uts.edu.au", "sugi@k.u-tokyo.ac.jp"], "authors": ["Bo Han", "Gang Niu", "Jiangchao Yao", "Xingrui Yu", "Miao Xu", "Ivor Tsang", "Masashi Sugiyama"], "TL;DR": "Starting from tomorrow, never worry about your DNNs memorizing noisy labels---forget bad labels by Pumpout in an active manner!", "pdf": "/pdf/527948693e0b46f29236bb6e1af3355962881a51.pdf", "paperhash": "han|pumpout_a_meta_approach_for_robustly_training_deep_neural_networks_with_noisy_labels", "_bibtex": "@misc{\nhan2019pumpout,\ntitle={Pumpout: A Meta Approach for Robustly Training Deep Neural Networks with Noisy Labels},\nauthor={Bo Han and Gang Niu and Jiangchao Yao and Xingrui Yu and Miao Xu and Ivor Tsang and Masashi Sugiyama},\nyear={2019},\nurl={https://openreview.net/forum?id=HyG1_j0cYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper317/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621611871, "tddate": null, "super": null, "final": null, "reply": {"forum": "HyG1_j0cYQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper317/Authors", "ICLR.cc/2019/Conference/Paper317/Reviewers", "ICLR.cc/2019/Conference/Paper317/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper317/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper317/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper317/Authors|ICLR.cc/2019/Conference/Paper317/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper317/Reviewers", "ICLR.cc/2019/Conference/Paper317/Authors", "ICLR.cc/2019/Conference/Paper317/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621611871}}}, {"id": "Ske2_4yxcm", "original": null, "number": 12, "cdate": 1538417779629, "ddate": null, "tcdate": 1538417779629, "tmdate": 1538420409597, "tddate": null, "forum": "HyG1_j0cYQ", "replyto": "ByehhGylqQ", "invitation": "ICLR.cc/2019/Conference/-/Paper317/Public_Comment", "content": {"comment": "Their paper builds on a NIPS 2018 paper. If your question is about the ICLR review process, then they probably consider ICML 2018 \"prior art\" but not necessarily NIPS 2018 papers except for NIPS 2018 papers authors choose to cite. Hence \"[2] or [3]\" not \"[2] and [3].\"", "title": "Reply"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper317/Reviewers/Unsubmitted"], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pumpout: A Meta Approach for Robustly Training Deep Neural Networks with Noisy Labels", "abstract": "It is challenging to train deep neural networks robustly on the industrial-level data, since labels of such data are heavily noisy, and their label generation processes are normally agnostic. To handle these issues, by using the memorization effects of deep neural networks, we may train deep neural networks on the whole dataset only the first few iterations. Then, we may employ early stopping or the small-loss trick to train them on selected instances. However, in such training procedures, deep neural networks inevitably memorize some noisy labels, which will degrade their generalization. In this paper, we propose a meta algorithm called Pumpout to overcome the problem of memorizing noisy labels. By using scaled stochastic gradient ascent, Pumpout actively squeezes out the negative effects of noisy labels from the training model, instead of passively forgetting these effects. We leverage Pumpout to upgrade two representative methods: MentorNet and Backward Correction. Empirical results on benchmark vision and text datasets demonstrate that Pumpout can significantly improve the robustness of representative methods.", "keywords": ["Noisy Labels", "Deep Learning", "Meta Approach"], "authorids": ["bo.han@riken.jp", "gang.niu@riken.jp", "jiangchao.yao@student.uts.edu.au", "xingrui.yu@student.uts.edu.au", "miao.xu@riken.jp", "ivor.tsang@uts.edu.au", "sugi@k.u-tokyo.ac.jp"], "authors": ["Bo Han", "Gang Niu", "Jiangchao Yao", "Xingrui Yu", "Miao Xu", "Ivor Tsang", "Masashi Sugiyama"], "TL;DR": "Starting from tomorrow, never worry about your DNNs memorizing noisy labels---forget bad labels by Pumpout in an active manner!", "pdf": "/pdf/527948693e0b46f29236bb6e1af3355962881a51.pdf", "paperhash": "han|pumpout_a_meta_approach_for_robustly_training_deep_neural_networks_with_noisy_labels", "_bibtex": "@misc{\nhan2019pumpout,\ntitle={Pumpout: A Meta Approach for Robustly Training Deep Neural Networks with Noisy Labels},\nauthor={Bo Han and Gang Niu and Jiangchao Yao and Xingrui Yu and Miao Xu and Ivor Tsang and Masashi Sugiyama},\nyear={2019},\nurl={https://openreview.net/forum?id=HyG1_j0cYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper317/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311868292, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "HyG1_j0cYQ", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper317/Authors", "ICLR.cc/2019/Conference/Paper317/Reviewers", "ICLR.cc/2019/Conference/Paper317/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper317/Authors", "ICLR.cc/2019/Conference/Paper317/Reviewers", "ICLR.cc/2019/Conference/Paper317/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311868292}}}, {"id": "HkeJt51eqX", "original": null, "number": 13, "cdate": 1538419318837, "ddate": null, "tcdate": 1538419318837, "tmdate": 1538419318837, "tddate": null, "forum": "HyG1_j0cYQ", "replyto": "HyG1_j0cYQ", "invitation": "ICLR.cc/2019/Conference/-/Paper317/Public_Comment", "content": {"comment": "I read your paper on Pumpout, and I think it offers an interesting new perspective on training DNNs with noisy labels. However, I have a question about your experiments on CIFAR-10. From Figure 3 (c) of your paper.  It seems that the performance of your \"Normal\" baselines using Resnet-32 is not as good as baselines by recent papers ([1][2][3][4]) which did experiments with similar architectures (resnet-32 and resnet-44). Do you have any explanation for why this might have happened? \n\n[1] Patrini, Giorgio, et al. \"Making deep neural networks robust to label noise: A loss correction approach.\" Proc. IEEE Conf. Comput. Vis. Pattern Recognit.(CVPR). 2017.\n[2] Zhang, Zhilu, and Mert R. Sabuncu. \"Generalized Cross Entropy Loss for Training Deep Neural Networks with Noisy Labels.\" arXiv preprint arXiv:1805.07836 (2018).\n[3] Ma, Xingjun, et al. \"Dimensionality-Driven Learning with Noisy Labels.\" arXiv preprint arXiv:1806.02612 (2018).\n[4] Tanaka, Daiki, et al. \"Joint optimization framework for learning with noisy labels.\" arXiv preprint arXiv:1803.11364 (2018).\n", "title": "Question about your experiments with CIFAR-10"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper317/Reviewers/Unsubmitted"], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pumpout: A Meta Approach for Robustly Training Deep Neural Networks with Noisy Labels", "abstract": "It is challenging to train deep neural networks robustly on the industrial-level data, since labels of such data are heavily noisy, and their label generation processes are normally agnostic. To handle these issues, by using the memorization effects of deep neural networks, we may train deep neural networks on the whole dataset only the first few iterations. Then, we may employ early stopping or the small-loss trick to train them on selected instances. However, in such training procedures, deep neural networks inevitably memorize some noisy labels, which will degrade their generalization. In this paper, we propose a meta algorithm called Pumpout to overcome the problem of memorizing noisy labels. By using scaled stochastic gradient ascent, Pumpout actively squeezes out the negative effects of noisy labels from the training model, instead of passively forgetting these effects. We leverage Pumpout to upgrade two representative methods: MentorNet and Backward Correction. Empirical results on benchmark vision and text datasets demonstrate that Pumpout can significantly improve the robustness of representative methods.", "keywords": ["Noisy Labels", "Deep Learning", "Meta Approach"], "authorids": ["bo.han@riken.jp", "gang.niu@riken.jp", "jiangchao.yao@student.uts.edu.au", "xingrui.yu@student.uts.edu.au", "miao.xu@riken.jp", "ivor.tsang@uts.edu.au", "sugi@k.u-tokyo.ac.jp"], "authors": ["Bo Han", "Gang Niu", "Jiangchao Yao", "Xingrui Yu", "Miao Xu", "Ivor Tsang", "Masashi Sugiyama"], "TL;DR": "Starting from tomorrow, never worry about your DNNs memorizing noisy labels---forget bad labels by Pumpout in an active manner!", "pdf": "/pdf/527948693e0b46f29236bb6e1af3355962881a51.pdf", "paperhash": "han|pumpout_a_meta_approach_for_robustly_training_deep_neural_networks_with_noisy_labels", "_bibtex": "@misc{\nhan2019pumpout,\ntitle={Pumpout: A Meta Approach for Robustly Training Deep Neural Networks with Noisy Labels},\nauthor={Bo Han and Gang Niu and Jiangchao Yao and Xingrui Yu and Miao Xu and Ivor Tsang and Masashi Sugiyama},\nyear={2019},\nurl={https://openreview.net/forum?id=HyG1_j0cYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper317/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311868292, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "HyG1_j0cYQ", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper317/Authors", "ICLR.cc/2019/Conference/Paper317/Reviewers", "ICLR.cc/2019/Conference/Paper317/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper317/Authors", "ICLR.cc/2019/Conference/Paper317/Reviewers", "ICLR.cc/2019/Conference/Paper317/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311868292}}}, {"id": "ByehhGylqQ", "original": null, "number": 11, "cdate": 1538417331624, "ddate": null, "tcdate": 1538417331624, "tmdate": 1538417331624, "tddate": null, "forum": "HyG1_j0cYQ", "replyto": "S1luH11e5X", "invitation": "ICLR.cc/2019/Conference/-/Paper317/Public_Comment", "content": {"comment": "I see that you have mentioned 2 NIPS paper that are accepted at this years' NIPS which has not held which means that these accepted papers are not officially published yet and are only available at ArXiV. is it ok to compare a work which is not officially published as of now?", "title": "Question about unpublished but accepted papers"}, "signatures": ["~Hassam_Sheikh1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper317/Reviewers/Unsubmitted"], "writers": ["~Hassam_Sheikh1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pumpout: A Meta Approach for Robustly Training Deep Neural Networks with Noisy Labels", "abstract": "It is challenging to train deep neural networks robustly on the industrial-level data, since labels of such data are heavily noisy, and their label generation processes are normally agnostic. To handle these issues, by using the memorization effects of deep neural networks, we may train deep neural networks on the whole dataset only the first few iterations. Then, we may employ early stopping or the small-loss trick to train them on selected instances. However, in such training procedures, deep neural networks inevitably memorize some noisy labels, which will degrade their generalization. In this paper, we propose a meta algorithm called Pumpout to overcome the problem of memorizing noisy labels. By using scaled stochastic gradient ascent, Pumpout actively squeezes out the negative effects of noisy labels from the training model, instead of passively forgetting these effects. We leverage Pumpout to upgrade two representative methods: MentorNet and Backward Correction. Empirical results on benchmark vision and text datasets demonstrate that Pumpout can significantly improve the robustness of representative methods.", "keywords": ["Noisy Labels", "Deep Learning", "Meta Approach"], "authorids": ["bo.han@riken.jp", "gang.niu@riken.jp", "jiangchao.yao@student.uts.edu.au", "xingrui.yu@student.uts.edu.au", "miao.xu@riken.jp", "ivor.tsang@uts.edu.au", "sugi@k.u-tokyo.ac.jp"], "authors": ["Bo Han", "Gang Niu", "Jiangchao Yao", "Xingrui Yu", "Miao Xu", "Ivor Tsang", "Masashi Sugiyama"], "TL;DR": "Starting from tomorrow, never worry about your DNNs memorizing noisy labels---forget bad labels by Pumpout in an active manner!", "pdf": "/pdf/527948693e0b46f29236bb6e1af3355962881a51.pdf", "paperhash": "han|pumpout_a_meta_approach_for_robustly_training_deep_neural_networks_with_noisy_labels", "_bibtex": "@misc{\nhan2019pumpout,\ntitle={Pumpout: A Meta Approach for Robustly Training Deep Neural Networks with Noisy Labels},\nauthor={Bo Han and Gang Niu and Jiangchao Yao and Xingrui Yu and Miao Xu and Ivor Tsang and Masashi Sugiyama},\nyear={2019},\nurl={https://openreview.net/forum?id=HyG1_j0cYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper317/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311868292, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "HyG1_j0cYQ", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper317/Authors", "ICLR.cc/2019/Conference/Paper317/Reviewers", "ICLR.cc/2019/Conference/Paper317/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper317/Authors", "ICLR.cc/2019/Conference/Paper317/Reviewers", "ICLR.cc/2019/Conference/Paper317/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311868292}}}, {"id": "S1luH11e5X", "original": null, "number": 9, "cdate": 1538416447672, "ddate": null, "tcdate": 1538416447672, "tmdate": 1538416447672, "tddate": null, "forum": "HyG1_j0cYQ", "replyto": "HyG1_j0cYQ", "invitation": "ICLR.cc/2019/Conference/-/Paper317/Public_Comment", "content": {"comment": "This paper has many similarities to [1]. Superficially, Figure 1 is identical to [1]'s figure. Algorithmically, the method is highly similar too. The authors acknowledge the algorithmic similarity, but they compare to MentorNet and do little to show an improvement over [1].\nAlso, the method requires a label noise estimate. They say \"We assume the noise level \\epsilon is known and... If \\epsilon is not known in advance, \\epsilon can be inferred using validation sets.\" In practice, \\epsilon needs to be estimated by manually labeling some examples. But if a small set of clean data is available, then the authors should compare to the label corruption techniques of [2] or [3] since these approaches assume access to a small trusted set of examples.\n[1] Bo Han, Quanming Yao, Xingrui Yu, Gang Niu, Miao Xu, Weihua Hu, Ivor Tsang, Masashi Sugiyama. Co-teaching: Robust Training Deep Neural Networks with Extremely Noisy Labels. NIPS, 2018.\n[2] Dan Hendrycks, Mantas Mazeika, Duncan Wilson, Kevin Gimpel. Using Trusted Data to Train Deep Networks on Labels Corrupted by Severe Noise. NIPS, 2018.\n[3] Mengye Ren, Wenyuan Zeng, Bin Yang, Raquel Urtasun. Learning to Reweight Examples for Robust Deep Learning. ICML, 2018.", "title": "Related Work"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper317/Reviewers/Unsubmitted"], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pumpout: A Meta Approach for Robustly Training Deep Neural Networks with Noisy Labels", "abstract": "It is challenging to train deep neural networks robustly on the industrial-level data, since labels of such data are heavily noisy, and their label generation processes are normally agnostic. To handle these issues, by using the memorization effects of deep neural networks, we may train deep neural networks on the whole dataset only the first few iterations. Then, we may employ early stopping or the small-loss trick to train them on selected instances. However, in such training procedures, deep neural networks inevitably memorize some noisy labels, which will degrade their generalization. In this paper, we propose a meta algorithm called Pumpout to overcome the problem of memorizing noisy labels. By using scaled stochastic gradient ascent, Pumpout actively squeezes out the negative effects of noisy labels from the training model, instead of passively forgetting these effects. We leverage Pumpout to upgrade two representative methods: MentorNet and Backward Correction. Empirical results on benchmark vision and text datasets demonstrate that Pumpout can significantly improve the robustness of representative methods.", "keywords": ["Noisy Labels", "Deep Learning", "Meta Approach"], "authorids": ["bo.han@riken.jp", "gang.niu@riken.jp", "jiangchao.yao@student.uts.edu.au", "xingrui.yu@student.uts.edu.au", "miao.xu@riken.jp", "ivor.tsang@uts.edu.au", "sugi@k.u-tokyo.ac.jp"], "authors": ["Bo Han", "Gang Niu", "Jiangchao Yao", "Xingrui Yu", "Miao Xu", "Ivor Tsang", "Masashi Sugiyama"], "TL;DR": "Starting from tomorrow, never worry about your DNNs memorizing noisy labels---forget bad labels by Pumpout in an active manner!", "pdf": "/pdf/527948693e0b46f29236bb6e1af3355962881a51.pdf", "paperhash": "han|pumpout_a_meta_approach_for_robustly_training_deep_neural_networks_with_noisy_labels", "_bibtex": "@misc{\nhan2019pumpout,\ntitle={Pumpout: A Meta Approach for Robustly Training Deep Neural Networks with Noisy Labels},\nauthor={Bo Han and Gang Niu and Jiangchao Yao and Xingrui Yu and Miao Xu and Ivor Tsang and Masashi Sugiyama},\nyear={2019},\nurl={https://openreview.net/forum?id=HyG1_j0cYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper317/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311868292, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "HyG1_j0cYQ", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper317/Authors", "ICLR.cc/2019/Conference/Paper317/Reviewers", "ICLR.cc/2019/Conference/Paper317/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper317/Authors", "ICLR.cc/2019/Conference/Paper317/Reviewers", "ICLR.cc/2019/Conference/Paper317/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311868292}}}, {"id": "Sklly4ZkqQ", "original": null, "number": 1, "cdate": 1538360280031, "ddate": null, "tcdate": 1538360280031, "tmdate": 1538360280031, "tddate": null, "forum": "HyG1_j0cYQ", "replyto": "S1lK0T3stQ", "invitation": "ICLR.cc/2019/Conference/-/Paper317/Official_Comment", "content": {"title": "Your understanding is correct", "comment": "Yes, your understanding is correct. The idea of our Pumpout is to use scaled stochastic gradient ascent to actively squeezes out the negative effects of noisy labels from the training model. The realization is simple and general, which will benefit orthogonal techniques in the area of deep learning with noisy labels."}, "signatures": ["ICLR.cc/2019/Conference/Paper317/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper317/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper317/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pumpout: A Meta Approach for Robustly Training Deep Neural Networks with Noisy Labels", "abstract": "It is challenging to train deep neural networks robustly on the industrial-level data, since labels of such data are heavily noisy, and their label generation processes are normally agnostic. To handle these issues, by using the memorization effects of deep neural networks, we may train deep neural networks on the whole dataset only the first few iterations. Then, we may employ early stopping or the small-loss trick to train them on selected instances. However, in such training procedures, deep neural networks inevitably memorize some noisy labels, which will degrade their generalization. In this paper, we propose a meta algorithm called Pumpout to overcome the problem of memorizing noisy labels. By using scaled stochastic gradient ascent, Pumpout actively squeezes out the negative effects of noisy labels from the training model, instead of passively forgetting these effects. We leverage Pumpout to upgrade two representative methods: MentorNet and Backward Correction. Empirical results on benchmark vision and text datasets demonstrate that Pumpout can significantly improve the robustness of representative methods.", "keywords": ["Noisy Labels", "Deep Learning", "Meta Approach"], "authorids": ["bo.han@riken.jp", "gang.niu@riken.jp", "jiangchao.yao@student.uts.edu.au", "xingrui.yu@student.uts.edu.au", "miao.xu@riken.jp", "ivor.tsang@uts.edu.au", "sugi@k.u-tokyo.ac.jp"], "authors": ["Bo Han", "Gang Niu", "Jiangchao Yao", "Xingrui Yu", "Miao Xu", "Ivor Tsang", "Masashi Sugiyama"], "TL;DR": "Starting from tomorrow, never worry about your DNNs memorizing noisy labels---forget bad labels by Pumpout in an active manner!", "pdf": "/pdf/527948693e0b46f29236bb6e1af3355962881a51.pdf", "paperhash": "han|pumpout_a_meta_approach_for_robustly_training_deep_neural_networks_with_noisy_labels", "_bibtex": "@misc{\nhan2019pumpout,\ntitle={Pumpout: A Meta Approach for Robustly Training Deep Neural Networks with Noisy Labels},\nauthor={Bo Han and Gang Niu and Jiangchao Yao and Xingrui Yu and Miao Xu and Ivor Tsang and Masashi Sugiyama},\nyear={2019},\nurl={https://openreview.net/forum?id=HyG1_j0cYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper317/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621611871, "tddate": null, "super": null, "final": null, "reply": {"forum": "HyG1_j0cYQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper317/Authors", "ICLR.cc/2019/Conference/Paper317/Reviewers", "ICLR.cc/2019/Conference/Paper317/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper317/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper317/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper317/Authors|ICLR.cc/2019/Conference/Paper317/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper317/Reviewers", "ICLR.cc/2019/Conference/Paper317/Authors", "ICLR.cc/2019/Conference/Paper317/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621611871}}}, {"id": "S1lK0T3stQ", "original": null, "number": 1, "cdate": 1538145745352, "ddate": null, "tcdate": 1538145745352, "tmdate": 1538145745352, "tddate": null, "forum": "HyG1_j0cYQ", "replyto": "HyG1_j0cYQ", "invitation": "ICLR.cc/2019/Conference/-/Paper317/Public_Comment", "content": {"title": "Simple and smart idea", "comment": "Pumpout gives an smart option for how to use information of noisy labels. Traditionally, we try to ignore the information by using data cleansing or robust reweighting methods. In this paper, the authors proposed using a scaled stochastic gradient ascent direction (instead of the normal gradient descent direction) as a more possibly correct update direction for noisy data. This idea is simple but reasonable and smart."}, "signatures": ["~Xiyu_Yu1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper317/Reviewers/Unsubmitted"], "writers": ["~Xiyu_Yu1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pumpout: A Meta Approach for Robustly Training Deep Neural Networks with Noisy Labels", "abstract": "It is challenging to train deep neural networks robustly on the industrial-level data, since labels of such data are heavily noisy, and their label generation processes are normally agnostic. To handle these issues, by using the memorization effects of deep neural networks, we may train deep neural networks on the whole dataset only the first few iterations. Then, we may employ early stopping or the small-loss trick to train them on selected instances. However, in such training procedures, deep neural networks inevitably memorize some noisy labels, which will degrade their generalization. In this paper, we propose a meta algorithm called Pumpout to overcome the problem of memorizing noisy labels. By using scaled stochastic gradient ascent, Pumpout actively squeezes out the negative effects of noisy labels from the training model, instead of passively forgetting these effects. We leverage Pumpout to upgrade two representative methods: MentorNet and Backward Correction. Empirical results on benchmark vision and text datasets demonstrate that Pumpout can significantly improve the robustness of representative methods.", "keywords": ["Noisy Labels", "Deep Learning", "Meta Approach"], "authorids": ["bo.han@riken.jp", "gang.niu@riken.jp", "jiangchao.yao@student.uts.edu.au", "xingrui.yu@student.uts.edu.au", "miao.xu@riken.jp", "ivor.tsang@uts.edu.au", "sugi@k.u-tokyo.ac.jp"], "authors": ["Bo Han", "Gang Niu", "Jiangchao Yao", "Xingrui Yu", "Miao Xu", "Ivor Tsang", "Masashi Sugiyama"], "TL;DR": "Starting from tomorrow, never worry about your DNNs memorizing noisy labels---forget bad labels by Pumpout in an active manner!", "pdf": "/pdf/527948693e0b46f29236bb6e1af3355962881a51.pdf", "paperhash": "han|pumpout_a_meta_approach_for_robustly_training_deep_neural_networks_with_noisy_labels", "_bibtex": "@misc{\nhan2019pumpout,\ntitle={Pumpout: A Meta Approach for Robustly Training Deep Neural Networks with Noisy Labels},\nauthor={Bo Han and Gang Niu and Jiangchao Yao and Xingrui Yu and Miao Xu and Ivor Tsang and Masashi Sugiyama},\nyear={2019},\nurl={https://openreview.net/forum?id=HyG1_j0cYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper317/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311868292, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "HyG1_j0cYQ", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper317/Authors", "ICLR.cc/2019/Conference/Paper317/Reviewers", "ICLR.cc/2019/Conference/Paper317/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper317/Authors", "ICLR.cc/2019/Conference/Paper317/Reviewers", "ICLR.cc/2019/Conference/Paper317/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311868292}}}], "count": 28}