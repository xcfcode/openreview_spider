{"notes": [{"id": "r1lFIiR9tQ", "original": "B1xPo34qtQ", "number": 195, "cdate": 1538087761178, "ddate": null, "tcdate": 1538087761178, "tmdate": 1545355390978, "tddate": null, "forum": "r1lFIiR9tQ", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Training generative latent models  by variational f-divergence minimization", "abstract": "Probabilistic models are often trained by maximum likelihood, which corresponds to minimizing a specific form of f-divergence between the model and data distribution. We derive an upper bound that holds for all f-divergences, showing the intuitive result that the divergence between two joint distributions is at least as great as the divergence between their corresponding marginals. Additionally, the f-divergence is not formally defined when two distributions have different supports. We thus propose a noisy version of f-divergence which is well defined in such situations. We demonstrate how the bound and the new version of f-divergence can be readily used to train complex probabilistic generative models of data and that the fitted model can depend significantly on the particular divergence used.", "keywords": ["variational inference", "generative model", "f divergence"], "authorids": ["mingtian.zhang.17@ucl.ac.uk", "thomas.bird@cs.ucl.ac.uk", "raza.habib@cs.ucl.ac.uk", "t.xu12@lse.ac.uk", "david.barber@ucl.ac.uk"], "authors": ["Mingtian Zhang", "Thomas Bird", "Raza Habib", "Tianlin Xu", "David Barber"], "TL;DR": "Training generative models using an upper bound of the f divergence.", "pdf": "/pdf/9cc42d11aa2483056869d06180949c7feab06c0d.pdf", "paperhash": "zhang|training_generative_latent_models_by_variational_fdivergence_minimization", "_bibtex": "@misc{\nzhang2019training,\ntitle={Training generative latent models  by variational f-divergence minimization},\nauthor={Mingtian Zhang and Thomas Bird and Raza Habib and Tianlin Xu and David Barber},\nyear={2019},\nurl={https://openreview.net/forum?id=r1lFIiR9tQ},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "SJlUjT3gxV", "original": null, "number": 1, "cdate": 1544764829656, "ddate": null, "tcdate": 1544764829656, "tmdate": 1545354519984, "tddate": null, "forum": "r1lFIiR9tQ", "replyto": "r1lFIiR9tQ", "invitation": "ICLR.cc/2019/Conference/-/Paper195/Meta_Review", "content": {"metareview": "The paper proposes a new method for training generative models by minimizing general f-divergences. The main technical idea is to optimize f-divergence between joint distributions which is rightly observed to be the upper bound of the f-divergence between the marginal distributions and address the disjoint support problem by convolving the data with a noise distribution.  The basic ideas in this work are not completely novel but are put together in a new way. \n\nHowever, the key weakness of this work, as all the reviewer noticed, is that the empirical results are too week to support the usefulness of the proposed approach. The only quantitive results are in table 2, which is only a simple Gaussian example. It essential to have more substantial empirical results for supporting the new algorithm. \n", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Reject", "title": "Rejection: interest idea but empirical results are too week "}, "signatures": ["ICLR.cc/2019/Conference/Paper195/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper195/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training generative latent models  by variational f-divergence minimization", "abstract": "Probabilistic models are often trained by maximum likelihood, which corresponds to minimizing a specific form of f-divergence between the model and data distribution. We derive an upper bound that holds for all f-divergences, showing the intuitive result that the divergence between two joint distributions is at least as great as the divergence between their corresponding marginals. Additionally, the f-divergence is not formally defined when two distributions have different supports. We thus propose a noisy version of f-divergence which is well defined in such situations. We demonstrate how the bound and the new version of f-divergence can be readily used to train complex probabilistic generative models of data and that the fitted model can depend significantly on the particular divergence used.", "keywords": ["variational inference", "generative model", "f divergence"], "authorids": ["mingtian.zhang.17@ucl.ac.uk", "thomas.bird@cs.ucl.ac.uk", "raza.habib@cs.ucl.ac.uk", "t.xu12@lse.ac.uk", "david.barber@ucl.ac.uk"], "authors": ["Mingtian Zhang", "Thomas Bird", "Raza Habib", "Tianlin Xu", "David Barber"], "TL;DR": "Training generative models using an upper bound of the f divergence.", "pdf": "/pdf/9cc42d11aa2483056869d06180949c7feab06c0d.pdf", "paperhash": "zhang|training_generative_latent_models_by_variational_fdivergence_minimization", "_bibtex": "@misc{\nzhang2019training,\ntitle={Training generative latent models  by variational f-divergence minimization},\nauthor={Mingtian Zhang and Thomas Bird and Raza Habib and Tianlin Xu and David Barber},\nyear={2019},\nurl={https://openreview.net/forum?id=r1lFIiR9tQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper195/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353302846, "tddate": null, "super": null, "final": null, "reply": {"forum": "r1lFIiR9tQ", "replyto": "r1lFIiR9tQ", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper195/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper195/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper195/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353302846}}}, {"id": "BklKFxuFC7", "original": null, "number": 3, "cdate": 1543237760591, "ddate": null, "tcdate": 1543237760591, "tmdate": 1543237760591, "tddate": null, "forum": "r1lFIiR9tQ", "replyto": "H1lMADZ727", "invitation": "ICLR.cc/2019/Conference/-/Paper195/Official_Comment", "content": {"title": "Response to reviewer 3", "comment": "Thank you for the constructive feedback on our paper. We respond below to the points raised.\n\n> \"For me, this seems that the author just introduced the new fancy objective. I think the motivation to introduce the new objective function should be stated clearly.\"\n\nWe would argue that we haven't really introduced a new objective at all, but simply provided a tool to expand the scope of existing objectives to a wider family of models. Using probabilistic divergences as training objectives is a standard tool in generative modelling. Indeed the VAE is simply a special case of a general f-divergence objective.\n\nWe do provide motivation for studying different training objectives in section 2. Simply put - different objectives result in different learned models, and so it can be of use to use a different objective depending on the desired use case for the model. For example if sharp samples are required then the reverse KL could be a suitable objective, whereas if good density estimation is required then the forward KL (maximum likelihood) may be a better objective.\n\nHowever, it is usually impossible to train a latent generative model that uses a divergence other than forward KL as the objective unless one uses a GAN style minimax optimization. The upper bound we propose in our paper both permits using any f-divergence as the objective, and is optimized via the minimization of an upper bound, which is more natural than the alternate tightening and minimizing of the Fenchel conjugate lower bound seen in the f-GAN optimization.\n\n> \"The only evidence that the learning tends to be stable is the Fig.8 in the appendix, but this is just the fitting of univariate Gaussian to a mixture of Gaussians, thus it is too weak as the evidence.\"\n\nThe comparison done to f-GAN training illustrates the advantage of minimizing an upper bound on the objective rather than alternately tightening and then minimizing a lower bound. This is only a toy comparison, but demonstrates that training is more straightforward in this case. For more complex examples, since we are minimizing an upper bound we have no reason to expect the training to be unstable.\n\n> \"About the sharp output, there are already many methods to overcome the blurred output of the usual VAE. No comparison is done in the paper.\"\n\nAlthough there are ways to sharpen the output of VAEs , we are not aware of methods which are based on using a different divergence as the objective - which is a simple and principled way to affect the learned model.\n\n> \"In page 4,  the variance of the p(y|x) and ptheta(y|z) are set to be the same. What is the intuition behind this trick? \"\n\nUsing the same variance for both distributions is as per the spread divergence methodology, as it allows us to invert the noise process easily. We have added to section 3.2 to make this clearer. For a deeper discussion see the spread divergence paper https://arxiv.org/abs/1811.08968."}, "signatures": ["ICLR.cc/2019/Conference/Paper195/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper195/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper195/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training generative latent models  by variational f-divergence minimization", "abstract": "Probabilistic models are often trained by maximum likelihood, which corresponds to minimizing a specific form of f-divergence between the model and data distribution. We derive an upper bound that holds for all f-divergences, showing the intuitive result that the divergence between two joint distributions is at least as great as the divergence between their corresponding marginals. Additionally, the f-divergence is not formally defined when two distributions have different supports. We thus propose a noisy version of f-divergence which is well defined in such situations. We demonstrate how the bound and the new version of f-divergence can be readily used to train complex probabilistic generative models of data and that the fitted model can depend significantly on the particular divergence used.", "keywords": ["variational inference", "generative model", "f divergence"], "authorids": ["mingtian.zhang.17@ucl.ac.uk", "thomas.bird@cs.ucl.ac.uk", "raza.habib@cs.ucl.ac.uk", "t.xu12@lse.ac.uk", "david.barber@ucl.ac.uk"], "authors": ["Mingtian Zhang", "Thomas Bird", "Raza Habib", "Tianlin Xu", "David Barber"], "TL;DR": "Training generative models using an upper bound of the f divergence.", "pdf": "/pdf/9cc42d11aa2483056869d06180949c7feab06c0d.pdf", "paperhash": "zhang|training_generative_latent_models_by_variational_fdivergence_minimization", "_bibtex": "@misc{\nzhang2019training,\ntitle={Training generative latent models  by variational f-divergence minimization},\nauthor={Mingtian Zhang and Thomas Bird and Raza Habib and Tianlin Xu and David Barber},\nyear={2019},\nurl={https://openreview.net/forum?id=r1lFIiR9tQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper195/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621607757, "tddate": null, "super": null, "final": null, "reply": {"forum": "r1lFIiR9tQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper195/Authors", "ICLR.cc/2019/Conference/Paper195/Reviewers", "ICLR.cc/2019/Conference/Paper195/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper195/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper195/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper195/Authors|ICLR.cc/2019/Conference/Paper195/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper195/Reviewers", "ICLR.cc/2019/Conference/Paper195/Authors", "ICLR.cc/2019/Conference/Paper195/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621607757}}}, {"id": "SyeXbx_YC7", "original": null, "number": 2, "cdate": 1543237627113, "ddate": null, "tcdate": 1543237627113, "tmdate": 1543237627113, "tddate": null, "forum": "r1lFIiR9tQ", "replyto": "HJlLzHlOn7", "invitation": "ICLR.cc/2019/Conference/-/Paper195/Official_Comment", "content": {"title": "Response to reviewer 2", "comment": "Thank you for the constructive feedback on the paper. We respond to your points below.\n\n> \"2) I cannot find on arXiv the reference \u201cD. Barber, M. Zhang, R. Habib, and T. Bird. Spread divergences. arXiv preprint, 2018.\u201d So I am not sure whether you can take credit from the \u201cspread f-divergence\u201d or not.\"\n\nThe paper has recently been uploaded to Arxiv. Here is the link: http://arxiv.org/abs/1811.08968\n\n> \"3) Important analysis/experiments on several key points are missing, for example, (i) how to specify the variance of the spread divergence in practice? (ii) how to estimate log p(y)? What is the influence?\"\n\nThe width of the noise-corruption process, p(y|x), in the spread divergence is a hyper-parameter to be set and tuned like any other in the training process. We have given in all our experiments the values we used. Conceptually, the larger the width of the spread divergence then the more stable the training, as there will be fewer regions of low probability mass that can destabilize training. However, if the noise is too large it could \"drown out\" the signal from the training data (to illustrate, consider the extreme case of infinite noise). \n\n> \"4) In the paragraph before Sec 4.2, how the sigma of the spread divergence is annealed?\"\n\nWe have added details in the third paragraph of Section 4.1 to clarify this point.\n\n> \"5) Despite the toy experiment in Sec 4.4, what are the advantages of the proposed f-divergence upper bound over the Fenchel-conjugate f-divergence lower bound? The current experimental results barely show any advantage.\"\n\nThis is an excellent question and at the core of our contribution. You are right that both methods aim to minimise an f-divergence but the Fenchel-conjugate achieves this by minimising a *lower bound*.\n\nUsing a lower bound for minimization is a very unnatural thing to do as minimising the bound may simply make it looser. This leads to the need for an unstable min-max training procedure where one first tries to make the bound tight and then minimises. Perhaps the best justification for this unnatural procedure is that it works.\n\nWhat we hope to have shown is that one need not take this approach. Its perfectly possible to get good results by minimizing a more natural upper-bound on the f-divergence and this leads to more stable training.\n\nThe toy comparison illustrates that our method slightly outperforms the lower bound optimization, though they are similar. The more important conclusion from our comparison is that, to achieve similar or even better results, one does not have to adopt the unstable minimax training procedure."}, "signatures": ["ICLR.cc/2019/Conference/Paper195/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper195/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper195/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training generative latent models  by variational f-divergence minimization", "abstract": "Probabilistic models are often trained by maximum likelihood, which corresponds to minimizing a specific form of f-divergence between the model and data distribution. We derive an upper bound that holds for all f-divergences, showing the intuitive result that the divergence between two joint distributions is at least as great as the divergence between their corresponding marginals. Additionally, the f-divergence is not formally defined when two distributions have different supports. We thus propose a noisy version of f-divergence which is well defined in such situations. We demonstrate how the bound and the new version of f-divergence can be readily used to train complex probabilistic generative models of data and that the fitted model can depend significantly on the particular divergence used.", "keywords": ["variational inference", "generative model", "f divergence"], "authorids": ["mingtian.zhang.17@ucl.ac.uk", "thomas.bird@cs.ucl.ac.uk", "raza.habib@cs.ucl.ac.uk", "t.xu12@lse.ac.uk", "david.barber@ucl.ac.uk"], "authors": ["Mingtian Zhang", "Thomas Bird", "Raza Habib", "Tianlin Xu", "David Barber"], "TL;DR": "Training generative models using an upper bound of the f divergence.", "pdf": "/pdf/9cc42d11aa2483056869d06180949c7feab06c0d.pdf", "paperhash": "zhang|training_generative_latent_models_by_variational_fdivergence_minimization", "_bibtex": "@misc{\nzhang2019training,\ntitle={Training generative latent models  by variational f-divergence minimization},\nauthor={Mingtian Zhang and Thomas Bird and Raza Habib and Tianlin Xu and David Barber},\nyear={2019},\nurl={https://openreview.net/forum?id=r1lFIiR9tQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper195/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621607757, "tddate": null, "super": null, "final": null, "reply": {"forum": "r1lFIiR9tQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper195/Authors", "ICLR.cc/2019/Conference/Paper195/Reviewers", "ICLR.cc/2019/Conference/Paper195/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper195/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper195/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper195/Authors|ICLR.cc/2019/Conference/Paper195/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper195/Reviewers", "ICLR.cc/2019/Conference/Paper195/Authors", "ICLR.cc/2019/Conference/Paper195/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621607757}}}, {"id": "rygly1uFR7", "original": null, "number": 1, "cdate": 1543237336088, "ddate": null, "tcdate": 1543237336088, "tmdate": 1543237336088, "tddate": null, "forum": "r1lFIiR9tQ", "replyto": "rkls138q27", "invitation": "ICLR.cc/2019/Conference/-/Paper195/Official_Comment", "content": {"title": "Response to reviewer 1", "comment": "We thank the reviewer for the thoughtful points raised.  We respond to your points below.\n\n> \"Regarding practicality, it appears the training details are specific to each experiment. This begs the question of how sensitive the results are to these settings.\"\n\nThe training details for both MNIST and the CelebA experiments are the same, aside from architectural and hyperparameter differences (which are to be expected). We interleave the same number of \\phi updates between each \\theta update, initialize with a partially trained VAE and use the same gradient estimator detailed in the supplementary materials section B (which has been added since the submission, and resulted in improved performance). The training details for the toy problem are simpler than for MNIST/CelebA, not requiring the VAE initialization or \\phi interleaving. But we would argue that this is not problematic, as the toy problem does not pose the kind of optimization challenges seen in the more complex datasets. \n\n> \"Regarding the methodology, it would have been nice to see the method of Nowozin et al. (2016) applied in all experiments since this is a direct competitor to the proposed method.\"\n\nWe have performed a relatively simple comparison with the f-GAN method of Nowozin et al. in section 4.4. Although it is a toy comparison, it does show some salient differences between our training methods, in particular the beneficial effect of having an upper bound versus a lower bound for minimizing an objective. We have not included the f-GAN results for the other experiments we perform because the performance of the f-GAN is fairly well understood by the machine learning community.\n\n> \"Finally, given that model training is the paper's focus, an explicit discussion of computational cost was missed.\"\n\nThe main computational difference to VAE optimization results from the requirement to calculate gradients of log p(y). We propose an approximate unbiased Monte Carlo estimator for this gradient in the supplementary materials section B. Compared to the VAE, we need to calculate an additional normalizer for each sample in equation 31, whose complexity is O(T*N*D). T is the number of Monte Carlo samples to estimate the gradient, D is the dimension of the data after doing the PCA projection. N is the size of the training dataset, which is a potential computational bottleneck, but we find it is not slow in practice. In section B, we also refer several methods which can do faster unbiased Monte Carlo estimation of the normalizer by using minibatch methods, which we leave to future work."}, "signatures": ["ICLR.cc/2019/Conference/Paper195/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper195/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper195/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training generative latent models  by variational f-divergence minimization", "abstract": "Probabilistic models are often trained by maximum likelihood, which corresponds to minimizing a specific form of f-divergence between the model and data distribution. We derive an upper bound that holds for all f-divergences, showing the intuitive result that the divergence between two joint distributions is at least as great as the divergence between their corresponding marginals. Additionally, the f-divergence is not formally defined when two distributions have different supports. We thus propose a noisy version of f-divergence which is well defined in such situations. We demonstrate how the bound and the new version of f-divergence can be readily used to train complex probabilistic generative models of data and that the fitted model can depend significantly on the particular divergence used.", "keywords": ["variational inference", "generative model", "f divergence"], "authorids": ["mingtian.zhang.17@ucl.ac.uk", "thomas.bird@cs.ucl.ac.uk", "raza.habib@cs.ucl.ac.uk", "t.xu12@lse.ac.uk", "david.barber@ucl.ac.uk"], "authors": ["Mingtian Zhang", "Thomas Bird", "Raza Habib", "Tianlin Xu", "David Barber"], "TL;DR": "Training generative models using an upper bound of the f divergence.", "pdf": "/pdf/9cc42d11aa2483056869d06180949c7feab06c0d.pdf", "paperhash": "zhang|training_generative_latent_models_by_variational_fdivergence_minimization", "_bibtex": "@misc{\nzhang2019training,\ntitle={Training generative latent models  by variational f-divergence minimization},\nauthor={Mingtian Zhang and Thomas Bird and Raza Habib and Tianlin Xu and David Barber},\nyear={2019},\nurl={https://openreview.net/forum?id=r1lFIiR9tQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper195/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621607757, "tddate": null, "super": null, "final": null, "reply": {"forum": "r1lFIiR9tQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper195/Authors", "ICLR.cc/2019/Conference/Paper195/Reviewers", "ICLR.cc/2019/Conference/Paper195/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper195/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper195/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper195/Authors|ICLR.cc/2019/Conference/Paper195/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper195/Reviewers", "ICLR.cc/2019/Conference/Paper195/Authors", "ICLR.cc/2019/Conference/Paper195/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621607757}}}, {"id": "rkls138q27", "original": null, "number": 3, "cdate": 1541200867380, "ddate": null, "tcdate": 1541200867380, "tmdate": 1541534204501, "tddate": null, "forum": "r1lFIiR9tQ", "replyto": "r1lFIiR9tQ", "invitation": "ICLR.cc/2019/Conference/-/Paper195/Official_Review", "content": {"title": "Nice trick to utilize an arbitrary f-divergence as the objective for training generative models.", "review": "The paper proposes a method for training generative models with general f-divergences between the model and empirical distribution (the VAE and GAN objectives are captured as specific instantiations). The trick that leads to computational tractability involves utilizing a latent variable and optimizing the f-divergence between joint distributions which is an upper bound to the (desired) f-divergence between the marginal distributions. Distribution support issues are handled by convolving the data space with a blurring function. Empirical results on image datasets demonstrate that the additional training flexibility results in qualitatively different learned models. Specifically, optimizing the reverse KL (and/or Jensen-Shannon divergence) leads to sharper images, perhaps at the loss of some variance in the data distribution.\n\nI liked the simplicity of the core idea, and appreciated the exposition being generally easy to follow. The application of upper bounds to general f-divergences for training generative models is novel as far as I know. My two issues are with the practicality of the implementation and evaluation methodology, both potentially affecting the significance of the work. Regarding practicality, it appears the training details are specific to each experiment. This begs the question of how sensitive the results are to these settings. Regarding the methodology, it would have been nice to see the method of Nowozin et al. (2016) applied in all experiments since this is a direct competitor to the proposed method. Moreover, the subjective nature of the results in the real dataset experiments makes it difficult to judge what the increased flexibility in training really provides - although I do note that the authors make this same point in the paper. Finally, given that model training is the paper's focus, an explicit discussion of computational cost was missed.\n\nEven with these issues, however, I believe the paper makes a contribution to the important problem of fitting expressive generative models. In my opinion, a more rigorous and thorough experimental exploration would increase the value, but the paper demonstrates that training with alternative f-divergences is feasible.", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper195/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training generative latent models  by variational f-divergence minimization", "abstract": "Probabilistic models are often trained by maximum likelihood, which corresponds to minimizing a specific form of f-divergence between the model and data distribution. We derive an upper bound that holds for all f-divergences, showing the intuitive result that the divergence between two joint distributions is at least as great as the divergence between their corresponding marginals. Additionally, the f-divergence is not formally defined when two distributions have different supports. We thus propose a noisy version of f-divergence which is well defined in such situations. We demonstrate how the bound and the new version of f-divergence can be readily used to train complex probabilistic generative models of data and that the fitted model can depend significantly on the particular divergence used.", "keywords": ["variational inference", "generative model", "f divergence"], "authorids": ["mingtian.zhang.17@ucl.ac.uk", "thomas.bird@cs.ucl.ac.uk", "raza.habib@cs.ucl.ac.uk", "t.xu12@lse.ac.uk", "david.barber@ucl.ac.uk"], "authors": ["Mingtian Zhang", "Thomas Bird", "Raza Habib", "Tianlin Xu", "David Barber"], "TL;DR": "Training generative models using an upper bound of the f divergence.", "pdf": "/pdf/9cc42d11aa2483056869d06180949c7feab06c0d.pdf", "paperhash": "zhang|training_generative_latent_models_by_variational_fdivergence_minimization", "_bibtex": "@misc{\nzhang2019training,\ntitle={Training generative latent models  by variational f-divergence minimization},\nauthor={Mingtian Zhang and Thomas Bird and Raza Habib and Tianlin Xu and David Barber},\nyear={2019},\nurl={https://openreview.net/forum?id=r1lFIiR9tQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper195/Official_Review", "cdate": 1542234517500, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "r1lFIiR9tQ", "replyto": "r1lFIiR9tQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper195/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335669561, "tmdate": 1552335669561, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper195/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "HJlLzHlOn7", "original": null, "number": 2, "cdate": 1541043470117, "ddate": null, "tcdate": 1541043470117, "tmdate": 1541534204302, "tddate": null, "forum": "r1lFIiR9tQ", "replyto": "r1lFIiR9tQ", "invitation": "ICLR.cc/2019/Conference/-/Paper195/Official_Review", "content": {"title": "An interesting paper with weak experiments", "review": "This paper proposed a novel variational upper bound for f-divergence, one example of which is the famous evidence lower bound for max-loglikelihood learning. The second contribution might be the spread f-divergence for distributions having different supports. Even though theoretically sound, I believe that the presented experimental results are not strong enough to support the effectiveness of the proposed techniques. Detailed comments are listed below.\n\n1) Notations are confusing, especially in Section 3 when introducing the SPREAD f -DIVERGENCE.\n2) I cannot find on arXiv the reference \u201cD. Barber, M. Zhang, R. Habib, and T. Bird. Spread divergences. arXiv preprint, 2018.\u201d So I am not sure whether you can take credit from the \u201cspread f-divergence\u201d or not.\n3) Important analysis/experiments on several key points are missing, for example, (i) how to specify the variance of the spread divergence in practice? (ii) how to estimate log p(y)? What is the influence?\n4) In the paragraph before Sec 4.2, how the sigma of the spread divergence is annealed?\n5) Despite the toy experiment in Sec 4.4, what are the advantages of the proposed f-divergence upper bound over the Fenchel-conjugate f-divergence lower bound? The current experimental results barely show any advantage.\n\nMinors:\n1) Page 6, under Figure 3. The statements of \u201cKL (moment matching)\u201d and \u201creverse KL (mode seeking)\u201d are not consistent with what\u2019s stated in Sec 2.2 (the paragraph under Eq (3)). \n2) \u201cRKL\u201d and \u201cJS\u201d are not defined. Forward KL and standard KL are both used in the paper.\n\n\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper195/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training generative latent models  by variational f-divergence minimization", "abstract": "Probabilistic models are often trained by maximum likelihood, which corresponds to minimizing a specific form of f-divergence between the model and data distribution. We derive an upper bound that holds for all f-divergences, showing the intuitive result that the divergence between two joint distributions is at least as great as the divergence between their corresponding marginals. Additionally, the f-divergence is not formally defined when two distributions have different supports. We thus propose a noisy version of f-divergence which is well defined in such situations. We demonstrate how the bound and the new version of f-divergence can be readily used to train complex probabilistic generative models of data and that the fitted model can depend significantly on the particular divergence used.", "keywords": ["variational inference", "generative model", "f divergence"], "authorids": ["mingtian.zhang.17@ucl.ac.uk", "thomas.bird@cs.ucl.ac.uk", "raza.habib@cs.ucl.ac.uk", "t.xu12@lse.ac.uk", "david.barber@ucl.ac.uk"], "authors": ["Mingtian Zhang", "Thomas Bird", "Raza Habib", "Tianlin Xu", "David Barber"], "TL;DR": "Training generative models using an upper bound of the f divergence.", "pdf": "/pdf/9cc42d11aa2483056869d06180949c7feab06c0d.pdf", "paperhash": "zhang|training_generative_latent_models_by_variational_fdivergence_minimization", "_bibtex": "@misc{\nzhang2019training,\ntitle={Training generative latent models  by variational f-divergence minimization},\nauthor={Mingtian Zhang and Thomas Bird and Raza Habib and Tianlin Xu and David Barber},\nyear={2019},\nurl={https://openreview.net/forum?id=r1lFIiR9tQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper195/Official_Review", "cdate": 1542234517500, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "r1lFIiR9tQ", "replyto": "r1lFIiR9tQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper195/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335669561, "tmdate": 1552335669561, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper195/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "H1lMADZ727", "original": null, "number": 1, "cdate": 1540720586282, "ddate": null, "tcdate": 1540720586282, "tmdate": 1541534204088, "tddate": null, "forum": "r1lFIiR9tQ", "replyto": "r1lFIiR9tQ", "invitation": "ICLR.cc/2019/Conference/-/Paper195/Official_Review", "content": {"title": "The research direction itself is interesting, but further experimental validation is needed", "review": "\\clarity & quality\nThe paper is easy to follow and self-contained. \nHowever, the motivation for minimizing the upper bound is not so clear for me. \nAs far as I understood from the paper, changing the objective function to the upper bound of f-divergence have two merits compared to the existing methods. One is that by using the reverse KL, we can obtain sharper outputs, and the second one is that the optimization process will be stable compared to that of the lower bound. \nIn the introduction, the author just mentioned that \"the f-divergence is generally computationally intractable for such complex models. The main contribution of our paper is the introduction of an upper bound on the f-divergence.\"\nFor me, this seems that the author just introduced the new fancy objective. I think the motivation to introduce the new objective function should be stated clearly.\n\n\\originality & significance\nAlthough the upper bound of the f-divergence is the trivial extension, the idea to optimize the upper bound for the latent model seems new and interesting.\n\nHowever, it is hard to evaluate the usefulness of the proposed method from the current experiments.\nIt seems that there are two merits about the proposed method as above.\nThe only evidence that the learning tends to be stable is the Fig.8 in the appendix, but this is just the fitting of univariate Gaussian to a mixture of Gaussians, thus it is too weak as the evidence.\nAbout the sharp output, there are already many methods to overcome the blurred output of the usual VAE. No comparison is done in the paper.\nSo I cannot tell whether the proposed objective is really useful to learn the deep generative models.\nI think further experimental results are needed to validate the proposed method.\n\n\\Question\nIn page 4,  the variance of the p(y|x) and p_\\theta(y|z) are set to be the same. What is the intuition behind this trick? \nSince this p(y|x) is used as the estimator for the log p(y) as the smoothed delta function whose Gaussian window width (the variance), and the Gaussian window width is crucial to this kind of estimator, I know why the author used this trick.", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper195/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training generative latent models  by variational f-divergence minimization", "abstract": "Probabilistic models are often trained by maximum likelihood, which corresponds to minimizing a specific form of f-divergence between the model and data distribution. We derive an upper bound that holds for all f-divergences, showing the intuitive result that the divergence between two joint distributions is at least as great as the divergence between their corresponding marginals. Additionally, the f-divergence is not formally defined when two distributions have different supports. We thus propose a noisy version of f-divergence which is well defined in such situations. We demonstrate how the bound and the new version of f-divergence can be readily used to train complex probabilistic generative models of data and that the fitted model can depend significantly on the particular divergence used.", "keywords": ["variational inference", "generative model", "f divergence"], "authorids": ["mingtian.zhang.17@ucl.ac.uk", "thomas.bird@cs.ucl.ac.uk", "raza.habib@cs.ucl.ac.uk", "t.xu12@lse.ac.uk", "david.barber@ucl.ac.uk"], "authors": ["Mingtian Zhang", "Thomas Bird", "Raza Habib", "Tianlin Xu", "David Barber"], "TL;DR": "Training generative models using an upper bound of the f divergence.", "pdf": "/pdf/9cc42d11aa2483056869d06180949c7feab06c0d.pdf", "paperhash": "zhang|training_generative_latent_models_by_variational_fdivergence_minimization", "_bibtex": "@misc{\nzhang2019training,\ntitle={Training generative latent models  by variational f-divergence minimization},\nauthor={Mingtian Zhang and Thomas Bird and Raza Habib and Tianlin Xu and David Barber},\nyear={2019},\nurl={https://openreview.net/forum?id=r1lFIiR9tQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper195/Official_Review", "cdate": 1542234517500, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "r1lFIiR9tQ", "replyto": "r1lFIiR9tQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper195/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335669561, "tmdate": 1552335669561, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper195/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 8}