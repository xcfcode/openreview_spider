{"notes": [{"id": "SkeNlJSKvS", "original": "Hyl2v7j_wH", "number": 1502, "cdate": 1569439467977, "ddate": null, "tcdate": 1569439467977, "tmdate": 1577168261477, "tddate": null, "forum": "SkeNlJSKvS", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["xhw15@mails.tsinghua.edu.cn", "chen-wx17@mails.tsinghua.edu.cn", "laijl16@mails.tsinghua.edu.cn", "lizhihan17@mails.tsinghua.edu.cn", "zhaoyoujian@tsinghua.edu.cn", "peidan@tsinghua.edu.cn"], "title": "Shallow VAEs with RealNVP Prior Can Perform as Well as Deep Hierarchical VAEs", "authors": ["Haowen Xu", "Wenxiao Chen", "Jinlin Lai", "Zhihan Li", "Youjian Zhao", "Dan Pei"], "pdf": "/pdf/f1b32c2a0cca43a600da9d1111e1dfe97f3dd902.pdf", "TL;DR": "We show that VAE with learned RealNVP prior and just one latent variable can have better test NLLs than some deep hierarchical VAEs with powerful posteriors, on several datasets.", "abstract": "Using powerful posterior distributions is a popular technique in variational inference.  However, recent works showed that the aggregated posterior may fail to match unit Gaussian prior, even with expressive posteriors, thus learning the prior becomes an alternative way to improve the variational lower-bound.  We show that using learned RealNVP prior and just one latent variable in VAE, we can achieve test NLL comparable to very deep state-of-the-art hierarchical VAE, outperforming many previous works with complex hierarchical VAE architectures.  We hypothesize that, when coupled with Gaussian posteriors, the learned prior can encourage appropriate posterior overlapping, which is likely to improve reconstruction loss and lower-bound, supported by our experimental results.  We demonstrate that, with learned RealNVP prior, \u00df-VAE can have better rate-distortion curve than using fixed Gaussian prior.", "keywords": ["Variational Auto-encoder", "RealNVP", "learnable prior"], "paperhash": "xu|shallow_vaes_with_realnvp_prior_can_perform_as_well_as_deep_hierarchical_vaes", "original_pdf": "/attachment/bdd1b172709403a12aa3b7e658db5412ef996435.pdf", "_bibtex": "@misc{\nxu2020shallow,\ntitle={Shallow {\\{}VAE{\\}}s with Real{\\{}NVP{\\}} Prior Can Perform as Well as Deep Hierarchical {\\{}VAE{\\}}s},\nauthor={Haowen Xu and Wenxiao Chen and Jinlin Lai and Zhihan Li and Youjian Zhao and Dan Pei},\nyear={2020},\nurl={https://openreview.net/forum?id=SkeNlJSKvS}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "U3sPdqwc7", "original": null, "number": 1, "cdate": 1576798724955, "ddate": null, "tcdate": 1576798724955, "tmdate": 1576800911552, "tddate": null, "forum": "SkeNlJSKvS", "replyto": "SkeNlJSKvS", "invitation": "ICLR.cc/2020/Conference/Paper1502/-/Decision", "content": {"decision": "Reject", "comment": "This paper provides an interesting insight into the fitting of variational autoencoders.  While much of the recent literature focuses on training ever more expressive models, the authors demonstrate that learning a flexible prior can provide an equally strong model.  Unfortunately one review is somewhat terse.  Among the other reviews, one reviewer found the paper very interesting and compelling but did not feel comfortable raising their score to \"accept\" in the discussion phase citing a lack of compelling empirical results in compared to baselines.  Both reviewers were concerned about novelty in light of Huang et al., in which a RealNVP prior is also learned in a VAE.  AnonReviewer3 also felt that the experiments were not thorough enough to back up the claims in the paper.  Unfortunately, for these reasons the recommendation is to reject.  More compelling empirical results with carefully chosen baselines to back up the claims of the paper and comparison to existing literature (Huang et al) would make this paper much stronger.\n\n", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["xhw15@mails.tsinghua.edu.cn", "chen-wx17@mails.tsinghua.edu.cn", "laijl16@mails.tsinghua.edu.cn", "lizhihan17@mails.tsinghua.edu.cn", "zhaoyoujian@tsinghua.edu.cn", "peidan@tsinghua.edu.cn"], "title": "Shallow VAEs with RealNVP Prior Can Perform as Well as Deep Hierarchical VAEs", "authors": ["Haowen Xu", "Wenxiao Chen", "Jinlin Lai", "Zhihan Li", "Youjian Zhao", "Dan Pei"], "pdf": "/pdf/f1b32c2a0cca43a600da9d1111e1dfe97f3dd902.pdf", "TL;DR": "We show that VAE with learned RealNVP prior and just one latent variable can have better test NLLs than some deep hierarchical VAEs with powerful posteriors, on several datasets.", "abstract": "Using powerful posterior distributions is a popular technique in variational inference.  However, recent works showed that the aggregated posterior may fail to match unit Gaussian prior, even with expressive posteriors, thus learning the prior becomes an alternative way to improve the variational lower-bound.  We show that using learned RealNVP prior and just one latent variable in VAE, we can achieve test NLL comparable to very deep state-of-the-art hierarchical VAE, outperforming many previous works with complex hierarchical VAE architectures.  We hypothesize that, when coupled with Gaussian posteriors, the learned prior can encourage appropriate posterior overlapping, which is likely to improve reconstruction loss and lower-bound, supported by our experimental results.  We demonstrate that, with learned RealNVP prior, \u00df-VAE can have better rate-distortion curve than using fixed Gaussian prior.", "keywords": ["Variational Auto-encoder", "RealNVP", "learnable prior"], "paperhash": "xu|shallow_vaes_with_realnvp_prior_can_perform_as_well_as_deep_hierarchical_vaes", "original_pdf": "/attachment/bdd1b172709403a12aa3b7e658db5412ef996435.pdf", "_bibtex": "@misc{\nxu2020shallow,\ntitle={Shallow {\\{}VAE{\\}}s with Real{\\{}NVP{\\}} Prior Can Perform as Well as Deep Hierarchical {\\{}VAE{\\}}s},\nauthor={Haowen Xu and Wenxiao Chen and Jinlin Lai and Zhihan Li and Youjian Zhao and Dan Pei},\nyear={2020},\nurl={https://openreview.net/forum?id=SkeNlJSKvS}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "SkeNlJSKvS", "replyto": "SkeNlJSKvS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795709685, "tmdate": 1576800258500, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1502/-/Decision"}}}, {"id": "ryeDf7PxFB", "original": null, "number": 1, "cdate": 1570956046688, "ddate": null, "tcdate": 1570956046688, "tmdate": 1572972460224, "tddate": null, "forum": "SkeNlJSKvS", "replyto": "SkeNlJSKvS", "invitation": "ICLR.cc/2020/Conference/Paper1502/-/Official_Review", "content": {"rating": "6: Weak Accept", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This submission shows that using learned autoregressive priors (real NVP) allows shallow VAEs to achieve comparable log-likelihood performances compared to more complex deep VAE architectures.\n\nI found this paper an enjoyable read, and its results quite intriguing. While most of VAE research focuses on building more powerful encoder and decoder architectures, these results show that focusing on a learned prior distribution is as important. \n\nThe models introduced in this paper are not novel, but the authors introduce some tricks (gradient and std clipping) that allow them to achieve nearly SOTA results with relatively simple architectures. \nI think these tricks should have been demonstrated more in detail in the experiments, for example:\n* why is the std clipped exactly at e^-11? What happens if I increase/decrease this number?\n* Can the authors clarify the differences between their model and that of Huang et al, which also uses real NVP priors? If I took the exact architecture of Huang et al and used the clipping trick would it perform similarly to your model?\n* Would other more complex SOTA models also benefit from your clipping tricks?\n\nA very interesting addition to the paper would be running some experiments on more complex data distributions such as the natural images of celebA or CIFAR10, to understand whether your model could achieve:\n(1) similar improvements in terms of  ELBO \n(2) more importantly, a quality of the generated samples comparable to deep VAE models such as VAE+IAF or BIVA.\n\nOverall I liked the paper so I am voting towards acceptance. However, while there are a considerable number of experiments in this paper, for me to increase the score I would like to see at least some of the experiments suggested above, since they could help better understand the behavior of VAEs with learned priors and make this an even more impactful paper.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1502/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1502/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["xhw15@mails.tsinghua.edu.cn", "chen-wx17@mails.tsinghua.edu.cn", "laijl16@mails.tsinghua.edu.cn", "lizhihan17@mails.tsinghua.edu.cn", "zhaoyoujian@tsinghua.edu.cn", "peidan@tsinghua.edu.cn"], "title": "Shallow VAEs with RealNVP Prior Can Perform as Well as Deep Hierarchical VAEs", "authors": ["Haowen Xu", "Wenxiao Chen", "Jinlin Lai", "Zhihan Li", "Youjian Zhao", "Dan Pei"], "pdf": "/pdf/f1b32c2a0cca43a600da9d1111e1dfe97f3dd902.pdf", "TL;DR": "We show that VAE with learned RealNVP prior and just one latent variable can have better test NLLs than some deep hierarchical VAEs with powerful posteriors, on several datasets.", "abstract": "Using powerful posterior distributions is a popular technique in variational inference.  However, recent works showed that the aggregated posterior may fail to match unit Gaussian prior, even with expressive posteriors, thus learning the prior becomes an alternative way to improve the variational lower-bound.  We show that using learned RealNVP prior and just one latent variable in VAE, we can achieve test NLL comparable to very deep state-of-the-art hierarchical VAE, outperforming many previous works with complex hierarchical VAE architectures.  We hypothesize that, when coupled with Gaussian posteriors, the learned prior can encourage appropriate posterior overlapping, which is likely to improve reconstruction loss and lower-bound, supported by our experimental results.  We demonstrate that, with learned RealNVP prior, \u00df-VAE can have better rate-distortion curve than using fixed Gaussian prior.", "keywords": ["Variational Auto-encoder", "RealNVP", "learnable prior"], "paperhash": "xu|shallow_vaes_with_realnvp_prior_can_perform_as_well_as_deep_hierarchical_vaes", "original_pdf": "/attachment/bdd1b172709403a12aa3b7e658db5412ef996435.pdf", "_bibtex": "@misc{\nxu2020shallow,\ntitle={Shallow {\\{}VAE{\\}}s with Real{\\{}NVP{\\}} Prior Can Perform as Well as Deep Hierarchical {\\{}VAE{\\}}s},\nauthor={Haowen Xu and Wenxiao Chen and Jinlin Lai and Zhihan Li and Youjian Zhao and Dan Pei},\nyear={2020},\nurl={https://openreview.net/forum?id=SkeNlJSKvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SkeNlJSKvS", "replyto": "SkeNlJSKvS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1502/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1502/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575601427841, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1502/Reviewers"], "noninvitees": [], "tcdate": 1570237736449, "tmdate": 1575601427858, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1502/-/Official_Review"}}}, {"id": "rkxnJ9o0YB", "original": null, "number": 2, "cdate": 1571891683667, "ddate": null, "tcdate": 1571891683667, "tmdate": 1572972460190, "tddate": null, "forum": "SkeNlJSKvS", "replyto": "SkeNlJSKvS", "invitation": "ICLR.cc/2020/Conference/Paper1502/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper claims that learning prior from the data could achieve superior performance than using a standard unit Gaussian prior. Experimental results further show that the proposed method could achieve a lower or comparable negative log-likelihood compared to other VAE variants using a complex hierarchical architecture.\n\nI have the following concerns about the paper:\n\nIt is widely accepted that the prior serves as the regularization for the Bayesian inference. Using a data-dependent prior is promising to derive a lower NNL than a data-independent prior. However, it would easily lead to an overfitting model with bad generalization, especially for a noisy dataset. \n\nIt is claimed in the abstract and conclusion that one latent variable is used for learning the RealNVP prior while the authors use the words \"shallow\" (refers to few latent variables) in the experiments. So how many latent variables are used in the experiment?\n\nIt is listed in the related work that \"Huang et al. (2017) applied RealNVP (Dinh et al., 2017) to learn the prior\", which means the idea using the learned RealNVP prior is not a new idea. Therefore, what is the contribution of this paper? A detailed discussion is needed to elaborate on the differences from previous methods using a prior learned from the data.\n\nThe equation of the aggregated posterior in page 2 after Eq.4 is wrong. The aggregated posterior should be the integration over x instead of z.\n\nThe drawn conclusion \"using both RealNVP posterior and prior shows no significant advantage over using RealNVP prior only, although the total flow depth of the former variant is twice as large as the latter one\" is quite unprofessional. Only one experiment set was conducted with k=20.  One obvious reason is that the current setting with k=20 makes that the model over-parameterized. More comparisons are needed for smaller k. \n\nAs claimed in the paper that the clipping would be a navie method to promote overlapping among the posterior. A comparison with a navie baseline using clipping is needed before drawing a conclusion that the learned RealNVP prior is the reason for enhancing the overlapping.\n\nIs the likelihood function p(x|z) a Bernoulli or Gaussian?\n\n\"Although BIVA has a much lower NLL on StaticMNIST, in contrast to our paper, the BIVA paper (Maal\u00f8e et al., 2019) ...... attributed to having fewer training data\". The author should confirm with the authors instead of giving a conjecture in a scientific paper. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1502/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1502/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["xhw15@mails.tsinghua.edu.cn", "chen-wx17@mails.tsinghua.edu.cn", "laijl16@mails.tsinghua.edu.cn", "lizhihan17@mails.tsinghua.edu.cn", "zhaoyoujian@tsinghua.edu.cn", "peidan@tsinghua.edu.cn"], "title": "Shallow VAEs with RealNVP Prior Can Perform as Well as Deep Hierarchical VAEs", "authors": ["Haowen Xu", "Wenxiao Chen", "Jinlin Lai", "Zhihan Li", "Youjian Zhao", "Dan Pei"], "pdf": "/pdf/f1b32c2a0cca43a600da9d1111e1dfe97f3dd902.pdf", "TL;DR": "We show that VAE with learned RealNVP prior and just one latent variable can have better test NLLs than some deep hierarchical VAEs with powerful posteriors, on several datasets.", "abstract": "Using powerful posterior distributions is a popular technique in variational inference.  However, recent works showed that the aggregated posterior may fail to match unit Gaussian prior, even with expressive posteriors, thus learning the prior becomes an alternative way to improve the variational lower-bound.  We show that using learned RealNVP prior and just one latent variable in VAE, we can achieve test NLL comparable to very deep state-of-the-art hierarchical VAE, outperforming many previous works with complex hierarchical VAE architectures.  We hypothesize that, when coupled with Gaussian posteriors, the learned prior can encourage appropriate posterior overlapping, which is likely to improve reconstruction loss and lower-bound, supported by our experimental results.  We demonstrate that, with learned RealNVP prior, \u00df-VAE can have better rate-distortion curve than using fixed Gaussian prior.", "keywords": ["Variational Auto-encoder", "RealNVP", "learnable prior"], "paperhash": "xu|shallow_vaes_with_realnvp_prior_can_perform_as_well_as_deep_hierarchical_vaes", "original_pdf": "/attachment/bdd1b172709403a12aa3b7e658db5412ef996435.pdf", "_bibtex": "@misc{\nxu2020shallow,\ntitle={Shallow {\\{}VAE{\\}}s with Real{\\{}NVP{\\}} Prior Can Perform as Well as Deep Hierarchical {\\{}VAE{\\}}s},\nauthor={Haowen Xu and Wenxiao Chen and Jinlin Lai and Zhihan Li and Youjian Zhao and Dan Pei},\nyear={2020},\nurl={https://openreview.net/forum?id=SkeNlJSKvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SkeNlJSKvS", "replyto": "SkeNlJSKvS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1502/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1502/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575601427841, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1502/Reviewers"], "noninvitees": [], "tcdate": 1570237736449, "tmdate": 1575601427858, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1502/-/Official_Review"}}}, {"id": "ryxeWqalqB", "original": null, "number": 3, "cdate": 1572030968155, "ddate": null, "tcdate": 1572030968155, "tmdate": 1572972460138, "tddate": null, "forum": "SkeNlJSKvS", "replyto": "SkeNlJSKvS", "invitation": "ICLR.cc/2020/Conference/Paper1502/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The authors propose to use learned RealNVP with a shallow VAE instead of deep hierarchical VAE, which will not hurt the performance. The authors conduct thorough experiments to backup their claims and hypotheses.\n\nCons:\n1. The writing needs to be improved. The four contributions listed do not have a clear logic flow.\n2. The proposed method seems like a combination of previous studies, making the paper more like a technical report.\n3. One advantage claimed by the authors is that only one latent variable has clear semantic meanings, which is not explicitly supported by the experiments."}, "signatures": ["ICLR.cc/2020/Conference/Paper1502/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1502/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["xhw15@mails.tsinghua.edu.cn", "chen-wx17@mails.tsinghua.edu.cn", "laijl16@mails.tsinghua.edu.cn", "lizhihan17@mails.tsinghua.edu.cn", "zhaoyoujian@tsinghua.edu.cn", "peidan@tsinghua.edu.cn"], "title": "Shallow VAEs with RealNVP Prior Can Perform as Well as Deep Hierarchical VAEs", "authors": ["Haowen Xu", "Wenxiao Chen", "Jinlin Lai", "Zhihan Li", "Youjian Zhao", "Dan Pei"], "pdf": "/pdf/f1b32c2a0cca43a600da9d1111e1dfe97f3dd902.pdf", "TL;DR": "We show that VAE with learned RealNVP prior and just one latent variable can have better test NLLs than some deep hierarchical VAEs with powerful posteriors, on several datasets.", "abstract": "Using powerful posterior distributions is a popular technique in variational inference.  However, recent works showed that the aggregated posterior may fail to match unit Gaussian prior, even with expressive posteriors, thus learning the prior becomes an alternative way to improve the variational lower-bound.  We show that using learned RealNVP prior and just one latent variable in VAE, we can achieve test NLL comparable to very deep state-of-the-art hierarchical VAE, outperforming many previous works with complex hierarchical VAE architectures.  We hypothesize that, when coupled with Gaussian posteriors, the learned prior can encourage appropriate posterior overlapping, which is likely to improve reconstruction loss and lower-bound, supported by our experimental results.  We demonstrate that, with learned RealNVP prior, \u00df-VAE can have better rate-distortion curve than using fixed Gaussian prior.", "keywords": ["Variational Auto-encoder", "RealNVP", "learnable prior"], "paperhash": "xu|shallow_vaes_with_realnvp_prior_can_perform_as_well_as_deep_hierarchical_vaes", "original_pdf": "/attachment/bdd1b172709403a12aa3b7e658db5412ef996435.pdf", "_bibtex": "@misc{\nxu2020shallow,\ntitle={Shallow {\\{}VAE{\\}}s with Real{\\{}NVP{\\}} Prior Can Perform as Well as Deep Hierarchical {\\{}VAE{\\}}s},\nauthor={Haowen Xu and Wenxiao Chen and Jinlin Lai and Zhihan Li and Youjian Zhao and Dan Pei},\nyear={2020},\nurl={https://openreview.net/forum?id=SkeNlJSKvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SkeNlJSKvS", "replyto": "SkeNlJSKvS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1502/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1502/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575601427841, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1502/Reviewers"], "noninvitees": [], "tcdate": 1570237736449, "tmdate": 1575601427858, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1502/-/Official_Review"}}}], "count": 5}