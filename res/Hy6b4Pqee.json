{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1488968022980, "tcdate": 1478288388656, "number": 377, "id": "Hy6b4Pqee", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "Hy6b4Pqee", "signatures": ["~Dustin_Tran1"], "readers": ["everyone"], "content": {"TL;DR": "", "title": "Deep Probabilistic Programming", "abstract": "We propose Edward, a Turing-complete probabilistic programming language. Edward defines two compositional representations\u2014random variables and inference. By treating inference as a first class citizen, on a par with modeling, we show that probabilistic programming can be as flexible and computationally efficient as traditional deep learning. For flexibility, Edward makes it easy to fit the same model using a variety of composable inference methods, ranging from point estimation to variational inference to MCMC. In addition, Edward can reuse the modeling representation as part of inference, facilitating the design of rich variational models and generative adversarial networks. For efficiency, Edward is integrated into TensorFlow, providing significant speedups over existing probabilistic systems. For example, we show on a benchmark logistic regression task that Edward is at least 35x faster than Stan and 6x faster than PyMC3. Further, Edward incurs no runtime overhead: it is as fast as handwritten TensorFlow.", "pdf": "/pdf/b51fb1ef5fa81889f226cfb62f5b6870b0ddb4fc.pdf", "paperhash": "tran|deep_probabilistic_programming", "keywords": [], "conflicts": ["adobe.com", "columbia.edu", "google.com"], "authors": ["Dustin Tran", "Matthew D. Hoffman", "Rif A. Saurous", "Eugene Brevdo", "Kevin Murphy", "David M. Blei"], "authorids": ["dustin@cs.columbia.edu", "mathoffm@adobe.com", "rif@google.com", "ebrevdo@google.com", "kpmurphy@google.com", "david.blei@columbia.edu"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 12, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396550572, "tcdate": 1486396550572, "number": 1, "id": "S1yq3fLul", "invitation": "ICLR.cc/2017/conference/-/paper377/acceptance", "forum": "Hy6b4Pqee", "replyto": "Hy6b4Pqee", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"title": "ICLR committee final decision", "comment": "There was general agreement from the reviewers that this looks like an important development in the area of probabilistic programming. Some reviewers felt the impact of the work could be very significant. The quality of the work and the paper were perceived as being quite high. The main weakness highlighted by the most negative reviewer (who felt the work was marginally below threshold) is the level of empirical evaluation given within the submitted manuscript. The authors did submit a revision and they outline the reviewer\u00d5s points that they have addressed. It appears that if accepted this manuscript would constitute the first peer-reviewed paper on the subject of this new software package (Edward). Based on both the numeric scores, the quality and potential significance of this work I recommend acceptance.", "decision": "Accept (Poster)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Deep Probabilistic Programming", "abstract": "We propose Edward, a Turing-complete probabilistic programming language. Edward defines two compositional representations\u2014random variables and inference. By treating inference as a first class citizen, on a par with modeling, we show that probabilistic programming can be as flexible and computationally efficient as traditional deep learning. For flexibility, Edward makes it easy to fit the same model using a variety of composable inference methods, ranging from point estimation to variational inference to MCMC. In addition, Edward can reuse the modeling representation as part of inference, facilitating the design of rich variational models and generative adversarial networks. For efficiency, Edward is integrated into TensorFlow, providing significant speedups over existing probabilistic systems. For example, we show on a benchmark logistic regression task that Edward is at least 35x faster than Stan and 6x faster than PyMC3. Further, Edward incurs no runtime overhead: it is as fast as handwritten TensorFlow.", "pdf": "/pdf/b51fb1ef5fa81889f226cfb62f5b6870b0ddb4fc.pdf", "paperhash": "tran|deep_probabilistic_programming", "keywords": [], "conflicts": ["adobe.com", "columbia.edu", "google.com"], "authors": ["Dustin Tran", "Matthew D. Hoffman", "Rif A. Saurous", "Eugene Brevdo", "Kevin Murphy", "David M. Blei"], "authorids": ["dustin@cs.columbia.edu", "mathoffm@adobe.com", "rif@google.com", "ebrevdo@google.com", "kpmurphy@google.com", "david.blei@columbia.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396551098, "id": "ICLR.cc/2017/conference/-/paper377/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "Hy6b4Pqee", "replyto": "Hy6b4Pqee", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396551098}}}, {"tddate": null, "tmdate": 1484379671671, "tcdate": 1484327265248, "number": 6, "id": "BkKvtFUUe", "invitation": "ICLR.cc/2017/conference/-/paper377/public/comment", "forum": "Hy6b4Pqee", "replyto": "SJUtsE4Ex", "signatures": ["~Dustin_Tran1"], "readers": ["everyone"], "writers": ["~Dustin_Tran1"], "content": {"title": "re:Exciting and promising approach which still needs to be demonstrated empirically", "comment": "Thanks for the feedback!\n\n> the most important factor of any PPL is whether it is practical for real-world use cases.\n\nWe agree that real-world application is a crucial yet underemphasized criteria for PPLs. We also want to highlight that a major contribution of the paper is not only the applications for developing in Edward, but also the methodological advances in enabling flexible experimentation. For example, it was an open challenge in PPLs to enable the design of rich variational models and generative adversarial networks. We believe we have significant advances in PPL design.\n\n> There are many example code snippets given in the paper, but most are not evaluated. The Dirichlet process mixture model example (Figure 12) is an important one: do the proposed black-box inference tools really work for this snippet? and will the GAN example (Figure 7) converge when optimised with real data?\n\nAs with other PPL papers (e.g., Church (https://arxiv.org/abs/1206.3255)), the purpose of those snippets are to demonstrate the representational capacity and interface of the language, following our methodological advances. However, we can also provide links to reproducible scripts demonstrating their success in a camera-ready. The short answer is that black box inference works for the DP mixture model\u2014we use black box inference with a truncated variational model but non-truncated probability model; see also Kurihara et al. (2007) (https://www.ics.uci.edu/~welling/publications/papers/cvdp-v6.pdf). The GAN example does not necessarily reach a stationary point (this is open GAN research), but halting it after a fixed number of iterations produces sensible results.\n\n> Paper presentation could be improved. [...]\n\nThanks for the suggestions. In the newly uploaded submission, we have added more forward signalling and clarified other miscellaneous details.\n\n> Why is the run time not reported in table 1?\n\nGreat point. Although not significant for the experiment's purpose, we can add these to the camera-ready.\n\n> What are the \"difficulties around convergence\" encountered with the analytical entropies? ... Are there tools to diagnose these with the provided toolbox?\n\nThis is an open research challenge in gradient estimators for variational inference; see, e.g., a NIPS 2016 workshop paper (http://approximateinference.org/accepted/RoederEtAl2016.pdf). Edward has no existing solution other than recommending not to use analytic entropies. Importantly, the goal of the experiment is to show that Edward is a platform for easily performing such experimentation. Later research may produce a solution that is then implemented in Edward.\n\n> Did HMC give sensible results in the experiment at the bottom of page 8?\n\nYes; we will also include a link to a reproducible script in the camera-ready.\n\n> How difficult is it to get the inference to work (eg HMC) when we don't have full control over the computational graph structure and sampler?\n\nThe difficulty depends on the problem setting. As with other HMC software, we have control over usual hyperparameters such as integrator step size and path length. The only control the user does not have is the specific graph that the HMC uses to produce samples; modifying it would result in another algorithm (which the user can fully control by building off these built-in classes).\n\n> What benchmarks do you intend to use in the Model Zoo? [...]\n\nAs with other open research challenges, we do not subscribe to a specific solution. As long as a proposed entry to the Model Zoo demonstrates use in being shared to the community\u2014whether it be outperforming a specific benchmark or just a useful application\u2014we plan to accept it. We think there's utility in building the platform as a first step. A next step for the community at large is to solve the important challenges you bring up, now that they are made more transparent when applying probabilistic models in the real world.\n\n> Minor comments\n\nThanks! In the newly uploaded submission, we have made these changes. We can also add a result comparing to Li & Turner with alpha=0.5 (alpha=-1 was chosen to produce the chi-divergence) in the camera-ready."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Deep Probabilistic Programming", "abstract": "We propose Edward, a Turing-complete probabilistic programming language. Edward defines two compositional representations\u2014random variables and inference. By treating inference as a first class citizen, on a par with modeling, we show that probabilistic programming can be as flexible and computationally efficient as traditional deep learning. For flexibility, Edward makes it easy to fit the same model using a variety of composable inference methods, ranging from point estimation to variational inference to MCMC. In addition, Edward can reuse the modeling representation as part of inference, facilitating the design of rich variational models and generative adversarial networks. For efficiency, Edward is integrated into TensorFlow, providing significant speedups over existing probabilistic systems. For example, we show on a benchmark logistic regression task that Edward is at least 35x faster than Stan and 6x faster than PyMC3. Further, Edward incurs no runtime overhead: it is as fast as handwritten TensorFlow.", "pdf": "/pdf/b51fb1ef5fa81889f226cfb62f5b6870b0ddb4fc.pdf", "paperhash": "tran|deep_probabilistic_programming", "keywords": [], "conflicts": ["adobe.com", "columbia.edu", "google.com"], "authors": ["Dustin Tran", "Matthew D. Hoffman", "Rif A. Saurous", "Eugene Brevdo", "Kevin Murphy", "David M. Blei"], "authorids": ["dustin@cs.columbia.edu", "mathoffm@adobe.com", "rif@google.com", "ebrevdo@google.com", "kpmurphy@google.com", "david.blei@columbia.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287600525, "id": "ICLR.cc/2017/conference/-/paper377/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Hy6b4Pqee", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper377/reviewers", "ICLR.cc/2017/conference/paper377/areachairs"], "cdate": 1485287600525}}}, {"tddate": null, "tmdate": 1484327644876, "tcdate": 1484326505183, "number": 3, "id": "S1-OIKIUg", "invitation": "ICLR.cc/2017/conference/-/paper377/public/comment", "forum": "Hy6b4Pqee", "replyto": "Hy6b4Pqee", "signatures": ["~Dustin_Tran1"], "readers": ["everyone"], "writers": ["~Dustin_Tran1"], "content": {"title": "Thank you to the reviewers!", "comment": "Thank you to all the reviewers for their insightful feedback. As the reviewers agree, we think Edward \"has the potential to transform the way we work in the probabilistic modelling community, allowing us to perform rapid-prototyping to iterate through ideas quickly\" (Reviewer 4). We make general comments here; specific comments are made on the reviewer's thread. We have also uploaded a revised submission following the feedback.\n\nA major contribution in the paper is in real-world application: \"the Edward library provides an extremely impressive collection of modern probabilistic inference methods in an easily usable form\" (Reviewer 1). This enables researchers to build on top of current methods and also easily compare to them as baselines.\n\nA second contribution in the paper is in methodology. We proposed new compositional representations, \"putt[ing] focus on compositional inference\" (Reviewer 3). This \"allow[s] the users to select the inference method to suit their needs and constraints\" (Reviewer 3), such as for designing rich variational models and generative adversarial networks. We also show how to integrate such a language into computational graph frameworks; this provides significant speedups with distributing training, parallelism, vectorisation, and GPU support.\n\nFinally, we note that Edward is growing with active contributions from the community (https://github.com/blei-lab/edward). It continues to expand its feature set and improve its robustness and performance. Community-oriented platforms such as the Probability Zoo (http://edwardlib.org/zoo) and contributed models (https://github.com/blei-lab/edward/issues/380) will continue to make progress more open in the field."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Deep Probabilistic Programming", "abstract": "We propose Edward, a Turing-complete probabilistic programming language. Edward defines two compositional representations\u2014random variables and inference. By treating inference as a first class citizen, on a par with modeling, we show that probabilistic programming can be as flexible and computationally efficient as traditional deep learning. For flexibility, Edward makes it easy to fit the same model using a variety of composable inference methods, ranging from point estimation to variational inference to MCMC. In addition, Edward can reuse the modeling representation as part of inference, facilitating the design of rich variational models and generative adversarial networks. For efficiency, Edward is integrated into TensorFlow, providing significant speedups over existing probabilistic systems. For example, we show on a benchmark logistic regression task that Edward is at least 35x faster than Stan and 6x faster than PyMC3. Further, Edward incurs no runtime overhead: it is as fast as handwritten TensorFlow.", "pdf": "/pdf/b51fb1ef5fa81889f226cfb62f5b6870b0ddb4fc.pdf", "paperhash": "tran|deep_probabilistic_programming", "keywords": [], "conflicts": ["adobe.com", "columbia.edu", "google.com"], "authors": ["Dustin Tran", "Matthew D. Hoffman", "Rif A. Saurous", "Eugene Brevdo", "Kevin Murphy", "David M. Blei"], "authorids": ["dustin@cs.columbia.edu", "mathoffm@adobe.com", "rif@google.com", "ebrevdo@google.com", "kpmurphy@google.com", "david.blei@columbia.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287600525, "id": "ICLR.cc/2017/conference/-/paper377/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Hy6b4Pqee", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper377/reviewers", "ICLR.cc/2017/conference/paper377/areachairs"], "cdate": 1485287600525}}}, {"tddate": null, "tmdate": 1484327110215, "tcdate": 1484327110215, "number": 5, "id": "By0TutLUl", "invitation": "ICLR.cc/2017/conference/-/paper377/public/comment", "forum": "Hy6b4Pqee", "replyto": "SklYddxNl", "signatures": ["~Dustin_Tran1"], "readers": ["everyone"], "writers": ["~Dustin_Tran1"], "content": {"title": "re:Very promising probabilistic programming language combining Bayes and deep learning", "comment": "Thanks for the feedback!\n\n> The first experiment (variational inference) would be more valuable if there was a clear link to complete code to reproduce the results provided.\n\nThis makes sense. We will have a separate script for each result, with a link to all of them in the camera-ready's appendix.\n\n> The HMC experiment looks OK, except the characterising Stan as a hand-optimised implementation seems unfair as the code is clearly not hand-optimised for this specific model and hardware configuration.\n\nThis is a valid point. In the newly uploaded submission, we have removed any potentially sensationalist claims, and have made more nuanced claims in the abstract, introduction, and experiments. In general we think there's a strong case for building on computational graphs, as indicated by the benchmark. We will also add an additional comparison using Edward on 1 CPU.\n\n> It would be very helpful if the authors could provide and clearly link to a machine-readable companion ... with complete runnable code for all the examples.\n\nExcellent idea. We will have a dedicated edwardlib.org webpage and Jupyter notebook for the paper available soon.\n\n> The main paper refers to Tran et al. (2016a) which covers a lot of similar material, although from a different perspective. It is unclear if the other paper has been published or submitted somewhere and if so, where.\n\nTran et al. (2016a) is an unpublished preprint that is not submitted anywhere. If or when it is submitted, we will be sure to remove any similar material.\n\n> In Sec. 2 you draw a clear distinction between specialised languages (including Stan) and Turing-complete languages such as Edward. This seems unfair as I believe Stan is also Turing complete.\n\nTuring complete-ness is a contentious topic with varying definitions; we will be more precise in the camera-ready version."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Deep Probabilistic Programming", "abstract": "We propose Edward, a Turing-complete probabilistic programming language. Edward defines two compositional representations\u2014random variables and inference. By treating inference as a first class citizen, on a par with modeling, we show that probabilistic programming can be as flexible and computationally efficient as traditional deep learning. For flexibility, Edward makes it easy to fit the same model using a variety of composable inference methods, ranging from point estimation to variational inference to MCMC. In addition, Edward can reuse the modeling representation as part of inference, facilitating the design of rich variational models and generative adversarial networks. For efficiency, Edward is integrated into TensorFlow, providing significant speedups over existing probabilistic systems. For example, we show on a benchmark logistic regression task that Edward is at least 35x faster than Stan and 6x faster than PyMC3. Further, Edward incurs no runtime overhead: it is as fast as handwritten TensorFlow.", "pdf": "/pdf/b51fb1ef5fa81889f226cfb62f5b6870b0ddb4fc.pdf", "paperhash": "tran|deep_probabilistic_programming", "keywords": [], "conflicts": ["adobe.com", "columbia.edu", "google.com"], "authors": ["Dustin Tran", "Matthew D. Hoffman", "Rif A. Saurous", "Eugene Brevdo", "Kevin Murphy", "David M. Blei"], "authorids": ["dustin@cs.columbia.edu", "mathoffm@adobe.com", "rif@google.com", "ebrevdo@google.com", "kpmurphy@google.com", "david.blei@columbia.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287600525, "id": "ICLR.cc/2017/conference/-/paper377/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Hy6b4Pqee", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper377/reviewers", "ICLR.cc/2017/conference/paper377/areachairs"], "cdate": 1485287600525}}}, {"tddate": null, "tmdate": 1484326882015, "tcdate": 1484326882015, "number": 4, "id": "ry9J_YU8e", "invitation": "ICLR.cc/2017/conference/-/paper377/public/comment", "forum": "Hy6b4Pqee", "replyto": "r12m-yJVx", "signatures": ["~Dustin_Tran1"], "readers": ["everyone"], "writers": ["~Dustin_Tran1"], "content": {"title": "re:A significant development to include the flexibility of inference to PPL", "comment": "Thanks for the feedback!\n\n> It seems hyper-parameter learning is also specified as inference, which makes sense if using MAP. However the authors also demonstrated other objective functions such as Renyi divergences,\n\nBy default, inference hyperparameters such as alpha in Renyi divergences are set by the user and not learned. Of course, you can build an algorithm that adaptively selects inference hyperparameters, such as the NUTS extension to HMC which adapts the leapfrog path length. This would be a different algorithm (inference class).\n\n> does that mean the user need to define a new class of inference method whenever they want to test an alternative loss function?\n\nFor algorithms that optimize a family of objective functions, there is only one inference class. For example, in Renyi divergences, alpha is a setting that the user selects in the class. This is different from one inference class existing for each fixed setting of alpha (this is also possible to implement, although impractical)."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Deep Probabilistic Programming", "abstract": "We propose Edward, a Turing-complete probabilistic programming language. Edward defines two compositional representations\u2014random variables and inference. By treating inference as a first class citizen, on a par with modeling, we show that probabilistic programming can be as flexible and computationally efficient as traditional deep learning. For flexibility, Edward makes it easy to fit the same model using a variety of composable inference methods, ranging from point estimation to variational inference to MCMC. In addition, Edward can reuse the modeling representation as part of inference, facilitating the design of rich variational models and generative adversarial networks. For efficiency, Edward is integrated into TensorFlow, providing significant speedups over existing probabilistic systems. For example, we show on a benchmark logistic regression task that Edward is at least 35x faster than Stan and 6x faster than PyMC3. Further, Edward incurs no runtime overhead: it is as fast as handwritten TensorFlow.", "pdf": "/pdf/b51fb1ef5fa81889f226cfb62f5b6870b0ddb4fc.pdf", "paperhash": "tran|deep_probabilistic_programming", "keywords": [], "conflicts": ["adobe.com", "columbia.edu", "google.com"], "authors": ["Dustin Tran", "Matthew D. Hoffman", "Rif A. Saurous", "Eugene Brevdo", "Kevin Murphy", "David M. Blei"], "authorids": ["dustin@cs.columbia.edu", "mathoffm@adobe.com", "rif@google.com", "ebrevdo@google.com", "kpmurphy@google.com", "david.blei@columbia.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287600525, "id": "ICLR.cc/2017/conference/-/paper377/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Hy6b4Pqee", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper377/reviewers", "ICLR.cc/2017/conference/paper377/areachairs"], "cdate": 1485287600525}}}, {"tddate": null, "tmdate": 1482079186788, "tcdate": 1482079101724, "number": 3, "id": "SJUtsE4Ex", "invitation": "ICLR.cc/2017/conference/-/paper377/official/review", "forum": "Hy6b4Pqee", "replyto": "Hy6b4Pqee", "signatures": ["ICLR.cc/2017/conference/paper377/AnonReviewer4"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper377/AnonReviewer4"], "content": {"title": "Exciting and promising approach which still needs to be demonstrated empirically", "rating": "5: Marginally below acceptance threshold", "review": "The authors propose a new software package for probabilistic programming, taking advantage of recent successful tools used in the deep learning community. The software looks very promising and has the potential to transform the way we work in the probabilistic modelling community, allowing us to perform rapid-prototyping to iterate through ideas quickly. The composability principles are used insightfully, and the extension of inference to HMC for example, going beyond VI inference (which is simple to implement using existing deep learning tools), makes the software even more compelling. \n\nHowever, the most important factor of any PPL is whether it is practical for real-world use cases. This was not demonstrated sufficiently in the submission. There are many example code snippets given in the paper, but most are not evaluated. The Dirichlet process mixture model example (Figure 12) is an important one: do the proposed black-box inference tools really work for this snippet? and will the GAN example (Figure 7) converge when optimised with real data? To convince the community of the practicality of the package it will be necessary to demonstrate these empirically. Currently the only evaluated model is a VAE with various inference techniques, which are not difficult to implement using pure TF.\n\nPresentation:\n* Paper presentation could be improved. For example the authors could use more signalling for what is about to be explained. On page 5 qbeta and qz are used without explanation - the authors could mention that an example will be given thereafter.\n* I would also suggest to the authors to explain in the preface how the layers are implemented, and how the KL is handled in VI for example.\nIt will be useful to discuss what values are optimised and what values change as inference is performed (even before section 4.4). This was not clear for the majority of the paper. \n\nExperiments:\n* Why is the run time not reported in table 1?\n* What are the \"difficulties around convergence\" encountered with the analytical entropies? inference issues become more difficult to diagnose as inference is automated. Are there tools to diagnose these with the provided toolbox? \n* Did HMC give sensible results in the experiment at the bottom of page 8? only run time is reported. \n* How difficult is it to get the inference to work (eg HMC) when we don't have full control over the computational graph structure and sampler?\n* It would be extremely insightful to give a table comparing the performance (run time, predictive log likelihood, etc) of the various inference tools on more models.\n* What benchmarks do you intend to use in the Model Zoo? the difficulty with probabilistic modelling is that there are no set benchmarks over which we can evaluate and compare many models. Model zoo is sensible for the Caffe ecosystem because there exist few benchmarks a large portion of the community was working on (ImageNet for example). What datasets would you use to compare the DPMM on for example?\n\nMinor comments:\n* Table 1: I would suggest to compare to Li & Turner with alpha=0.5 (the equivalent of Hellinger distance) as they concluded this value performs best. I'm not sure why alpha=-1 was chosen here. \n* How do you handle discrete distributions (eg Figure 5)?\n* x_real is not defined in Figure 7.\n* I would suggest highlighting M in Figure 8.\n* Comma instead of period after \"rized), In\" on page 8.\n\nIn conclusion I would say that the software developments presented here are quite exciting, and I'm glad the authors are pushing towards practical and accessible \"inference for all\". In its current form though I am forced to give the submission itself a score of 5.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Deep Probabilistic Programming", "abstract": "We propose Edward, a Turing-complete probabilistic programming language. Edward defines two compositional representations\u2014random variables and inference. By treating inference as a first class citizen, on a par with modeling, we show that probabilistic programming can be as flexible and computationally efficient as traditional deep learning. For flexibility, Edward makes it easy to fit the same model using a variety of composable inference methods, ranging from point estimation to variational inference to MCMC. In addition, Edward can reuse the modeling representation as part of inference, facilitating the design of rich variational models and generative adversarial networks. For efficiency, Edward is integrated into TensorFlow, providing significant speedups over existing probabilistic systems. For example, we show on a benchmark logistic regression task that Edward is at least 35x faster than Stan and 6x faster than PyMC3. Further, Edward incurs no runtime overhead: it is as fast as handwritten TensorFlow.", "pdf": "/pdf/b51fb1ef5fa81889f226cfb62f5b6870b0ddb4fc.pdf", "paperhash": "tran|deep_probabilistic_programming", "keywords": [], "conflicts": ["adobe.com", "columbia.edu", "google.com"], "authors": ["Dustin Tran", "Matthew D. Hoffman", "Rif A. Saurous", "Eugene Brevdo", "Kevin Murphy", "David M. Blei"], "authorids": ["dustin@cs.columbia.edu", "mathoffm@adobe.com", "rif@google.com", "ebrevdo@google.com", "kpmurphy@google.com", "david.blei@columbia.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512604898, "id": "ICLR.cc/2017/conference/-/paper377/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper377/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper377/AnonReviewer3", "ICLR.cc/2017/conference/paper377/AnonReviewer1", "ICLR.cc/2017/conference/paper377/AnonReviewer4"], "reply": {"forum": "Hy6b4Pqee", "replyto": "Hy6b4Pqee", "writers": {"values-regex": "ICLR.cc/2017/conference/paper377/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper377/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512604898}}}, {"tddate": null, "tmdate": 1481832567791, "tcdate": 1481832567784, "number": 2, "id": "SklYddxNl", "invitation": "ICLR.cc/2017/conference/-/paper377/official/review", "forum": "Hy6b4Pqee", "replyto": "Hy6b4Pqee", "signatures": ["ICLR.cc/2017/conference/paper377/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper377/AnonReviewer1"], "content": {"title": "Very promising probabilistic programming language combining Bayes and deep learning", "rating": "8: Top 50% of accepted papers, clear accept", "review": "The paper introduces Edward, a probabilistic programming language\nbuilt over TensorFlow and Python, and supporting a broad range of most\npopular contemporary methods in probabilistic machine learning.\n\n\nQuality:\n\nThe Edward library provides an extremely impressive collection of\nmodern probabilistic inference methods in an easily usable form.\nThe paper provides a brief review of the most important techniques\nespecially from a representation learning perspective, combined with\ntwo experiments on implementing various modern variational inference\nmethods and GPU-accelerated HMC.\n\nThe first experiment (variational inference) would be more valuable if\nthere was a clear link to complete code to reproduce the results\nprovided. The HMC experiment looks OK, except the characterising Stan\nas a hand-optimised implementation seems unfair as the code is clearly\nnot hand-optimised for this specific model and hardware configuration.\nI do not think anyone doubts the quality of your implementation, so\nplease do not ruin the picture by unsubstantiated sensationalist\nclaims. Instead of current drama, I would suggest comparing\nhead-to-head against Stan on single core and separately reporting the\nextra speedups you gain from parallelisation and GPU. These numbers\nwould also help the readers to estimate the performance of the method\nfor other hardware configurations.\n\n\nClarity:\n\nThe paper is in general clearly written and easy to read. The numerous\ncode examples are helpful, but also difficult as it is sometimes\nunclear what is missing. It would be very helpful if the authors could\nprovide and clearly link to a machine-readable companion (a Jupyter\nnotebook would be great, but even text or HTML would be easier to\ncopy-paste from than a pdf like the paper) with complete runnable code\nfor all the examples.\n\n\nOriginality:\n\nThe Edward library is clearly a unique collection of probabilistic\ninference methods. In terms of the paper, the main threat to novelty\ncomes from previous publications of the same group. The main paper\nrefers to Tran et al. (2016a) which covers a lot of similar material,\nalthough from a different perspective. It is unclear if the other\npaper has been published or submitted somewhere and if so, where.\n\n\nSignificance:\n\nIt seems very likely Edward will have a profound impact on the field\nof Bayesian machine learning and deep learning.\n\n\nOther comments:\n\nIn Sec. 2 you draw a clear distinction between specialised languages\n(including Stan) and Turing-complete languages such as Edward. This\nseems unfair as I believe Stan is also Turing complete. Additionally\nno proof is provided to support the Turing-completeness of Edward.\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Deep Probabilistic Programming", "abstract": "We propose Edward, a Turing-complete probabilistic programming language. Edward defines two compositional representations\u2014random variables and inference. By treating inference as a first class citizen, on a par with modeling, we show that probabilistic programming can be as flexible and computationally efficient as traditional deep learning. For flexibility, Edward makes it easy to fit the same model using a variety of composable inference methods, ranging from point estimation to variational inference to MCMC. In addition, Edward can reuse the modeling representation as part of inference, facilitating the design of rich variational models and generative adversarial networks. For efficiency, Edward is integrated into TensorFlow, providing significant speedups over existing probabilistic systems. For example, we show on a benchmark logistic regression task that Edward is at least 35x faster than Stan and 6x faster than PyMC3. Further, Edward incurs no runtime overhead: it is as fast as handwritten TensorFlow.", "pdf": "/pdf/b51fb1ef5fa81889f226cfb62f5b6870b0ddb4fc.pdf", "paperhash": "tran|deep_probabilistic_programming", "keywords": [], "conflicts": ["adobe.com", "columbia.edu", "google.com"], "authors": ["Dustin Tran", "Matthew D. Hoffman", "Rif A. Saurous", "Eugene Brevdo", "Kevin Murphy", "David M. Blei"], "authorids": ["dustin@cs.columbia.edu", "mathoffm@adobe.com", "rif@google.com", "ebrevdo@google.com", "kpmurphy@google.com", "david.blei@columbia.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512604898, "id": "ICLR.cc/2017/conference/-/paper377/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper377/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper377/AnonReviewer3", "ICLR.cc/2017/conference/paper377/AnonReviewer1", "ICLR.cc/2017/conference/paper377/AnonReviewer4"], "reply": {"forum": "Hy6b4Pqee", "replyto": "Hy6b4Pqee", "writers": {"values-regex": "ICLR.cc/2017/conference/paper377/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper377/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512604898}}}, {"tddate": null, "tmdate": 1481728292474, "tcdate": 1481728292468, "number": 1, "id": "r12m-yJVx", "invitation": "ICLR.cc/2017/conference/-/paper377/official/review", "forum": "Hy6b4Pqee", "replyto": "Hy6b4Pqee", "signatures": ["ICLR.cc/2017/conference/paper377/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper377/AnonReviewer3"], "content": {"title": "A significant development to include the flexibility of inference to PPL", "rating": "7: Good paper, accept", "review": "Thank you for an interesting read.\n\nI found this paper very interesting. Since I don't think (deterministic) approximate inference is separated from the modelling procedure (cf. exact inference), it is important to allow the users to select the inference method to suit their needs and constraints. I'm not an expert of PPL, but to my knowledge this is the first package that I've seen which put more focus on compositional inference. Leveraging tensorflow is also a plus, which allows flexible computation graph design as well as parallel computation using GPUs.\n\nThe only question I have is about the design of flexible objective functions to learn hyper-parameters (or in the paper those variables associated with delta q distributions). It seems hyper-parameter learning is also specified as inference, which makes sense if using MAP. However the authors also demonstrated other objective functions such as Renyi divergences, does that mean the user need to define a new class of inference method whenever they want to test an alternative loss function?", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Deep Probabilistic Programming", "abstract": "We propose Edward, a Turing-complete probabilistic programming language. Edward defines two compositional representations\u2014random variables and inference. By treating inference as a first class citizen, on a par with modeling, we show that probabilistic programming can be as flexible and computationally efficient as traditional deep learning. For flexibility, Edward makes it easy to fit the same model using a variety of composable inference methods, ranging from point estimation to variational inference to MCMC. In addition, Edward can reuse the modeling representation as part of inference, facilitating the design of rich variational models and generative adversarial networks. For efficiency, Edward is integrated into TensorFlow, providing significant speedups over existing probabilistic systems. For example, we show on a benchmark logistic regression task that Edward is at least 35x faster than Stan and 6x faster than PyMC3. Further, Edward incurs no runtime overhead: it is as fast as handwritten TensorFlow.", "pdf": "/pdf/b51fb1ef5fa81889f226cfb62f5b6870b0ddb4fc.pdf", "paperhash": "tran|deep_probabilistic_programming", "keywords": [], "conflicts": ["adobe.com", "columbia.edu", "google.com"], "authors": ["Dustin Tran", "Matthew D. Hoffman", "Rif A. Saurous", "Eugene Brevdo", "Kevin Murphy", "David M. Blei"], "authorids": ["dustin@cs.columbia.edu", "mathoffm@adobe.com", "rif@google.com", "ebrevdo@google.com", "kpmurphy@google.com", "david.blei@columbia.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512604898, "id": "ICLR.cc/2017/conference/-/paper377/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper377/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper377/AnonReviewer3", "ICLR.cc/2017/conference/paper377/AnonReviewer1", "ICLR.cc/2017/conference/paper377/AnonReviewer4"], "reply": {"forum": "Hy6b4Pqee", "replyto": "Hy6b4Pqee", "writers": {"values-regex": "ICLR.cc/2017/conference/paper377/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper377/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512604898}}}, {"tddate": null, "tmdate": 1480726044224, "tcdate": 1480725062100, "number": 2, "id": "HkRHG5kQx", "invitation": "ICLR.cc/2017/conference/-/paper377/public/comment", "forum": "Hy6b4Pqee", "replyto": "rkKQykkQx", "signatures": ["~Dustin_Tran1"], "readers": ["everyone"], "writers": ["~Dustin_Tran1"], "content": {"title": "re:Clarification to the Bayesian RNN", "comment": "Thanks for the question on clarification!\n\nIn general, the model snippets are comprehensive in that they only omit import statements (e.g., \"from edward.models import Normal\") and constants such as N, M, H, D, etc. No inference is shown for them.\n\nThe complete examples in the Appendix only omit the import statements. I have updated the submission to reflect them. For example, running the VAE script will not only train the model but also generate images during training and export them to a directory. (The embedding example omits a function which builds data mini-batches; this requires tedious data pre-processing which we hope to clean up before putting in the paper.)\n\nOur apologies on the Bayesian RNN confusion. We hoped to focus attention on the difficult piece, which is hidden states with unspecified length. I have updated the submission so that the example also includes a response variable of unspecified length."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Deep Probabilistic Programming", "abstract": "We propose Edward, a Turing-complete probabilistic programming language. Edward defines two compositional representations\u2014random variables and inference. By treating inference as a first class citizen, on a par with modeling, we show that probabilistic programming can be as flexible and computationally efficient as traditional deep learning. For flexibility, Edward makes it easy to fit the same model using a variety of composable inference methods, ranging from point estimation to variational inference to MCMC. In addition, Edward can reuse the modeling representation as part of inference, facilitating the design of rich variational models and generative adversarial networks. For efficiency, Edward is integrated into TensorFlow, providing significant speedups over existing probabilistic systems. For example, we show on a benchmark logistic regression task that Edward is at least 35x faster than Stan and 6x faster than PyMC3. Further, Edward incurs no runtime overhead: it is as fast as handwritten TensorFlow.", "pdf": "/pdf/b51fb1ef5fa81889f226cfb62f5b6870b0ddb4fc.pdf", "paperhash": "tran|deep_probabilistic_programming", "keywords": [], "conflicts": ["adobe.com", "columbia.edu", "google.com"], "authors": ["Dustin Tran", "Matthew D. Hoffman", "Rif A. Saurous", "Eugene Brevdo", "Kevin Murphy", "David M. Blei"], "authorids": ["dustin@cs.columbia.edu", "mathoffm@adobe.com", "rif@google.com", "ebrevdo@google.com", "kpmurphy@google.com", "david.blei@columbia.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287600525, "id": "ICLR.cc/2017/conference/-/paper377/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Hy6b4Pqee", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper377/reviewers", "ICLR.cc/2017/conference/paper377/areachairs"], "cdate": 1485287600525}}}, {"tddate": null, "tmdate": 1480720628163, "tcdate": 1480720628158, "number": 1, "id": "SJ2eZFkXg", "invitation": "ICLR.cc/2017/conference/-/paper377/public/comment", "forum": "Hy6b4Pqee", "replyto": "Sy0_4dsGl", "signatures": ["~Dustin_Tran1"], "readers": ["everyone"], "writers": ["~Dustin_Tran1"], "content": {"title": "re:Experiment 5.1", "comment": "Thanks for the question! You can find source code for the different gradient estimators here:\n\nhttps://github.com/blei-lab/edward/blob/master/edward/inferences/klqp.py\n\nEach objective is written as a separate function, and a user can leverage a specific gradient estimator such as \"ReparameterizationEntropyKLqp\": this minimizes the KL(q||p) divergence measure using reparameterization gradients and an analytic entropy.\n\nEach of these modifications is a modification in how to estimate the objective and its gradients. For example, the one mentioned above only takes Monte Carlo samples to estimate the model's joint log density, E_{q(z | lambda)} [ log p(x, z) ], and it calculates the entropy term analytically, - E_{q(z | lambda)} [ log q(z | lambda) ].\n\nApologies on the zoo website. We should have it up again shortly."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Deep Probabilistic Programming", "abstract": "We propose Edward, a Turing-complete probabilistic programming language. Edward defines two compositional representations\u2014random variables and inference. By treating inference as a first class citizen, on a par with modeling, we show that probabilistic programming can be as flexible and computationally efficient as traditional deep learning. For flexibility, Edward makes it easy to fit the same model using a variety of composable inference methods, ranging from point estimation to variational inference to MCMC. In addition, Edward can reuse the modeling representation as part of inference, facilitating the design of rich variational models and generative adversarial networks. For efficiency, Edward is integrated into TensorFlow, providing significant speedups over existing probabilistic systems. For example, we show on a benchmark logistic regression task that Edward is at least 35x faster than Stan and 6x faster than PyMC3. Further, Edward incurs no runtime overhead: it is as fast as handwritten TensorFlow.", "pdf": "/pdf/b51fb1ef5fa81889f226cfb62f5b6870b0ddb4fc.pdf", "paperhash": "tran|deep_probabilistic_programming", "keywords": [], "conflicts": ["adobe.com", "columbia.edu", "google.com"], "authors": ["Dustin Tran", "Matthew D. Hoffman", "Rif A. Saurous", "Eugene Brevdo", "Kevin Murphy", "David M. Blei"], "authorids": ["dustin@cs.columbia.edu", "mathoffm@adobe.com", "rif@google.com", "ebrevdo@google.com", "kpmurphy@google.com", "david.blei@columbia.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287600525, "id": "ICLR.cc/2017/conference/-/paper377/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Hy6b4Pqee", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper377/reviewers", "ICLR.cc/2017/conference/paper377/areachairs"], "cdate": 1485287600525}}}, {"tddate": null, "tmdate": 1480679201356, "tcdate": 1480679201351, "number": 2, "id": "rkKQykkQx", "invitation": "ICLR.cc/2017/conference/-/paper377/pre-review/question", "forum": "Hy6b4Pqee", "replyto": "Hy6b4Pqee", "signatures": ["ICLR.cc/2017/conference/paper377/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper377/AnonReviewer1"], "content": {"title": "Clarification to the Bayesian RNN", "question": "Thanks for the interesting paper!\n\nI found the code examples difficult to follow because they were clearly incomplete snippets and it was unclear how much and what was being left out. Even the supposedly complete examples in the Appendix are not really complete in the sense that they could be run as written (missing imports, maybe something else too?). This was especially challenging with the Bayesian RNN (Fig. 3) which did not seem to connect to any observations, making it unclear how it could be used. Perhaps the authors could clarify this?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Deep Probabilistic Programming", "abstract": "We propose Edward, a Turing-complete probabilistic programming language. Edward defines two compositional representations\u2014random variables and inference. By treating inference as a first class citizen, on a par with modeling, we show that probabilistic programming can be as flexible and computationally efficient as traditional deep learning. For flexibility, Edward makes it easy to fit the same model using a variety of composable inference methods, ranging from point estimation to variational inference to MCMC. In addition, Edward can reuse the modeling representation as part of inference, facilitating the design of rich variational models and generative adversarial networks. For efficiency, Edward is integrated into TensorFlow, providing significant speedups over existing probabilistic systems. For example, we show on a benchmark logistic regression task that Edward is at least 35x faster than Stan and 6x faster than PyMC3. Further, Edward incurs no runtime overhead: it is as fast as handwritten TensorFlow.", "pdf": "/pdf/b51fb1ef5fa81889f226cfb62f5b6870b0ddb4fc.pdf", "paperhash": "tran|deep_probabilistic_programming", "keywords": [], "conflicts": ["adobe.com", "columbia.edu", "google.com"], "authors": ["Dustin Tran", "Matthew D. Hoffman", "Rif A. Saurous", "Eugene Brevdo", "Kevin Murphy", "David M. Blei"], "authorids": ["dustin@cs.columbia.edu", "mathoffm@adobe.com", "rif@google.com", "ebrevdo@google.com", "kpmurphy@google.com", "david.blei@columbia.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959313313, "id": "ICLR.cc/2017/conference/-/paper377/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper377/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper377/AnonReviewer3", "ICLR.cc/2017/conference/paper377/AnonReviewer1"], "reply": {"forum": "Hy6b4Pqee", "replyto": "Hy6b4Pqee", "writers": {"values-regex": "ICLR.cc/2017/conference/paper377/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper377/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959313313}}}, {"tddate": null, "tmdate": 1480455285686, "tcdate": 1480455285681, "number": 1, "id": "Sy0_4dsGl", "invitation": "ICLR.cc/2017/conference/-/paper377/pre-review/question", "forum": "Hy6b4Pqee", "replyto": "Hy6b4Pqee", "signatures": ["ICLR.cc/2017/conference/paper377/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper377/AnonReviewer3"], "content": {"title": "Experiment 5.1", "question": "Thank you for an interesting read.\n\nRegarding experiments reported in section 5.1: could you say a bit more on how you modify the decoder objective? Does your library provide different decoder objectives to the users?\n\nAlso the zoo website is unavailable."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Deep Probabilistic Programming", "abstract": "We propose Edward, a Turing-complete probabilistic programming language. Edward defines two compositional representations\u2014random variables and inference. By treating inference as a first class citizen, on a par with modeling, we show that probabilistic programming can be as flexible and computationally efficient as traditional deep learning. For flexibility, Edward makes it easy to fit the same model using a variety of composable inference methods, ranging from point estimation to variational inference to MCMC. In addition, Edward can reuse the modeling representation as part of inference, facilitating the design of rich variational models and generative adversarial networks. For efficiency, Edward is integrated into TensorFlow, providing significant speedups over existing probabilistic systems. For example, we show on a benchmark logistic regression task that Edward is at least 35x faster than Stan and 6x faster than PyMC3. Further, Edward incurs no runtime overhead: it is as fast as handwritten TensorFlow.", "pdf": "/pdf/b51fb1ef5fa81889f226cfb62f5b6870b0ddb4fc.pdf", "paperhash": "tran|deep_probabilistic_programming", "keywords": [], "conflicts": ["adobe.com", "columbia.edu", "google.com"], "authors": ["Dustin Tran", "Matthew D. Hoffman", "Rif A. Saurous", "Eugene Brevdo", "Kevin Murphy", "David M. Blei"], "authorids": ["dustin@cs.columbia.edu", "mathoffm@adobe.com", "rif@google.com", "ebrevdo@google.com", "kpmurphy@google.com", "david.blei@columbia.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959313313, "id": "ICLR.cc/2017/conference/-/paper377/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper377/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper377/AnonReviewer3", "ICLR.cc/2017/conference/paper377/AnonReviewer1"], "reply": {"forum": "Hy6b4Pqee", "replyto": "Hy6b4Pqee", "writers": {"values-regex": "ICLR.cc/2017/conference/paper377/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper377/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959313313}}}], "count": 13}