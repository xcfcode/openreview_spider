{"notes": [{"id": "BkxREeHKPS", "original": "S1xt2vlFDS", "number": 2267, "cdate": 1569439797886, "ddate": null, "tcdate": 1569439797886, "tmdate": 1577168231991, "tddate": null, "forum": "BkxREeHKPS", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["kuba.swiatkowski@gmail.com", "kevin.roth@inf.ethz.ch", "basveeling@gmail.com", "linh.tran@imperial.ac.uk", "jvdillon@google.com", "jaspersnoek@gmail.com", "stephan.mandt@gmail.com", "salimans@google.com", "rjenatton@google.com", "nowozin@google.com"], "title": "On the Parameterization of Gaussian Mean Field Posteriors in Bayesian Neural Networks", "authors": ["Jakub \u015awi\u0105tkowski", "Kevin Roth", "Bastiaan S. Veeling", "Linh Tran", "Joshua V. Dillon", "Jasper Snoek", "Stephan Mandt", "Tim Salimans", "Rodolphe Jenatton", "Sebastian Nowozin"], "pdf": "/pdf/3ef480cf857a0991b5fcdfe7e4d1f9b2c6ef201b.pdf", "TL;DR": "Mean field VB uses twice as many parameters; we tie variance parameters in mean field VB without any loss in ELBO, gaining speed and lower variance gradients.", "abstract": "Variational Bayesian Inference is a popular methodology for approximating posterior distributions in Bayesian neural networks. Recent work developing this class of methods has explored ever richer parameterizations of the approximate posterior in the hope of improving performance. In contrast, here we share a curious experimental finding that suggests instead restricting the variational distribution to a more compact parameterization. For a variety of deep Bayesian neural networks trained using Gaussian mean-field variational inference, we find that the posterior standard deviations consistently exhibits strong low-rank structure after convergence. This means that by decomposing these variational parameters into a low-rank factorization, we can make our variational approximation more compact without decreasing the models' performance. What's more, we find that such factorized parameterizations are easier to train since they improve the signal-to-noise ratio of stochastic gradient estimates of the variational lower bound, resulting in faster convergence.", "keywords": ["variational Bayes", "Bayesian neural networks", "mean field"], "paperhash": "witkowski|on_the_parameterization_of_gaussian_mean_field_posteriors_in_bayesian_neural_networks", "original_pdf": "/attachment/3c8bcb524d0dc338b65be1d5ae0d970127bc4d85.pdf", "_bibtex": "@misc{\n{\\'s}wi{\\k{a}}tkowski2020on,\ntitle={On the Parameterization of Gaussian Mean Field Posteriors in Bayesian Neural Networks},\nauthor={Jakub {\\'S}wi{\\k{a}}tkowski and Kevin Roth and Bastiaan S. Veeling and Linh Tran and Joshua V. Dillon and Jasper Snoek and Stephan Mandt and Tim Salimans and Rodolphe Jenatton and Sebastian Nowozin},\nyear={2020},\nurl={https://openreview.net/forum?id=BkxREeHKPS}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "fJW8r359dn", "original": null, "number": 1, "cdate": 1576798744755, "ddate": null, "tcdate": 1576798744755, "tmdate": 1576800891402, "tddate": null, "forum": "BkxREeHKPS", "replyto": "BkxREeHKPS", "invitation": "ICLR.cc/2020/Conference/Paper2267/-/Decision", "content": {"decision": "Reject", "comment": "This paper proposes to reduce the number of variational parameters for mean-field VI. A low-rank approximation is used for this purpose. Results on a few small problems are reported.\n\nAs R3 has pointed out, the main reason to reject this paper is the lack of comparison of uncertainty estimates. I also agree that, recent Adam-like optimizers do use preconditioning that can be interpreted as variances, so it is not clear why reducing this will give better results.\n\nI agree with R2's comments about missing the \"point estimate\" baseline. Also the reason for rank 1,2,3 giving better accuracies is unclear and I think the reasons provided by the authors is speculative.\n\nI do believe that reducing the parameterization is a reasonable idea and could be useful. But it is not clear if the proposal of this paper is the right one. Due to this reason, I recommend to reject this paper. However, I highly encourage the authors to improve their paper taking these points into account.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["kuba.swiatkowski@gmail.com", "kevin.roth@inf.ethz.ch", "basveeling@gmail.com", "linh.tran@imperial.ac.uk", "jvdillon@google.com", "jaspersnoek@gmail.com", "stephan.mandt@gmail.com", "salimans@google.com", "rjenatton@google.com", "nowozin@google.com"], "title": "On the Parameterization of Gaussian Mean Field Posteriors in Bayesian Neural Networks", "authors": ["Jakub \u015awi\u0105tkowski", "Kevin Roth", "Bastiaan S. Veeling", "Linh Tran", "Joshua V. Dillon", "Jasper Snoek", "Stephan Mandt", "Tim Salimans", "Rodolphe Jenatton", "Sebastian Nowozin"], "pdf": "/pdf/3ef480cf857a0991b5fcdfe7e4d1f9b2c6ef201b.pdf", "TL;DR": "Mean field VB uses twice as many parameters; we tie variance parameters in mean field VB without any loss in ELBO, gaining speed and lower variance gradients.", "abstract": "Variational Bayesian Inference is a popular methodology for approximating posterior distributions in Bayesian neural networks. Recent work developing this class of methods has explored ever richer parameterizations of the approximate posterior in the hope of improving performance. In contrast, here we share a curious experimental finding that suggests instead restricting the variational distribution to a more compact parameterization. For a variety of deep Bayesian neural networks trained using Gaussian mean-field variational inference, we find that the posterior standard deviations consistently exhibits strong low-rank structure after convergence. This means that by decomposing these variational parameters into a low-rank factorization, we can make our variational approximation more compact without decreasing the models' performance. What's more, we find that such factorized parameterizations are easier to train since they improve the signal-to-noise ratio of stochastic gradient estimates of the variational lower bound, resulting in faster convergence.", "keywords": ["variational Bayes", "Bayesian neural networks", "mean field"], "paperhash": "witkowski|on_the_parameterization_of_gaussian_mean_field_posteriors_in_bayesian_neural_networks", "original_pdf": "/attachment/3c8bcb524d0dc338b65be1d5ae0d970127bc4d85.pdf", "_bibtex": "@misc{\n{\\'s}wi{\\k{a}}tkowski2020on,\ntitle={On the Parameterization of Gaussian Mean Field Posteriors in Bayesian Neural Networks},\nauthor={Jakub {\\'S}wi{\\k{a}}tkowski and Kevin Roth and Bastiaan S. Veeling and Linh Tran and Joshua V. Dillon and Jasper Snoek and Stephan Mandt and Tim Salimans and Rodolphe Jenatton and Sebastian Nowozin},\nyear={2020},\nurl={https://openreview.net/forum?id=BkxREeHKPS}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "BkxREeHKPS", "replyto": "BkxREeHKPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795705345, "tmdate": 1576800253102, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2267/-/Decision"}}}, {"id": "rylP-SK3jr", "original": null, "number": 3, "cdate": 1573848318952, "ddate": null, "tcdate": 1573848318952, "tmdate": 1573848318952, "tddate": null, "forum": "BkxREeHKPS", "replyto": "B1xP-y9tFr", "invitation": "ICLR.cc/2020/Conference/Paper2267/-/Official_Comment", "content": {"title": "Response to AnonReviewer3", "comment": "[R3.1]\nWhile the observation is somewhat interesting, currently it is only verified in a narrow range of network architectures, and it's unclear if the observation and the proposed method will still be useful on network architectures used in real-world applications. As such, I believe this work would be more suitable as a workshop presentation. \n\nMore specifically, the models considered are MLP on MNIST, LeNet on CIFAR-100 and LSTM on IMDB. These choices are not practical, as the reported performance indicates (e.g. 45% accuracy on CIFAR-100); as such these results cannot support the claim that the proposed low-rank parameterization could be useful in practice: while MFVI can be useful on some model architectures, it lead to pathologies on others, especially on smaller networks. See Fig.1 in [1] for an example and [2] for a possible explanation. Also note that the reported accuracy on MNIST is ~2% worse than the typical values in BNN papers using a comparable setting, e.g. [3]. These facts unfortunately lead to the doubt that the proposed low-rank parameterization could only match the performance of MFVI when MFVI is not that useful.\n\n[R3.1 our response]\nEncouraged by the reviewer, we investigated the behaviour on a larger ResNet-18 model (provided by https://github.com/tensorflow/probability/blob/master/tensorflow_probability/examples/cifar10_bnn.py) and demonstrate that the approximation quality claimed in our paper holds also for convolution layers and for the deeper ResNet-18 model (see Figure 3 in Section 2 of the updated paper). We are therefore confident the finding is of broad applicability and plan to include further experiments in the final version of the paper. \n\nThe 2% gap in the MNIST accuracies is due to the difference in the training procedure we used and the procedures which are commonly used in other MFVI papers. We train the models until full ELBO convergence, without early stopping (which is commonly used in the MFVI literature). Early stopping can increase the validation accuracy by e.g. 2% compared to the accuracy at full convergence. The reason for this is that the MFVI models tend to start under-fitting as training progresses when using the full contribution of the KL term. This is due to the fact that the posterior variances increase to reach the prior variance and introduce large amount of noise which reduces how well the model can fit the training data. This is a limitation of the MFVI approach more generally and not specific to our method, but we agree that it is important to understand and address this shortcoming of the MFVI approach in future research. \n\n[R3.2]\nAnother major concern is that I'm not sure if the proposed low-rank variational would actually save parameters in practice, since the variance parameter in MFVI could already be stored as the preconditioners in Adam-like optimizers [4-5]. \n\n[R3.2 our response]\nWe agree that if preconditioning is feasible then the training time memory savings are irrelevant.  However, after training the preconditioner is typically discarded and for test-time inference our method does approximately halve the required model size as shown in Tables 2 and 5.\n\n[R3.3]\nSuggestions for future improvement:\n* Re-do the experiments using more complex network architectures, and optionally, on larger datasets / more complex tasks (e.g. image segmentation as in [6]).\n* Also, consider setups more commonly used in previous BNN papers, e.g. VGG/ResNet on CIFAR-10 has been used in [4,7,8]. CIFAR-100 could be sufficiently complex as a BNN benchmark, but few papers reported results on it.\n* Report the quality of the learned uncertainty, either directly as in [9] or using performance of downstream tasks, e.g. RL and adversarial robustness as in [4,8].\n\n[R3.3 our response]\nAs the reviewer suggested, we extended the experiments using a more complex network architecture (ResNet) trained on a commonly used CIFAR-10 benchmark. In the future, we also plan to extend the analysis to report the quality of the learned uncertainty as the reviewer suggested."}, "signatures": ["ICLR.cc/2020/Conference/Paper2267/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2267/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["kuba.swiatkowski@gmail.com", "kevin.roth@inf.ethz.ch", "basveeling@gmail.com", "linh.tran@imperial.ac.uk", "jvdillon@google.com", "jaspersnoek@gmail.com", "stephan.mandt@gmail.com", "salimans@google.com", "rjenatton@google.com", "nowozin@google.com"], "title": "On the Parameterization of Gaussian Mean Field Posteriors in Bayesian Neural Networks", "authors": ["Jakub \u015awi\u0105tkowski", "Kevin Roth", "Bastiaan S. Veeling", "Linh Tran", "Joshua V. Dillon", "Jasper Snoek", "Stephan Mandt", "Tim Salimans", "Rodolphe Jenatton", "Sebastian Nowozin"], "pdf": "/pdf/3ef480cf857a0991b5fcdfe7e4d1f9b2c6ef201b.pdf", "TL;DR": "Mean field VB uses twice as many parameters; we tie variance parameters in mean field VB without any loss in ELBO, gaining speed and lower variance gradients.", "abstract": "Variational Bayesian Inference is a popular methodology for approximating posterior distributions in Bayesian neural networks. Recent work developing this class of methods has explored ever richer parameterizations of the approximate posterior in the hope of improving performance. In contrast, here we share a curious experimental finding that suggests instead restricting the variational distribution to a more compact parameterization. For a variety of deep Bayesian neural networks trained using Gaussian mean-field variational inference, we find that the posterior standard deviations consistently exhibits strong low-rank structure after convergence. This means that by decomposing these variational parameters into a low-rank factorization, we can make our variational approximation more compact without decreasing the models' performance. What's more, we find that such factorized parameterizations are easier to train since they improve the signal-to-noise ratio of stochastic gradient estimates of the variational lower bound, resulting in faster convergence.", "keywords": ["variational Bayes", "Bayesian neural networks", "mean field"], "paperhash": "witkowski|on_the_parameterization_of_gaussian_mean_field_posteriors_in_bayesian_neural_networks", "original_pdf": "/attachment/3c8bcb524d0dc338b65be1d5ae0d970127bc4d85.pdf", "_bibtex": "@misc{\n{\\'s}wi{\\k{a}}tkowski2020on,\ntitle={On the Parameterization of Gaussian Mean Field Posteriors in Bayesian Neural Networks},\nauthor={Jakub {\\'S}wi{\\k{a}}tkowski and Kevin Roth and Bastiaan S. Veeling and Linh Tran and Joshua V. Dillon and Jasper Snoek and Stephan Mandt and Tim Salimans and Rodolphe Jenatton and Sebastian Nowozin},\nyear={2020},\nurl={https://openreview.net/forum?id=BkxREeHKPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BkxREeHKPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2267/Authors", "ICLR.cc/2020/Conference/Paper2267/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2267/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2267/Reviewers", "ICLR.cc/2020/Conference/Paper2267/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2267/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2267/Authors|ICLR.cc/2020/Conference/Paper2267/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504143890, "tmdate": 1576860554408, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2267/Authors", "ICLR.cc/2020/Conference/Paper2267/Reviewers", "ICLR.cc/2020/Conference/Paper2267/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2267/-/Official_Comment"}}}, {"id": "r1lC94K3iS", "original": null, "number": 2, "cdate": 1573848214502, "ddate": null, "tcdate": 1573848214502, "tmdate": 1573848214502, "tddate": null, "forum": "BkxREeHKPS", "replyto": "HJxXIP82YB", "invitation": "ICLR.cc/2020/Conference/Paper2267/-/Official_Comment", "content": {"title": "Response to AnonReviewer2", "comment": "[R2.1]\n- The authors restrict their analysis to dense layers only. Moreover, it remains conceptually unclear, why and when the proposed model should be useful and represent a good approximation of the full rank model. \n\n- The experimental study is restricted to small models, e.g. in the case of CIFAR a quite shallow version of LeNet which achieves only ~45% validation accuracy. It remains unclear, whether the observed low rank structure of the variance matrices (of the dense layers) will scale to deeper models that could achieve competitive validation accuracies.\n\n[R2.1 our response]\nEncouraged by the reviewer, we investigated the behaviour on a larger ResNet-18 model (provided by https://github.com/tensorflow/probability/blob/master/tensorflow_probability/examples/cifar10_bnn.py) and demonstrate that the approximation quality claimed in our paper for the dense layers of the MLP, CNN and LSTM models holds also for the convolution layers and of the deeper ResNet-18 model (see Figure 3 in Section 2 and Appendix C of the updated paper). We are therefore confident the finding is of broad applicability and plan to include further experiments in the final version of the paper. \n\nExploring the theoretical justification for the observed phenomena remains an interesting future work. In particular, we are currently considering simpler settings, e.g., shallow Bayesian neural networks with linear activation functions, where some derivations can be obtained in closed forms, which constitutes a good starting point for a theoretical analysis.\n\n[R2.2] \nWhen measuring the impact of the low rank tying of the posterior variances, the authors compare to the full rank model only. I am missing the \"point estimate\" baseline for these models. For if the the positive impact of the Bayesian inference approach with the full rank model is small when compared to the point estimate, then, as a consequence, the impact of the low rank tying must be small when compared to the full rank model.\n\n[R2.2 our response]\nThe goal of our paper is not to prove the positive impact of the Bayesian inference or the low rank tying over the point estimate models. In contrast, we start from the observation that scaling up mean-field and making it more robust is still a challenging problem ([0]). We try to tackle this problem by reducing the number of parameters to optimize---while maintaining a comparable predictive behavior---and obtaining less noisy gradient estimates.\n\n[R2.3] \nThe numbers reported in Table 3 (second group of experiments) raise some questions not addressed by the authors. It remains unclear, why the low rank model with ranks 1,2,3 gives better accuracies on CIFAR than the full rank model. The same holds for the LSTM experiment. This may indicate some kind of \"overfitting\" and should have been analysed by the authors in order to \"disentangle\" possible overfitting issues from the question of validity of the proposed low rank model.\n\n[R2.3 our response]\nWe are confident that the results are not influenced by the \"overfitting\" issues. By varying k of the k-tied Normal, we are varying the parameterization of the posterior distribution, not the original model. Restricting the posterior approximation will not lessen overfitting. In fact, the point estimate (zero variance) will overfit most of all posterior approximations. We hypothesise that the improvement in the CNN model from using the low-rank model can be attributed to the improved learning dynamics (higher gradient signal-to-noise ratio)."}, "signatures": ["ICLR.cc/2020/Conference/Paper2267/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2267/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["kuba.swiatkowski@gmail.com", "kevin.roth@inf.ethz.ch", "basveeling@gmail.com", "linh.tran@imperial.ac.uk", "jvdillon@google.com", "jaspersnoek@gmail.com", "stephan.mandt@gmail.com", "salimans@google.com", "rjenatton@google.com", "nowozin@google.com"], "title": "On the Parameterization of Gaussian Mean Field Posteriors in Bayesian Neural Networks", "authors": ["Jakub \u015awi\u0105tkowski", "Kevin Roth", "Bastiaan S. Veeling", "Linh Tran", "Joshua V. Dillon", "Jasper Snoek", "Stephan Mandt", "Tim Salimans", "Rodolphe Jenatton", "Sebastian Nowozin"], "pdf": "/pdf/3ef480cf857a0991b5fcdfe7e4d1f9b2c6ef201b.pdf", "TL;DR": "Mean field VB uses twice as many parameters; we tie variance parameters in mean field VB without any loss in ELBO, gaining speed and lower variance gradients.", "abstract": "Variational Bayesian Inference is a popular methodology for approximating posterior distributions in Bayesian neural networks. Recent work developing this class of methods has explored ever richer parameterizations of the approximate posterior in the hope of improving performance. In contrast, here we share a curious experimental finding that suggests instead restricting the variational distribution to a more compact parameterization. For a variety of deep Bayesian neural networks trained using Gaussian mean-field variational inference, we find that the posterior standard deviations consistently exhibits strong low-rank structure after convergence. This means that by decomposing these variational parameters into a low-rank factorization, we can make our variational approximation more compact without decreasing the models' performance. What's more, we find that such factorized parameterizations are easier to train since they improve the signal-to-noise ratio of stochastic gradient estimates of the variational lower bound, resulting in faster convergence.", "keywords": ["variational Bayes", "Bayesian neural networks", "mean field"], "paperhash": "witkowski|on_the_parameterization_of_gaussian_mean_field_posteriors_in_bayesian_neural_networks", "original_pdf": "/attachment/3c8bcb524d0dc338b65be1d5ae0d970127bc4d85.pdf", "_bibtex": "@misc{\n{\\'s}wi{\\k{a}}tkowski2020on,\ntitle={On the Parameterization of Gaussian Mean Field Posteriors in Bayesian Neural Networks},\nauthor={Jakub {\\'S}wi{\\k{a}}tkowski and Kevin Roth and Bastiaan S. Veeling and Linh Tran and Joshua V. Dillon and Jasper Snoek and Stephan Mandt and Tim Salimans and Rodolphe Jenatton and Sebastian Nowozin},\nyear={2020},\nurl={https://openreview.net/forum?id=BkxREeHKPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BkxREeHKPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2267/Authors", "ICLR.cc/2020/Conference/Paper2267/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2267/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2267/Reviewers", "ICLR.cc/2020/Conference/Paper2267/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2267/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2267/Authors|ICLR.cc/2020/Conference/Paper2267/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504143890, "tmdate": 1576860554408, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2267/Authors", "ICLR.cc/2020/Conference/Paper2267/Reviewers", "ICLR.cc/2020/Conference/Paper2267/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2267/-/Official_Comment"}}}, {"id": "SJlwMNK2iB", "original": null, "number": 1, "cdate": 1573848079185, "ddate": null, "tcdate": 1573848079185, "tmdate": 1573848079185, "tddate": null, "forum": "BkxREeHKPS", "replyto": "S1xrRMiJcr", "invitation": "ICLR.cc/2020/Conference/Paper2267/-/Official_Comment", "content": {"title": "Response to AnonReviewer1", "comment": "[R1.1] \nThe paper proposes a low-rank approximation to the diagonal of Gaussian mean field posterior which reduces the number of parameters to fit. They show that the predictive performance doesn't drop much compared with the full covariance but the number of parameters is significantly reduced. \n\n[R1.1 our response] \nTo be precise we are not comparing our method to the full covariance, which would not be tractable in our settings (the full covariance scales quadratically with respect to the total number of network parameters). Instead, we are comparing our method to the full parametrization of the diagonal covariance, i.e., the standard mean-field approximation. Our method is a further factorization of the diagonal covariance in the parameter space. See Figure 6 for a visual representation of this explanation.\n\n[R1.2] \nWhy Matrix normal distribution is related to k-tied Normal distribution when k=1. When k=1, the rank of UV^\\top is 1. The covariance is matrix normal is U\\otimes V, whose rank is rank(U)rank(V). Also MN has a full covariance for the Gaussian approximation, not mean field. MN is only equal to 1-tied when only diagonal row and column covariances are considered. If that's what the paper means, k-tied is only compared to MN with diagonal row and column covariances. Better make this point clear. \n\n[R1.2 our response] \nWe thank the reviewer for the comment. We changed the sentence below from our paper as suggested by the reviewer to make the comparison with the Matrix normal distribution clearer:\n\"<MN distribution> is related to our k-tied Normal distribution when k = 1\" -> \"<MN distribution> is related to our k-tied Normal distribution with $k=1$ when MN uses diagonal row and column covariances.\"\nWe have also added a figure (see Figure 6) to clarify this explanation.\n\n[R1.3] \nFigure 4 should show running time instead of training step. I don't think low-rank approximation is gonna influence the convergence that much. It should affect the evaluation speed of each step. \n\n[R1.3 our response] \nWe measured the evaluation speed of each step for a simple model with 2 dense layers [0]. For this model, the measurements show that the k-tied Normal posterior does not decrease the evaluation speed when compared to the standard parameterization of the GMFVI. This experiment can be replicated using the Colab notebook in [1]. We updated the end of section 3.2 and Appendix D to include these results. \n\nMore generally, we do not expect a significant change in the evaluation speed of each step when using the k-tied Normal posterior. The biggest additional operation per step when using the k-tied Normal posterior compared to the standard parameterization is the UV^T multiplication to materialize the posterior standard deviations matrix A, where U \\in R^{m \\times k}, V \\in R^{m \\times k} and A \\in R^{m \\times n}. The time complexity of this operations is O(kmn). This time complexity is usually negligible when compared to the complexity of data-weight matrix multiplication with time complexity of O(bmn), where b is the batch size. \n\n[0] https://www.tensorflow.org/tutorials/keras/classification\n[1] https://colab.research.google.com/drive/14pqe_VG5s49xlcXB-Jf8S9GoTFyjv4OF\n\n[R1.4]\nI think the trick the paper uses is a practical one but not significantly novel enough for the ICLR community. It feels like a standard trick people would do when fitting parameters for large matrices, i.e. exploring the low-rank structure and fitting the factorized matrices. Matrix normal is more significant since it reduces the number of parameters as well as maintaining a full covariance matrix with structures. If just focusing on the diagonal covariance, it already throws away the full covariance. Low-rank won't help much with improving the posterior distribution.\n\n[R1.4 our response]\nWe acknowledge that MN covers more expressive posterior distribution. However, the goal of our paper is not to investigate richer posterior distributions, beyond mean-field. Instead, we start from the observation that scaling up mean-field and making it more robust is still a challenging problem ([0]). We try to tackle this problem by reducing the number of parameters to optimize---while maintaining a comparable predictive behavior---and obtaining less noisy gradient estimates.\n\n[0] Practical Deep Learning with Bayesian Principles\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2267/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2267/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["kuba.swiatkowski@gmail.com", "kevin.roth@inf.ethz.ch", "basveeling@gmail.com", "linh.tran@imperial.ac.uk", "jvdillon@google.com", "jaspersnoek@gmail.com", "stephan.mandt@gmail.com", "salimans@google.com", "rjenatton@google.com", "nowozin@google.com"], "title": "On the Parameterization of Gaussian Mean Field Posteriors in Bayesian Neural Networks", "authors": ["Jakub \u015awi\u0105tkowski", "Kevin Roth", "Bastiaan S. Veeling", "Linh Tran", "Joshua V. Dillon", "Jasper Snoek", "Stephan Mandt", "Tim Salimans", "Rodolphe Jenatton", "Sebastian Nowozin"], "pdf": "/pdf/3ef480cf857a0991b5fcdfe7e4d1f9b2c6ef201b.pdf", "TL;DR": "Mean field VB uses twice as many parameters; we tie variance parameters in mean field VB without any loss in ELBO, gaining speed and lower variance gradients.", "abstract": "Variational Bayesian Inference is a popular methodology for approximating posterior distributions in Bayesian neural networks. Recent work developing this class of methods has explored ever richer parameterizations of the approximate posterior in the hope of improving performance. In contrast, here we share a curious experimental finding that suggests instead restricting the variational distribution to a more compact parameterization. For a variety of deep Bayesian neural networks trained using Gaussian mean-field variational inference, we find that the posterior standard deviations consistently exhibits strong low-rank structure after convergence. This means that by decomposing these variational parameters into a low-rank factorization, we can make our variational approximation more compact without decreasing the models' performance. What's more, we find that such factorized parameterizations are easier to train since they improve the signal-to-noise ratio of stochastic gradient estimates of the variational lower bound, resulting in faster convergence.", "keywords": ["variational Bayes", "Bayesian neural networks", "mean field"], "paperhash": "witkowski|on_the_parameterization_of_gaussian_mean_field_posteriors_in_bayesian_neural_networks", "original_pdf": "/attachment/3c8bcb524d0dc338b65be1d5ae0d970127bc4d85.pdf", "_bibtex": "@misc{\n{\\'s}wi{\\k{a}}tkowski2020on,\ntitle={On the Parameterization of Gaussian Mean Field Posteriors in Bayesian Neural Networks},\nauthor={Jakub {\\'S}wi{\\k{a}}tkowski and Kevin Roth and Bastiaan S. Veeling and Linh Tran and Joshua V. Dillon and Jasper Snoek and Stephan Mandt and Tim Salimans and Rodolphe Jenatton and Sebastian Nowozin},\nyear={2020},\nurl={https://openreview.net/forum?id=BkxREeHKPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BkxREeHKPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2267/Authors", "ICLR.cc/2020/Conference/Paper2267/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2267/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2267/Reviewers", "ICLR.cc/2020/Conference/Paper2267/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2267/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2267/Authors|ICLR.cc/2020/Conference/Paper2267/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504143890, "tmdate": 1576860554408, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2267/Authors", "ICLR.cc/2020/Conference/Paper2267/Reviewers", "ICLR.cc/2020/Conference/Paper2267/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2267/-/Official_Comment"}}}, {"id": "B1xP-y9tFr", "original": null, "number": 1, "cdate": 1571557118899, "ddate": null, "tcdate": 1571557118899, "tmdate": 1572972361227, "tddate": null, "forum": "BkxREeHKPS", "replyto": "BkxREeHKPS", "invitation": "ICLR.cc/2020/Conference/Paper2267/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper showed the (diagonal) variance parameters in mean-field VI for BNNs exhibit a low-rank structure, and that training from scratch using such a low-rank parameterization lead to comparable performance as well as increased SNR of the gradient. \n\nWhile the observation is somewhat interesting, currently it is only verified in a narrow range of network architectures, and it's unclear if the observation and the proposed method will still be useful on network architectures used in real-world applications. As such, I believe this work would be more suitable as a workshop presentation. \n\nMore specifically, the models considered are MLP on MNIST, LeNet on CIFAR-100 and LSTM on IMDB. These choices are not practical, as the reported performance indicates (e.g. 45% accuracy on CIFAR-100); as such these results cannot support the claim that the proposed low-rank parameterization could be useful in practice: while MFVI can be useful on some model architectures, it lead to pathologies on others, especially on smaller networks. See Fig.1 in [1] for an example and [2] for a possible explanation. Also note that the reported accuracy on MNIST is ~2% worse than the typical values in BNN papers using a comparable setting, e.g. [3]. These facts unfortunately lead to the doubt that the proposed low-rank parameterization could only match the performance of MFVI when MFVI is not that useful.\n\nAnother major concern is that I'm not sure if the proposed low-rank variational would actually save parameters in practice, since the variance parameter in MFVI could already be stored as the preconditioners in Adam-like optimizers [4-5]. \n\nSuggestions for future improvement:\n* Re-do the experiments using more complex network architectures, and optionally, on larger datasets / more complex tasks (e.g. image segmentation as in [6]).\n* Also, consider setups more commonly used in previous BNN papers, e.g. VGG/ResNet on CIFAR-10 has been used in [4,7,8]. CIFAR-100 could be sufficiently complex as a BNN benchmark, but few papers reported results on it.\n* Report the quality of the learned uncertainty, either directly as in [9] or using performance of downstream tasks, e.g. RL and adversarial robustness as in [4,8].\n\n# References\n[1] Structured and Efficient Variational Deep Learning with Matrix Gaussian Posteriors\n[2] Overpruning in Variational Bayesian Neural Networks\n[3] A Unified Particle-Optimization Framework for Scalable Bayesian Sampling\n[4] Noisy Natural Gradient as Variational Inference\n[5] Fast and Scalable Bayesian Deep Learning by Weight-Perturbation in Adam\n[6] Bayesian Uncertainty Estimation for Batch Normalized Deep Networks\n[7] Learning Weight Uncertainty with Stochastic Gradient MCMC for Shape Classification\n[8] Function Space Particle Optimization for Bayesian Neural Networks\n[9] Can You Trust Your Model\u2019s Uncertainty? Evaluating Predictive Uncertainty Under Dataset Shift"}, "signatures": ["ICLR.cc/2020/Conference/Paper2267/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2267/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["kuba.swiatkowski@gmail.com", "kevin.roth@inf.ethz.ch", "basveeling@gmail.com", "linh.tran@imperial.ac.uk", "jvdillon@google.com", "jaspersnoek@gmail.com", "stephan.mandt@gmail.com", "salimans@google.com", "rjenatton@google.com", "nowozin@google.com"], "title": "On the Parameterization of Gaussian Mean Field Posteriors in Bayesian Neural Networks", "authors": ["Jakub \u015awi\u0105tkowski", "Kevin Roth", "Bastiaan S. Veeling", "Linh Tran", "Joshua V. Dillon", "Jasper Snoek", "Stephan Mandt", "Tim Salimans", "Rodolphe Jenatton", "Sebastian Nowozin"], "pdf": "/pdf/3ef480cf857a0991b5fcdfe7e4d1f9b2c6ef201b.pdf", "TL;DR": "Mean field VB uses twice as many parameters; we tie variance parameters in mean field VB without any loss in ELBO, gaining speed and lower variance gradients.", "abstract": "Variational Bayesian Inference is a popular methodology for approximating posterior distributions in Bayesian neural networks. Recent work developing this class of methods has explored ever richer parameterizations of the approximate posterior in the hope of improving performance. In contrast, here we share a curious experimental finding that suggests instead restricting the variational distribution to a more compact parameterization. For a variety of deep Bayesian neural networks trained using Gaussian mean-field variational inference, we find that the posterior standard deviations consistently exhibits strong low-rank structure after convergence. This means that by decomposing these variational parameters into a low-rank factorization, we can make our variational approximation more compact without decreasing the models' performance. What's more, we find that such factorized parameterizations are easier to train since they improve the signal-to-noise ratio of stochastic gradient estimates of the variational lower bound, resulting in faster convergence.", "keywords": ["variational Bayes", "Bayesian neural networks", "mean field"], "paperhash": "witkowski|on_the_parameterization_of_gaussian_mean_field_posteriors_in_bayesian_neural_networks", "original_pdf": "/attachment/3c8bcb524d0dc338b65be1d5ae0d970127bc4d85.pdf", "_bibtex": "@misc{\n{\\'s}wi{\\k{a}}tkowski2020on,\ntitle={On the Parameterization of Gaussian Mean Field Posteriors in Bayesian Neural Networks},\nauthor={Jakub {\\'S}wi{\\k{a}}tkowski and Kevin Roth and Bastiaan S. Veeling and Linh Tran and Joshua V. Dillon and Jasper Snoek and Stephan Mandt and Tim Salimans and Rodolphe Jenatton and Sebastian Nowozin},\nyear={2020},\nurl={https://openreview.net/forum?id=BkxREeHKPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BkxREeHKPS", "replyto": "BkxREeHKPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2267/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2267/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574722376000, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2267/Reviewers"], "noninvitees": [], "tcdate": 1570237725282, "tmdate": 1574723090303, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2267/-/Official_Review"}}}, {"id": "HJxXIP82YB", "original": null, "number": 2, "cdate": 1571739467360, "ddate": null, "tcdate": 1571739467360, "tmdate": 1572972361190, "tddate": null, "forum": "BkxREeHKPS", "replyto": "BkxREeHKPS", "invitation": "ICLR.cc/2020/Conference/Paper2267/-/Official_Review", "content": {"rating": "1: Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "The paper considers variational Bayesian inference (learning) for neural networks assuming that both the prior distributions and the posterior distributions of the network weights are factorising Gaussians. It is well known that the respective optimisation task for the parameters of the posterior weight distributions (ELBO) is tractable by stochastic gradient ascent.  The authors propose to simplify (restrict) the model even further, by assuming that the posterior variances for fully connected layers, (when seen as a matrix) have low rank. It is pretty obvious that the ELBO optimisation task remains tractable in this case.\n\nThe authors perform two types of experiments to show that the proposed simpler model does not decrease the performance of the network, when compared to the full rank factorising model. They first show that the learned variances of dense layers indeed exhibit a low rank structure for three models learned on the corresponding data (MLP on MNIST, LeNet on CIFAR and LSTM on IMDB). Then, in a second step, they consider the proposed rank constraint during learning and show that with rank k >= 2 the models are able to achieve performance competitive with the full rank model and, moreover, exhibit better signal to noise ratio for the gradients during learning.\n\nThe paper is well written, clearly structured and technically correct. However, in my opinion, its novelty and new insights are restricted, which is why I suggest to reject the paper. The reasons for this are the following.\n\n- The authors restrict their analysis to dense layers only. Moreover, it remains conceptually unclear, why and when the proposed model should be useful and represent a good approximation of the full rank model. \n\n- The experimental study is restricted to small models, e.g. in the case of CIFAR a quite shallow version of LeNet which achieves only ~45% validation accuracy. It remains unclear, whether the observed low rank structure of the variance matrices (of the dense layers) will scale to deeper models that could achieve competitive validation accuracies.\n\n- When measuring the impact of the low rank tying of the posterior variances, the authors compare to the full rank model only. I am missing the \"point estimate\" baseline for these models. For if the the positive impact of the Bayesian inference approach with the full rank model is small when compared to the point estimate, then, as a consequence, the impact of the low rank tying must be small when compared to the full rank model.\n\n- The numbers reported in Table 3 (second group of experiments) raise some questions not addressed by the authors. It remains unclear, why the low rank model with ranks 1,2,3 gives better accuracies on CIFAR than the full rank model. The same holds for the LSTM experiment. This may indicate some kind of \"overfitting\" and should have been analysed by the authors in order to \"disentangle\" possible overfitting issues from the question of validity of the proposed low rank model."}, "signatures": ["ICLR.cc/2020/Conference/Paper2267/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2267/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["kuba.swiatkowski@gmail.com", "kevin.roth@inf.ethz.ch", "basveeling@gmail.com", "linh.tran@imperial.ac.uk", "jvdillon@google.com", "jaspersnoek@gmail.com", "stephan.mandt@gmail.com", "salimans@google.com", "rjenatton@google.com", "nowozin@google.com"], "title": "On the Parameterization of Gaussian Mean Field Posteriors in Bayesian Neural Networks", "authors": ["Jakub \u015awi\u0105tkowski", "Kevin Roth", "Bastiaan S. Veeling", "Linh Tran", "Joshua V. Dillon", "Jasper Snoek", "Stephan Mandt", "Tim Salimans", "Rodolphe Jenatton", "Sebastian Nowozin"], "pdf": "/pdf/3ef480cf857a0991b5fcdfe7e4d1f9b2c6ef201b.pdf", "TL;DR": "Mean field VB uses twice as many parameters; we tie variance parameters in mean field VB without any loss in ELBO, gaining speed and lower variance gradients.", "abstract": "Variational Bayesian Inference is a popular methodology for approximating posterior distributions in Bayesian neural networks. Recent work developing this class of methods has explored ever richer parameterizations of the approximate posterior in the hope of improving performance. In contrast, here we share a curious experimental finding that suggests instead restricting the variational distribution to a more compact parameterization. For a variety of deep Bayesian neural networks trained using Gaussian mean-field variational inference, we find that the posterior standard deviations consistently exhibits strong low-rank structure after convergence. This means that by decomposing these variational parameters into a low-rank factorization, we can make our variational approximation more compact without decreasing the models' performance. What's more, we find that such factorized parameterizations are easier to train since they improve the signal-to-noise ratio of stochastic gradient estimates of the variational lower bound, resulting in faster convergence.", "keywords": ["variational Bayes", "Bayesian neural networks", "mean field"], "paperhash": "witkowski|on_the_parameterization_of_gaussian_mean_field_posteriors_in_bayesian_neural_networks", "original_pdf": "/attachment/3c8bcb524d0dc338b65be1d5ae0d970127bc4d85.pdf", "_bibtex": "@misc{\n{\\'s}wi{\\k{a}}tkowski2020on,\ntitle={On the Parameterization of Gaussian Mean Field Posteriors in Bayesian Neural Networks},\nauthor={Jakub {\\'S}wi{\\k{a}}tkowski and Kevin Roth and Bastiaan S. Veeling and Linh Tran and Joshua V. Dillon and Jasper Snoek and Stephan Mandt and Tim Salimans and Rodolphe Jenatton and Sebastian Nowozin},\nyear={2020},\nurl={https://openreview.net/forum?id=BkxREeHKPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BkxREeHKPS", "replyto": "BkxREeHKPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2267/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2267/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574722376000, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2267/Reviewers"], "noninvitees": [], "tcdate": 1570237725282, "tmdate": 1574723090303, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2267/-/Official_Review"}}}, {"id": "S1xrRMiJcr", "original": null, "number": 3, "cdate": 1571955404851, "ddate": null, "tcdate": 1571955404851, "tmdate": 1572972361149, "tddate": null, "forum": "BkxREeHKPS", "replyto": "BkxREeHKPS", "invitation": "ICLR.cc/2020/Conference/Paper2267/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The paper proposes a low-rank approximation to the diagonal of Gaussian mean field posterior which reduces the number of parameters to fit. They show that the predictive performance doesn't drop much compared with the full covariance but the number of parameters is significantly reduced. \n\n1. Why Matrix normal distribution is related to k-tied Normal distribution when k=1. When k=1, the rank of UV^\\top is 1. The covariance is matrix normal is U\\otimes V, whose rank is rank(U)rank(V). Also MN has a full covariance for the Gaussian approximation, not mean field. MN is only equal to 1-tied when only diagonal row and column covariances are considered. If that's what the paper means, k-tied is only compared to MN with diagonal row and column covariances. Better make this point clear. \n\n2. Figure 4 should show running time instead of training step. I don't think low-rank approximation is gonna influence the convergence that much. It should affect the evaluation speed of each step. \n\nI think the trick the paper uses is a practical one but not significantly novel enough for the ICLR community. It feels like a standard trick people would do when fitting parameters for large matrices, i.e. exploring the low-rank structure and fitting the factorized matrices. Matrix normal is more significant since it reduces the number of parameters as well as maintaining a full covariance matrix with structures. If just focusing on the diagonal covariance, it already throws away the full covariance. Low-rank won't help much with improving the posterior distribution. "}, "signatures": ["ICLR.cc/2020/Conference/Paper2267/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2267/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["kuba.swiatkowski@gmail.com", "kevin.roth@inf.ethz.ch", "basveeling@gmail.com", "linh.tran@imperial.ac.uk", "jvdillon@google.com", "jaspersnoek@gmail.com", "stephan.mandt@gmail.com", "salimans@google.com", "rjenatton@google.com", "nowozin@google.com"], "title": "On the Parameterization of Gaussian Mean Field Posteriors in Bayesian Neural Networks", "authors": ["Jakub \u015awi\u0105tkowski", "Kevin Roth", "Bastiaan S. Veeling", "Linh Tran", "Joshua V. Dillon", "Jasper Snoek", "Stephan Mandt", "Tim Salimans", "Rodolphe Jenatton", "Sebastian Nowozin"], "pdf": "/pdf/3ef480cf857a0991b5fcdfe7e4d1f9b2c6ef201b.pdf", "TL;DR": "Mean field VB uses twice as many parameters; we tie variance parameters in mean field VB without any loss in ELBO, gaining speed and lower variance gradients.", "abstract": "Variational Bayesian Inference is a popular methodology for approximating posterior distributions in Bayesian neural networks. Recent work developing this class of methods has explored ever richer parameterizations of the approximate posterior in the hope of improving performance. In contrast, here we share a curious experimental finding that suggests instead restricting the variational distribution to a more compact parameterization. For a variety of deep Bayesian neural networks trained using Gaussian mean-field variational inference, we find that the posterior standard deviations consistently exhibits strong low-rank structure after convergence. This means that by decomposing these variational parameters into a low-rank factorization, we can make our variational approximation more compact without decreasing the models' performance. What's more, we find that such factorized parameterizations are easier to train since they improve the signal-to-noise ratio of stochastic gradient estimates of the variational lower bound, resulting in faster convergence.", "keywords": ["variational Bayes", "Bayesian neural networks", "mean field"], "paperhash": "witkowski|on_the_parameterization_of_gaussian_mean_field_posteriors_in_bayesian_neural_networks", "original_pdf": "/attachment/3c8bcb524d0dc338b65be1d5ae0d970127bc4d85.pdf", "_bibtex": "@misc{\n{\\'s}wi{\\k{a}}tkowski2020on,\ntitle={On the Parameterization of Gaussian Mean Field Posteriors in Bayesian Neural Networks},\nauthor={Jakub {\\'S}wi{\\k{a}}tkowski and Kevin Roth and Bastiaan S. Veeling and Linh Tran and Joshua V. Dillon and Jasper Snoek and Stephan Mandt and Tim Salimans and Rodolphe Jenatton and Sebastian Nowozin},\nyear={2020},\nurl={https://openreview.net/forum?id=BkxREeHKPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BkxREeHKPS", "replyto": "BkxREeHKPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2267/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2267/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574722376000, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2267/Reviewers"], "noninvitees": [], "tcdate": 1570237725282, "tmdate": 1574723090303, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2267/-/Official_Review"}}}], "count": 8}