{"notes": [{"ddate": null, "legacy_migration": true, "tmdate": 1402411440000, "tcdate": 1402411440000, "number": 11, "id": "CuniPDEmBQuTX", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "kklr_MTHMRQjG", "replyto": "kklr_MTHMRQjG", "signatures": ["walid saba"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "Very interesting paper with very 'intriguing' results indeed.\r\nI believe there are serious implications to these findings.\r\nI wonder if this paper is in anyway related to this (https://moalquraishi.wordpress.com/2014/05/25/what-does-a-neural-network-actually-do/) post, where the main finding/claim is that depth in neural networks can be represented by additional nodes in the (single) hidden layer - which says that deep networks do not have a representational power over traditional NNs."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Intriguing properties of neural networks", "decision": "submitted, no decision", "abstract": "Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains of the semantic information in the high layers of neural networks. Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extend. Specifically, we find that we can cause the network to misclassify an image by applying a certain imperceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.", "pdf": "https://arxiv.org/abs/1312.6199", "paperhash": "bruna|intriguing_properties_of_neural_networks", "keywords": [], "conflicts": [], "authors": ["Joan Bruna", "Christian Szegedy", "Ilya Sutskever", "Ian Goodfellow", "Wojciech Zaremba", "Rob Fergus", "Dumitru Erhan"], "authorids": ["joan.bruna@gmail.com", "szegedy@google.com", "ilya.at.cs@gmail.com", "goodfellow.ian@gmail.com", "woj.zaremba@gmail.com", "robfergus@gmail.com", "dumitru.erhan@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1401426600000, "tcdate": 1401426600000, "number": 10, "id": "v07zUlDhlOUpZ", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "kklr_MTHMRQjG", "replyto": "kklr_MTHMRQjG", "signatures": ["Bob Durrant"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "Hi Folks,\r\nInteresting manuscript that I enjoyed reading, but I have some concerns about how you establish the conclusion that 'The explanation is that the set of adversarial negatives is of extremely low probability, and thus is never (or rarely) observed in the test set, yet it is dense (much like the rational numbers, and so it is found near every\r\nvirtually every test case.'\r\nFWIW I think this is very likely the right conclusion, but I really don't think you show that.\r\n\r\nFilling in the gaps, I assume you did some sort of monte carlo using the images with Gaussian white noise, found few that were unclassifiable, and then concluded that the adversarial images must have low probability?\r\nI think you should say that, if that is what you did, and give the number of trials and enough other detail to replicate the experiments. Otherwise I think you should give enough detail so that we can understand how you reached this conclusion - your assertion is so blunt I can't imagine it is just a conjecture, and I don't think I'm the only reader who would like to know how you reached this conclusion.\r\n\r\nBut that is not my main concern.\r\n\r\nA more serious problem, I think, is that you were probably looking in the wrong place. I can give you both a heuristic argument and a formal one for why I think this:\r\n A heuristic argument for why you should believe this is that the adversarial images in the paper all look like the originals, while the noisy ones don't.\r\nA formal argument would be that with n=784, adding Gaussian white noise with std dev sigma is essentially the same as shifting the image in a random direction by sqrt{784}*sigma - this follows from well-known results in measure concentration. Of course, that is not close to the original images or to the adversarial examples that you found.\r\n\r\nFortunately this is easy to correct however - take the images that you generated adversarial versions of (or a random subsample of them) and simply do monte carlo with Gaussian white noise added to the original image where sigma = Av. distortion of the adversarial version of that image. Then you are searching the sphere where the adversarial image lies and if there aren't many adversaries there, then you won't find them even with a whole bunch of MC trials.\r\n\r\n(Of course, if you do find them then that's even more intriguing - why should small distortions of the images become unclassifiable but large ones not...?)"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Intriguing properties of neural networks", "decision": "submitted, no decision", "abstract": "Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains of the semantic information in the high layers of neural networks. Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extend. Specifically, we find that we can cause the network to misclassify an image by applying a certain imperceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.", "pdf": "https://arxiv.org/abs/1312.6199", "paperhash": "bruna|intriguing_properties_of_neural_networks", "keywords": [], "conflicts": [], "authors": ["Joan Bruna", "Christian Szegedy", "Ilya Sutskever", "Ian Goodfellow", "Wojciech Zaremba", "Rob Fergus", "Dumitru Erhan"], "authorids": ["joan.bruna@gmail.com", "szegedy@google.com", "ilya.at.cs@gmail.com", "goodfellow.ian@gmail.com", "woj.zaremba@gmail.com", "robfergus@gmail.com", "dumitru.erhan@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392686580000, "tcdate": 1392686580000, "number": 2, "id": "2bvxbJp4JZ8qP", "invitation": "ICLR.cc/2014/-/submission/conference/reply", "forum": "kklr_MTHMRQjG", "replyto": "W-bn2AaUnSf6Q", "signatures": ["Joan Bruna"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "I have rewritten section 4.3 trying to put emphasis on the purpose of the analysis and with further explanations. I hope you'll find the new version easier to follow. Thanks."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Intriguing properties of neural networks", "decision": "submitted, no decision", "abstract": "Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains of the semantic information in the high layers of neural networks. Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extend. Specifically, we find that we can cause the network to misclassify an image by applying a certain imperceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.", "pdf": "https://arxiv.org/abs/1312.6199", "paperhash": "bruna|intriguing_properties_of_neural_networks", "keywords": [], "conflicts": [], "authors": ["Joan Bruna", "Christian Szegedy", "Ilya Sutskever", "Ian Goodfellow", "Wojciech Zaremba", "Rob Fergus", "Dumitru Erhan"], "authorids": ["joan.bruna@gmail.com", "szegedy@google.com", "ilya.at.cs@gmail.com", "goodfellow.ian@gmail.com", "woj.zaremba@gmail.com", "robfergus@gmail.com", "dumitru.erhan@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392686520000, "tcdate": 1392686520000, "number": 2, "id": "OOXW6YANWnqfe", "invitation": "ICLR.cc/2014/-/submission/conference/reply", "forum": "kklr_MTHMRQjG", "replyto": "xaMckCfnv4aZ3", "signatures": ["Joan Bruna"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "I shall address here the comments relating to section 4.3.\r\nThe comment 'The sentence ...  and not doing it quite right' is an opinion that we cannot contest factually, but we believe that a first step towards understanding these unstabilities might come from the mathematical analysis of additive stability. \r\nI have rewritten the section trying to simplify the message and with more detailed explanations on what the equations mean. I hope the analysis will become clearer in this new version."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Intriguing properties of neural networks", "decision": "submitted, no decision", "abstract": "Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains of the semantic information in the high layers of neural networks. Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extend. Specifically, we find that we can cause the network to misclassify an image by applying a certain imperceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.", "pdf": "https://arxiv.org/abs/1312.6199", "paperhash": "bruna|intriguing_properties_of_neural_networks", "keywords": [], "conflicts": [], "authors": ["Joan Bruna", "Christian Szegedy", "Ilya Sutskever", "Ian Goodfellow", "Wojciech Zaremba", "Rob Fergus", "Dumitru Erhan"], "authorids": ["joan.bruna@gmail.com", "szegedy@google.com", "ilya.at.cs@gmail.com", "goodfellow.ian@gmail.com", "woj.zaremba@gmail.com", "robfergus@gmail.com", "dumitru.erhan@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392686040000, "tcdate": 1392686040000, "number": 3, "id": "GCrE7EB_-wGeF", "invitation": "ICLR.cc/2014/-/submission/conference/reply", "forum": "kklr_MTHMRQjG", "replyto": "44JXVXNDXf9Kw", "signatures": ["Joan Bruna"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "I shall address here the comments relating to section 4.3. \r\n\r\nI have rewritten the section to make it more accessible, it will be recently available in the arxiv. \r\nI have also included missing notation and fixed a couple of typos. A contractive operator O (linear or nonlinear) has the property that ||Ox - Ox'|| <= ||x-x'|| and hence does not expand the distance between a pair of input points. I clarified this in the text."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Intriguing properties of neural networks", "decision": "submitted, no decision", "abstract": "Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains of the semantic information in the high layers of neural networks. Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extend. Specifically, we find that we can cause the network to misclassify an image by applying a certain imperceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.", "pdf": "https://arxiv.org/abs/1312.6199", "paperhash": "bruna|intriguing_properties_of_neural_networks", "keywords": [], "conflicts": [], "authors": ["Joan Bruna", "Christian Szegedy", "Ilya Sutskever", "Ian Goodfellow", "Wojciech Zaremba", "Rob Fergus", "Dumitru Erhan"], "authorids": ["joan.bruna@gmail.com", "szegedy@google.com", "ilya.at.cs@gmail.com", "goodfellow.ian@gmail.com", "woj.zaremba@gmail.com", "robfergus@gmail.com", "dumitru.erhan@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392277200000, "tcdate": 1392277200000, "number": 1, "id": "yy8AJx-qzi6X4", "invitation": "ICLR.cc/2014/-/submission/conference/reply", "forum": "kklr_MTHMRQjG", "replyto": "SbcPbVLdC8SJZ", "signatures": ["Christian Szegedy"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "Hi Abishek,\r\n\r\nI would reflect to your thoughts on the blind spot part.\r\n\r\nThanks a lot for the exciting insights on the relationship with the edge detectors. \r\n\r\nAccording to your intuition a low-pass filter should remove most of the hardness of the newly generated examples.  A simple enough experiment I will put on top of my todo queue. \r\n\r\nBTW, as I mentioned the 'trainig with adversarial examples' section the adversarial examples seem to be most effective (for regularization) if they are generated for the higher layers. The notion of high-frequency noise is not well defined here. \r\n\r\nIn general, it seems that adversarial examples are a more general phenomenon than just vision. An exciting item of study is trying the for networks other than vision (e.g. speach and maybe language?)"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Intriguing properties of neural networks", "decision": "submitted, no decision", "abstract": "Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains of the semantic information in the high layers of neural networks. Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extend. Specifically, we find that we can cause the network to misclassify an image by applying a certain imperceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.", "pdf": "https://arxiv.org/abs/1312.6199", "paperhash": "bruna|intriguing_properties_of_neural_networks", "keywords": [], "conflicts": [], "authors": ["Joan Bruna", "Christian Szegedy", "Ilya Sutskever", "Ian Goodfellow", "Wojciech Zaremba", "Rob Fergus", "Dumitru Erhan"], "authorids": ["joan.bruna@gmail.com", "szegedy@google.com", "ilya.at.cs@gmail.com", "goodfellow.ian@gmail.com", "woj.zaremba@gmail.com", "robfergus@gmail.com", "dumitru.erhan@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392276660000, "tcdate": 1392276660000, "number": 1, "id": "WV3EWEL7rqWzf", "invitation": "ICLR.cc/2014/-/submission/conference/reply", "forum": "kklr_MTHMRQjG", "replyto": "rryGxGt0NShbP", "signatures": ["Christian Szegedy"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "Thanks a lot!"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Intriguing properties of neural networks", "decision": "submitted, no decision", "abstract": "Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains of the semantic information in the high layers of neural networks. Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extend. Specifically, we find that we can cause the network to misclassify an image by applying a certain imperceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.", "pdf": "https://arxiv.org/abs/1312.6199", "paperhash": "bruna|intriguing_properties_of_neural_networks", "keywords": [], "conflicts": [], "authors": ["Joan Bruna", "Christian Szegedy", "Ilya Sutskever", "Ian Goodfellow", "Wojciech Zaremba", "Rob Fergus", "Dumitru Erhan"], "authorids": ["joan.bruna@gmail.com", "szegedy@google.com", "ilya.at.cs@gmail.com", "goodfellow.ian@gmail.com", "woj.zaremba@gmail.com", "robfergus@gmail.com", "dumitru.erhan@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392276600000, "tcdate": 1392276600000, "number": 2, "id": "wMNY-xV3Ynvm6", "invitation": "ICLR.cc/2014/-/submission/conference/reply", "forum": "kklr_MTHMRQjG", "replyto": "44JXVXNDXf9Kw", "signatures": ["Christian Szegedy"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "Here is a link to the uncompressed original image examples used for the imagenet experiments: \r\n\r\nhttp://goo.gl/huaGPb"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Intriguing properties of neural networks", "decision": "submitted, no decision", "abstract": "Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains of the semantic information in the high layers of neural networks. Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extend. Specifically, we find that we can cause the network to misclassify an image by applying a certain imperceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.", "pdf": "https://arxiv.org/abs/1312.6199", "paperhash": "bruna|intriguing_properties_of_neural_networks", "keywords": [], "conflicts": [], "authors": ["Joan Bruna", "Christian Szegedy", "Ilya Sutskever", "Ian Goodfellow", "Wojciech Zaremba", "Rob Fergus", "Dumitru Erhan"], "authorids": ["joan.bruna@gmail.com", "szegedy@google.com", "ilya.at.cs@gmail.com", "goodfellow.ian@gmail.com", "woj.zaremba@gmail.com", "robfergus@gmail.com", "dumitru.erhan@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392276180000, "tcdate": 1392276180000, "number": 1, "id": "ngUW3c5usIgTl", "invitation": "ICLR.cc/2014/-/submission/conference/reply", "forum": "kklr_MTHMRQjG", "replyto": "W-bn2AaUnSf6Q", "signatures": ["Christian Szegedy"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "You get the same qualitative effect if the number of classes is small.\r\n\r\nThe car example in the paper is performed with a binary classifier. Similar results were obtained on networks other than that of Quoc.\r\n\r\nIt would be interesting to study the quantitative dependence of the necessary distortion on the number of classes.\r\n\r\nContractive autoencoders were suggested by Yoshua Bengio in 2012, but I have not tried them. [Personal communication]\r\n\r\nIt is an interesting observation on the dependence of depth and necessary distortion. In general I would caution to try generalize such a limited set of examples. There was no additional regularization for the deeper model besides the autoencoder. My personal experiment log contains a somewhat larger selection of experiments I will put on my public page as appendix to the paper.  Your statement seems to continue to hold there. Not sure about  convolutional and other types of networks.\r\nI have a few uneducated guesses why it might be the case in general, but probably my guesses as good as anybodies."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Intriguing properties of neural networks", "decision": "submitted, no decision", "abstract": "Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains of the semantic information in the high layers of neural networks. Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extend. Specifically, we find that we can cause the network to misclassify an image by applying a certain imperceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.", "pdf": "https://arxiv.org/abs/1312.6199", "paperhash": "bruna|intriguing_properties_of_neural_networks", "keywords": [], "conflicts": [], "authors": ["Joan Bruna", "Christian Szegedy", "Ilya Sutskever", "Ian Goodfellow", "Wojciech Zaremba", "Rob Fergus", "Dumitru Erhan"], "authorids": ["joan.bruna@gmail.com", "szegedy@google.com", "ilya.at.cs@gmail.com", "goodfellow.ian@gmail.com", "woj.zaremba@gmail.com", "robfergus@gmail.com", "dumitru.erhan@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392271740000, "tcdate": 1392271740000, "number": 9, "id": "DGcr4rYqdQDI-", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "kklr_MTHMRQjG", "replyto": "kklr_MTHMRQjG", "signatures": ["Christian Szegedy"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "All the examples shown are strictly random selections. They are not preselected in any ways. I will updated the paper to reflect this fact."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Intriguing properties of neural networks", "decision": "submitted, no decision", "abstract": "Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains of the semantic information in the high layers of neural networks. Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extend. Specifically, we find that we can cause the network to misclassify an image by applying a certain imperceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.", "pdf": "https://arxiv.org/abs/1312.6199", "paperhash": "bruna|intriguing_properties_of_neural_networks", "keywords": [], "conflicts": [], "authors": ["Joan Bruna", "Christian Szegedy", "Ilya Sutskever", "Ian Goodfellow", "Wojciech Zaremba", "Rob Fergus", "Dumitru Erhan"], "authorids": ["joan.bruna@gmail.com", "szegedy@google.com", "ilya.at.cs@gmail.com", "goodfellow.ian@gmail.com", "woj.zaremba@gmail.com", "robfergus@gmail.com", "dumitru.erhan@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392269280000, "tcdate": 1392269280000, "number": 1, "id": "Wuj8W8v0yPuUg", "invitation": "ICLR.cc/2014/-/submission/conference/reply", "forum": "kklr_MTHMRQjG", "replyto": "QhdJh-ekJVsba", "signatures": ["Christian Szegedy"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "Thanks a lot!\r\n\r\nI will upload an updated paper soon. The 1.2% result used weight decay, but no dropout. (update also added in the paper.)"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Intriguing properties of neural networks", "decision": "submitted, no decision", "abstract": "Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains of the semantic information in the high layers of neural networks. Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extend. Specifically, we find that we can cause the network to misclassify an image by applying a certain imperceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.", "pdf": "https://arxiv.org/abs/1312.6199", "paperhash": "bruna|intriguing_properties_of_neural_networks", "keywords": [], "conflicts": [], "authors": ["Joan Bruna", "Christian Szegedy", "Ilya Sutskever", "Ian Goodfellow", "Wojciech Zaremba", "Rob Fergus", "Dumitru Erhan"], "authorids": ["joan.bruna@gmail.com", "szegedy@google.com", "ilya.at.cs@gmail.com", "goodfellow.ian@gmail.com", "woj.zaremba@gmail.com", "robfergus@gmail.com", "dumitru.erhan@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392267180000, "tcdate": 1392267180000, "number": 1, "id": "22Gh2jLthJ7Bq", "invitation": "ICLR.cc/2014/-/submission/conference/reply", "forum": "kklr_MTHMRQjG", "replyto": "xaMckCfnv4aZ3", "signatures": ["Christian Szegedy"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "First of all, thanks a lot for the thorough reading of the paper and the lof of detailed helpful suggestions. \r\n\r\nVery soon I will update the paper to incorporate most of the suggestion.\r\n\r\nI agree with a lot of the criticisms and will make changes to improve our exposion, I would just like to dispute some of the terminogical points regarding the use of the word 'experiment'.\r\n\r\nI agree that the 'visually indistinguishable' part is not verified by a scientific double-blind experiment. However, what we present here is not some subtle statistical phenomenon, but an exceptionally clear effect that works in >99% of  the cases. \r\n\r\nYes, it is an informal statement. However, so far nobody who bothered to examine the output of our counter-exemple generator for his/her network felt that there is a need for a clinical trial to verify this statement on human subjects. ;)\r\n\r\nI think it is justified to call the other statistical experiments as such. Yes, they are not medical experiments. The term here refers to mathematical experiments. It is common in the domain of computer vision to  demonstrate a method by running computer experiments in controlled setting. Here we try to support cross-model and cross-data set generalization of the adversarial examples. Given the number of examples generated, their error on the randomly chosen data sets, we think that you could agree that the measured error-rates can't be attributed to random chance.\r\n\r\nIn general, the word 'experiment' refers to the whole process of creating the counterexample generator, running it with different networks and observe its effect. Which was a well defined, scientific process designed to refute or support certain hypotheses we had come up for the source of the observed qualitative effects.\r\n\r\nThere are a lot of open questions regarding the explanation of the observed effects and I don't want to pretend to understand the exact underlying reasons. I must have to wait for my coauthor Wojciech for clarifications or changes to the theoretical section."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Intriguing properties of neural networks", "decision": "submitted, no decision", "abstract": "Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains of the semantic information in the high layers of neural networks. Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extend. Specifically, we find that we can cause the network to misclassify an image by applying a certain imperceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.", "pdf": "https://arxiv.org/abs/1312.6199", "paperhash": "bruna|intriguing_properties_of_neural_networks", "keywords": [], "conflicts": [], "authors": ["Joan Bruna", "Christian Szegedy", "Ilya Sutskever", "Ian Goodfellow", "Wojciech Zaremba", "Rob Fergus", "Dumitru Erhan"], "authorids": ["joan.bruna@gmail.com", "szegedy@google.com", "ilya.at.cs@gmail.com", "goodfellow.ian@gmail.com", "woj.zaremba@gmail.com", "robfergus@gmail.com", "dumitru.erhan@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392264240000, "tcdate": 1392264240000, "number": 1, "id": "96ke6XdvXB9V7", "invitation": "ICLR.cc/2014/-/submission/conference/reply", "forum": "kklr_MTHMRQjG", "replyto": "44JXVXNDXf9Kw", "signatures": ["Christian Szegedy"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "In 4.1 D(x, l) is is definied by the sentence: 'we denote one such x+r for an arbitrary minimizer by D(x, l)'\r\n\r\nI've added some explanation of the minimization approach. (penalty function method).  In fact we aim for the optimal solution of the hard-constrained version which we could find for convex losses, however the neural networks are non-convex, so our adversarial examples might end up to be non-minimal in theory. \r\n\r\nI am about to upload an updated version of the paper fixing the issues you raised.\r\n\r\nI leave the defense of section 4.3 to Wojciech."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Intriguing properties of neural networks", "decision": "submitted, no decision", "abstract": "Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains of the semantic information in the high layers of neural networks. Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extend. Specifically, we find that we can cause the network to misclassify an image by applying a certain imperceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.", "pdf": "https://arxiv.org/abs/1312.6199", "paperhash": "bruna|intriguing_properties_of_neural_networks", "keywords": [], "conflicts": [], "authors": ["Joan Bruna", "Christian Szegedy", "Ilya Sutskever", "Ian Goodfellow", "Wojciech Zaremba", "Rob Fergus", "Dumitru Erhan"], "authorids": ["joan.bruna@gmail.com", "szegedy@google.com", "ilya.at.cs@gmail.com", "goodfellow.ian@gmail.com", "woj.zaremba@gmail.com", "robfergus@gmail.com", "dumitru.erhan@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391906520000, "tcdate": 1391906520000, "number": 8, "id": "hN0KTJ7oaBI5j", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "kklr_MTHMRQjG", "replyto": "kklr_MTHMRQjG", "signatures": ["anonymous reviewer 9edb"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Intriguing properties of neural networks", "review": "The paper highlights two counterintuitive properties of neural networks: (1) a tendency to encode factors of variation in multiple units rather than single units, and (2) an absence of local generalization. The first hypothesis is tested by finding a set of samples correlating most with certain directions in the feature space and visually inspecting their similarity. The second hypothesis is tested by generating 'adversarial samples' that are close neighbors to the data points but predicted of different class.\r\n\r\nI think that the first property makes a lot of sense in the context of sigmoidal networks, as the squashing effect of the sigmoid clearly encourages the representation to be redundantly encoded in a large number of neurons in order to preserve the principal components of the data. However, I am wondering whether these findings apply broadly or whether some modeling decision (e.g. sparsity, type of nonlinearity, competition between units, etc) do favor the alignment of independent components to the canonical coordinates.\r\n\r\nThe second property is very interesting and shows that imperceptible deviations from a data point in the input space can cause the neural network to change its prediction. This finding is likely to generate further research on the nature of this effect. In particular, I would find it interesting to study whether these adversarial samples are a degenerate effect caused by the limited capacity of the network or a more fundamental issue with the learning algorithm that would occur for networks of any size.\r\n\r\nAuthors provide several examples of adversarial samples for both the MNIST and the image datasets. However, I could not find whether these samples are typical (with distortions corresponding to the average minimum distortion) or whether they have been preselected to have low distortion."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Intriguing properties of neural networks", "decision": "submitted, no decision", "abstract": "Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains of the semantic information in the high layers of neural networks. Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extend. Specifically, we find that we can cause the network to misclassify an image by applying a certain imperceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.", "pdf": "https://arxiv.org/abs/1312.6199", "paperhash": "bruna|intriguing_properties_of_neural_networks", "keywords": [], "conflicts": [], "authors": ["Joan Bruna", "Christian Szegedy", "Ilya Sutskever", "Ian Goodfellow", "Wojciech Zaremba", "Rob Fergus", "Dumitru Erhan"], "authorids": ["joan.bruna@gmail.com", "szegedy@google.com", "ilya.at.cs@gmail.com", "goodfellow.ian@gmail.com", "woj.zaremba@gmail.com", "robfergus@gmail.com", "dumitru.erhan@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391858040000, "tcdate": 1391858040000, "number": 7, "id": "W-bn2AaUnSf6Q", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "kklr_MTHMRQjG", "replyto": "kklr_MTHMRQjG", "signatures": ["anonymous reviewer e8df"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Intriguing properties of neural networks", "review": "This paper studies the interesting case of showing where deep nets fail. It proposes a simple mechanism to demonstrate that even small (visually almost imperceptible distortion) to the training samples can cause drastic changes in recognition (ie, decision boundaries). \r\n\r\nThis is a fresh perspective and I applaud the authors for providing interesting diagnostic means to help to understand deep nets.\r\n\r\nA few comments:\r\n\r\n(1) the paper actually does not state how to address the issue of deep nets learning very rugged decision boundaries:  a few ideas come into mind, though. Using drop-out in training process might eliminate this problem? If # of classes is small, would this problem be less severe? (For example, the required distortion is large so one can detect ).   \r\n\r\n(2) I could not follow the analysis in section 4.3 -- not clear where it leads to\r\n\r\n(3) Looking at Table 1/2: I notice that the required distortion is larger for more complex models. So perhaps this model needs to be better regularized?  Have you tried contractive autoencoders?"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Intriguing properties of neural networks", "decision": "submitted, no decision", "abstract": "Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains of the semantic information in the high layers of neural networks. Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extend. Specifically, we find that we can cause the network to misclassify an image by applying a certain imperceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.", "pdf": "https://arxiv.org/abs/1312.6199", "paperhash": "bruna|intriguing_properties_of_neural_networks", "keywords": [], "conflicts": [], "authors": ["Joan Bruna", "Christian Szegedy", "Ilya Sutskever", "Ian Goodfellow", "Wojciech Zaremba", "Rob Fergus", "Dumitru Erhan"], "authorids": ["joan.bruna@gmail.com", "szegedy@google.com", "ilya.at.cs@gmail.com", "goodfellow.ian@gmail.com", "woj.zaremba@gmail.com", "robfergus@gmail.com", "dumitru.erhan@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1390512780000, "tcdate": 1390512780000, "number": 6, "id": "SbcPbVLdC8SJZ", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "kklr_MTHMRQjG", "replyto": "kklr_MTHMRQjG", "signatures": ["abhishek sharma"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "It is a nicely written and interesting find, but I think some of the unexplained anomalies and deviations from intuition about the behavior of a deep neural network discussed in the paper have some intuitive justification. For example - \r\n'Moreover, it puts into question the notion that neural networks disentangle variation factors across coordinates.' I think this statement is not fully justified by the observation that even a random direction in the final layers output space can lead to some sort of semantic clustering of the inputs. For instance, lets plot a thousand people on weight vs height 2D graph. Not all, but a lot of straight lines emanating from the origin will have a clearly perceivable semantic meaning associated with them to a human, like a line with very little slope will have super skinny people, line with big slope will have fat people, something in between we will have fit people and so on. I am not saying that height and weight are totally independent but are fairly untangled features. This simply analogy sort of hints towards an intuitive explaination of observance of semantic features when using random directions in higher layer output space because this higher layers have a very rich bag of weakly untangled variations and their random combination can still provide us with strongly perceivable semantic features, because humans are really good at finding patterns in a collection of object.\r\nRegarding the blind-spot observation, I think the paper sort of played around the well-known insensitivity of human visual system towards high-frequency structured noise in images. Whereas, for machine vision, high-frequency signals play a vital role, therefore, it is easy to make a drastic change for the machine while keeping it almost imperceptible for the humans. The other way round of this phenomenon ie drastic change to the human eye but no change to the machine is already in use as the basis of a very popular edge extractor, 'Edge detection with embedded confidence' by Peter Meer. And such behaviour is sort of expected from a deep network because the final layer neurons when projected back to the visual input nodes using Deconvolution networks are mostly edge-profiles of the objects and thus making small adversarial changes to them is expected to alter the final classification output. \r\nThe use of this phenomenon in training is interesting, but an evaluation of the incorrect classifications on some datasets like Image-net to see if a small imperceptible change to the wrongly classified images can indeed alter the final label to the correct label of the image, will possibly be a way to improve the results of the network even further.\r\nThe comments are general observations and my own thoughts after reading the paper and not intended towards any sort of review or feedback, scholarly or otherwise."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Intriguing properties of neural networks", "decision": "submitted, no decision", "abstract": "Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains of the semantic information in the high layers of neural networks. Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extend. Specifically, we find that we can cause the network to misclassify an image by applying a certain imperceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.", "pdf": "https://arxiv.org/abs/1312.6199", "paperhash": "bruna|intriguing_properties_of_neural_networks", "keywords": [], "conflicts": [], "authors": ["Joan Bruna", "Christian Szegedy", "Ilya Sutskever", "Ian Goodfellow", "Wojciech Zaremba", "Rob Fergus", "Dumitru Erhan"], "authorids": ["joan.bruna@gmail.com", "szegedy@google.com", "ilya.at.cs@gmail.com", "goodfellow.ian@gmail.com", "woj.zaremba@gmail.com", "robfergus@gmail.com", "dumitru.erhan@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1389991740000, "tcdate": 1389991740000, "number": 2, "id": "QhdJh-ekJVsba", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "kklr_MTHMRQjG", "replyto": "kklr_MTHMRQjG", "signatures": ["Rodrigo Benenson"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "Keeping up the 'let us catch typos' game, I point out:\r\n\r\n- 'we perfomed' -> 'we performed'\r\n\r\nOn a higher level I have little to add to reviewer 887c, I have the same overall impression of very interesting discovery, but immature insights around it.\r\n\r\nThree improvements I might suggest are:\r\n\r\n- 'A subtle, but essential detail is'; the description that follows is unclear to me. If this is so essential, please describe in more detail.\r\n\r\n- The experiments regarding 'using adversarial examples during training' are somewhat unclear. Is 1.2% obtained using weight decay and dropout, plus the adversarial examples ?\r\n\r\n- The paragraph 'For space considerations, ...' is too long and hard to read. Some reformatting would be most welcome.\r\n\r\nLooking forward to see where this research direction leads us !"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Intriguing properties of neural networks", "decision": "submitted, no decision", "abstract": "Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains of the semantic information in the high layers of neural networks. Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extend. Specifically, we find that we can cause the network to misclassify an image by applying a certain imperceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.", "pdf": "https://arxiv.org/abs/1312.6199", "paperhash": "bruna|intriguing_properties_of_neural_networks", "keywords": [], "conflicts": [], "authors": ["Joan Bruna", "Christian Szegedy", "Ilya Sutskever", "Ian Goodfellow", "Wojciech Zaremba", "Rob Fergus", "Dumitru Erhan"], "authorids": ["joan.bruna@gmail.com", "szegedy@google.com", "ilya.at.cs@gmail.com", "goodfellow.ian@gmail.com", "woj.zaremba@gmail.com", "robfergus@gmail.com", "dumitru.erhan@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1389838020000, "tcdate": 1389838020000, "number": 5, "id": "xaMckCfnv4aZ3", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "kklr_MTHMRQjG", "replyto": "kklr_MTHMRQjG", "signatures": ["anonymous reviewer 887c"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Intriguing properties of neural networks", "review": "The paper presents some empirical analysis of various types of neural network\r\nwith regard to finding examples close to the training examples which are\r\nmisclassified.  It also proposes a technique to train on these perturbed\r\nexamples, and they mention some improved results on MNIST (it would be nice\r\nto see more focus on this method, rather than so much on the analysis).\r\n\r\nNovelty: it's very novel, I'd say.\r\nQuality: so-so.  The experimental justification of the training method is quite\r\n   limited, and there are some flaws in the analysis in my opinion.\r\n\r\nPro: The paper has an original idea.\r\nCon: There are various weaknesses in the analysis and the presentation.\r\n  The paper gives the impression of being a little half-baked.  But I think it's\r\n  interesting enough to publish; the issues could probably be fixed without too\r\n  much trouble.\r\n\r\n\r\n\r\nDetailed comments are below.\r\n\r\n------\r\ntypo in abstract:\r\ncontains of->contains\r\nextend->extent\r\n\r\nWhen you say 'Our experiments show that ... properties.'...  I don't think what\r\nyou are talking about is really a scientific experiment.  It's something\r\nanecdotal and informal.  A proper experiment would have human test subjects\r\njudging the extent of semantic similarities of sets of images, where the tests\r\nhad been chosen using different methods and the test subjects didn't know which\r\nmethod the set of images had been chosen from.  Until you do this and show\r\nstatistical analysis of some kind, you can't really say anything for sure.\r\nOf course, you can still report the anecdotal observations, just don't say they\r\nare experiments.\r\n\r\nWhen you say 'this can never occur with smooth classifiers by their\r\ndefinition'...  I think your definition must be problematic.  There will always\r\nbe a classification boundary, and it will always be possible to find images\r\nthat are close to that boundary.  The only question is how easy is it to find\r\nsuch images.  Unless you quantify that somehow, and compare across different\r\nmodeling strategies (e.g. the average distance you have to go to find\r\nthe negative example), I don't think you can say very much.\r\n\r\nFig. 5: I think you can safely assume the reader knows what an ostrich is.\r\n\r\nYou might want to check that you don't have your description of hard negatives\r\nbackwards (I'm not a computer vision expert, but something seems wrong).\r\n\r\ndisclassified->misclassified\r\n\r\nThe sentence 'The instability is expressed mathematically as...' is to me quite\r\nproblematic.  I think you are trying to formalize something that is really a little\r\nvague, and not doing it quite right.\r\nThe rest of the spectral analysis is interesting though, but with the stuff\r\nregarding A(omega), I think it would be more useful if you tried to explain in\r\nwords what is happening rather than force the reader to wade through notation.\r\n\r\nI don't think the comparison to the set of rational numbers is useful, given\r\nthat we know the last hidden layer is a continuous function of the input\r\n(and thus the sets we're dealing with are much more well-behaved than the\r\nset of rational numbers).\r\n\r\nOverall, some of the analysis and explanation in the paper is a little problematic,\r\nand sometimes over-stated, but the basic idea is I think quite interesting, and I think\r\nyou should emphasize more your training method, which in my mind is probably more\r\ninteresting than the analysis itself."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Intriguing properties of neural networks", "decision": "submitted, no decision", "abstract": "Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains of the semantic information in the high layers of neural networks. Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extend. Specifically, we find that we can cause the network to misclassify an image by applying a certain imperceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.", "pdf": "https://arxiv.org/abs/1312.6199", "paperhash": "bruna|intriguing_properties_of_neural_networks", "keywords": [], "conflicts": [], "authors": ["Joan Bruna", "Christian Szegedy", "Ilya Sutskever", "Ian Goodfellow", "Wojciech Zaremba", "Rob Fergus", "Dumitru Erhan"], "authorids": ["joan.bruna@gmail.com", "szegedy@google.com", "ilya.at.cs@gmail.com", "goodfellow.ian@gmail.com", "woj.zaremba@gmail.com", "robfergus@gmail.com", "dumitru.erhan@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1388839260000, "tcdate": 1388839260000, "number": 4, "id": "44JXVXNDXf9Kw", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "kklr_MTHMRQjG", "replyto": "kklr_MTHMRQjG", "signatures": ["David Krueger"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "Having read the paper, I have some more comments.  I can only speak to my own experience, of course, and may be plain wrong about a few things.  Many of my comments are minor edits.  \r\n\r\nFirst, it seems to me that x_eps, r, and n_i are all used variously to refer to the 'adversarial shift' (i.e. middle columns of 5a,b).  I don't see any reason not to standardize.\r\n\r\nIn the second paragraph of section 1, I would remove 'Moreover'.  In the same paragraph, I did not know what 'up to a rotation of the space' meant.\r\n\r\n2nd to last paragraph of section 1: hyperparemeter -> hyperparameter\r\n\r\nIn section 3, e_i is undefined.  Also, I would use different names for the different definitions of x'.  I think the sentence before the second definition is not a proper sentence, as well.  And there is an extraneous in ('random basis in for') in the sentence after.  Some quotes point the wrong way as well.\r\n\r\nSection 4, paragraph 2: 'In other words...' I would add 'according to [2]' or something else that clarifies that this is the hypothesis represented in that paper, and not an assertion you are making.\r\n\r\nparagraph 4:  I think the statement 'this can never occur with smooth classifiers by their definition' is too strong.  While the perturbations are imperceptible, they are not arbitrarily small.  I think this statement would only be justified if you could always change the classification output by changing the intensity of one pixel by the minimum amount possible given your discretization.\r\n\r\nparagraph 5:  'finding adversarial' -> 'find adversarial'\r\n\r\nFigure 5: I think at least on larger example like this would be nice.  The perturbations might be more perceptible with larger images.\r\n\r\nparagraph before 4.1:  I'm not sure what it means 'the training set is then changed...' in general (eg paragraph before 4.3), I was not clear on what dataset (training/test/etc.) was used for what part of the procedure.  \r\n\r\nIn 4.1, I don't think D(x,l) was ever defined.  \r\n\r\nBased on the last line of 4.1, it appears that your optimization problem uses a soft constraint on the misclassification.  The write-up suggests that you are forcing a misclassification (hard constraint).  \r\n\r\nbottom of page 6: 'The columns of the table show the error (ratio of disclassified' -> '...(proportion of misclassified'.   I'm not sure what it means: 'The last two rows are special'.  \r\n\r\nTable 3 caption: what does it mean 'trained to study'?  \r\n\r\nSection 4.3 I found confusing in general.\r\nI don't know what 'max(0,x) is contractive' means.  I would also specify the norm that you use for W_k in the next line (I assume operator norm).  \r\n\r\nI found the equation for Wx hard to parse and I don't think w_{c,f} is defined.  There are missing commas before and after the ...'s.  \r\n\r\n'x_c denoted the c input feature image' -> 'x_c denoted the c-th input feature image' ?\r\n\r\nFinally, In the discussion:\r\n'the set of adversarial negatives is of extremely low probabilities' - did you show this?  \r\n'...yet it is dense' - this I don't think makes sense mathematically in a discrete space.  I would not use this terminology unless you can prove it using the formal definition."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Intriguing properties of neural networks", "decision": "submitted, no decision", "abstract": "Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains of the semantic information in the high layers of neural networks. Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extend. Specifically, we find that we can cause the network to misclassify an image by applying a certain imperceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.", "pdf": "https://arxiv.org/abs/1312.6199", "paperhash": "bruna|intriguing_properties_of_neural_networks", "keywords": [], "conflicts": [], "authors": ["Joan Bruna", "Christian Szegedy", "Ilya Sutskever", "Ian Goodfellow", "Wojciech Zaremba", "Rob Fergus", "Dumitru Erhan"], "authorids": ["joan.bruna@gmail.com", "szegedy@google.com", "ilya.at.cs@gmail.com", "goodfellow.ian@gmail.com", "woj.zaremba@gmail.com", "robfergus@gmail.com", "dumitru.erhan@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1388207280000, "tcdate": 1388207280000, "number": 1, "id": "rryGxGt0NShbP", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "kklr_MTHMRQjG", "replyto": "kklr_MTHMRQjG", "signatures": ["Sam Bowman"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "Very interesting stuff! While I try to think of something intelligent to say, here are a couple more typos from the running text:\r\n\r\n'...e [remove many] s...'\r\n\r\n '...AlexNet [9].(Left) is correctly...'"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Intriguing properties of neural networks", "decision": "submitted, no decision", "abstract": "Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains of the semantic information in the high layers of neural networks. Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extend. Specifically, we find that we can cause the network to misclassify an image by applying a certain imperceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.", "pdf": "https://arxiv.org/abs/1312.6199", "paperhash": "bruna|intriguing_properties_of_neural_networks", "keywords": [], "conflicts": [], "authors": ["Joan Bruna", "Christian Szegedy", "Ilya Sutskever", "Ian Goodfellow", "Wojciech Zaremba", "Rob Fergus", "Dumitru Erhan"], "authorids": ["joan.bruna@gmail.com", "szegedy@google.com", "ilya.at.cs@gmail.com", "goodfellow.ian@gmail.com", "woj.zaremba@gmail.com", "robfergus@gmail.com", "dumitru.erhan@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1388047980000, "tcdate": 1388047980000, "number": 3, "id": "TTO717UY7e1EM", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "kklr_MTHMRQjG", "replyto": "kklr_MTHMRQjG", "signatures": ["David Krueger"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "This paper appears 7 times on the list of papers.  \r\n\r\n 'It suggests that it is the space, rather than the individual units, that contains of' \r\nshould be \r\n'...contains'\r\n\r\n'Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extend'\r\nshould be \r\n'...extent'\r\nalso, 'fairly discontinuous to a significant extent' sounds redundant to me.\r\n\r\nLooking forward to reading it!"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Intriguing properties of neural networks", "decision": "submitted, no decision", "abstract": "Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains of the semantic information in the high layers of neural networks. Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extend. Specifically, we find that we can cause the network to misclassify an image by applying a certain imperceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.", "pdf": "https://arxiv.org/abs/1312.6199", "paperhash": "bruna|intriguing_properties_of_neural_networks", "keywords": [], "conflicts": [], "authors": ["Joan Bruna", "Christian Szegedy", "Ilya Sutskever", "Ian Goodfellow", "Wojciech Zaremba", "Rob Fergus", "Dumitru Erhan"], "authorids": ["joan.bruna@gmail.com", "szegedy@google.com", "ilya.at.cs@gmail.com", "goodfellow.ian@gmail.com", "woj.zaremba@gmail.com", "robfergus@gmail.com", "dumitru.erhan@gmail.com"]}, "tags": [], "invitation": {}}}, {"replyto": null, "ddate": null, "legacy_migration": true, "tmdate": 1387874040000, "tcdate": 1387874040000, "number": 48, "id": "kklr_MTHMRQjG", "invitation": "ICLR.cc/2014/conference/-/submission", "forum": "kklr_MTHMRQjG", "signatures": ["joan.bruna@gmail.com"], "readers": ["everyone"], "content": {"title": "Intriguing properties of neural networks", "decision": "submitted, no decision", "abstract": "Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains of the semantic information in the high layers of neural networks. Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extend. Specifically, we find that we can cause the network to misclassify an image by applying a certain imperceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.", "pdf": "https://arxiv.org/abs/1312.6199", "paperhash": "bruna|intriguing_properties_of_neural_networks", "keywords": [], "conflicts": [], "authors": ["Joan Bruna", "Christian Szegedy", "Ilya Sutskever", "Ian Goodfellow", "Wojciech Zaremba", "Rob Fergus", "Dumitru Erhan"], "authorids": ["joan.bruna@gmail.com", "szegedy@google.com", "ilya.at.cs@gmail.com", "goodfellow.ian@gmail.com", "woj.zaremba@gmail.com", "robfergus@gmail.com", "dumitru.erhan@gmail.com"]}, "writers": [], "details": {"replyCount": 21, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1369422751717, "tmdate": 1496674357195, "id": "ICLR.cc/2014/conference/-/submission", "writers": ["ICLR.cc/2014"], "signatures": ["OpenReview.net"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": []}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1377198751717, "cdate": 1496674357195}}}], "count": 22}