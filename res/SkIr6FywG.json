{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1528124443383, "tcdate": 1518472509754, "number": 325, "cdate": 1518472509754, "id": "SkIr6FywG", "invitation": "ICLR.cc/2018/Workshop/-/Submission", "forum": "SkIr6FywG", "signatures": ["~Ekaterina_Lobacheva1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop"], "content": {"title": "Adaptive prediction time for sequence classification", "abstract": "In this paper, we propose a recurrent neural network architecture for early sequence classification, when the model is required to output a label as soon as possible with negligible decline in accuracy. Our model is capable of learning how many sequence tokens it needs to observe in order to make a prediction; moreover, the number of steps required differs for each sequence. Experiments on sequential MNIST show that the proposed architecture focuses on different sequence parts during inference, which correspond to contours of the handwritten digits. We also demonstrate the improvement in the prediction quality with a simultaneous reduction in the prefix size used, the extent of which depends on the distribution of distinct class features over time.", "paperhash": "ryabinin|adaptive_prediction_time_for_sequence_classification", "keywords": ["Recurrent neural networks", "Adaptive computational time", "Early sequence classification"], "_bibtex": "@misc{\n  ryabinin2018adaptive,\n  title={Adaptive prediction time for sequence classification},\n  author={Maksim Ryabinin and Ekaterina Lobacheva},\n  year={2018},\n  url={https://openreview.net/forum?id=SkIr6FywG}\n}", "authorids": ["mkryabinin@edu.hse.ru", "elobacheva@hse.ru"], "authors": ["Maksim Ryabinin", "Ekaterina Lobacheva"], "TL;DR": "We propose a recurrent model for early sequence classification based on the idea of Adaptive computational time.", "pdf": "/pdf/2820cd04e3b87a99dd380b683d84e6e7a4a5cb28.pdf"}, "nonreaders": [], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1518472800000, "tmdate": 1518474081690, "id": "ICLR.cc/2018/Workshop/-/Submission", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Workshop"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2018/Workshop", "description": "Your authorized identity to be associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 9, "value-regex": "upload", "description": "Upload a PDF file that ends with .pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 8, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names. Please provide real names; identities will be anonymized."}, "keywords": {"order": 6, "values-regex": "(^$)|[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of keywords."}, "TL;DR": {"required": false, "order": 7, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,500}"}, "authorids": {"required": true, "order": 3, "values-regex": "([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,},){0,}([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,})", "description": "Comma separated list of author email addresses, lowercased, in the same order as above. For authors with existing OpenReview accounts, please make sure that the provided email address(es) match those listed in the author's profile. Please provide real emails; identities will be anonymized."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1526248800000, "cdate": 1518474081690}}}, {"tddate": null, "ddate": null, "tmdate": 1525969969609, "tcdate": 1525969969609, "number": 3, "cdate": 1525969969609, "id": "BJqNNefRG", "invitation": "ICLR.cc/2018/Workshop/-/Paper325/Official_Comment", "forum": "SkIr6FywG", "replyto": "SyQuqrh_G", "signatures": ["ICLR.cc/2018/Workshop/Paper325/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper325/Authors"], "content": {"title": "Author's response ", "comment": "We would like to thank you for your thorough review and address some of the concerns in detail below. \n\n\u00abThe choice of calling h_t a 'halting probability' is somewhat confusing. At no point is the model stopped by sampling from a h_t Bernouilli\u00bb\nYou are right about the name for h_t being rather misleading for the readers. However, as our setup is inspired by Graves (2016) paper, we tried to use the same notation to describe the model, but it is probably a good idea to add additional clarification to the paper.\n\n\u00abFirst, there is no guarantee that p_t sum to one: what happens e.g. id \\epsilon = 0.2, h_1 = 0.75 and h_2 = 0.75 ? then, according to equations (1)-(3), we have p_1 + p_2 = 1.5\u00bb\nSum of all p_t is equal to one, because the last p_t is always set to complement other probabilities to 1. If h_1=0.75 and h_2=0.75, then h_1+h_2>=1-\\epsilon, then N=2 and p_2=0.25, so we get p_1+p_2=1 by construction.\n\n\u00abSecondly, R is not a function of the sequence length, and so I am not sure I understand how \\lambda trades off between accuracy and computation.\u00bb\nThank you for this observation, the explanation was indeed needed to be more thorough. The idea is that gradients of R with respect to all halting probabilities except the last equal to -1; thus by minimizing R we increase these halting probabilities and because of this the overall sum exceeds 1-\\epsilon at an earlier step. The initial motivation for optimizing R comes from introducing ponder cost as the penalty term which is calculated as R+N; however, because N is piecewise constant, its gradients are discarded for the optimization process.\n\n\u00abIn the Experiments section, the authors need to give more details about how the number of steps is counted: I assume that it is the average value of N as defined in Equation (N), but it is not absolutely clear\u00bb\nAs with the previous question, we will extend the description in the next version of the paper. The number of steps is calculated as the number of RNN state updates for each sequence, averaged over the test set. This equals to the average value of N for our model, the average sequence length for GRU baseline, and the average number of steps for which the state update gate was set to 1 for Skip GRU model.\n\n\u00abFinally, claiming an improvement over Skip-RNN on the basis of a reduction in the prefix length on Reordered SeqMNIST does not make sense\u2026this is only possible because the specific reordering gives them access to important information from the future earlier\u00bb\nWe agree that the comparison between our model and Skip-RNN on the reordered sequences is unfair, as the reordered sequences have several hundred black pixels in the end, making the network forget the initial information even when the updates are rare. However, the goal of this experiment was not to show an improvement in classification quality or sequence length in comparison with other models but to show that it is possible for our network to significantly reduce the length of the prefix used depending on the location of discriminative information in the sequence. In other words, this was just a toy example to prove that our model is indeed capable of predicting much earlier than reaching the end of the sequence."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive prediction time for sequence classification", "abstract": "In this paper, we propose a recurrent neural network architecture for early sequence classification, when the model is required to output a label as soon as possible with negligible decline in accuracy. Our model is capable of learning how many sequence tokens it needs to observe in order to make a prediction; moreover, the number of steps required differs for each sequence. Experiments on sequential MNIST show that the proposed architecture focuses on different sequence parts during inference, which correspond to contours of the handwritten digits. We also demonstrate the improvement in the prediction quality with a simultaneous reduction in the prefix size used, the extent of which depends on the distribution of distinct class features over time.", "paperhash": "ryabinin|adaptive_prediction_time_for_sequence_classification", "keywords": ["Recurrent neural networks", "Adaptive computational time", "Early sequence classification"], "_bibtex": "@misc{\n  ryabinin2018adaptive,\n  title={Adaptive prediction time for sequence classification},\n  author={Maksim Ryabinin and Ekaterina Lobacheva},\n  year={2018},\n  url={https://openreview.net/forum?id=SkIr6FywG}\n}", "authorids": ["mkryabinin@edu.hse.ru", "elobacheva@hse.ru"], "authors": ["Maksim Ryabinin", "Ekaterina Lobacheva"], "TL;DR": "We propose a recurrent model for early sequence classification based on the idea of Adaptive computational time.", "pdf": "/pdf/2820cd04e3b87a99dd380b683d84e6e7a4a5cb28.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1519222445613, "id": "ICLR.cc/2018/Workshop/-/Paper325/Official_Comment", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "SkIr6FywG", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper325/AnonReviewer[0-9]+|ICLR.cc/2018/Workshop/Paper325/Authors|ICLR.cc/2018/Workshop/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper325/AnonReviewer[0-9]+|ICLR.cc/2018/Workshop/Paper325/Authors|ICLR.cc/2018/Workshop/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Workshop/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Workshop/Paper325/Reviewers", "ICLR.cc/2018/Workshop/Paper325/Authors", "ICLR.cc/2018/Workshop/Program_Chairs"], "cdate": 1519222445613}}, "tauthor": "lobacheva.tjulja@gmail.com"}, {"tddate": null, "ddate": null, "tmdate": 1525969820908, "tcdate": 1525969820908, "number": 2, "cdate": 1525969820908, "id": "SyrsmefAz", "invitation": "ICLR.cc/2018/Workshop/-/Paper325/Official_Comment", "forum": "SkIr6FywG", "replyto": "HJ18c-WtM", "signatures": ["ICLR.cc/2018/Workshop/Paper325/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper325/Authors"], "content": {"title": "Author's response", "comment": "Thank you for your feedback. We are going to report other experimental results on data such as video and text in the next revision of the paper."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive prediction time for sequence classification", "abstract": "In this paper, we propose a recurrent neural network architecture for early sequence classification, when the model is required to output a label as soon as possible with negligible decline in accuracy. Our model is capable of learning how many sequence tokens it needs to observe in order to make a prediction; moreover, the number of steps required differs for each sequence. Experiments on sequential MNIST show that the proposed architecture focuses on different sequence parts during inference, which correspond to contours of the handwritten digits. We also demonstrate the improvement in the prediction quality with a simultaneous reduction in the prefix size used, the extent of which depends on the distribution of distinct class features over time.", "paperhash": "ryabinin|adaptive_prediction_time_for_sequence_classification", "keywords": ["Recurrent neural networks", "Adaptive computational time", "Early sequence classification"], "_bibtex": "@misc{\n  ryabinin2018adaptive,\n  title={Adaptive prediction time for sequence classification},\n  author={Maksim Ryabinin and Ekaterina Lobacheva},\n  year={2018},\n  url={https://openreview.net/forum?id=SkIr6FywG}\n}", "authorids": ["mkryabinin@edu.hse.ru", "elobacheva@hse.ru"], "authors": ["Maksim Ryabinin", "Ekaterina Lobacheva"], "TL;DR": "We propose a recurrent model for early sequence classification based on the idea of Adaptive computational time.", "pdf": "/pdf/2820cd04e3b87a99dd380b683d84e6e7a4a5cb28.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1519222445613, "id": "ICLR.cc/2018/Workshop/-/Paper325/Official_Comment", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "SkIr6FywG", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper325/AnonReviewer[0-9]+|ICLR.cc/2018/Workshop/Paper325/Authors|ICLR.cc/2018/Workshop/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper325/AnonReviewer[0-9]+|ICLR.cc/2018/Workshop/Paper325/Authors|ICLR.cc/2018/Workshop/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Workshop/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Workshop/Paper325/Reviewers", "ICLR.cc/2018/Workshop/Paper325/Authors", "ICLR.cc/2018/Workshop/Program_Chairs"], "cdate": 1519222445613}}, "tauthor": "lobacheva.tjulja@gmail.com"}, {"tddate": null, "ddate": null, "tmdate": 1525969783687, "tcdate": 1525969783687, "number": 1, "cdate": 1525969783687, "id": "SyxtQxz0z", "invitation": "ICLR.cc/2018/Workshop/-/Paper325/Official_Comment", "forum": "SkIr6FywG", "replyto": "HkBPZX-tz", "signatures": ["ICLR.cc/2018/Workshop/Paper325/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper325/Authors"], "content": {"title": "Author's response", "comment": "Thank you for your helpful comments. \n\nRegarding the question about fewer steps for Skip GRU on reordered MNIST: as with our model, the network automatically determines how many steps to make depending on the data and attempts to skip noninformative sequence elements. Because nearly all pixels from the center of the image are moved to the beginning, the network might read most of them (possibly skipping redundant ones) as in the original dataset and then skip large image portions which are mainly black until the end of the sequence. We will add a detailed comparison with illustrations to the next revision of the paper.\n\nAs for dependence of reduction in the number of steps on the data ordering, we will further explore this phenomenon in additional experiments using other datasets in order to conduct a more comprehensive study."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive prediction time for sequence classification", "abstract": "In this paper, we propose a recurrent neural network architecture for early sequence classification, when the model is required to output a label as soon as possible with negligible decline in accuracy. Our model is capable of learning how many sequence tokens it needs to observe in order to make a prediction; moreover, the number of steps required differs for each sequence. Experiments on sequential MNIST show that the proposed architecture focuses on different sequence parts during inference, which correspond to contours of the handwritten digits. We also demonstrate the improvement in the prediction quality with a simultaneous reduction in the prefix size used, the extent of which depends on the distribution of distinct class features over time.", "paperhash": "ryabinin|adaptive_prediction_time_for_sequence_classification", "keywords": ["Recurrent neural networks", "Adaptive computational time", "Early sequence classification"], "_bibtex": "@misc{\n  ryabinin2018adaptive,\n  title={Adaptive prediction time for sequence classification},\n  author={Maksim Ryabinin and Ekaterina Lobacheva},\n  year={2018},\n  url={https://openreview.net/forum?id=SkIr6FywG}\n}", "authorids": ["mkryabinin@edu.hse.ru", "elobacheva@hse.ru"], "authors": ["Maksim Ryabinin", "Ekaterina Lobacheva"], "TL;DR": "We propose a recurrent model for early sequence classification based on the idea of Adaptive computational time.", "pdf": "/pdf/2820cd04e3b87a99dd380b683d84e6e7a4a5cb28.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1519222445613, "id": "ICLR.cc/2018/Workshop/-/Paper325/Official_Comment", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "SkIr6FywG", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper325/AnonReviewer[0-9]+|ICLR.cc/2018/Workshop/Paper325/Authors|ICLR.cc/2018/Workshop/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper325/AnonReviewer[0-9]+|ICLR.cc/2018/Workshop/Paper325/Authors|ICLR.cc/2018/Workshop/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Workshop/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Workshop/Paper325/Reviewers", "ICLR.cc/2018/Workshop/Paper325/Authors", "ICLR.cc/2018/Workshop/Program_Chairs"], "cdate": 1519222445613}}, "tauthor": "lobacheva.tjulja@gmail.com"}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582927158, "tcdate": 1520355947385, "number": 1, "cdate": 1520355947385, "id": "SyQuqrh_G", "invitation": "ICLR.cc/2018/Workshop/-/Paper325/Official_Review", "forum": "SkIr6FywG", "replyto": "SkIr6FywG", "signatures": ["ICLR.cc/2018/Workshop/Paper325/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper325/AnonReviewer2"], "content": {"title": "Some positive results, but the paper suffers from unfortunate nomenclature and poorly justified claims.", "rating": "4: Ok but not good enough - rejection", "review": "The authors propose a sequence classification model which uses an attention mechanism whose structure allows the model to only use a prefix of the input. This allows them to achieve a small increase in accuracy over some baselines on the sequential MNIST dataset, using a number of operations comparable to the SkipRNN model of Campos et al.\n\nThe choice of calling h_t a 'halting probability' is somewhat confusing. At no point is the model stopped by sampling from a h_t Bernouilli, as one might expect from the name. Instead, the h_t correspond to attention weights computed sequentially as independent sigmoids until their sum reaches 1, at which point the model stops reading the input.\n\nThere are several issues with that setup. First, there is no guarantee that p_t sum to one: what happens e.g. id \\epsilon = 0.2, h_1 = 0.75 and h_2 = 0.75 ? then, according to equations (1)-(3), we have p_1 + p_2 = 1.5. Secondly, R is not a function of the sequence length, and so I am not sure I understand how \\lambda trades off between accuracy and computation.\n\nIn the Experiments section, the authors need to give more details about how the number of steps is counted: I assume that it is the average value of N as defined in Equation (N), but it is not absolutely clear.\n\nFinally, claiming an improvement over Skip-RNN on the basis of a reduction in the prefix length on Reordered SeqMNIST does not make sense: as the authors note themselves, this is only possible because the specific reordering gives them access to important information from the future earlier, and does not correspond to any reality of the motivating applications mentioned in the Introduction.\n ", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive prediction time for sequence classification", "abstract": "In this paper, we propose a recurrent neural network architecture for early sequence classification, when the model is required to output a label as soon as possible with negligible decline in accuracy. Our model is capable of learning how many sequence tokens it needs to observe in order to make a prediction; moreover, the number of steps required differs for each sequence. Experiments on sequential MNIST show that the proposed architecture focuses on different sequence parts during inference, which correspond to contours of the handwritten digits. We also demonstrate the improvement in the prediction quality with a simultaneous reduction in the prefix size used, the extent of which depends on the distribution of distinct class features over time.", "paperhash": "ryabinin|adaptive_prediction_time_for_sequence_classification", "keywords": ["Recurrent neural networks", "Adaptive computational time", "Early sequence classification"], "_bibtex": "@misc{\n  ryabinin2018adaptive,\n  title={Adaptive prediction time for sequence classification},\n  author={Maksim Ryabinin and Ekaterina Lobacheva},\n  year={2018},\n  url={https://openreview.net/forum?id=SkIr6FywG}\n}", "authorids": ["mkryabinin@edu.hse.ru", "elobacheva@hse.ru"], "authors": ["Maksim Ryabinin", "Ekaterina Lobacheva"], "TL;DR": "We propose a recurrent model for early sequence classification based on the idea of Adaptive computational time.", "pdf": "/pdf/2820cd04e3b87a99dd380b683d84e6e7a4a5cb28.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582926935, "id": "ICLR.cc/2018/Workshop/-/Paper325/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper325/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper325/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper325/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper325/AnonReviewer1"], "reply": {"forum": "SkIr6FywG", "replyto": "SkIr6FywG", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper325/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper325/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582926935}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582736016, "tcdate": 1520667207281, "number": 2, "cdate": 1520667207281, "id": "HJ18c-WtM", "invitation": "ICLR.cc/2018/Workshop/-/Paper325/Official_Review", "forum": "SkIr6FywG", "replyto": "SkIr6FywG", "signatures": ["ICLR.cc/2018/Workshop/Paper325/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper325/AnonReviewer3"], "content": {"title": "useful and appropriate application of existing method, not much novelty", "rating": "6: Marginally above acceptance threshold", "review": "The paper is inspired by Graves (2016), which allows the model to apply RNN multiple times (# of times also learned by the model) for each input to the RNN. The paper uses the same mechanism to halt the RNN early on sequential modeling while maintaining differentiability. So it is using an existing method for a different goal. This seems to be an appropriate application and could be useful but there is not much novelty of the method. The experimental visualizations are convincing and interesting but could have been better if also used on other sequence datasets (such as language classification) than MNIST.", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive prediction time for sequence classification", "abstract": "In this paper, we propose a recurrent neural network architecture for early sequence classification, when the model is required to output a label as soon as possible with negligible decline in accuracy. Our model is capable of learning how many sequence tokens it needs to observe in order to make a prediction; moreover, the number of steps required differs for each sequence. Experiments on sequential MNIST show that the proposed architecture focuses on different sequence parts during inference, which correspond to contours of the handwritten digits. We also demonstrate the improvement in the prediction quality with a simultaneous reduction in the prefix size used, the extent of which depends on the distribution of distinct class features over time.", "paperhash": "ryabinin|adaptive_prediction_time_for_sequence_classification", "keywords": ["Recurrent neural networks", "Adaptive computational time", "Early sequence classification"], "_bibtex": "@misc{\n  ryabinin2018adaptive,\n  title={Adaptive prediction time for sequence classification},\n  author={Maksim Ryabinin and Ekaterina Lobacheva},\n  year={2018},\n  url={https://openreview.net/forum?id=SkIr6FywG}\n}", "authorids": ["mkryabinin@edu.hse.ru", "elobacheva@hse.ru"], "authors": ["Maksim Ryabinin", "Ekaterina Lobacheva"], "TL;DR": "We propose a recurrent model for early sequence classification based on the idea of Adaptive computational time.", "pdf": "/pdf/2820cd04e3b87a99dd380b683d84e6e7a4a5cb28.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582926935, "id": "ICLR.cc/2018/Workshop/-/Paper325/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper325/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper325/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper325/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper325/AnonReviewer1"], "reply": {"forum": "SkIr6FywG", "replyto": "SkIr6FywG", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper325/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper325/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582926935}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582727236, "tcdate": 1520673116895, "number": 3, "cdate": 1520673116895, "id": "HkBPZX-tz", "invitation": "ICLR.cc/2018/Workshop/-/Paper325/Official_Review", "forum": "SkIr6FywG", "replyto": "SkIr6FywG", "signatures": ["ICLR.cc/2018/Workshop/Paper325/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper325/AnonReviewer1"], "content": {"title": "Simple solution to adaptive prediction time.", "rating": "7: Good paper, accept", "review": "The paper considers the task of adaptive prediction time - or in other words learning to produce answers when enough data was observed.\n\nThe proposed solution uses the ACT (Graves 2016) computation as the stopping criterion. However, (Graves 2016) used it to determine the number of ponder steps made after every RNN transition, while in this submission the computation is done globally, to determine the overall number of transition steps.\n\nQuestions:\nwhy does the skip GRU make fewer steps in the reordered MNIST rather than the regular MNIST? It seems that in both cases it was looking ad every second pixel. Or was this number tuned? Then you may want to indicate it in the table.\n\nPros:\n- It is nice to see that the ACT computation, originally developed to handle sequences of a few ponder steps scales to hundreds of steps.\n\nCons:\n- The approach seems to be limited by the data ordering - the biggest gain in time reduction is obtained when a better permutation of MNIST pixels is used. This suggests that the technique must find its own proper benchmark problems.", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive prediction time for sequence classification", "abstract": "In this paper, we propose a recurrent neural network architecture for early sequence classification, when the model is required to output a label as soon as possible with negligible decline in accuracy. Our model is capable of learning how many sequence tokens it needs to observe in order to make a prediction; moreover, the number of steps required differs for each sequence. Experiments on sequential MNIST show that the proposed architecture focuses on different sequence parts during inference, which correspond to contours of the handwritten digits. We also demonstrate the improvement in the prediction quality with a simultaneous reduction in the prefix size used, the extent of which depends on the distribution of distinct class features over time.", "paperhash": "ryabinin|adaptive_prediction_time_for_sequence_classification", "keywords": ["Recurrent neural networks", "Adaptive computational time", "Early sequence classification"], "_bibtex": "@misc{\n  ryabinin2018adaptive,\n  title={Adaptive prediction time for sequence classification},\n  author={Maksim Ryabinin and Ekaterina Lobacheva},\n  year={2018},\n  url={https://openreview.net/forum?id=SkIr6FywG}\n}", "authorids": ["mkryabinin@edu.hse.ru", "elobacheva@hse.ru"], "authors": ["Maksim Ryabinin", "Ekaterina Lobacheva"], "TL;DR": "We propose a recurrent model for early sequence classification based on the idea of Adaptive computational time.", "pdf": "/pdf/2820cd04e3b87a99dd380b683d84e6e7a4a5cb28.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582926935, "id": "ICLR.cc/2018/Workshop/-/Paper325/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper325/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper325/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper325/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper325/AnonReviewer1"], "reply": {"forum": "SkIr6FywG", "replyto": "SkIr6FywG", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper325/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper325/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582926935}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521573576862, "tcdate": 1521573576862, "number": 143, "cdate": 1521573576520, "id": "H1WAAAAtM", "invitation": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "forum": "SkIr6FywG", "replyto": "SkIr6FywG", "signatures": ["ICLR.cc/2018/Workshop/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Program_Chairs"], "content": {"decision": "Reject", "title": "ICLR 2018 Workshop Acceptance Decision", "comment": "Based on the reviews, this paper has not been accepted for presentation at the ICLR workshop. However, the conversation and updates can continue to appear here on OpenReview."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive prediction time for sequence classification", "abstract": "In this paper, we propose a recurrent neural network architecture for early sequence classification, when the model is required to output a label as soon as possible with negligible decline in accuracy. Our model is capable of learning how many sequence tokens it needs to observe in order to make a prediction; moreover, the number of steps required differs for each sequence. Experiments on sequential MNIST show that the proposed architecture focuses on different sequence parts during inference, which correspond to contours of the handwritten digits. We also demonstrate the improvement in the prediction quality with a simultaneous reduction in the prefix size used, the extent of which depends on the distribution of distinct class features over time.", "paperhash": "ryabinin|adaptive_prediction_time_for_sequence_classification", "keywords": ["Recurrent neural networks", "Adaptive computational time", "Early sequence classification"], "_bibtex": "@misc{\n  ryabinin2018adaptive,\n  title={Adaptive prediction time for sequence classification},\n  author={Maksim Ryabinin and Ekaterina Lobacheva},\n  year={2018},\n  url={https://openreview.net/forum?id=SkIr6FywG}\n}", "authorids": ["mkryabinin@edu.hse.ru", "elobacheva@hse.ru"], "authors": ["Maksim Ryabinin", "Ekaterina Lobacheva"], "TL;DR": "We propose a recurrent model for early sequence classification based on the idea of Adaptive computational time.", "pdf": "/pdf/2820cd04e3b87a99dd380b683d84e6e7a4a5cb28.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518629844880, "id": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Program_Chairs"], "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Workshop/-/Submission", "writers": {"values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Workshop/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Workshop Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept", "Reject"]}}}, "nonreaders": [], "noninvitees": [], "cdate": 1518629844880}}}], "count": 8}