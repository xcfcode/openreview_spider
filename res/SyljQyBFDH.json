{"notes": [{"id": "SyljQyBFDH", "original": "SylmHd2OPB", "number": 1629, "cdate": 1569439522900, "ddate": null, "tcdate": 1569439522900, "tmdate": 1583912049800, "tddate": null, "forum": "SyljQyBFDH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["bartunov@google.com", "jwrae@google.com", "osindero@google.com", "countzero@google.com"], "title": "Meta-Learning Deep Energy-Based Memory Models", "authors": ["Sergey Bartunov", "Jack Rae", "Simon Osindero", "Timothy Lillicrap"], "pdf": "/pdf/20b1e0f615c85a10a0c9c4e71de709dbb7f731a4.pdf", "TL;DR": "Deep associative memory models using arbitrary neural networks as a storage.", "abstract": "We study the problem of learning an associative memory model -- a system which is able to retrieve a remembered pattern based on its distorted or incomplete version.\nAttractor networks provide a sound model of associative memory: patterns are stored as attractors of the network dynamics and associative retrieval is performed by running the dynamics starting from a query pattern until it converges to an attractor. \nIn such models the dynamics are often implemented as an optimization procedure that minimizes an energy function, such as in the classical Hopfield network. \nIn general it is difficult to derive a writing rule for a given dynamics and energy that is both compressive and fast.\nThus, most research in energy-based memory has been limited either to tractable energy models not expressive enough to handle complex high-dimensional objects such as natural images, or to models that do not offer fast writing.\nWe present a novel meta-learning approach to energy-based memory models (EBMM) that allows one to use an arbitrary neural architecture as an energy model and quickly store patterns in its weights. \nWe demonstrate experimentally that our EBMM approach can build compressed memories for synthetic and natural data, and is capable of associative retrieval that outperforms existing memory systems in terms of the reconstruction error and compression rate.", "keywords": ["associative memory", "energy-based memory", "meta-learning", "compressive memory"], "paperhash": "bartunov|metalearning_deep_energybased_memory_models", "_bibtex": "@inproceedings{\nBartunov2020Meta-Learning,\ntitle={Meta-Learning Deep Energy-Based Memory Models},\nauthor={Sergey Bartunov and Jack Rae and Simon Osindero and Timothy Lillicrap},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SyljQyBFDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/3605041945b73708d75246e49b04ff1250ecfec6.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 14, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "7UnhLzb3A5", "original": null, "number": 1, "cdate": 1577995564386, "ddate": null, "tcdate": 1577995564386, "tmdate": 1577995564386, "tddate": null, "forum": "SyljQyBFDH", "replyto": "SyljQyBFDH", "invitation": "ICLR.cc/2020/Conference/Paper1629/-/Public_Comment", "content": {"title": "Missing related reference about Energy-based models using  neural networks to approximate the energy function", "comment": "Dear Authors, \n\nCongratulations on your nice accepted paper. \n\nI would like to point out some papers that are highly related to your current one, and hope you can cite them in your final version.  All of them are about generative models, which are in the forms of energy-based models parameterized by neural nets. \n\nThe seminal paper that proposes an energy-based model parameterized by modern deep neural network and learned it by Langevin based MLE is in (Xie. ICML 2016) [1].  The paper also involves theory about the connection with the discriminative ConvNet, Hopfield network, and Contrastive divergence.  The model is called \"Energy-based\" generative ConvNet, because it is naturally derived from the discriminative ConvNet, instead of manually designed.\n\n(Xie. CVPR 2017) [2] proposed to use Spatial-Temporal ConvNet as the energy function for video modeling. In the theory part, it firstly provides a self-adversarial interpretation for the MCMC-based learning of the EBM with ConvNet as energy functions.\n\n(Xie. CVPR 2018) [3] proposed to use volumetric ConvNet as the energy function for 3D shape patterns generation. It is called 3D descriptor Net. \n\n(Gao. CVPR 2018) [4] proposed multi-grid MCMC to learn EBM with ConvNet as energy function. \n\n(Nijkamp 2019) [5] proposed short-run MCMC to learn EBM with ConvNet as energy function. \n\nThank you :)\n\nReference\n[1] A Theory of Generative ConvNet. \nJianwen Xie *, Yang Lu *, Song-Chun Zhu, Ying Nian Wu (ICML 2016)\n\n[2] Synthesizing Dynamic Pattern by Spatial-Temporal Generative ConvNet\nJianwen Xie, Song-Chun Zhu, Ying Nian Wu (CVPR 2017)\n\n[3] Learning Descriptor Networks for 3D Shape Synthesis and Analysis\nJianwen Xie *, Zilong Zheng *, Ruiqi Gao, Wenguan Wang, Song-Chun Zhu, Ying Nian Wu (CVPR) 2018 \n\n[4]  Learning generative ConvNets via multigrid modeling and sampling. \nR Gao*, Y Lu*, J Zhou, SC Zhu, and YN Wu (CVPR 2018).  \n\n[5] On learning non-convergent non-persistent short-run MCMC toward energy-based model. \nE Nijkamp, M Hill, SC Zhu, and YN Wu (NeurIPS 2019)"}, "signatures": ["~Jianwen_Xie1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Jianwen_Xie1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["bartunov@google.com", "jwrae@google.com", "osindero@google.com", "countzero@google.com"], "title": "Meta-Learning Deep Energy-Based Memory Models", "authors": ["Sergey Bartunov", "Jack Rae", "Simon Osindero", "Timothy Lillicrap"], "pdf": "/pdf/20b1e0f615c85a10a0c9c4e71de709dbb7f731a4.pdf", "TL;DR": "Deep associative memory models using arbitrary neural networks as a storage.", "abstract": "We study the problem of learning an associative memory model -- a system which is able to retrieve a remembered pattern based on its distorted or incomplete version.\nAttractor networks provide a sound model of associative memory: patterns are stored as attractors of the network dynamics and associative retrieval is performed by running the dynamics starting from a query pattern until it converges to an attractor. \nIn such models the dynamics are often implemented as an optimization procedure that minimizes an energy function, such as in the classical Hopfield network. \nIn general it is difficult to derive a writing rule for a given dynamics and energy that is both compressive and fast.\nThus, most research in energy-based memory has been limited either to tractable energy models not expressive enough to handle complex high-dimensional objects such as natural images, or to models that do not offer fast writing.\nWe present a novel meta-learning approach to energy-based memory models (EBMM) that allows one to use an arbitrary neural architecture as an energy model and quickly store patterns in its weights. \nWe demonstrate experimentally that our EBMM approach can build compressed memories for synthetic and natural data, and is capable of associative retrieval that outperforms existing memory systems in terms of the reconstruction error and compression rate.", "keywords": ["associative memory", "energy-based memory", "meta-learning", "compressive memory"], "paperhash": "bartunov|metalearning_deep_energybased_memory_models", "_bibtex": "@inproceedings{\nBartunov2020Meta-Learning,\ntitle={Meta-Learning Deep Energy-Based Memory Models},\nauthor={Sergey Bartunov and Jack Rae and Simon Osindero and Timothy Lillicrap},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SyljQyBFDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/3605041945b73708d75246e49b04ff1250ecfec6.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SyljQyBFDH", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504192047, "tmdate": 1576860587730, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper1629/Authors", "ICLR.cc/2020/Conference/Paper1629/Reviewers", "ICLR.cc/2020/Conference/Paper1629/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1629/-/Public_Comment"}}}, {"id": "k897WTvqj6", "original": null, "number": 1, "cdate": 1576798728276, "ddate": null, "tcdate": 1576798728276, "tmdate": 1576800908263, "tddate": null, "forum": "SyljQyBFDH", "replyto": "SyljQyBFDH", "invitation": "ICLR.cc/2020/Conference/Paper1629/-/Decision", "content": {"decision": "Accept (Poster)", "comment": "Four knowledgable reviewers recommend accept. Good job!", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["bartunov@google.com", "jwrae@google.com", "osindero@google.com", "countzero@google.com"], "title": "Meta-Learning Deep Energy-Based Memory Models", "authors": ["Sergey Bartunov", "Jack Rae", "Simon Osindero", "Timothy Lillicrap"], "pdf": "/pdf/20b1e0f615c85a10a0c9c4e71de709dbb7f731a4.pdf", "TL;DR": "Deep associative memory models using arbitrary neural networks as a storage.", "abstract": "We study the problem of learning an associative memory model -- a system which is able to retrieve a remembered pattern based on its distorted or incomplete version.\nAttractor networks provide a sound model of associative memory: patterns are stored as attractors of the network dynamics and associative retrieval is performed by running the dynamics starting from a query pattern until it converges to an attractor. \nIn such models the dynamics are often implemented as an optimization procedure that minimizes an energy function, such as in the classical Hopfield network. \nIn general it is difficult to derive a writing rule for a given dynamics and energy that is both compressive and fast.\nThus, most research in energy-based memory has been limited either to tractable energy models not expressive enough to handle complex high-dimensional objects such as natural images, or to models that do not offer fast writing.\nWe present a novel meta-learning approach to energy-based memory models (EBMM) that allows one to use an arbitrary neural architecture as an energy model and quickly store patterns in its weights. \nWe demonstrate experimentally that our EBMM approach can build compressed memories for synthetic and natural data, and is capable of associative retrieval that outperforms existing memory systems in terms of the reconstruction error and compression rate.", "keywords": ["associative memory", "energy-based memory", "meta-learning", "compressive memory"], "paperhash": "bartunov|metalearning_deep_energybased_memory_models", "_bibtex": "@inproceedings{\nBartunov2020Meta-Learning,\ntitle={Meta-Learning Deep Energy-Based Memory Models},\nauthor={Sergey Bartunov and Jack Rae and Simon Osindero and Timothy Lillicrap},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SyljQyBFDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/3605041945b73708d75246e49b04ff1250ecfec6.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "SyljQyBFDH", "replyto": "SyljQyBFDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795709266, "tmdate": 1576800257962, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1629/-/Decision"}}}, {"id": "HyekxA0EcS", "original": null, "number": 2, "cdate": 1572298214998, "ddate": null, "tcdate": 1572298214998, "tmdate": 1574488670784, "tddate": null, "forum": "SyljQyBFDH", "replyto": "SyljQyBFDH", "invitation": "ICLR.cc/2020/Conference/Paper1629/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #2", "review": "Thanks for the extensive answers. I updated my rating based on the provided clarification and extra experiments.\n\n=====================\nMy understanding of eq 1-5 is that the algorithm finds an energy landscape (by modifying \\theta) for each dataset (task) such that in this landscape, the inputs from the distribution are reachable by truncated gradient-descent initiating at a query (distorted input with respect to some distortion model).\n\n1- The connection to meta-learning is unclear in your experiments. Can you elaborate on that? \n\n2- The expectation in eq 5 is over different input patterns, which I assume that a set of input patterns belong to a task. What is that you write in memory?  For each experiment, what are the different input patterns (tasks) that you have written in the memory?\n\n3- What is the \\theta that is feed to the read function at the test time? \n\n4- Are you testing on the tasks that you already trained on?\n\n5- How this approach generalizes to unseen (or relatively close) task? \n\n6- Can it recover any query that is not constructed with respect to the distortion model that is trained on? or what happens if the distorted image at test times comes from a different distortion model? (image blocking, for example)\n\n7- How many distorted samples are used for training?\n\n\n8- For the chosen tasks, I am curious to see the experimental comparison to deep image prior (Ulyanov, 2018). Deep image prior would be very similar to the read operator (although the gradient descent is over the parameter of model) without having write operations when you define the energy as MSE.\n\n\n\nTypos and writing style:\n-- The expectation in eq 2 should independently show what the expectation is taken with respect to. \n-- input patters -> input patterns\n-- figure 3 -> Figure 3\n-- section 1 -> Section 1\n-- 4 random images -> four random images \n-- the Figure 5b -> Figure 5b\n-- Models such as (Ba et al., 2016; Miconi et al., 2018) enable ->  Models such as Ba et al. (2016) and Miconi et al. (2018) enable", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper1629/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1629/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["bartunov@google.com", "jwrae@google.com", "osindero@google.com", "countzero@google.com"], "title": "Meta-Learning Deep Energy-Based Memory Models", "authors": ["Sergey Bartunov", "Jack Rae", "Simon Osindero", "Timothy Lillicrap"], "pdf": "/pdf/20b1e0f615c85a10a0c9c4e71de709dbb7f731a4.pdf", "TL;DR": "Deep associative memory models using arbitrary neural networks as a storage.", "abstract": "We study the problem of learning an associative memory model -- a system which is able to retrieve a remembered pattern based on its distorted or incomplete version.\nAttractor networks provide a sound model of associative memory: patterns are stored as attractors of the network dynamics and associative retrieval is performed by running the dynamics starting from a query pattern until it converges to an attractor. \nIn such models the dynamics are often implemented as an optimization procedure that minimizes an energy function, such as in the classical Hopfield network. \nIn general it is difficult to derive a writing rule for a given dynamics and energy that is both compressive and fast.\nThus, most research in energy-based memory has been limited either to tractable energy models not expressive enough to handle complex high-dimensional objects such as natural images, or to models that do not offer fast writing.\nWe present a novel meta-learning approach to energy-based memory models (EBMM) that allows one to use an arbitrary neural architecture as an energy model and quickly store patterns in its weights. \nWe demonstrate experimentally that our EBMM approach can build compressed memories for synthetic and natural data, and is capable of associative retrieval that outperforms existing memory systems in terms of the reconstruction error and compression rate.", "keywords": ["associative memory", "energy-based memory", "meta-learning", "compressive memory"], "paperhash": "bartunov|metalearning_deep_energybased_memory_models", "_bibtex": "@inproceedings{\nBartunov2020Meta-Learning,\ntitle={Meta-Learning Deep Energy-Based Memory Models},\nauthor={Sergey Bartunov and Jack Rae and Simon Osindero and Timothy Lillicrap},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SyljQyBFDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/3605041945b73708d75246e49b04ff1250ecfec6.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SyljQyBFDH", "replyto": "SyljQyBFDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1629/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1629/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575852608908, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1629/Reviewers"], "noninvitees": [], "tcdate": 1570237734599, "tmdate": 1575852608922, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1629/-/Official_Review"}}}, {"id": "B1e_L5zpKH", "original": null, "number": 1, "cdate": 1571789392088, "ddate": null, "tcdate": 1571789392088, "tmdate": 1574390490515, "tddate": null, "forum": "SyljQyBFDH", "replyto": "SyljQyBFDH", "invitation": "ICLR.cc/2020/Conference/Paper1629/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #3", "review": "\n======================================== Update after revisions ============================================\n\nI appreciate the effort the authors have put into the revision and the rebuttal. I'm happy to increase my score and recommend acceptance based on the revised paper. \n\nHowever, I have to say that some of my worries still linger. With respect to non-memory baselines, the authors have responded that the memory based models will outperform non-memory models in cases where prior structure is less important than memory and provided a demonstration of this with an extreme example, i.e. a case with no structure (random binary strings example). I understand the point being made here, but this is a rather pedantic and uninteresting example. The authors have provided another (more interesting) example in Appendix A3 and shown that the memory-based model outperforms some simpler baselines such as the DAE even in this case. But no explanation is given for this result. Why is the memory based model outperforming the DAE in this case, given that this is an example where prior *is* very important? I'm a bit worried that the DAE results may perhaps be due to a non-optimized architecture or training setup (and what exactly is the architecture used for the DAE here)? I would appreciate it if the authors could clarify these issues in the final version.\n\nI have also spotted several typos. For the final version, please make sure to go through the paper thoroughly at least once and fix all the typos.\n\n========================================================================================================\n\nThis paper proposes a meta-learning approach to learning fast read and write mechanisms in an energy-based model so that a given set of images can be quickly inducted into memory and retrieved from memory with noisy queries. The paper is well-written and the proposed approach seems interesting and novel enough. However, I have some concerns about the paper that need to be addressed. Here are the main issues for me:\n\n1) I am in general not really convinced about the supposed advantages of these attractor memory models (this paper and the earlier Kanerva machine) over more standard and much simpler approaches. For example, for the problem of retrieval from noisy queries, a more standard approach would be a simple autoencoder. Note that in an autoencoder, reading (inference) is already fast. The authors might point out that writing (training) will not be fast, which is correct. However, the meta-learning phase proposed in this paper will also not be fast and perhaps the fair comparison should be between the meta-learning phase of this paper and the standard training phase of an autoencoder. Note that the autoencoder will have additional benefits. For example, with the autoencoder, one is not constrained by memory storage requirements and can make use of a much larger set of images to train the model. This allows the model to learn a richer structure in images. Moreover, with a large enough feedforward net, one can approximate arbitrarily complex dependencies in images. However, in attractor memory models, on the other hand, one necessarily restricts oneself to a particular model class that can be expressed as gradient descent dynamics in an energy landscape both during reading and writing. This seems overly restrictive to me. So, perhaps, the authors can clarify the supposed advantages of these attractor memory models a bit better. For example, I would be interested in seeing some comparative results with, say, a denoising autoencoder model.\n\n2) Currently, the paper only uses a specific type of \u201cblock-noise\u201d corruption. One thing that would be nice to see is some results with other noise models. I think this is important to demonstrate that the approach is general enough to handle different kinds of noise. Also, a salt-and-pepper noise will allow the authors to compare their results with the dynamic Kanerva machine (the authors note that the DKM failed to train successfully for the block-noise used here). \n\n3) It would be good to say something about the meta-learned parameters, theta_bar, r, tau. Is there any meaningful structure in these parameters that distinguishes them from their random initial values? Is one of these parameters more important than the others? For example, what happens if you just use generic step size decay rules for gamma and eta (or perhaps no decay at all)?", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"}, "signatures": ["ICLR.cc/2020/Conference/Paper1629/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1629/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["bartunov@google.com", "jwrae@google.com", "osindero@google.com", "countzero@google.com"], "title": "Meta-Learning Deep Energy-Based Memory Models", "authors": ["Sergey Bartunov", "Jack Rae", "Simon Osindero", "Timothy Lillicrap"], "pdf": "/pdf/20b1e0f615c85a10a0c9c4e71de709dbb7f731a4.pdf", "TL;DR": "Deep associative memory models using arbitrary neural networks as a storage.", "abstract": "We study the problem of learning an associative memory model -- a system which is able to retrieve a remembered pattern based on its distorted or incomplete version.\nAttractor networks provide a sound model of associative memory: patterns are stored as attractors of the network dynamics and associative retrieval is performed by running the dynamics starting from a query pattern until it converges to an attractor. \nIn such models the dynamics are often implemented as an optimization procedure that minimizes an energy function, such as in the classical Hopfield network. \nIn general it is difficult to derive a writing rule for a given dynamics and energy that is both compressive and fast.\nThus, most research in energy-based memory has been limited either to tractable energy models not expressive enough to handle complex high-dimensional objects such as natural images, or to models that do not offer fast writing.\nWe present a novel meta-learning approach to energy-based memory models (EBMM) that allows one to use an arbitrary neural architecture as an energy model and quickly store patterns in its weights. \nWe demonstrate experimentally that our EBMM approach can build compressed memories for synthetic and natural data, and is capable of associative retrieval that outperforms existing memory systems in terms of the reconstruction error and compression rate.", "keywords": ["associative memory", "energy-based memory", "meta-learning", "compressive memory"], "paperhash": "bartunov|metalearning_deep_energybased_memory_models", "_bibtex": "@inproceedings{\nBartunov2020Meta-Learning,\ntitle={Meta-Learning Deep Energy-Based Memory Models},\nauthor={Sergey Bartunov and Jack Rae and Simon Osindero and Timothy Lillicrap},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SyljQyBFDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/3605041945b73708d75246e49b04ff1250ecfec6.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SyljQyBFDH", "replyto": "SyljQyBFDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1629/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1629/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575852608908, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1629/Reviewers"], "noninvitees": [], "tcdate": 1570237734599, "tmdate": 1575852608922, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1629/-/Official_Review"}}}, {"id": "BJlwh7Xnsr", "original": null, "number": 9, "cdate": 1573823406929, "ddate": null, "tcdate": 1573823406929, "tmdate": 1573823406929, "tddate": null, "forum": "SyljQyBFDH", "replyto": "SyljQyBFDH", "invitation": "ICLR.cc/2020/Conference/Paper1629/-/Official_Comment", "content": {"title": "Summary of changes", "comment": "We would like to summarize the changes we made to the initial submission based on the reviewers feedback.\n\n1) The training process is clarified.\n2) A discussion of EBMM in the context of other modern techniques such as GANs and VAEs is added to Section 5.\n3) Section 6 has been enriched with a more detailed discussion of existing limitations of EBMM and potential ways of improving on them.\n4) Appendix A.3 has been added where we provide a direct comparison to three different non-memory baselines.\n5) Comparison to Kanerva Machine has been added as Appendix A.4\n6) We performed experiments with different distortion models in Appendix A.5. EBMM shows satisfactory behavior with both decreased and increased level of noise.\n7) We tested the ability to transfer writing and reading capabilities of EBMM on the example of Omniglot and MNIST in Appendix A.6.\n8) We performed an experiment with the correlated batches in Appendix A.7 and found that training on correlated batches leads to better memory consolidation.\n\nWe hope our reviewers and area chairs will find these changes as a significant improvement of the paper and as a comprehensive answer to all raised questions."}, "signatures": ["ICLR.cc/2020/Conference/Paper1629/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1629/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["bartunov@google.com", "jwrae@google.com", "osindero@google.com", "countzero@google.com"], "title": "Meta-Learning Deep Energy-Based Memory Models", "authors": ["Sergey Bartunov", "Jack Rae", "Simon Osindero", "Timothy Lillicrap"], "pdf": "/pdf/20b1e0f615c85a10a0c9c4e71de709dbb7f731a4.pdf", "TL;DR": "Deep associative memory models using arbitrary neural networks as a storage.", "abstract": "We study the problem of learning an associative memory model -- a system which is able to retrieve a remembered pattern based on its distorted or incomplete version.\nAttractor networks provide a sound model of associative memory: patterns are stored as attractors of the network dynamics and associative retrieval is performed by running the dynamics starting from a query pattern until it converges to an attractor. \nIn such models the dynamics are often implemented as an optimization procedure that minimizes an energy function, such as in the classical Hopfield network. \nIn general it is difficult to derive a writing rule for a given dynamics and energy that is both compressive and fast.\nThus, most research in energy-based memory has been limited either to tractable energy models not expressive enough to handle complex high-dimensional objects such as natural images, or to models that do not offer fast writing.\nWe present a novel meta-learning approach to energy-based memory models (EBMM) that allows one to use an arbitrary neural architecture as an energy model and quickly store patterns in its weights. \nWe demonstrate experimentally that our EBMM approach can build compressed memories for synthetic and natural data, and is capable of associative retrieval that outperforms existing memory systems in terms of the reconstruction error and compression rate.", "keywords": ["associative memory", "energy-based memory", "meta-learning", "compressive memory"], "paperhash": "bartunov|metalearning_deep_energybased_memory_models", "_bibtex": "@inproceedings{\nBartunov2020Meta-Learning,\ntitle={Meta-Learning Deep Energy-Based Memory Models},\nauthor={Sergey Bartunov and Jack Rae and Simon Osindero and Timothy Lillicrap},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SyljQyBFDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/3605041945b73708d75246e49b04ff1250ecfec6.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SyljQyBFDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1629/Authors", "ICLR.cc/2020/Conference/Paper1629/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1629/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1629/Reviewers", "ICLR.cc/2020/Conference/Paper1629/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1629/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1629/Authors|ICLR.cc/2020/Conference/Paper1629/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504153198, "tmdate": 1576860554573, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1629/Authors", "ICLR.cc/2020/Conference/Paper1629/Reviewers", "ICLR.cc/2020/Conference/Paper1629/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1629/-/Official_Comment"}}}, {"id": "Sye2RlX3sH", "original": null, "number": 8, "cdate": 1573822676262, "ddate": null, "tcdate": 1573822676262, "tmdate": 1573822676262, "tddate": null, "forum": "SyljQyBFDH", "replyto": "BkeZXMcccB", "invitation": "ICLR.cc/2020/Conference/Paper1629/-/Official_Comment", "content": {"title": "Reply (continued)", "comment": "> Experiments on correlated batches\n\nWe display results for randomly chosen images within the test set. If storing very similar images this can actually make correct reconstruction more difficult, as there is more ambiguity in locating the original image from the occluded query image. We found that if the model was not trained on correlated batches, it does not benefit from them at the test time. However, when trained and tested on batches of 2 Omniglot classes, EBMM achieves significantly lower reconstruction error. Please see the new Appendix A.7 for details. We will be able to provide a comprehensive study for the final version of the paper."}, "signatures": ["ICLR.cc/2020/Conference/Paper1629/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1629/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["bartunov@google.com", "jwrae@google.com", "osindero@google.com", "countzero@google.com"], "title": "Meta-Learning Deep Energy-Based Memory Models", "authors": ["Sergey Bartunov", "Jack Rae", "Simon Osindero", "Timothy Lillicrap"], "pdf": "/pdf/20b1e0f615c85a10a0c9c4e71de709dbb7f731a4.pdf", "TL;DR": "Deep associative memory models using arbitrary neural networks as a storage.", "abstract": "We study the problem of learning an associative memory model -- a system which is able to retrieve a remembered pattern based on its distorted or incomplete version.\nAttractor networks provide a sound model of associative memory: patterns are stored as attractors of the network dynamics and associative retrieval is performed by running the dynamics starting from a query pattern until it converges to an attractor. \nIn such models the dynamics are often implemented as an optimization procedure that minimizes an energy function, such as in the classical Hopfield network. \nIn general it is difficult to derive a writing rule for a given dynamics and energy that is both compressive and fast.\nThus, most research in energy-based memory has been limited either to tractable energy models not expressive enough to handle complex high-dimensional objects such as natural images, or to models that do not offer fast writing.\nWe present a novel meta-learning approach to energy-based memory models (EBMM) that allows one to use an arbitrary neural architecture as an energy model and quickly store patterns in its weights. \nWe demonstrate experimentally that our EBMM approach can build compressed memories for synthetic and natural data, and is capable of associative retrieval that outperforms existing memory systems in terms of the reconstruction error and compression rate.", "keywords": ["associative memory", "energy-based memory", "meta-learning", "compressive memory"], "paperhash": "bartunov|metalearning_deep_energybased_memory_models", "_bibtex": "@inproceedings{\nBartunov2020Meta-Learning,\ntitle={Meta-Learning Deep Energy-Based Memory Models},\nauthor={Sergey Bartunov and Jack Rae and Simon Osindero and Timothy Lillicrap},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SyljQyBFDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/3605041945b73708d75246e49b04ff1250ecfec6.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SyljQyBFDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1629/Authors", "ICLR.cc/2020/Conference/Paper1629/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1629/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1629/Reviewers", "ICLR.cc/2020/Conference/Paper1629/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1629/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1629/Authors|ICLR.cc/2020/Conference/Paper1629/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504153198, "tmdate": 1576860554573, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1629/Authors", "ICLR.cc/2020/Conference/Paper1629/Reviewers", "ICLR.cc/2020/Conference/Paper1629/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1629/-/Official_Comment"}}}, {"id": "SylSrLlojr", "original": null, "number": 7, "cdate": 1573746237329, "ddate": null, "tcdate": 1573746237329, "tmdate": 1573746237329, "tddate": null, "forum": "SyljQyBFDH", "replyto": "BkeZXMcccB", "invitation": "ICLR.cc/2020/Conference/Paper1629/-/Official_Comment", "content": {"title": "Reply", "comment": "> - Dataset / batching details \n\nFor Omniglot, Cifar and ImageNet, we used the natural train/test splits in the datasets. We perform meta-learning training for at most 2 million iterations. For Omniglot and Cifar we used a batch size of 64, for ImageNet we select a batch of 32 images at a time. Batches were sampled uniformly. We have made this much clearer in the text now.\n\n> - Experiments across multiple SNR and generalization on noise patterns\n\nThank you for the suggestion, we have now added Appendix A.5 where we test generalization to different SNR on Omniglot. The model perfectly adapts to lower SNR and is able to perform reasonably well with non-significantly stronger noise.\nWe will be able to provide a more complete study in the final version of the paper.\n\n> - Missing related work\n\nThis is indeed a relevant paper, thank you for the suggestion. Training attractor models using implicit differentiation is a promising direction for future work.\n\n> - Large batch sizes for ImageNet\n\nThis work is very relevant, we now cite it.\n\n> - Mentioning Appendix D in the main paper\n\nThe appendix is now referenced.\n\n> - Related paper at NeurIPS this year \n\nWe agree it is a very relevant paper and we already cite it in the Related work section. To avoid any confusion, we would like to emphasize that our works differ in the interface of a memory module. In the MNM it is the key-value retrieval of non-structured vectors, while EBMM is focused on more association problems, e.g. where any part of the stored pattern can be used as a key. \n\n> - Comments on scalability\n\nScalability can be understood as a multi-dimensional concept. We made an improvement on the expressivity (by adopting modern deep learning techniques) and the speed (by utilizing gradient-based meta-learning) dimensions, but we do not demonstrate yet an advantage over slot-based memory in temporal tasks with incremental updates. Although our early experiments suggest that EBMM is more than viable in this setting, training on long sequences is less straightforward than traditional recurrent models. The papers you suggested as related work can be very helpful to improve on this dimension. We have refined Section 6 to reflect on this."}, "signatures": ["ICLR.cc/2020/Conference/Paper1629/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1629/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["bartunov@google.com", "jwrae@google.com", "osindero@google.com", "countzero@google.com"], "title": "Meta-Learning Deep Energy-Based Memory Models", "authors": ["Sergey Bartunov", "Jack Rae", "Simon Osindero", "Timothy Lillicrap"], "pdf": "/pdf/20b1e0f615c85a10a0c9c4e71de709dbb7f731a4.pdf", "TL;DR": "Deep associative memory models using arbitrary neural networks as a storage.", "abstract": "We study the problem of learning an associative memory model -- a system which is able to retrieve a remembered pattern based on its distorted or incomplete version.\nAttractor networks provide a sound model of associative memory: patterns are stored as attractors of the network dynamics and associative retrieval is performed by running the dynamics starting from a query pattern until it converges to an attractor. \nIn such models the dynamics are often implemented as an optimization procedure that minimizes an energy function, such as in the classical Hopfield network. \nIn general it is difficult to derive a writing rule for a given dynamics and energy that is both compressive and fast.\nThus, most research in energy-based memory has been limited either to tractable energy models not expressive enough to handle complex high-dimensional objects such as natural images, or to models that do not offer fast writing.\nWe present a novel meta-learning approach to energy-based memory models (EBMM) that allows one to use an arbitrary neural architecture as an energy model and quickly store patterns in its weights. \nWe demonstrate experimentally that our EBMM approach can build compressed memories for synthetic and natural data, and is capable of associative retrieval that outperforms existing memory systems in terms of the reconstruction error and compression rate.", "keywords": ["associative memory", "energy-based memory", "meta-learning", "compressive memory"], "paperhash": "bartunov|metalearning_deep_energybased_memory_models", "_bibtex": "@inproceedings{\nBartunov2020Meta-Learning,\ntitle={Meta-Learning Deep Energy-Based Memory Models},\nauthor={Sergey Bartunov and Jack Rae and Simon Osindero and Timothy Lillicrap},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SyljQyBFDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/3605041945b73708d75246e49b04ff1250ecfec6.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SyljQyBFDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1629/Authors", "ICLR.cc/2020/Conference/Paper1629/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1629/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1629/Reviewers", "ICLR.cc/2020/Conference/Paper1629/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1629/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1629/Authors|ICLR.cc/2020/Conference/Paper1629/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504153198, "tmdate": 1576860554573, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1629/Authors", "ICLR.cc/2020/Conference/Paper1629/Reviewers", "ICLR.cc/2020/Conference/Paper1629/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1629/-/Official_Comment"}}}, {"id": "rJgjRC1jor", "original": null, "number": 6, "cdate": 1573744339131, "ddate": null, "tcdate": 1573744339131, "tmdate": 1573744339131, "tddate": null, "forum": "SyljQyBFDH", "replyto": "B1e_L5zpKH", "invitation": "ICLR.cc/2020/Conference/Paper1629/-/Official_Comment", "content": {"title": "Reply", "comment": "We\u2019re pleased that the reviewer found the manuscript interesting, novel, and well-written. We have addressed each of the major concerns below.\n\n> 1) I am in general not really convinced about the supposed advantages of these attractor memory models (this paper and the earlier Kanerva machine) over more standard and much simpler approaches. \n\nThe point of our paper is not to manifest superiority of attractor- or energy-based models in any sense. We use this framework because it naturally allows us to 1) perform associative retrieval and 2) implement fast writing that is compatible with 1). \nWe agree that both denoising and variational autoencoder frameworks can implement associative retrieval in a some sense, however, only to the extent to which any prior model can do so. In the new Appendix A.3 we evaluate VAE, DAE and Deep Image Prior (which is state of the art denoising model) and find that they perform significantly worse than EBMM and other memory baselines under the relevant test conditions. Moreover, as we explain in the text, in settings where the importance of a prior is less than the importance of memory, these models will ultimately fail. A good example of such setting is the experiment with binary strings in Appendix A.2 where it is clear that since patterns are sampled from a uniform distribution, no prior would implement associative retrieval. \n\n> Note that in an autoencoder, reading (inference) is already fast. The authors might point out that writing (training) will not be fast, which is correct. However, the meta-learning phase proposed in this paper will also not be fast and perhaps the fair comparison should be between the meta-learning phase of this paper and the standard training phase of an autoencoder. \n\nIt is true that there is a relatively slow meta-learning phase required by our approach. However, unlike denoising autoencoders, once our model has finished training it can read and write very quickly. Autoencoders are capable of storing and denoising data, but require a new slow and expensive training procedure each time one wishes to store new data. Depending on the intended use case one may prefer one or the other approach. For example, variants of our EBMM approach may be useful in the case that we wish to quickly and cheaply store data into a relatively volatile memory of the recent past (e.g. as when training an RL agent). Our approach is not strictly better than autoencoders. Rather, each has potential use cases that depend on downstream goals. \n\nWith respect to the comment about model class it should be noted that we can take advantage of very large and arbitrarily configured networks which included inductive biases (e.g. convolutional structure) if desired.  Thus, our models can make use of large, structured, feedforward networks in the inner loop, and are thus able to learn about the rich structure in images and other complex data types.\n\n> 2) Currently, the paper only uses a specific type of \u201cblock-noise\u201d corruption. One thing that would be nice to see is some results with other noise models. \n\nAgreed. We have now run experiments and report results with a variety of different noise models, please refer to Appendix A.5.\n\n> 3) It would be good to say something about the meta-learned parameters, theta_bar, r, tau. Is there any meaningful structure in these parameters that distinguishes them from their random initial values? \n\nThese parameters have very different roles, so it is difficult to say which ones are more important. Theta_bar is the initialization for the writing process and meta-learning these is crucial, just as in any other MAML-like model. Note that \\bar{\\theta} is many orders of magnitude larger in size than r and \\tau. Indeed, as the initial configuration for our neural network, \\bar{\\theta} contains rich structured information about the data domain. \nUsing generic step size decay rules for \\gamma and \\eta works, but the stability of training greatly improves if these parameters are trained, see e.g. \u201cAntoniou, A., Edwards, H., & Storkey, A. (2018). How to train your MAML. arXiv preprint arXiv:1810.09502\u201d."}, "signatures": ["ICLR.cc/2020/Conference/Paper1629/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1629/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["bartunov@google.com", "jwrae@google.com", "osindero@google.com", "countzero@google.com"], "title": "Meta-Learning Deep Energy-Based Memory Models", "authors": ["Sergey Bartunov", "Jack Rae", "Simon Osindero", "Timothy Lillicrap"], "pdf": "/pdf/20b1e0f615c85a10a0c9c4e71de709dbb7f731a4.pdf", "TL;DR": "Deep associative memory models using arbitrary neural networks as a storage.", "abstract": "We study the problem of learning an associative memory model -- a system which is able to retrieve a remembered pattern based on its distorted or incomplete version.\nAttractor networks provide a sound model of associative memory: patterns are stored as attractors of the network dynamics and associative retrieval is performed by running the dynamics starting from a query pattern until it converges to an attractor. \nIn such models the dynamics are often implemented as an optimization procedure that minimizes an energy function, such as in the classical Hopfield network. \nIn general it is difficult to derive a writing rule for a given dynamics and energy that is both compressive and fast.\nThus, most research in energy-based memory has been limited either to tractable energy models not expressive enough to handle complex high-dimensional objects such as natural images, or to models that do not offer fast writing.\nWe present a novel meta-learning approach to energy-based memory models (EBMM) that allows one to use an arbitrary neural architecture as an energy model and quickly store patterns in its weights. \nWe demonstrate experimentally that our EBMM approach can build compressed memories for synthetic and natural data, and is capable of associative retrieval that outperforms existing memory systems in terms of the reconstruction error and compression rate.", "keywords": ["associative memory", "energy-based memory", "meta-learning", "compressive memory"], "paperhash": "bartunov|metalearning_deep_energybased_memory_models", "_bibtex": "@inproceedings{\nBartunov2020Meta-Learning,\ntitle={Meta-Learning Deep Energy-Based Memory Models},\nauthor={Sergey Bartunov and Jack Rae and Simon Osindero and Timothy Lillicrap},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SyljQyBFDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/3605041945b73708d75246e49b04ff1250ecfec6.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SyljQyBFDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1629/Authors", "ICLR.cc/2020/Conference/Paper1629/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1629/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1629/Reviewers", "ICLR.cc/2020/Conference/Paper1629/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1629/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1629/Authors|ICLR.cc/2020/Conference/Paper1629/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504153198, "tmdate": 1576860554573, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1629/Authors", "ICLR.cc/2020/Conference/Paper1629/Reviewers", "ICLR.cc/2020/Conference/Paper1629/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1629/-/Official_Comment"}}}, {"id": "SyeMLFkijB", "original": null, "number": 5, "cdate": 1573742921789, "ddate": null, "tcdate": 1573742921789, "tmdate": 1573742921789, "tddate": null, "forum": "SyljQyBFDH", "replyto": "HyekxA0EcS", "invitation": "ICLR.cc/2020/Conference/Paper1629/-/Official_Comment", "content": {"title": "Reply (continued)", "comment": "> 8- For the chosen tasks, I am curious to see the experimental comparison to deep image prior (Ulyanov, 2018). Deep image prior would be very similar to the read operator (although the gradient descent is over the parameter of model) without having write operations when you define the energy as MSE.\n\nWe agree that this indeed is a relevant baseline and we performed a series of experiments with non-memory baselines, including Deep Image Prior. Please refer to Appendix A.3 for the quantitative study. In short, they performed strictly worse than models with memory because while a prior model can produce a plausible reconstruction it is not very helpful for the task of exact recall. In the case of Deep Image Prior it is important to note that it requires privileged information about location of the occluded area (equation 6), while we work without this assumption. Without this information, the model gets confused even by the relatively simple salt and pepper distortion.\n\n> Typos and writing style:\n\nThank you! We have fixed all of these typos and style issues.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1629/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1629/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["bartunov@google.com", "jwrae@google.com", "osindero@google.com", "countzero@google.com"], "title": "Meta-Learning Deep Energy-Based Memory Models", "authors": ["Sergey Bartunov", "Jack Rae", "Simon Osindero", "Timothy Lillicrap"], "pdf": "/pdf/20b1e0f615c85a10a0c9c4e71de709dbb7f731a4.pdf", "TL;DR": "Deep associative memory models using arbitrary neural networks as a storage.", "abstract": "We study the problem of learning an associative memory model -- a system which is able to retrieve a remembered pattern based on its distorted or incomplete version.\nAttractor networks provide a sound model of associative memory: patterns are stored as attractors of the network dynamics and associative retrieval is performed by running the dynamics starting from a query pattern until it converges to an attractor. \nIn such models the dynamics are often implemented as an optimization procedure that minimizes an energy function, such as in the classical Hopfield network. \nIn general it is difficult to derive a writing rule for a given dynamics and energy that is both compressive and fast.\nThus, most research in energy-based memory has been limited either to tractable energy models not expressive enough to handle complex high-dimensional objects such as natural images, or to models that do not offer fast writing.\nWe present a novel meta-learning approach to energy-based memory models (EBMM) that allows one to use an arbitrary neural architecture as an energy model and quickly store patterns in its weights. \nWe demonstrate experimentally that our EBMM approach can build compressed memories for synthetic and natural data, and is capable of associative retrieval that outperforms existing memory systems in terms of the reconstruction error and compression rate.", "keywords": ["associative memory", "energy-based memory", "meta-learning", "compressive memory"], "paperhash": "bartunov|metalearning_deep_energybased_memory_models", "_bibtex": "@inproceedings{\nBartunov2020Meta-Learning,\ntitle={Meta-Learning Deep Energy-Based Memory Models},\nauthor={Sergey Bartunov and Jack Rae and Simon Osindero and Timothy Lillicrap},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SyljQyBFDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/3605041945b73708d75246e49b04ff1250ecfec6.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SyljQyBFDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1629/Authors", "ICLR.cc/2020/Conference/Paper1629/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1629/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1629/Reviewers", "ICLR.cc/2020/Conference/Paper1629/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1629/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1629/Authors|ICLR.cc/2020/Conference/Paper1629/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504153198, "tmdate": 1576860554573, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1629/Authors", "ICLR.cc/2020/Conference/Paper1629/Reviewers", "ICLR.cc/2020/Conference/Paper1629/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1629/-/Official_Comment"}}}, {"id": "HJeE4YJijS", "original": null, "number": 4, "cdate": 1573742892495, "ddate": null, "tcdate": 1573742892495, "tmdate": 1573742892495, "tddate": null, "forum": "SyljQyBFDH", "replyto": "HyekxA0EcS", "invitation": "ICLR.cc/2020/Conference/Paper1629/-/Official_Comment", "content": {"title": "Reply", "comment": "> equations (1) - (5)\n\nYes. Another way to view our approach is as follows: we meta-learn a set of initial parameters for our neural network, \\bar{\\theta}. These initial parameters correspond to an initial energy landscape. Meta-learning insures that from this point in parameter space it is easy to make only a handful of gradient updates to produce a new energy landscape that effectively stores a new batch of data into memory. Once they are stored, it is possible to retrieve memories by inputting a query and then descending the energy function to retrieve an associated memory.\n\n> 1- The connection to meta-learning is unclear in your experiments. Can you elaborate on that? \n\nMeta-learning is used in our model to learn a good set of starting parameters \\theta, from which it is easy to quickly write memories into a network via gradient descent. \nAnother way to say this is: In the outer learning loop we learn the initial parameters \\bar{\\theta}, and in the inner loop we optimize the parameters to minimize the writing loss for the memories we want to store for the current batch/episode. Thus, the model learns to get good at quickly learning (or storing) new memories.\n\n> 2- The expectation in eq 5 is over different input patterns, which I assume that a set of input patterns belong to a task. What is that you write in memory?  For each experiment, what are the different input patterns (tasks) that you have written in the memory?\n\nOur explanation of this process was confusing. Thank you for pointing out the issue here. We have fixed the explanation and mathematical notation around equations 4 & 5 to make this easier to follow. We refer the reviewer to this updated section of the text for a detailed explanation.\nBriefly, we write a batch of N patterns into memory. These N patterns are sampled randomly from a larger dataset of training (or testing - during evaluation) patterns. Then we construct a reconstruction loss as a squared difference between the originally stored patterns and patterns retrieved from randomly distorted queries. This loss can now be used to compute a stochastic gradient that updates all parameters (theta_bar, r and tau).\n\n> 3- What is the \\theta that is feed to the read function at the test time? \n\nThe \\theta used by the read function at test time is created as follows:\nWe start with the parameters of the model that have been meta-learned: \\bar{\\theta}.\nWe update these parameters using the batch of data to be stored using the writing procedure given by  eq 4: \\theta^{T}\nWe then test what the model can remember by querying it via the read operation.\nCrucially, the data stored via step #2 has never been seen at test time.\n\n> Are you testing on the tasks that you already trained on?\n\nWe test on a set of held-out data, respecting the original train/test splits in Omniglot and ImageNet. The general task of storing data and retrieving corrupted examples is consistent during training and test.\n\n> How this approach generalizes to unseen (or relatively close) task? \n\nAs with many deep learning approaches, our models can capture the underlying statistics of the kinds of data that they are trained on (e.g. the structure of natural images as in the case of ImageNet). Our model learns general initial network parameters \\bar{\\theta} from which it can quickly and easily store data in a compressed format. \nIn the new Appendix A.6 we verify that the Omniglot model successfully transfers to MNIST data.\n\n> 6- Can it recover any query that is not constructed with respect to the distortion model that is trained on? or what happens if the distorted image at test times comes from a different distortion model? (image blocking, for example)\n\nAs we show in the new Appendix A.5 - to some extent, yes. The model perfectly generalizes to smaller levels of noise and performs reasonably well with larger levels. We did not observe generalization to a different distortion model, which would indeed be a nice property, but such generalization is arguably difficult to expect with respect a distortion model completely unknown during the training. \nNote that it is straightforward to use our approach to learn more general storage procedures by training across a distribution of distortion models.\n\n> 7- How many distorted samples are used for training?\n\nWe trained the model for at most 2 million iterations for all of our experiments (Appendix A). For the ImageNet experiment we trained with 32 images per iteration, so the model trained on 64M distorted images."}, "signatures": ["ICLR.cc/2020/Conference/Paper1629/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1629/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["bartunov@google.com", "jwrae@google.com", "osindero@google.com", "countzero@google.com"], "title": "Meta-Learning Deep Energy-Based Memory Models", "authors": ["Sergey Bartunov", "Jack Rae", "Simon Osindero", "Timothy Lillicrap"], "pdf": "/pdf/20b1e0f615c85a10a0c9c4e71de709dbb7f731a4.pdf", "TL;DR": "Deep associative memory models using arbitrary neural networks as a storage.", "abstract": "We study the problem of learning an associative memory model -- a system which is able to retrieve a remembered pattern based on its distorted or incomplete version.\nAttractor networks provide a sound model of associative memory: patterns are stored as attractors of the network dynamics and associative retrieval is performed by running the dynamics starting from a query pattern until it converges to an attractor. \nIn such models the dynamics are often implemented as an optimization procedure that minimizes an energy function, such as in the classical Hopfield network. \nIn general it is difficult to derive a writing rule for a given dynamics and energy that is both compressive and fast.\nThus, most research in energy-based memory has been limited either to tractable energy models not expressive enough to handle complex high-dimensional objects such as natural images, or to models that do not offer fast writing.\nWe present a novel meta-learning approach to energy-based memory models (EBMM) that allows one to use an arbitrary neural architecture as an energy model and quickly store patterns in its weights. \nWe demonstrate experimentally that our EBMM approach can build compressed memories for synthetic and natural data, and is capable of associative retrieval that outperforms existing memory systems in terms of the reconstruction error and compression rate.", "keywords": ["associative memory", "energy-based memory", "meta-learning", "compressive memory"], "paperhash": "bartunov|metalearning_deep_energybased_memory_models", "_bibtex": "@inproceedings{\nBartunov2020Meta-Learning,\ntitle={Meta-Learning Deep Energy-Based Memory Models},\nauthor={Sergey Bartunov and Jack Rae and Simon Osindero and Timothy Lillicrap},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SyljQyBFDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/3605041945b73708d75246e49b04ff1250ecfec6.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SyljQyBFDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1629/Authors", "ICLR.cc/2020/Conference/Paper1629/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1629/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1629/Reviewers", "ICLR.cc/2020/Conference/Paper1629/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1629/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1629/Authors|ICLR.cc/2020/Conference/Paper1629/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504153198, "tmdate": 1576860554573, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1629/Authors", "ICLR.cc/2020/Conference/Paper1629/Reviewers", "ICLR.cc/2020/Conference/Paper1629/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1629/-/Official_Comment"}}}, {"id": "ryxTh1ncjr", "original": null, "number": 3, "cdate": 1573728180699, "ddate": null, "tcdate": 1573728180699, "tmdate": 1573728180699, "tddate": null, "forum": "SyljQyBFDH", "replyto": "Hye1YmQTqr", "invitation": "ICLR.cc/2020/Conference/Paper1629/-/Official_Comment", "content": {"title": "Reply", "comment": "Thank you for your review. We agree that a discussion about our model and modern models such as GANs and VAEs was somewhat missing in our initial submission. We have added a paragraph in the Related work section to better position EBMM within modern deep learning and also performed a comparison with a number of non-memory baselines (Appendix A.3). We hope this confirms both conceptual and empirical contributions of our paper."}, "signatures": ["ICLR.cc/2020/Conference/Paper1629/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1629/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["bartunov@google.com", "jwrae@google.com", "osindero@google.com", "countzero@google.com"], "title": "Meta-Learning Deep Energy-Based Memory Models", "authors": ["Sergey Bartunov", "Jack Rae", "Simon Osindero", "Timothy Lillicrap"], "pdf": "/pdf/20b1e0f615c85a10a0c9c4e71de709dbb7f731a4.pdf", "TL;DR": "Deep associative memory models using arbitrary neural networks as a storage.", "abstract": "We study the problem of learning an associative memory model -- a system which is able to retrieve a remembered pattern based on its distorted or incomplete version.\nAttractor networks provide a sound model of associative memory: patterns are stored as attractors of the network dynamics and associative retrieval is performed by running the dynamics starting from a query pattern until it converges to an attractor. \nIn such models the dynamics are often implemented as an optimization procedure that minimizes an energy function, such as in the classical Hopfield network. \nIn general it is difficult to derive a writing rule for a given dynamics and energy that is both compressive and fast.\nThus, most research in energy-based memory has been limited either to tractable energy models not expressive enough to handle complex high-dimensional objects such as natural images, or to models that do not offer fast writing.\nWe present a novel meta-learning approach to energy-based memory models (EBMM) that allows one to use an arbitrary neural architecture as an energy model and quickly store patterns in its weights. \nWe demonstrate experimentally that our EBMM approach can build compressed memories for synthetic and natural data, and is capable of associative retrieval that outperforms existing memory systems in terms of the reconstruction error and compression rate.", "keywords": ["associative memory", "energy-based memory", "meta-learning", "compressive memory"], "paperhash": "bartunov|metalearning_deep_energybased_memory_models", "_bibtex": "@inproceedings{\nBartunov2020Meta-Learning,\ntitle={Meta-Learning Deep Energy-Based Memory Models},\nauthor={Sergey Bartunov and Jack Rae and Simon Osindero and Timothy Lillicrap},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SyljQyBFDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/3605041945b73708d75246e49b04ff1250ecfec6.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SyljQyBFDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1629/Authors", "ICLR.cc/2020/Conference/Paper1629/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1629/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1629/Reviewers", "ICLR.cc/2020/Conference/Paper1629/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1629/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1629/Authors|ICLR.cc/2020/Conference/Paper1629/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504153198, "tmdate": 1576860554573, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1629/Authors", "ICLR.cc/2020/Conference/Paper1629/Reviewers", "ICLR.cc/2020/Conference/Paper1629/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1629/-/Official_Comment"}}}, {"id": "SJgY4RocoH", "original": null, "number": 2, "cdate": 1573727792631, "ddate": null, "tcdate": 1573727792631, "tmdate": 1573727792631, "tddate": null, "forum": "SyljQyBFDH", "replyto": "SyljQyBFDH", "invitation": "ICLR.cc/2020/Conference/Paper1629/-/Official_Comment", "content": {"title": "From authors", "comment": "We would like to thank our reviewers for their time and valuable feedback, many of the comments helped us to improve the paper and obtain more results. We will soon be replying directly to each of the reviewers. Some of the requested experiments are still running and we will be updating the paper with new results.\nWe believe we positively addressed most of the feedback and ask the reviewers to assess our replies.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1629/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1629/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["bartunov@google.com", "jwrae@google.com", "osindero@google.com", "countzero@google.com"], "title": "Meta-Learning Deep Energy-Based Memory Models", "authors": ["Sergey Bartunov", "Jack Rae", "Simon Osindero", "Timothy Lillicrap"], "pdf": "/pdf/20b1e0f615c85a10a0c9c4e71de709dbb7f731a4.pdf", "TL;DR": "Deep associative memory models using arbitrary neural networks as a storage.", "abstract": "We study the problem of learning an associative memory model -- a system which is able to retrieve a remembered pattern based on its distorted or incomplete version.\nAttractor networks provide a sound model of associative memory: patterns are stored as attractors of the network dynamics and associative retrieval is performed by running the dynamics starting from a query pattern until it converges to an attractor. \nIn such models the dynamics are often implemented as an optimization procedure that minimizes an energy function, such as in the classical Hopfield network. \nIn general it is difficult to derive a writing rule for a given dynamics and energy that is both compressive and fast.\nThus, most research in energy-based memory has been limited either to tractable energy models not expressive enough to handle complex high-dimensional objects such as natural images, or to models that do not offer fast writing.\nWe present a novel meta-learning approach to energy-based memory models (EBMM) that allows one to use an arbitrary neural architecture as an energy model and quickly store patterns in its weights. \nWe demonstrate experimentally that our EBMM approach can build compressed memories for synthetic and natural data, and is capable of associative retrieval that outperforms existing memory systems in terms of the reconstruction error and compression rate.", "keywords": ["associative memory", "energy-based memory", "meta-learning", "compressive memory"], "paperhash": "bartunov|metalearning_deep_energybased_memory_models", "_bibtex": "@inproceedings{\nBartunov2020Meta-Learning,\ntitle={Meta-Learning Deep Energy-Based Memory Models},\nauthor={Sergey Bartunov and Jack Rae and Simon Osindero and Timothy Lillicrap},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SyljQyBFDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/3605041945b73708d75246e49b04ff1250ecfec6.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SyljQyBFDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1629/Authors", "ICLR.cc/2020/Conference/Paper1629/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1629/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1629/Reviewers", "ICLR.cc/2020/Conference/Paper1629/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1629/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1629/Authors|ICLR.cc/2020/Conference/Paper1629/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504153198, "tmdate": 1576860554573, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1629/Authors", "ICLR.cc/2020/Conference/Paper1629/Reviewers", "ICLR.cc/2020/Conference/Paper1629/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1629/-/Official_Comment"}}}, {"id": "BkeZXMcccB", "original": null, "number": 3, "cdate": 1572672024562, "ddate": null, "tcdate": 1572672024562, "tmdate": 1573047954587, "tddate": null, "forum": "SyljQyBFDH", "replyto": "SyljQyBFDH", "invitation": "ICLR.cc/2020/Conference/Paper1629/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #4", "review": "*Summary:*\n\nThe authors propose to tackle the associative memory problem by recasting read/write operations to read/write by optimizing the parameters/input of an energy based model. Writing is reformulated as training a parametric energy model (EBMM) to have local minima of energy w.r.t. the parameters at memorized data points. Reading is performed by performing (projected) gradient descent on the corrupted/incomplete input to minimize the energy. To ensure the operations are fast (read and write with minimal gradient steps), the authors propose to take inspiration from modern meta-learning literature and learn initialization parameters of the energy model (and other hyperparameters for GD during read/write) from which writing is fast while ensuring reading is also fast, since the models are trained to maximize read/write performance within a constrained number of gradient steps.  Experimentally, the authors show that EBMM reading performs similar to baseline methods (but better across many memory sizes) on the standard Omniglot task. On CIFAR-10 and downsized ImageNet, they show much better L2 reconstruction error of corrupted images. They also show that the learnt energy \n\n*Recommendation:*\n\nI believe this is a very neat idea, and utilizes large parametric models for \"smart\" overfitting and compression of data for the associative memory task. The proposed meta-learning approach to training the model seems to perform well across multiple simple and challenging datasets, and therefore I would recommend accept. My current recommendation is very borderline (weak accept) because of a lack of some experimental rigour (which I would love clarifications on), and missing related work, which I mention below.\n\n*Discussion Points and Concerns from the Reviewer:*\n\n- Dataset / batching details \nPlease mention how the datasets were split for training and testing the models. How much training data is utilized to meta-learn the EBMM initialization? How is batching performed? I believe these details are very important to mention in the paper for reproducibility of results. \n\nAre there any correlations in the batch selection? Can you evaluate how good the associative memory performs across different correlation levels in the batch (A well learnt algorithm should demonstrate better reconstruction at lower memory levels for correlated batches). \n\n- Experiments across multiple SNR and generalization on noise patterns\nThe authors mention at the beginning of Section 4 that a random block is corrupted, but in the end the experiments are done on a constant corruption size on the CIFAR and ImageNet images. How do the models perform across different signal-to-noise ratios? Similarly, the model is trained on simple noise patterns \n\n- Missing related work\nThere is related work [1] in learning in Hopfield Networks using the implicit function theorem and finding stationary points of the dynamics. This work is not mentioned in the paper, and is a valid baseline for this paper as well.\n\n- Mentioning Appendix D in the main paper\nAppendix D is not mentioned in the main paper and has a short discussion on the mismatch between the reading process and the writing loss during meta-training. It also mentions additional tricks required for the training, and I believe it should be mentioned in the main paper like other sections are appropriately referenced. \n\n- Large batch sizes for ImageNet\nWork from [2] can be utilized to backpropagate through very long optimization sequences and therefore can be utilized to train with larger batch sizes in the ImageNet example. It is important to see how the small model utilized for ImageNet works to compress higher batch sizes, as that is one of the major practical issues with the algorithm.\n\n- Related paper at NeurIPS this year \n[3] is a related paper from Neurips this year, which the authors could consider adding as contemporary work\n\n- Comments on scalability\nThe associative memory papers have often been criticized for lack of scalability, and I think the authors make progress towards making this better with the use of unconstrained energy models in the learning process. It would be nice to have a discussion of the scalability from the authors, highlighting issues in the current model and future directions\n\nReferences:\n[1] Reviving and Improving Recurrent Back-Propagation, ICML '18\n[2] Gradient-based Hyperparameter Optimization through Reversible Learning, Maclaurin et al. ICML '15\n[3] Metalearned Neural Memory, Munkhdalai et al. NeurIPS '19", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper1629/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1629/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["bartunov@google.com", "jwrae@google.com", "osindero@google.com", "countzero@google.com"], "title": "Meta-Learning Deep Energy-Based Memory Models", "authors": ["Sergey Bartunov", "Jack Rae", "Simon Osindero", "Timothy Lillicrap"], "pdf": "/pdf/20b1e0f615c85a10a0c9c4e71de709dbb7f731a4.pdf", "TL;DR": "Deep associative memory models using arbitrary neural networks as a storage.", "abstract": "We study the problem of learning an associative memory model -- a system which is able to retrieve a remembered pattern based on its distorted or incomplete version.\nAttractor networks provide a sound model of associative memory: patterns are stored as attractors of the network dynamics and associative retrieval is performed by running the dynamics starting from a query pattern until it converges to an attractor. \nIn such models the dynamics are often implemented as an optimization procedure that minimizes an energy function, such as in the classical Hopfield network. \nIn general it is difficult to derive a writing rule for a given dynamics and energy that is both compressive and fast.\nThus, most research in energy-based memory has been limited either to tractable energy models not expressive enough to handle complex high-dimensional objects such as natural images, or to models that do not offer fast writing.\nWe present a novel meta-learning approach to energy-based memory models (EBMM) that allows one to use an arbitrary neural architecture as an energy model and quickly store patterns in its weights. \nWe demonstrate experimentally that our EBMM approach can build compressed memories for synthetic and natural data, and is capable of associative retrieval that outperforms existing memory systems in terms of the reconstruction error and compression rate.", "keywords": ["associative memory", "energy-based memory", "meta-learning", "compressive memory"], "paperhash": "bartunov|metalearning_deep_energybased_memory_models", "_bibtex": "@inproceedings{\nBartunov2020Meta-Learning,\ntitle={Meta-Learning Deep Energy-Based Memory Models},\nauthor={Sergey Bartunov and Jack Rae and Simon Osindero and Timothy Lillicrap},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SyljQyBFDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/3605041945b73708d75246e49b04ff1250ecfec6.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SyljQyBFDH", "replyto": "SyljQyBFDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1629/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1629/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575852608908, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1629/Reviewers"], "noninvitees": [], "tcdate": 1570237734599, "tmdate": 1575852608922, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1629/-/Official_Review"}}}, {"id": "Hye1YmQTqr", "original": null, "number": 4, "cdate": 1572840311090, "ddate": null, "tcdate": 1572840311090, "tmdate": 1572972443828, "tddate": null, "forum": "SyljQyBFDH", "replyto": "SyljQyBFDH", "invitation": "ICLR.cc/2020/Conference/Paper1629/-/Official_Review", "content": {"rating": "6: Weak Accept", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "Summary:\nThis paper proposes a new type of energy-based models, a class of non-normalized generative models that relies on an energy function to retrieve patterns that correspond to its minima. The goal that is tackled by the authors is to implement an associative memory system, i.e. a mechanism that is able to retrieve any one of a set of patterns, given a distorted copy of these patterns. This task is traditionally carried out using attractor neural networks like the Hopfield model, a recurrent neural network model endowed with a learning rule that allows it to quickly embed a given set of patterns in its weight matrix such that the patterns become stable fix points of its dynamics. As the authors point out though, models like the Hopfield model are limited in their capacity to assimilate attractor patterns and in term of their expressiveness. On the other hand, more complex models based on deep architectures trained with gradient descent are slow at updating their weights to create new attractors.\nThe authors propose a new method to make up for the weaknesses of these two approaches. Their method is based on meta-learning, and in short consists in meta-training an energy function parametrized as a neural network such that  executing a write dynamics on the weights results in a model whose read dynamics (a gradient descent on the energy function) is able to denoise distorted inputs and retrieve the original ones. In practice, the write dynamics is obtained as a gradient descent procedure on a writing loss (which is itself dependent on the energy function) as a function of the weights. The meta-learning procedure minimizes the discrepancy between the original patterns and the retrieved ones by optimizing end-to-end the learning schedule parameters and initial conditions of the weights, analogously to gradient-based meta-learning methods like MAML.\nThe authors then carry out a series of experiments to check that their model is competitive with Memory-Augmented Neural Network (MANN) and Memory Networks (MemNets) in retrieving samples from Omniglot, CIFAR and ImageNet, in terms of retrieving abilities for a given memory size. In the supplementary material section they in addition compare their model's performance against the Hopfield model and recurrent networks on the classical toy task of retrieving random binary patterns, also with good results for the new model.\n\nDecision:\nThis paper is very clearly and compactly written. The idea of training an energy-based model through gradient-based meta-learning seems novel and innovative. \nOne thing that the the paper is arguably missing, is a convincing motivation section for focusing on energy-based models. The panorama of generative models has radically changed since attractor neural networks and energy-based models were first introduced. At the time powerful methods like variational autoencoders, normalizing flows and GANs didn't exist. But nowadays, one could arguably expect that energy-based models should be contextualized and motivated in the perspective of comparing them with these new breeds of deep generative models. I am absolutely not suggesting that the authors should providing experimental comparisons between their models and GAN or VAE, but simply that they compare them to their style of generative modeling in terms of advantages, disadvantages, use cases, and potential for applications."}, "signatures": ["ICLR.cc/2020/Conference/Paper1629/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1629/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["bartunov@google.com", "jwrae@google.com", "osindero@google.com", "countzero@google.com"], "title": "Meta-Learning Deep Energy-Based Memory Models", "authors": ["Sergey Bartunov", "Jack Rae", "Simon Osindero", "Timothy Lillicrap"], "pdf": "/pdf/20b1e0f615c85a10a0c9c4e71de709dbb7f731a4.pdf", "TL;DR": "Deep associative memory models using arbitrary neural networks as a storage.", "abstract": "We study the problem of learning an associative memory model -- a system which is able to retrieve a remembered pattern based on its distorted or incomplete version.\nAttractor networks provide a sound model of associative memory: patterns are stored as attractors of the network dynamics and associative retrieval is performed by running the dynamics starting from a query pattern until it converges to an attractor. \nIn such models the dynamics are often implemented as an optimization procedure that minimizes an energy function, such as in the classical Hopfield network. \nIn general it is difficult to derive a writing rule for a given dynamics and energy that is both compressive and fast.\nThus, most research in energy-based memory has been limited either to tractable energy models not expressive enough to handle complex high-dimensional objects such as natural images, or to models that do not offer fast writing.\nWe present a novel meta-learning approach to energy-based memory models (EBMM) that allows one to use an arbitrary neural architecture as an energy model and quickly store patterns in its weights. \nWe demonstrate experimentally that our EBMM approach can build compressed memories for synthetic and natural data, and is capable of associative retrieval that outperforms existing memory systems in terms of the reconstruction error and compression rate.", "keywords": ["associative memory", "energy-based memory", "meta-learning", "compressive memory"], "paperhash": "bartunov|metalearning_deep_energybased_memory_models", "_bibtex": "@inproceedings{\nBartunov2020Meta-Learning,\ntitle={Meta-Learning Deep Energy-Based Memory Models},\nauthor={Sergey Bartunov and Jack Rae and Simon Osindero and Timothy Lillicrap},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SyljQyBFDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/3605041945b73708d75246e49b04ff1250ecfec6.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SyljQyBFDH", "replyto": "SyljQyBFDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1629/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1629/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575852608908, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1629/Reviewers"], "noninvitees": [], "tcdate": 1570237734599, "tmdate": 1575852608922, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1629/-/Official_Review"}}}], "count": 15}