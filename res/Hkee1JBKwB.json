{"notes": [{"id": "Hkee1JBKwB", "original": "rkg4wiquvS", "number": 1454, "cdate": 1569439447644, "ddate": null, "tcdate": 1569439447644, "tmdate": 1577168276913, "tddate": null, "forum": "Hkee1JBKwB", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Convolutional Tensor-Train LSTM for Long-Term Video Prediction", "authors": ["Jiahao Su", "Wonmin Byeon", "Furong Huang", "Jan Kautz", "Animashree Anandkumar"], "authorids": ["jiahaosu@terpmail.umd.edu", "wonmin.byeon@gmail.com", "furongh@cs.umd.edu", "jkautz@nvidia.com", "animakumar@gmail.com"], "keywords": ["Tensor decomposition", "Video prediction"], "TL;DR": "we propose convolutional tensor-train LSTM,  which learns higher-order Convolutional LSTM efficiently using convolutional tensor-train decomposition. ", "abstract": "Long-term video prediction is highly challenging since it entails simultaneously capturing spatial and temporal information across a long range of image frames.Standard recurrent models are ineffective since they are prone to error propagation and cannot effectively capture higher-order correlations. A potential solution is to extend to higher-order spatio-temporal recurrent models. However, such a model requires  a  large number of parameters and operations, making it intractable  to learn in practice and is prone to overfitting. In this work, we propose convolutional tensor-train LSTM (Conv-TT-LSTM), which  learns higher-orderConvolutional LSTM (ConvLSTM) efficiently using convolutional  tensor-train decomposition (CTTD). Our proposed model naturally incorporates higher-order spatio-temporal information at a small cost of memory and computation by using efficient low-rank tensor representations. We evaluate our model on Moving-MNIST and KTH datasets and show improvements over standard ConvLSTM and better/comparable results to other ConvLSTM-based approaches, but with much fewer parameters.", "pdf": "/pdf/921c7a245bef64ed33d4cc827be4db59c547fbac.pdf", "paperhash": "su|convolutional_tensortrain_lstm_for_longterm_video_prediction", "original_pdf": "/attachment/3c12925aa4c470d88168cda664b5b82f88478033.pdf", "_bibtex": "@misc{\nsu2020convolutional,\ntitle={Convolutional Tensor-Train {\\{}LSTM{\\}} for Long-Term Video Prediction},\nauthor={Jiahao Su and Wonmin Byeon and Furong Huang and Jan Kautz and Animashree Anandkumar},\nyear={2020},\nurl={https://openreview.net/forum?id=Hkee1JBKwB}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "ONZA6dxbvd", "original": null, "number": 1, "cdate": 1576798723705, "ddate": null, "tcdate": 1576798723705, "tmdate": 1576800912829, "tddate": null, "forum": "Hkee1JBKwB", "replyto": "Hkee1JBKwB", "invitation": "ICLR.cc/2020/Conference/Paper1454/-/Decision", "content": {"decision": "Reject", "comment": "This paper proposes Conv-TT-LSTM for long-term video prediction. The proposed method saves memory and computation by low-rank tensor representations via tensor decomposition and is evaluated in Moving MNIST and KTH datasets.\n\nAll reviews argue that the novelty of the paper does not meet the standard of ICLR. In the rebuttal, the authors polish the experiment design, which fails to change any reviewer\u2019s decision.\n\nOverall, the paper is not good enough for ICLR.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Convolutional Tensor-Train LSTM for Long-Term Video Prediction", "authors": ["Jiahao Su", "Wonmin Byeon", "Furong Huang", "Jan Kautz", "Animashree Anandkumar"], "authorids": ["jiahaosu@terpmail.umd.edu", "wonmin.byeon@gmail.com", "furongh@cs.umd.edu", "jkautz@nvidia.com", "animakumar@gmail.com"], "keywords": ["Tensor decomposition", "Video prediction"], "TL;DR": "we propose convolutional tensor-train LSTM,  which learns higher-order Convolutional LSTM efficiently using convolutional tensor-train decomposition. ", "abstract": "Long-term video prediction is highly challenging since it entails simultaneously capturing spatial and temporal information across a long range of image frames.Standard recurrent models are ineffective since they are prone to error propagation and cannot effectively capture higher-order correlations. A potential solution is to extend to higher-order spatio-temporal recurrent models. However, such a model requires  a  large number of parameters and operations, making it intractable  to learn in practice and is prone to overfitting. In this work, we propose convolutional tensor-train LSTM (Conv-TT-LSTM), which  learns higher-orderConvolutional LSTM (ConvLSTM) efficiently using convolutional  tensor-train decomposition (CTTD). Our proposed model naturally incorporates higher-order spatio-temporal information at a small cost of memory and computation by using efficient low-rank tensor representations. We evaluate our model on Moving-MNIST and KTH datasets and show improvements over standard ConvLSTM and better/comparable results to other ConvLSTM-based approaches, but with much fewer parameters.", "pdf": "/pdf/921c7a245bef64ed33d4cc827be4db59c547fbac.pdf", "paperhash": "su|convolutional_tensortrain_lstm_for_longterm_video_prediction", "original_pdf": "/attachment/3c12925aa4c470d88168cda664b5b82f88478033.pdf", "_bibtex": "@misc{\nsu2020convolutional,\ntitle={Convolutional Tensor-Train {\\{}LSTM{\\}} for Long-Term Video Prediction},\nauthor={Jiahao Su and Wonmin Byeon and Furong Huang and Jan Kautz and Animashree Anandkumar},\nyear={2020},\nurl={https://openreview.net/forum?id=Hkee1JBKwB}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "Hkee1JBKwB", "replyto": "Hkee1JBKwB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795715939, "tmdate": 1576800265965, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1454/-/Decision"}}}, {"id": "S1l4Ac4hor", "original": null, "number": 4, "cdate": 1573829323916, "ddate": null, "tcdate": 1573829323916, "tmdate": 1573829323916, "tddate": null, "forum": "Hkee1JBKwB", "replyto": "HyegM4fo9H", "invitation": "ICLR.cc/2020/Conference/Paper1454/-/Official_Comment", "content": {"title": "Response to Official Blind Review #4", "comment": "Thank you very much for your thoughtful comments.\n\n(1) Novelty of our work.\n(a) Yu et al. 2017 shows that higher-order models (compressed by standard tensor-train decomposition) perform better than first-order models in synthetic regression problems. However, their approach can not be easily extended to video prediction, since standard tensor-train cannot cope with the essential convolutional operations. We perform an ablation study regarding the necessity of convolutions in Tensor-Train (requested by Reviewer 2). The training is not finished yet, but the validation curve ( https://postimg.cc/vx6rGnbH ) shows that our higher-order model with Convolutional Tensor-Train is an important part of our proposed method.\n\n(2) Comparison against PredRNN++ (especially in terms of MSE).\nWe added updated results in the revised version (Table 2 and 4). We found that our method produces sharper predictions, but MSE scores are lower (see Fig. 3, 4, 6-11). Therefore, we decided to add another metric, LPIPS, which better represents human perception. We discuss this issue in the experiment section. In the end, our methods outperform PredRNN++ on both SSIM and LPIPS metrics. \n\n(3) Necessity of model compression for higher-order models. \nFor higher-order spatio-temporal models (such as higher-order Conv-LSTM considered in our paper), the parameters grow exponentially with the order, therefore the higher-order models cannot be built without model compression. In this paper, we show that even heavily compressed higher-order models can have better performance than (uncompressed) first-order models.\n\n[a] Zhang, Richard, et al. \"The unreasonable effectiveness of deep features as a perceptual metric.\" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018."}, "signatures": ["ICLR.cc/2020/Conference/Paper1454/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1454/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Convolutional Tensor-Train LSTM for Long-Term Video Prediction", "authors": ["Jiahao Su", "Wonmin Byeon", "Furong Huang", "Jan Kautz", "Animashree Anandkumar"], "authorids": ["jiahaosu@terpmail.umd.edu", "wonmin.byeon@gmail.com", "furongh@cs.umd.edu", "jkautz@nvidia.com", "animakumar@gmail.com"], "keywords": ["Tensor decomposition", "Video prediction"], "TL;DR": "we propose convolutional tensor-train LSTM,  which learns higher-order Convolutional LSTM efficiently using convolutional tensor-train decomposition. ", "abstract": "Long-term video prediction is highly challenging since it entails simultaneously capturing spatial and temporal information across a long range of image frames.Standard recurrent models are ineffective since they are prone to error propagation and cannot effectively capture higher-order correlations. A potential solution is to extend to higher-order spatio-temporal recurrent models. However, such a model requires  a  large number of parameters and operations, making it intractable  to learn in practice and is prone to overfitting. In this work, we propose convolutional tensor-train LSTM (Conv-TT-LSTM), which  learns higher-orderConvolutional LSTM (ConvLSTM) efficiently using convolutional  tensor-train decomposition (CTTD). Our proposed model naturally incorporates higher-order spatio-temporal information at a small cost of memory and computation by using efficient low-rank tensor representations. We evaluate our model on Moving-MNIST and KTH datasets and show improvements over standard ConvLSTM and better/comparable results to other ConvLSTM-based approaches, but with much fewer parameters.", "pdf": "/pdf/921c7a245bef64ed33d4cc827be4db59c547fbac.pdf", "paperhash": "su|convolutional_tensortrain_lstm_for_longterm_video_prediction", "original_pdf": "/attachment/3c12925aa4c470d88168cda664b5b82f88478033.pdf", "_bibtex": "@misc{\nsu2020convolutional,\ntitle={Convolutional Tensor-Train {\\{}LSTM{\\}} for Long-Term Video Prediction},\nauthor={Jiahao Su and Wonmin Byeon and Furong Huang and Jan Kautz and Animashree Anandkumar},\nyear={2020},\nurl={https://openreview.net/forum?id=Hkee1JBKwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Hkee1JBKwB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1454/Authors", "ICLR.cc/2020/Conference/Paper1454/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1454/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1454/Reviewers", "ICLR.cc/2020/Conference/Paper1454/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1454/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1454/Authors|ICLR.cc/2020/Conference/Paper1454/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504155774, "tmdate": 1576860536775, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1454/Authors", "ICLR.cc/2020/Conference/Paper1454/Reviewers", "ICLR.cc/2020/Conference/Paper1454/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1454/-/Official_Comment"}}}, {"id": "BJeqsFEhsS", "original": null, "number": 3, "cdate": 1573829026286, "ddate": null, "tcdate": 1573829026286, "tmdate": 1573829069583, "tddate": null, "forum": "Hkee1JBKwB", "replyto": "rkgc4-Cy5S", "invitation": "ICLR.cc/2020/Conference/Paper1454/-/Official_Comment", "content": {"title": "Response to Official Blind Review #2", "comment": "Thank you for your efforts in reviewing our submission and your valuable suggestions of ablation studies. \n\n(A) Comparison to [Yang et al 2017] and necessity of Conv-LSTM (ablation studies (1) and (2)).\nThe problem of video prediction is considerably more difficult than the one of video classification tackled in Yang et al, 2017: while only a single label is returned in video classification, video prediction requires producing all pixels for future frames. The work Yang et al, 2017 is based on fully-connected LSTM, which is generally not sufficient for video prediction. The original ConvLSTM paper [a] explains the necessity of convolutions for the video prediction problem. We believe this already answers your concern about using ConvLSTM instead of LSTM for video prediction.\n\n(B) Necessity of higher-order models (ablation study (4))  \nWe found that higher-order models generally perform better than first-order models, which justifies our preference for higher-order models. For example, if we reduce the reported higher-order model to first-order fixing other hyper-parameters unchanged, the PSNR will decrease by 1.1, and SSIM by 0.015.\n\n(C) Necessity of CTT (ablation studies (3) and (5)).\nWe perform two ablation studies: the convolution filter size is fixed to 1 for CTT (which effectively reduce to TT) in Conv-TT-LSTM with higher-order and a single order. They corresponds to the ablation studies (3) and (5) as suggested. Unfortunately, both models are still under training but the validation curve ( https://postimg.cc/vx6rGnbH ) show that Conv-TT-LSTM without CTT is much worse than our proposed model. We expect them not much better than the ConvLSTM baseline. It suggests that convolutions in tensor-train is a very important component for capturing information in video prediction. \n\n(D) Question of backpropagation in CTT.\nIn Equation (4), we derive an efficient sequential algorithm for using CTT in higher-order models. Therefore, in our current implementation, we use the built-in auto-differentiation for backpropagation, which effectively reverses the order of the forward iterations.   \n\n(E) Question of error propagation issue. \nCompared to first-order models, higher-order models explicitly capture higher-order correlation, and therefore reduce the predictive error at each single step. As a result, the accumulation of the errors are slower over time, which benefits long-term prediction.  \n\n[a] Xingjian, S. H. I., et al. \"Convolutional LSTM network: A machine learning approach for precipitation nowcasting.\" Advances in neural information processing systems. 2015."}, "signatures": ["ICLR.cc/2020/Conference/Paper1454/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1454/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Convolutional Tensor-Train LSTM for Long-Term Video Prediction", "authors": ["Jiahao Su", "Wonmin Byeon", "Furong Huang", "Jan Kautz", "Animashree Anandkumar"], "authorids": ["jiahaosu@terpmail.umd.edu", "wonmin.byeon@gmail.com", "furongh@cs.umd.edu", "jkautz@nvidia.com", "animakumar@gmail.com"], "keywords": ["Tensor decomposition", "Video prediction"], "TL;DR": "we propose convolutional tensor-train LSTM,  which learns higher-order Convolutional LSTM efficiently using convolutional tensor-train decomposition. ", "abstract": "Long-term video prediction is highly challenging since it entails simultaneously capturing spatial and temporal information across a long range of image frames.Standard recurrent models are ineffective since they are prone to error propagation and cannot effectively capture higher-order correlations. A potential solution is to extend to higher-order spatio-temporal recurrent models. However, such a model requires  a  large number of parameters and operations, making it intractable  to learn in practice and is prone to overfitting. In this work, we propose convolutional tensor-train LSTM (Conv-TT-LSTM), which  learns higher-orderConvolutional LSTM (ConvLSTM) efficiently using convolutional  tensor-train decomposition (CTTD). Our proposed model naturally incorporates higher-order spatio-temporal information at a small cost of memory and computation by using efficient low-rank tensor representations. We evaluate our model on Moving-MNIST and KTH datasets and show improvements over standard ConvLSTM and better/comparable results to other ConvLSTM-based approaches, but with much fewer parameters.", "pdf": "/pdf/921c7a245bef64ed33d4cc827be4db59c547fbac.pdf", "paperhash": "su|convolutional_tensortrain_lstm_for_longterm_video_prediction", "original_pdf": "/attachment/3c12925aa4c470d88168cda664b5b82f88478033.pdf", "_bibtex": "@misc{\nsu2020convolutional,\ntitle={Convolutional Tensor-Train {\\{}LSTM{\\}} for Long-Term Video Prediction},\nauthor={Jiahao Su and Wonmin Byeon and Furong Huang and Jan Kautz and Animashree Anandkumar},\nyear={2020},\nurl={https://openreview.net/forum?id=Hkee1JBKwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Hkee1JBKwB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1454/Authors", "ICLR.cc/2020/Conference/Paper1454/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1454/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1454/Reviewers", "ICLR.cc/2020/Conference/Paper1454/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1454/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1454/Authors|ICLR.cc/2020/Conference/Paper1454/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504155774, "tmdate": 1576860536775, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1454/Authors", "ICLR.cc/2020/Conference/Paper1454/Reviewers", "ICLR.cc/2020/Conference/Paper1454/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1454/-/Official_Comment"}}}, {"id": "Hkgf1YEhjB", "original": null, "number": 2, "cdate": 1573828825586, "ddate": null, "tcdate": 1573828825586, "tmdate": 1573828825586, "tddate": null, "forum": "Hkee1JBKwB", "replyto": "BkxAlRIRYr", "invitation": "ICLR.cc/2020/Conference/Paper1454/-/Official_Comment", "content": {"title": "Response to Official Blind Review #1", "comment": "We thank the reviewer for the valuable comments. \n\n(1) Only comparing to Conv-LSTM, and the video quality not being improved. \nThe improved results are provided in the revised version for both Moving-MNIST (in Table 2 and Figures 2, 3) and KTH (Table 4 and Figure 4). Our new results outperforms the state-of-the-art model, PredRNN++, on both SSIM and LPIPS metrics. In Figures 3, 5 and6-11, the visual examples show that our models produce much sharper predictions compared to PredRNN++.   \n\n(2) Missing baselines from Villegas et al., 2017 and Denton et al., 2017.\nPSNR and SSIM scores of Villegas et al., 2017 has been included in Table 4, and are no better than our Conv-TT-LSTM models.\n\n(3) No video is provided.\nWe included per-frame visualization (Fig. 3 and 5). We believe it is easier to judge the perceptual quality by looking at predicted results for each individual frame. We also added more samples in Fig. 6-11. "}, "signatures": ["ICLR.cc/2020/Conference/Paper1454/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1454/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Convolutional Tensor-Train LSTM for Long-Term Video Prediction", "authors": ["Jiahao Su", "Wonmin Byeon", "Furong Huang", "Jan Kautz", "Animashree Anandkumar"], "authorids": ["jiahaosu@terpmail.umd.edu", "wonmin.byeon@gmail.com", "furongh@cs.umd.edu", "jkautz@nvidia.com", "animakumar@gmail.com"], "keywords": ["Tensor decomposition", "Video prediction"], "TL;DR": "we propose convolutional tensor-train LSTM,  which learns higher-order Convolutional LSTM efficiently using convolutional tensor-train decomposition. ", "abstract": "Long-term video prediction is highly challenging since it entails simultaneously capturing spatial and temporal information across a long range of image frames.Standard recurrent models are ineffective since they are prone to error propagation and cannot effectively capture higher-order correlations. A potential solution is to extend to higher-order spatio-temporal recurrent models. However, such a model requires  a  large number of parameters and operations, making it intractable  to learn in practice and is prone to overfitting. In this work, we propose convolutional tensor-train LSTM (Conv-TT-LSTM), which  learns higher-orderConvolutional LSTM (ConvLSTM) efficiently using convolutional  tensor-train decomposition (CTTD). Our proposed model naturally incorporates higher-order spatio-temporal information at a small cost of memory and computation by using efficient low-rank tensor representations. We evaluate our model on Moving-MNIST and KTH datasets and show improvements over standard ConvLSTM and better/comparable results to other ConvLSTM-based approaches, but with much fewer parameters.", "pdf": "/pdf/921c7a245bef64ed33d4cc827be4db59c547fbac.pdf", "paperhash": "su|convolutional_tensortrain_lstm_for_longterm_video_prediction", "original_pdf": "/attachment/3c12925aa4c470d88168cda664b5b82f88478033.pdf", "_bibtex": "@misc{\nsu2020convolutional,\ntitle={Convolutional Tensor-Train {\\{}LSTM{\\}} for Long-Term Video Prediction},\nauthor={Jiahao Su and Wonmin Byeon and Furong Huang and Jan Kautz and Animashree Anandkumar},\nyear={2020},\nurl={https://openreview.net/forum?id=Hkee1JBKwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Hkee1JBKwB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1454/Authors", "ICLR.cc/2020/Conference/Paper1454/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1454/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1454/Reviewers", "ICLR.cc/2020/Conference/Paper1454/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1454/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1454/Authors|ICLR.cc/2020/Conference/Paper1454/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504155774, "tmdate": 1576860536775, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1454/Authors", "ICLR.cc/2020/Conference/Paper1454/Reviewers", "ICLR.cc/2020/Conference/Paper1454/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1454/-/Official_Comment"}}}, {"id": "SJxYGCFcjr", "original": null, "number": 1, "cdate": 1573719568691, "ddate": null, "tcdate": 1573719568691, "tmdate": 1573719568691, "tddate": null, "forum": "Hkee1JBKwB", "replyto": "Hkee1JBKwB", "invitation": "ICLR.cc/2020/Conference/Paper1454/-/Official_Comment", "content": {"title": "Updated paper with new results", "comment": "In this revised revision, we update the following parts of the paper: \n\n1.  We add a perceptual metric, LPIPS [a] for all comparisons (Table 2-4). This metric is known to be close to human perception, compared to traditional metrics such as MSE and SSIM. We added a paragraph about a discussion of these metrics at the beginning of the experimental section.\n\n2.  We reproduce the state-of-the-art ConvLSTM-based method, PredRNN++ [c], using their source code [b] on both Moving-MNIST and KTH datasets. By performing an additional hyper-parameter search, we obtained better performance than the numbers reported in the original paper. Compared to these results, our Conv-TT-LSTM outperforms PredRNN++ in both SSIM and LPIPS (reported in Table 2, 4, per-frame comparison in Fig 2). The visual samples show that our proposed methods are much sharper than PredRNN++ for long-term prediction on both datasets (Fig. 3, 4, 6-11). \n\n3. Additional baseline, [Villegas et al., 2017] has been included in Fig. 4.\n\nI believe this update answers the concerns about the quality of our method (reviewer 1 and 4). We will answer the rest of comments soon. \n\n[a] Zhang, Richard, et al. \"The unreasonable effectiveness of deep features as a perceptual metric.\" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018.\n[b] https://github.com/Yunbo426/predrnn-pp\n[c] Wang, Yunbo, et al. \"Predrnn++: Towards a resolution of the deep-in-time dilemma in spatiotemporal predictive learning.\" arXiv preprint arXiv:1804.06300 (2018). "}, "signatures": ["ICLR.cc/2020/Conference/Paper1454/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1454/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Convolutional Tensor-Train LSTM for Long-Term Video Prediction", "authors": ["Jiahao Su", "Wonmin Byeon", "Furong Huang", "Jan Kautz", "Animashree Anandkumar"], "authorids": ["jiahaosu@terpmail.umd.edu", "wonmin.byeon@gmail.com", "furongh@cs.umd.edu", "jkautz@nvidia.com", "animakumar@gmail.com"], "keywords": ["Tensor decomposition", "Video prediction"], "TL;DR": "we propose convolutional tensor-train LSTM,  which learns higher-order Convolutional LSTM efficiently using convolutional tensor-train decomposition. ", "abstract": "Long-term video prediction is highly challenging since it entails simultaneously capturing spatial and temporal information across a long range of image frames.Standard recurrent models are ineffective since they are prone to error propagation and cannot effectively capture higher-order correlations. A potential solution is to extend to higher-order spatio-temporal recurrent models. However, such a model requires  a  large number of parameters and operations, making it intractable  to learn in practice and is prone to overfitting. In this work, we propose convolutional tensor-train LSTM (Conv-TT-LSTM), which  learns higher-orderConvolutional LSTM (ConvLSTM) efficiently using convolutional  tensor-train decomposition (CTTD). Our proposed model naturally incorporates higher-order spatio-temporal information at a small cost of memory and computation by using efficient low-rank tensor representations. We evaluate our model on Moving-MNIST and KTH datasets and show improvements over standard ConvLSTM and better/comparable results to other ConvLSTM-based approaches, but with much fewer parameters.", "pdf": "/pdf/921c7a245bef64ed33d4cc827be4db59c547fbac.pdf", "paperhash": "su|convolutional_tensortrain_lstm_for_longterm_video_prediction", "original_pdf": "/attachment/3c12925aa4c470d88168cda664b5b82f88478033.pdf", "_bibtex": "@misc{\nsu2020convolutional,\ntitle={Convolutional Tensor-Train {\\{}LSTM{\\}} for Long-Term Video Prediction},\nauthor={Jiahao Su and Wonmin Byeon and Furong Huang and Jan Kautz and Animashree Anandkumar},\nyear={2020},\nurl={https://openreview.net/forum?id=Hkee1JBKwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Hkee1JBKwB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1454/Authors", "ICLR.cc/2020/Conference/Paper1454/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1454/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1454/Reviewers", "ICLR.cc/2020/Conference/Paper1454/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1454/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1454/Authors|ICLR.cc/2020/Conference/Paper1454/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504155774, "tmdate": 1576860536775, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1454/Authors", "ICLR.cc/2020/Conference/Paper1454/Reviewers", "ICLR.cc/2020/Conference/Paper1454/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1454/-/Official_Comment"}}}, {"id": "BkxAlRIRYr", "original": null, "number": 1, "cdate": 1571872246304, "ddate": null, "tcdate": 1571872246304, "tmdate": 1572972466826, "tddate": null, "forum": "Hkee1JBKwB", "replyto": "Hkee1JBKwB", "invitation": "ICLR.cc/2020/Conference/Paper1454/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary:\nThis paper proposes a method that saves memory and computation in the task of video prediction by low-rank tensor representations via tensor decomposition. The method is able to outperform standard convolutional lstm and other methods by using less parameters when testing it in the Moving MNIST and KTH datasets. The authors also present a proof to validate their method.\n\n\nPros:\n+ Interesting method for decomposing tensors operations in convolutional architectures\n+ Outperforms immediate baseline (Convolutional LSTM)\n\nWeaknesses / comments:\n- Weak experimental section\nThe authors mainly compare against Convolutional LSTM. The performance increase is there but the difference in parameters is not that significant in comparison to the performance. Needing fewer parameters is one of the claims in this paper and I am not fully convinced of the trade-off between the complexity of the model and the gain in parameter reduction / performance. In addition, the show videos do not look that much improved. The paper is also missing baselines from Villegas et al., 2017 and Denton et al., 2017 which both have available models for the KTH dataset.\n\n- No videos provided\nThe paper does not provide any videos which is a must for video prediction papers. Judging the video quality from images in the paper is not easy, and also the used metrics have been shown to not be very objective in terms of video prediction quality or image generation in general.\n\n\nConclusion:\nThe proposed decomposition method is interesting, but the experimental section fails to convince me as to whether the methods performance validates the complicated formulations. My current score is between weak reject and reject so I will give a weak reject."}, "signatures": ["ICLR.cc/2020/Conference/Paper1454/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1454/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Convolutional Tensor-Train LSTM for Long-Term Video Prediction", "authors": ["Jiahao Su", "Wonmin Byeon", "Furong Huang", "Jan Kautz", "Animashree Anandkumar"], "authorids": ["jiahaosu@terpmail.umd.edu", "wonmin.byeon@gmail.com", "furongh@cs.umd.edu", "jkautz@nvidia.com", "animakumar@gmail.com"], "keywords": ["Tensor decomposition", "Video prediction"], "TL;DR": "we propose convolutional tensor-train LSTM,  which learns higher-order Convolutional LSTM efficiently using convolutional tensor-train decomposition. ", "abstract": "Long-term video prediction is highly challenging since it entails simultaneously capturing spatial and temporal information across a long range of image frames.Standard recurrent models are ineffective since they are prone to error propagation and cannot effectively capture higher-order correlations. A potential solution is to extend to higher-order spatio-temporal recurrent models. However, such a model requires  a  large number of parameters and operations, making it intractable  to learn in practice and is prone to overfitting. In this work, we propose convolutional tensor-train LSTM (Conv-TT-LSTM), which  learns higher-orderConvolutional LSTM (ConvLSTM) efficiently using convolutional  tensor-train decomposition (CTTD). Our proposed model naturally incorporates higher-order spatio-temporal information at a small cost of memory and computation by using efficient low-rank tensor representations. We evaluate our model on Moving-MNIST and KTH datasets and show improvements over standard ConvLSTM and better/comparable results to other ConvLSTM-based approaches, but with much fewer parameters.", "pdf": "/pdf/921c7a245bef64ed33d4cc827be4db59c547fbac.pdf", "paperhash": "su|convolutional_tensortrain_lstm_for_longterm_video_prediction", "original_pdf": "/attachment/3c12925aa4c470d88168cda664b5b82f88478033.pdf", "_bibtex": "@misc{\nsu2020convolutional,\ntitle={Convolutional Tensor-Train {\\{}LSTM{\\}} for Long-Term Video Prediction},\nauthor={Jiahao Su and Wonmin Byeon and Furong Huang and Jan Kautz and Animashree Anandkumar},\nyear={2020},\nurl={https://openreview.net/forum?id=Hkee1JBKwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Hkee1JBKwB", "replyto": "Hkee1JBKwB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1454/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1454/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575630088154, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1454/Reviewers"], "noninvitees": [], "tcdate": 1570237737158, "tmdate": 1575630088168, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1454/-/Official_Review"}}}, {"id": "rkgc4-Cy5S", "original": null, "number": 2, "cdate": 1571967281541, "ddate": null, "tcdate": 1571967281541, "tmdate": 1572972466774, "tddate": null, "forum": "Hkee1JBKwB", "replyto": "Hkee1JBKwB", "invitation": "ICLR.cc/2020/Conference/Paper1454/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposed a convolutional tensor-train (CTT) format based high-order and convolutional LSTM approach for long-term video prediction. This paper is well-motivated. Video data usually have high dimensional input, and the proposed method aims to explicitly take into account more than one hidden representation of previous frames - both lead to a huge number of parameters. Therefore, some sort of parameter reduction is needed. This paper considers two different types of operations - convolution and tensor-train (TT) decomposition - in an interleaved way. The basic model considered in this paper is a high-order variant of convolutional LSTM (convLSTM).\n\u00a0\nThere exist several works using tensor decomposition methods including TT to compress a fully connected layer or a convolutional layer in neural nets, to break the memory bottleneck and accelerate computation. This paper takes a different direction - it further embeds convolution into the TT decomposition and thus defines a new type of tensor decomposition, termed convolutional tensor train (CTT) decomposition. CTT is used to represent the huge weight matrices arisen in the high-order convLSTM. To my best knowledge, this combination of convolution and TT decomposition is new.\n\u00a0\nThe paper is well-written as the literature review is well done. Experimental results demonstrate improved performance over the convolutional LSTM baseline, a fewer number of parameters, and the qualitative results show sharp and clean digits. This improvement could be attributed to multiple causes: the high-order, the tensor decomposition-based compression, or the CTT. The authors also provide an ablation study, but it mainly concerns comparisons with ConvLSTM.\u00a0 \u00a0\n\u00a0\nDespite the promising results, this paper is not ready for ICLR yet. Below is a list of suggested points needed to address:\n(1) Yang et al 2017 claim that TT-RNN without convolution can also capture spatial and temporal dependence patterns in video modeling. This is an important baseline but missing in the current version of the paper.\u00a0\n(2) The justification of high-order modeling in long-term prediction. The first-order model also implicitly aggregates multiple past steps. It would be good to add more experimental evidence to support the necessity of the high-order.\n(3) There exists some unjustified complexity for the CTT approach. How does it compare to TT for high-order ConvLSTM?\n\u00a0\nPerhaps, a more complete ablation study should include:\n(1) LSTM with TT but without high-order and convolution\n(2) LSTM with high-order and TT but without convolution\n(3) ConvLSTM with TT\n(4) ConvLSTM with CTT\n(5) ConvLSTM with high-order and TT\n(6) ConvLSTM with high-order and CTT\n\u00a0\nQuestion:\n\u2022 How is the backpropagation done for the CTT core tensors?\u00a0\n\u2022 What is the error propagation issue of first-order methods and how does the high-order one not prone to it?\u00a0"}, "signatures": ["ICLR.cc/2020/Conference/Paper1454/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1454/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Convolutional Tensor-Train LSTM for Long-Term Video Prediction", "authors": ["Jiahao Su", "Wonmin Byeon", "Furong Huang", "Jan Kautz", "Animashree Anandkumar"], "authorids": ["jiahaosu@terpmail.umd.edu", "wonmin.byeon@gmail.com", "furongh@cs.umd.edu", "jkautz@nvidia.com", "animakumar@gmail.com"], "keywords": ["Tensor decomposition", "Video prediction"], "TL;DR": "we propose convolutional tensor-train LSTM,  which learns higher-order Convolutional LSTM efficiently using convolutional tensor-train decomposition. ", "abstract": "Long-term video prediction is highly challenging since it entails simultaneously capturing spatial and temporal information across a long range of image frames.Standard recurrent models are ineffective since they are prone to error propagation and cannot effectively capture higher-order correlations. A potential solution is to extend to higher-order spatio-temporal recurrent models. However, such a model requires  a  large number of parameters and operations, making it intractable  to learn in practice and is prone to overfitting. In this work, we propose convolutional tensor-train LSTM (Conv-TT-LSTM), which  learns higher-orderConvolutional LSTM (ConvLSTM) efficiently using convolutional  tensor-train decomposition (CTTD). Our proposed model naturally incorporates higher-order spatio-temporal information at a small cost of memory and computation by using efficient low-rank tensor representations. We evaluate our model on Moving-MNIST and KTH datasets and show improvements over standard ConvLSTM and better/comparable results to other ConvLSTM-based approaches, but with much fewer parameters.", "pdf": "/pdf/921c7a245bef64ed33d4cc827be4db59c547fbac.pdf", "paperhash": "su|convolutional_tensortrain_lstm_for_longterm_video_prediction", "original_pdf": "/attachment/3c12925aa4c470d88168cda664b5b82f88478033.pdf", "_bibtex": "@misc{\nsu2020convolutional,\ntitle={Convolutional Tensor-Train {\\{}LSTM{\\}} for Long-Term Video Prediction},\nauthor={Jiahao Su and Wonmin Byeon and Furong Huang and Jan Kautz and Animashree Anandkumar},\nyear={2020},\nurl={https://openreview.net/forum?id=Hkee1JBKwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Hkee1JBKwB", "replyto": "Hkee1JBKwB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1454/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1454/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575630088154, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1454/Reviewers"], "noninvitees": [], "tcdate": 1570237737158, "tmdate": 1575630088168, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1454/-/Official_Review"}}}, {"id": "HyegM4fo9H", "original": null, "number": 3, "cdate": 1572705287567, "ddate": null, "tcdate": 1572705287567, "tmdate": 1572972466724, "tddate": null, "forum": "Hkee1JBKwB", "replyto": "Hkee1JBKwB", "invitation": "ICLR.cc/2020/Conference/Paper1454/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #4", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper build a higher-order spatio-temporal model by means of combining Convolutional Tensor-Train Decomposition(CTTD) and ConvLSTM, and utilize the combination method to solve long-term video prediction problems. The CTTD factorizes a large convolutional kernel into a chain of smaller tensors, so as to relieve the difficult convergence and overfitting problems caused by too much model params. \nExperiments on Moving-MNIST and KTH datasets show that the proposed method achieved better results than standard ConvLSTM, and in some way comparable with SOTA model. Ablation Studies are also provided.\n\nAlthough it seems novel combing CTTD with ConvLSTM, the idea of CTTD and the combination mainly comes from [Yu et al.,2017] and [Yang et al.,2017]\uff0cthis paper use the method in a new problem of video prediction,  I think the theoretical innovation is not enough for ICLR.\nAlthough the experimental results were better than ConvLSTM(2015), but not as good as PredRNN++(2018), especially in terms of the MSE metrics. Since the prediction accuracy has not yet achieved, I don't think the reduction of model params is a matter of primary importance.  What\u2019s more, Moving-MNIST and KTH are relatively simple datasets, video prediction on a more complicated datasets such as UCF101 will be more convincing.\nConclusion:\nThis paper is in some way novel, but not enough for ICLR, and the experiment results seems not enough convincing, so I will give a weak reject.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1454/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1454/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Convolutional Tensor-Train LSTM for Long-Term Video Prediction", "authors": ["Jiahao Su", "Wonmin Byeon", "Furong Huang", "Jan Kautz", "Animashree Anandkumar"], "authorids": ["jiahaosu@terpmail.umd.edu", "wonmin.byeon@gmail.com", "furongh@cs.umd.edu", "jkautz@nvidia.com", "animakumar@gmail.com"], "keywords": ["Tensor decomposition", "Video prediction"], "TL;DR": "we propose convolutional tensor-train LSTM,  which learns higher-order Convolutional LSTM efficiently using convolutional tensor-train decomposition. ", "abstract": "Long-term video prediction is highly challenging since it entails simultaneously capturing spatial and temporal information across a long range of image frames.Standard recurrent models are ineffective since they are prone to error propagation and cannot effectively capture higher-order correlations. A potential solution is to extend to higher-order spatio-temporal recurrent models. However, such a model requires  a  large number of parameters and operations, making it intractable  to learn in practice and is prone to overfitting. In this work, we propose convolutional tensor-train LSTM (Conv-TT-LSTM), which  learns higher-orderConvolutional LSTM (ConvLSTM) efficiently using convolutional  tensor-train decomposition (CTTD). Our proposed model naturally incorporates higher-order spatio-temporal information at a small cost of memory and computation by using efficient low-rank tensor representations. We evaluate our model on Moving-MNIST and KTH datasets and show improvements over standard ConvLSTM and better/comparable results to other ConvLSTM-based approaches, but with much fewer parameters.", "pdf": "/pdf/921c7a245bef64ed33d4cc827be4db59c547fbac.pdf", "paperhash": "su|convolutional_tensortrain_lstm_for_longterm_video_prediction", "original_pdf": "/attachment/3c12925aa4c470d88168cda664b5b82f88478033.pdf", "_bibtex": "@misc{\nsu2020convolutional,\ntitle={Convolutional Tensor-Train {\\{}LSTM{\\}} for Long-Term Video Prediction},\nauthor={Jiahao Su and Wonmin Byeon and Furong Huang and Jan Kautz and Animashree Anandkumar},\nyear={2020},\nurl={https://openreview.net/forum?id=Hkee1JBKwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Hkee1JBKwB", "replyto": "Hkee1JBKwB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1454/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1454/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575630088154, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1454/Reviewers"], "noninvitees": [], "tcdate": 1570237737158, "tmdate": 1575630088168, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1454/-/Official_Review"}}}], "count": 9}