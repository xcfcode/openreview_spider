{"notes": [{"tddate": null, "ddate": null, "tmdate": 1525696639439, "tcdate": 1525696639439, "number": 15, "cdate": 1525696639439, "id": "SkvYd6T6M", "invitation": "ICLR.cc/2018/Conference/-/Paper353/Official_Comment", "forum": "HJNMYceCW", "replyto": "SkMzEJ6Hf", "signatures": ["ICLR.cc/2018/Conference/Paper353/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper353/Authors"], "content": {"title": "Brief thank you and comment on colors", "comment": "Sorry for the much-delayed reply. First, thanks for the feedback. We worked hard to address the issues that arose in the theoretical analysis, and are hugely indebted to Reviewer 3 for being helpful here. We appreciate the AC and reviewers for taking so much time for this paper!\n\nOne comment on the colors in the plots. The colors are non-standard, but they were chosen specifically to be both color-blind safe and print-friendly, per ColorBrewer2. (Unfortunately we had to add one additional color because there were no color-blind safe \"qualitative\" colorschemes that contained six colors.) If anyone knows of a better set of colorblind colors we would like to take this into account in future papers, but for now did not change them because they're the best we know of right now."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Residual Loss Prediction: Reinforcement Learning With No Incremental Feedback", "abstract": "We consider reinforcement learning and bandit structured prediction problems with very sparse loss feedback: only at the end of an episode. We introduce a novel algorithm, RESIDUAL LOSS PREDICTION (RESLOPE), that solves such problems by automatically learning an internal representation of a denser reward function. RESLOPE operates as a reduction to contextual bandits, using its learned loss representation to solve the credit assignment problem, and a contextual bandit oracle to trade-off exploration and exploitation. RESLOPE enjoys a no-regret reduction-style theoretical guarantee and outperforms state of the art reinforcement learning algorithms in both MDP environments and bandit structured prediction settings.", "pdf": "/pdf/56ab4f2257300654c47e017e7ae3d440ab1150d8.pdf", "TL;DR": "We present a novel algorithm for solving reinforcement learning and bandit structured prediction problems with very sparse loss feedback.", "paperhash": "iii|residual_loss_prediction_reinforcement_learning_with_no_incremental_feedback", "_bibtex": "@inproceedings{\ndaum\u00e92018residual,\ntitle={{RESIDUAL} {LOSS} {PREDICTION}: {REINFORCEMENT} {LEARNING} {WITH} {NO} {INCREMENTAL} {FEEDBACK}},\nauthor={Hal Daum\u00e9 III and John Langford and Paul Mineiro and Amr Sharaf},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=HJNMYceCW},\n}", "keywords": ["Reinforcement Learning", "Structured Prediction", "Contextual Bandits", "Learning Reduction"], "authors": ["Hal Daum\u00e9 III", "John Langford", "Amr Sharaf"], "authorids": ["hal@umiacs.umd.edu", "jl@hunch.net", "amr@cs.umd.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825735103, "id": "ICLR.cc/2018/Conference/-/Paper353/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "HJNMYceCW", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper353/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper353/Authors|ICLR.cc/2018/Conference/Paper353/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper353/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper353/Authors|ICLR.cc/2018/Conference/Paper353/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper353/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper353/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper353/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper353/Reviewers", "ICLR.cc/2018/Conference/Paper353/Authors", "ICLR.cc/2018/Conference/Paper353/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825735103}}}, {"tddate": null, "ddate": null, "tmdate": 1524508867364, "tcdate": 1509103884112, "number": 353, "cdate": 1518730180761, "id": "HJNMYceCW", "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "forum": "HJNMYceCW", "original": "HkQMF9eR-", "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference"], "content": {"title": "Residual Loss Prediction: Reinforcement Learning With No Incremental Feedback", "abstract": "We consider reinforcement learning and bandit structured prediction problems with very sparse loss feedback: only at the end of an episode. We introduce a novel algorithm, RESIDUAL LOSS PREDICTION (RESLOPE), that solves such problems by automatically learning an internal representation of a denser reward function. RESLOPE operates as a reduction to contextual bandits, using its learned loss representation to solve the credit assignment problem, and a contextual bandit oracle to trade-off exploration and exploitation. RESLOPE enjoys a no-regret reduction-style theoretical guarantee and outperforms state of the art reinforcement learning algorithms in both MDP environments and bandit structured prediction settings.", "pdf": "/pdf/56ab4f2257300654c47e017e7ae3d440ab1150d8.pdf", "TL;DR": "We present a novel algorithm for solving reinforcement learning and bandit structured prediction problems with very sparse loss feedback.", "paperhash": "iii|residual_loss_prediction_reinforcement_learning_with_no_incremental_feedback", "_bibtex": "@inproceedings{\ndaum\u00e92018residual,\ntitle={{RESIDUAL} {LOSS} {PREDICTION}: {REINFORCEMENT} {LEARNING} {WITH} {NO} {INCREMENTAL} {FEEDBACK}},\nauthor={Hal Daum\u00e9 III and John Langford and Paul Mineiro and Amr Sharaf},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=HJNMYceCW},\n}", "keywords": ["Reinforcement Learning", "Structured Prediction", "Contextual Bandits", "Learning Reduction"], "authors": ["Hal Daum\u00e9 III", "John Langford", "Amr Sharaf"], "authorids": ["hal@umiacs.umd.edu", "jl@hunch.net", "amr@cs.umd.edu"]}, "nonreaders": [], "details": {"replyCount": 16, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1506717071958, "id": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Conference"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference"]}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"authors": {"required": false, "order": 1, "values-regex": ".*", "description": "Comma separated list of author names, as they appear in the paper."}, "authorids": {"required": false, "order": 2, "values-regex": ".*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "cdate": 1506717071958}}, "tauthor": "ICLR.cc/2018/Conference"}, {"tddate": null, "ddate": null, "tmdate": 1519401968301, "tcdate": 1519401968301, "number": 14, "cdate": 1519401968301, "id": "Hy_gn36vM", "invitation": "ICLR.cc/2018/Conference/-/Paper353/Official_Comment", "forum": "HJNMYceCW", "replyto": "r1uutbTNG", "signatures": ["ICLR.cc/2018/Conference/Paper353/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper353/Authors"], "content": {"title": "TD(lambda) and MC Sampling", "comment": "Thanks again for going back and forth on this. We're still trying to understand exactly what you are hoping to see here. Our main uncertainty regarding doing TD(lambda) experiments in section 5.3 is how to do this, in particular how to do eligibility traces, for non-linear function approximators. We know (Sutton & von Seijen, 2014) how to do this in the linear case, but suspect that you might have something different in mind because extending this to the non-linear case is hardly a \"naive baseline.\" We could add MC sampling (because everything is in simulation, arbitrarily precise estimates could be obtained, though of course that does not carry over to the online setting where you only get to evaluate reward once), but are not entirely sure what would be learned by doing so: obviously if you get enough MC samples you will have a very good estimator.\n\n\nWe'd definitely like to include some more discussion of eligibility traces to help better connect our work with what has been done before, but we're at a bit of a loss understanding exactly what your comments are getting at. If you would be willing to help us understand better what connections you're referring to (that we appear to be missing) we'd greatly appreciate it... feel free to either reply here or email us directly. Thanks!"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Residual Loss Prediction: Reinforcement Learning With No Incremental Feedback", "abstract": "We consider reinforcement learning and bandit structured prediction problems with very sparse loss feedback: only at the end of an episode. We introduce a novel algorithm, RESIDUAL LOSS PREDICTION (RESLOPE), that solves such problems by automatically learning an internal representation of a denser reward function. RESLOPE operates as a reduction to contextual bandits, using its learned loss representation to solve the credit assignment problem, and a contextual bandit oracle to trade-off exploration and exploitation. RESLOPE enjoys a no-regret reduction-style theoretical guarantee and outperforms state of the art reinforcement learning algorithms in both MDP environments and bandit structured prediction settings.", "pdf": "/pdf/56ab4f2257300654c47e017e7ae3d440ab1150d8.pdf", "TL;DR": "We present a novel algorithm for solving reinforcement learning and bandit structured prediction problems with very sparse loss feedback.", "paperhash": "iii|residual_loss_prediction_reinforcement_learning_with_no_incremental_feedback", "_bibtex": "@inproceedings{\ndaum\u00e92018residual,\ntitle={{RESIDUAL} {LOSS} {PREDICTION}: {REINFORCEMENT} {LEARNING} {WITH} {NO} {INCREMENTAL} {FEEDBACK}},\nauthor={Hal Daum\u00e9 III and John Langford and Paul Mineiro and Amr Sharaf},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=HJNMYceCW},\n}", "keywords": ["Reinforcement Learning", "Structured Prediction", "Contextual Bandits", "Learning Reduction"], "authors": ["Hal Daum\u00e9 III", "John Langford", "Amr Sharaf"], "authorids": ["hal@umiacs.umd.edu", "jl@hunch.net", "amr@cs.umd.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825735103, "id": "ICLR.cc/2018/Conference/-/Paper353/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "HJNMYceCW", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper353/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper353/Authors|ICLR.cc/2018/Conference/Paper353/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper353/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper353/Authors|ICLR.cc/2018/Conference/Paper353/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper353/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper353/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper353/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper353/Reviewers", "ICLR.cc/2018/Conference/Paper353/Authors", "ICLR.cc/2018/Conference/Paper353/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825735103}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1517260093260, "tcdate": 1517249546036, "number": 303, "cdate": 1517249546021, "id": "SkMzEJ6Hf", "invitation": "ICLR.cc/2018/Conference/-/Acceptance_Decision", "forum": "HJNMYceCW", "replyto": "HJNMYceCW", "signatures": ["ICLR.cc/2018/Conference/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Program_Chairs"], "content": {"title": "ICLR 2018 Conference Acceptance Decision", "comment": "The reviewers agree that the problem of learning learning credit assignment from terminal rewards is interesting, and that the presented approach is promising. There are some concerns regarding the rigor and correctness of the theoretical results, and I ask the authors to improve those aspects of the paper. I also ask the authors to the result figures easier to read. The chosen colors are not ideal and the use of log-scale x-axis is not standard. Finally, including DAgger in the same plot is confusing assuming that DAgger user more information.", "decision": "Accept (Poster)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Residual Loss Prediction: Reinforcement Learning With No Incremental Feedback", "abstract": "We consider reinforcement learning and bandit structured prediction problems with very sparse loss feedback: only at the end of an episode. We introduce a novel algorithm, RESIDUAL LOSS PREDICTION (RESLOPE), that solves such problems by automatically learning an internal representation of a denser reward function. RESLOPE operates as a reduction to contextual bandits, using its learned loss representation to solve the credit assignment problem, and a contextual bandit oracle to trade-off exploration and exploitation. RESLOPE enjoys a no-regret reduction-style theoretical guarantee and outperforms state of the art reinforcement learning algorithms in both MDP environments and bandit structured prediction settings.", "pdf": "/pdf/56ab4f2257300654c47e017e7ae3d440ab1150d8.pdf", "TL;DR": "We present a novel algorithm for solving reinforcement learning and bandit structured prediction problems with very sparse loss feedback.", "paperhash": "iii|residual_loss_prediction_reinforcement_learning_with_no_incremental_feedback", "_bibtex": "@inproceedings{\ndaum\u00e92018residual,\ntitle={{RESIDUAL} {LOSS} {PREDICTION}: {REINFORCEMENT} {LEARNING} {WITH} {NO} {INCREMENTAL} {FEEDBACK}},\nauthor={Hal Daum\u00e9 III and John Langford and Paul Mineiro and Amr Sharaf},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=HJNMYceCW},\n}", "keywords": ["Reinforcement Learning", "Structured Prediction", "Contextual Bandits", "Learning Reduction"], "authors": ["Hal Daum\u00e9 III", "John Langford", "Amr Sharaf"], "authorids": ["hal@umiacs.umd.edu", "jl@hunch.net", "amr@cs.umd.edu"]}, "tags": [], "invitation": {"id": "ICLR.cc/2018/Conference/-/Acceptance_Decision", "rdate": null, "ddate": null, "expdate": 1541175629000, "duedate": null, "tmdate": 1541177635767, "tddate": null, "super": null, "final": null, "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": {"values": ["ICLR.cc/2018/Conference/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Conference/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Conference Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": [], "noninvitees": [], "writers": ["ICLR.cc/2018/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1541177635767}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1517179230999, "tcdate": 1511754763662, "number": 2, "cdate": 1511754763662, "id": "r14M3-KxM", "invitation": "ICLR.cc/2018/Conference/-/Paper353/Official_Review", "forum": "HJNMYceCW", "replyto": "HJNMYceCW", "signatures": ["ICLR.cc/2018/Conference/Paper353/AnonReviewer3"], "readers": ["everyone"], "content": {"title": "Solid paper, many confusions in the original paper have been answered", "rating": "7: Good paper, accept", "review": "After reading the other reviews and the authors' responses, I am satisfied that this paper is above the accept threshold.  I think there are many areas of further discussion that the authors can flesh out (as mentioned below and in other reviews), but overall the contribution seems solid.   I also appreciate the reviewers' efforts to run more experiments and flesh out the discussion in the revised version of the submission.\n\nFinal concluding thoughts:\n-- Perhaps pi^ref was somehow better for the structured prediction problems than RL problems?\n-- Can one show regret bound for multi-deviation if one doesn't have to learn x (i.e., we were given a good x a priori)?\n\n\n\n---------------------------------------------\nORIGINAL REVIEW\n\nFirst off, I think this paper is potentially above the accept threshold.  The ideas presented are interesting and the results are potentially interesting as well.   However, I have some reservations, a significant portion of which stem from not understanding aspects of the proposed approach and theoretical results, as outlined below.\n\n\n\nThe algorithm design and theoretical results in the appendix could be made substantially more rigorous. Specifically:\n\n--  basic notations such as regret (in Theorem 1), the total reward (J), Q-value (Q), and value function (V) are not defined.  While these concepts are fairly standard, it would be highly beneficial to define them formally. \n\n-- I'm not convinced that the \"terms in the parentheses\" (Eq. 7) are \"exactly the contextual bandit cost\".  I would like to see a more rigorous derivation of the connection.  For instance, one could imagine that the policy disadvantage should be the difference between the residual costs of the bandit algorithm and the reference policy, rather than just the residual cost of the bandit algorithm. \n\n-- I'm a little unclear in the proof of Theorem 1 where Q(s,pi_n) from Eq 7 fits into Eq 8.\n\n-- The residual cost used for CB.update depends on estimated costs at other time steps h!=h_dev.  Presumably, these estimated costs will change as learning progresses.  How does one reconcile that?  I imagine that it could somehow work out using a bandit algorithm with adversarial guarantees, but I can also imagine it not working out.  I would like to see a rigorous treatment of this issue.\n\n-- It would be nice to see an end-to-end result that instantiates Theorem 1 (and/or Theorem 2) with a contextual bandit algorithm to see a fully instantiated guarantee.  \n\n\n\nWith regards to the algorithm design itself, I have some confusions:\n\n-- How does one create x in practice? I believe this is described in Appendix H, but it's not obvious.  \n\n-- What happens if we don't have a good way to generate x and it must be learned as well?  I'd imagine one would need larger RNNs in that case. \n\n-- If x is actually learned on-the-fly, how does that impact the theoretical results?\n\n-- I find it curious that there's no notion of future reward learning in the learning algorithm.  For instance, in Q learning, one directly models the the long-term (discounted) rewards during learning.  In fact, the theoretical analysis talks about advantage functions as well.  It would be nice to comment on this aspect at an intuitive level.\n\n\n\nWith regards to the experiments:\n\n-- I find it very curious that the results are so negative for using only 1-dev compared to multi-dev (Figure 9 in Appendix).  Given that much of the paper is devoted to 1-dev, it's a bit disappointing that this issue is not analyzed in more detail, and furthermore the results are mostly hidden in the appendix.\n\n-- It's not clear if a reference policy was used in the experiments and what value of beta was used.\n\n-- Can the authors speculate about the difference in performance between the RL and bandit structured prediction settings?  My personal conjecture is that the bandit structured prediction settings are more easily decomposable additively, which leads to a greater advantage of the proposed approach, but I would like to hear the authors' thoughts.\n\n\n\nFinally, the overall presentation of this paper could be substantially improved.  In addition to the above uncertainties, some more points are described below.  I don't view these points as \"deal breakers\" for determining accept/reject.\n\n-- This paper uses too many examples, from part-of-speech tagging to credit assignment in determining paths.  I recommend sticking to one running example, which substantially reduces context switching for the reader.  In every such example, there are extraneous details are not relevant to making the point, and the reader needs to spend considerable effort figuring that out for each example used.  \n\n-- Inconsistent language. For instance, x is sometimes referred to as the \"input example\", \"context\" and \"features\".\n\n-- At the end of page 4, \"Internally, ReslopePolicy takes a standard learning to search step.\"  Two issues: 1) ReslopePolicy is not defined or referred to anywhere else.  2) is the remainder of that paragraph a description of a \"standard learning to search step\"?\n\n-- As mentioned before, Regret is not defined in Theorem 1 & 2.\n\n-- The discussion of the high-level algorithmic concepts is a bit diffuse or lacking.  For instance, one key idea in the algorithmic development is that it's sufficient to make a uniformly random deviation.  Is this idea from the learning to search literature?  If so, it would be nice to highlight this in Section 2.2.\n\n", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Residual Loss Prediction: Reinforcement Learning With No Incremental Feedback", "abstract": "We consider reinforcement learning and bandit structured prediction problems with very sparse loss feedback: only at the end of an episode. We introduce a novel algorithm, RESIDUAL LOSS PREDICTION (RESLOPE), that solves such problems by automatically learning an internal representation of a denser reward function. RESLOPE operates as a reduction to contextual bandits, using its learned loss representation to solve the credit assignment problem, and a contextual bandit oracle to trade-off exploration and exploitation. RESLOPE enjoys a no-regret reduction-style theoretical guarantee and outperforms state of the art reinforcement learning algorithms in both MDP environments and bandit structured prediction settings.", "pdf": "/pdf/56ab4f2257300654c47e017e7ae3d440ab1150d8.pdf", "TL;DR": "We present a novel algorithm for solving reinforcement learning and bandit structured prediction problems with very sparse loss feedback.", "paperhash": "iii|residual_loss_prediction_reinforcement_learning_with_no_incremental_feedback", "_bibtex": "@inproceedings{\ndaum\u00e92018residual,\ntitle={{RESIDUAL} {LOSS} {PREDICTION}: {REINFORCEMENT} {LEARNING} {WITH} {NO} {INCREMENTAL} {FEEDBACK}},\nauthor={Hal Daum\u00e9 III and John Langford and Paul Mineiro and Amr Sharaf},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=HJNMYceCW},\n}", "keywords": ["Reinforcement Learning", "Structured Prediction", "Contextual Bandits", "Learning Reduction"], "authors": ["Hal Daum\u00e9 III", "John Langford", "Amr Sharaf"], "authorids": ["hal@umiacs.umd.edu", "jl@hunch.net", "amr@cs.umd.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642437617, "id": "ICLR.cc/2018/Conference/-/Paper353/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper353/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper353/AnonReviewer2", "ICLR.cc/2018/Conference/Paper353/AnonReviewer3", "ICLR.cc/2018/Conference/Paper353/AnonReviewer1"], "reply": {"forum": "HJNMYceCW", "replyto": "HJNMYceCW", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper353/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642437617}}}, {"tddate": null, "ddate": null, "tmdate": 1517178572854, "tcdate": 1517178572854, "number": 13, "cdate": 1517178572854, "id": "S1BCCpjrM", "invitation": "ICLR.cc/2018/Conference/-/Paper353/Official_Comment", "forum": "HJNMYceCW", "replyto": "Hy1aVOp7z", "signatures": ["ICLR.cc/2018/Conference/Paper353/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper353/AnonReviewer3"], "content": {"title": "acknowledgement of author response", "comment": "This comment acknowledges the author response.  My official review has been edited."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Residual Loss Prediction: Reinforcement Learning With No Incremental Feedback", "abstract": "We consider reinforcement learning and bandit structured prediction problems with very sparse loss feedback: only at the end of an episode. We introduce a novel algorithm, RESIDUAL LOSS PREDICTION (RESLOPE), that solves such problems by automatically learning an internal representation of a denser reward function. RESLOPE operates as a reduction to contextual bandits, using its learned loss representation to solve the credit assignment problem, and a contextual bandit oracle to trade-off exploration and exploitation. RESLOPE enjoys a no-regret reduction-style theoretical guarantee and outperforms state of the art reinforcement learning algorithms in both MDP environments and bandit structured prediction settings.", "pdf": "/pdf/56ab4f2257300654c47e017e7ae3d440ab1150d8.pdf", "TL;DR": "We present a novel algorithm for solving reinforcement learning and bandit structured prediction problems with very sparse loss feedback.", "paperhash": "iii|residual_loss_prediction_reinforcement_learning_with_no_incremental_feedback", "_bibtex": "@inproceedings{\ndaum\u00e92018residual,\ntitle={{RESIDUAL} {LOSS} {PREDICTION}: {REINFORCEMENT} {LEARNING} {WITH} {NO} {INCREMENTAL} {FEEDBACK}},\nauthor={Hal Daum\u00e9 III and John Langford and Paul Mineiro and Amr Sharaf},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=HJNMYceCW},\n}", "keywords": ["Reinforcement Learning", "Structured Prediction", "Contextual Bandits", "Learning Reduction"], "authors": ["Hal Daum\u00e9 III", "John Langford", "Amr Sharaf"], "authorids": ["hal@umiacs.umd.edu", "jl@hunch.net", "amr@cs.umd.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825735103, "id": "ICLR.cc/2018/Conference/-/Paper353/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "HJNMYceCW", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper353/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper353/Authors|ICLR.cc/2018/Conference/Paper353/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper353/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper353/Authors|ICLR.cc/2018/Conference/Paper353/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper353/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper353/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper353/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper353/Reviewers", "ICLR.cc/2018/Conference/Paper353/Authors", "ICLR.cc/2018/Conference/Paper353/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825735103}}}, {"tddate": null, "ddate": null, "tmdate": 1517178513755, "tcdate": 1517178513755, "number": 12, "cdate": 1517178513755, "id": "H159Cpirz", "invitation": "ICLR.cc/2018/Conference/-/Paper353/Official_Comment", "forum": "HJNMYceCW", "replyto": "ryRaK9sHM", "signatures": ["ICLR.cc/2018/Conference/Paper353/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper353/AnonReviewer3"], "content": {"title": "Interactions between two online learning reductions?", "comment": "Hmm, I'm not sure the solution is that simple.\n\nCB.cost is predicting the advantage cost of pi^mix or pi^learn, both of which are evolving functions because the policy is learning over time.  Hence, I don't see a realizability assumption as reasonable except for characterizing the CB.cost of the final pi you learn.  \n\nAs for the regret analysis, there is an issue with two interacting online learning reductions, one for learning CB.Cost and one for learning Pi (i.e., CB.Act).  The regret analysis of CB.Act will depend on the convergence of CB.Cost and vice versa.  \n\nThis issue arises in other settings:\n-- Learnability of (approximate) Nash equilibria in two-player zero-sum games by using two no-regret online learning algorithms for the two players.  The convergence analysis of each player depends on the convergence of the other player.\n[1] http://www.cs.cmu.edu/~avrim/ML07/lect1028-1102.pdf\n\n\n-- Convergence analysis in online learning of GANs (where both the generator and discriminator are trained via online learning): \n[2] https://arxiv.org/abs/1706.03269\n\n-- Convergence analysis of sparring-style reductions of the dueling bandits problem: \n[3] https://arxiv.org/abs/1502.06362\n[4] https://arxiv.org/abs/1705.00253\n\nIn [1] and [3], one resorts to using online learning algorithms with adversarial guarantees (which includes settings where the \"environment\" is influenced by another online learning algorithm). \n\nIn [2] and [4], the authors are able to more carefully analyze the structure of the interaction, and do not have to resort to the adversarial setting (online learning algorithms with adversarial guarantees can be very inefficient in practice).\n\nIn lieu of more carefully analyzing the interaction between these two online learning procedures, I suspect you'll have to resort to online learning algorithms (for CB.Cost and CB.Act) that have guarantees in the adversarial setting.  I think that will probably work out, but this whole discussion needs to be much more thoroughly fleshed out in the paper."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Residual Loss Prediction: Reinforcement Learning With No Incremental Feedback", "abstract": "We consider reinforcement learning and bandit structured prediction problems with very sparse loss feedback: only at the end of an episode. We introduce a novel algorithm, RESIDUAL LOSS PREDICTION (RESLOPE), that solves such problems by automatically learning an internal representation of a denser reward function. RESLOPE operates as a reduction to contextual bandits, using its learned loss representation to solve the credit assignment problem, and a contextual bandit oracle to trade-off exploration and exploitation. RESLOPE enjoys a no-regret reduction-style theoretical guarantee and outperforms state of the art reinforcement learning algorithms in both MDP environments and bandit structured prediction settings.", "pdf": "/pdf/56ab4f2257300654c47e017e7ae3d440ab1150d8.pdf", "TL;DR": "We present a novel algorithm for solving reinforcement learning and bandit structured prediction problems with very sparse loss feedback.", "paperhash": "iii|residual_loss_prediction_reinforcement_learning_with_no_incremental_feedback", "_bibtex": "@inproceedings{\ndaum\u00e92018residual,\ntitle={{RESIDUAL} {LOSS} {PREDICTION}: {REINFORCEMENT} {LEARNING} {WITH} {NO} {INCREMENTAL} {FEEDBACK}},\nauthor={Hal Daum\u00e9 III and John Langford and Paul Mineiro and Amr Sharaf},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=HJNMYceCW},\n}", "keywords": ["Reinforcement Learning", "Structured Prediction", "Contextual Bandits", "Learning Reduction"], "authors": ["Hal Daum\u00e9 III", "John Langford", "Amr Sharaf"], "authorids": ["hal@umiacs.umd.edu", "jl@hunch.net", "amr@cs.umd.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825735103, "id": "ICLR.cc/2018/Conference/-/Paper353/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "HJNMYceCW", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper353/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper353/Authors|ICLR.cc/2018/Conference/Paper353/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper353/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper353/Authors|ICLR.cc/2018/Conference/Paper353/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper353/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper353/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper353/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper353/Reviewers", "ICLR.cc/2018/Conference/Paper353/Authors", "ICLR.cc/2018/Conference/Paper353/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825735103}}}, {"tddate": null, "ddate": null, "tmdate": 1517164998014, "tcdate": 1517164998014, "number": 11, "cdate": 1517164998014, "id": "ryRaK9sHM", "invitation": "ICLR.cc/2018/Conference/-/Paper353/Official_Comment", "forum": "HJNMYceCW", "replyto": "HJ3M8diHM", "signatures": ["ICLR.cc/2018/Conference/Paper353/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper353/Authors"], "content": {"title": "CB cost & analysis - Response", "comment": "Thank you for asking about this; indeed, you're right, there's a missing term. In going from Eq 7 to Eq 8 as it stands right now, we're assuming that we have access to exact quantities, which is not actually the case in practice. In order to account for this, we need to add an additional term \\epsilon_{CS} that captures the regret of the cost-sensitive learner. This will then be an additive term in Eq 8. Under a realizability assumption this will go to zero over time, so the impact on Theorem 1 is that an additional realizability assumption is required, or (probably better) to explicitly pull \\epsilon_{CS} out as an additional approximation error term in the final bound. We really appreciate you catching this!\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Residual Loss Prediction: Reinforcement Learning With No Incremental Feedback", "abstract": "We consider reinforcement learning and bandit structured prediction problems with very sparse loss feedback: only at the end of an episode. We introduce a novel algorithm, RESIDUAL LOSS PREDICTION (RESLOPE), that solves such problems by automatically learning an internal representation of a denser reward function. RESLOPE operates as a reduction to contextual bandits, using its learned loss representation to solve the credit assignment problem, and a contextual bandit oracle to trade-off exploration and exploitation. RESLOPE enjoys a no-regret reduction-style theoretical guarantee and outperforms state of the art reinforcement learning algorithms in both MDP environments and bandit structured prediction settings.", "pdf": "/pdf/56ab4f2257300654c47e017e7ae3d440ab1150d8.pdf", "TL;DR": "We present a novel algorithm for solving reinforcement learning and bandit structured prediction problems with very sparse loss feedback.", "paperhash": "iii|residual_loss_prediction_reinforcement_learning_with_no_incremental_feedback", "_bibtex": "@inproceedings{\ndaum\u00e92018residual,\ntitle={{RESIDUAL} {LOSS} {PREDICTION}: {REINFORCEMENT} {LEARNING} {WITH} {NO} {INCREMENTAL} {FEEDBACK}},\nauthor={Hal Daum\u00e9 III and John Langford and Paul Mineiro and Amr Sharaf},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=HJNMYceCW},\n}", "keywords": ["Reinforcement Learning", "Structured Prediction", "Contextual Bandits", "Learning Reduction"], "authors": ["Hal Daum\u00e9 III", "John Langford", "Amr Sharaf"], "authorids": ["hal@umiacs.umd.edu", "jl@hunch.net", "amr@cs.umd.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825735103, "id": "ICLR.cc/2018/Conference/-/Paper353/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "HJNMYceCW", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper353/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper353/Authors|ICLR.cc/2018/Conference/Paper353/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper353/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper353/Authors|ICLR.cc/2018/Conference/Paper353/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper353/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper353/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper353/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper353/Reviewers", "ICLR.cc/2018/Conference/Paper353/Authors", "ICLR.cc/2018/Conference/Paper353/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825735103}}}, {"tddate": null, "ddate": null, "tmdate": 1517155931787, "tcdate": 1517155859987, "number": 10, "cdate": 1517155859987, "id": "HJ3M8diHM", "invitation": "ICLR.cc/2018/Conference/-/Paper353/Official_Comment", "forum": "HJNMYceCW", "replyto": "rJeBU_a7M", "signatures": ["ICLR.cc/2018/Conference/Paper353/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper353/AnonReviewer3"], "content": {"title": "CB cost & analysis", "comment": "I'm still confused on this point.  The terms in the (...) in Eq.7 uses the \"true\" Q values.  Whereas the RESLOPE algorithm uses the CB.cost function (Line 17 & Line 20 in Alg 1).  As far as I can tell, CB.cost is an **estimate** of the (dis)advantage.  Thus, I don't understand how \"is exactly the expected value of the target of the contextual bandit cost\", as stated in the proof of Theorem 1.  Are you saying that, CB.cost used in Line 20 of Alg 1 is an unbiased estimate?\n\nEverything else about the paper seems OK to me, modulo polishing."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Residual Loss Prediction: Reinforcement Learning With No Incremental Feedback", "abstract": "We consider reinforcement learning and bandit structured prediction problems with very sparse loss feedback: only at the end of an episode. We introduce a novel algorithm, RESIDUAL LOSS PREDICTION (RESLOPE), that solves such problems by automatically learning an internal representation of a denser reward function. RESLOPE operates as a reduction to contextual bandits, using its learned loss representation to solve the credit assignment problem, and a contextual bandit oracle to trade-off exploration and exploitation. RESLOPE enjoys a no-regret reduction-style theoretical guarantee and outperforms state of the art reinforcement learning algorithms in both MDP environments and bandit structured prediction settings.", "pdf": "/pdf/56ab4f2257300654c47e017e7ae3d440ab1150d8.pdf", "TL;DR": "We present a novel algorithm for solving reinforcement learning and bandit structured prediction problems with very sparse loss feedback.", "paperhash": "iii|residual_loss_prediction_reinforcement_learning_with_no_incremental_feedback", "_bibtex": "@inproceedings{\ndaum\u00e92018residual,\ntitle={{RESIDUAL} {LOSS} {PREDICTION}: {REINFORCEMENT} {LEARNING} {WITH} {NO} {INCREMENTAL} {FEEDBACK}},\nauthor={Hal Daum\u00e9 III and John Langford and Paul Mineiro and Amr Sharaf},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=HJNMYceCW},\n}", "keywords": ["Reinforcement Learning", "Structured Prediction", "Contextual Bandits", "Learning Reduction"], "authors": ["Hal Daum\u00e9 III", "John Langford", "Amr Sharaf"], "authorids": ["hal@umiacs.umd.edu", "jl@hunch.net", "amr@cs.umd.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825735103, "id": "ICLR.cc/2018/Conference/-/Paper353/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "HJNMYceCW", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper353/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper353/Authors|ICLR.cc/2018/Conference/Paper353/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper353/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper353/Authors|ICLR.cc/2018/Conference/Paper353/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper353/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper353/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper353/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper353/Reviewers", "ICLR.cc/2018/Conference/Paper353/Authors", "ICLR.cc/2018/Conference/Paper353/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825735103}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1516352103495, "tcdate": 1511446774338, "number": 1, "cdate": 1511446774338, "id": "ByCeFUNgz", "invitation": "ICLR.cc/2018/Conference/-/Paper353/Official_Review", "forum": "HJNMYceCW", "replyto": "HJNMYceCW", "signatures": ["ICLR.cc/2018/Conference/Paper353/AnonReviewer2"], "readers": ["everyone"], "content": {"title": "RESIDUAL LOSS PREDICTION: episodic RL and learning to search without incremental feedback", "rating": "7: Good paper, accept", "review": "The authors propose a new episodic reinforcement learning algorithm based on contextual bandit oracles.\nThe key specificity of this algorithm is its ability to deal with the credit assignment problem by learning automatically a progressive \"reward shaping\" (the residual losses) from a feedback that is only provided at the end of the epochs.\n\nThe paper is dense but well written. \n\nThe theoretical grounding is a bit thin or hard to follow.\nThe authors provide a few regret theoretical results (that I did not check deeply) obtained by reduction to \"value-aware\" contextual bandits.\n\nThe experimental section is solid. The method is evaluated on several RL environments against state of the art RL algorithms. It is also evaluated on bandit structured prediction tasks.\nAn interesting synthetic experiment (Figure 4) is also proposed to study the ability of the algorithm to work on both decomposable and non-decomposable structured prediction tasks.\n\n\nQuestion 1: The credit assignment approach you propose seems way more sophisticated than eligibility traces in TD learning. But sometimes old and simple methods are not that bad. Could you develop a bit on the relation between RESLOPE and eligibility traces ?\n\nQuestion 2: RESLOPE is built upon contextual bandits which require a stationary environment. Does RESLOPE inherit from this assumption?\n\n\nTypos:\npage 1 \n\"scalar loss that output.\" -> \"scalar loss.\"\n\", effectively a representation\" -> \". By effective we mean effective in term of credit assignment.\"\npage 5\n\"and MTR\" -> \"and DR\"\npage 6\n\"in simultaneously.\" -> ???\n\".In greedy\" -> \". In greedy\"\n", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Residual Loss Prediction: Reinforcement Learning With No Incremental Feedback", "abstract": "We consider reinforcement learning and bandit structured prediction problems with very sparse loss feedback: only at the end of an episode. We introduce a novel algorithm, RESIDUAL LOSS PREDICTION (RESLOPE), that solves such problems by automatically learning an internal representation of a denser reward function. RESLOPE operates as a reduction to contextual bandits, using its learned loss representation to solve the credit assignment problem, and a contextual bandit oracle to trade-off exploration and exploitation. RESLOPE enjoys a no-regret reduction-style theoretical guarantee and outperforms state of the art reinforcement learning algorithms in both MDP environments and bandit structured prediction settings.", "pdf": "/pdf/56ab4f2257300654c47e017e7ae3d440ab1150d8.pdf", "TL;DR": "We present a novel algorithm for solving reinforcement learning and bandit structured prediction problems with very sparse loss feedback.", "paperhash": "iii|residual_loss_prediction_reinforcement_learning_with_no_incremental_feedback", "_bibtex": "@inproceedings{\ndaum\u00e92018residual,\ntitle={{RESIDUAL} {LOSS} {PREDICTION}: {REINFORCEMENT} {LEARNING} {WITH} {NO} {INCREMENTAL} {FEEDBACK}},\nauthor={Hal Daum\u00e9 III and John Langford and Paul Mineiro and Amr Sharaf},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=HJNMYceCW},\n}", "keywords": ["Reinforcement Learning", "Structured Prediction", "Contextual Bandits", "Learning Reduction"], "authors": ["Hal Daum\u00e9 III", "John Langford", "Amr Sharaf"], "authorids": ["hal@umiacs.umd.edu", "jl@hunch.net", "amr@cs.umd.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642437617, "id": "ICLR.cc/2018/Conference/-/Paper353/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper353/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper353/AnonReviewer2", "ICLR.cc/2018/Conference/Paper353/AnonReviewer3", "ICLR.cc/2018/Conference/Paper353/AnonReviewer1"], "reply": {"forum": "HJNMYceCW", "replyto": "HJNMYceCW", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper353/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642437617}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515976555468, "tcdate": 1511817125262, "number": 3, "cdate": 1511817125262, "id": "r1ajkbceM", "invitation": "ICLR.cc/2018/Conference/-/Paper353/Official_Review", "forum": "HJNMYceCW", "replyto": "HJNMYceCW", "signatures": ["ICLR.cc/2018/Conference/Paper353/AnonReviewer1"], "readers": ["everyone"], "content": {"title": "Fairly novel approach for solving credit assignment in sparse reward RL, however comparison with other algorithms and explanations arent thorough enough to know if its a significant advance", "rating": "6: Marginally above acceptance threshold", "review": "The authors present a new RL algorithm for sparse reward tasks. The work is fairly novel in its approach, combining a learned reward estimator with a contextual bandit algorithm for exploration/exploitation. The paper was mostly clear in its exposition, however some additional information of the motivation for why the said reduction is better than simpler alternatives would help.  \n\nPros\n1. The results on bandit structured prediction problems are pretty good\n2. The idea of a learnt credit assignment function, and using that to separate credit assignment from the exploration/exploitation tradeoff is good. \n\nCons: \n1. The method seems fairly more complicated than PPO / A2C, yet those methods seem to perform equally well on the RL problems (Figure 2.). It also seems to be designed only for discrete action spaces.\n2. Reslope Boltzmann performs much worse than Reslope Bootstrap, thus having a bag of policies helps. However, in the comparison in Figures 2 and 3, the policy gradient methods dont have the advantage of using a bag of policies. A fairer comparison would be to compare with methods that use ensembles of Q-functions. (like this https://arxiv.org/abs/1706.01502 by Chen et al.). The Q learning methods in general would also have better sample efficiency than the policy gradient methods.\n3. The method claims to learn an internal representation of a denser reward function for the sparse reward problem, however the experimental analysis of this is pretty limited (Section 5.3). It would be useful to do a more thorough investigation of whether it learnt a good credit assignment function in the games. One way to do this would be to check the qualitative aspects of the function in a well understood game, like Blackjack.\n\nSuggestions:\n1. What is the advantage of the method over a simple RL method that predicts a reward at every step (such that the dense rewards add up to match the sparse reward for the episode), and uses this predicted dense reward to perform RL? This, and also a bigger discussion on prior bandit learning methods like LOLS will help under the context for why we\u2019re performing the reduction stated in the paper.  \n\nSignificance: While the method is novel and interesting, the experimental analysis and the explanations in the paper leave it unclear as to whether its significant compared to prior work.\n\nRevision: I thank the authors for addressing some of my concerns. The comparison with relative gain of bootstrap wrt ensemble of policies still needs more thorough experimentation, but the approach is novel and as the authors point out, does improve continually with better Contextual Bandit algorithms. I update my review to 6. ", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Residual Loss Prediction: Reinforcement Learning With No Incremental Feedback", "abstract": "We consider reinforcement learning and bandit structured prediction problems with very sparse loss feedback: only at the end of an episode. We introduce a novel algorithm, RESIDUAL LOSS PREDICTION (RESLOPE), that solves such problems by automatically learning an internal representation of a denser reward function. RESLOPE operates as a reduction to contextual bandits, using its learned loss representation to solve the credit assignment problem, and a contextual bandit oracle to trade-off exploration and exploitation. RESLOPE enjoys a no-regret reduction-style theoretical guarantee and outperforms state of the art reinforcement learning algorithms in both MDP environments and bandit structured prediction settings.", "pdf": "/pdf/56ab4f2257300654c47e017e7ae3d440ab1150d8.pdf", "TL;DR": "We present a novel algorithm for solving reinforcement learning and bandit structured prediction problems with very sparse loss feedback.", "paperhash": "iii|residual_loss_prediction_reinforcement_learning_with_no_incremental_feedback", "_bibtex": "@inproceedings{\ndaum\u00e92018residual,\ntitle={{RESIDUAL} {LOSS} {PREDICTION}: {REINFORCEMENT} {LEARNING} {WITH} {NO} {INCREMENTAL} {FEEDBACK}},\nauthor={Hal Daum\u00e9 III and John Langford and Paul Mineiro and Amr Sharaf},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=HJNMYceCW},\n}", "keywords": ["Reinforcement Learning", "Structured Prediction", "Contextual Bandits", "Learning Reduction"], "authors": ["Hal Daum\u00e9 III", "John Langford", "Amr Sharaf"], "authorids": ["hal@umiacs.umd.edu", "jl@hunch.net", "amr@cs.umd.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642437617, "id": "ICLR.cc/2018/Conference/-/Paper353/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper353/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper353/AnonReviewer2", "ICLR.cc/2018/Conference/Paper353/AnonReviewer3", "ICLR.cc/2018/Conference/Paper353/AnonReviewer1"], "reply": {"forum": "HJNMYceCW", "replyto": "HJNMYceCW", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper353/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642437617}}}, {"tddate": null, "ddate": null, "tmdate": 1515196858271, "tcdate": 1515189816625, "number": 4, "cdate": 1515189816625, "id": "rJeBU_a7M", "invitation": "ICLR.cc/2018/Conference/-/Paper353/Official_Comment", "forum": "HJNMYceCW", "replyto": "HJNMYceCW", "signatures": ["ICLR.cc/2018/Conference/Paper353/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper353/Authors"], "content": {"title": "Paper Structure, Exposure, and List of Changes", "comment": "The authors appreciate the reviewer\u2019s suggestions for improving the overall exposure of the paper. In order to make it easier for reviewers\u2019 to track the changes we kept the structure largely consistent with the original submission, but we\u2019ll take all of these comments into account in the final version.\n\n@ AnonReviewer3\nThanks for the clarification suggestions on the analysis; we can add explicit definitions of J, Q and V in the background material. The terms in the parentheses are the CB costs because these are exactly the residuals computed and shown as costs to the CB algorithm by construction (essentially the analysis says exactly what these costs should be). We will try to find a way to make this clearer. The issue of non-stationarity is discussed below in greater detail.\n\nList of changes in this version:\n\n1) Extended the discussion sections (section 6) to include some of the open problems and comments highlighted by the reviewers;\n2) Added Appendix K. This appendix includes experiments performed for the analysis of the loss representation for the grid world environment;\n3) Fixed all the typos highlighted by the reviewers;\n4) Updated Appendix H to include the set of values used for tuning the roll-out probability beta."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Residual Loss Prediction: Reinforcement Learning With No Incremental Feedback", "abstract": "We consider reinforcement learning and bandit structured prediction problems with very sparse loss feedback: only at the end of an episode. We introduce a novel algorithm, RESIDUAL LOSS PREDICTION (RESLOPE), that solves such problems by automatically learning an internal representation of a denser reward function. RESLOPE operates as a reduction to contextual bandits, using its learned loss representation to solve the credit assignment problem, and a contextual bandit oracle to trade-off exploration and exploitation. RESLOPE enjoys a no-regret reduction-style theoretical guarantee and outperforms state of the art reinforcement learning algorithms in both MDP environments and bandit structured prediction settings.", "pdf": "/pdf/56ab4f2257300654c47e017e7ae3d440ab1150d8.pdf", "TL;DR": "We present a novel algorithm for solving reinforcement learning and bandit structured prediction problems with very sparse loss feedback.", "paperhash": "iii|residual_loss_prediction_reinforcement_learning_with_no_incremental_feedback", "_bibtex": "@inproceedings{\ndaum\u00e92018residual,\ntitle={{RESIDUAL} {LOSS} {PREDICTION}: {REINFORCEMENT} {LEARNING} {WITH} {NO} {INCREMENTAL} {FEEDBACK}},\nauthor={Hal Daum\u00e9 III and John Langford and Paul Mineiro and Amr Sharaf},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=HJNMYceCW},\n}", "keywords": ["Reinforcement Learning", "Structured Prediction", "Contextual Bandits", "Learning Reduction"], "authors": ["Hal Daum\u00e9 III", "John Langford", "Amr Sharaf"], "authorids": ["hal@umiacs.umd.edu", "jl@hunch.net", "amr@cs.umd.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825735103, "id": "ICLR.cc/2018/Conference/-/Paper353/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "HJNMYceCW", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper353/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper353/Authors|ICLR.cc/2018/Conference/Paper353/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper353/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper353/Authors|ICLR.cc/2018/Conference/Paper353/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper353/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper353/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper353/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper353/Reviewers", "ICLR.cc/2018/Conference/Paper353/Authors", "ICLR.cc/2018/Conference/Paper353/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825735103}}}, {"tddate": null, "ddate": null, "tmdate": 1515195080571, "tcdate": 1515195080571, "number": 7, "cdate": 1515195080571, "id": "HkZ05YT7z", "invitation": "ICLR.cc/2018/Conference/-/Paper353/Official_Comment", "forum": "HJNMYceCW", "replyto": "Bk6bEOT7f", "signatures": ["ICLR.cc/2018/Conference/Paper353/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper353/Authors"], "content": {"title": "References", "comment": "[1] Kai-Wei Chang, Akshay Krishnamurthy, Alekh Agarwal, Hal Daume \u0301, III, and John Langford. Learning to search better than your teacher. In Proceedings of the 32Nd International Conference on International Conference on Machine Learning - Volume 37, ICML\u201915, pp. 2058\u20132066. JMLR.org, 2015. URL http://dl.acm.org/citation.cfm?id=3045118.3045337. \n[2] Amr Sharaf and Hal Daume \u0301, III. Structured prediction via learning to search under bandit feedback. In Proceedings of the 2nd Workshop on Structured Prediction for Natural Language Processing, pp. 17\u201326, Copenhagen, Denmark, September 2017. Association for Computational Linguistics. URL http://www.aclweb.org/anthology/W17-4304.\n[3] Miroslav Dud \u0301\u0131k, Dumitru Erhan, John Langford, and Lihong Li. Doubly robust policy evaluation and optimization. Statist. Sci., 29(4):485\u2013511, 11 2014. doi: 10.1214/14-STS500. URL https: //doi.org/10.1214/14-STS500.\n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Residual Loss Prediction: Reinforcement Learning With No Incremental Feedback", "abstract": "We consider reinforcement learning and bandit structured prediction problems with very sparse loss feedback: only at the end of an episode. We introduce a novel algorithm, RESIDUAL LOSS PREDICTION (RESLOPE), that solves such problems by automatically learning an internal representation of a denser reward function. RESLOPE operates as a reduction to contextual bandits, using its learned loss representation to solve the credit assignment problem, and a contextual bandit oracle to trade-off exploration and exploitation. RESLOPE enjoys a no-regret reduction-style theoretical guarantee and outperforms state of the art reinforcement learning algorithms in both MDP environments and bandit structured prediction settings.", "pdf": "/pdf/56ab4f2257300654c47e017e7ae3d440ab1150d8.pdf", "TL;DR": "We present a novel algorithm for solving reinforcement learning and bandit structured prediction problems with very sparse loss feedback.", "paperhash": "iii|residual_loss_prediction_reinforcement_learning_with_no_incremental_feedback", "_bibtex": "@inproceedings{\ndaum\u00e92018residual,\ntitle={{RESIDUAL} {LOSS} {PREDICTION}: {REINFORCEMENT} {LEARNING} {WITH} {NO} {INCREMENTAL} {FEEDBACK}},\nauthor={Hal Daum\u00e9 III and John Langford and Paul Mineiro and Amr Sharaf},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=HJNMYceCW},\n}", "keywords": ["Reinforcement Learning", "Structured Prediction", "Contextual Bandits", "Learning Reduction"], "authors": ["Hal Daum\u00e9 III", "John Langford", "Amr Sharaf"], "authorids": ["hal@umiacs.umd.edu", "jl@hunch.net", "amr@cs.umd.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825735103, "id": "ICLR.cc/2018/Conference/-/Paper353/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "HJNMYceCW", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper353/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper353/Authors|ICLR.cc/2018/Conference/Paper353/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper353/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper353/Authors|ICLR.cc/2018/Conference/Paper353/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper353/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper353/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper353/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper353/Reviewers", "ICLR.cc/2018/Conference/Paper353/Authors", "ICLR.cc/2018/Conference/Paper353/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825735103}}}, {"tddate": null, "ddate": null, "tmdate": 1515195062152, "tcdate": 1515189252801, "number": 2, "cdate": 1515189252801, "id": "Bk6bEOT7f", "invitation": "ICLR.cc/2018/Conference/-/Paper353/Official_Comment", "forum": "HJNMYceCW", "replyto": "r1ajkbceM", "signatures": ["ICLR.cc/2018/Conference/Paper353/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper353/Authors"], "content": {"title": "Authors' Response to AnonReviewer1", "comment": "Authors\u2019 Response to highlighted cons:\n\nRESLOPE is more complicated than PPO / A2C\n\nWe suppose this depends on how \"complicated\" is measured. Given a known, fixed contextual bandit algorithm, RESLOPE becomes quite straightforward to implement, certainly more simple (in lines of code) than PPO/A2C would be if you did not have access to, for instance, an autodiff toolkit. Given good lower-level abstractions (autodiff, CB, etc.), both are quote straightforward to implement. Furthermore, RESLOPE comes with significant advantage over PPO/A2C: RESLOPE continually improves as better contextual bandit algorithms become available, a property lacked by PPO/A2C. RESLOPE also fares well empirically, and comes with some nice theoretical guarantees (which, for instance, A2C lacks).\n\t\n\nRESLOPE and Continuous Action Spaces\n\nIt\u2019s true that RESLOPE is designed for discrete action spaces. Extension to continuous action spaces remains an open problem. We have updated the discussion section (section 6) to include this extension as a future work.\n\nComparison with Ensemble Learning Methods\n\nThis is a great question, thank you for raising it! Indeed, the original submission did not adequately separate gains from more complex representation (bag of policies) and alternative estimation methods (bootstrap).\n\nTo address this, we have done an addition set of experiments to answer the following question empirically: what is the relative gain of bootstrap exploration with respect to using an ensemble of policies.\n\nEnsemble exploration trains a bag of multiple policies simultaneously. Each policy in the bag generates a Boltzmann probability distribution over actions. These probability distributions are aggregated by averaging. An action is sampled from the aggregated distribution. This has the property of identical network representation, but not using the Bootstrap estimation method.\n\nThe result is that in the MTR setting, the Ensemble method is worse than Bootstrap by factors of 3.52, 0.757 and 0.815 respectively on the first three RL tests and, surprisingly, better by a factor of 6.39 on the last. We plan to complete these experiments with more rigor, and extend to the SP setting, in a final version.\n\nEvaluating the learned loss representation for a well-understood RL Environment\n\nWe added additional experiments for evaluating the learned loss representation in the grid world reinforcement \nlearning environment (Appendix K). This experiment is akin to the experimental setting in section 5.3, however\nit\u2019s performed on the grid world RL environment, where the quantitative aspects of the loss function is well understood. Results are very similar to the structured prediction setting (section 5.3). Performance is better when the loss is additive vs non-additive.\n\nAuthors\u2019 Response to proposed suggestions:\n\nRESLOPE & Reward Prediction at Every Step\n\nWe\u2019re not aware of a different way for learning the reward in every time step without computing the residual loss as we do in RESLOPE. After estimating the residual losses, RESLOPE reduces the problem to a contextual bandit oracle. This is crucial for accounting for the exploration probability and is necessary for obtaining an unbiased and convergent estimates for the loss. It\u2019s not clear how standard RL can account for the exploration probability when the estimated rewards is used instead of the true reward values, and thus, we didn\u2019t consider this approach in our experiments. (But we're open to suggestions!)\n\nRESLOPE vs LOLS\n\nBoth RESLOPE and the bandit version of LOLS (Chang et al., 2015) aim to learn from sparse reward signals by building on the bandit learning to search frameworks. As highlighted in the discussion section (Section 6), they differ significantly in both theory and practice:\n The \u201cbandit\u201d version of LOLS was analyzed theoretically but not empirically in the original paper; Sharaf & Daum\u00e9 (2017) found that it failed to learn empirically;\nRESLOPE learns a representation for the episodic loss as a decomposition over time-steps, while LOLS learns directly from the episodic loss signal, this is prone to high variance and doesn\u2019t work in practice (Sharaf & Daum\u00e9 2017);\nRESLOPE separates the problem of credit assignment from the exploration problem via a reduction to a contextual bandit oracle. This enables the usage of better variance reduction techniques (e.g. Doubly Robust cost estimation & Multi-task Regression) as well as different exploration algorithms (e.g. bootstrap exploration). LOLS can only use Inverse Propensity Scoring and greedy exploration."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Residual Loss Prediction: Reinforcement Learning With No Incremental Feedback", "abstract": "We consider reinforcement learning and bandit structured prediction problems with very sparse loss feedback: only at the end of an episode. We introduce a novel algorithm, RESIDUAL LOSS PREDICTION (RESLOPE), that solves such problems by automatically learning an internal representation of a denser reward function. RESLOPE operates as a reduction to contextual bandits, using its learned loss representation to solve the credit assignment problem, and a contextual bandit oracle to trade-off exploration and exploitation. RESLOPE enjoys a no-regret reduction-style theoretical guarantee and outperforms state of the art reinforcement learning algorithms in both MDP environments and bandit structured prediction settings.", "pdf": "/pdf/56ab4f2257300654c47e017e7ae3d440ab1150d8.pdf", "TL;DR": "We present a novel algorithm for solving reinforcement learning and bandit structured prediction problems with very sparse loss feedback.", "paperhash": "iii|residual_loss_prediction_reinforcement_learning_with_no_incremental_feedback", "_bibtex": "@inproceedings{\ndaum\u00e92018residual,\ntitle={{RESIDUAL} {LOSS} {PREDICTION}: {REINFORCEMENT} {LEARNING} {WITH} {NO} {INCREMENTAL} {FEEDBACK}},\nauthor={Hal Daum\u00e9 III and John Langford and Paul Mineiro and Amr Sharaf},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=HJNMYceCW},\n}", "keywords": ["Reinforcement Learning", "Structured Prediction", "Contextual Bandits", "Learning Reduction"], "authors": ["Hal Daum\u00e9 III", "John Langford", "Amr Sharaf"], "authorids": ["hal@umiacs.umd.edu", "jl@hunch.net", "amr@cs.umd.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825735103, "id": "ICLR.cc/2018/Conference/-/Paper353/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "HJNMYceCW", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper353/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper353/Authors|ICLR.cc/2018/Conference/Paper353/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper353/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper353/Authors|ICLR.cc/2018/Conference/Paper353/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper353/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper353/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper353/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper353/Reviewers", "ICLR.cc/2018/Conference/Paper353/Authors", "ICLR.cc/2018/Conference/Paper353/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825735103}}}, {"tddate": null, "ddate": null, "tmdate": 1515191297610, "tcdate": 1515188994908, "number": 1, "cdate": 1515188994908, "id": "HkjWXd67G", "invitation": "ICLR.cc/2018/Conference/-/Paper353/Official_Comment", "forum": "HJNMYceCW", "replyto": "ByCeFUNgz", "signatures": ["ICLR.cc/2018/Conference/Paper353/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper353/Authors"], "content": {"title": "Authors' Response To AnonReviewer2", "comment": "Question 1: RESLOPE and Eligibility Traces\n\nBoth RESLOPE and eligibility trace algorithms tackles the problem of credit assignment when learning by interaction with the environment. In eligibility trace algorithms, e.g. TD(\u03bb), a state is eligible for credit assignment if it was recently visited, with the eligibility declining over time [1]. In our episodic setting, our notion of eligibility decay is \"the end of the episode\": any reward from this episode is eligible, and reward from other episodes is not. The \"degree\" of eligibility is most similar to the probability of the exploration event which created the observation (the deviation). This is particularly important for getting unbiased & convergent estimates.\n\nQuestion 2: RESLOPE and Non-stationary Environments\n\nThank you for raising this point: we were remiss to not include this in the initial draft and have now added a bit of discussion in the last section. The issue pointed out here is that because the policy is changing, the reward decomposition is changing, so the costs that the CB algorithm sees are also changing. While many CB algorithms operate effectively under shifting distributions of x (e.g. most online CB algorithms), many cannot work with the \"label distribution\" shifts. There has been some work on CB in an adversarial environment, but to our knowledge none of these algorithms is efficient. It seems likely that the RESLOPE setting is probably not as bad as full adversarial, and perhaps something could be done in the middle, but this is still an open question.\n\n[1] Satinder P. Singh and Richard S. Sutton, Reinforcement learning with replacing eligibility traces, pp. 123\u2013158, Springer US, Boston, MA, 1996.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Residual Loss Prediction: Reinforcement Learning With No Incremental Feedback", "abstract": "We consider reinforcement learning and bandit structured prediction problems with very sparse loss feedback: only at the end of an episode. We introduce a novel algorithm, RESIDUAL LOSS PREDICTION (RESLOPE), that solves such problems by automatically learning an internal representation of a denser reward function. RESLOPE operates as a reduction to contextual bandits, using its learned loss representation to solve the credit assignment problem, and a contextual bandit oracle to trade-off exploration and exploitation. RESLOPE enjoys a no-regret reduction-style theoretical guarantee and outperforms state of the art reinforcement learning algorithms in both MDP environments and bandit structured prediction settings.", "pdf": "/pdf/56ab4f2257300654c47e017e7ae3d440ab1150d8.pdf", "TL;DR": "We present a novel algorithm for solving reinforcement learning and bandit structured prediction problems with very sparse loss feedback.", "paperhash": "iii|residual_loss_prediction_reinforcement_learning_with_no_incremental_feedback", "_bibtex": "@inproceedings{\ndaum\u00e92018residual,\ntitle={{RESIDUAL} {LOSS} {PREDICTION}: {REINFORCEMENT} {LEARNING} {WITH} {NO} {INCREMENTAL} {FEEDBACK}},\nauthor={Hal Daum\u00e9 III and John Langford and Paul Mineiro and Amr Sharaf},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=HJNMYceCW},\n}", "keywords": ["Reinforcement Learning", "Structured Prediction", "Contextual Bandits", "Learning Reduction"], "authors": ["Hal Daum\u00e9 III", "John Langford", "Amr Sharaf"], "authorids": ["hal@umiacs.umd.edu", "jl@hunch.net", "amr@cs.umd.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825735103, "id": "ICLR.cc/2018/Conference/-/Paper353/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "HJNMYceCW", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper353/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper353/Authors|ICLR.cc/2018/Conference/Paper353/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper353/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper353/Authors|ICLR.cc/2018/Conference/Paper353/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper353/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper353/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper353/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper353/Reviewers", "ICLR.cc/2018/Conference/Paper353/Authors", "ICLR.cc/2018/Conference/Paper353/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825735103}}}, {"tddate": null, "ddate": null, "tmdate": 1515191117987, "tcdate": 1515189431297, "number": 3, "cdate": 1515189431297, "id": "Hy1aVOp7z", "invitation": "ICLR.cc/2018/Conference/-/Paper353/Official_Comment", "forum": "HJNMYceCW", "replyto": "r14M3-KxM", "signatures": ["ICLR.cc/2018/Conference/Paper353/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper353/Authors"], "content": {"title": "Authors' Response to AnonReviewer3", "comment": "How does RESLOPE create x?\n\nRESLOPE learns a representation for the input x on the fly using a neural  network architecture as described in Appendix H. We start off with a simple feature representation in all the problems and the model learns a better representation using a neural network architecture. We\u2019d appreciate any comments regarding the clarity of this section and we\u2019ll incorporate any suggestions in the final version.\n\nAs a recap: For English POS tagging and dependency parsing we use 300 dimensional word embeddings, 300 dimensional 1 layer LSTM, and 2 layer 300 dimensional RNN policy; for the Chinese POS tagging: we use 300 dimensional word embeddings, 50 dimensional two layer LSTM, one layer 50 dimensional RNN policy. For reinforcement learning, we chose a two layer RNN policy with 20 dimensional vectors. We start off with a simple initial state representation and learn a better representation using the policy network. The initial state representation is task dependant. For instance, in cartpole, the state is represented by a four dimensional vector: [position of cart, velocity of cart, angle of pole, rotation rate of pole].\n\nWhat happens if we don't have a good way to generate x and it must be learned as well?\n\nThis is the case in all our experiments. We start-off with simple features and learn a better representation on the fly using a neural network architecture. For structured prediction tasks, the simple features are just the word indices in the dictionary, we learn word embedding for these words and keep track of the state using an RNN architecture (as described above). For RL tasks we start off by simple features of the current state and feed these features to an RNN network to compute the final input x. \n\nIf x is learned on the fly, how does that impact the theoretical results?\n\nIn the single deviation case, one can think of the \"x\" used at the deviation point as the result of applying a deep (unrolled) neural network to the base features (eg word indices). The contextual bandit problem, then, is to learn that neural network well. This basically reduces the question to: are there good CB algorithms for learning neural networks. But the analysis for RESLOPE holds.\n\nIn the multi-deviation case, things are much more complicated. In fact, this is one of the things that blocked us from a good analysis in the multi-deviation setting. The problem is that if you deviate at steps 2 and 5, what might be good for improving the reward prediction at step 2 could be bad for step 5 or vice versa, because these two decisions are tied through the network structure as well as the action sequence. (This issue also arises in other learning to search algorithms, like CPI and Searn, which effectively use a sufficiently small learning rate the ensure that there's only one deviation per episode.)\n\t\n\nModeling Notions of future Reward\n\nRather than modeling the Q-function, RESLOPE aims at modeling the advantage function instead, which could be easier to learn in several cases. Learning either the Q-function or the advantage function is sufficient for extracting a greedy policy. Lemma 1 shows that the difference in total loss between two policies can always be computed exactly as a sum of per-time-step advantages of one over the other. We chose to learn the advantages rather than Q-functions as it might be easier to learn and more local. For example, in POS tagging, learning advantages corresponds to learning whether or not the policy made a prediction mistake at a single word which is much easier to learn than the Q-function which requires keeping track of the number of mistakes made from the beginning of the sequence.\n\nReference Policy Used & Value for Beta\n\nFor the structured prediction experiments, the reference policy is a pre-trained model on supervised data (Appendix G). The roll-out probability \u03b2 is a hyper-parameter that we tune along all the other hyperparameters as described in Appendix H. We pick the best value for \u03b2 from the set: {0.0, 0.5, 1.0}. \n\nFor the reinforcement learning experiments, we don\u2019t assume access to a reference policy and the roll-out probability \u03b2 is always set to zero.\n\nNote, though, that in the multi-deviation algorithm, there is not a separate notion of a \"rollout\" policy, like there is in the single-deviation setting.\n\t\t\t\nDifference in performance between the RL and Structured Prediction\n \nThis is a good question that unfortunately we don't have a good answer to; we are particularly confused by the poor performance of RESLOPE on cartpole, which is the only place where its behavior is really subpar to even simple approaches like reinforce with baseline (reinforce without a baseline fails quite poorly here, much worse than RESLOPE). This could partially be because RESLOPE came out of a line of work focusing on structured prediction and so the algorithmic style simply is a better fit there, but that's not at all a convincing answer. More work is needed here.\n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Residual Loss Prediction: Reinforcement Learning With No Incremental Feedback", "abstract": "We consider reinforcement learning and bandit structured prediction problems with very sparse loss feedback: only at the end of an episode. We introduce a novel algorithm, RESIDUAL LOSS PREDICTION (RESLOPE), that solves such problems by automatically learning an internal representation of a denser reward function. RESLOPE operates as a reduction to contextual bandits, using its learned loss representation to solve the credit assignment problem, and a contextual bandit oracle to trade-off exploration and exploitation. RESLOPE enjoys a no-regret reduction-style theoretical guarantee and outperforms state of the art reinforcement learning algorithms in both MDP environments and bandit structured prediction settings.", "pdf": "/pdf/56ab4f2257300654c47e017e7ae3d440ab1150d8.pdf", "TL;DR": "We present a novel algorithm for solving reinforcement learning and bandit structured prediction problems with very sparse loss feedback.", "paperhash": "iii|residual_loss_prediction_reinforcement_learning_with_no_incremental_feedback", "_bibtex": "@inproceedings{\ndaum\u00e92018residual,\ntitle={{RESIDUAL} {LOSS} {PREDICTION}: {REINFORCEMENT} {LEARNING} {WITH} {NO} {INCREMENTAL} {FEEDBACK}},\nauthor={Hal Daum\u00e9 III and John Langford and Paul Mineiro and Amr Sharaf},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=HJNMYceCW},\n}", "keywords": ["Reinforcement Learning", "Structured Prediction", "Contextual Bandits", "Learning Reduction"], "authors": ["Hal Daum\u00e9 III", "John Langford", "Amr Sharaf"], "authorids": ["hal@umiacs.umd.edu", "jl@hunch.net", "amr@cs.umd.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825735103, "id": "ICLR.cc/2018/Conference/-/Paper353/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "HJNMYceCW", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper353/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper353/Authors|ICLR.cc/2018/Conference/Paper353/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper353/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper353/Authors|ICLR.cc/2018/Conference/Paper353/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper353/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper353/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper353/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper353/Reviewers", "ICLR.cc/2018/Conference/Paper353/Authors", "ICLR.cc/2018/Conference/Paper353/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825735103}}}, {"tddate": null, "ddate": null, "tmdate": 1515190769938, "tcdate": 1515190769938, "number": 6, "cdate": 1515190769938, "id": "Bkcg5uT7z", "invitation": "ICLR.cc/2018/Conference/-/Paper353/Official_Comment", "forum": "HJNMYceCW", "replyto": "r14M3-KxM", "signatures": ["ICLR.cc/2018/Conference/Paper353/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper353/Authors"], "content": {"title": "Results for 1-Deviation vs Multiple Deviation", "comment": "It\u2019s true that the empirical results for the one-step deviation setting is are worse (particularly in terms of the number of samples needed to learn) than doing multiple deviations. While we don\u2019t have a theoretical analysis for the multi-deviation case, empirically, we found this to be crucial empirically. Although the generated samples for the same episode are not independent, this is made-up for by the huge increase in the number of available samples for training. This is a case where there is a gap between what we can prove theoretically and what works best in practice. We can restructure the outline of the paper to promote the display of the 1-step deviation results on earlier exposure. \n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Residual Loss Prediction: Reinforcement Learning With No Incremental Feedback", "abstract": "We consider reinforcement learning and bandit structured prediction problems with very sparse loss feedback: only at the end of an episode. We introduce a novel algorithm, RESIDUAL LOSS PREDICTION (RESLOPE), that solves such problems by automatically learning an internal representation of a denser reward function. RESLOPE operates as a reduction to contextual bandits, using its learned loss representation to solve the credit assignment problem, and a contextual bandit oracle to trade-off exploration and exploitation. RESLOPE enjoys a no-regret reduction-style theoretical guarantee and outperforms state of the art reinforcement learning algorithms in both MDP environments and bandit structured prediction settings.", "pdf": "/pdf/56ab4f2257300654c47e017e7ae3d440ab1150d8.pdf", "TL;DR": "We present a novel algorithm for solving reinforcement learning and bandit structured prediction problems with very sparse loss feedback.", "paperhash": "iii|residual_loss_prediction_reinforcement_learning_with_no_incremental_feedback", "_bibtex": "@inproceedings{\ndaum\u00e92018residual,\ntitle={{RESIDUAL} {LOSS} {PREDICTION}: {REINFORCEMENT} {LEARNING} {WITH} {NO} {INCREMENTAL} {FEEDBACK}},\nauthor={Hal Daum\u00e9 III and John Langford and Paul Mineiro and Amr Sharaf},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=HJNMYceCW},\n}", "keywords": ["Reinforcement Learning", "Structured Prediction", "Contextual Bandits", "Learning Reduction"], "authors": ["Hal Daum\u00e9 III", "John Langford", "Amr Sharaf"], "authorids": ["hal@umiacs.umd.edu", "jl@hunch.net", "amr@cs.umd.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825735103, "id": "ICLR.cc/2018/Conference/-/Paper353/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "HJNMYceCW", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper353/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper353/Authors|ICLR.cc/2018/Conference/Paper353/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper353/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper353/Authors|ICLR.cc/2018/Conference/Paper353/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper353/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper353/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper353/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper353/Reviewers", "ICLR.cc/2018/Conference/Paper353/Authors", "ICLR.cc/2018/Conference/Paper353/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825735103}}}], "count": 17}