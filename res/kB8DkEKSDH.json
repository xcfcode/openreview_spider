{"notes": [{"id": "kB8DkEKSDH", "original": "LWb7nda5Gqh", "number": 3784, "cdate": 1601308421200, "ddate": null, "tcdate": 1601308421200, "tmdate": 1614985678588, "tddate": null, "forum": "kB8DkEKSDH", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Hellinger Distance Constrained Regression", "authorids": ["~Egor_Rotinov1"], "authors": ["Egor Rotinov"], "keywords": ["offline", "Reinforcement Learning", "off-policy", "control"], "abstract": "This paper introduces an off-policy reinforcement learning method that uses Hellinger distance between sampling policy (from what samples were collected) and current policy (policy being optimized) as a constraint. \nHellinger distance squared multiplied by two is greater than or equal to total variation distance squared and less than or equal to Kullback-Leibler divergence, therefore a lower bound for expected discounted return for the new policy is improved compared to the lower bound for training with KL. \nAlso, Hellinger distance is less than or equal to 1, so there is a policy-independent lower bound for expected discounted return. \nHDCR is capable of training with Experience Replay, a common setting for distributed RL when collecting trajectories using different policies and learning from this data centralized. \nHDCR shows results comparable to or better than Advantage-weighted Behavior Model and Advantage-Weighted Regression on MuJoCo tasks using tiny offline datasets collected by random agents. On bigger datasets (100k timesteps) obtained by pretrained behavioral policy, HDCR outperforms ABM and AWR methods on 3 out of 4 tasks. ", "one-sentence_summary": "This paper presents an offline reinforcement learning method based on the Hellinger distance constraint.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "rotinov|hellinger_distance_constrained_regression", "pdf": "/pdf/b2424686d8d6aba3981f12c8c0777b99db8fc501.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=QmXbHQwKa7", "_bibtex": "@misc{\nrotinov2021hellinger,\ntitle={Hellinger Distance Constrained Regression},\nauthor={Egor Rotinov},\nyear={2021},\nurl={https://openreview.net/forum?id=kB8DkEKSDH}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "kOtJnMbqed7", "original": null, "number": 1, "cdate": 1610040475947, "ddate": null, "tcdate": 1610040475947, "tmdate": 1610474080462, "tddate": null, "forum": "kB8DkEKSDH", "replyto": "kB8DkEKSDH", "invitation": "ICLR.cc/2021/Conference/Paper3784/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "The reviewer concerns generally centered around the novelty of replacing the distance metric for a policy constraint. While the authors clarified many of the reviewer concerns and added some additional comparisons, in the end it was not clear why the proposed approach was interesting: while it is true that this particular distance metric has not been evaluated in prior work, and the result would have been interesting if it resulted in some clear benefits either empirically or theoretically, in the absence of clear and unambiguous benefit, it's not clear how valuable this concept really is. After discussion, the reviewers generally found the paper to not be ready for publication in its present state."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hellinger Distance Constrained Regression", "authorids": ["~Egor_Rotinov1"], "authors": ["Egor Rotinov"], "keywords": ["offline", "Reinforcement Learning", "off-policy", "control"], "abstract": "This paper introduces an off-policy reinforcement learning method that uses Hellinger distance between sampling policy (from what samples were collected) and current policy (policy being optimized) as a constraint. \nHellinger distance squared multiplied by two is greater than or equal to total variation distance squared and less than or equal to Kullback-Leibler divergence, therefore a lower bound for expected discounted return for the new policy is improved compared to the lower bound for training with KL. \nAlso, Hellinger distance is less than or equal to 1, so there is a policy-independent lower bound for expected discounted return. \nHDCR is capable of training with Experience Replay, a common setting for distributed RL when collecting trajectories using different policies and learning from this data centralized. \nHDCR shows results comparable to or better than Advantage-weighted Behavior Model and Advantage-Weighted Regression on MuJoCo tasks using tiny offline datasets collected by random agents. On bigger datasets (100k timesteps) obtained by pretrained behavioral policy, HDCR outperforms ABM and AWR methods on 3 out of 4 tasks. ", "one-sentence_summary": "This paper presents an offline reinforcement learning method based on the Hellinger distance constraint.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "rotinov|hellinger_distance_constrained_regression", "pdf": "/pdf/b2424686d8d6aba3981f12c8c0777b99db8fc501.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=QmXbHQwKa7", "_bibtex": "@misc{\nrotinov2021hellinger,\ntitle={Hellinger Distance Constrained Regression},\nauthor={Egor Rotinov},\nyear={2021},\nurl={https://openreview.net/forum?id=kB8DkEKSDH}\n}"}, "tags": [], "invitation": {"reply": {"forum": "kB8DkEKSDH", "replyto": "kB8DkEKSDH", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040475933, "tmdate": 1610474080446, "id": "ICLR.cc/2021/Conference/Paper3784/-/Decision"}}}, {"id": "1SGzy2WDuCz", "original": null, "number": 9, "cdate": 1606101398948, "ddate": null, "tcdate": 1606101398948, "tmdate": 1606101398948, "tddate": null, "forum": "kB8DkEKSDH", "replyto": "S9XhelPzTBC", "invitation": "ICLR.cc/2021/Conference/Paper3784/-/Official_Comment", "content": {"title": "Read authors' response", "comment": "I have read the author's response and the updated experiments. Unfortunately, the empirical results are simply not convincing at all. Even just compared against the ABM and AWR baselines, the proposed method does not demonstrate a clear advantage (final returns are often within 1-std deviation of each other, while learning curves generally look very similar with no clear winner), whlie being significantly outperformed by BCQ. \n\nRegarding BCQ taking the gradient through the Q-function: I don't see any inherent reason why taking the gradient through the Q function necessarily leads to better results. It could certainly lead to faster optimization, but that isn't necessarily very relevant in an offline RL setting. Additionally, in the AWR paper, their results in both online and offline experiments were reasonably competitive with RL methods that used Q-function gradients. I would recommend the authors make a more clear and explicit argument as to when using critic gradients like BCQ should perform poorly and empirically demonstrate a setting where such methods fail and methods like HDCR do. "}, "signatures": ["ICLR.cc/2021/Conference/Paper3784/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3784/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hellinger Distance Constrained Regression", "authorids": ["~Egor_Rotinov1"], "authors": ["Egor Rotinov"], "keywords": ["offline", "Reinforcement Learning", "off-policy", "control"], "abstract": "This paper introduces an off-policy reinforcement learning method that uses Hellinger distance between sampling policy (from what samples were collected) and current policy (policy being optimized) as a constraint. \nHellinger distance squared multiplied by two is greater than or equal to total variation distance squared and less than or equal to Kullback-Leibler divergence, therefore a lower bound for expected discounted return for the new policy is improved compared to the lower bound for training with KL. \nAlso, Hellinger distance is less than or equal to 1, so there is a policy-independent lower bound for expected discounted return. \nHDCR is capable of training with Experience Replay, a common setting for distributed RL when collecting trajectories using different policies and learning from this data centralized. \nHDCR shows results comparable to or better than Advantage-weighted Behavior Model and Advantage-Weighted Regression on MuJoCo tasks using tiny offline datasets collected by random agents. On bigger datasets (100k timesteps) obtained by pretrained behavioral policy, HDCR outperforms ABM and AWR methods on 3 out of 4 tasks. ", "one-sentence_summary": "This paper presents an offline reinforcement learning method based on the Hellinger distance constraint.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "rotinov|hellinger_distance_constrained_regression", "pdf": "/pdf/b2424686d8d6aba3981f12c8c0777b99db8fc501.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=QmXbHQwKa7", "_bibtex": "@misc{\nrotinov2021hellinger,\ntitle={Hellinger Distance Constrained Regression},\nauthor={Egor Rotinov},\nyear={2021},\nurl={https://openreview.net/forum?id=kB8DkEKSDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "kB8DkEKSDH", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3784/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3784/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3784/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3784/Authors|ICLR.cc/2021/Conference/Paper3784/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3784/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923834005, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3784/-/Official_Comment"}}}, {"id": "4FSCHtZmIAn", "original": null, "number": 5, "cdate": 1605787102729, "ddate": null, "tcdate": 1605787102729, "tmdate": 1605787102729, "tddate": null, "forum": "kB8DkEKSDH", "replyto": "j89VeJR7xh9", "invitation": "ICLR.cc/2021/Conference/Paper3784/-/Official_Comment", "content": {"title": "Response to AnonReviewer4", "comment": "Thank you for the review.\n1. We do not know any papers where authors propose new metrics (except KL and Total Variation Divergence) which use is motivated theoretically and offline method for this metric is derived. The main purpose of the paper was to derive an offline method that will provide an alternative to FQI, AWR, and AWAC methods, which use KL divergence.\n2. Actually, using Hellinger distance allow us to make bigger steps because the distance's derivative asymptotically tends to zero."}, "signatures": ["ICLR.cc/2021/Conference/Paper3784/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3784/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hellinger Distance Constrained Regression", "authorids": ["~Egor_Rotinov1"], "authors": ["Egor Rotinov"], "keywords": ["offline", "Reinforcement Learning", "off-policy", "control"], "abstract": "This paper introduces an off-policy reinforcement learning method that uses Hellinger distance between sampling policy (from what samples were collected) and current policy (policy being optimized) as a constraint. \nHellinger distance squared multiplied by two is greater than or equal to total variation distance squared and less than or equal to Kullback-Leibler divergence, therefore a lower bound for expected discounted return for the new policy is improved compared to the lower bound for training with KL. \nAlso, Hellinger distance is less than or equal to 1, so there is a policy-independent lower bound for expected discounted return. \nHDCR is capable of training with Experience Replay, a common setting for distributed RL when collecting trajectories using different policies and learning from this data centralized. \nHDCR shows results comparable to or better than Advantage-weighted Behavior Model and Advantage-Weighted Regression on MuJoCo tasks using tiny offline datasets collected by random agents. On bigger datasets (100k timesteps) obtained by pretrained behavioral policy, HDCR outperforms ABM and AWR methods on 3 out of 4 tasks. ", "one-sentence_summary": "This paper presents an offline reinforcement learning method based on the Hellinger distance constraint.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "rotinov|hellinger_distance_constrained_regression", "pdf": "/pdf/b2424686d8d6aba3981f12c8c0777b99db8fc501.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=QmXbHQwKa7", "_bibtex": "@misc{\nrotinov2021hellinger,\ntitle={Hellinger Distance Constrained Regression},\nauthor={Egor Rotinov},\nyear={2021},\nurl={https://openreview.net/forum?id=kB8DkEKSDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "kB8DkEKSDH", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3784/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3784/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3784/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3784/Authors|ICLR.cc/2021/Conference/Paper3784/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3784/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923834005, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3784/-/Official_Comment"}}}, {"id": "Rj_GV35b4s6", "original": null, "number": 4, "cdate": 1605786509603, "ddate": null, "tcdate": 1605786509603, "tmdate": 1605786509603, "tddate": null, "forum": "kB8DkEKSDH", "replyto": "hV0TNUsII3F", "invitation": "ICLR.cc/2021/Conference/Paper3784/-/Official_Comment", "content": {"title": "Response to AnonReviewer2", "comment": "Thank you for your review.\nWe carefully revised the paper and clarified the mentioned phrases as well as some others. Also, we provided a y-axis title, which is an averaged episode reward of policies during evaluation. X-axes are iterations of the training."}, "signatures": ["ICLR.cc/2021/Conference/Paper3784/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3784/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hellinger Distance Constrained Regression", "authorids": ["~Egor_Rotinov1"], "authors": ["Egor Rotinov"], "keywords": ["offline", "Reinforcement Learning", "off-policy", "control"], "abstract": "This paper introduces an off-policy reinforcement learning method that uses Hellinger distance between sampling policy (from what samples were collected) and current policy (policy being optimized) as a constraint. \nHellinger distance squared multiplied by two is greater than or equal to total variation distance squared and less than or equal to Kullback-Leibler divergence, therefore a lower bound for expected discounted return for the new policy is improved compared to the lower bound for training with KL. \nAlso, Hellinger distance is less than or equal to 1, so there is a policy-independent lower bound for expected discounted return. \nHDCR is capable of training with Experience Replay, a common setting for distributed RL when collecting trajectories using different policies and learning from this data centralized. \nHDCR shows results comparable to or better than Advantage-weighted Behavior Model and Advantage-Weighted Regression on MuJoCo tasks using tiny offline datasets collected by random agents. On bigger datasets (100k timesteps) obtained by pretrained behavioral policy, HDCR outperforms ABM and AWR methods on 3 out of 4 tasks. ", "one-sentence_summary": "This paper presents an offline reinforcement learning method based on the Hellinger distance constraint.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "rotinov|hellinger_distance_constrained_regression", "pdf": "/pdf/b2424686d8d6aba3981f12c8c0777b99db8fc501.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=QmXbHQwKa7", "_bibtex": "@misc{\nrotinov2021hellinger,\ntitle={Hellinger Distance Constrained Regression},\nauthor={Egor Rotinov},\nyear={2021},\nurl={https://openreview.net/forum?id=kB8DkEKSDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "kB8DkEKSDH", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3784/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3784/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3784/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3784/Authors|ICLR.cc/2021/Conference/Paper3784/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3784/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923834005, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3784/-/Official_Comment"}}}, {"id": "S9XhelPzTBC", "original": null, "number": 3, "cdate": 1605786363843, "ddate": null, "tcdate": 1605786363843, "tmdate": 1605786363843, "tddate": null, "forum": "kB8DkEKSDH", "replyto": "VBIr6lYuNZO", "invitation": "ICLR.cc/2021/Conference/Paper3784/-/Official_Comment", "content": {"title": "Response to AnonReviewer1", "comment": "Thank you for your review.\nFollowing your advice, we made a second experiment as in the BCQ paper. Despite HDCR being lower than methods that train policy function by taking gradient through Q-function, it provides better results than other methods that directly optimize policy function.\nAbout other metrics:\nAsymptotically, derivatives of both MMD and Wasserstein tend to 1. This issue can affect training, especially when learning from offline data. This problem also exists in methods that use KL. In contrast, the derivative of Hellinger distance tends to 0, providing less conservative updates."}, "signatures": ["ICLR.cc/2021/Conference/Paper3784/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3784/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hellinger Distance Constrained Regression", "authorids": ["~Egor_Rotinov1"], "authors": ["Egor Rotinov"], "keywords": ["offline", "Reinforcement Learning", "off-policy", "control"], "abstract": "This paper introduces an off-policy reinforcement learning method that uses Hellinger distance between sampling policy (from what samples were collected) and current policy (policy being optimized) as a constraint. \nHellinger distance squared multiplied by two is greater than or equal to total variation distance squared and less than or equal to Kullback-Leibler divergence, therefore a lower bound for expected discounted return for the new policy is improved compared to the lower bound for training with KL. \nAlso, Hellinger distance is less than or equal to 1, so there is a policy-independent lower bound for expected discounted return. \nHDCR is capable of training with Experience Replay, a common setting for distributed RL when collecting trajectories using different policies and learning from this data centralized. \nHDCR shows results comparable to or better than Advantage-weighted Behavior Model and Advantage-Weighted Regression on MuJoCo tasks using tiny offline datasets collected by random agents. On bigger datasets (100k timesteps) obtained by pretrained behavioral policy, HDCR outperforms ABM and AWR methods on 3 out of 4 tasks. ", "one-sentence_summary": "This paper presents an offline reinforcement learning method based on the Hellinger distance constraint.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "rotinov|hellinger_distance_constrained_regression", "pdf": "/pdf/b2424686d8d6aba3981f12c8c0777b99db8fc501.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=QmXbHQwKa7", "_bibtex": "@misc{\nrotinov2021hellinger,\ntitle={Hellinger Distance Constrained Regression},\nauthor={Egor Rotinov},\nyear={2021},\nurl={https://openreview.net/forum?id=kB8DkEKSDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "kB8DkEKSDH", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3784/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3784/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3784/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3784/Authors|ICLR.cc/2021/Conference/Paper3784/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3784/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923834005, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3784/-/Official_Comment"}}}, {"id": "wBuE66h834h", "original": null, "number": 2, "cdate": 1605785878100, "ddate": null, "tcdate": 1605785878100, "tmdate": 1605785878100, "tddate": null, "forum": "kB8DkEKSDH", "replyto": "kB8DkEKSDH", "invitation": "ICLR.cc/2021/Conference/Paper3784/-/Official_Comment", "content": {"title": "Experiments update", "comment": "We would like to thank reviewers for their detailed reviews. \nReviewers noted insufficient experimental part of the paper. Therefore we reshaped the second experiment to make it more conventional. We trained a behavioral policy (DDPG) on 1M timesteps and then collected a buffer of 100k timesteps as it is in BCQ paper. All four methods used the same buffer (dataset) during the whole training. \nAlso, we publish individual responses below."}, "signatures": ["ICLR.cc/2021/Conference/Paper3784/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3784/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hellinger Distance Constrained Regression", "authorids": ["~Egor_Rotinov1"], "authors": ["Egor Rotinov"], "keywords": ["offline", "Reinforcement Learning", "off-policy", "control"], "abstract": "This paper introduces an off-policy reinforcement learning method that uses Hellinger distance between sampling policy (from what samples were collected) and current policy (policy being optimized) as a constraint. \nHellinger distance squared multiplied by two is greater than or equal to total variation distance squared and less than or equal to Kullback-Leibler divergence, therefore a lower bound for expected discounted return for the new policy is improved compared to the lower bound for training with KL. \nAlso, Hellinger distance is less than or equal to 1, so there is a policy-independent lower bound for expected discounted return. \nHDCR is capable of training with Experience Replay, a common setting for distributed RL when collecting trajectories using different policies and learning from this data centralized. \nHDCR shows results comparable to or better than Advantage-weighted Behavior Model and Advantage-Weighted Regression on MuJoCo tasks using tiny offline datasets collected by random agents. On bigger datasets (100k timesteps) obtained by pretrained behavioral policy, HDCR outperforms ABM and AWR methods on 3 out of 4 tasks. ", "one-sentence_summary": "This paper presents an offline reinforcement learning method based on the Hellinger distance constraint.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "rotinov|hellinger_distance_constrained_regression", "pdf": "/pdf/b2424686d8d6aba3981f12c8c0777b99db8fc501.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=QmXbHQwKa7", "_bibtex": "@misc{\nrotinov2021hellinger,\ntitle={Hellinger Distance Constrained Regression},\nauthor={Egor Rotinov},\nyear={2021},\nurl={https://openreview.net/forum?id=kB8DkEKSDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "kB8DkEKSDH", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3784/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3784/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3784/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3784/Authors|ICLR.cc/2021/Conference/Paper3784/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3784/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923834005, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3784/-/Official_Comment"}}}, {"id": "30wqdURjFu7", "original": null, "number": 1, "cdate": 1603778215350, "ddate": null, "tcdate": 1603778215350, "tmdate": 1605023940948, "tddate": null, "forum": "kB8DkEKSDH", "replyto": "kB8DkEKSDH", "invitation": "ICLR.cc/2021/Conference/Paper3784/-/Official_Review", "content": {"title": "Hellinger Distance Constrained Regression", "review": "Summary: \n \nThis paper proposes a supervised learning for off-policy reinforcement learning. It exploits the Hellinger distance instead of KL divergence. Thus it achieves tighter lower bound of the expected culmulative return than that using the KL divergence. Moreover, the new lower bound is policy independent. The experimental results show that the proposed method slightly outperforms other baselines when only small amount of data are given, while the algorithms fail to learn on several environments.\n \n\nReasons for score:\n \nThough it has some advantages, I vote to reject this paper. This is because, it has low novelty, the experiments are wrongly designed, and thus it is hard to believe the results. The specific details are below.\n \n\nPros\n \n+ Hellinger divergence is used instead of KL divergence, and thus the lower bound become tighter than that using KL divergence.\n \n+ The loss function for policy can be derived by theory\n \n\nCons\n \n- Changing KL distance to Hellinger divergence has low novelty.  Also, the derivation of the loss function using Hellinger distance isn't difficult.  Hellinger distance and KL divergence are all under the class of Amari alpha-divergence. When alpha = +/- 1, Amari alpha-divergence becomes KL and when alpha=0, Amari alpha-divergence becomes the Hellinger distance = integral [sqrt(p) - sqrt(q)]^2 dx.   Indeed, HD is symmetric and satisfies the axioms of distance. Basically, when we consider the HD on the space of probability distribution, we consider Euclidean geometry on the space of probability distribution, whereas the KLD induces the Boltzman interpretation, i.e., p ~ exp( -KLD).\n\n- In addition to the issue of significance in novelty,  the numerical results show that the performance improvement is insignificant or negligible. .\n \n- The experiments used data sampled by random policies or first few samples of on-policy data, but I think that this is a little strange training setting. Most of the previous works in this line use samples at a certain performance (NOT DRAWN BY RANDOM POLICY). For example, in ABM paper[1], it used first 10,000 episodes (if the length of an episode is 1,000, it uses first 10 million samples), or first 2,000 episodes (first 2 million samples) to show its performance when it uses high performed samples, or low performed samples, respectively. These contain good performed samples relative to the random samples. However, experiments in this paper use almost random samples to train policies. We cannot expect a good policy at a certain performance using these random samples. This expectation is also shown in the results. Some learning curves go down as learning proceeds, and this  means that the learning fails on these environments. If the proposed method learns successfully while the others fail to learn, it is a meaningful result, but it is not, otherwise. I think that the authors should evaluate performance using better samples to prove that the proposed method outperforms others.\n \n\nReference\n \n[1] Noah Siegel, et al. Keep doing what worked: Behavior modelling priors for offline reinforcement learning. In International Conference on Learning Representations, 2020.\n \n", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper3784/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3784/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hellinger Distance Constrained Regression", "authorids": ["~Egor_Rotinov1"], "authors": ["Egor Rotinov"], "keywords": ["offline", "Reinforcement Learning", "off-policy", "control"], "abstract": "This paper introduces an off-policy reinforcement learning method that uses Hellinger distance between sampling policy (from what samples were collected) and current policy (policy being optimized) as a constraint. \nHellinger distance squared multiplied by two is greater than or equal to total variation distance squared and less than or equal to Kullback-Leibler divergence, therefore a lower bound for expected discounted return for the new policy is improved compared to the lower bound for training with KL. \nAlso, Hellinger distance is less than or equal to 1, so there is a policy-independent lower bound for expected discounted return. \nHDCR is capable of training with Experience Replay, a common setting for distributed RL when collecting trajectories using different policies and learning from this data centralized. \nHDCR shows results comparable to or better than Advantage-weighted Behavior Model and Advantage-Weighted Regression on MuJoCo tasks using tiny offline datasets collected by random agents. On bigger datasets (100k timesteps) obtained by pretrained behavioral policy, HDCR outperforms ABM and AWR methods on 3 out of 4 tasks. ", "one-sentence_summary": "This paper presents an offline reinforcement learning method based on the Hellinger distance constraint.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "rotinov|hellinger_distance_constrained_regression", "pdf": "/pdf/b2424686d8d6aba3981f12c8c0777b99db8fc501.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=QmXbHQwKa7", "_bibtex": "@misc{\nrotinov2021hellinger,\ntitle={Hellinger Distance Constrained Regression},\nauthor={Egor Rotinov},\nyear={2021},\nurl={https://openreview.net/forum?id=kB8DkEKSDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "kB8DkEKSDH", "replyto": "kB8DkEKSDH", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3784/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538070020, "tmdate": 1606915793626, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3784/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3784/-/Official_Review"}}}, {"id": "VBIr6lYuNZO", "original": null, "number": 2, "cdate": 1603841764411, "ddate": null, "tcdate": 1603841764411, "tmdate": 1605023940878, "tddate": null, "forum": "kB8DkEKSDH", "replyto": "kB8DkEKSDH", "invitation": "ICLR.cc/2021/Conference/Paper3784/-/Official_Review", "content": {"title": "Insufficient contribution and experimental validation", "review": "The authors propose the use of the Hellinger distance instead of KL divergence to constrain new policies to remain close to the behavior policy. The technical aspects are straightforward, noting that Hellinger provides tighter bounds on total variation than KL, and can straightforwardly be plugged into the CPI/TRPO bounds for policy improvement. They also propose an offline reinforcement learning algorithm based on enforcing a Hellinger constraint to the data policy, deriving iterative optimization procedure, and evaluate it on offline\n\nI find the experimental evaluation highly lacking. It seems with the datasets and envs evaluated, policy performance actually *drops* as policy optimization is conducted, so it is not clear to me that these evaluations actually provide meaningful information towards which methods perform better in scenarious where we would want to use offline RL. I would like to see much more extensive evaluation of this method compared to other offline RL algorithms like BCQ https://arxiv.org/abs/1812.02900, BRAC https://arxiv.org/abs/1911.11361, or CQL https://arxiv.org/abs/2006.04779, over a much wider variety of datasets. \n\nIn general, I'm not convinced that simply using the Hellinger distance instead of KL will lead to significant improvements on its own, given that in the BRAC paper, the authors experimented with different trust regions including Wasserstein, MMD, and KL and didn't find huge differences in the tested domains. Overall, the contribution does not seem significant enough to warrant publication without strong experimental results, which this paper lacks.", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3784/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3784/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hellinger Distance Constrained Regression", "authorids": ["~Egor_Rotinov1"], "authors": ["Egor Rotinov"], "keywords": ["offline", "Reinforcement Learning", "off-policy", "control"], "abstract": "This paper introduces an off-policy reinforcement learning method that uses Hellinger distance between sampling policy (from what samples were collected) and current policy (policy being optimized) as a constraint. \nHellinger distance squared multiplied by two is greater than or equal to total variation distance squared and less than or equal to Kullback-Leibler divergence, therefore a lower bound for expected discounted return for the new policy is improved compared to the lower bound for training with KL. \nAlso, Hellinger distance is less than or equal to 1, so there is a policy-independent lower bound for expected discounted return. \nHDCR is capable of training with Experience Replay, a common setting for distributed RL when collecting trajectories using different policies and learning from this data centralized. \nHDCR shows results comparable to or better than Advantage-weighted Behavior Model and Advantage-Weighted Regression on MuJoCo tasks using tiny offline datasets collected by random agents. On bigger datasets (100k timesteps) obtained by pretrained behavioral policy, HDCR outperforms ABM and AWR methods on 3 out of 4 tasks. ", "one-sentence_summary": "This paper presents an offline reinforcement learning method based on the Hellinger distance constraint.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "rotinov|hellinger_distance_constrained_regression", "pdf": "/pdf/b2424686d8d6aba3981f12c8c0777b99db8fc501.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=QmXbHQwKa7", "_bibtex": "@misc{\nrotinov2021hellinger,\ntitle={Hellinger Distance Constrained Regression},\nauthor={Egor Rotinov},\nyear={2021},\nurl={https://openreview.net/forum?id=kB8DkEKSDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "kB8DkEKSDH", "replyto": "kB8DkEKSDH", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3784/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538070020, "tmdate": 1606915793626, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3784/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3784/-/Official_Review"}}}, {"id": "j89VeJR7xh9", "original": null, "number": 3, "cdate": 1603900232949, "ddate": null, "tcdate": 1603900232949, "tmdate": 1605023940807, "tddate": null, "forum": "kB8DkEKSDH", "replyto": "kB8DkEKSDH", "invitation": "ICLR.cc/2021/Conference/Paper3784/-/Official_Review", "content": {"title": "Idea is not novel enough; results are not significantly better than baselines", "review": "##########################################################################\n\nSummary:\n\n \nThe paper provides a new metric - Hellinger distance to be combined with trust region ideas in policy optimization. The major difference from prior work is the change of this distance metric. The paper shows that with this distance metric, along with Lagrangian relaxation, one could show analytic results of improved policies. The paper also shows similar lower bound improvement results and compared with baselines on offline rl tasks.\n\n##########################################################################\n\nReasons for score: \n\n \nOverall, I vote for rejection. I think the idea of changing the distance metric is not novel enough. Critically, I do not think so far in the paper there is a strong enough motivation to use this distance metric: both innovation-wise and result-wise. I will explain in details below.\n \n##########################################################################Pros: \n\n \n1. Idea is not novel: the overall idea of using an alternative metric does not seem novel. Though the authors motivated an 'improved' version of the trust region lower bound, by using the fact that the Hellinger distance is upper bounded by KL - I think such an improvement in the lower bound is a bit trivial and does not provide new perspectives on the old results.\n \n2. This new lower bound also might not provide additional benefits in practice - because in practice such lower bounds are generally too conservative.\n\n3. Experiment results are also not strong enough. I will explain below.\n \n##########################################################################\n\nCons: \n\n \n1. The final performance of all three baseline algorithms are fairly bad in terms of final rewards (e.g. for halfcheetah, all returns are negative, yet we know that online algorithms could achieve >3000 at least and in some cases >6000). I wonder if this general inferior performance is a result of using offline dataset - in that sense, does the agent learn anything meaningful at all?\n\n2. From both fig 1 and fig 2, about for half of the tasks the performance seem to drop (or stay at the same level) as the case where no training is done (x-axis at the origin). Does this also corroborate my previous concern that these agents do not learn much at all?\n \n3. From the curves presented in Fig1,2, as well as mean+std results in Table 1,2, it does not seem that the new method provides much significant gains either.\n \n##########################################################################\n\nQuestions during rebuttal period: \n\nPlease address and clarify the cons above. Thanks.\n\n \n#########################################################################", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3784/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3784/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hellinger Distance Constrained Regression", "authorids": ["~Egor_Rotinov1"], "authors": ["Egor Rotinov"], "keywords": ["offline", "Reinforcement Learning", "off-policy", "control"], "abstract": "This paper introduces an off-policy reinforcement learning method that uses Hellinger distance between sampling policy (from what samples were collected) and current policy (policy being optimized) as a constraint. \nHellinger distance squared multiplied by two is greater than or equal to total variation distance squared and less than or equal to Kullback-Leibler divergence, therefore a lower bound for expected discounted return for the new policy is improved compared to the lower bound for training with KL. \nAlso, Hellinger distance is less than or equal to 1, so there is a policy-independent lower bound for expected discounted return. \nHDCR is capable of training with Experience Replay, a common setting for distributed RL when collecting trajectories using different policies and learning from this data centralized. \nHDCR shows results comparable to or better than Advantage-weighted Behavior Model and Advantage-Weighted Regression on MuJoCo tasks using tiny offline datasets collected by random agents. On bigger datasets (100k timesteps) obtained by pretrained behavioral policy, HDCR outperforms ABM and AWR methods on 3 out of 4 tasks. ", "one-sentence_summary": "This paper presents an offline reinforcement learning method based on the Hellinger distance constraint.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "rotinov|hellinger_distance_constrained_regression", "pdf": "/pdf/b2424686d8d6aba3981f12c8c0777b99db8fc501.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=QmXbHQwKa7", "_bibtex": "@misc{\nrotinov2021hellinger,\ntitle={Hellinger Distance Constrained Regression},\nauthor={Egor Rotinov},\nyear={2021},\nurl={https://openreview.net/forum?id=kB8DkEKSDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "kB8DkEKSDH", "replyto": "kB8DkEKSDH", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3784/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538070020, "tmdate": 1606915793626, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3784/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3784/-/Official_Review"}}}, {"id": "hV0TNUsII3F", "original": null, "number": 4, "cdate": 1603939574131, "ddate": null, "tcdate": 1603939574131, "tmdate": 1605023940738, "tddate": null, "forum": "kB8DkEKSDH", "replyto": "kB8DkEKSDH", "invitation": "ICLR.cc/2021/Conference/Paper3784/-/Official_Review", "content": {"title": "Short and interesting paper, needs minor clarifications", "review": "This paper proposes an algorithm for off-policy reinforcement learning using the Hellinger distance between the sampling policy and optimized policy as a constraint. The motivation for the proposed method is explained in the preliminaries section. The actual algorithm and experiments run using the proposed algorithm are also provided. \n\nThe derivation is easy to follow, and this is because of the well-known lower and upper bounds on the Hellinger distance. \n\nThe writing of the paper needs work. For example, the abstract talks about the sampling policy and current policy. By current policy, what the authors mean is the policy that is being optimized. The sampling policy is the policy that was run offline. Clarifying these terms would help. Similarly, I did not follow \"return for the new policy is improved comparing to KL\". \n\nIn paragraph 3: \"With the use of Lagrangian, have been derived\" needs proofreading. In eqn 13, what is beta?\n\nIn the figures, what are the axes?", "rating": "5: Marginally below acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2021/Conference/Paper3784/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3784/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hellinger Distance Constrained Regression", "authorids": ["~Egor_Rotinov1"], "authors": ["Egor Rotinov"], "keywords": ["offline", "Reinforcement Learning", "off-policy", "control"], "abstract": "This paper introduces an off-policy reinforcement learning method that uses Hellinger distance between sampling policy (from what samples were collected) and current policy (policy being optimized) as a constraint. \nHellinger distance squared multiplied by two is greater than or equal to total variation distance squared and less than or equal to Kullback-Leibler divergence, therefore a lower bound for expected discounted return for the new policy is improved compared to the lower bound for training with KL. \nAlso, Hellinger distance is less than or equal to 1, so there is a policy-independent lower bound for expected discounted return. \nHDCR is capable of training with Experience Replay, a common setting for distributed RL when collecting trajectories using different policies and learning from this data centralized. \nHDCR shows results comparable to or better than Advantage-weighted Behavior Model and Advantage-Weighted Regression on MuJoCo tasks using tiny offline datasets collected by random agents. On bigger datasets (100k timesteps) obtained by pretrained behavioral policy, HDCR outperforms ABM and AWR methods on 3 out of 4 tasks. ", "one-sentence_summary": "This paper presents an offline reinforcement learning method based on the Hellinger distance constraint.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "rotinov|hellinger_distance_constrained_regression", "pdf": "/pdf/b2424686d8d6aba3981f12c8c0777b99db8fc501.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=QmXbHQwKa7", "_bibtex": "@misc{\nrotinov2021hellinger,\ntitle={Hellinger Distance Constrained Regression},\nauthor={Egor Rotinov},\nyear={2021},\nurl={https://openreview.net/forum?id=kB8DkEKSDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "kB8DkEKSDH", "replyto": "kB8DkEKSDH", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3784/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538070020, "tmdate": 1606915793626, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3784/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3784/-/Official_Review"}}}], "count": 11}