{"notes": [{"id": "BJxG_0EtDS", "original": "BklEiowuwH", "number": 1204, "cdate": 1569439338467, "ddate": null, "tcdate": 1569439338467, "tmdate": 1583912027589, "tddate": null, "forum": "BJxG_0EtDS", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Prediction, Consistency, Curvature: Representation Learning for Locally-Linear Control", "authors": ["Nir Levine", "Yinlam Chow", "Rui Shu", "Ang Li", "Mohammad Ghavamzadeh", "Hung Bui"], "authorids": ["nirlevine@google.com", "yinlamchow@google.com", "ruishu@stanford.edu", "anglili@google.com", "mgh@fb.com", "v.hungbh1@vinai.io"], "keywords": ["Embed-to-Control", "Representation Learning", "Stochastic Optimal Control", "VAE", "iLQR"], "TL;DR": "Learning embedding for control with high-dimensional observations", "abstract": "Many real-world sequential decision-making problems can be formulated as optimal control with high-dimensional observations and unknown dynamics. A promising approach is to embed the high-dimensional observations into a lower-dimensional latent representation space, estimate the latent dynamics model, then utilize this model for control in the latent space. An important open question is how to learn a representation that is amenable to existing control algorithms? In this paper, we focus on learning representations for locally-linear control algorithms, such as iterative LQR (iLQR). By formulating and analyzing the representation learning problem from an optimal control perspective, we establish three underlying principles that the learned representation should comprise: 1) accurate prediction in the observation space, 2) consistency between latent and observation space dynamics, and 3) low curvature in the latent space transitions. These principles naturally correspond to a loss function that consists of three terms: prediction, consistency, and curvature (PCC). Crucially, to make PCC tractable, we derive an amortized variational bound for the PCC loss function. Extensive experiments on benchmark domains demonstrate that the new variational-PCC learning algorithm benefits from significantly more stable and reproducible training, and leads to superior control performance.  Further ablation studies give support to the importance of all three PCC components for learning a good latent space for control.", "pdf": "/pdf/45fa6ffcf44b62ef51bf27ff0cd00e506b9d116d.pdf", "paperhash": "levine|prediction_consistency_curvature_representation_learning_for_locallylinear_control", "_bibtex": "@inproceedings{\nLevine2020Prediction,,\ntitle={Prediction, Consistency, Curvature: Representation Learning for Locally-Linear Control},\nauthor={Nir Levine and Yinlam Chow and Rui Shu and Ang Li and Mohammad Ghavamzadeh and Hung Bui},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BJxG_0EtDS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/16be8d9f3e4ad898a15eb90334cc700a3027ed7c.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "-LmwgMEna", "original": null, "number": 1, "cdate": 1576798717325, "ddate": null, "tcdate": 1576798717325, "tmdate": 1576800919218, "tddate": null, "forum": "BJxG_0EtDS", "replyto": "BJxG_0EtDS", "invitation": "ICLR.cc/2020/Conference/Paper1204/-/Decision", "content": {"decision": "Accept (Poster)", "comment": "This paper studies optimal control with low-dimensional representation.  The paper presents interesting progress, although I urge the authors to address all issues raised by reviewers in their revisions.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Prediction, Consistency, Curvature: Representation Learning for Locally-Linear Control", "authors": ["Nir Levine", "Yinlam Chow", "Rui Shu", "Ang Li", "Mohammad Ghavamzadeh", "Hung Bui"], "authorids": ["nirlevine@google.com", "yinlamchow@google.com", "ruishu@stanford.edu", "anglili@google.com", "mgh@fb.com", "v.hungbh1@vinai.io"], "keywords": ["Embed-to-Control", "Representation Learning", "Stochastic Optimal Control", "VAE", "iLQR"], "TL;DR": "Learning embedding for control with high-dimensional observations", "abstract": "Many real-world sequential decision-making problems can be formulated as optimal control with high-dimensional observations and unknown dynamics. A promising approach is to embed the high-dimensional observations into a lower-dimensional latent representation space, estimate the latent dynamics model, then utilize this model for control in the latent space. An important open question is how to learn a representation that is amenable to existing control algorithms? In this paper, we focus on learning representations for locally-linear control algorithms, such as iterative LQR (iLQR). By formulating and analyzing the representation learning problem from an optimal control perspective, we establish three underlying principles that the learned representation should comprise: 1) accurate prediction in the observation space, 2) consistency between latent and observation space dynamics, and 3) low curvature in the latent space transitions. These principles naturally correspond to a loss function that consists of three terms: prediction, consistency, and curvature (PCC). Crucially, to make PCC tractable, we derive an amortized variational bound for the PCC loss function. Extensive experiments on benchmark domains demonstrate that the new variational-PCC learning algorithm benefits from significantly more stable and reproducible training, and leads to superior control performance.  Further ablation studies give support to the importance of all three PCC components for learning a good latent space for control.", "pdf": "/pdf/45fa6ffcf44b62ef51bf27ff0cd00e506b9d116d.pdf", "paperhash": "levine|prediction_consistency_curvature_representation_learning_for_locallylinear_control", "_bibtex": "@inproceedings{\nLevine2020Prediction,,\ntitle={Prediction, Consistency, Curvature: Representation Learning for Locally-Linear Control},\nauthor={Nir Levine and Yinlam Chow and Rui Shu and Ang Li and Mohammad Ghavamzadeh and Hung Bui},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BJxG_0EtDS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/16be8d9f3e4ad898a15eb90334cc700a3027ed7c.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "BJxG_0EtDS", "replyto": "BJxG_0EtDS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795727465, "tmdate": 1576800279709, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1204/-/Decision"}}}, {"id": "r1xykC03qS", "original": null, "number": 3, "cdate": 1572822487430, "ddate": null, "tcdate": 1572822487430, "tmdate": 1575036816987, "tddate": null, "forum": "BJxG_0EtDS", "replyto": "BJxG_0EtDS", "invitation": "ICLR.cc/2020/Conference/Paper1204/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "title": "Official Blind Review #4", "review": "This paper considers learning low-dimensional representations from high-dimensional observations for control purposes. The authors extend the E2C framework by introducing the new PCC-Loss function. This new loss function aims to reflect the prediction in the observation space, the consistency between latent and observation dynamics, and the low curvature in the latent dynamics. The low curvature term is used to bias the latent dynamics towards models that can be better approximated as locally linear models. The authors provide theory (error bounds) to justify their proposed PCC-Loss function. Then variational PCC is developed to make the algorithm tractable. The proposed method is evaluated in 5 different simulated tasks and compared with the original E2C method and the RCE method. \nThe paper is well-written. \n\nPros:\n- The idea in this paper is quite original. The three principles used to formulate the loss function provide some new insights.\n\n- The authors have proposed a theory to justify the use of their loss function. The technical quality of this part seems solid.\n\n- Simulations have been used to show that the proposed PCC method outperforms E2C and RCE.\n\n-The paper is well written. \n\nCons:\n- The tasks in this paper are not that complicated. It is unclear whether the proposed method outperforms other model-based RL methods such as Solar and DSAE for practical robotic applications. More comparisons are needed.\n\n- It is also not that clear why one wants to SOC3 to be close to SOC1 in the first place. It seems the true optimization problem should be posed on the space of the original state s. SOC1 is just a surrogate problem for the original problem.\n\n- There seems to be a gap between the proposed theory  and the algorithm implementation. This makes the theory part less useful.\n\nOverall, I think the idea in this paper is interesting. The authors have made a serious effort in coming up principles for model-based RL control. But at this moment it is not that convincing the proposed method will be the best model-based RL method for practical robotic applications. If the authors can address my comments, I will be willing to increase my score.\n\n\n\nMinor Comments:\n\n- It seems that for the task the authors have tested their method, it is not that difficult to directly estimate the state. Am I correct here? Can the authors make a comment on this? How to compare their approach and a more direct control approach using estimation of state s? \n\n- I have never seen the curvature principle in any control papers. Any control reference on why this is a good principle? It seems that the linearization works well when the control inputs are around the reference points. Does the curvature really matter that much for ILQR to work? \n\n- How to justify the Markovian assumption on x? Just by observation or there is a more principle way to test this assumption on the buffered images?\n\n====================================================\nPost-Rebuttal:\nAfter reading the authors' response, I am changing my score to weak accept. Lemma 4 is nice. I have not seen anything similar to this in the controls literature before. The authors have addressed most of my concerns. I still have a few comments for preparing the final version of this paper. \n1. I still don't see why SOC1 is the \"original problem.\" Yes, it is assumed that the true state cannot be directly observed. But if the observations are Markov eventually, then some estimated version of the states can be obtained, right? I think treating SOC1 as the original problem is one possible way of doing things and clearly the authors have built a principled framework for doing things in this way. But treating SOC1 as the original problem seems not the only way of doing things. I hope the authors can clarify this and do not oversell the proposed approach. \n2. I think it is still worth comparing SOLAR and PCC empirically. This will help the readers to choose algorithms when they need. \n3. The comment on the verification of the Markov assumption is hand-waving.  The authors said \"A simple test would be to see if a control algorithm with the Markovian assumption works well with our representation or not.\" Does this mean that the users will not be able to verify this assumption before using the proposed approach to obtain controllers? It will be helpful if the authors can explain this step for one specific example in details.\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper1204/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1204/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Prediction, Consistency, Curvature: Representation Learning for Locally-Linear Control", "authors": ["Nir Levine", "Yinlam Chow", "Rui Shu", "Ang Li", "Mohammad Ghavamzadeh", "Hung Bui"], "authorids": ["nirlevine@google.com", "yinlamchow@google.com", "ruishu@stanford.edu", "anglili@google.com", "mgh@fb.com", "v.hungbh1@vinai.io"], "keywords": ["Embed-to-Control", "Representation Learning", "Stochastic Optimal Control", "VAE", "iLQR"], "TL;DR": "Learning embedding for control with high-dimensional observations", "abstract": "Many real-world sequential decision-making problems can be formulated as optimal control with high-dimensional observations and unknown dynamics. A promising approach is to embed the high-dimensional observations into a lower-dimensional latent representation space, estimate the latent dynamics model, then utilize this model for control in the latent space. An important open question is how to learn a representation that is amenable to existing control algorithms? In this paper, we focus on learning representations for locally-linear control algorithms, such as iterative LQR (iLQR). By formulating and analyzing the representation learning problem from an optimal control perspective, we establish three underlying principles that the learned representation should comprise: 1) accurate prediction in the observation space, 2) consistency between latent and observation space dynamics, and 3) low curvature in the latent space transitions. These principles naturally correspond to a loss function that consists of three terms: prediction, consistency, and curvature (PCC). Crucially, to make PCC tractable, we derive an amortized variational bound for the PCC loss function. Extensive experiments on benchmark domains demonstrate that the new variational-PCC learning algorithm benefits from significantly more stable and reproducible training, and leads to superior control performance.  Further ablation studies give support to the importance of all three PCC components for learning a good latent space for control.", "pdf": "/pdf/45fa6ffcf44b62ef51bf27ff0cd00e506b9d116d.pdf", "paperhash": "levine|prediction_consistency_curvature_representation_learning_for_locallylinear_control", "_bibtex": "@inproceedings{\nLevine2020Prediction,,\ntitle={Prediction, Consistency, Curvature: Representation Learning for Locally-Linear Control},\nauthor={Nir Levine and Yinlam Chow and Rui Shu and Ang Li and Mohammad Ghavamzadeh and Hung Bui},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BJxG_0EtDS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/16be8d9f3e4ad898a15eb90334cc700a3027ed7c.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BJxG_0EtDS", "replyto": "BJxG_0EtDS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1204/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1204/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575119096542, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1204/Reviewers"], "noninvitees": [], "tcdate": 1570237740800, "tmdate": 1575119096558, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1204/-/Official_Review"}}}, {"id": "BkxlDTiMor", "original": null, "number": 4, "cdate": 1573203288475, "ddate": null, "tcdate": 1573203288475, "tmdate": 1573203440478, "tddate": null, "forum": "BJxG_0EtDS", "replyto": "r1xykC03qS", "invitation": "ICLR.cc/2020/Conference/Paper1204/-/Official_Comment", "content": {"title": "Thank you. Please find our response to your questions/comments below", "comment": "We thank the reviewer for the detailed and useful comments.\n\n\u201cit is not that convincing the proposed method will be the best model-based RL method for practical robotic applications\u201d\n\nThe goal of the paper is not to come up with the best model-based RL algorithm for practical robotics applications. We aim to provide a principled framework to model the control-in-latent-space problem, and propose a concrete instantiation of the framework for learning controllable representations in particular for locally linear control (LLC) algorithms. Our method is potentially suitable for any control problem (not necessarily robotics) in which the true state space is not observed and the observations are high-dimensional. We emphasize that this problem setting of embedding high-dimensional decision processes into lower dimensional representation spaces in which classical control algorithms can be applied has been of growing interest in the recent literature (e.g. E2C, RCE, SOLAR, PlaNet), and we believe that the novel insight gained from our work (consistency, low-curvature) makes an important contribution in this space by improving significantly the robustness of existing representation for LLC approaches (E2C, RCE). \n\nPCC belongs to the family of E2C and RCE algorithms, in which learning takes place in two phases: learning the latent space and dynamics, followed by control. This is different than SOLAR in which learning the representation and control are done together in an online fashion. This is why we left comparison with SOLAR as a future work. It is important to note that for more complex problems, PCC should be implemented more interactive, by repeating its two phases several times.\n\n\u201cwhy one wants to SOC3 to be close to SOC1\u201d\nThe main reason for introducing the relationship between SOC3 and SOC1 is to motivate the prediction term in the PCC model. \n\n\u201cSOC1 is just a surrogate problem for the original problem\u201d\nIt is true that the original problem should be posed in the true state space S. However, since we assume from the very beginning that the true state of the system is not observable, SOC1 is in fact the original problem. \n\n\u201cGap between theory and algorithm implementation\u201d\nThe theory of the paper is mainly to motivate and support the three-part loss function used by the PCC algorithm, and not as a recipe for implementing it. Variational PCC is one possible implementation of the three-part loss. There could be many others.\n\n\u201cdirectly estimating the underlying state\u201d\nEstimating the underlying states from the observations and then conducting control in the space of the estimated states is definitely a possible approach to the problems studied in our paper. This approach requires pre-processing (state estimation) and may work well in some problems and fail in others. In this paper, we take a different approach, and instead of estimating the underlying state space, we learn a latent space suitable for the class of locally linear control (LLC) algorithms, learn the model, and conduct the control there.   \n\n\u201ccurvature term and its relationship with control theory (iLQR)\u201d\nUnlike classical control problems, in which the system dynamics is pre-defined and cannot be changed, in the embed-to-control setting, we have the luxury to enforce the latent dynamics to be suitable for our choice of control algorithm. Our main message here is that the choice of the control algorithm should play a role in learning the representation/latent space. Since the control algorithm of our choice is LLC, we believe introducing the curvature penalty term could greatly simplifies the complexity of the downstream control task. The reason behind this (intuitively) is that when the curvature is low, the size of the neighbourhood in which the local linearity assumption holds is large, and thus, LLC algorithms work better. A more detailed discussion of the relationship between curvature and the performance of LLC algorithms can be found in Appendix A.5, in particular, see Lemma 4. \n\n\u201cMarkovian observation assumption and its justification\u201d\nSince most control algorithms work under the Markovian assumption, it is common to put the observations together in a way to be Markov or close to Markov in the observation space (the Atari domains are good examples). This is definite a pre-processing step, and it is straightforward in some problems and difficult in others. There are ways to test whether our representation is (approximately) Markov or not. Having knowledge about the physics of the system is definitely helpful. A simple test would be to see if a control algorithm with the Markovian assumption works well with our representation or not. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1204/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1204/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Prediction, Consistency, Curvature: Representation Learning for Locally-Linear Control", "authors": ["Nir Levine", "Yinlam Chow", "Rui Shu", "Ang Li", "Mohammad Ghavamzadeh", "Hung Bui"], "authorids": ["nirlevine@google.com", "yinlamchow@google.com", "ruishu@stanford.edu", "anglili@google.com", "mgh@fb.com", "v.hungbh1@vinai.io"], "keywords": ["Embed-to-Control", "Representation Learning", "Stochastic Optimal Control", "VAE", "iLQR"], "TL;DR": "Learning embedding for control with high-dimensional observations", "abstract": "Many real-world sequential decision-making problems can be formulated as optimal control with high-dimensional observations and unknown dynamics. A promising approach is to embed the high-dimensional observations into a lower-dimensional latent representation space, estimate the latent dynamics model, then utilize this model for control in the latent space. An important open question is how to learn a representation that is amenable to existing control algorithms? In this paper, we focus on learning representations for locally-linear control algorithms, such as iterative LQR (iLQR). By formulating and analyzing the representation learning problem from an optimal control perspective, we establish three underlying principles that the learned representation should comprise: 1) accurate prediction in the observation space, 2) consistency between latent and observation space dynamics, and 3) low curvature in the latent space transitions. These principles naturally correspond to a loss function that consists of three terms: prediction, consistency, and curvature (PCC). Crucially, to make PCC tractable, we derive an amortized variational bound for the PCC loss function. Extensive experiments on benchmark domains demonstrate that the new variational-PCC learning algorithm benefits from significantly more stable and reproducible training, and leads to superior control performance.  Further ablation studies give support to the importance of all three PCC components for learning a good latent space for control.", "pdf": "/pdf/45fa6ffcf44b62ef51bf27ff0cd00e506b9d116d.pdf", "paperhash": "levine|prediction_consistency_curvature_representation_learning_for_locallylinear_control", "_bibtex": "@inproceedings{\nLevine2020Prediction,,\ntitle={Prediction, Consistency, Curvature: Representation Learning for Locally-Linear Control},\nauthor={Nir Levine and Yinlam Chow and Rui Shu and Ang Li and Mohammad Ghavamzadeh and Hung Bui},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BJxG_0EtDS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/16be8d9f3e4ad898a15eb90334cc700a3027ed7c.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJxG_0EtDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1204/Authors", "ICLR.cc/2020/Conference/Paper1204/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1204/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1204/Reviewers", "ICLR.cc/2020/Conference/Paper1204/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1204/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1204/Authors|ICLR.cc/2020/Conference/Paper1204/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504159624, "tmdate": 1576860539676, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1204/Authors", "ICLR.cc/2020/Conference/Paper1204/Reviewers", "ICLR.cc/2020/Conference/Paper1204/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1204/-/Official_Comment"}}}, {"id": "Bkg_J0ofor", "original": null, "number": 5, "cdate": 1573203423980, "ddate": null, "tcdate": 1573203423980, "tmdate": 1573203423980, "tddate": null, "forum": "BJxG_0EtDS", "replyto": "HkxhB8v8KS", "invitation": "ICLR.cc/2020/Conference/Paper1204/-/Official_Comment", "content": {"title": "Thank you. Please find our response to your questions/comments below.", "comment": "We thank the reviewer for useful comments.\n\n1) HMM is definitely an option for learning the latent space. We note that the concepts we highlight in this paper (prediction, consistency, curvature) remain relevant even if the underlying dynamics model of choice is an HMM. However, since our paper makes the simplifying assumption that the observation space is Markovian, and since training an HMM is generally more challenging than training Markovian dynamics models, we chose not to employ an HMM in our paper. \n\n2) The three terms in PCC are prediction, consistency, and curvature. For prediction, the goal is to enforce that the process of encoding, transitioning via the latent dynamics, and then decoding, adheres to the true observation dynamics. For consistency, the goal is to make sure that the latent dynamics is consistent with the encoded trajectory. Figure 1 clearly shows the relation/difference between the evolution of the system in the latent space and the evolution of the encoded observations. Finally, for curvature, the goal is to learn a latent space that is suitable for LLC algorithms. As stated by the reviewer, the main motivation of the curvature term is to ensure that LLC algorithms, such as iLQR, work well. When the curvature is low, the size of the neighbourhood in which the local linearity assumption holds is large, and thus, LLC algorithms work better. A more detailed discussion of the relationship between curvature and the performance of LLC algorithms can be found in Appendix A.5, in particular, see Lemma 4. \n\n3) From Lemma 3, we see that \\lambda_p and \\lambda_c should be of the same order. In practice, we used this observation and optimized the hyper-parameters \\lambda_p, \\lambda_c, and \\lambda_cur by standard grid-search.\n\n\u201cMinor: (5) may contain some typos\u201d\nYou are right. There is a typo in (5) on the 2-norm. We will fix it in the final version of the paper. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1204/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1204/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Prediction, Consistency, Curvature: Representation Learning for Locally-Linear Control", "authors": ["Nir Levine", "Yinlam Chow", "Rui Shu", "Ang Li", "Mohammad Ghavamzadeh", "Hung Bui"], "authorids": ["nirlevine@google.com", "yinlamchow@google.com", "ruishu@stanford.edu", "anglili@google.com", "mgh@fb.com", "v.hungbh1@vinai.io"], "keywords": ["Embed-to-Control", "Representation Learning", "Stochastic Optimal Control", "VAE", "iLQR"], "TL;DR": "Learning embedding for control with high-dimensional observations", "abstract": "Many real-world sequential decision-making problems can be formulated as optimal control with high-dimensional observations and unknown dynamics. A promising approach is to embed the high-dimensional observations into a lower-dimensional latent representation space, estimate the latent dynamics model, then utilize this model for control in the latent space. An important open question is how to learn a representation that is amenable to existing control algorithms? In this paper, we focus on learning representations for locally-linear control algorithms, such as iterative LQR (iLQR). By formulating and analyzing the representation learning problem from an optimal control perspective, we establish three underlying principles that the learned representation should comprise: 1) accurate prediction in the observation space, 2) consistency between latent and observation space dynamics, and 3) low curvature in the latent space transitions. These principles naturally correspond to a loss function that consists of three terms: prediction, consistency, and curvature (PCC). Crucially, to make PCC tractable, we derive an amortized variational bound for the PCC loss function. Extensive experiments on benchmark domains demonstrate that the new variational-PCC learning algorithm benefits from significantly more stable and reproducible training, and leads to superior control performance.  Further ablation studies give support to the importance of all three PCC components for learning a good latent space for control.", "pdf": "/pdf/45fa6ffcf44b62ef51bf27ff0cd00e506b9d116d.pdf", "paperhash": "levine|prediction_consistency_curvature_representation_learning_for_locallylinear_control", "_bibtex": "@inproceedings{\nLevine2020Prediction,,\ntitle={Prediction, Consistency, Curvature: Representation Learning for Locally-Linear Control},\nauthor={Nir Levine and Yinlam Chow and Rui Shu and Ang Li and Mohammad Ghavamzadeh and Hung Bui},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BJxG_0EtDS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/16be8d9f3e4ad898a15eb90334cc700a3027ed7c.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJxG_0EtDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1204/Authors", "ICLR.cc/2020/Conference/Paper1204/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1204/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1204/Reviewers", "ICLR.cc/2020/Conference/Paper1204/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1204/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1204/Authors|ICLR.cc/2020/Conference/Paper1204/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504159624, "tmdate": 1576860539676, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1204/Authors", "ICLR.cc/2020/Conference/Paper1204/Reviewers", "ICLR.cc/2020/Conference/Paper1204/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1204/-/Official_Comment"}}}, {"id": "BkggJasfoS", "original": null, "number": 3, "cdate": 1573203160186, "ddate": null, "tcdate": 1573203160186, "tmdate": 1573203160186, "tddate": null, "forum": "BJxG_0EtDS", "replyto": "HylFTIdRYB", "invitation": "ICLR.cc/2020/Conference/Paper1204/-/Official_Comment", "content": {"title": "Thank you", "comment": "We thank the reviewer for appreciating our work in terms of novelty, theory, and experiments. \n\nWe will improve the notations and presentation of the mathematical results, especially in Section 4.2, in the final version of the paper. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1204/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1204/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Prediction, Consistency, Curvature: Representation Learning for Locally-Linear Control", "authors": ["Nir Levine", "Yinlam Chow", "Rui Shu", "Ang Li", "Mohammad Ghavamzadeh", "Hung Bui"], "authorids": ["nirlevine@google.com", "yinlamchow@google.com", "ruishu@stanford.edu", "anglili@google.com", "mgh@fb.com", "v.hungbh1@vinai.io"], "keywords": ["Embed-to-Control", "Representation Learning", "Stochastic Optimal Control", "VAE", "iLQR"], "TL;DR": "Learning embedding for control with high-dimensional observations", "abstract": "Many real-world sequential decision-making problems can be formulated as optimal control with high-dimensional observations and unknown dynamics. A promising approach is to embed the high-dimensional observations into a lower-dimensional latent representation space, estimate the latent dynamics model, then utilize this model for control in the latent space. An important open question is how to learn a representation that is amenable to existing control algorithms? In this paper, we focus on learning representations for locally-linear control algorithms, such as iterative LQR (iLQR). By formulating and analyzing the representation learning problem from an optimal control perspective, we establish three underlying principles that the learned representation should comprise: 1) accurate prediction in the observation space, 2) consistency between latent and observation space dynamics, and 3) low curvature in the latent space transitions. These principles naturally correspond to a loss function that consists of three terms: prediction, consistency, and curvature (PCC). Crucially, to make PCC tractable, we derive an amortized variational bound for the PCC loss function. Extensive experiments on benchmark domains demonstrate that the new variational-PCC learning algorithm benefits from significantly more stable and reproducible training, and leads to superior control performance.  Further ablation studies give support to the importance of all three PCC components for learning a good latent space for control.", "pdf": "/pdf/45fa6ffcf44b62ef51bf27ff0cd00e506b9d116d.pdf", "paperhash": "levine|prediction_consistency_curvature_representation_learning_for_locallylinear_control", "_bibtex": "@inproceedings{\nLevine2020Prediction,,\ntitle={Prediction, Consistency, Curvature: Representation Learning for Locally-Linear Control},\nauthor={Nir Levine and Yinlam Chow and Rui Shu and Ang Li and Mohammad Ghavamzadeh and Hung Bui},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BJxG_0EtDS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/16be8d9f3e4ad898a15eb90334cc700a3027ed7c.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJxG_0EtDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1204/Authors", "ICLR.cc/2020/Conference/Paper1204/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1204/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1204/Reviewers", "ICLR.cc/2020/Conference/Paper1204/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1204/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1204/Authors|ICLR.cc/2020/Conference/Paper1204/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504159624, "tmdate": 1576860539676, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1204/Authors", "ICLR.cc/2020/Conference/Paper1204/Reviewers", "ICLR.cc/2020/Conference/Paper1204/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1204/-/Official_Comment"}}}, {"id": "HkxhB8v8KS", "original": null, "number": 1, "cdate": 1571350083793, "ddate": null, "tcdate": 1571350083793, "tmdate": 1572972499350, "tddate": null, "forum": "BJxG_0EtDS", "replyto": "BJxG_0EtDS", "invitation": "ICLR.cc/2020/Conference/Paper1204/-/Official_Review", "content": {"rating": "6: Weak Accept", "experience_assessment": "I do not know much about this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This work proposes a regularization strategy for learning optimal policy for a dynamic control problem in a latent low-dimensional domain. The work is based on LCE approach, but with in-depth analysis on how to choose/design the regularization for the \\hat{P} operator, which consists of an encoder, a decoder, and dynamics in the latent space. In particular, the author argued that three principles (prediction, consistency, and curvature) should be taken into consideration when designing the regularizer of the learning cost function - so that the learned latent domain can serve better for the purpose of optimizing the long-term cost in the ambient domain. \n\nThe paper is well written and pleasant to read. One possible shortcoming is that the notations are a bit dazzling. It is almost impossible to follow the notation when first reading this paper. The proofs are very lengthy and thus the reviewer did not check in detail. \n\nThe reviewer has several question:\n\n1) Of course SOC2 makes sense. But what if one models the whole problem as an HMM, and perform control algorithms in the hidden domain of the HMM (and the hidden states can be of much smaller alphabets compared to the observable states), will there be any fundamental difference? Of course learning an HMM is challenging, but approachable. Any comments?\n\n2) The three design principles make sense, but may need more elaboration. For example, it is a bit unclear why f_Z should be with low curvature -- does it mean that you wish the control problem in the latent domain is more like a linear dynamical system, so that the LLC algorithm works better? The argument is a bit unclear, since \"locally linear\" is not a rigorous term. Any smooth function is ``\"locally linear\". Here, how to measure the difficulty of the latent control problem may need more discussion.\n\nMinor: btw,  (5) may contain some typos.\n\n3) In practice, how to balance the three parameters lambda_p, lambda_c, lambda_cur?\n\n "}, "signatures": ["ICLR.cc/2020/Conference/Paper1204/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1204/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Prediction, Consistency, Curvature: Representation Learning for Locally-Linear Control", "authors": ["Nir Levine", "Yinlam Chow", "Rui Shu", "Ang Li", "Mohammad Ghavamzadeh", "Hung Bui"], "authorids": ["nirlevine@google.com", "yinlamchow@google.com", "ruishu@stanford.edu", "anglili@google.com", "mgh@fb.com", "v.hungbh1@vinai.io"], "keywords": ["Embed-to-Control", "Representation Learning", "Stochastic Optimal Control", "VAE", "iLQR"], "TL;DR": "Learning embedding for control with high-dimensional observations", "abstract": "Many real-world sequential decision-making problems can be formulated as optimal control with high-dimensional observations and unknown dynamics. A promising approach is to embed the high-dimensional observations into a lower-dimensional latent representation space, estimate the latent dynamics model, then utilize this model for control in the latent space. An important open question is how to learn a representation that is amenable to existing control algorithms? In this paper, we focus on learning representations for locally-linear control algorithms, such as iterative LQR (iLQR). By formulating and analyzing the representation learning problem from an optimal control perspective, we establish three underlying principles that the learned representation should comprise: 1) accurate prediction in the observation space, 2) consistency between latent and observation space dynamics, and 3) low curvature in the latent space transitions. These principles naturally correspond to a loss function that consists of three terms: prediction, consistency, and curvature (PCC). Crucially, to make PCC tractable, we derive an amortized variational bound for the PCC loss function. Extensive experiments on benchmark domains demonstrate that the new variational-PCC learning algorithm benefits from significantly more stable and reproducible training, and leads to superior control performance.  Further ablation studies give support to the importance of all three PCC components for learning a good latent space for control.", "pdf": "/pdf/45fa6ffcf44b62ef51bf27ff0cd00e506b9d116d.pdf", "paperhash": "levine|prediction_consistency_curvature_representation_learning_for_locallylinear_control", "_bibtex": "@inproceedings{\nLevine2020Prediction,,\ntitle={Prediction, Consistency, Curvature: Representation Learning for Locally-Linear Control},\nauthor={Nir Levine and Yinlam Chow and Rui Shu and Ang Li and Mohammad Ghavamzadeh and Hung Bui},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BJxG_0EtDS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/16be8d9f3e4ad898a15eb90334cc700a3027ed7c.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BJxG_0EtDS", "replyto": "BJxG_0EtDS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1204/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1204/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575119096542, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1204/Reviewers"], "noninvitees": [], "tcdate": 1570237740800, "tmdate": 1575119096558, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1204/-/Official_Review"}}}, {"id": "HylFTIdRYB", "original": null, "number": 2, "cdate": 1571878593269, "ddate": null, "tcdate": 1571878593269, "tmdate": 1572972499313, "tddate": null, "forum": "BJxG_0EtDS", "replyto": "BJxG_0EtDS", "invitation": "ICLR.cc/2020/Conference/Paper1204/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "This paper considers from a high level the problem of learning a latent representation of high dimensional observations with underlying dynamics for control.  The authors specifically describe some desiredata for latent representations for LLC algorithms. The authors rigorously construct a learning framework that can satisfy the desiredata and then show how this can be tractably instantiated. \n\nThe paper overall is clear, however there is many equations in 4.2  with heavy subscritping making it sometimes difficult to read. The authours could attempt to better highlight the more critical parts of their propositins (e.g. eq. 8/9). \n\nThe methodology and insights appear novel and well motivated, however I am not familiar with many of the prior work.  The experiments compared to competing methods  show substantial improvement. The authors also motivate well why these improvements over the existing methods should occur and provide ablations to validate all the components of the final loss. Overall the paper appears very solid and may motivate insights and research  in more complex model based control and planning \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1204/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1204/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Prediction, Consistency, Curvature: Representation Learning for Locally-Linear Control", "authors": ["Nir Levine", "Yinlam Chow", "Rui Shu", "Ang Li", "Mohammad Ghavamzadeh", "Hung Bui"], "authorids": ["nirlevine@google.com", "yinlamchow@google.com", "ruishu@stanford.edu", "anglili@google.com", "mgh@fb.com", "v.hungbh1@vinai.io"], "keywords": ["Embed-to-Control", "Representation Learning", "Stochastic Optimal Control", "VAE", "iLQR"], "TL;DR": "Learning embedding for control with high-dimensional observations", "abstract": "Many real-world sequential decision-making problems can be formulated as optimal control with high-dimensional observations and unknown dynamics. A promising approach is to embed the high-dimensional observations into a lower-dimensional latent representation space, estimate the latent dynamics model, then utilize this model for control in the latent space. An important open question is how to learn a representation that is amenable to existing control algorithms? In this paper, we focus on learning representations for locally-linear control algorithms, such as iterative LQR (iLQR). By formulating and analyzing the representation learning problem from an optimal control perspective, we establish three underlying principles that the learned representation should comprise: 1) accurate prediction in the observation space, 2) consistency between latent and observation space dynamics, and 3) low curvature in the latent space transitions. These principles naturally correspond to a loss function that consists of three terms: prediction, consistency, and curvature (PCC). Crucially, to make PCC tractable, we derive an amortized variational bound for the PCC loss function. Extensive experiments on benchmark domains demonstrate that the new variational-PCC learning algorithm benefits from significantly more stable and reproducible training, and leads to superior control performance.  Further ablation studies give support to the importance of all three PCC components for learning a good latent space for control.", "pdf": "/pdf/45fa6ffcf44b62ef51bf27ff0cd00e506b9d116d.pdf", "paperhash": "levine|prediction_consistency_curvature_representation_learning_for_locallylinear_control", "_bibtex": "@inproceedings{\nLevine2020Prediction,,\ntitle={Prediction, Consistency, Curvature: Representation Learning for Locally-Linear Control},\nauthor={Nir Levine and Yinlam Chow and Rui Shu and Ang Li and Mohammad Ghavamzadeh and Hung Bui},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BJxG_0EtDS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/16be8d9f3e4ad898a15eb90334cc700a3027ed7c.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BJxG_0EtDS", "replyto": "BJxG_0EtDS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1204/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1204/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575119096542, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1204/Reviewers"], "noninvitees": [], "tcdate": 1570237740800, "tmdate": 1575119096558, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1204/-/Official_Review"}}}], "count": 8}