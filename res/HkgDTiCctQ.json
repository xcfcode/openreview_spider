{"notes": [{"id": "HkgDTiCctQ", "original": "ryg37biqKQ", "number": 808, "cdate": 1538087870524, "ddate": null, "tcdate": 1538087870524, "tmdate": 1545355402109, "tddate": null, "forum": "HkgDTiCctQ", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Knowledge Distillation from Few Samples", "abstract": "Current knowledge distillation methods require full training data to distill knowledge from a large \"teacher\" network to a compact \"student\" network by matching certain statistics between \"teacher\" and \"student\" such as softmax outputs and feature responses. This is not only time-consuming but also inconsistent with human cognition in which children can learn knowledge from adults with few examples. This paper proposes a novel and simple method for knowledge distillation from few samples. Taking the assumption that both \"teacher\" and \"student\" have the same  feature map sizes at each corresponding block, we add a $1\\times 1$ conv-layer at the end of each block in the student-net, and align the block-level outputs between \"teacher\" and \"student\" by estimating the parameters of the added layer with limited samples. We prove that the added layer can be absorbed/merged into the previous conv-layer \\hl{to formulate a new conv-layer with the same size of parameters and computation cost as previous one. Experiments verifies that the proposed method is very efficient and effective to distill knowledge from teacher-net to student-net constructing in different ways on various datasets.", "keywords": ["knowledge distillation", "few-sample learning", "network compression"], "authorids": ["tianhong@mit.edu", "jianguo.li@intel.com", "zhuangl@berkeley.edu", "zcs@mail.tsinghua.edu.cn"], "authors": ["Tianhong Li", "Jianguo Li", "Zhuang Liu", "Changshui Zhang"], "TL;DR": "This paper proposes a novel and simple method for knowledge distillation from few samples.", "pdf": "/pdf/4effd15c3c2f9b1359e748bbbb9d649f116fe4a5.pdf", "paperhash": "li|knowledge_distillation_from_few_samples", "_bibtex": "@misc{\nli2019knowledge,\ntitle={Knowledge Distillation from Few Samples},\nauthor={Tianhong Li and Jianguo Li and Zhuang Liu and Changshui Zhang},\nyear={2019},\nurl={https://openreview.net/forum?id=HkgDTiCctQ},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 18, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "HkxXmGQlx4", "original": null, "number": 1, "cdate": 1544725019401, "ddate": null, "tcdate": 1544725019401, "tmdate": 1545354510770, "tddate": null, "forum": "HkgDTiCctQ", "replyto": "HkgDTiCctQ", "invitation": "ICLR.cc/2019/Conference/-/Paper808/Meta_Review", "content": {"metareview": "The paper considers the problem of knowledge distillation from a few samples. The proposed solution is to align feature representations of the student network with the teacher by adding 1x1 convolutions to each student block, and learning only the parameters of those layers. As noted by Reviewers 1 and 2, the performance of the proposed method is rather poor in absolute terms, and the use case considered (distillation from a few samples) is not motivated well enough. Reviewers also note the method is quite simplistic and incremental.", "confidence": "5: The area chair is absolutely certain", "recommendation": "Reject", "title": "limited novelty, unclear motivation"}, "signatures": ["ICLR.cc/2019/Conference/Paper808/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper808/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Knowledge Distillation from Few Samples", "abstract": "Current knowledge distillation methods require full training data to distill knowledge from a large \"teacher\" network to a compact \"student\" network by matching certain statistics between \"teacher\" and \"student\" such as softmax outputs and feature responses. This is not only time-consuming but also inconsistent with human cognition in which children can learn knowledge from adults with few examples. This paper proposes a novel and simple method for knowledge distillation from few samples. Taking the assumption that both \"teacher\" and \"student\" have the same  feature map sizes at each corresponding block, we add a $1\\times 1$ conv-layer at the end of each block in the student-net, and align the block-level outputs between \"teacher\" and \"student\" by estimating the parameters of the added layer with limited samples. We prove that the added layer can be absorbed/merged into the previous conv-layer \\hl{to formulate a new conv-layer with the same size of parameters and computation cost as previous one. Experiments verifies that the proposed method is very efficient and effective to distill knowledge from teacher-net to student-net constructing in different ways on various datasets.", "keywords": ["knowledge distillation", "few-sample learning", "network compression"], "authorids": ["tianhong@mit.edu", "jianguo.li@intel.com", "zhuangl@berkeley.edu", "zcs@mail.tsinghua.edu.cn"], "authors": ["Tianhong Li", "Jianguo Li", "Zhuang Liu", "Changshui Zhang"], "TL;DR": "This paper proposes a novel and simple method for knowledge distillation from few samples.", "pdf": "/pdf/4effd15c3c2f9b1359e748bbbb9d649f116fe4a5.pdf", "paperhash": "li|knowledge_distillation_from_few_samples", "_bibtex": "@misc{\nli2019knowledge,\ntitle={Knowledge Distillation from Few Samples},\nauthor={Tianhong Li and Jianguo Li and Zhuang Liu and Changshui Zhang},\nyear={2019},\nurl={https://openreview.net/forum?id=HkgDTiCctQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper808/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353080567, "tddate": null, "super": null, "final": null, "reply": {"forum": "HkgDTiCctQ", "replyto": "HkgDTiCctQ", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper808/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper808/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper808/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353080567}}}, {"id": "rye7rQUc1V", "original": null, "number": 13, "cdate": 1544344379029, "ddate": null, "tcdate": 1544344379029, "tmdate": 1544352516039, "tddate": null, "forum": "HkgDTiCctQ", "replyto": "r1ldl_Nqy4", "invitation": "ICLR.cc/2019/Conference/-/Paper808/Official_Comment", "content": {"title": "further response", "comment": "Thanks for the valuable comments and suggestions. Below further response your concerns. \n\n### Hong et al focuses on convex optimization problem\nWe agree that CNN optimization problem ins non-convex. However, our problem is not a standard CNN optimization problem. The loss function in eq(2) contains multiple disjoint blocks without non-linear activation function in each block (only between two blocks), while all the other network parts are fixed.\nEven considering the non-linear activation function after each block (Q), the loss is piece-wise linear. \nWith a prox-linear surrogate, the global convergence can be found by minimizing the prox-linear  surrogate as supposed by [1]. \n[1] Xu, Yangyang, and Wotao Yin. \"A globally convergent algorithm for nonconvex optimization based on block coordinate update.\" Journal of Scientific Computing 72, no. 2 (2017): 700-734.\n\n### BCD requires few samples, why not setting parameter to 1?\nSorry for the inaccurate descriptions. For our cases, the blocks are multiple disjoint blocks, it is not like those of coordinate descent in which variables are correlated to each other. Due to the disjoint properties, we can use relatively fewer samples to estimate parameters in each block. \n\n### performance on zero-student-net not comparable to teacher-net. \nWe should  emphasize that there are some new attempts which try to realize knowledge distillation with few samples, while all of them do not show good performance, including [2] and [3] on MNIST, [4] on CIFAR-10 and MNIST. \nWe should emphasize again that we have only use quite a few samples without data augmentation for this study, which still achieves much better accuracy than SGD and FitNet. With data augmentation, the accuracy can be improved about 5%. This is acceptable considering such a few original samples used. \nFurthermore, we emphasize the great benefits of the proposed framework on student-net by\nextremely decomposed/pruned from teacher-net. We specially emphasize the usage of this setting at the second but last paragraph. \n\n[2] Akisato Kimura, Zoubin Ghahramani, Koh Takeuchi, et al. Few-shot learning of neural networks from scratch by pseudo example optimization.  (big gap on MNIST SOTA performance with few samples). \n[3] Raphael Gontijo Lopes, Stefano Fenu, Thad Starner, et al. Data-free knowledge distillation for deep neural networks. arXiv preprint arXiv:1710.07535, 2017.\n[4] dataset distillation, ICLR 2019 submission. \n\n\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper808/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper808/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper808/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Knowledge Distillation from Few Samples", "abstract": "Current knowledge distillation methods require full training data to distill knowledge from a large \"teacher\" network to a compact \"student\" network by matching certain statistics between \"teacher\" and \"student\" such as softmax outputs and feature responses. This is not only time-consuming but also inconsistent with human cognition in which children can learn knowledge from adults with few examples. This paper proposes a novel and simple method for knowledge distillation from few samples. Taking the assumption that both \"teacher\" and \"student\" have the same  feature map sizes at each corresponding block, we add a $1\\times 1$ conv-layer at the end of each block in the student-net, and align the block-level outputs between \"teacher\" and \"student\" by estimating the parameters of the added layer with limited samples. We prove that the added layer can be absorbed/merged into the previous conv-layer \\hl{to formulate a new conv-layer with the same size of parameters and computation cost as previous one. Experiments verifies that the proposed method is very efficient and effective to distill knowledge from teacher-net to student-net constructing in different ways on various datasets.", "keywords": ["knowledge distillation", "few-sample learning", "network compression"], "authorids": ["tianhong@mit.edu", "jianguo.li@intel.com", "zhuangl@berkeley.edu", "zcs@mail.tsinghua.edu.cn"], "authors": ["Tianhong Li", "Jianguo Li", "Zhuang Liu", "Changshui Zhang"], "TL;DR": "This paper proposes a novel and simple method for knowledge distillation from few samples.", "pdf": "/pdf/4effd15c3c2f9b1359e748bbbb9d649f116fe4a5.pdf", "paperhash": "li|knowledge_distillation_from_few_samples", "_bibtex": "@misc{\nli2019knowledge,\ntitle={Knowledge Distillation from Few Samples},\nauthor={Tianhong Li and Jianguo Li and Zhuang Liu and Changshui Zhang},\nyear={2019},\nurl={https://openreview.net/forum?id=HkgDTiCctQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper808/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621604760, "tddate": null, "super": null, "final": null, "reply": {"forum": "HkgDTiCctQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper808/Authors", "ICLR.cc/2019/Conference/Paper808/Reviewers", "ICLR.cc/2019/Conference/Paper808/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper808/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper808/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper808/Authors|ICLR.cc/2019/Conference/Paper808/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper808/Reviewers", "ICLR.cc/2019/Conference/Paper808/Authors", "ICLR.cc/2019/Conference/Paper808/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621604760}}}, {"id": "r1ldl_Nqy4", "original": null, "number": 11, "cdate": 1544337391881, "ddate": null, "tcdate": 1544337391881, "tmdate": 1544337391881, "tddate": null, "forum": "HkgDTiCctQ", "replyto": "rye8Ow1cAQ", "invitation": "ICLR.cc/2019/Conference/-/Paper808/Official_Comment", "content": {"title": "Rebuttal read. Concerns remain.", "comment": "- Hong et al [1] focuses on BCD for convex optimization problem, which is very different from the proposed formulation. So I think its theoretical result has nothing to do with the your method.\n\n- \u201cThe BCD algorithm considers each block separately, thus there are much fewer parameters in each block, so that we could use much fewer samples for the block-level estimation.\u201d\n\nThis explanation sounds weird to me. According to this logic, we can always set the number of parameters in each block to be 1. Then it will become even more sample efficient.\n\n- \u201c83% on CIFAR-10 and about 47% on CIFAR-100\u201d\n\nThose are really bad performances, given that the teacher network can achieve 93.38% and 72.08% for CIFAR-10 and CIFAR-100. Usually, a distillation network can achieve similar performance as the teacher network (see [1]). Then I confirm my conclusion that the proposed technique is far from being used.\n\nTherefore, I will keep my rating.\n\n[1] Distilling the Knowledge in a Neural Network. Geoffrey Hinton, Oriol Vinyals, Jeff Dean. NIPS 2015.\n\n\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper808/AnonReviewer1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper808/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper808/AnonReviewer1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Knowledge Distillation from Few Samples", "abstract": "Current knowledge distillation methods require full training data to distill knowledge from a large \"teacher\" network to a compact \"student\" network by matching certain statistics between \"teacher\" and \"student\" such as softmax outputs and feature responses. This is not only time-consuming but also inconsistent with human cognition in which children can learn knowledge from adults with few examples. This paper proposes a novel and simple method for knowledge distillation from few samples. Taking the assumption that both \"teacher\" and \"student\" have the same  feature map sizes at each corresponding block, we add a $1\\times 1$ conv-layer at the end of each block in the student-net, and align the block-level outputs between \"teacher\" and \"student\" by estimating the parameters of the added layer with limited samples. We prove that the added layer can be absorbed/merged into the previous conv-layer \\hl{to formulate a new conv-layer with the same size of parameters and computation cost as previous one. Experiments verifies that the proposed method is very efficient and effective to distill knowledge from teacher-net to student-net constructing in different ways on various datasets.", "keywords": ["knowledge distillation", "few-sample learning", "network compression"], "authorids": ["tianhong@mit.edu", "jianguo.li@intel.com", "zhuangl@berkeley.edu", "zcs@mail.tsinghua.edu.cn"], "authors": ["Tianhong Li", "Jianguo Li", "Zhuang Liu", "Changshui Zhang"], "TL;DR": "This paper proposes a novel and simple method for knowledge distillation from few samples.", "pdf": "/pdf/4effd15c3c2f9b1359e748bbbb9d649f116fe4a5.pdf", "paperhash": "li|knowledge_distillation_from_few_samples", "_bibtex": "@misc{\nli2019knowledge,\ntitle={Knowledge Distillation from Few Samples},\nauthor={Tianhong Li and Jianguo Li and Zhuang Liu and Changshui Zhang},\nyear={2019},\nurl={https://openreview.net/forum?id=HkgDTiCctQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper808/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621604760, "tddate": null, "super": null, "final": null, "reply": {"forum": "HkgDTiCctQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper808/Authors", "ICLR.cc/2019/Conference/Paper808/Reviewers", "ICLR.cc/2019/Conference/Paper808/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper808/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper808/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper808/Authors|ICLR.cc/2019/Conference/Paper808/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper808/Reviewers", "ICLR.cc/2019/Conference/Paper808/Authors", "ICLR.cc/2019/Conference/Paper808/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621604760}}}, {"id": "HJlp2q150Q", "original": null, "number": 9, "cdate": 1543269044829, "ddate": null, "tcdate": 1543269044829, "tmdate": 1543283860302, "tddate": null, "forum": "HkgDTiCctQ", "replyto": "HkgDTiCctQ", "invitation": "ICLR.cc/2019/Conference/-/Paper808/Official_Comment", "content": {"title": "Revision list for the updated version", "comment": "Thanks to the valuable suggestions and comments from reviewers and anonymous commenter, we carefully revise our paper accordingly. Here we list major revision points below.\n1)      Add reasons why knowledge distillation from few samples is important.\n2)      Revise abstract/conclusion to replace ImageNet description with general descriptions.\n3)      Add comparison to two related papers in related work\n4)      Revise Theorem-1 to include the definition of \u201cabsorb\u201d, and text after Corollary-1 why Q should be squared.\n5)      Revise text after Eq-2 to clearly present that our algorithm is based on block coordinate descent (BCD), and the advantages of BCD.\n6)      Fig2 add experimental results comparison between FSKD-BCD and FSKD-SGD.\n7)      Add Fig3b to show the BCD accuracy improvement along with block alignment.\n8)      Table1~4, add columns for parameters number and pruned.\n9)      Zero-net adds more descriptions and analysis.\n10)    Appendix-B: the BCD algorithm and one experiment to show the impact of iteration number.\n11)    Appendix-C: Fig5 and Fig6 for illustrations of how FSKD works on filter pruning and network decoupling.\n12)    Appendix-D: iterative pruning and FSKD to achieve extremely pruned network and one experiment (scheme-C) on VGG-16 on CIFAR-10.\n13)    Appendix-E: verification of the hypothesis pointwise convolution is more critical for performance than depthwise convolution.\n14)    Appendix-F: Filter visualization on zero-student-net before SGD, after SGD, and after SGD+FSKD.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper808/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper808/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper808/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Knowledge Distillation from Few Samples", "abstract": "Current knowledge distillation methods require full training data to distill knowledge from a large \"teacher\" network to a compact \"student\" network by matching certain statistics between \"teacher\" and \"student\" such as softmax outputs and feature responses. This is not only time-consuming but also inconsistent with human cognition in which children can learn knowledge from adults with few examples. This paper proposes a novel and simple method for knowledge distillation from few samples. Taking the assumption that both \"teacher\" and \"student\" have the same  feature map sizes at each corresponding block, we add a $1\\times 1$ conv-layer at the end of each block in the student-net, and align the block-level outputs between \"teacher\" and \"student\" by estimating the parameters of the added layer with limited samples. We prove that the added layer can be absorbed/merged into the previous conv-layer \\hl{to formulate a new conv-layer with the same size of parameters and computation cost as previous one. Experiments verifies that the proposed method is very efficient and effective to distill knowledge from teacher-net to student-net constructing in different ways on various datasets.", "keywords": ["knowledge distillation", "few-sample learning", "network compression"], "authorids": ["tianhong@mit.edu", "jianguo.li@intel.com", "zhuangl@berkeley.edu", "zcs@mail.tsinghua.edu.cn"], "authors": ["Tianhong Li", "Jianguo Li", "Zhuang Liu", "Changshui Zhang"], "TL;DR": "This paper proposes a novel and simple method for knowledge distillation from few samples.", "pdf": "/pdf/4effd15c3c2f9b1359e748bbbb9d649f116fe4a5.pdf", "paperhash": "li|knowledge_distillation_from_few_samples", "_bibtex": "@misc{\nli2019knowledge,\ntitle={Knowledge Distillation from Few Samples},\nauthor={Tianhong Li and Jianguo Li and Zhuang Liu and Changshui Zhang},\nyear={2019},\nurl={https://openreview.net/forum?id=HkgDTiCctQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper808/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621604760, "tddate": null, "super": null, "final": null, "reply": {"forum": "HkgDTiCctQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper808/Authors", "ICLR.cc/2019/Conference/Paper808/Reviewers", "ICLR.cc/2019/Conference/Paper808/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper808/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper808/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper808/Authors|ICLR.cc/2019/Conference/Paper808/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper808/Reviewers", "ICLR.cc/2019/Conference/Paper808/Authors", "ICLR.cc/2019/Conference/Paper808/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621604760}}}, {"id": "SJxjSu1cRm", "original": null, "number": 7, "cdate": 1543268419232, "ddate": null, "tcdate": 1543268419232, "tmdate": 1543269688892, "tddate": null, "forum": "HkgDTiCctQ", "replyto": "SkgVZoJ7h7", "invitation": "ICLR.cc/2019/Conference/-/Paper808/Official_Comment", "content": {"title": "Response to Reviewer3", "comment": "Thank you for your review and suggestions, we provide our response as follows.\n\nQ: Learning 1*1-conv restrict the representation power, any hypothesis why the added 1x1 works so well?\nAs the added 1*1 conv-layer is finally absorbed into the decoupled 1*1 conv-layer, we here hypothesize that pointwise (1*1) convolution is more critical for performance than the depthwise convolution since it occupies >=80% parameters of the decoupled network. We design an experiment in Appendix-E to verify this hypothesis by comparing the full training to only training the 1*1 conv-layer with 3*3 initialized to be orthogonal from random data, on CIFAR-10/100 with VGG16 and ResNet50. The results show that full training works noticeably worse than our designed case. This interesting result verifies our hypothesis, and may inspire more researches to further understand CNN training optimization.\n\nQ: * in Eq-1/2 is undefined.\nThanks for pointing out this problem. We have added the definition in the revision.\n\nQ: Confused bold-face in the table.\nThanks for point out this problem. The boldface just wants to show the best results by FSKD, which may be not the best for all the cases.  We remove the bold-face in the table in our revision.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper808/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper808/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper808/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Knowledge Distillation from Few Samples", "abstract": "Current knowledge distillation methods require full training data to distill knowledge from a large \"teacher\" network to a compact \"student\" network by matching certain statistics between \"teacher\" and \"student\" such as softmax outputs and feature responses. This is not only time-consuming but also inconsistent with human cognition in which children can learn knowledge from adults with few examples. This paper proposes a novel and simple method for knowledge distillation from few samples. Taking the assumption that both \"teacher\" and \"student\" have the same  feature map sizes at each corresponding block, we add a $1\\times 1$ conv-layer at the end of each block in the student-net, and align the block-level outputs between \"teacher\" and \"student\" by estimating the parameters of the added layer with limited samples. We prove that the added layer can be absorbed/merged into the previous conv-layer \\hl{to formulate a new conv-layer with the same size of parameters and computation cost as previous one. Experiments verifies that the proposed method is very efficient and effective to distill knowledge from teacher-net to student-net constructing in different ways on various datasets.", "keywords": ["knowledge distillation", "few-sample learning", "network compression"], "authorids": ["tianhong@mit.edu", "jianguo.li@intel.com", "zhuangl@berkeley.edu", "zcs@mail.tsinghua.edu.cn"], "authors": ["Tianhong Li", "Jianguo Li", "Zhuang Liu", "Changshui Zhang"], "TL;DR": "This paper proposes a novel and simple method for knowledge distillation from few samples.", "pdf": "/pdf/4effd15c3c2f9b1359e748bbbb9d649f116fe4a5.pdf", "paperhash": "li|knowledge_distillation_from_few_samples", "_bibtex": "@misc{\nli2019knowledge,\ntitle={Knowledge Distillation from Few Samples},\nauthor={Tianhong Li and Jianguo Li and Zhuang Liu and Changshui Zhang},\nyear={2019},\nurl={https://openreview.net/forum?id=HkgDTiCctQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper808/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621604760, "tddate": null, "super": null, "final": null, "reply": {"forum": "HkgDTiCctQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper808/Authors", "ICLR.cc/2019/Conference/Paper808/Reviewers", "ICLR.cc/2019/Conference/Paper808/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper808/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper808/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper808/Authors|ICLR.cc/2019/Conference/Paper808/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper808/Reviewers", "ICLR.cc/2019/Conference/Paper808/Authors", "ICLR.cc/2019/Conference/Paper808/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621604760}}}, {"id": "HylT0D1qAm", "original": null, "number": 6, "cdate": 1543268308682, "ddate": null, "tcdate": 1543268308682, "tmdate": 1543269596533, "tddate": null, "forum": "HkgDTiCctQ", "replyto": "HkxK78ZK27", "invitation": "ICLR.cc/2019/Conference/-/Paper808/Official_Comment", "content": {"title": "Response to Reviewer2", "comment": "Thank you for your review and suggestions, we are happy to address your concerns.\n\nQ: Limited applications\nWe agree that teacher-student framework may make student-net has better accuracy than training the student-net from scratch with classification loss (like cross-entropy loss). Our target is fast knowledge distillation from few samples, which has many potential situations for application:\nFirst, on-device learning to compression when the device has resource constraints which require cheap knowledge distillation solutions.\nSecond, when software/hardware vendors want to use knowledge distillation for model compression while the full data is not available due to privacy and confidential issues.\nThird, there is a strict time budget for model optimization so that full training or fine-tuning is not allowed.\nThank you for your suggestions and we will articulate these reasons in the revision.\n\nQ: Resource usages in the Tables?\nThanks for your suggestions, we added the resources usages for all our experiments (Table-1~4). Note the parameter pruned ratio is usually higher than the FLOPs reduced ratio.\n\nQ: Aggressive compression case\nIn Table1~3, previously we only show the FLOPs reduction ratio, which may make people confused that our compression ratio is relatively low. However, when we add the compression ratio, we could find that in some cases, the compression ratio is also very high (>=85%) not just for Table-4. Nevertheless, we introduce a novel \u201citeratively pruning and FSKD\u201d procedure similar to that in filter-pruning and network slimming, in Appendix-D. This procedure can produce even larger compression ratio (88%), as shown in the experiment on CIFAR-10.\n\nQ: Visualize the filter before and after FSKD for zero student net?\nThe 3*3 kernel is too small to be visualized. However, based on the network decoupling theory, any regular convolution could be decoupled into a sum of depthwise separable convolution blocks, where each block consists of a depthwise conv-layer followed by a pointwise (1*1) conv-layer. The pointwise layer is just a linear combination of channels from the depthwise layer. We then visualize the pointwise conv-layer before SGD, after SGD, after SGD + FSKD for the zero-student case in Appendix-F.\nThe experiment shows FSKD improves the smoothness of pointwise convolution tensors.\n\nQ: Cite and comparison with two papers?\nThanks for pointing us to these two papers. We have cited and compared with them in our revision.\n\nQ: Cite for fine-tuning?\nYes, the fine-tuning for filter-pruning/network slimming is the same as these two papers did. We have cited them in the revision.\n\nQ: Illustrate the FSKD case for filter-pruning and network decoupling\nWe have illustrated these two cases in Figure-5 and Figure-6 in Appendix-C. Please see the revised paper for more details.\n\nQ: Why Q should be squared?\nSquared Q ensures model compression and connectable to the next block (output channel number matches to the input channel number of next block). We have mentioned this in our revision after Theorem-1."}, "signatures": ["ICLR.cc/2019/Conference/Paper808/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper808/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper808/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Knowledge Distillation from Few Samples", "abstract": "Current knowledge distillation methods require full training data to distill knowledge from a large \"teacher\" network to a compact \"student\" network by matching certain statistics between \"teacher\" and \"student\" such as softmax outputs and feature responses. This is not only time-consuming but also inconsistent with human cognition in which children can learn knowledge from adults with few examples. This paper proposes a novel and simple method for knowledge distillation from few samples. Taking the assumption that both \"teacher\" and \"student\" have the same  feature map sizes at each corresponding block, we add a $1\\times 1$ conv-layer at the end of each block in the student-net, and align the block-level outputs between \"teacher\" and \"student\" by estimating the parameters of the added layer with limited samples. We prove that the added layer can be absorbed/merged into the previous conv-layer \\hl{to formulate a new conv-layer with the same size of parameters and computation cost as previous one. Experiments verifies that the proposed method is very efficient and effective to distill knowledge from teacher-net to student-net constructing in different ways on various datasets.", "keywords": ["knowledge distillation", "few-sample learning", "network compression"], "authorids": ["tianhong@mit.edu", "jianguo.li@intel.com", "zhuangl@berkeley.edu", "zcs@mail.tsinghua.edu.cn"], "authors": ["Tianhong Li", "Jianguo Li", "Zhuang Liu", "Changshui Zhang"], "TL;DR": "This paper proposes a novel and simple method for knowledge distillation from few samples.", "pdf": "/pdf/4effd15c3c2f9b1359e748bbbb9d649f116fe4a5.pdf", "paperhash": "li|knowledge_distillation_from_few_samples", "_bibtex": "@misc{\nli2019knowledge,\ntitle={Knowledge Distillation from Few Samples},\nauthor={Tianhong Li and Jianguo Li and Zhuang Liu and Changshui Zhang},\nyear={2019},\nurl={https://openreview.net/forum?id=HkgDTiCctQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper808/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621604760, "tddate": null, "super": null, "final": null, "reply": {"forum": "HkgDTiCctQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper808/Authors", "ICLR.cc/2019/Conference/Paper808/Reviewers", "ICLR.cc/2019/Conference/Paper808/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper808/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper808/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper808/Authors|ICLR.cc/2019/Conference/Paper808/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper808/Reviewers", "ICLR.cc/2019/Conference/Paper808/Authors", "ICLR.cc/2019/Conference/Paper808/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621604760}}}, {"id": "rye8Ow1cAQ", "original": null, "number": 5, "cdate": 1543268206355, "ddate": null, "tcdate": 1543268206355, "tmdate": 1543269519112, "tddate": null, "forum": "HkgDTiCctQ", "replyto": "B1e5MapjhQ", "invitation": "ICLR.cc/2019/Conference/-/Paper808/Official_Comment", "content": {"title": "Response to Reviewer1", "comment": "Thanks for the valuable comments and suggestions. We give detailed responses to each item below.\n\n1. Meaning of \u201cabsorb\u201d\nWe add a 1x1 conv-layer Q \\in R^{n_o\u2019 * n_o * 1 * 1} to student-net after the conv-layer W\\in R^{n_o *n_i * k* k} before non-linear layer, \u201cabsorb\u201d here means Q can be merged into W to obtain a new conv-layer W\u2019 \\in R^{n_o\u2019 * n_i *k *k}. If Q is squared (n_o\u2019=n_o), then W\u2019 \\in R^{n_o * n_i *k*k} has the same size as W. Previously we put this information at appendix-A, and now we revise the description of Theorem-1 to include the information. \n \n2&3: why not SGD, why use one-step block coordinate descent?\nYes, what we use is in fact one-step block coordinate descent (BCD) algorithm. We add a description of our BCD algorithm in the appendix-B, also include experiment comparison to FSKD-SGD (total loss optimization with SGD on all added 1x1 convs\u2019 parameters together) and FSKD-BCD in the experiments on filter-pruning. The experiments show that FSKD-BCD clearly outperforms FSKD-SGD in all cases. The advantages of the BCD algorithm are also listed in the revision. One major reason is that BCD each time handles few parameters (one block) which can be solved with limited samples, while SGD always takes all added 1x1 convs\u2019 parameters into consideration, thus requires more data in the optimization.\nOur experiments do not show benefit from more iterations of BCD.\nThis may be due to the fact that the added 1x1 conv-layer is before non-linear activations so that one-step linear estimation is accurate enough to get exact minimization. Hong et al [1] show BCD can reach sublinear convergence when each block is exactly minimized, which is consistent with our experiments. We also add Fig3b to illustrate the accuracy improvement along with block alignment sequentially.\n \n[1] Hong, Mingyi, et al. \"Iteration complexity analysis of block coordinate descent methods.\" Mathematical Programming 163.1-2 (2017): 85-114.\n\n4. Confusing on SGD+FSKD and FitNet+FSKD\nWe denote SGD as optimization without using teacher-net info. For the zero-net experiment, SGD+FSKD first uses SGD to initialize the student-net on few samples without using teacher-net info, then uses FSKD to further improve the performance with teacher-net info. While the Fitnet+FSKD first uses FitNet to initialize the student-net on few-samples with teacher-net guidance, then uses FSKD to further improve the performance with teacher-net info from our FSKD perspective. We clarify this in our revision.\n\n5. Why FSKD is sample efficient?\nAs is known, fewer parameters tend to require fewer samples for estimation. The BCD algorithm considers each block separately, thus there are much fewer parameters in each block, so that we could use much fewer samples for the block-level estimation. Our experiments also verify this point when comparing FSKD-BCD to FSKD-SGD in Figure-2, especially when samples < 100.  \n\n6. Not good on redesigned student-net?\nYes, in Figure-4, the student-net accuracy is about 83% on CIFAR-10 and about 47% on CIFAR-100. We should emphasize that this result is obtained with a very limited number of training samples and without data augmentation. If data augmentation is enabled, about 5% accuracy improvement could be achieved with FSKD. We design this experiments to demonstrate the effectiveness of FSKD over FitNet and SGD on the few-sample settings, and we do not compare this result with full-data training.\n\n7. Only mention results on ImageNet in abstract and conclusion.\nWe have revised our abstract and conclusion accordingly, even though the results on ImageNet sounds more significant to us."}, "signatures": ["ICLR.cc/2019/Conference/Paper808/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper808/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper808/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Knowledge Distillation from Few Samples", "abstract": "Current knowledge distillation methods require full training data to distill knowledge from a large \"teacher\" network to a compact \"student\" network by matching certain statistics between \"teacher\" and \"student\" such as softmax outputs and feature responses. This is not only time-consuming but also inconsistent with human cognition in which children can learn knowledge from adults with few examples. This paper proposes a novel and simple method for knowledge distillation from few samples. Taking the assumption that both \"teacher\" and \"student\" have the same  feature map sizes at each corresponding block, we add a $1\\times 1$ conv-layer at the end of each block in the student-net, and align the block-level outputs between \"teacher\" and \"student\" by estimating the parameters of the added layer with limited samples. We prove that the added layer can be absorbed/merged into the previous conv-layer \\hl{to formulate a new conv-layer with the same size of parameters and computation cost as previous one. Experiments verifies that the proposed method is very efficient and effective to distill knowledge from teacher-net to student-net constructing in different ways on various datasets.", "keywords": ["knowledge distillation", "few-sample learning", "network compression"], "authorids": ["tianhong@mit.edu", "jianguo.li@intel.com", "zhuangl@berkeley.edu", "zcs@mail.tsinghua.edu.cn"], "authors": ["Tianhong Li", "Jianguo Li", "Zhuang Liu", "Changshui Zhang"], "TL;DR": "This paper proposes a novel and simple method for knowledge distillation from few samples.", "pdf": "/pdf/4effd15c3c2f9b1359e748bbbb9d649f116fe4a5.pdf", "paperhash": "li|knowledge_distillation_from_few_samples", "_bibtex": "@misc{\nli2019knowledge,\ntitle={Knowledge Distillation from Few Samples},\nauthor={Tianhong Li and Jianguo Li and Zhuang Liu and Changshui Zhang},\nyear={2019},\nurl={https://openreview.net/forum?id=HkgDTiCctQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper808/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621604760, "tddate": null, "super": null, "final": null, "reply": {"forum": "HkgDTiCctQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper808/Authors", "ICLR.cc/2019/Conference/Paper808/Reviewers", "ICLR.cc/2019/Conference/Paper808/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper808/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper808/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper808/Authors|ICLR.cc/2019/Conference/Paper808/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper808/Reviewers", "ICLR.cc/2019/Conference/Paper808/Authors", "ICLR.cc/2019/Conference/Paper808/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621604760}}}, {"id": "S1gj3tk50X", "original": null, "number": 8, "cdate": 1543268786569, "ddate": null, "tcdate": 1543268786569, "tmdate": 1543269424694, "tddate": null, "forum": "HkgDTiCctQ", "replyto": "S1lp283D6Q", "invitation": "ICLR.cc/2019/Conference/-/Paper808/Official_Comment", "content": {"title": "absorbing is defined in the revision", "comment": "Thanks for the question. We add definition in our revision. \nPlease see the text we answer to Reviewer-1 for more explanation. \nWe also visualize how FSKD works for filter pruning/network slimming and the network decoupling cases. \nPlease check our Appendix-C for more details. \n \n"}, "signatures": ["ICLR.cc/2019/Conference/Paper808/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper808/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper808/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Knowledge Distillation from Few Samples", "abstract": "Current knowledge distillation methods require full training data to distill knowledge from a large \"teacher\" network to a compact \"student\" network by matching certain statistics between \"teacher\" and \"student\" such as softmax outputs and feature responses. This is not only time-consuming but also inconsistent with human cognition in which children can learn knowledge from adults with few examples. This paper proposes a novel and simple method for knowledge distillation from few samples. Taking the assumption that both \"teacher\" and \"student\" have the same  feature map sizes at each corresponding block, we add a $1\\times 1$ conv-layer at the end of each block in the student-net, and align the block-level outputs between \"teacher\" and \"student\" by estimating the parameters of the added layer with limited samples. We prove that the added layer can be absorbed/merged into the previous conv-layer \\hl{to formulate a new conv-layer with the same size of parameters and computation cost as previous one. Experiments verifies that the proposed method is very efficient and effective to distill knowledge from teacher-net to student-net constructing in different ways on various datasets.", "keywords": ["knowledge distillation", "few-sample learning", "network compression"], "authorids": ["tianhong@mit.edu", "jianguo.li@intel.com", "zhuangl@berkeley.edu", "zcs@mail.tsinghua.edu.cn"], "authors": ["Tianhong Li", "Jianguo Li", "Zhuang Liu", "Changshui Zhang"], "TL;DR": "This paper proposes a novel and simple method for knowledge distillation from few samples.", "pdf": "/pdf/4effd15c3c2f9b1359e748bbbb9d649f116fe4a5.pdf", "paperhash": "li|knowledge_distillation_from_few_samples", "_bibtex": "@misc{\nli2019knowledge,\ntitle={Knowledge Distillation from Few Samples},\nauthor={Tianhong Li and Jianguo Li and Zhuang Liu and Changshui Zhang},\nyear={2019},\nurl={https://openreview.net/forum?id=HkgDTiCctQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper808/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621604760, "tddate": null, "super": null, "final": null, "reply": {"forum": "HkgDTiCctQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper808/Authors", "ICLR.cc/2019/Conference/Paper808/Reviewers", "ICLR.cc/2019/Conference/Paper808/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper808/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper808/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper808/Authors|ICLR.cc/2019/Conference/Paper808/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper808/Reviewers", "ICLR.cc/2019/Conference/Paper808/Authors", "ICLR.cc/2019/Conference/Paper808/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621604760}}}, {"id": "S1lp283D6Q", "original": null, "number": 3, "cdate": 1542076085338, "ddate": null, "tcdate": 1542076085338, "tmdate": 1542076085338, "tddate": null, "forum": "HkgDTiCctQ", "replyto": "HkgDTiCctQ", "invitation": "ICLR.cc/2019/Conference/-/Paper808/Public_Comment", "content": {"comment": "What does it mean with absorb 1x1 conv layer to previous conv layer?\nI think absorb is too ambiguous for a word\nCopy weights to previous conv layer?\nStack trained 1x1 conv on top of previous layer but below ReLU?\n\nNeed better explainability, some figure to visualize would help\n\n", "title": "absorb convolution?"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Knowledge Distillation from Few Samples", "abstract": "Current knowledge distillation methods require full training data to distill knowledge from a large \"teacher\" network to a compact \"student\" network by matching certain statistics between \"teacher\" and \"student\" such as softmax outputs and feature responses. This is not only time-consuming but also inconsistent with human cognition in which children can learn knowledge from adults with few examples. This paper proposes a novel and simple method for knowledge distillation from few samples. Taking the assumption that both \"teacher\" and \"student\" have the same  feature map sizes at each corresponding block, we add a $1\\times 1$ conv-layer at the end of each block in the student-net, and align the block-level outputs between \"teacher\" and \"student\" by estimating the parameters of the added layer with limited samples. We prove that the added layer can be absorbed/merged into the previous conv-layer \\hl{to formulate a new conv-layer with the same size of parameters and computation cost as previous one. Experiments verifies that the proposed method is very efficient and effective to distill knowledge from teacher-net to student-net constructing in different ways on various datasets.", "keywords": ["knowledge distillation", "few-sample learning", "network compression"], "authorids": ["tianhong@mit.edu", "jianguo.li@intel.com", "zhuangl@berkeley.edu", "zcs@mail.tsinghua.edu.cn"], "authors": ["Tianhong Li", "Jianguo Li", "Zhuang Liu", "Changshui Zhang"], "TL;DR": "This paper proposes a novel and simple method for knowledge distillation from few samples.", "pdf": "/pdf/4effd15c3c2f9b1359e748bbbb9d649f116fe4a5.pdf", "paperhash": "li|knowledge_distillation_from_few_samples", "_bibtex": "@misc{\nli2019knowledge,\ntitle={Knowledge Distillation from Few Samples},\nauthor={Tianhong Li and Jianguo Li and Zhuang Liu and Changshui Zhang},\nyear={2019},\nurl={https://openreview.net/forum?id=HkgDTiCctQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper808/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311748052, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "HkgDTiCctQ", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper808/Authors", "ICLR.cc/2019/Conference/Paper808/Reviewers", "ICLR.cc/2019/Conference/Paper808/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper808/Authors", "ICLR.cc/2019/Conference/Paper808/Reviewers", "ICLR.cc/2019/Conference/Paper808/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311748052}}}, {"id": "B1e5MapjhQ", "original": null, "number": 3, "cdate": 1541295378059, "ddate": null, "tcdate": 1541295378059, "tmdate": 1541533673335, "tddate": null, "forum": "HkgDTiCctQ", "replyto": "HkgDTiCctQ", "invitation": "ICLR.cc/2019/Conference/-/Paper808/Official_Review", "content": {"title": "A new formulation of knowledge distillation", "review": "This paper proposes a framework for few-sample knowledge distillation of convolution neural networks. The basic idea is to fit the output of the student network and that of the teacher network layer-wisely. Such a regression problem is parameterized by a 1x1 point-wise convolution per layer (i.e. minimizing the fitting objective over the parameters of 1x1 convolutions). The author claims such an approach, called FSKD, is much more sample-efficient than previous works on knowledge distillation. Besides, it is also fast to finish the alignment procedure as the number of parameters is smaller than that in previous works. The sample efficiency is confirmed in the experiments on CIFAR-10, CIFAR-100 and ImageNet with various pruning techniques. In particular, FSKD outperforms the FitNet and fine-tuning by non-trivial margins if only small amount of samples are provided (e.g. 100).\n\nHere are some comments:\n\n1. What exactly does \u201cabsorb\u201d mean? Is it formally defined in the paper?\n\n2. \u201cwe do not optimize this loss all together using SGD due to that too much hyper-parameters need tuning in SGD\u201d. I don\u2019t understand (1) why does SGD require \u201ctoo much\u201d hyper-parameters tuning and (2) if not SGD, what algorithm do you use?  \n\n3. According to the illustration in 3.3, the algorithm looks like a coordinate decent that optimizing L over one Q_j at a time, with the rest fixed. However, the sentence \u201cuntil we reach the last block in the student-net\u201d means the algorithm only runs one iteration, which I suspect might not be sufficient to converge.\n\n4. It is also confusing to use the notation SGD+FSKD v.s. FitNet+FSKD, as it seems SGD and FitNet are referring to the same type of terminology. However, SGD is an algorithm, while FitNet is an approach for neural network distillation. \n\n5. While I understand the training of student network with FSKD should be faster because the 1x1 convolution has fewer parameters to optimize, why is it also sample-efficient? \n\n6. I assume the Top-1 accuracies of teacher networks in Figure 4 are the same as table 2 and 3, i.e. 93.38% and 72.08% for CIFAR-10 and CIFAR-100 respectively. Then the student networks have much worse performance (~85% for CIFAR-10 and ~48% for CIFAR-100) than the teachers. So does it mean FSKD is not good for redesigned student networks?\n\n7. While most of the experiments are on CIFAR10 and CIFAR100, the abstract and conclusion only mention the results of ImageNet. Why?", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper808/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Knowledge Distillation from Few Samples", "abstract": "Current knowledge distillation methods require full training data to distill knowledge from a large \"teacher\" network to a compact \"student\" network by matching certain statistics between \"teacher\" and \"student\" such as softmax outputs and feature responses. This is not only time-consuming but also inconsistent with human cognition in which children can learn knowledge from adults with few examples. This paper proposes a novel and simple method for knowledge distillation from few samples. Taking the assumption that both \"teacher\" and \"student\" have the same  feature map sizes at each corresponding block, we add a $1\\times 1$ conv-layer at the end of each block in the student-net, and align the block-level outputs between \"teacher\" and \"student\" by estimating the parameters of the added layer with limited samples. We prove that the added layer can be absorbed/merged into the previous conv-layer \\hl{to formulate a new conv-layer with the same size of parameters and computation cost as previous one. Experiments verifies that the proposed method is very efficient and effective to distill knowledge from teacher-net to student-net constructing in different ways on various datasets.", "keywords": ["knowledge distillation", "few-sample learning", "network compression"], "authorids": ["tianhong@mit.edu", "jianguo.li@intel.com", "zhuangl@berkeley.edu", "zcs@mail.tsinghua.edu.cn"], "authors": ["Tianhong Li", "Jianguo Li", "Zhuang Liu", "Changshui Zhang"], "TL;DR": "This paper proposes a novel and simple method for knowledge distillation from few samples.", "pdf": "/pdf/4effd15c3c2f9b1359e748bbbb9d649f116fe4a5.pdf", "paperhash": "li|knowledge_distillation_from_few_samples", "_bibtex": "@misc{\nli2019knowledge,\ntitle={Knowledge Distillation from Few Samples},\nauthor={Tianhong Li and Jianguo Li and Zhuang Liu and Changshui Zhang},\nyear={2019},\nurl={https://openreview.net/forum?id=HkgDTiCctQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper808/Official_Review", "cdate": 1542234372275, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HkgDTiCctQ", "replyto": "HkgDTiCctQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper808/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335807103, "tmdate": 1552335807103, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper808/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "HkxK78ZK27", "original": null, "number": 2, "cdate": 1541113377035, "ddate": null, "tcdate": 1541113377035, "tmdate": 1541533673127, "tddate": null, "forum": "HkgDTiCctQ", "replyto": "HkgDTiCctQ", "invitation": "ICLR.cc/2019/Conference/-/Paper808/Official_Review", "content": {"title": "Surprisingly good model distillation given few samples and non-iterative solution, but practical implications are unclear", "review": "Model distillation can be tricky and in my own experience can take a lot of samples (albeit unlabeled, so cheaper and more readily available), as well as time to train. This simple trick seems to be doing quite well at training students quickly with few samples. However, it departs from most student-teacher training that find its primary purpose by actually outperforming students trained from scratch (on the full dataset without time constraints). This trick does not outperform this baseline, so its emphasis is entirely on quick and cheap. However, it's unclear to me how often that is actually necessary and I don't think the paper makes a compelling case in this regard. I am borderline on this work and could probably be swayed either way.\n\nStrengths:\n- It's a very simple and fast technique. As I will cover in a later bullet point (under weaknesses), the paper does not make it clear why this type of model distillation is that useful (since it doesn't improve the student model over full fine-tuning, unlike most student-teacher work). However, the reason why I do see some potential for this paper is because there might be a use case in quickly being able to adapt a pretrained network. It is very common to start from a pretrained model and then attach a new loss and fine-tune. Under this paradigm, it is harder to make architectural adjustments, since you are starting from a finite set of pretrained models made available by other folks (or accept the cost of re-training one yourself). However, it is unclear how careful one needs to treat the pretrained model if more fine-tuning is going to occur. If for instance you could just remove layers, drop some channels, glue it all together, and then that model would still be reasonable as a pretrained model since the fine-tuning stage could tidy everything up, then this method would not be useful in this situation.\n- The fact that least squares solvers can be used at each stage, without the need for a final end-to-end fine-tune is interesting.\n- It is good that the paper demonstrates improvements coupled with three separate compression techniques (Li et al., Liu et al., Guo et al.).\n- The paper is technically thorough.\n- It's good that the method is evaluated on different styles of networks (VGG, ResNet, DenseNet).\n\nWeaknesses:\n- Limited application because it only makes the distillation faster and cheaper. The primary goal of student-teacher training in literature is to outperform a student trained from scratch by the wisdom of the teacher. It ties into this notion that networks are grossly over-parameterized, but perhaps that is where the training magic comes from. Student-teacher training acknowledges this and tries to find a way to benefit from the over-parameterized training and still end up with a small model. I think the same motivation is used for work in  low-rank decomposition and many other network compression methods. However, in Table 1 the \"full fine-tune\" model is actually the clear winner and presented almost as an upper bound here, so the only benefit this paper presents is quick and cheap model distillation, not better models. Because of this, I think this paper needs to spend more time making a case for why this is so important.\n- Since this technique doesn't outperform full fine-tuning, the goal of this work is much more focused on pure model compression. This could put emphasis on reducing model size, RAM usage reduction, or FLOPS reduction. The paper focuses on the last one, which is an important one as it correlates fairly well with power (the biggest constraint in most on-device scenarios). However, it would be great if the paper gave a broader comparison with compression technique that may have slightly different focus, such as low-rank decomposition. Size and memory usage could be included as columns in tables like 1, along with a few of these methods. \n- Does it work for aggressive compression? The paper presents mostly modest reductions (30-50%). I thin even if accuracy takes a hit, it could still work to various degrees. From what I can see, the biggest reduction is in Table 4, but FSKD is used throughout this table, so there is no comparison for aggressive compression with other techniques. \n- The method requires appropriate blocks to line up. If you completely re-design a network, it is not as straightforward as regular student-teacher training. Even the zero-student method requires the same number of channels at certain block ends and it is unclear from the experiments how robust this is. Actually, a bit more analysis into the zero student would be great. For instance, it's very interesting how you randomly initialize (let's say 3x3) kernels, and then the final kernels are actually just linear combinations of these - so, will they look random or will they look fairly good? What if this was done at the initial layer where we can visualize the filters, will they look smooth or not?\n\nOther comments:\n- A comparison with \"Deep Mutual Learning\" might be relevant (Zhang et al.). I think there are also some papers on gradually adjusting neural network architectures (by adding/dropping layers/channels) that are not addressed but seem relevant. I didn't read this recently, but perhaps \"Gradual DropIn of Layers to Train Very Deep Neural Networks\" could be relevant. There is at least one more like this that I've seen that I can't seem to find now.\n- It could be more clear in the tables exactly what cited method is. For instance, in Table 1, does \"Fine-tuning\" (without FitNet/FSKD) correspond to the work of Li et al. (2016)? I think this should be made more clear, for instance by including a citation in the table for the correct row. Right now, at a glance, it would seem that these results are only comparing against prior work when it compares to FitNet, but as I read further, I understood that's not the case.\n- The paper could use a visual aid for explaining pruning/slimming/decoupling.\n\nMinor comments:\n- page 4, \"due to that too much hyper-parameters\"\n- page 4, \"each of the M term\" -> \"terms\"\n- page 6, methods like FitNet provides\" -> \"provide\"", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper808/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Knowledge Distillation from Few Samples", "abstract": "Current knowledge distillation methods require full training data to distill knowledge from a large \"teacher\" network to a compact \"student\" network by matching certain statistics between \"teacher\" and \"student\" such as softmax outputs and feature responses. This is not only time-consuming but also inconsistent with human cognition in which children can learn knowledge from adults with few examples. This paper proposes a novel and simple method for knowledge distillation from few samples. Taking the assumption that both \"teacher\" and \"student\" have the same  feature map sizes at each corresponding block, we add a $1\\times 1$ conv-layer at the end of each block in the student-net, and align the block-level outputs between \"teacher\" and \"student\" by estimating the parameters of the added layer with limited samples. We prove that the added layer can be absorbed/merged into the previous conv-layer \\hl{to formulate a new conv-layer with the same size of parameters and computation cost as previous one. Experiments verifies that the proposed method is very efficient and effective to distill knowledge from teacher-net to student-net constructing in different ways on various datasets.", "keywords": ["knowledge distillation", "few-sample learning", "network compression"], "authorids": ["tianhong@mit.edu", "jianguo.li@intel.com", "zhuangl@berkeley.edu", "zcs@mail.tsinghua.edu.cn"], "authors": ["Tianhong Li", "Jianguo Li", "Zhuang Liu", "Changshui Zhang"], "TL;DR": "This paper proposes a novel and simple method for knowledge distillation from few samples.", "pdf": "/pdf/4effd15c3c2f9b1359e748bbbb9d649f116fe4a5.pdf", "paperhash": "li|knowledge_distillation_from_few_samples", "_bibtex": "@misc{\nli2019knowledge,\ntitle={Knowledge Distillation from Few Samples},\nauthor={Tianhong Li and Jianguo Li and Zhuang Liu and Changshui Zhang},\nyear={2019},\nurl={https://openreview.net/forum?id=HkgDTiCctQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper808/Official_Review", "cdate": 1542234372275, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HkgDTiCctQ", "replyto": "HkgDTiCctQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper808/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335807103, "tmdate": 1552335807103, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper808/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "SkgVZoJ7h7", "original": null, "number": 1, "cdate": 1540713211522, "ddate": null, "tcdate": 1540713211522, "tmdate": 1541533672886, "tddate": null, "forum": "HkgDTiCctQ", "replyto": "HkgDTiCctQ", "invitation": "ICLR.cc/2019/Conference/-/Paper808/Official_Review", "content": {"title": "A practical method", "review": "In this paper, an efficient re-training algorithm for neural networks is proposed. The essence is like Hinton's distillation, but in addition to use the output of the last layer, the outputs of intermediate layers are also used. The core idea is to add 1x1 convolutions to the end of each layer and train them by fixing other parameters. Since the number of parameters to train is small, it performs well with the small number of samples such as 500 samples. \n\nThe proposed method named FKSD is simple yet achieves good performance. Also, it performs well with a few samples, which is desirable in terms of time complexity. \n\nThe downside of this paper is that there is no clear explanation of why the FKSD method goes well. For me, adding 1x1 convolution after the original convolution and fitting the kernel of the 1x1 conv instead of the original kernel looks a kind of reparametrization trick. Of course, learning 1x1 conv is easier than learning original conv because of a few parameters. However, it also restricts the representation power so we cannot say which one is always better. Do you have any hypothesis of why 1x1 conv works so well?\n\n\n\nMinor:\n\nThe operator * in (1) is undefined.\n\nWhat does the boldface in tables of the experiments mean? I was confused because, in Table 1, the accuracy achieved by FKSD is in bold but is not the highest one.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper808/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Knowledge Distillation from Few Samples", "abstract": "Current knowledge distillation methods require full training data to distill knowledge from a large \"teacher\" network to a compact \"student\" network by matching certain statistics between \"teacher\" and \"student\" such as softmax outputs and feature responses. This is not only time-consuming but also inconsistent with human cognition in which children can learn knowledge from adults with few examples. This paper proposes a novel and simple method for knowledge distillation from few samples. Taking the assumption that both \"teacher\" and \"student\" have the same  feature map sizes at each corresponding block, we add a $1\\times 1$ conv-layer at the end of each block in the student-net, and align the block-level outputs between \"teacher\" and \"student\" by estimating the parameters of the added layer with limited samples. We prove that the added layer can be absorbed/merged into the previous conv-layer \\hl{to formulate a new conv-layer with the same size of parameters and computation cost as previous one. Experiments verifies that the proposed method is very efficient and effective to distill knowledge from teacher-net to student-net constructing in different ways on various datasets.", "keywords": ["knowledge distillation", "few-sample learning", "network compression"], "authorids": ["tianhong@mit.edu", "jianguo.li@intel.com", "zhuangl@berkeley.edu", "zcs@mail.tsinghua.edu.cn"], "authors": ["Tianhong Li", "Jianguo Li", "Zhuang Liu", "Changshui Zhang"], "TL;DR": "This paper proposes a novel and simple method for knowledge distillation from few samples.", "pdf": "/pdf/4effd15c3c2f9b1359e748bbbb9d649f116fe4a5.pdf", "paperhash": "li|knowledge_distillation_from_few_samples", "_bibtex": "@misc{\nli2019knowledge,\ntitle={Knowledge Distillation from Few Samples},\nauthor={Tianhong Li and Jianguo Li and Zhuang Liu and Changshui Zhang},\nyear={2019},\nurl={https://openreview.net/forum?id=HkgDTiCctQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper808/Official_Review", "cdate": 1542234372275, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HkgDTiCctQ", "replyto": "HkgDTiCctQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper808/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335807103, "tmdate": 1552335807103, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper808/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "BJgfkfRUhQ", "original": null, "number": 4, "cdate": 1540968921877, "ddate": null, "tcdate": 1540968921877, "tmdate": 1540970181311, "tddate": null, "forum": "HkgDTiCctQ", "replyto": "H1x7gjxLnm", "invitation": "ICLR.cc/2019/Conference/-/Paper808/Official_Comment", "content": {"title": "Three detailed cases how Q is defined", "comment": "Thanks for your comments!\n\nFirst we clarify the initialization problem. In section 3.3, we conduct two sets of experiments. The first set obtains student-net from compressing teacher-net, including filter pruning, network slimming, and network decoupling. The second set fully redesigned the student-net with a different structure from the teacher-net and random initialized the parameters (i.e., zero net in the paper). \nFor the first set of experiments, the student-net already has an initialization from original teacher-net's weights. \nFor the second set of experiment, we start the student-net with random weights, then use SGD to initialize the student-net using few samples before adopting our FSKD. \n\nSecond, Q is required to be squared in condition-4 (c4) due to two reasons. \n(1) Q must be squared to make current layer and next layer connectable. \n(2) If Q is not squared, it will decrease the compression effect after absorbing Q into previous layer. \nLet's give an example to explain that. \nSuppose current conv-layer is 64*64*k^2 (64 channels in and 64 out, k^2 is the spatial kernel size), and next layer is 64*128*k^2 (64 channels in and 128 out). \nIf we set Q being 64*128*1^2, after absorbing, the current layer will be 64*128*k^2, which can't connect to next layer (with size 64*128*k^2).\nBesides, it also increases the parameter number and computing cost quite a lot for current layer. \n\nThird, we list the 3 cases how Q is defined. \n(1) For the fully redesigned student network (zero net), we ensure that the corresponding block-level output channels are matched between teacher and student.  If the block output channel number is n, then Q is a matrix with size n*n. \n(2) For the network decoupling case, the regular convolution and depthwise separable block has the same number of output channels, it is also straightforward to define the size of Q. \n(3) For the  pruning and slimming cases, there are two different sub-cases. \n3a) When there are multi-layers within an alignment block, the student-net may either have less layers or smaller intermediate channels in the block comparing to the teacher-net, but still keep both teacher and student have the same number of output channels for that corresponding block. For instance, the teacher-net has a block with 2 conv-layers (64*64*k^2 followed by 64*128*k^2), the student-net may just have less conv-layers (1 here) in the block as (64*128*k^2). Or the student-net may have smaller number of internal channels in the block as (64*32*k^2, 32*128*k^2), here 32 is the number of  output channels for the first layer and number of input channel for the second layer.  For both examples, Q should be 128*128, so that it could be absorbed into previous conv-layer. \n\n3b) When we do per-layer alignment, suppose the layer of the teacher-net is 64*128*k^2. After pruning, we have the corresponding layer in student-net as 64*64*k^2. The #channels between teacher and student are different here. We split the 128 channels of teacher-net into two parts, pruned 64 channels and unpruned 64 channels. We make a linear regression between the student-net and the unpruned 64 channels of teacher-net, so that we defined the matrix Q with size 64*64.  For the first layer, the alignment matrix Q is an identical matrix since the unpruned part of teacher-net is copied to the student-net. However, when moving to next layer, for teacher-net, the input information will come from all the 128 channels of previous conv-layer. But for the student-net, the input information will come from only the 64 channels of previous conv-layer.  There is obvious information loss, so that we estimate Q to alleviate this information loss. \n    \nWe will clarify this in our revision with text explanation and figure illustrations. "}, "signatures": ["ICLR.cc/2019/Conference/Paper808/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper808/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper808/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Knowledge Distillation from Few Samples", "abstract": "Current knowledge distillation methods require full training data to distill knowledge from a large \"teacher\" network to a compact \"student\" network by matching certain statistics between \"teacher\" and \"student\" such as softmax outputs and feature responses. This is not only time-consuming but also inconsistent with human cognition in which children can learn knowledge from adults with few examples. This paper proposes a novel and simple method for knowledge distillation from few samples. Taking the assumption that both \"teacher\" and \"student\" have the same  feature map sizes at each corresponding block, we add a $1\\times 1$ conv-layer at the end of each block in the student-net, and align the block-level outputs between \"teacher\" and \"student\" by estimating the parameters of the added layer with limited samples. We prove that the added layer can be absorbed/merged into the previous conv-layer \\hl{to formulate a new conv-layer with the same size of parameters and computation cost as previous one. Experiments verifies that the proposed method is very efficient and effective to distill knowledge from teacher-net to student-net constructing in different ways on various datasets.", "keywords": ["knowledge distillation", "few-sample learning", "network compression"], "authorids": ["tianhong@mit.edu", "jianguo.li@intel.com", "zhuangl@berkeley.edu", "zcs@mail.tsinghua.edu.cn"], "authors": ["Tianhong Li", "Jianguo Li", "Zhuang Liu", "Changshui Zhang"], "TL;DR": "This paper proposes a novel and simple method for knowledge distillation from few samples.", "pdf": "/pdf/4effd15c3c2f9b1359e748bbbb9d649f116fe4a5.pdf", "paperhash": "li|knowledge_distillation_from_few_samples", "_bibtex": "@misc{\nli2019knowledge,\ntitle={Knowledge Distillation from Few Samples},\nauthor={Tianhong Li and Jianguo Li and Zhuang Liu and Changshui Zhang},\nyear={2019},\nurl={https://openreview.net/forum?id=HkgDTiCctQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper808/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621604760, "tddate": null, "super": null, "final": null, "reply": {"forum": "HkgDTiCctQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper808/Authors", "ICLR.cc/2019/Conference/Paper808/Reviewers", "ICLR.cc/2019/Conference/Paper808/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper808/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper808/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper808/Authors|ICLR.cc/2019/Conference/Paper808/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper808/Reviewers", "ICLR.cc/2019/Conference/Paper808/Authors", "ICLR.cc/2019/Conference/Paper808/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621604760}}}, {"id": "H1x7gjxLnm", "original": null, "number": 3, "cdate": 1540913898977, "ddate": null, "tcdate": 1540913898977, "tmdate": 1540913898977, "tddate": null, "forum": "HkgDTiCctQ", "replyto": "HkgDTiCctQ", "invitation": "ICLR.cc/2019/Conference/-/Paper808/Official_Comment", "content": {"title": "Clarification on non-square Q", "comment": "I'm a bit confused by the fact that in sec 3.3, Q is said to be square (satisfy c4). Why is this always satisfied here, because I thought the whole point was that the student might have fewer channels than the teacher. I guess there are a couple of different situations to consider (such as reducing channels, or reducing layers). In the former, the way I understood the training was as following: We start with after the first layer (let's say teacher output has 128 channels and student 64). We construct a Q with shape (64, 128, 1, 1) and and assume the first layer in the student is the same as the teacher. We solve it and absorb Q into this weight for the student. Note, at this point if the student also had 128 channels output, there would be no need to do solve for a Q. Next, we look at the activations of the teacher after the second layer (let's say it's still 128). This is where we run into the first issue I'm not sure how to address, since the weights of the teacher layer is no longer compatible with the student, so we cannot use it anymore. We would have to first absorb the inverse of the previous Q into this weight, to get us back to 128 channels going into that layers. I guess that's when you don't copy weights from the teacher and instead initialize randomly (zero student).\n\nAnyway, a bit more clarity on when you can re-use original teacher weights and when you have to randomly initialize - as well as when Q is square and when Q is non-square. Thanks!"}, "signatures": ["ICLR.cc/2019/Conference/Paper808/AnonReviewer2"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper808/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper808/AnonReviewer2", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Knowledge Distillation from Few Samples", "abstract": "Current knowledge distillation methods require full training data to distill knowledge from a large \"teacher\" network to a compact \"student\" network by matching certain statistics between \"teacher\" and \"student\" such as softmax outputs and feature responses. This is not only time-consuming but also inconsistent with human cognition in which children can learn knowledge from adults with few examples. This paper proposes a novel and simple method for knowledge distillation from few samples. Taking the assumption that both \"teacher\" and \"student\" have the same  feature map sizes at each corresponding block, we add a $1\\times 1$ conv-layer at the end of each block in the student-net, and align the block-level outputs between \"teacher\" and \"student\" by estimating the parameters of the added layer with limited samples. We prove that the added layer can be absorbed/merged into the previous conv-layer \\hl{to formulate a new conv-layer with the same size of parameters and computation cost as previous one. Experiments verifies that the proposed method is very efficient and effective to distill knowledge from teacher-net to student-net constructing in different ways on various datasets.", "keywords": ["knowledge distillation", "few-sample learning", "network compression"], "authorids": ["tianhong@mit.edu", "jianguo.li@intel.com", "zhuangl@berkeley.edu", "zcs@mail.tsinghua.edu.cn"], "authors": ["Tianhong Li", "Jianguo Li", "Zhuang Liu", "Changshui Zhang"], "TL;DR": "This paper proposes a novel and simple method for knowledge distillation from few samples.", "pdf": "/pdf/4effd15c3c2f9b1359e748bbbb9d649f116fe4a5.pdf", "paperhash": "li|knowledge_distillation_from_few_samples", "_bibtex": "@misc{\nli2019knowledge,\ntitle={Knowledge Distillation from Few Samples},\nauthor={Tianhong Li and Jianguo Li and Zhuang Liu and Changshui Zhang},\nyear={2019},\nurl={https://openreview.net/forum?id=HkgDTiCctQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper808/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621604760, "tddate": null, "super": null, "final": null, "reply": {"forum": "HkgDTiCctQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper808/Authors", "ICLR.cc/2019/Conference/Paper808/Reviewers", "ICLR.cc/2019/Conference/Paper808/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper808/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper808/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper808/Authors|ICLR.cc/2019/Conference/Paper808/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper808/Reviewers", "ICLR.cc/2019/Conference/Paper808/Authors", "ICLR.cc/2019/Conference/Paper808/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621604760}}}, {"id": "BJg7ZjdQh7", "original": null, "number": 2, "cdate": 1540750074649, "ddate": null, "tcdate": 1540750074649, "tmdate": 1540778570998, "tddate": null, "forum": "HkgDTiCctQ", "replyto": "SkgvhosGhQ", "invitation": "ICLR.cc/2019/Conference/-/Paper808/Official_Comment", "content": {"title": "Clarification on Q", "comment": "Thanks for your comment!\n\nWe use Q to denote the parameters of the added 1x1 conv-layer, the size is nxmx1x1, where n is the input channel number, m is the output channel number, 1x1 is the kernel size. The 4D tensor is then degraded to a matrix with size nxm. Q acts as a linear combination of the input and output channels. For more information about the 1x1 convolution, please refer to [1]. \nTo be more specific, suppose Q_{ij} is the element of the matrix  Q at i-th row and j-th column, it reflects the combination coefficient between input channel i and output channel j.  This is how we represent Q as a matrix.\n\n[1] Min Lin, Qiang Chen, and Shuicheng Yan. \"Network in network.\" arXiv preprint arXiv:1312.4400 (2013)."}, "signatures": ["ICLR.cc/2019/Conference/Paper808/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper808/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper808/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Knowledge Distillation from Few Samples", "abstract": "Current knowledge distillation methods require full training data to distill knowledge from a large \"teacher\" network to a compact \"student\" network by matching certain statistics between \"teacher\" and \"student\" such as softmax outputs and feature responses. This is not only time-consuming but also inconsistent with human cognition in which children can learn knowledge from adults with few examples. This paper proposes a novel and simple method for knowledge distillation from few samples. Taking the assumption that both \"teacher\" and \"student\" have the same  feature map sizes at each corresponding block, we add a $1\\times 1$ conv-layer at the end of each block in the student-net, and align the block-level outputs between \"teacher\" and \"student\" by estimating the parameters of the added layer with limited samples. We prove that the added layer can be absorbed/merged into the previous conv-layer \\hl{to formulate a new conv-layer with the same size of parameters and computation cost as previous one. Experiments verifies that the proposed method is very efficient and effective to distill knowledge from teacher-net to student-net constructing in different ways on various datasets.", "keywords": ["knowledge distillation", "few-sample learning", "network compression"], "authorids": ["tianhong@mit.edu", "jianguo.li@intel.com", "zhuangl@berkeley.edu", "zcs@mail.tsinghua.edu.cn"], "authors": ["Tianhong Li", "Jianguo Li", "Zhuang Liu", "Changshui Zhang"], "TL;DR": "This paper proposes a novel and simple method for knowledge distillation from few samples.", "pdf": "/pdf/4effd15c3c2f9b1359e748bbbb9d649f116fe4a5.pdf", "paperhash": "li|knowledge_distillation_from_few_samples", "_bibtex": "@misc{\nli2019knowledge,\ntitle={Knowledge Distillation from Few Samples},\nauthor={Tianhong Li and Jianguo Li and Zhuang Liu and Changshui Zhang},\nyear={2019},\nurl={https://openreview.net/forum?id=HkgDTiCctQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper808/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621604760, "tddate": null, "super": null, "final": null, "reply": {"forum": "HkgDTiCctQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper808/Authors", "ICLR.cc/2019/Conference/Paper808/Reviewers", "ICLR.cc/2019/Conference/Paper808/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper808/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper808/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper808/Authors|ICLR.cc/2019/Conference/Paper808/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper808/Reviewers", "ICLR.cc/2019/Conference/Paper808/Authors", "ICLR.cc/2019/Conference/Paper808/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621604760}}}, {"id": "HJgOAKO7nQ", "original": null, "number": 1, "cdate": 1540749776501, "ddate": null, "tcdate": 1540749776501, "tmdate": 1540778115454, "tddate": null, "forum": "HkgDTiCctQ", "replyto": "rJlpXbFb27", "invitation": "ICLR.cc/2019/Conference/-/Paper808/Official_Comment", "content": {"title": "Clarification on the optimization process", "comment": "Thanks for your comment! \n\nYes, our problem can be optimized using SGD with loss function defined in Eq(2) . However, we did not use it to report results in the paper. Instead, we estimate the 1x1 conv-layer parameter Q by solving the least squared problem layer by layer sequentially. \n\nTo be more specific, given randomly selected few-samples, we forward the data to the alignment point of the first block in both student network and teacher network, and obtain the feature map responses at this point. Suppose the teacher network response is X^t, and the student-net response is X^s, we obtain Q using X^s and X^t with Eq(1). Then based on our Theorem-1, we absorb the 1x1 conv defined by Q into previous conv-layer. After that, we move to the alignment point of the next block, and repeat this procedure until we reach to the final alignment point. This simple solution works well since the alignment point is before non-linear activation function. Figure-3 shows the block-level correlation before and after alignment between teacher and student, which also demonstrate this effectiveness of this linear approximation. \n\nWe use this procedure instead of the SGD based optimization due to the following reasons. \n\n(1)  We in fact implement the SGD based solution on the filter-pruning and slimming experiments. But we did not find noticeable results difference between these two solutions. \n\n(2)  SGD requires tuning several hyper parameters, while our simple solution is hyper-parameter free. We find it is relatively difficult to tune SGD based solution on the network decoupling case due to multi-branch network structures. There are no advantages on time budget over the proposed simple solution.\n\n(3)  Our experiments demonstrates the proposed simple solution works pretty well for the cases, and we also have some figures which illustrates steady accuracy improvement during block-by-block alignment. And we will include that in the revision. \n\nWe will also make our source code available in the near future."}, "signatures": ["ICLR.cc/2019/Conference/Paper808/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper808/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper808/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Knowledge Distillation from Few Samples", "abstract": "Current knowledge distillation methods require full training data to distill knowledge from a large \"teacher\" network to a compact \"student\" network by matching certain statistics between \"teacher\" and \"student\" such as softmax outputs and feature responses. This is not only time-consuming but also inconsistent with human cognition in which children can learn knowledge from adults with few examples. This paper proposes a novel and simple method for knowledge distillation from few samples. Taking the assumption that both \"teacher\" and \"student\" have the same  feature map sizes at each corresponding block, we add a $1\\times 1$ conv-layer at the end of each block in the student-net, and align the block-level outputs between \"teacher\" and \"student\" by estimating the parameters of the added layer with limited samples. We prove that the added layer can be absorbed/merged into the previous conv-layer \\hl{to formulate a new conv-layer with the same size of parameters and computation cost as previous one. Experiments verifies that the proposed method is very efficient and effective to distill knowledge from teacher-net to student-net constructing in different ways on various datasets.", "keywords": ["knowledge distillation", "few-sample learning", "network compression"], "authorids": ["tianhong@mit.edu", "jianguo.li@intel.com", "zhuangl@berkeley.edu", "zcs@mail.tsinghua.edu.cn"], "authors": ["Tianhong Li", "Jianguo Li", "Zhuang Liu", "Changshui Zhang"], "TL;DR": "This paper proposes a novel and simple method for knowledge distillation from few samples.", "pdf": "/pdf/4effd15c3c2f9b1359e748bbbb9d649f116fe4a5.pdf", "paperhash": "li|knowledge_distillation_from_few_samples", "_bibtex": "@misc{\nli2019knowledge,\ntitle={Knowledge Distillation from Few Samples},\nauthor={Tianhong Li and Jianguo Li and Zhuang Liu and Changshui Zhang},\nyear={2019},\nurl={https://openreview.net/forum?id=HkgDTiCctQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper808/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621604760, "tddate": null, "super": null, "final": null, "reply": {"forum": "HkgDTiCctQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper808/Authors", "ICLR.cc/2019/Conference/Paper808/Reviewers", "ICLR.cc/2019/Conference/Paper808/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper808/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper808/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper808/Authors|ICLR.cc/2019/Conference/Paper808/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper808/Reviewers", "ICLR.cc/2019/Conference/Paper808/Authors", "ICLR.cc/2019/Conference/Paper808/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621604760}}}, {"id": "SkgvhosGhQ", "original": null, "number": 2, "cdate": 1540697007281, "ddate": null, "tcdate": 1540697007281, "tmdate": 1540697007281, "tddate": null, "forum": "HkgDTiCctQ", "replyto": "HkgDTiCctQ", "invitation": "ICLR.cc/2019/Conference/-/Paper808/Public_Comment", "content": {"comment": "I don't know \"Q is degarded to the matrix form\". Could you tell what is the specific operation here? Any other references?\n\nThanks\uff01", "title": "About \"Q is degarded to the matrix form\""}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper808/Reviewers/Unsubmitted"], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Knowledge Distillation from Few Samples", "abstract": "Current knowledge distillation methods require full training data to distill knowledge from a large \"teacher\" network to a compact \"student\" network by matching certain statistics between \"teacher\" and \"student\" such as softmax outputs and feature responses. This is not only time-consuming but also inconsistent with human cognition in which children can learn knowledge from adults with few examples. This paper proposes a novel and simple method for knowledge distillation from few samples. Taking the assumption that both \"teacher\" and \"student\" have the same  feature map sizes at each corresponding block, we add a $1\\times 1$ conv-layer at the end of each block in the student-net, and align the block-level outputs between \"teacher\" and \"student\" by estimating the parameters of the added layer with limited samples. We prove that the added layer can be absorbed/merged into the previous conv-layer \\hl{to formulate a new conv-layer with the same size of parameters and computation cost as previous one. Experiments verifies that the proposed method is very efficient and effective to distill knowledge from teacher-net to student-net constructing in different ways on various datasets.", "keywords": ["knowledge distillation", "few-sample learning", "network compression"], "authorids": ["tianhong@mit.edu", "jianguo.li@intel.com", "zhuangl@berkeley.edu", "zcs@mail.tsinghua.edu.cn"], "authors": ["Tianhong Li", "Jianguo Li", "Zhuang Liu", "Changshui Zhang"], "TL;DR": "This paper proposes a novel and simple method for knowledge distillation from few samples.", "pdf": "/pdf/4effd15c3c2f9b1359e748bbbb9d649f116fe4a5.pdf", "paperhash": "li|knowledge_distillation_from_few_samples", "_bibtex": "@misc{\nli2019knowledge,\ntitle={Knowledge Distillation from Few Samples},\nauthor={Tianhong Li and Jianguo Li and Zhuang Liu and Changshui Zhang},\nyear={2019},\nurl={https://openreview.net/forum?id=HkgDTiCctQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper808/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311748052, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "HkgDTiCctQ", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper808/Authors", "ICLR.cc/2019/Conference/Paper808/Reviewers", "ICLR.cc/2019/Conference/Paper808/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper808/Authors", "ICLR.cc/2019/Conference/Paper808/Reviewers", "ICLR.cc/2019/Conference/Paper808/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311748052}}}, {"id": "rJlpXbFb27", "original": null, "number": 1, "cdate": 1540620580898, "ddate": null, "tcdate": 1540620580898, "tmdate": 1540620601550, "tddate": null, "forum": "HkgDTiCctQ", "replyto": "HkgDTiCctQ", "invitation": "ICLR.cc/2019/Conference/-/Paper808/Public_Comment", "content": {"comment": "I'm trying to reproduce your results in Sec. 4, but had a question about the the optimize of loss in Sec. 3.3 Algorithm 1: \n1. not use SGD optimize the loss, instead by Algorithm 1, but how does it work well, I can't understand here. What more can you describe?.\n\n\n\nThanks!", "title": "Question about the optimize of loss"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper808/Reviewers/Unsubmitted"], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Knowledge Distillation from Few Samples", "abstract": "Current knowledge distillation methods require full training data to distill knowledge from a large \"teacher\" network to a compact \"student\" network by matching certain statistics between \"teacher\" and \"student\" such as softmax outputs and feature responses. This is not only time-consuming but also inconsistent with human cognition in which children can learn knowledge from adults with few examples. This paper proposes a novel and simple method for knowledge distillation from few samples. Taking the assumption that both \"teacher\" and \"student\" have the same  feature map sizes at each corresponding block, we add a $1\\times 1$ conv-layer at the end of each block in the student-net, and align the block-level outputs between \"teacher\" and \"student\" by estimating the parameters of the added layer with limited samples. We prove that the added layer can be absorbed/merged into the previous conv-layer \\hl{to formulate a new conv-layer with the same size of parameters and computation cost as previous one. Experiments verifies that the proposed method is very efficient and effective to distill knowledge from teacher-net to student-net constructing in different ways on various datasets.", "keywords": ["knowledge distillation", "few-sample learning", "network compression"], "authorids": ["tianhong@mit.edu", "jianguo.li@intel.com", "zhuangl@berkeley.edu", "zcs@mail.tsinghua.edu.cn"], "authors": ["Tianhong Li", "Jianguo Li", "Zhuang Liu", "Changshui Zhang"], "TL;DR": "This paper proposes a novel and simple method for knowledge distillation from few samples.", "pdf": "/pdf/4effd15c3c2f9b1359e748bbbb9d649f116fe4a5.pdf", "paperhash": "li|knowledge_distillation_from_few_samples", "_bibtex": "@misc{\nli2019knowledge,\ntitle={Knowledge Distillation from Few Samples},\nauthor={Tianhong Li and Jianguo Li and Zhuang Liu and Changshui Zhang},\nyear={2019},\nurl={https://openreview.net/forum?id=HkgDTiCctQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper808/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311748052, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "HkgDTiCctQ", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper808/Authors", "ICLR.cc/2019/Conference/Paper808/Reviewers", "ICLR.cc/2019/Conference/Paper808/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper808/Authors", "ICLR.cc/2019/Conference/Paper808/Reviewers", "ICLR.cc/2019/Conference/Paper808/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311748052}}}], "count": 19}