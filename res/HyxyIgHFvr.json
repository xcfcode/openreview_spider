{"notes": [{"id": "HyxyIgHFvr", "original": "B1xRHYxtwH", "number": 2307, "cdate": 1569439814905, "ddate": null, "tcdate": 1569439814905, "tmdate": 1588010876192, "tddate": null, "forum": "HyxyIgHFvr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["goldblumcello@gmail.com", "jonas.geiping@uni-siegen.de", "avi1@umd.edu", "michael.moeller@uni-siegen.de", "tomg@cs.umd.edu"], "title": "Truth or backpropaganda? An empirical investigation of deep learning theory", "authors": ["Micah Goldblum", "Jonas Geiping", "Avi Schwarzschild", "Michael Moeller", "Tom Goldstein"], "pdf": "/pdf/70c2d968c8b055683fe661442a25866fef5784de.pdf", "abstract": "We empirically evaluate common assumptions about neural networks that are widely held by practitioners and theorists alike.  In this work, we: (1) prove the widespread existence of suboptimal local minima in the loss landscape of neural networks, and we use our theory to find examples; (2) show that small-norm parameters are not optimal for generalization; (3) demonstrate that ResNets do not conform to wide-network theories, such as the neural tangent kernel, and that the interaction between skip connections and batch normalization plays a role;  (4) find that rank does not correlate with generalization or robustness in a practical setting.", "keywords": ["Deep learning", "generalization", "loss landscape", "robustness"], "paperhash": "goldblum|truth_or_backpropaganda_an_empirical_investigation_of_deep_learning_theory", "TL;DR": "We call into question commonly held beliefs regarding the loss landscape, optimization, network width, and rank.", "code": "https://github.com/goldblum/TruthOrBackpropaganda", "_bibtex": "@inproceedings{\nGoldblum2020Truth,\ntitle={Truth or backpropaganda? An empirical investigation of deep learning theory},\nauthor={Micah Goldblum and Jonas Geiping and Avi Schwarzschild and Michael Moeller and Tom Goldstein},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HyxyIgHFvr}\n}", "original_pdf": "/attachment/1c633809ad1e82b04900a0bec0d6d1268a0b2ff3.pdf"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 15, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "Yvguv1SCw", "original": null, "number": 1, "cdate": 1576798745779, "ddate": null, "tcdate": 1576798745779, "tmdate": 1576800890346, "tddate": null, "forum": "HyxyIgHFvr", "replyto": "HyxyIgHFvr", "invitation": "ICLR.cc/2020/Conference/Paper2307/-/Decision", "content": {"decision": "Accept (Spotlight)", "comment": "The authors take a closer look at widely held beliefs about neural networks. Using a mix of analysis and experiment, they shed some light on the ways these assumptions break down. The paper contributes to our understanding of various phenomena and their connection to generalization, and should be a useful paper for theoreticians searching for predictive theories.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["goldblumcello@gmail.com", "jonas.geiping@uni-siegen.de", "avi1@umd.edu", "michael.moeller@uni-siegen.de", "tomg@cs.umd.edu"], "title": "Truth or backpropaganda? An empirical investigation of deep learning theory", "authors": ["Micah Goldblum", "Jonas Geiping", "Avi Schwarzschild", "Michael Moeller", "Tom Goldstein"], "pdf": "/pdf/70c2d968c8b055683fe661442a25866fef5784de.pdf", "abstract": "We empirically evaluate common assumptions about neural networks that are widely held by practitioners and theorists alike.  In this work, we: (1) prove the widespread existence of suboptimal local minima in the loss landscape of neural networks, and we use our theory to find examples; (2) show that small-norm parameters are not optimal for generalization; (3) demonstrate that ResNets do not conform to wide-network theories, such as the neural tangent kernel, and that the interaction between skip connections and batch normalization plays a role;  (4) find that rank does not correlate with generalization or robustness in a practical setting.", "keywords": ["Deep learning", "generalization", "loss landscape", "robustness"], "paperhash": "goldblum|truth_or_backpropaganda_an_empirical_investigation_of_deep_learning_theory", "TL;DR": "We call into question commonly held beliefs regarding the loss landscape, optimization, network width, and rank.", "code": "https://github.com/goldblum/TruthOrBackpropaganda", "_bibtex": "@inproceedings{\nGoldblum2020Truth,\ntitle={Truth or backpropaganda? An empirical investigation of deep learning theory},\nauthor={Micah Goldblum and Jonas Geiping and Avi Schwarzschild and Michael Moeller and Tom Goldstein},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HyxyIgHFvr}\n}", "original_pdf": "/attachment/1c633809ad1e82b04900a0bec0d6d1268a0b2ff3.pdf"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "HyxyIgHFvr", "replyto": "HyxyIgHFvr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795713325, "tmdate": 1576800262914, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2307/-/Decision"}}}, {"id": "SJgN-L5mcH", "original": null, "number": 3, "cdate": 1572214267686, "ddate": null, "tcdate": 1572214267686, "tmdate": 1574507610995, "tddate": null, "forum": "HyxyIgHFvr", "replyto": "HyxyIgHFvr", "invitation": "ICLR.cc/2020/Conference/Paper2307/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "title": "Official Blind Review #2", "review": "In this paper, the authors seek to examine carefully some assumptions investigated in the theory of deep neural networks. The paper attempts to answer the following theoretical assumptions: the existence of local minima in loss landscapes, the relevance of weight decay with small L2-norm solutions, the connection between deep neural networks to kernel-based learning theory, and the generalization ability of networks with low-rank layers.\n\nWe think that this work is timely and of significant interest, since theoretical work on deep learning has made significant progress in recent years.\n\nSince this paper seeks to provide an empirical study on the assumptions in deep learning theory, we think that the results are somehow weak as the paper is missing extensive analysis, using several well-known datasets and several deep architectures and settings. For example, only the CIFAR-10 dataset is considered in the paper, and it is not clear whether the obtained results will generalize to other datasets. This also goes to the neural network architecture, as only MLP is considered to answer the assumption about the existence of suboptimal minima, while only ResNet is considered to study the generalization abilities with low-rank layers. We think that this is not enough for a paper that tries to provide an empirical study.\n\n------- \nReply to rebuttal\n\nWe thank the authors for taking into consideration our previous comments and suggestions, including going beyond MLP and adding experiments on other datasets. For this reason, we have increased the rating from \"Weak Accept\" to \"Accept\".", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper2307/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2307/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["goldblumcello@gmail.com", "jonas.geiping@uni-siegen.de", "avi1@umd.edu", "michael.moeller@uni-siegen.de", "tomg@cs.umd.edu"], "title": "Truth or backpropaganda? An empirical investigation of deep learning theory", "authors": ["Micah Goldblum", "Jonas Geiping", "Avi Schwarzschild", "Michael Moeller", "Tom Goldstein"], "pdf": "/pdf/70c2d968c8b055683fe661442a25866fef5784de.pdf", "abstract": "We empirically evaluate common assumptions about neural networks that are widely held by practitioners and theorists alike.  In this work, we: (1) prove the widespread existence of suboptimal local minima in the loss landscape of neural networks, and we use our theory to find examples; (2) show that small-norm parameters are not optimal for generalization; (3) demonstrate that ResNets do not conform to wide-network theories, such as the neural tangent kernel, and that the interaction between skip connections and batch normalization plays a role;  (4) find that rank does not correlate with generalization or robustness in a practical setting.", "keywords": ["Deep learning", "generalization", "loss landscape", "robustness"], "paperhash": "goldblum|truth_or_backpropaganda_an_empirical_investigation_of_deep_learning_theory", "TL;DR": "We call into question commonly held beliefs regarding the loss landscape, optimization, network width, and rank.", "code": "https://github.com/goldblum/TruthOrBackpropaganda", "_bibtex": "@inproceedings{\nGoldblum2020Truth,\ntitle={Truth or backpropaganda? An empirical investigation of deep learning theory},\nauthor={Micah Goldblum and Jonas Geiping and Avi Schwarzschild and Michael Moeller and Tom Goldstein},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HyxyIgHFvr}\n}", "original_pdf": "/attachment/1c633809ad1e82b04900a0bec0d6d1268a0b2ff3.pdf"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HyxyIgHFvr", "replyto": "HyxyIgHFvr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2307/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2307/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575988608887, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2307/Reviewers"], "noninvitees": [], "tcdate": 1570237724696, "tmdate": 1575988608899, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2307/-/Official_Review"}}}, {"id": "HJxg3bwisS", "original": null, "number": 9, "cdate": 1573773736306, "ddate": null, "tcdate": 1573773736306, "tmdate": 1573773736306, "tddate": null, "forum": "HyxyIgHFvr", "replyto": "HJeQuWDTtr", "invitation": "ICLR.cc/2020/Conference/Paper2307/-/Official_Comment", "content": {"title": "Reply to reviewer #3", "comment": "We appreciate the positive feedback, and we thank the reviewer for the thoughtful comments. We have added results from further suboptimal minima experiments to the appendix."}, "signatures": ["ICLR.cc/2020/Conference/Paper2307/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2307/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["goldblumcello@gmail.com", "jonas.geiping@uni-siegen.de", "avi1@umd.edu", "michael.moeller@uni-siegen.de", "tomg@cs.umd.edu"], "title": "Truth or backpropaganda? An empirical investigation of deep learning theory", "authors": ["Micah Goldblum", "Jonas Geiping", "Avi Schwarzschild", "Michael Moeller", "Tom Goldstein"], "pdf": "/pdf/70c2d968c8b055683fe661442a25866fef5784de.pdf", "abstract": "We empirically evaluate common assumptions about neural networks that are widely held by practitioners and theorists alike.  In this work, we: (1) prove the widespread existence of suboptimal local minima in the loss landscape of neural networks, and we use our theory to find examples; (2) show that small-norm parameters are not optimal for generalization; (3) demonstrate that ResNets do not conform to wide-network theories, such as the neural tangent kernel, and that the interaction between skip connections and batch normalization plays a role;  (4) find that rank does not correlate with generalization or robustness in a practical setting.", "keywords": ["Deep learning", "generalization", "loss landscape", "robustness"], "paperhash": "goldblum|truth_or_backpropaganda_an_empirical_investigation_of_deep_learning_theory", "TL;DR": "We call into question commonly held beliefs regarding the loss landscape, optimization, network width, and rank.", "code": "https://github.com/goldblum/TruthOrBackpropaganda", "_bibtex": "@inproceedings{\nGoldblum2020Truth,\ntitle={Truth or backpropaganda? An empirical investigation of deep learning theory},\nauthor={Micah Goldblum and Jonas Geiping and Avi Schwarzschild and Michael Moeller and Tom Goldstein},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HyxyIgHFvr}\n}", "original_pdf": "/attachment/1c633809ad1e82b04900a0bec0d6d1268a0b2ff3.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HyxyIgHFvr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2307/Authors", "ICLR.cc/2020/Conference/Paper2307/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2307/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2307/Reviewers", "ICLR.cc/2020/Conference/Paper2307/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2307/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2307/Authors|ICLR.cc/2020/Conference/Paper2307/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504143296, "tmdate": 1576860530551, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2307/Authors", "ICLR.cc/2020/Conference/Paper2307/Reviewers", "ICLR.cc/2020/Conference/Paper2307/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2307/-/Official_Comment"}}}, {"id": "S1l7D-voiB", "original": null, "number": 8, "cdate": 1573773658625, "ddate": null, "tcdate": 1573773658625, "tmdate": 1573773658625, "tddate": null, "forum": "HyxyIgHFvr", "replyto": "SJgN-L5mcH", "invitation": "ICLR.cc/2020/Conference/Paper2307/-/Official_Comment", "content": {"title": "Reply to reviewer #2", "comment": "We thank the reviewer for the time and effort spent on our paper. We agree about the note concerning the breadth of our experiments and have made the following additions to the paper.\n* Experiments have been run on CIFAR-100 data and the results, which agree with our previous findings, are in the appendix.\n* Our study of suboptimal minima had included experiments with ResNet-18. Results are in the appendix. As mentioned above, we have since added these experiments on CIFAR-100 for diversity of data sets.\n* The section on rank has been updated to reflect further experiments with new architectures. Specifically, we tested ResNet-18 without skip connections and MLP. See the updated appendix for full details and results."}, "signatures": ["ICLR.cc/2020/Conference/Paper2307/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2307/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["goldblumcello@gmail.com", "jonas.geiping@uni-siegen.de", "avi1@umd.edu", "michael.moeller@uni-siegen.de", "tomg@cs.umd.edu"], "title": "Truth or backpropaganda? An empirical investigation of deep learning theory", "authors": ["Micah Goldblum", "Jonas Geiping", "Avi Schwarzschild", "Michael Moeller", "Tom Goldstein"], "pdf": "/pdf/70c2d968c8b055683fe661442a25866fef5784de.pdf", "abstract": "We empirically evaluate common assumptions about neural networks that are widely held by practitioners and theorists alike.  In this work, we: (1) prove the widespread existence of suboptimal local minima in the loss landscape of neural networks, and we use our theory to find examples; (2) show that small-norm parameters are not optimal for generalization; (3) demonstrate that ResNets do not conform to wide-network theories, such as the neural tangent kernel, and that the interaction between skip connections and batch normalization plays a role;  (4) find that rank does not correlate with generalization or robustness in a practical setting.", "keywords": ["Deep learning", "generalization", "loss landscape", "robustness"], "paperhash": "goldblum|truth_or_backpropaganda_an_empirical_investigation_of_deep_learning_theory", "TL;DR": "We call into question commonly held beliefs regarding the loss landscape, optimization, network width, and rank.", "code": "https://github.com/goldblum/TruthOrBackpropaganda", "_bibtex": "@inproceedings{\nGoldblum2020Truth,\ntitle={Truth or backpropaganda? An empirical investigation of deep learning theory},\nauthor={Micah Goldblum and Jonas Geiping and Avi Schwarzschild and Michael Moeller and Tom Goldstein},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HyxyIgHFvr}\n}", "original_pdf": "/attachment/1c633809ad1e82b04900a0bec0d6d1268a0b2ff3.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HyxyIgHFvr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2307/Authors", "ICLR.cc/2020/Conference/Paper2307/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2307/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2307/Reviewers", "ICLR.cc/2020/Conference/Paper2307/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2307/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2307/Authors|ICLR.cc/2020/Conference/Paper2307/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504143296, "tmdate": 1576860530551, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2307/Authors", "ICLR.cc/2020/Conference/Paper2307/Reviewers", "ICLR.cc/2020/Conference/Paper2307/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2307/-/Official_Comment"}}}, {"id": "ryxZZWwsiS", "original": null, "number": 7, "cdate": 1573773560835, "ddate": null, "tcdate": 1573773560835, "tmdate": 1573773560835, "tddate": null, "forum": "HyxyIgHFvr", "replyto": "H1x75kATFr", "invitation": "ICLR.cc/2020/Conference/Paper2307/-/Official_Comment", "content": {"title": "Reply to reviewer #1", "comment": "Thank you for your thoughtful input on our work. We address your comments in order:\n* Our work here is focused on finding suboptimal minima, and we show that certain poor initializations motivated by theory can lead to this.  We agree that suboptimal local minima which arise from bad initializations in standard practice would be interesting to study in future work.\n* We have changed the conclusion of the NTK section to more clearly discuss and conceptualize our findings, and we have added additional plots.\n* The order of the topics has been fixed, thank you for bringing this to our attention.\n* We have added details regarding the confidence intervals.\n* The constant \\mu is chosen heuristically by studying the norm of parameter vectors that result from standard weight decay, and setting \\mu to be higher to make sure that networks trained with norm-bias indeed have a higher norm than those trained with weight decay. This explanation is now included in the section on weight norms. "}, "signatures": ["ICLR.cc/2020/Conference/Paper2307/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2307/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["goldblumcello@gmail.com", "jonas.geiping@uni-siegen.de", "avi1@umd.edu", "michael.moeller@uni-siegen.de", "tomg@cs.umd.edu"], "title": "Truth or backpropaganda? An empirical investigation of deep learning theory", "authors": ["Micah Goldblum", "Jonas Geiping", "Avi Schwarzschild", "Michael Moeller", "Tom Goldstein"], "pdf": "/pdf/70c2d968c8b055683fe661442a25866fef5784de.pdf", "abstract": "We empirically evaluate common assumptions about neural networks that are widely held by practitioners and theorists alike.  In this work, we: (1) prove the widespread existence of suboptimal local minima in the loss landscape of neural networks, and we use our theory to find examples; (2) show that small-norm parameters are not optimal for generalization; (3) demonstrate that ResNets do not conform to wide-network theories, such as the neural tangent kernel, and that the interaction between skip connections and batch normalization plays a role;  (4) find that rank does not correlate with generalization or robustness in a practical setting.", "keywords": ["Deep learning", "generalization", "loss landscape", "robustness"], "paperhash": "goldblum|truth_or_backpropaganda_an_empirical_investigation_of_deep_learning_theory", "TL;DR": "We call into question commonly held beliefs regarding the loss landscape, optimization, network width, and rank.", "code": "https://github.com/goldblum/TruthOrBackpropaganda", "_bibtex": "@inproceedings{\nGoldblum2020Truth,\ntitle={Truth or backpropaganda? An empirical investigation of deep learning theory},\nauthor={Micah Goldblum and Jonas Geiping and Avi Schwarzschild and Michael Moeller and Tom Goldstein},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HyxyIgHFvr}\n}", "original_pdf": "/attachment/1c633809ad1e82b04900a0bec0d6d1268a0b2ff3.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HyxyIgHFvr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2307/Authors", "ICLR.cc/2020/Conference/Paper2307/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2307/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2307/Reviewers", "ICLR.cc/2020/Conference/Paper2307/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2307/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2307/Authors|ICLR.cc/2020/Conference/Paper2307/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504143296, "tmdate": 1576860530551, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2307/Authors", "ICLR.cc/2020/Conference/Paper2307/Reviewers", "ICLR.cc/2020/Conference/Paper2307/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2307/-/Official_Comment"}}}, {"id": "HJeQuWDTtr", "original": null, "number": 1, "cdate": 1571807595188, "ddate": null, "tcdate": 1571807595188, "tmdate": 1572972355922, "tddate": null, "forum": "HyxyIgHFvr", "replyto": "HyxyIgHFvr", "invitation": "ICLR.cc/2020/Conference/Paper2307/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The authors seek to challenge some presumptions about training deep neural networks, such as the robustness of low rank linear layers and the existence of suboptimal local minima. They provide analytical insight as well as a few experiments.\n\nI give this paper an accept. They analytically explore four relevant topics of deep learning, and provide experimental insight. In particular, they provide solid analytical reasoning behind their claims that suboptimal local minima exist and that their lack of prevalence is due to improvements in other aspects of deep networks, such as initialization and optimizers. In addition, they present a norm-bias regularizer generalization that consistently increases accuracy. I am especially pleased with this, as the results are averaged over several runs (a practice that seems to be not so widespread these days). \n\nIf I were to have one thing on my wish list for this paper, it would be the small issue of having some multiple experiment version of the local minima experiments (I understand why it is not all that necessary for the rank and stability experiments).\n\nNevertheless, I think this paper gives useful insight as to the behavior of deep neural networks that can help advance the field on a foundational level."}, "signatures": ["ICLR.cc/2020/Conference/Paper2307/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2307/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["goldblumcello@gmail.com", "jonas.geiping@uni-siegen.de", "avi1@umd.edu", "michael.moeller@uni-siegen.de", "tomg@cs.umd.edu"], "title": "Truth or backpropaganda? An empirical investigation of deep learning theory", "authors": ["Micah Goldblum", "Jonas Geiping", "Avi Schwarzschild", "Michael Moeller", "Tom Goldstein"], "pdf": "/pdf/70c2d968c8b055683fe661442a25866fef5784de.pdf", "abstract": "We empirically evaluate common assumptions about neural networks that are widely held by practitioners and theorists alike.  In this work, we: (1) prove the widespread existence of suboptimal local minima in the loss landscape of neural networks, and we use our theory to find examples; (2) show that small-norm parameters are not optimal for generalization; (3) demonstrate that ResNets do not conform to wide-network theories, such as the neural tangent kernel, and that the interaction between skip connections and batch normalization plays a role;  (4) find that rank does not correlate with generalization or robustness in a practical setting.", "keywords": ["Deep learning", "generalization", "loss landscape", "robustness"], "paperhash": "goldblum|truth_or_backpropaganda_an_empirical_investigation_of_deep_learning_theory", "TL;DR": "We call into question commonly held beliefs regarding the loss landscape, optimization, network width, and rank.", "code": "https://github.com/goldblum/TruthOrBackpropaganda", "_bibtex": "@inproceedings{\nGoldblum2020Truth,\ntitle={Truth or backpropaganda? An empirical investigation of deep learning theory},\nauthor={Micah Goldblum and Jonas Geiping and Avi Schwarzschild and Michael Moeller and Tom Goldstein},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HyxyIgHFvr}\n}", "original_pdf": "/attachment/1c633809ad1e82b04900a0bec0d6d1268a0b2ff3.pdf"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HyxyIgHFvr", "replyto": "HyxyIgHFvr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2307/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2307/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575988608887, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2307/Reviewers"], "noninvitees": [], "tcdate": 1570237724696, "tmdate": 1575988608899, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2307/-/Official_Review"}}}, {"id": "H1x75kATFr", "original": null, "number": 2, "cdate": 1571835787071, "ddate": null, "tcdate": 1571835787071, "tmdate": 1572972355875, "tddate": null, "forum": "HyxyIgHFvr", "replyto": "HyxyIgHFvr", "invitation": "ICLR.cc/2020/Conference/Paper2307/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The authors look at empirical properties of deep neural networks and discuss their connection to past theoretical work on the following issues:\n\n* Local minima: they give an example of setting where bad local minima (far from the global minimum) are obtained. More specifically, they show such minima can be obtained by initializing with large random biases for MLPs with ReLU activation. They also provide a theoretical result that can be used to find a small set of such minima. I believe this is a useful incremental step towards a better understanding of local minima in deep learning, although it is not clear how many practical implications this has. One question that would ideally be answered is: in practical settings, to what degree does bad initialization cause bad performance specifically due to bad minima? (as opposed to, say, slow convergence or bad generalization performance).\n\n* Weight decay: the authors penalize the size of the norm of the weights as it diverges from a constant, as opposed to when it diverges from 0 as is normally done for weight decay. They show that this works as well or better than normal weight decay in a number of settings. This seem to put into question the belief sometimes held that solutions with smaller norms will generalize better.\n\n* Kernel theory: the authors try to reproduce some of the empirical properties predicted in the Neural Tangent Kernel paper (Jacot et al., 2018) in particular by using more realistic architectures. The results, however, do not appear very conclusive. This might be the weakest part of the paper, as it is hard to draw anything conclusive from their empirical results.\n\n* Rank: The authors challenge the common belief that low rank provides better generalization and more robustness towards adversarial attacks. When enforcing a low or high rank weight matrices during training on ResNet-18 trained on CIFAR-10, the two settings have similar performance and are similarly robust to adversarial attacks, showing at least one counter example.\n\nI think overall this is a useful although somewhat incremental paper, that makes progress in the understanding of the behavior of neural networks in practice, and can help guide further theoretical work and the  development of new and improved training techniques and initialization regimes for deep learning.\n\nOther comments/notes:\n* minor: the order of the last 2 sub topics covered (rank and NTK) is flipped in the introduction, compared to the abstract and the order of the chapters\n* in the table confidence intervals are given, it would be nice to have more details on how they are computed, (e.g. +- 1.96 * std error)\n* how is the constant \\mu in the norm-bias chosen?"}, "signatures": ["ICLR.cc/2020/Conference/Paper2307/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2307/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["goldblumcello@gmail.com", "jonas.geiping@uni-siegen.de", "avi1@umd.edu", "michael.moeller@uni-siegen.de", "tomg@cs.umd.edu"], "title": "Truth or backpropaganda? An empirical investigation of deep learning theory", "authors": ["Micah Goldblum", "Jonas Geiping", "Avi Schwarzschild", "Michael Moeller", "Tom Goldstein"], "pdf": "/pdf/70c2d968c8b055683fe661442a25866fef5784de.pdf", "abstract": "We empirically evaluate common assumptions about neural networks that are widely held by practitioners and theorists alike.  In this work, we: (1) prove the widespread existence of suboptimal local minima in the loss landscape of neural networks, and we use our theory to find examples; (2) show that small-norm parameters are not optimal for generalization; (3) demonstrate that ResNets do not conform to wide-network theories, such as the neural tangent kernel, and that the interaction between skip connections and batch normalization plays a role;  (4) find that rank does not correlate with generalization or robustness in a practical setting.", "keywords": ["Deep learning", "generalization", "loss landscape", "robustness"], "paperhash": "goldblum|truth_or_backpropaganda_an_empirical_investigation_of_deep_learning_theory", "TL;DR": "We call into question commonly held beliefs regarding the loss landscape, optimization, network width, and rank.", "code": "https://github.com/goldblum/TruthOrBackpropaganda", "_bibtex": "@inproceedings{\nGoldblum2020Truth,\ntitle={Truth or backpropaganda? An empirical investigation of deep learning theory},\nauthor={Micah Goldblum and Jonas Geiping and Avi Schwarzschild and Michael Moeller and Tom Goldstein},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HyxyIgHFvr}\n}", "original_pdf": "/attachment/1c633809ad1e82b04900a0bec0d6d1268a0b2ff3.pdf"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HyxyIgHFvr", "replyto": "HyxyIgHFvr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2307/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2307/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575988608887, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2307/Reviewers"], "noninvitees": [], "tcdate": 1570237724696, "tmdate": 1575988608899, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2307/-/Official_Review"}}}, {"id": "r1xCi-IjKr", "original": null, "number": 4, "cdate": 1571672486199, "ddate": null, "tcdate": 1571672486199, "tmdate": 1572920334095, "tddate": null, "forum": "HyxyIgHFvr", "replyto": "SJlPOVxutB", "invitation": "ICLR.cc/2020/Conference/Paper2307/-/Official_Comment", "content": {"title": "Interesting previous work", "comment": "Hello Charlie, \n\nThank you for bringing your work to our attention.  We agree that it is highly relevant, and we are eager to discuss and contextualize these results in the next version of this submission. We agree that previous work on the existence of local minima was limited in comparison to [1] and find that your work bridges the gap between these and our work.\n\nIn terms of differences, Theorem 1 from [1], to our understanding, applies to networks with a single hidden layer and squared error, whereas Theorem 1 in our work applies to networks of arbitrary depth and any continuous loss function. Furthermore, we do not assume that all data points are unique and that output is one-dimensional.\n\nAside from these more technical terms, we think it is crucial to note that even if the data can be fitted with a linear classifier of dimension m, our work shows that any network with a smaller width n still contains spurious local minima, corresponding to linear classifiers with rank <= n. What we further find interesting is that our result can be recursively extended to local minima at which a network behaves like a shallower subnetwork on the training data.  This extension may not follow directly from [1] since outputs are univariate.  Our proof also applies to networks with convolutional layers since they can form the identity necessary for our construction.  \n\nWe further like the idea of generalizing to other activation functions.   We chose ReLUs for simplicity and their wide use, but any activation functions which are affine-linear with nonzero slope on some open interval are equally suitable under our proof technique.  Such a corollary inspired by your variant would be a good fit for the next version.\n\nBest Regards,\nThe Authors\n\n[1] Small nonlinearities in activation functions create bad local minima in neural networks, ICLR 2019"}, "signatures": ["ICLR.cc/2020/Conference/Paper2307/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2307/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["goldblumcello@gmail.com", "jonas.geiping@uni-siegen.de", "avi1@umd.edu", "michael.moeller@uni-siegen.de", "tomg@cs.umd.edu"], "title": "Truth or backpropaganda? An empirical investigation of deep learning theory", "authors": ["Micah Goldblum", "Jonas Geiping", "Avi Schwarzschild", "Michael Moeller", "Tom Goldstein"], "pdf": "/pdf/70c2d968c8b055683fe661442a25866fef5784de.pdf", "abstract": "We empirically evaluate common assumptions about neural networks that are widely held by practitioners and theorists alike.  In this work, we: (1) prove the widespread existence of suboptimal local minima in the loss landscape of neural networks, and we use our theory to find examples; (2) show that small-norm parameters are not optimal for generalization; (3) demonstrate that ResNets do not conform to wide-network theories, such as the neural tangent kernel, and that the interaction between skip connections and batch normalization plays a role;  (4) find that rank does not correlate with generalization or robustness in a practical setting.", "keywords": ["Deep learning", "generalization", "loss landscape", "robustness"], "paperhash": "goldblum|truth_or_backpropaganda_an_empirical_investigation_of_deep_learning_theory", "TL;DR": "We call into question commonly held beliefs regarding the loss landscape, optimization, network width, and rank.", "code": "https://github.com/goldblum/TruthOrBackpropaganda", "_bibtex": "@inproceedings{\nGoldblum2020Truth,\ntitle={Truth or backpropaganda? An empirical investigation of deep learning theory},\nauthor={Micah Goldblum and Jonas Geiping and Avi Schwarzschild and Michael Moeller and Tom Goldstein},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HyxyIgHFvr}\n}", "original_pdf": "/attachment/1c633809ad1e82b04900a0bec0d6d1268a0b2ff3.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HyxyIgHFvr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2307/Authors", "ICLR.cc/2020/Conference/Paper2307/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2307/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2307/Reviewers", "ICLR.cc/2020/Conference/Paper2307/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2307/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2307/Authors|ICLR.cc/2020/Conference/Paper2307/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504143296, "tmdate": 1576860530551, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2307/Authors", "ICLR.cc/2020/Conference/Paper2307/Reviewers", "ICLR.cc/2020/Conference/Paper2307/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2307/-/Official_Comment"}}}, {"id": "Bygm_Tz0KS", "original": null, "number": 5, "cdate": 1571855723454, "ddate": null, "tcdate": 1571855723454, "tmdate": 1571855723454, "tddate": null, "forum": "HyxyIgHFvr", "replyto": "SygoTRPiFB", "invitation": "ICLR.cc/2020/Conference/Paper2307/-/Official_Comment", "content": {"comment": "Thank you for letting us know about your paper.  The phenomenon of low-rank hidden states is an interesting direction for research.", "title": "Interesting research direction"}, "signatures": ["ICLR.cc/2020/Conference/Paper2307/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2307/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["goldblumcello@gmail.com", "jonas.geiping@uni-siegen.de", "avi1@umd.edu", "michael.moeller@uni-siegen.de", "tomg@cs.umd.edu"], "title": "Truth or backpropaganda? An empirical investigation of deep learning theory", "authors": ["Micah Goldblum", "Jonas Geiping", "Avi Schwarzschild", "Michael Moeller", "Tom Goldstein"], "pdf": "/pdf/70c2d968c8b055683fe661442a25866fef5784de.pdf", "abstract": "We empirically evaluate common assumptions about neural networks that are widely held by practitioners and theorists alike.  In this work, we: (1) prove the widespread existence of suboptimal local minima in the loss landscape of neural networks, and we use our theory to find examples; (2) show that small-norm parameters are not optimal for generalization; (3) demonstrate that ResNets do not conform to wide-network theories, such as the neural tangent kernel, and that the interaction between skip connections and batch normalization plays a role;  (4) find that rank does not correlate with generalization or robustness in a practical setting.", "keywords": ["Deep learning", "generalization", "loss landscape", "robustness"], "paperhash": "goldblum|truth_or_backpropaganda_an_empirical_investigation_of_deep_learning_theory", "TL;DR": "We call into question commonly held beliefs regarding the loss landscape, optimization, network width, and rank.", "code": "https://github.com/goldblum/TruthOrBackpropaganda", "_bibtex": "@inproceedings{\nGoldblum2020Truth,\ntitle={Truth or backpropaganda? An empirical investigation of deep learning theory},\nauthor={Micah Goldblum and Jonas Geiping and Avi Schwarzschild and Michael Moeller and Tom Goldstein},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HyxyIgHFvr}\n}", "original_pdf": "/attachment/1c633809ad1e82b04900a0bec0d6d1268a0b2ff3.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HyxyIgHFvr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2307/Authors", "ICLR.cc/2020/Conference/Paper2307/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2307/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2307/Reviewers", "ICLR.cc/2020/Conference/Paper2307/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2307/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2307/Authors|ICLR.cc/2020/Conference/Paper2307/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504143296, "tmdate": 1576860530551, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2307/Authors", "ICLR.cc/2020/Conference/Paper2307/Reviewers", "ICLR.cc/2020/Conference/Paper2307/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2307/-/Official_Comment"}}}, {"id": "SygoTRPiFB", "original": null, "number": 4, "cdate": 1571679939219, "ddate": null, "tcdate": 1571679939219, "tmdate": 1571679939219, "tddate": null, "forum": "HyxyIgHFvr", "replyto": "BJgFUN6OB", "invitation": "ICLR.cc/2020/Conference/Paper2307/-/Public_Comment", "content": {"title": "Low Rank Representations", "comment": "1. Given the discussion about low rank hidden states, I thought I would point out our work on low rank representations and its effect on adversarial robustness.\n https://arxiv.org/abs/1804.07090\n\n2. While Linear operators can make the pre-activations low rank, (and the skip connection may or may not increase its rank), the non-linear activation function often increases it to give high rank hidden states (eg. Appendix A in the paper above).\n\n"}, "signatures": ["~Amartya_Sanyal2"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Amartya_Sanyal2", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["goldblumcello@gmail.com", "jonas.geiping@uni-siegen.de", "avi1@umd.edu", "michael.moeller@uni-siegen.de", "tomg@cs.umd.edu"], "title": "Truth or backpropaganda? An empirical investigation of deep learning theory", "authors": ["Micah Goldblum", "Jonas Geiping", "Avi Schwarzschild", "Michael Moeller", "Tom Goldstein"], "pdf": "/pdf/70c2d968c8b055683fe661442a25866fef5784de.pdf", "abstract": "We empirically evaluate common assumptions about neural networks that are widely held by practitioners and theorists alike.  In this work, we: (1) prove the widespread existence of suboptimal local minima in the loss landscape of neural networks, and we use our theory to find examples; (2) show that small-norm parameters are not optimal for generalization; (3) demonstrate that ResNets do not conform to wide-network theories, such as the neural tangent kernel, and that the interaction between skip connections and batch normalization plays a role;  (4) find that rank does not correlate with generalization or robustness in a practical setting.", "keywords": ["Deep learning", "generalization", "loss landscape", "robustness"], "paperhash": "goldblum|truth_or_backpropaganda_an_empirical_investigation_of_deep_learning_theory", "TL;DR": "We call into question commonly held beliefs regarding the loss landscape, optimization, network width, and rank.", "code": "https://github.com/goldblum/TruthOrBackpropaganda", "_bibtex": "@inproceedings{\nGoldblum2020Truth,\ntitle={Truth or backpropaganda? An empirical investigation of deep learning theory},\nauthor={Micah Goldblum and Jonas Geiping and Avi Schwarzschild and Michael Moeller and Tom Goldstein},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HyxyIgHFvr}\n}", "original_pdf": "/attachment/1c633809ad1e82b04900a0bec0d6d1268a0b2ff3.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HyxyIgHFvr", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504182177, "tmdate": 1576860564262, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper2307/Authors", "ICLR.cc/2020/Conference/Paper2307/Reviewers", "ICLR.cc/2020/Conference/Paper2307/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2307/-/Public_Comment"}}}, {"id": "SJlPOVxutB", "original": null, "number": 3, "cdate": 1571452014656, "ddate": null, "tcdate": 1571452014656, "tmdate": 1571452014656, "tddate": null, "forum": "HyxyIgHFvr", "replyto": "HyxyIgHFvr", "invitation": "ICLR.cc/2020/Conference/Paper2307/-/Public_Comment", "content": {"comment": "Dear authors,\n\nI enjoyed reading your submission. Thanks for the interesting paper! \n\nAfter reading the paper, I wanted to bring to your attention a paper of ours on the existence of bad local minima that seems quite relevant:\n[1] Small nonlinearities in activation functions create bad local minima in neural networks, ICLR 2019, https://openreview.net/forum?id=rke_YiRct7\n\nIn particular, Theorem 1 of [1] constructs local minima of neural networks whose predictions perform just as well as the linear predictor, and shows that for general datasets that these local minima are not globally optimal. As far as I understand, the key idea of the proof of Theorem 1 in this submission looks very similar to [1]: pushing the bias high enough so that the network becomes linear. In my opinion, the theoretical results in this submission and [1] are highly relevant, so it would be very helpful if the authors could compare them in the paper.\n\nI\u2019d also like to note that Theorem 1 of [1] also implies that even with slightest nonlinearity (slope 1+\\epsilon on positive side and slope 1 on negative side) and for general datasets, there exist bad local minima. Furthermore, I believe the assumptions in [1] are milder than the other previous results cited in Section 2 of this submission.\n\nOverall, I believe [1] is highly relevant to this submission. Thus, we would appreciate it if the authors could cite our paper as well as contextualize their results with ours. We hope that the authors will be able to bring out the differences and potential subtleties, if any.\n\nThank you!\nCharlie Yun", "title": "Relevant work on the existence of bad local minima"}, "signatures": ["~Chulhee_Yun1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Chulhee_Yun1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["goldblumcello@gmail.com", "jonas.geiping@uni-siegen.de", "avi1@umd.edu", "michael.moeller@uni-siegen.de", "tomg@cs.umd.edu"], "title": "Truth or backpropaganda? An empirical investigation of deep learning theory", "authors": ["Micah Goldblum", "Jonas Geiping", "Avi Schwarzschild", "Michael Moeller", "Tom Goldstein"], "pdf": "/pdf/70c2d968c8b055683fe661442a25866fef5784de.pdf", "abstract": "We empirically evaluate common assumptions about neural networks that are widely held by practitioners and theorists alike.  In this work, we: (1) prove the widespread existence of suboptimal local minima in the loss landscape of neural networks, and we use our theory to find examples; (2) show that small-norm parameters are not optimal for generalization; (3) demonstrate that ResNets do not conform to wide-network theories, such as the neural tangent kernel, and that the interaction between skip connections and batch normalization plays a role;  (4) find that rank does not correlate with generalization or robustness in a practical setting.", "keywords": ["Deep learning", "generalization", "loss landscape", "robustness"], "paperhash": "goldblum|truth_or_backpropaganda_an_empirical_investigation_of_deep_learning_theory", "TL;DR": "We call into question commonly held beliefs regarding the loss landscape, optimization, network width, and rank.", "code": "https://github.com/goldblum/TruthOrBackpropaganda", "_bibtex": "@inproceedings{\nGoldblum2020Truth,\ntitle={Truth or backpropaganda? An empirical investigation of deep learning theory},\nauthor={Micah Goldblum and Jonas Geiping and Avi Schwarzschild and Michael Moeller and Tom Goldstein},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HyxyIgHFvr}\n}", "original_pdf": "/attachment/1c633809ad1e82b04900a0bec0d6d1268a0b2ff3.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HyxyIgHFvr", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504182177, "tmdate": 1576860564262, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper2307/Authors", "ICLR.cc/2020/Conference/Paper2307/Reviewers", "ICLR.cc/2020/Conference/Paper2307/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2307/-/Public_Comment"}}}, {"id": "BJgFUN6OB", "original": null, "number": 3, "cdate": 1570748023516, "ddate": null, "tcdate": 1570748023516, "tmdate": 1570748023516, "tddate": null, "forum": "HyxyIgHFvr", "replyto": "ryeEsMcLOS", "invitation": "ICLR.cc/2020/Conference/Paper2307/-/Official_Comment", "content": {"comment": "Thank you for the insightful comments:\n1)  We only consider low-rank linear operators since this topic is studied in the generalization and robustness works we discuss such as Neyshabur et al. (2017) and Langenberg et al. (2019).  However, we agree that low-rank hidden states may also be an interesting topic for empirical work.\n2)  The rank of linear operators in ResNets indeed contributes differently to the behavior of the network than the rank of linear operators in MLPs.  In the case of linear ResNets, for example, if the applied weight matrix is the negative identity, then after a skip connection, the combined layer would be rank-0.  In fact, the combined layer which includes a skip connection may be a low or high rank affine transformation.  However, in the non-linear case, it is not clear what the analogous rank measurements would be since there are nonlinearities between affine transformations and skip connections, and thus, we cannot collapse layers and skip connections into one combined affine transformation.\nWe chose ResNet-18 in order to determine if intuitions developed by theory transfer to a realistic architecture.  Even so, we have also tested these claims in the context of MLPs and found that the same results hold as do for ResNets in the case of generalization, and more notably, a naturally trained MLP with RankMax achieves higher robust accuracy than the same MLP trained with RankMin.  For example, an MLP with RankMin achieved 49.94% robust accuracy on CIFAR-10 against the small-radius PGD attack from the paper, while the same MLP with RankMax achieved 51.45% robust accuracy.  We may include these MLP results in the next version of our paper if there is interest.", "title": "Rank in residual networks"}, "signatures": ["ICLR.cc/2020/Conference/Paper2307/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2307/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["goldblumcello@gmail.com", "jonas.geiping@uni-siegen.de", "avi1@umd.edu", "michael.moeller@uni-siegen.de", "tomg@cs.umd.edu"], "title": "Truth or backpropaganda? An empirical investigation of deep learning theory", "authors": ["Micah Goldblum", "Jonas Geiping", "Avi Schwarzschild", "Michael Moeller", "Tom Goldstein"], "pdf": "/pdf/70c2d968c8b055683fe661442a25866fef5784de.pdf", "abstract": "We empirically evaluate common assumptions about neural networks that are widely held by practitioners and theorists alike.  In this work, we: (1) prove the widespread existence of suboptimal local minima in the loss landscape of neural networks, and we use our theory to find examples; (2) show that small-norm parameters are not optimal for generalization; (3) demonstrate that ResNets do not conform to wide-network theories, such as the neural tangent kernel, and that the interaction between skip connections and batch normalization plays a role;  (4) find that rank does not correlate with generalization or robustness in a practical setting.", "keywords": ["Deep learning", "generalization", "loss landscape", "robustness"], "paperhash": "goldblum|truth_or_backpropaganda_an_empirical_investigation_of_deep_learning_theory", "TL;DR": "We call into question commonly held beliefs regarding the loss landscape, optimization, network width, and rank.", "code": "https://github.com/goldblum/TruthOrBackpropaganda", "_bibtex": "@inproceedings{\nGoldblum2020Truth,\ntitle={Truth or backpropaganda? An empirical investigation of deep learning theory},\nauthor={Micah Goldblum and Jonas Geiping and Avi Schwarzschild and Michael Moeller and Tom Goldstein},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HyxyIgHFvr}\n}", "original_pdf": "/attachment/1c633809ad1e82b04900a0bec0d6d1268a0b2ff3.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HyxyIgHFvr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2307/Authors", "ICLR.cc/2020/Conference/Paper2307/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2307/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2307/Reviewers", "ICLR.cc/2020/Conference/Paper2307/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2307/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2307/Authors|ICLR.cc/2020/Conference/Paper2307/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504143296, "tmdate": 1576860530551, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2307/Authors", "ICLR.cc/2020/Conference/Paper2307/Reviewers", "ICLR.cc/2020/Conference/Paper2307/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2307/-/Official_Comment"}}}, {"id": "ryeEsMcLOS", "original": null, "number": 2, "cdate": 1570312859633, "ddate": null, "tcdate": 1570312859633, "tmdate": 1570312859633, "tddate": null, "forum": "HyxyIgHFvr", "replyto": "HyxyIgHFvr", "invitation": "ICLR.cc/2020/Conference/Paper2307/-/Public_Comment", "content": {"comment": "\n1.  As far as I can tell, this section discusses the issue of the weight matrices in a deep network being low-rank.  However the discussion in the literature focuses on both the rank of the weight matrices as well as the rank of the hidden states.  \n\nIt's not clear to me how these issues are related to each other, especially in non-linear networks.  Do you think the results in your paper also have some relevance for the study of low-rank hidden states or would you consider it to be separate issue?  \n\n2.  Do you think it's worth analyzing the resnet and non-resnet cases separately here?  If I think about a linear neural network, the residual network variant will always amount to a full-rank affine transformation on each layer as a result of the skip connection, even if the applied weight matrix W is low-rank.  ", "title": "Comments about Section 5 on Rank"}, "signatures": ["~Alex_Matthew_Lamb1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Alex_Matthew_Lamb1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["goldblumcello@gmail.com", "jonas.geiping@uni-siegen.de", "avi1@umd.edu", "michael.moeller@uni-siegen.de", "tomg@cs.umd.edu"], "title": "Truth or backpropaganda? An empirical investigation of deep learning theory", "authors": ["Micah Goldblum", "Jonas Geiping", "Avi Schwarzschild", "Michael Moeller", "Tom Goldstein"], "pdf": "/pdf/70c2d968c8b055683fe661442a25866fef5784de.pdf", "abstract": "We empirically evaluate common assumptions about neural networks that are widely held by practitioners and theorists alike.  In this work, we: (1) prove the widespread existence of suboptimal local minima in the loss landscape of neural networks, and we use our theory to find examples; (2) show that small-norm parameters are not optimal for generalization; (3) demonstrate that ResNets do not conform to wide-network theories, such as the neural tangent kernel, and that the interaction between skip connections and batch normalization plays a role;  (4) find that rank does not correlate with generalization or robustness in a practical setting.", "keywords": ["Deep learning", "generalization", "loss landscape", "robustness"], "paperhash": "goldblum|truth_or_backpropaganda_an_empirical_investigation_of_deep_learning_theory", "TL;DR": "We call into question commonly held beliefs regarding the loss landscape, optimization, network width, and rank.", "code": "https://github.com/goldblum/TruthOrBackpropaganda", "_bibtex": "@inproceedings{\nGoldblum2020Truth,\ntitle={Truth or backpropaganda? An empirical investigation of deep learning theory},\nauthor={Micah Goldblum and Jonas Geiping and Avi Schwarzschild and Michael Moeller and Tom Goldstein},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HyxyIgHFvr}\n}", "original_pdf": "/attachment/1c633809ad1e82b04900a0bec0d6d1268a0b2ff3.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HyxyIgHFvr", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504182177, "tmdate": 1576860564262, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper2307/Authors", "ICLR.cc/2020/Conference/Paper2307/Reviewers", "ICLR.cc/2020/Conference/Paper2307/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2307/-/Public_Comment"}}}, {"id": "B1xvkIhNOB", "original": null, "number": 2, "cdate": 1570190814891, "ddate": null, "tcdate": 1570190814891, "tmdate": 1570190814891, "tddate": null, "forum": "HyxyIgHFvr", "replyto": "H1gFcHcmur", "invitation": "ICLR.cc/2020/Conference/Paper2307/-/Official_Comment", "content": {"comment": "Hi Pedro,\nThank you for pointing out this paper.  We agree that the relationship between explicit regularizers, like weight decay and norm-bias, during training and Bayesian priors at inference may be an interesting direction for future work.", "title": "Interesting connection"}, "signatures": ["ICLR.cc/2020/Conference/Paper2307/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2307/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["goldblumcello@gmail.com", "jonas.geiping@uni-siegen.de", "avi1@umd.edu", "michael.moeller@uni-siegen.de", "tomg@cs.umd.edu"], "title": "Truth or backpropaganda? An empirical investigation of deep learning theory", "authors": ["Micah Goldblum", "Jonas Geiping", "Avi Schwarzschild", "Michael Moeller", "Tom Goldstein"], "pdf": "/pdf/70c2d968c8b055683fe661442a25866fef5784de.pdf", "abstract": "We empirically evaluate common assumptions about neural networks that are widely held by practitioners and theorists alike.  In this work, we: (1) prove the widespread existence of suboptimal local minima in the loss landscape of neural networks, and we use our theory to find examples; (2) show that small-norm parameters are not optimal for generalization; (3) demonstrate that ResNets do not conform to wide-network theories, such as the neural tangent kernel, and that the interaction between skip connections and batch normalization plays a role;  (4) find that rank does not correlate with generalization or robustness in a practical setting.", "keywords": ["Deep learning", "generalization", "loss landscape", "robustness"], "paperhash": "goldblum|truth_or_backpropaganda_an_empirical_investigation_of_deep_learning_theory", "TL;DR": "We call into question commonly held beliefs regarding the loss landscape, optimization, network width, and rank.", "code": "https://github.com/goldblum/TruthOrBackpropaganda", "_bibtex": "@inproceedings{\nGoldblum2020Truth,\ntitle={Truth or backpropaganda? An empirical investigation of deep learning theory},\nauthor={Micah Goldblum and Jonas Geiping and Avi Schwarzschild and Michael Moeller and Tom Goldstein},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HyxyIgHFvr}\n}", "original_pdf": "/attachment/1c633809ad1e82b04900a0bec0d6d1268a0b2ff3.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HyxyIgHFvr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2307/Authors", "ICLR.cc/2020/Conference/Paper2307/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2307/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2307/Reviewers", "ICLR.cc/2020/Conference/Paper2307/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2307/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2307/Authors|ICLR.cc/2020/Conference/Paper2307/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504143296, "tmdate": 1576860530551, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2307/Authors", "ICLR.cc/2020/Conference/Paper2307/Reviewers", "ICLR.cc/2020/Conference/Paper2307/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2307/-/Official_Comment"}}}, {"id": "H1gFcHcmur", "original": null, "number": 1, "cdate": 1570117008535, "ddate": null, "tcdate": 1570117008535, "tmdate": 1570117031205, "tddate": null, "forum": "HyxyIgHFvr", "replyto": "HyxyIgHFvr", "invitation": "ICLR.cc/2020/Conference/Paper2307/-/Public_Comment", "content": {"comment": "The work Bayesian Neural Network Ensembles by Pearce et al (https://arxiv.org/abs/1811.12188) proposes to use an ensemble of neural networks that are each trained with L2 regularization from a normal distribution sample. They show this is form of approximate Bayesian inference.\n\nThat idea is somewhat similar to the \"norm-bias\" regularizer proposed in this paper, with the difference that the weights attracted to a normal distribution sample rather than a fixed value.\n\nI just wanted to point out this connection, which may be relevant to explain why norm-bias works.", "title": "Bayesian Neural Network Ensembles connection"}, "signatures": ["~Pedro_Tabacof1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Pedro_Tabacof1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["goldblumcello@gmail.com", "jonas.geiping@uni-siegen.de", "avi1@umd.edu", "michael.moeller@uni-siegen.de", "tomg@cs.umd.edu"], "title": "Truth or backpropaganda? An empirical investigation of deep learning theory", "authors": ["Micah Goldblum", "Jonas Geiping", "Avi Schwarzschild", "Michael Moeller", "Tom Goldstein"], "pdf": "/pdf/70c2d968c8b055683fe661442a25866fef5784de.pdf", "abstract": "We empirically evaluate common assumptions about neural networks that are widely held by practitioners and theorists alike.  In this work, we: (1) prove the widespread existence of suboptimal local minima in the loss landscape of neural networks, and we use our theory to find examples; (2) show that small-norm parameters are not optimal for generalization; (3) demonstrate that ResNets do not conform to wide-network theories, such as the neural tangent kernel, and that the interaction between skip connections and batch normalization plays a role;  (4) find that rank does not correlate with generalization or robustness in a practical setting.", "keywords": ["Deep learning", "generalization", "loss landscape", "robustness"], "paperhash": "goldblum|truth_or_backpropaganda_an_empirical_investigation_of_deep_learning_theory", "TL;DR": "We call into question commonly held beliefs regarding the loss landscape, optimization, network width, and rank.", "code": "https://github.com/goldblum/TruthOrBackpropaganda", "_bibtex": "@inproceedings{\nGoldblum2020Truth,\ntitle={Truth or backpropaganda? An empirical investigation of deep learning theory},\nauthor={Micah Goldblum and Jonas Geiping and Avi Schwarzschild and Michael Moeller and Tom Goldstein},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HyxyIgHFvr}\n}", "original_pdf": "/attachment/1c633809ad1e82b04900a0bec0d6d1268a0b2ff3.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HyxyIgHFvr", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504182177, "tmdate": 1576860564262, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper2307/Authors", "ICLR.cc/2020/Conference/Paper2307/Reviewers", "ICLR.cc/2020/Conference/Paper2307/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2307/-/Public_Comment"}}}], "count": 16}