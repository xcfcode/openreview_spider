{"notes": [{"id": "BylUMxSFwS", "original": "SylgSzeYPH", "number": 2175, "cdate": 1569439758350, "ddate": null, "tcdate": 1569439758350, "tmdate": 1577168219447, "tddate": null, "forum": "BylUMxSFwS", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["crgrimm@umich.edu", "irinah@google.com", "andrebarreto@google.com", "teplyashin@google.com", "mwulfmeier@google.com", "thertweck@google.com", "raia@google.com", "baveja@google.com"], "title": "Disentangled Cumulants Help Successor Representations Transfer to New Tasks", "authors": ["Chris Grimm", "Irina Higgins", "Andre Barreto", "Denis Teplyashin", "Markus Wulfmeier", "Tim Hertweck", "Raia Hadsell", "Satinder Singh"], "pdf": "/pdf/c5e5066f2b70f0cf4e9f4631953ea191a87fd2a4.pdf", "TL;DR": "We show in the absence of external reward that agents can leverage knowledge from unsupervised control of latent features to solve downstream tasks and that when these latent features are disentangled superior performance is achieved.", "abstract": "Biological intelligence can learn to solve many diverse tasks in a data efficient manner by re-using basic knowledge and skills from one task to another. Furthermore, many of such skills are acquired through something called latent learning, where no explicit supervision for skill acquisition is provided. This is in contrast to the state-of-the-art reinforcement learning agents, which typically start learning each new task from scratch and struggle with knowledge transfer. In this paper we propose a principled way to learn and recombine a basis set of policies, which comes with certain guarantees on the coverage of the final task space. In particular, we construct a learning pipeline where an agent invests time to learn to perform intrinsically generated, goal-based tasks, and subsequently leverages this experience to quickly achieve a high level of performance on externally specified, often significantly more complex tasks through generalised policy improvement. We demonstrate both theoretically and empirically that such goal-based intrinsic tasks produce more transferable policies when the goals are specified in a space that exhibits a form of disentanglement. ", "keywords": ["reinforcement learning", "representation learning", "intrinsic reward", "intrinsic control", "endogenous", "generalized policy improvement", "successor features", "variational", "monet", "disentangled"], "paperhash": "grimm|disentangled_cumulants_help_successor_representations_transfer_to_new_tasks", "original_pdf": "/attachment/9b9a25d40c80adfee7362024cb569c6da0e0c750.pdf", "_bibtex": "@misc{\ngrimm2020disentangled,\ntitle={Disentangled Cumulants Help Successor Representations Transfer to New Tasks},\nauthor={Chris Grimm and Irina Higgins and Andre Barreto and Denis Teplyashin and Markus Wulfmeier and Tim Hertweck and Raia Hadsell and Satinder Singh},\nyear={2020},\nurl={https://openreview.net/forum?id=BylUMxSFwS}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "1cjSIaigOJ", "original": null, "number": 1, "cdate": 1576798742445, "ddate": null, "tcdate": 1576798742445, "tmdate": 1576800893768, "tddate": null, "forum": "BylUMxSFwS", "replyto": "BylUMxSFwS", "invitation": "ICLR.cc/2020/Conference/Paper2175/-/Decision", "content": {"decision": "Reject", "comment": "The author propose a method to first learn policies for intrinsically generated goal-based tasks, and then leverage the learned representations to improve the learning of a new task in a generalized policy iteration framework.  The reviewers had significant issues about clarity of writing that were largely addressed in the rebuttal.  However, there were also concerns about the magnitude of the contribution (especially if it was added anything significant to the existing literature on GPI, successor features, etc), and the simplicity (and small number of) test domains.  These concerns persisted after the rebuttal and discussion.  Thus, I recommend rejection at this time.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["crgrimm@umich.edu", "irinah@google.com", "andrebarreto@google.com", "teplyashin@google.com", "mwulfmeier@google.com", "thertweck@google.com", "raia@google.com", "baveja@google.com"], "title": "Disentangled Cumulants Help Successor Representations Transfer to New Tasks", "authors": ["Chris Grimm", "Irina Higgins", "Andre Barreto", "Denis Teplyashin", "Markus Wulfmeier", "Tim Hertweck", "Raia Hadsell", "Satinder Singh"], "pdf": "/pdf/c5e5066f2b70f0cf4e9f4631953ea191a87fd2a4.pdf", "TL;DR": "We show in the absence of external reward that agents can leverage knowledge from unsupervised control of latent features to solve downstream tasks and that when these latent features are disentangled superior performance is achieved.", "abstract": "Biological intelligence can learn to solve many diverse tasks in a data efficient manner by re-using basic knowledge and skills from one task to another. Furthermore, many of such skills are acquired through something called latent learning, where no explicit supervision for skill acquisition is provided. This is in contrast to the state-of-the-art reinforcement learning agents, which typically start learning each new task from scratch and struggle with knowledge transfer. In this paper we propose a principled way to learn and recombine a basis set of policies, which comes with certain guarantees on the coverage of the final task space. In particular, we construct a learning pipeline where an agent invests time to learn to perform intrinsically generated, goal-based tasks, and subsequently leverages this experience to quickly achieve a high level of performance on externally specified, often significantly more complex tasks through generalised policy improvement. We demonstrate both theoretically and empirically that such goal-based intrinsic tasks produce more transferable policies when the goals are specified in a space that exhibits a form of disentanglement. ", "keywords": ["reinforcement learning", "representation learning", "intrinsic reward", "intrinsic control", "endogenous", "generalized policy improvement", "successor features", "variational", "monet", "disentangled"], "paperhash": "grimm|disentangled_cumulants_help_successor_representations_transfer_to_new_tasks", "original_pdf": "/attachment/9b9a25d40c80adfee7362024cb569c6da0e0c750.pdf", "_bibtex": "@misc{\ngrimm2020disentangled,\ntitle={Disentangled Cumulants Help Successor Representations Transfer to New Tasks},\nauthor={Chris Grimm and Irina Higgins and Andre Barreto and Denis Teplyashin and Markus Wulfmeier and Tim Hertweck and Raia Hadsell and Satinder Singh},\nyear={2020},\nurl={https://openreview.net/forum?id=BylUMxSFwS}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "BylUMxSFwS", "replyto": "BylUMxSFwS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795720595, "tmdate": 1576800271453, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2175/-/Decision"}}}, {"id": "B1xqEuK0Kr", "original": null, "number": 2, "cdate": 1571883058344, "ddate": null, "tcdate": 1571883058344, "tmdate": 1574524169090, "tddate": null, "forum": "BylUMxSFwS", "replyto": "BylUMxSFwS", "invitation": "ICLR.cc/2020/Conference/Paper2175/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "title": "Official Blind Review #2", "review": "The paper tackles the challenging problem of transfer learning and few shot learning in RL setting and provides some theoretical guarantees for the downstream task coverage. \n\nThe paper structure can be further improved by adding a background subsection on successor representation (SR) in RL; SR is not a very well known representation in RL and a brief subsection on that can help the reader in understanding the motivation behind using it. In terms of related work ,another work which can also be mentioned (although not directly related) is \u201cDARLA: Improving Zero-Shot Transfer in Reinforcement Learning\u201d which also uses disentangled representations for zero-shot transfer learning. The paper also needs to be more clear in terms of contributions; it seems that there is a significant overlap between this work and (Barreto et al., 2017, 2018); some clarification would be helpful here. \n\nIn terms of empirical results; the authors can also compare with other transfer learning methods in deep RL such as Hansen 2019 or Nair 2018 or explain why these are not reasonable baselines. Also the results for DIAYN are a bit surprising to me since in all the experiments the performance of the method is underwhelming; this is especially surprising because in the original DIAYN paper the method performed well in reasonably complex tasks. Can you provide an intuition on why DIAYN performs poorly even in the agent tasks. ", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper2175/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2175/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["crgrimm@umich.edu", "irinah@google.com", "andrebarreto@google.com", "teplyashin@google.com", "mwulfmeier@google.com", "thertweck@google.com", "raia@google.com", "baveja@google.com"], "title": "Disentangled Cumulants Help Successor Representations Transfer to New Tasks", "authors": ["Chris Grimm", "Irina Higgins", "Andre Barreto", "Denis Teplyashin", "Markus Wulfmeier", "Tim Hertweck", "Raia Hadsell", "Satinder Singh"], "pdf": "/pdf/c5e5066f2b70f0cf4e9f4631953ea191a87fd2a4.pdf", "TL;DR": "We show in the absence of external reward that agents can leverage knowledge from unsupervised control of latent features to solve downstream tasks and that when these latent features are disentangled superior performance is achieved.", "abstract": "Biological intelligence can learn to solve many diverse tasks in a data efficient manner by re-using basic knowledge and skills from one task to another. Furthermore, many of such skills are acquired through something called latent learning, where no explicit supervision for skill acquisition is provided. This is in contrast to the state-of-the-art reinforcement learning agents, which typically start learning each new task from scratch and struggle with knowledge transfer. In this paper we propose a principled way to learn and recombine a basis set of policies, which comes with certain guarantees on the coverage of the final task space. In particular, we construct a learning pipeline where an agent invests time to learn to perform intrinsically generated, goal-based tasks, and subsequently leverages this experience to quickly achieve a high level of performance on externally specified, often significantly more complex tasks through generalised policy improvement. We demonstrate both theoretically and empirically that such goal-based intrinsic tasks produce more transferable policies when the goals are specified in a space that exhibits a form of disentanglement. ", "keywords": ["reinforcement learning", "representation learning", "intrinsic reward", "intrinsic control", "endogenous", "generalized policy improvement", "successor features", "variational", "monet", "disentangled"], "paperhash": "grimm|disentangled_cumulants_help_successor_representations_transfer_to_new_tasks", "original_pdf": "/attachment/9b9a25d40c80adfee7362024cb569c6da0e0c750.pdf", "_bibtex": "@misc{\ngrimm2020disentangled,\ntitle={Disentangled Cumulants Help Successor Representations Transfer to New Tasks},\nauthor={Chris Grimm and Irina Higgins and Andre Barreto and Denis Teplyashin and Markus Wulfmeier and Tim Hertweck and Raia Hadsell and Satinder Singh},\nyear={2020},\nurl={https://openreview.net/forum?id=BylUMxSFwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BylUMxSFwS", "replyto": "BylUMxSFwS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2175/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2175/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575508729823, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2175/Reviewers"], "noninvitees": [], "tcdate": 1570237726625, "tmdate": 1575508729835, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2175/-/Official_Review"}}}, {"id": "H1x4DQURYS", "original": null, "number": 1, "cdate": 1571869532349, "ddate": null, "tcdate": 1571869532349, "tmdate": 1574513602777, "tddate": null, "forum": "BylUMxSFwS", "replyto": "BylUMxSFwS", "invitation": "ICLR.cc/2020/Conference/Paper2175/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #1", "review": "The paper addresses the problem of policy transfer in reinforcement learning, which is an extremely relevant open problem in RL, and is being actively studied by the community. \u2028The authors propose a framework for discovering a set of policies without external supervision which can then be used to produce reasonable performance on extrinsic tasks. \u2028The work exhibits originality in that it shows that disentangled representations, learned by intrinsic rewards,  can lead to learn behaviours that are transferable to novel situations. \u2028\n\nAlthough the problem talked here is of high relevance and the approach proposed is original and supported by theoretical results, I am leaning to reject for the following reasons:\n\n- Missing connection to some existing works in the literature. In particular, it seems that there is a link to previous works that focus on discovering reward agnostic options (such as [1]).\n- Clarity. The method description is somehow difficult to read, mainly because some variables are introduced without explanation/definition. On page 2, please define n and explain the choices of m and k. The font of the axes and legends in Figure 4 is too small, not readable when printed.\n- Experiments: My first concern is that I do not think I would be able to reproduce the results solely given the paper and supplementary material. It would be necessary to either have access to the code, or a very detailed implementation report.\n- I would have loved to see either another experiment or at least an intuition on how the framework extends to a very different domain. If not, it should be made clearer that this framework works on 2d domains, where the tasks are navigation tasks. (I am specifically referring to the representation learning phase).\n\nMinor comments:\nPage 1, first paragraph \u201c(controlling the position of fruits and nuts)\u201d\npage 5, below (9): shouldn\u2019t it be \u201cWhile \\pi_w is not \u2026\u201d?\nPage 8, last sentence of Sec 5, \u201ccould only learn to perform\u201d.\n\n1. Machado et al, EIGENOPTION DISCOVERY THROUGH THE DEEP SUCCESSOR REPRESENTATION, ICLR 2018 \n\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper2175/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2175/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["crgrimm@umich.edu", "irinah@google.com", "andrebarreto@google.com", "teplyashin@google.com", "mwulfmeier@google.com", "thertweck@google.com", "raia@google.com", "baveja@google.com"], "title": "Disentangled Cumulants Help Successor Representations Transfer to New Tasks", "authors": ["Chris Grimm", "Irina Higgins", "Andre Barreto", "Denis Teplyashin", "Markus Wulfmeier", "Tim Hertweck", "Raia Hadsell", "Satinder Singh"], "pdf": "/pdf/c5e5066f2b70f0cf4e9f4631953ea191a87fd2a4.pdf", "TL;DR": "We show in the absence of external reward that agents can leverage knowledge from unsupervised control of latent features to solve downstream tasks and that when these latent features are disentangled superior performance is achieved.", "abstract": "Biological intelligence can learn to solve many diverse tasks in a data efficient manner by re-using basic knowledge and skills from one task to another. Furthermore, many of such skills are acquired through something called latent learning, where no explicit supervision for skill acquisition is provided. This is in contrast to the state-of-the-art reinforcement learning agents, which typically start learning each new task from scratch and struggle with knowledge transfer. In this paper we propose a principled way to learn and recombine a basis set of policies, which comes with certain guarantees on the coverage of the final task space. In particular, we construct a learning pipeline where an agent invests time to learn to perform intrinsically generated, goal-based tasks, and subsequently leverages this experience to quickly achieve a high level of performance on externally specified, often significantly more complex tasks through generalised policy improvement. We demonstrate both theoretically and empirically that such goal-based intrinsic tasks produce more transferable policies when the goals are specified in a space that exhibits a form of disentanglement. ", "keywords": ["reinforcement learning", "representation learning", "intrinsic reward", "intrinsic control", "endogenous", "generalized policy improvement", "successor features", "variational", "monet", "disentangled"], "paperhash": "grimm|disentangled_cumulants_help_successor_representations_transfer_to_new_tasks", "original_pdf": "/attachment/9b9a25d40c80adfee7362024cb569c6da0e0c750.pdf", "_bibtex": "@misc{\ngrimm2020disentangled,\ntitle={Disentangled Cumulants Help Successor Representations Transfer to New Tasks},\nauthor={Chris Grimm and Irina Higgins and Andre Barreto and Denis Teplyashin and Markus Wulfmeier and Tim Hertweck and Raia Hadsell and Satinder Singh},\nyear={2020},\nurl={https://openreview.net/forum?id=BylUMxSFwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BylUMxSFwS", "replyto": "BylUMxSFwS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2175/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2175/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575508729823, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2175/Reviewers"], "noninvitees": [], "tcdate": 1570237726625, "tmdate": 1575508729835, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2175/-/Official_Review"}}}, {"id": "ByenD63fqH", "original": null, "number": 3, "cdate": 1572158819561, "ddate": null, "tcdate": 1572158819561, "tmdate": 1572972373204, "tddate": null, "forum": "BylUMxSFwS", "replyto": "BylUMxSFwS", "invitation": "ICLR.cc/2020/Conference/Paper2175/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes to pre-train policies on some goal-reaching tasks, and then leverage the associated successor features to improve the learning of a new task. The method heavily draws from the Generalized Policy Evaluation/Improvement framework without adding much to it. The only relevant point would be showing (as the title indicates) how to obtain disentangled cumulants, and whether they help transfer to new tasks. Nevertheless, both the definition, the full method, and the claimed benefits are quite ambiguous.\n\nAmong other concerns showing that the theory needs more formal treatment, the pillar definition of \u201cOptimal independent controllability\u201d is very confusing because it seems to depend on \u201ca trajectory generated by following \\pi_i^*\u201d. But what If the environment is stochastic? Then following that policy might give different trajectories! This definition needs to be revisited. More concerning examples are given at the end of this review.\n\nOn the experimental side, Fig. 4 is the only reported result, and it has an x-axis that is not clearly explained. What are the \u201csteps (min)\u201d?\nIt is also not clear what they mean by the \u201coff-diagonal trick\u201d, which seems so important for the good performance.\nFurthermore, it seems that their method doesn\u2019t really learn anything new in most of the tasks, it just stays at the same performance that is started with after the whole pre-training steps. It is not clearly stated how much computation effort is required to obtain the desired cumulants, and this invalidates quite strongly any result they report. Even if there\u2019s no \u201creward\u201d needed during the pre-training, which arguably is not even true because you do need the rewards related to whether you have achieved a specific change in a feature!\nIn fact, it would be greatly appreciated if the \u201cfinal\u201d tasks could be expressed in a similar notation than the rest of the pre-training tasks, or vice-versa. As far as I understand, the pre-training tasks consist of making a certain feature fall into a certain subset of its possible values. Can\u2019t the final tasks, like \u201cmove the agent to the top right\u201d be also expressed in that form. The link between the two kinds of tasks needs to be much more explicit to be able to assess the relevance of this work.\n\nFinally, they only test their algorithm on Spritworld, which is a small discrete state-action space environment. Even if they try different kinds of tasks in this environment, more detailed analysis or more extensive experiments are needed to assess the benefits of the proposed approach.\nThis is particularly timely because their method relies on a discretization of some given features that represent the state, which will probably not be very practical in higher dimensional environments.\nFinally, I would like a comment on how this method interacts with discrete versus continuous action-state spaces.\n\nMisc comments:\n- Why do the authors introduce the terminology \u201cEndogenous RL\u201d, and then say it\u2019s the same as doing RL with intrinsic motivation? This seems like introducing a new name for the same concept, which seems pointless and confusing.\n- The connection with \u201clatent learning\u201d of Tolman 1984 is very unclear.\n- There\u2019s a \u201cRepresentation Learning\u201d section, but it\u2019s not clear at all whether any features are actually learned, or whether the features are actually hand-defined. Is the number of features n also hand-defined?\n- There might be a typo in the first sentence after equation (9): \u201cWhile \\phi_w is not guaranteed to be optimal with respect to \\phi_w\u201d.\n\nBecause of all these concerns, I suggest the paper to be weakly rejected. "}, "signatures": ["ICLR.cc/2020/Conference/Paper2175/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2175/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["crgrimm@umich.edu", "irinah@google.com", "andrebarreto@google.com", "teplyashin@google.com", "mwulfmeier@google.com", "thertweck@google.com", "raia@google.com", "baveja@google.com"], "title": "Disentangled Cumulants Help Successor Representations Transfer to New Tasks", "authors": ["Chris Grimm", "Irina Higgins", "Andre Barreto", "Denis Teplyashin", "Markus Wulfmeier", "Tim Hertweck", "Raia Hadsell", "Satinder Singh"], "pdf": "/pdf/c5e5066f2b70f0cf4e9f4631953ea191a87fd2a4.pdf", "TL;DR": "We show in the absence of external reward that agents can leverage knowledge from unsupervised control of latent features to solve downstream tasks and that when these latent features are disentangled superior performance is achieved.", "abstract": "Biological intelligence can learn to solve many diverse tasks in a data efficient manner by re-using basic knowledge and skills from one task to another. Furthermore, many of such skills are acquired through something called latent learning, where no explicit supervision for skill acquisition is provided. This is in contrast to the state-of-the-art reinforcement learning agents, which typically start learning each new task from scratch and struggle with knowledge transfer. In this paper we propose a principled way to learn and recombine a basis set of policies, which comes with certain guarantees on the coverage of the final task space. In particular, we construct a learning pipeline where an agent invests time to learn to perform intrinsically generated, goal-based tasks, and subsequently leverages this experience to quickly achieve a high level of performance on externally specified, often significantly more complex tasks through generalised policy improvement. We demonstrate both theoretically and empirically that such goal-based intrinsic tasks produce more transferable policies when the goals are specified in a space that exhibits a form of disentanglement. ", "keywords": ["reinforcement learning", "representation learning", "intrinsic reward", "intrinsic control", "endogenous", "generalized policy improvement", "successor features", "variational", "monet", "disentangled"], "paperhash": "grimm|disentangled_cumulants_help_successor_representations_transfer_to_new_tasks", "original_pdf": "/attachment/9b9a25d40c80adfee7362024cb569c6da0e0c750.pdf", "_bibtex": "@misc{\ngrimm2020disentangled,\ntitle={Disentangled Cumulants Help Successor Representations Transfer to New Tasks},\nauthor={Chris Grimm and Irina Higgins and Andre Barreto and Denis Teplyashin and Markus Wulfmeier and Tim Hertweck and Raia Hadsell and Satinder Singh},\nyear={2020},\nurl={https://openreview.net/forum?id=BylUMxSFwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BylUMxSFwS", "replyto": "BylUMxSFwS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2175/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2175/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575508729823, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2175/Reviewers"], "noninvitees": [], "tcdate": 1570237726625, "tmdate": 1575508729835, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2175/-/Official_Review"}}}], "count": 5}