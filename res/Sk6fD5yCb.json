{"notes": [{"tddate": null, "ddate": null, "tmdate": 1518730187432, "tcdate": 1509037845450, "number": 148, "cdate": 1518730187420, "id": "Sk6fD5yCb", "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "forum": "Sk6fD5yCb", "original": "BypMD910b", "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference"], "content": {"title": "Espresso: Efficient Forward Propagation for Binary Deep Neural Networks", "abstract": "  There are many applications scenarios for which the computational\n  performance and memory footprint of the prediction phase of Deep\n  Neural Networks (DNNs) need to be optimized. Binary Deep Neural\n  Networks (BDNNs) have been shown to be an effective way of achieving\n  this objective. In this paper, we show how Convolutional Neural\n  Networks (CNNs) can be implemented using binary\n  representations. Espresso is a compact, yet powerful\n  library written in C/CUDA that features all the functionalities\n  required for the forward propagation of CNNs, in a binary file less\n  than 400KB, without any external dependencies. Although it is mainly\n  designed to take advantage of massive GPU parallelism, Espresso also\n  provides an equivalent CPU implementation for CNNs. Espresso\n  provides special convolutional and dense layers for BCNNs,\n  leveraging bit-packing and bit-wise computations\n  for efficient execution. These techniques provide a speed-up of\n  matrix-multiplication routines, and at the same time, reduce memory\n  usage when storing parameters and activations. We experimentally\n  show that Espresso is significantly faster than existing\n  implementations of optimized binary neural networks (~ 2\n  orders of magnitude). Espresso is released under the Apache 2.0\n  license and is available at http://github.com/organization/project.", "pdf": "/pdf/5f3a59e74a0684045b2f6a725979a044e77e533b.pdf", "TL;DR": "state-of-the-art computational performance implementation of binary neural networks", "paperhash": "pedersoli|espresso_efficient_forward_propagation_for_binary_deep_neural_networks", "_bibtex": "@inproceedings{\npedersoli2018espresso,\ntitle={Espresso: Efficient Forward Propagation for Binary Deep Neural Networks},\nauthor={Fabrizio Pedersoli and George Tzanetakis and Andrea Tagliasacchi},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=Sk6fD5yCb},\n}", "keywords": ["binary deep neural networks", "optimized implementation", "bitwise computations"], "authors": ["Fabrizio Pedersoli", "George Tzanetakis", "Andrea Tagliasacchi"], "authorids": ["fpeder@uvic.ca", "gtzan@uvic.ca", "ataiya@uvic.ca"]}, "nonreaders": [], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1506717071958, "id": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Conference"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference"]}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"authors": {"required": false, "order": 1, "values-regex": ".*", "description": "Comma separated list of author names, as they appear in the paper."}, "authorids": {"required": false, "order": 2, "values-regex": ".*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "cdate": 1506717071958}}, "tauthor": "ICLR.cc/2018/Conference"}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1517260100439, "tcdate": 1517249226243, "number": 42, "cdate": 1517249226230, "id": "S1zRMkarf", "invitation": "ICLR.cc/2018/Conference/-/Acceptance_Decision", "forum": "Sk6fD5yCb", "replyto": "Sk6fD5yCb", "signatures": ["ICLR.cc/2018/Conference/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Program_Chairs"], "content": {"title": "ICLR 2018 Conference Acceptance Decision", "comment": "This paper describes a new library for forward propagation of binary CNNs. R1 for clarification on the contributions and novelty, which the authors provided. They subsequently updated their score. I think that optimized code with permissive licensing (as R2 points out) benefits the community. The paper will benefit those who decide to work with the library.", "decision": "Accept (Poster)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Espresso: Efficient Forward Propagation for Binary Deep Neural Networks", "abstract": "  There are many applications scenarios for which the computational\n  performance and memory footprint of the prediction phase of Deep\n  Neural Networks (DNNs) need to be optimized. Binary Deep Neural\n  Networks (BDNNs) have been shown to be an effective way of achieving\n  this objective. In this paper, we show how Convolutional Neural\n  Networks (CNNs) can be implemented using binary\n  representations. Espresso is a compact, yet powerful\n  library written in C/CUDA that features all the functionalities\n  required for the forward propagation of CNNs, in a binary file less\n  than 400KB, without any external dependencies. Although it is mainly\n  designed to take advantage of massive GPU parallelism, Espresso also\n  provides an equivalent CPU implementation for CNNs. Espresso\n  provides special convolutional and dense layers for BCNNs,\n  leveraging bit-packing and bit-wise computations\n  for efficient execution. These techniques provide a speed-up of\n  matrix-multiplication routines, and at the same time, reduce memory\n  usage when storing parameters and activations. We experimentally\n  show that Espresso is significantly faster than existing\n  implementations of optimized binary neural networks (~ 2\n  orders of magnitude). Espresso is released under the Apache 2.0\n  license and is available at http://github.com/organization/project.", "pdf": "/pdf/5f3a59e74a0684045b2f6a725979a044e77e533b.pdf", "TL;DR": "state-of-the-art computational performance implementation of binary neural networks", "paperhash": "pedersoli|espresso_efficient_forward_propagation_for_binary_deep_neural_networks", "_bibtex": "@inproceedings{\npedersoli2018espresso,\ntitle={Espresso: Efficient Forward Propagation for Binary Deep Neural Networks},\nauthor={Fabrizio Pedersoli and George Tzanetakis and Andrea Tagliasacchi},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=Sk6fD5yCb},\n}", "keywords": ["binary deep neural networks", "optimized implementation", "bitwise computations"], "authors": ["Fabrizio Pedersoli", "George Tzanetakis", "Andrea Tagliasacchi"], "authorids": ["fpeder@uvic.ca", "gtzan@uvic.ca", "ataiya@uvic.ca"]}, "tags": [], "invitation": {"id": "ICLR.cc/2018/Conference/-/Acceptance_Decision", "rdate": null, "ddate": null, "expdate": 1541175629000, "duedate": null, "tmdate": 1541177635767, "tddate": null, "super": null, "final": null, "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": {"values": ["ICLR.cc/2018/Conference/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Conference/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Conference Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": [], "noninvitees": [], "writers": ["ICLR.cc/2018/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1541177635767}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515963637560, "tcdate": 1511759958537, "number": 2, "cdate": 1511759958537, "id": "HJCLeXtgM", "invitation": "ICLR.cc/2018/Conference/-/Paper148/Official_Review", "forum": "Sk6fD5yCb", "replyto": "Sk6fD5yCb", "signatures": ["ICLR.cc/2018/Conference/Paper148/AnonReviewer1"], "readers": ["everyone"], "content": {"title": "Review revised", "rating": "6: Marginally above acceptance threshold", "review": "The paper presents a library written in C/CUDA that features all the functionalities required for the forward propagation of BCNNs. The library is significantly faster than existing implementations of optimized binary neural networks (\u2248 2 orders of magnitude), and will be released on github.\n\nBCNNs have been able to perform well on large-scale datasets with increased speed and decreased  energy consumption, and implementing efficient kernels for them can be very useful for mobile applications. The paper describes three implementations CPU, GPU and GPU_opt, but it is not entirely clear what the differences are and why GPU_opt is faster than GPU implementation.\n\nAre BDNN and BCNN used to mean the same concept? If yes, could you please use only one of them?\n\nThe subsection title \u201cTraining Espresso\u201d should be changed to \u201cConverting a network to Espresso\u201d, or \u201cTraining a network for Espresso\u201d.\n\nWhat is the main difference between GPU and GPU_opt implementations?\n\nThe unrolling and lifting operations are shown in Figure 2. Isn\u2019t accelerating convolution by this method a very well known one which is implemented in many deep learning frameworks for both CPU and GPU?\n\nWhat is the main contribution that makes the framework here faster than the other compared work? In Figure1, Espresso implementations are compared with other implementations in (a)dense binary matrix multiplication and (b)BMLP and not (c)BCNN. Can others ( BinaryNet\n(Hubara et al., 2016) or Intel Nervana/neon (NervanaSystems)) run CNNs?\n\n6.2 MULTI-LAYER PERCEPTRON ON MNIST \u2013 FIGURE 1B AND FIGURE 1E. It should be Figure 1d instead of 1e? \n\nAll in all, the novelty in this paper is not very clear to me. Is it bit-packing?\n\n\nUPDATE: \nThank you for the revision and clarifications. I increase my rating to 6.\n\n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Espresso: Efficient Forward Propagation for Binary Deep Neural Networks", "abstract": "  There are many applications scenarios for which the computational\n  performance and memory footprint of the prediction phase of Deep\n  Neural Networks (DNNs) need to be optimized. Binary Deep Neural\n  Networks (BDNNs) have been shown to be an effective way of achieving\n  this objective. In this paper, we show how Convolutional Neural\n  Networks (CNNs) can be implemented using binary\n  representations. Espresso is a compact, yet powerful\n  library written in C/CUDA that features all the functionalities\n  required for the forward propagation of CNNs, in a binary file less\n  than 400KB, without any external dependencies. Although it is mainly\n  designed to take advantage of massive GPU parallelism, Espresso also\n  provides an equivalent CPU implementation for CNNs. Espresso\n  provides special convolutional and dense layers for BCNNs,\n  leveraging bit-packing and bit-wise computations\n  for efficient execution. These techniques provide a speed-up of\n  matrix-multiplication routines, and at the same time, reduce memory\n  usage when storing parameters and activations. We experimentally\n  show that Espresso is significantly faster than existing\n  implementations of optimized binary neural networks (~ 2\n  orders of magnitude). Espresso is released under the Apache 2.0\n  license and is available at http://github.com/organization/project.", "pdf": "/pdf/5f3a59e74a0684045b2f6a725979a044e77e533b.pdf", "TL;DR": "state-of-the-art computational performance implementation of binary neural networks", "paperhash": "pedersoli|espresso_efficient_forward_propagation_for_binary_deep_neural_networks", "_bibtex": "@inproceedings{\npedersoli2018espresso,\ntitle={Espresso: Efficient Forward Propagation for Binary Deep Neural Networks},\nauthor={Fabrizio Pedersoli and George Tzanetakis and Andrea Tagliasacchi},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=Sk6fD5yCb},\n}", "keywords": ["binary deep neural networks", "optimized implementation", "bitwise computations"], "authors": ["Fabrizio Pedersoli", "George Tzanetakis", "Andrea Tagliasacchi"], "authorids": ["fpeder@uvic.ca", "gtzan@uvic.ca", "ataiya@uvic.ca"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642398951, "id": "ICLR.cc/2018/Conference/-/Paper148/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper148/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper148/AnonReviewer2", "ICLR.cc/2018/Conference/Paper148/AnonReviewer1", "ICLR.cc/2018/Conference/Paper148/AnonReviewer3"], "reply": {"forum": "Sk6fD5yCb", "replyto": "Sk6fD5yCb", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper148/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642398951}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642399040, "tcdate": 1510781734271, "number": 1, "cdate": 1510781734271, "id": "SyRQ7Vq1G", "invitation": "ICLR.cc/2018/Conference/-/Paper148/Official_Review", "forum": "Sk6fD5yCb", "replyto": "Sk6fD5yCb", "signatures": ["ICLR.cc/2018/Conference/Paper148/AnonReviewer2"], "readers": ["everyone"], "content": {"title": "A full implementation of binary CNN with code", "rating": "7: Good paper, accept", "review": "This paper builds on Binary-NET [Hubara et al. 2016] and expands it to CNN architectures. It also provides optimizations that substantially improve the speed of the forward pass: packing layer bits along the channel dimension, pre-allocation of CUDA resources and binary-optimized CUDA kernels for matrix multiplications. The authors compare their framework to BinaryNET and Nervana/Neon and show a 8x speedup for 8092 matrix-matrix multiplication and a 68x speedup for MLP networks. For CNN, they a speedup of 5x is obtained from the GPU to binary-optimizimed-GPU. A gain in memory size of 32x is also achieved by using binary weight and activation during the forward pass.\n\nThe main contribution of this paper is an optimized code for Binary CNN. The authors provide the code with permissive licensing. As is often the case with such comparisons, it is hard to disentangle from where exactly come the speedups. The authors should provide a table with actual numbers instead of the hard-to-read bar graphs. Otherwise the paper is well written and relatively clear, although the flow is somewhat unwieldy. \n\nOverall, i think it makes a good contribution to a field that is gaining importance for mobile and embedded applications of deep convnets. I think it is a good fit for a poster.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Espresso: Efficient Forward Propagation for Binary Deep Neural Networks", "abstract": "  There are many applications scenarios for which the computational\n  performance and memory footprint of the prediction phase of Deep\n  Neural Networks (DNNs) need to be optimized. Binary Deep Neural\n  Networks (BDNNs) have been shown to be an effective way of achieving\n  this objective. In this paper, we show how Convolutional Neural\n  Networks (CNNs) can be implemented using binary\n  representations. Espresso is a compact, yet powerful\n  library written in C/CUDA that features all the functionalities\n  required for the forward propagation of CNNs, in a binary file less\n  than 400KB, without any external dependencies. Although it is mainly\n  designed to take advantage of massive GPU parallelism, Espresso also\n  provides an equivalent CPU implementation for CNNs. Espresso\n  provides special convolutional and dense layers for BCNNs,\n  leveraging bit-packing and bit-wise computations\n  for efficient execution. These techniques provide a speed-up of\n  matrix-multiplication routines, and at the same time, reduce memory\n  usage when storing parameters and activations. We experimentally\n  show that Espresso is significantly faster than existing\n  implementations of optimized binary neural networks (~ 2\n  orders of magnitude). Espresso is released under the Apache 2.0\n  license and is available at http://github.com/organization/project.", "pdf": "/pdf/5f3a59e74a0684045b2f6a725979a044e77e533b.pdf", "TL;DR": "state-of-the-art computational performance implementation of binary neural networks", "paperhash": "pedersoli|espresso_efficient_forward_propagation_for_binary_deep_neural_networks", "_bibtex": "@inproceedings{\npedersoli2018espresso,\ntitle={Espresso: Efficient Forward Propagation for Binary Deep Neural Networks},\nauthor={Fabrizio Pedersoli and George Tzanetakis and Andrea Tagliasacchi},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=Sk6fD5yCb},\n}", "keywords": ["binary deep neural networks", "optimized implementation", "bitwise computations"], "authors": ["Fabrizio Pedersoli", "George Tzanetakis", "Andrea Tagliasacchi"], "authorids": ["fpeder@uvic.ca", "gtzan@uvic.ca", "ataiya@uvic.ca"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642398951, "id": "ICLR.cc/2018/Conference/-/Paper148/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper148/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper148/AnonReviewer2", "ICLR.cc/2018/Conference/Paper148/AnonReviewer1", "ICLR.cc/2018/Conference/Paper148/AnonReviewer3"], "reply": {"forum": "Sk6fD5yCb", "replyto": "Sk6fD5yCb", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper148/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642398951}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642398965, "tcdate": 1511983962815, "number": 3, "cdate": 1511983962815, "id": "HymwoY3lM", "invitation": "ICLR.cc/2018/Conference/-/Paper148/Official_Review", "forum": "Sk6fD5yCb", "replyto": "Sk6fD5yCb", "signatures": ["ICLR.cc/2018/Conference/Paper148/AnonReviewer3"], "readers": ["everyone"], "content": {"title": "fast implementation of binary networks", "rating": "7: Good paper, accept", "review": "The paper presents an implementation strategy (with code link anonymized for review) for fast computations of binary forward inference. The paper makes the approach seem straightforward (clever?) and there has been lots of work on fast inference of quantized, low-bit-width neural networks, but if indeed the implementation is significantly faster than commercial alternatives (e.g. from Intel) then I expect the authors have made a novel and useful contribution.\n\nThe paper is written clearly, but I am not an expert in alternative approaches in this area.", "confidence": "1: The reviewer's evaluation is an educated guess"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Espresso: Efficient Forward Propagation for Binary Deep Neural Networks", "abstract": "  There are many applications scenarios for which the computational\n  performance and memory footprint of the prediction phase of Deep\n  Neural Networks (DNNs) need to be optimized. Binary Deep Neural\n  Networks (BDNNs) have been shown to be an effective way of achieving\n  this objective. In this paper, we show how Convolutional Neural\n  Networks (CNNs) can be implemented using binary\n  representations. Espresso is a compact, yet powerful\n  library written in C/CUDA that features all the functionalities\n  required for the forward propagation of CNNs, in a binary file less\n  than 400KB, without any external dependencies. Although it is mainly\n  designed to take advantage of massive GPU parallelism, Espresso also\n  provides an equivalent CPU implementation for CNNs. Espresso\n  provides special convolutional and dense layers for BCNNs,\n  leveraging bit-packing and bit-wise computations\n  for efficient execution. These techniques provide a speed-up of\n  matrix-multiplication routines, and at the same time, reduce memory\n  usage when storing parameters and activations. We experimentally\n  show that Espresso is significantly faster than existing\n  implementations of optimized binary neural networks (~ 2\n  orders of magnitude). Espresso is released under the Apache 2.0\n  license and is available at http://github.com/organization/project.", "pdf": "/pdf/5f3a59e74a0684045b2f6a725979a044e77e533b.pdf", "TL;DR": "state-of-the-art computational performance implementation of binary neural networks", "paperhash": "pedersoli|espresso_efficient_forward_propagation_for_binary_deep_neural_networks", "_bibtex": "@inproceedings{\npedersoli2018espresso,\ntitle={Espresso: Efficient Forward Propagation for Binary Deep Neural Networks},\nauthor={Fabrizio Pedersoli and George Tzanetakis and Andrea Tagliasacchi},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=Sk6fD5yCb},\n}", "keywords": ["binary deep neural networks", "optimized implementation", "bitwise computations"], "authors": ["Fabrizio Pedersoli", "George Tzanetakis", "Andrea Tagliasacchi"], "authorids": ["fpeder@uvic.ca", "gtzan@uvic.ca", "ataiya@uvic.ca"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642398951, "id": "ICLR.cc/2018/Conference/-/Paper148/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper148/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper148/AnonReviewer2", "ICLR.cc/2018/Conference/Paper148/AnonReviewer1", "ICLR.cc/2018/Conference/Paper148/AnonReviewer3"], "reply": {"forum": "Sk6fD5yCb", "replyto": "Sk6fD5yCb", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper148/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642398951}}}], "count": 5}