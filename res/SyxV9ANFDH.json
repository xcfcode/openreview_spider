{"notes": [{"id": "SyxV9ANFDH", "original": "SJxFRt_OPB", "number": 1279, "cdate": 1569439372039, "ddate": null, "tcdate": 1569439372039, "tmdate": 1583912030791, "tddate": null, "forum": "SyxV9ANFDH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Economy Statistical Recurrent Units For Inferring Nonlinear Granger Causality", "authors": ["Saurabh Khanna", "Vincent Y. F. Tan"], "authorids": ["elesaur@nus.edu.sg", "vtan@nus.edu.sg"], "keywords": ["Recurrent neural networks", "Granger causality", "Causal inference", "Statistical Recurrent Unit"], "TL;DR": "A new recurrent neural network architecture for detecting pairwise Granger causality between nonlinearly interacting time series. ", "abstract": "Granger causality is a widely-used criterion for analyzing interactions in large-scale networks. As most physical interactions are inherently nonlinear, we consider the problem of inferring the existence of pairwise Granger causality between nonlinearly interacting stochastic processes from their time series measurements. Our proposed approach relies on modeling the embedded nonlinearities in the measurements using a component-wise time series prediction model based on Statistical Recurrent Units (SRUs). We make a case that the network topology of Granger causal relations is directly inferrable from a structured sparse estimate of the internal parameters of the SRU networks trained to predict the processes\u2019 time series measurements. We propose a variant of SRU, called economy-SRU, which, by design has considerably fewer trainable parameters, and therefore less prone to overfitting. The economy-SRU computes a low-dimensional sketch of its high-dimensional hidden state in the form of random projections to generate the feedback for its recurrent processing. Additionally, the internal weight parameters of the economy-SRU are strategically regularized in a group-wise manner to facilitate the proposed network in extracting meaningful predictive features that are highly time-localized to mimic real-world causal events. Extensive experiments are carried out to demonstrate that the proposed economy-SRU based time series prediction model outperforms the MLP, LSTM and attention-gated CNN-based time series models considered previously for inferring Granger causality. ", "pdf": "/pdf/b77f50150d3f24a52b70cfe3b4549acc0bda1e48.pdf", "code": "https://github.com/sakhanna/SRU_for_GCI", "paperhash": "khanna|economy_statistical_recurrent_units_for_inferring_nonlinear_granger_causality", "_bibtex": "@inproceedings{\nKhanna2020Economy,\ntitle={Economy Statistical Recurrent Units For Inferring Nonlinear Granger Causality},\nauthor={Saurabh Khanna and Vincent Y. F. Tan},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SyxV9ANFDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/9e458e303b22faf8a757dad874b88f02f1e5d5a7.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "Qt_JZF7kWT", "original": null, "number": 1, "cdate": 1576798719207, "ddate": null, "tcdate": 1576798719207, "tmdate": 1576800917319, "tddate": null, "forum": "SyxV9ANFDH", "replyto": "SyxV9ANFDH", "invitation": "ICLR.cc/2020/Conference/Paper1279/-/Decision", "content": {"decision": "Accept (Poster)", "comment": "The authors propose a modification of the statistical recurrent unit for modelling mutliple time series and show that it can be very useful in practice for identifying granger causality when the time series are non-linearly related. The contributions are primarily conceptual and empirical. The reviewers agree that this is a useful contribution in the causality literature.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Economy Statistical Recurrent Units For Inferring Nonlinear Granger Causality", "authors": ["Saurabh Khanna", "Vincent Y. F. Tan"], "authorids": ["elesaur@nus.edu.sg", "vtan@nus.edu.sg"], "keywords": ["Recurrent neural networks", "Granger causality", "Causal inference", "Statistical Recurrent Unit"], "TL;DR": "A new recurrent neural network architecture for detecting pairwise Granger causality between nonlinearly interacting time series. ", "abstract": "Granger causality is a widely-used criterion for analyzing interactions in large-scale networks. As most physical interactions are inherently nonlinear, we consider the problem of inferring the existence of pairwise Granger causality between nonlinearly interacting stochastic processes from their time series measurements. Our proposed approach relies on modeling the embedded nonlinearities in the measurements using a component-wise time series prediction model based on Statistical Recurrent Units (SRUs). We make a case that the network topology of Granger causal relations is directly inferrable from a structured sparse estimate of the internal parameters of the SRU networks trained to predict the processes\u2019 time series measurements. We propose a variant of SRU, called economy-SRU, which, by design has considerably fewer trainable parameters, and therefore less prone to overfitting. The economy-SRU computes a low-dimensional sketch of its high-dimensional hidden state in the form of random projections to generate the feedback for its recurrent processing. Additionally, the internal weight parameters of the economy-SRU are strategically regularized in a group-wise manner to facilitate the proposed network in extracting meaningful predictive features that are highly time-localized to mimic real-world causal events. Extensive experiments are carried out to demonstrate that the proposed economy-SRU based time series prediction model outperforms the MLP, LSTM and attention-gated CNN-based time series models considered previously for inferring Granger causality. ", "pdf": "/pdf/b77f50150d3f24a52b70cfe3b4549acc0bda1e48.pdf", "code": "https://github.com/sakhanna/SRU_for_GCI", "paperhash": "khanna|economy_statistical_recurrent_units_for_inferring_nonlinear_granger_causality", "_bibtex": "@inproceedings{\nKhanna2020Economy,\ntitle={Economy Statistical Recurrent Units For Inferring Nonlinear Granger Causality},\nauthor={Saurabh Khanna and Vincent Y. F. Tan},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SyxV9ANFDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/9e458e303b22faf8a757dad874b88f02f1e5d5a7.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "SyxV9ANFDH", "replyto": "SyxV9ANFDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795711921, "tmdate": 1576800261202, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1279/-/Decision"}}}, {"id": "HJl3kmaisB", "original": null, "number": 6, "cdate": 1573798627711, "ddate": null, "tcdate": 1573798627711, "tmdate": 1573798627711, "tddate": null, "forum": "SyxV9ANFDH", "replyto": "SyxV9ANFDH", "invitation": "ICLR.cc/2020/Conference/Paper1279/-/Official_Comment", "content": {"title": "Summary of revision", "comment": "We have uploaded a revision of the paper with the following updates:\n\n1. Added appendix (C.1) containing new experiments to compare different design choices for the encoder $D_{r}$ in the eSRU design, as part of our response to a comment by reviewer #1. \n\n2.  Added a sentence (in page-6, last line) to motivate one of the SRU modifications as part of our response to a comment by reviewer #2.\n\n3. Added an ablation study (as appendix (C.2)) to demonstrate the impact of group-sparse regularization of the weight matrix $W_{o}$ on eSRU's performance, as advised by reviewer #3.  \n\n4. Added a footnote in page-6 recommending the use of a common encoder $D_{r}$ across all SRUs predicting the different time series components, as part of our response to a comment by reviewer #1.    \n\n5. Fixed two typos in Appendix G: \n     a.  the 'batch-size' entries for NetSim and Dream-3 were mistakenly swapped in Table 7-11. We fixed it. \n     b.  In table-9, we corrected the tuned value of '#Layers' hyperparameter reported for the attention-gated CNN model.       \n  "}, "signatures": ["ICLR.cc/2020/Conference/Paper1279/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1279/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Economy Statistical Recurrent Units For Inferring Nonlinear Granger Causality", "authors": ["Saurabh Khanna", "Vincent Y. F. Tan"], "authorids": ["elesaur@nus.edu.sg", "vtan@nus.edu.sg"], "keywords": ["Recurrent neural networks", "Granger causality", "Causal inference", "Statistical Recurrent Unit"], "TL;DR": "A new recurrent neural network architecture for detecting pairwise Granger causality between nonlinearly interacting time series. ", "abstract": "Granger causality is a widely-used criterion for analyzing interactions in large-scale networks. As most physical interactions are inherently nonlinear, we consider the problem of inferring the existence of pairwise Granger causality between nonlinearly interacting stochastic processes from their time series measurements. Our proposed approach relies on modeling the embedded nonlinearities in the measurements using a component-wise time series prediction model based on Statistical Recurrent Units (SRUs). We make a case that the network topology of Granger causal relations is directly inferrable from a structured sparse estimate of the internal parameters of the SRU networks trained to predict the processes\u2019 time series measurements. We propose a variant of SRU, called economy-SRU, which, by design has considerably fewer trainable parameters, and therefore less prone to overfitting. The economy-SRU computes a low-dimensional sketch of its high-dimensional hidden state in the form of random projections to generate the feedback for its recurrent processing. Additionally, the internal weight parameters of the economy-SRU are strategically regularized in a group-wise manner to facilitate the proposed network in extracting meaningful predictive features that are highly time-localized to mimic real-world causal events. Extensive experiments are carried out to demonstrate that the proposed economy-SRU based time series prediction model outperforms the MLP, LSTM and attention-gated CNN-based time series models considered previously for inferring Granger causality. ", "pdf": "/pdf/b77f50150d3f24a52b70cfe3b4549acc0bda1e48.pdf", "code": "https://github.com/sakhanna/SRU_for_GCI", "paperhash": "khanna|economy_statistical_recurrent_units_for_inferring_nonlinear_granger_causality", "_bibtex": "@inproceedings{\nKhanna2020Economy,\ntitle={Economy Statistical Recurrent Units For Inferring Nonlinear Granger Causality},\nauthor={Saurabh Khanna and Vincent Y. F. Tan},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SyxV9ANFDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/9e458e303b22faf8a757dad874b88f02f1e5d5a7.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SyxV9ANFDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1279/Authors", "ICLR.cc/2020/Conference/Paper1279/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1279/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1279/Reviewers", "ICLR.cc/2020/Conference/Paper1279/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1279/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1279/Authors|ICLR.cc/2020/Conference/Paper1279/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504158447, "tmdate": 1576860532168, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1279/Authors", "ICLR.cc/2020/Conference/Paper1279/Reviewers", "ICLR.cc/2020/Conference/Paper1279/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1279/-/Official_Comment"}}}, {"id": "Syxgfyocjr", "original": null, "number": 5, "cdate": 1573723912111, "ddate": null, "tcdate": 1573723912111, "tmdate": 1573724809444, "tddate": null, "forum": "SyxV9ANFDH", "replyto": "HyxuKq0CtB", "invitation": "ICLR.cc/2020/Conference/Paper1279/-/Official_Comment", "content": {"title": "Response to reviewer #1 (Part-2) ", "comment": "Continuing from part-1...\n\n>> Regarding the scarification of the output matrix: the number of parameters is effectively reduced but the computational requirements are still the same. Did the authors try any explicit methods, maybe matrix factorization, to exploit the very specific sparse structure of $W_{o}$ and reduce the number of operations and parameters?\n\n[Reply] \nWe thank the reviewer for this interesting suggestion. Splitting the weight matrix $W_{o}$ into low-rank factor matrices will certainly reduce the number of trainable parameters, however, the goal here is not just limited to reducing the number of trainable parameters but also to promote a specific mixing of the multi-scale summary statistics in order to learn time localized predictive features. While, one can certainly serve the latter goal by group-sparse regularizing the coefficients in the right-sided factor matrix ($Z$ in $W_{o} = H Z$), there is an inadvertent loss in expressibility of the predictive features in the feature vector $o_{t}$ which is now restricted to lie in an $r$-dimensional subspace ($r$ is the rank of the factor matrices H and Z, which by design is typically much smaller than $d_{o}$). Furthermore, the unnecessary coupling/mixing due to the dense factor matrix $H$ results in generation of correlated features in $o_{t}$ which will be difficult to interpret.\n \nIn our opinion, the proposed strategy based on the use of a mixed $\\ell_{1}-\\ell_{2}$ norm penalty is the most straightforward way to implement the desired mixing of the summary statistics\u2019 components in order to promote learning of time-localized predictive feature/component in $o_{t}$. The proposed regularization strategy also preserves the interpretability of the predictive features in the sense that for the $i^{\\text{th}}$ predictive feature, its sensitivity to past measurements can be profiled across time by analyzing the weights in the active groups in the $i^{\\text{th}}$ row of $W_{o}$.       \n\n\n\n>> Finally, while the results are undeniable, just observing the non-zero columns of Win is as I understand it a mostly ad hoc rule. It would have been interesting to empirically verify its validity by contrasting it with the results given by following equation 2\n\n[Reply] \nWe would like to clarify that the strategy of deducing the Granger causal components from the sparse column support of the input weight matrices $W_{in}^{(i)}$ in the individual eSRU models is not ad hoc but  in fact directly motivated from the condition in equation-2 as explained below. \n\nUnder the assumption that the true Granger causal network is sparsely connected, the function invariance condition in equation-2 translates to input-selectivity of the unknown generative function $f_{i}$ , i.e., $f_{i}$\u2019s output depends on the input past measurements of only a small subset of the total n component series. This viewpoint guides us to learn an economy-SRU model which closely approximates $f_{i}$\u2019s output, while simultaneously being input-selective. From equation-3b, it is easy to see that the column support of the input weight matrix $W_{in}^{(i)}$ dictates exactly which subset of the n components series ultimately play a role in the generation of the recurrent statistics and the predicted future samples of the $i^{\\text{th}}$ series. In other words, column sparsity of $W_{in}^{(i)}$  is equivalent to input-selectivity of the $i^{\\text{th}}$ SRU model. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1279/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1279/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Economy Statistical Recurrent Units For Inferring Nonlinear Granger Causality", "authors": ["Saurabh Khanna", "Vincent Y. F. Tan"], "authorids": ["elesaur@nus.edu.sg", "vtan@nus.edu.sg"], "keywords": ["Recurrent neural networks", "Granger causality", "Causal inference", "Statistical Recurrent Unit"], "TL;DR": "A new recurrent neural network architecture for detecting pairwise Granger causality between nonlinearly interacting time series. ", "abstract": "Granger causality is a widely-used criterion for analyzing interactions in large-scale networks. As most physical interactions are inherently nonlinear, we consider the problem of inferring the existence of pairwise Granger causality between nonlinearly interacting stochastic processes from their time series measurements. Our proposed approach relies on modeling the embedded nonlinearities in the measurements using a component-wise time series prediction model based on Statistical Recurrent Units (SRUs). We make a case that the network topology of Granger causal relations is directly inferrable from a structured sparse estimate of the internal parameters of the SRU networks trained to predict the processes\u2019 time series measurements. We propose a variant of SRU, called economy-SRU, which, by design has considerably fewer trainable parameters, and therefore less prone to overfitting. The economy-SRU computes a low-dimensional sketch of its high-dimensional hidden state in the form of random projections to generate the feedback for its recurrent processing. Additionally, the internal weight parameters of the economy-SRU are strategically regularized in a group-wise manner to facilitate the proposed network in extracting meaningful predictive features that are highly time-localized to mimic real-world causal events. Extensive experiments are carried out to demonstrate that the proposed economy-SRU based time series prediction model outperforms the MLP, LSTM and attention-gated CNN-based time series models considered previously for inferring Granger causality. ", "pdf": "/pdf/b77f50150d3f24a52b70cfe3b4549acc0bda1e48.pdf", "code": "https://github.com/sakhanna/SRU_for_GCI", "paperhash": "khanna|economy_statistical_recurrent_units_for_inferring_nonlinear_granger_causality", "_bibtex": "@inproceedings{\nKhanna2020Economy,\ntitle={Economy Statistical Recurrent Units For Inferring Nonlinear Granger Causality},\nauthor={Saurabh Khanna and Vincent Y. F. Tan},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SyxV9ANFDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/9e458e303b22faf8a757dad874b88f02f1e5d5a7.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SyxV9ANFDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1279/Authors", "ICLR.cc/2020/Conference/Paper1279/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1279/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1279/Reviewers", "ICLR.cc/2020/Conference/Paper1279/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1279/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1279/Authors|ICLR.cc/2020/Conference/Paper1279/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504158447, "tmdate": 1576860532168, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1279/Authors", "ICLR.cc/2020/Conference/Paper1279/Reviewers", "ICLR.cc/2020/Conference/Paper1279/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1279/-/Official_Comment"}}}, {"id": "B1lg-5c9jH", "original": null, "number": 3, "cdate": 1573722615973, "ddate": null, "tcdate": 1573722615973, "tmdate": 1573723276797, "tddate": null, "forum": "SyxV9ANFDH", "replyto": "HyxuKq0CtB", "invitation": "ICLR.cc/2020/Conference/Paper1279/-/Official_Comment", "content": {"title": "Response to reviewer #1 (Part-1)", "comment": " We thank the reviewer for the detailed comments and suggestions. Please find our point by point responses below.\n\n>> First, regarding the random projections, have the authors tried learning the projections? While it increases the risk of overfitting it is possible, with enough regularization, that it may help learn good representations.\n\n[Reply] \nIt is difficult to surmise up front if learning the encoder $D_{r}$ as trainable parameters will lead to better performance compared to the proposed approach wherein $D_{r}$ is taken to be a fixed Gaussian map.  To gain clarity on this, we conducted an ablation study to gauge the relative performance of these two implementations of the eSRU model. The following table reports the average AUROC obtained for different datasets.\n\n Dataset:                                         Randomly constructed                 Encoder Dr as trainable \n                                                          encoder Dr                                       parameters\n========================================================================\nLorenz (T = 250, F = 10)\t              0.95 \u00b1 0.02                                      0.97 \u00b1 0.01\nLorenz (T = 500, F = 10) \t              0.98 \u00b1 0.01                                      0.99 \u00b1 0.0\nLorenz (T = 250, F = 40) \t              0.99 \u00b1 0.0                                        0.98 \u00b1 0.01\nLorenz (T = 500, F = 40) \t              1.0 \u00b1 0                                             1.0 \u00b1 0.0\nVAR (T = 500)                                      0.93 \u00b1 0.05                                      0.91 \u00b1 0.04\nVAR (T = 1000)                                    0.98 \u00b1 0.01                                      0.98 \u00b1 0.01\nNetSim                                                0.84 \u00b1 0.03                                      0.80 \u00b1 0.02\n\nWe find that the two approaches are statistically tied in performance, which suggests that the random projections are able to successfully distill the necessary information from the high-dimensional sufficient statistics required for generating the feedback. Therefore, our recommendation for the final eSRU design is to avoid learning the encoder matrix $D_{r}$ as trainable parameters and instead use a randomly initialized $D_{r}$ to reduce the overall training complexity.\n\nIn the revised manuscript, we have added a new appendix (C.1) which discusses the above ablation study in more detail. \n\n\n>> Secondly, as I understand it a single projection is drawn at random for every component. While we know from Johnson-Lindenstrauss's Lemma that this will in average be a good strategy, how stable is the model to spurious projections?\n\n[Reply] \nGood point! We agree with the reviewer that the strategy of drawing $D_{r}$ from a Gaussian ensemble may occasionally result in spurious instantiations which are characteristically bad JL transforms. However, the probability that a randomly drawn $D_{r}$ is a bad JL transform can be made arbitrary small by choosing a sufficiently high value for the dimension $d_{r}^{\\prime}$.  \n\nNonetheless, the issue can be mitigated significantly by using the *same* encoder $D_{r}$ for each of the n SRUs predicting the distinct time series components. By doing so, the probability that $D_{r}$ (now drawn only once from a Gaussian ensemble) turns out to be a bad JL transform reduces by n-fold, where n is the number of time series. \n\nThe revised manuscript now includes a footnote on page-6 highlighting this issue and recommending the use of a common encoder $D_{r}$ across the n SRUs.\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1279/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1279/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Economy Statistical Recurrent Units For Inferring Nonlinear Granger Causality", "authors": ["Saurabh Khanna", "Vincent Y. F. Tan"], "authorids": ["elesaur@nus.edu.sg", "vtan@nus.edu.sg"], "keywords": ["Recurrent neural networks", "Granger causality", "Causal inference", "Statistical Recurrent Unit"], "TL;DR": "A new recurrent neural network architecture for detecting pairwise Granger causality between nonlinearly interacting time series. ", "abstract": "Granger causality is a widely-used criterion for analyzing interactions in large-scale networks. As most physical interactions are inherently nonlinear, we consider the problem of inferring the existence of pairwise Granger causality between nonlinearly interacting stochastic processes from their time series measurements. Our proposed approach relies on modeling the embedded nonlinearities in the measurements using a component-wise time series prediction model based on Statistical Recurrent Units (SRUs). We make a case that the network topology of Granger causal relations is directly inferrable from a structured sparse estimate of the internal parameters of the SRU networks trained to predict the processes\u2019 time series measurements. We propose a variant of SRU, called economy-SRU, which, by design has considerably fewer trainable parameters, and therefore less prone to overfitting. The economy-SRU computes a low-dimensional sketch of its high-dimensional hidden state in the form of random projections to generate the feedback for its recurrent processing. Additionally, the internal weight parameters of the economy-SRU are strategically regularized in a group-wise manner to facilitate the proposed network in extracting meaningful predictive features that are highly time-localized to mimic real-world causal events. Extensive experiments are carried out to demonstrate that the proposed economy-SRU based time series prediction model outperforms the MLP, LSTM and attention-gated CNN-based time series models considered previously for inferring Granger causality. ", "pdf": "/pdf/b77f50150d3f24a52b70cfe3b4549acc0bda1e48.pdf", "code": "https://github.com/sakhanna/SRU_for_GCI", "paperhash": "khanna|economy_statistical_recurrent_units_for_inferring_nonlinear_granger_causality", "_bibtex": "@inproceedings{\nKhanna2020Economy,\ntitle={Economy Statistical Recurrent Units For Inferring Nonlinear Granger Causality},\nauthor={Saurabh Khanna and Vincent Y. F. Tan},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SyxV9ANFDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/9e458e303b22faf8a757dad874b88f02f1e5d5a7.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SyxV9ANFDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1279/Authors", "ICLR.cc/2020/Conference/Paper1279/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1279/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1279/Reviewers", "ICLR.cc/2020/Conference/Paper1279/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1279/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1279/Authors|ICLR.cc/2020/Conference/Paper1279/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504158447, "tmdate": 1576860532168, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1279/Authors", "ICLR.cc/2020/Conference/Paper1279/Reviewers", "ICLR.cc/2020/Conference/Paper1279/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1279/-/Official_Comment"}}}, {"id": "HkgqsvqqoS", "original": null, "number": 2, "cdate": 1573722017981, "ddate": null, "tcdate": 1573722017981, "tmdate": 1573722017981, "tddate": null, "forum": "SyxV9ANFDH", "replyto": "rJgVWTWJ5S", "invitation": "ICLR.cc/2020/Conference/Paper1279/-/Official_Comment", "content": {"title": "Response to reviewer #2", "comment": "We thank the reviewer for the useful feedback. Please find our pointwise responses below.\n\n>> They compared the performance with existing models with MLP/LSTM and show some gains in a few examples (but not all.) \n\n[Reply] \nWe would like to emphasize here that the proposed economy-SRU model is the top performing model for 9 out of the 12 datasets, and among the top-2 models for 11 out of the 12 datasets considered in our experiments. The tables below provide a detailed breakdown of the top performing models. \n         \nModel:                                            In top-1     In top-2\n=============================================\nMLP                                                   1/12         9/12\nLSTM                                                 0/12         1/12\nAttention Gated-CNN (TCDF)       2/12         3/12\n(Proposed) Economy-SRU            9/12         11/12\n\n(*As the SRU model is only an intermediate step in the development of the proposed eSRU model, we exclude it from our comparisons)\n\nList of best and second-best performing models for different datasets\nDataset                                  Best model (Avg. AUROC)             Second-best model (Avg. AUROC)\n==================================================================================\nLorenz (F = 10, T = 250)        Economy-SRU (0.95)                        MLP (0.93)\nLorenz (F = 10, T = 500)        Economy-SRU (0.98)                        MLP (0.96)\nLorenz (F = 40, T = 250)        Economy-SRU (0.99)                        MLP (0.85)\nLorenz (F = 40, T = 500)        Economy-SRU (1.0)                          MLP (0.94)\nVAR (T = 500)                         MLP (0.94)                                          Economy-SRU (0.93)\nVAR (T = 1000)                       Economy-SRU (0.98)                         MLP (0.93)\nNetsim (BOLD-FMRI)           Economy-SRU (0.84)                         MLP (0.81)\nDream-3 (Ecoli-1)                 Economy-SRU (0.66)                         MLP (0.644)\nDream-3 (Ecoli-2)                 AG-CNN (0.647)                                 Economy-SRU (0.629)\nDream-3 (Yeast-1)                Economy-SRU (0.627)                      MLP (0.585)\nDream-3 (Yeast-2)                Economy-SRU (0.557)                      AG-CNN (0.556)\nDream-3 (Yeast-3)                AG-CNN (0.557)                                 LSTM (0.555)               ( eSRU (0.55) in third place)   \n\nBased on the above performance breakdown, it is fair to conclude that the proposed economy-SRU model with its unique ability to learn time-localized predictive features comprehensively outperforms the MLP, LSTM and AG-CNN models in detecting nonlinear Granger causality. It should be further noted that for the three datasets where the proposed eSRU model is not the best, the relative drop in the avg. AUROC against the top performing model is marginal.\n\n\n\n>> the proposal is interesting, but the experiment section might need further strengthening. currently, the experimental results do not immediately pop out as showing eSRU particularly useful.\n\n[Reply] \nWhile the performance gains from using the economy-SRU model may not be exceptionally better across the board, there is > 5-16 % improvement in avg. AUROC observed for Lorenz (F40, T 250/500), VAR (T=1000) and Dream-3 (Yeast-1) datasets.  \n\nNonetheless, the key idea that we would like to stress in this paper is that most time series encountered in practice have the distinguishing feature that the occurrence of any future pattern in a time series can be attributed to the past occurrences of a few time-localized patterns in the ancestral time series. The proposed eSRU model is designed specifically to enforce such a causal prescription. in order to learn time-localized predictive features for the purpose of predicting the future time series samples. Our experiments reinforce the veracity of the above causal description, especially since the eSRU model is found to generally outperform the existing MLP/LSTM/Attention-Gated CNN models (which do not explicitly enforce time-localization of the learned predictive features). \nIn page-6 of the revised manuscript, we have added a sentence to emphasize the above causal rule as the motivation behind one of the SRU modifications."}, "signatures": ["ICLR.cc/2020/Conference/Paper1279/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1279/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Economy Statistical Recurrent Units For Inferring Nonlinear Granger Causality", "authors": ["Saurabh Khanna", "Vincent Y. F. Tan"], "authorids": ["elesaur@nus.edu.sg", "vtan@nus.edu.sg"], "keywords": ["Recurrent neural networks", "Granger causality", "Causal inference", "Statistical Recurrent Unit"], "TL;DR": "A new recurrent neural network architecture for detecting pairwise Granger causality between nonlinearly interacting time series. ", "abstract": "Granger causality is a widely-used criterion for analyzing interactions in large-scale networks. As most physical interactions are inherently nonlinear, we consider the problem of inferring the existence of pairwise Granger causality between nonlinearly interacting stochastic processes from their time series measurements. Our proposed approach relies on modeling the embedded nonlinearities in the measurements using a component-wise time series prediction model based on Statistical Recurrent Units (SRUs). We make a case that the network topology of Granger causal relations is directly inferrable from a structured sparse estimate of the internal parameters of the SRU networks trained to predict the processes\u2019 time series measurements. We propose a variant of SRU, called economy-SRU, which, by design has considerably fewer trainable parameters, and therefore less prone to overfitting. The economy-SRU computes a low-dimensional sketch of its high-dimensional hidden state in the form of random projections to generate the feedback for its recurrent processing. Additionally, the internal weight parameters of the economy-SRU are strategically regularized in a group-wise manner to facilitate the proposed network in extracting meaningful predictive features that are highly time-localized to mimic real-world causal events. Extensive experiments are carried out to demonstrate that the proposed economy-SRU based time series prediction model outperforms the MLP, LSTM and attention-gated CNN-based time series models considered previously for inferring Granger causality. ", "pdf": "/pdf/b77f50150d3f24a52b70cfe3b4549acc0bda1e48.pdf", "code": "https://github.com/sakhanna/SRU_for_GCI", "paperhash": "khanna|economy_statistical_recurrent_units_for_inferring_nonlinear_granger_causality", "_bibtex": "@inproceedings{\nKhanna2020Economy,\ntitle={Economy Statistical Recurrent Units For Inferring Nonlinear Granger Causality},\nauthor={Saurabh Khanna and Vincent Y. F. Tan},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SyxV9ANFDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/9e458e303b22faf8a757dad874b88f02f1e5d5a7.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SyxV9ANFDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1279/Authors", "ICLR.cc/2020/Conference/Paper1279/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1279/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1279/Reviewers", "ICLR.cc/2020/Conference/Paper1279/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1279/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1279/Authors|ICLR.cc/2020/Conference/Paper1279/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504158447, "tmdate": 1576860532168, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1279/Authors", "ICLR.cc/2020/Conference/Paper1279/Reviewers", "ICLR.cc/2020/Conference/Paper1279/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1279/-/Official_Comment"}}}, {"id": "SkgppS55oB", "original": null, "number": 1, "cdate": 1573721541149, "ddate": null, "tcdate": 1573721541149, "tmdate": 1573721541149, "tddate": null, "forum": "SyxV9ANFDH", "replyto": "r1gRwkYpFr", "invitation": "ICLR.cc/2020/Conference/Paper1279/-/Official_Comment", "content": {"title": "Response to reviewer #3", "comment": "We thank the reviewer for the useful comments. Please find our pointwise responses below.\n \n>> The effectiveness of the algorithm depends on the sparse regularization, which lacks theoretical guarantee for non-convex optimization. Based on the experimental results, it doesn't seem a big problem though. But I'd like to see some analysis of the actual sparsity of the weight with the proposed group sparse regularization.\n\n[Reply]\nAs suggested by the reviewer, to gain a better understanding of the impact of the group-sparse penalization of $W_{o}$ , we have conducted an ablation study comparing eSRU\u2019s performance in the following two cases: \n(i)\tproposed group-sparse penalization of $W_{o}$  \n(ii)\tunstructured ridge regularization of $W_{o}$. \nThe average AUROC in detecting Granger causality achieved by these two implementations of the eSRU model are reported in the following table. \n\n--------------------------------------------------------------------------------------------------------\n                                              | Proposed group sparse       | Ridge regularization\nDataset                                | regularization of $W_{o}$   | of $W_{o}$ \n--------------------------------------------------------------------------------------------------------\nLorenz (T = 250, F = 10)    |   0.95 \u00b1 0.02                            |   0.93 \u00b1 0.03\nLorenz (T = 500, F = 10)    |   0.98 \u00b1 0.01                            |   0.93 \u00b1 0.04\nLorenz (T = 250, F = 40)    |   0.99   \u00b1 0.0                            |   0.99 \u00b1 0.0\nLorenz (T = 500, F = 40)    |   1.0 \u00b1 0.0                                |   1.0 \u00b1 0.0\nVAR (T = 500)                      |   0.93 \u00b1 0.05                           |   0.90 \u00b1 0.03\nNetSim                                |   0.84 \u00b1 0.03                           |   0.83 \u00b1 0.03\n-------------------------------------------------------------------------------------------------------\n\nIn the above results, we can see that the average AUROC improves in all cases (barring the Lorenz, T = 250/500, F=40 dataset for which there is near perfect recovery of Granger causal network) by switching from unstructured ridge regularization to the proposed group-sparse regularization of the output weight matrix $W_{o}$. \n\nPerforming a theoretical analysis of the group-sparse regularization for this model is out of the scope of the paper. The above ablation study is hopefully sufficiently convincing in reinforcing the idea that the proposed group-sparsity in the output weight matrix generally improves the performance.  \n\nThe revised manuscript discusses the above ablation study in the newly added Appendix C.2. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1279/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1279/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Economy Statistical Recurrent Units For Inferring Nonlinear Granger Causality", "authors": ["Saurabh Khanna", "Vincent Y. F. Tan"], "authorids": ["elesaur@nus.edu.sg", "vtan@nus.edu.sg"], "keywords": ["Recurrent neural networks", "Granger causality", "Causal inference", "Statistical Recurrent Unit"], "TL;DR": "A new recurrent neural network architecture for detecting pairwise Granger causality between nonlinearly interacting time series. ", "abstract": "Granger causality is a widely-used criterion for analyzing interactions in large-scale networks. As most physical interactions are inherently nonlinear, we consider the problem of inferring the existence of pairwise Granger causality between nonlinearly interacting stochastic processes from their time series measurements. Our proposed approach relies on modeling the embedded nonlinearities in the measurements using a component-wise time series prediction model based on Statistical Recurrent Units (SRUs). We make a case that the network topology of Granger causal relations is directly inferrable from a structured sparse estimate of the internal parameters of the SRU networks trained to predict the processes\u2019 time series measurements. We propose a variant of SRU, called economy-SRU, which, by design has considerably fewer trainable parameters, and therefore less prone to overfitting. The economy-SRU computes a low-dimensional sketch of its high-dimensional hidden state in the form of random projections to generate the feedback for its recurrent processing. Additionally, the internal weight parameters of the economy-SRU are strategically regularized in a group-wise manner to facilitate the proposed network in extracting meaningful predictive features that are highly time-localized to mimic real-world causal events. Extensive experiments are carried out to demonstrate that the proposed economy-SRU based time series prediction model outperforms the MLP, LSTM and attention-gated CNN-based time series models considered previously for inferring Granger causality. ", "pdf": "/pdf/b77f50150d3f24a52b70cfe3b4549acc0bda1e48.pdf", "code": "https://github.com/sakhanna/SRU_for_GCI", "paperhash": "khanna|economy_statistical_recurrent_units_for_inferring_nonlinear_granger_causality", "_bibtex": "@inproceedings{\nKhanna2020Economy,\ntitle={Economy Statistical Recurrent Units For Inferring Nonlinear Granger Causality},\nauthor={Saurabh Khanna and Vincent Y. F. Tan},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SyxV9ANFDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/9e458e303b22faf8a757dad874b88f02f1e5d5a7.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SyxV9ANFDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1279/Authors", "ICLR.cc/2020/Conference/Paper1279/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1279/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1279/Reviewers", "ICLR.cc/2020/Conference/Paper1279/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1279/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1279/Authors|ICLR.cc/2020/Conference/Paper1279/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504158447, "tmdate": 1576860532168, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1279/Authors", "ICLR.cc/2020/Conference/Paper1279/Reviewers", "ICLR.cc/2020/Conference/Paper1279/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1279/-/Official_Comment"}}}, {"id": "r1gRwkYpFr", "original": null, "number": 1, "cdate": 1571815269641, "ddate": null, "tcdate": 1571815269641, "tmdate": 1572972489505, "tddate": null, "forum": "SyxV9ANFDH", "replyto": "SyxV9ANFDH", "invitation": "ICLR.cc/2020/Conference/Paper1279/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper proposed to use SRU for inferring nonlinear granger causality. It also provided two extensions of SRU with regularization to alleviate the issue of overfitting.\n\nSRU was proposed in a previous paper. This paper extended this algorithm for inferring granger causality through applying group sparse regularization, which is a pretty smart design to me. Another major innovation comes from the two modifications to combat the possible overfitting when using very fine-grained scale parameters in SRU.\n\nThe paper was well-written in general. I can follow the paper without problem.\nThe effectiveness of the algorithm depends on the sparse regularization, which lacks theoretical guarantee for non-convex optimization. Based on the experimental results, it doesn't seem a big problem though. But I'd like to see some analysis of the actual sparsity of the weight with the proposed group sparse regularization.\n\nThe experiments seem convincing to me and I really appreciate the authors provided the tuned hyper-parameters in the appendix.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1279/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1279/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Economy Statistical Recurrent Units For Inferring Nonlinear Granger Causality", "authors": ["Saurabh Khanna", "Vincent Y. F. Tan"], "authorids": ["elesaur@nus.edu.sg", "vtan@nus.edu.sg"], "keywords": ["Recurrent neural networks", "Granger causality", "Causal inference", "Statistical Recurrent Unit"], "TL;DR": "A new recurrent neural network architecture for detecting pairwise Granger causality between nonlinearly interacting time series. ", "abstract": "Granger causality is a widely-used criterion for analyzing interactions in large-scale networks. As most physical interactions are inherently nonlinear, we consider the problem of inferring the existence of pairwise Granger causality between nonlinearly interacting stochastic processes from their time series measurements. Our proposed approach relies on modeling the embedded nonlinearities in the measurements using a component-wise time series prediction model based on Statistical Recurrent Units (SRUs). We make a case that the network topology of Granger causal relations is directly inferrable from a structured sparse estimate of the internal parameters of the SRU networks trained to predict the processes\u2019 time series measurements. We propose a variant of SRU, called economy-SRU, which, by design has considerably fewer trainable parameters, and therefore less prone to overfitting. The economy-SRU computes a low-dimensional sketch of its high-dimensional hidden state in the form of random projections to generate the feedback for its recurrent processing. Additionally, the internal weight parameters of the economy-SRU are strategically regularized in a group-wise manner to facilitate the proposed network in extracting meaningful predictive features that are highly time-localized to mimic real-world causal events. Extensive experiments are carried out to demonstrate that the proposed economy-SRU based time series prediction model outperforms the MLP, LSTM and attention-gated CNN-based time series models considered previously for inferring Granger causality. ", "pdf": "/pdf/b77f50150d3f24a52b70cfe3b4549acc0bda1e48.pdf", "code": "https://github.com/sakhanna/SRU_for_GCI", "paperhash": "khanna|economy_statistical_recurrent_units_for_inferring_nonlinear_granger_causality", "_bibtex": "@inproceedings{\nKhanna2020Economy,\ntitle={Economy Statistical Recurrent Units For Inferring Nonlinear Granger Causality},\nauthor={Saurabh Khanna and Vincent Y. F. Tan},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SyxV9ANFDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/9e458e303b22faf8a757dad874b88f02f1e5d5a7.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SyxV9ANFDH", "replyto": "SyxV9ANFDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1279/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1279/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576078599644, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1279/Reviewers"], "noninvitees": [], "tcdate": 1570237739690, "tmdate": 1576078599657, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1279/-/Official_Review"}}}, {"id": "HyxuKq0CtB", "original": null, "number": 2, "cdate": 1571904128424, "ddate": null, "tcdate": 1571904128424, "tmdate": 1572972489459, "tddate": null, "forum": "SyxV9ANFDH", "replyto": "SyxV9ANFDH", "invitation": "ICLR.cc/2020/Conference/Paper1279/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "In this paper the authors propose using Statistical Recurrent Units to predict the network for Granger causality. They motivate this choice by the high representation power of SRUs for multivariate time series, the good performance they usually enjoy and as a way to alleviate the vanishing gradient problem. More importantly the particular form of the SRUs gives a very simple predictor and therefore explanability for Granger causality: the authors propose to simply mark serie $i$ as Granger caused by $j$ if the $j$th column of the input mixing matrix of the $g_i$ is non-zero.\nIn order to force whole columns of the input matrix to be negative, the authors use a group regularization on the columns of the input matrix $W_{\\text{in}}$. The resulting problem is then optimized by proximal gradient descent.\nThe second contribution of the authors is eSRU, a smaller (in terms of parameters) variant of SRU in order to prevent overfitting.\nFirst the authors introduce a dimensionality reduction layer before the SRU units by using fixed non-trainable random projections. Then the authors propose adding a regularization term for the output mixing matrix, which represents the bulks of the parameters of the model.\nThe eSRU and SRU causal models presented are then compared to the previous state of the art on several datasets, both synthetic and real, where they manage to reach a new state of the art.\nRegarding the two improvements in eSRU proposed by the authors, I have several questions:\nFirst, regarding the random projections, have the authors tried learning the projections ? While it increases the risk of overfitting it is possible, with enough regularization, that it may help learn good representations. Secondly, as I understand it a single projection is drawn at random for every component. While we know from Johnson-Lindenstrauss's Lemma that this will in average be a good strategy, how stable is the model to spurious projections ?\nRegarding the scarification of the output matrix: the number of parameters is effectively reduced but the computational requirements are still the same. Did the authors try any explicit methods, maybe matrix factorization, to exploit the very specific sparse structure of $W_o$ and reduce the number of operations and parameters ?\nFinally, while the results are undeniable, just observing the non-zero columns of $W_\\text{in}$ is as I understand it a mostly ad hoc rule. It would have been interesting to empirically verify its validity by contrasting it with the results given by following equation 2.\nOverall the paper presents an interesting model for inferring Granger causality. The authors clearly present their two main contributions and verify them using varied datasets.\nThe authors also made a clear and appreciated effort toward reproducibility by including all hyperparameters and implementation details as well as the code, including those of competing techniques.\nThe inclusion of the literature review in the appendix is also greatly appreciated."}, "signatures": ["ICLR.cc/2020/Conference/Paper1279/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1279/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Economy Statistical Recurrent Units For Inferring Nonlinear Granger Causality", "authors": ["Saurabh Khanna", "Vincent Y. F. Tan"], "authorids": ["elesaur@nus.edu.sg", "vtan@nus.edu.sg"], "keywords": ["Recurrent neural networks", "Granger causality", "Causal inference", "Statistical Recurrent Unit"], "TL;DR": "A new recurrent neural network architecture for detecting pairwise Granger causality between nonlinearly interacting time series. ", "abstract": "Granger causality is a widely-used criterion for analyzing interactions in large-scale networks. As most physical interactions are inherently nonlinear, we consider the problem of inferring the existence of pairwise Granger causality between nonlinearly interacting stochastic processes from their time series measurements. Our proposed approach relies on modeling the embedded nonlinearities in the measurements using a component-wise time series prediction model based on Statistical Recurrent Units (SRUs). We make a case that the network topology of Granger causal relations is directly inferrable from a structured sparse estimate of the internal parameters of the SRU networks trained to predict the processes\u2019 time series measurements. We propose a variant of SRU, called economy-SRU, which, by design has considerably fewer trainable parameters, and therefore less prone to overfitting. The economy-SRU computes a low-dimensional sketch of its high-dimensional hidden state in the form of random projections to generate the feedback for its recurrent processing. Additionally, the internal weight parameters of the economy-SRU are strategically regularized in a group-wise manner to facilitate the proposed network in extracting meaningful predictive features that are highly time-localized to mimic real-world causal events. Extensive experiments are carried out to demonstrate that the proposed economy-SRU based time series prediction model outperforms the MLP, LSTM and attention-gated CNN-based time series models considered previously for inferring Granger causality. ", "pdf": "/pdf/b77f50150d3f24a52b70cfe3b4549acc0bda1e48.pdf", "code": "https://github.com/sakhanna/SRU_for_GCI", "paperhash": "khanna|economy_statistical_recurrent_units_for_inferring_nonlinear_granger_causality", "_bibtex": "@inproceedings{\nKhanna2020Economy,\ntitle={Economy Statistical Recurrent Units For Inferring Nonlinear Granger Causality},\nauthor={Saurabh Khanna and Vincent Y. F. Tan},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SyxV9ANFDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/9e458e303b22faf8a757dad874b88f02f1e5d5a7.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SyxV9ANFDH", "replyto": "SyxV9ANFDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1279/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1279/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576078599644, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1279/Reviewers"], "noninvitees": [], "tcdate": 1570237739690, "tmdate": 1576078599657, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1279/-/Official_Review"}}}, {"id": "rJgVWTWJ5S", "original": null, "number": 3, "cdate": 1571917051561, "ddate": null, "tcdate": 1571917051561, "tmdate": 1572972489410, "tddate": null, "forum": "SyxV9ANFDH", "replyto": "SyxV9ANFDH", "invitation": "ICLR.cc/2020/Conference/Paper1279/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "the paper attempts to infer Granger causality between nonlinearly interacting stochastic processes from their time series measurements. instead of using MLP/LSTM etc to to model time series measurement, the paper proposed to use component-wise time series prediction model with Statistical Recurrent Units to model the measurements. they consider a low-dimensional version of SRU, which they call economy-SRU. in particular, they use group-wise regularizing to accompany the particular structure of the model to aid interpretability. they compared the performance with existing models with MLP/LSTM and show some gains in a few examples (but not all.) the proposal is interesting, but the experiment section might need further strengthening. currently, the experimental results do not immediately pop out as showing eSRU particularly useful."}, "signatures": ["ICLR.cc/2020/Conference/Paper1279/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1279/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Economy Statistical Recurrent Units For Inferring Nonlinear Granger Causality", "authors": ["Saurabh Khanna", "Vincent Y. F. Tan"], "authorids": ["elesaur@nus.edu.sg", "vtan@nus.edu.sg"], "keywords": ["Recurrent neural networks", "Granger causality", "Causal inference", "Statistical Recurrent Unit"], "TL;DR": "A new recurrent neural network architecture for detecting pairwise Granger causality between nonlinearly interacting time series. ", "abstract": "Granger causality is a widely-used criterion for analyzing interactions in large-scale networks. As most physical interactions are inherently nonlinear, we consider the problem of inferring the existence of pairwise Granger causality between nonlinearly interacting stochastic processes from their time series measurements. Our proposed approach relies on modeling the embedded nonlinearities in the measurements using a component-wise time series prediction model based on Statistical Recurrent Units (SRUs). We make a case that the network topology of Granger causal relations is directly inferrable from a structured sparse estimate of the internal parameters of the SRU networks trained to predict the processes\u2019 time series measurements. We propose a variant of SRU, called economy-SRU, which, by design has considerably fewer trainable parameters, and therefore less prone to overfitting. The economy-SRU computes a low-dimensional sketch of its high-dimensional hidden state in the form of random projections to generate the feedback for its recurrent processing. Additionally, the internal weight parameters of the economy-SRU are strategically regularized in a group-wise manner to facilitate the proposed network in extracting meaningful predictive features that are highly time-localized to mimic real-world causal events. Extensive experiments are carried out to demonstrate that the proposed economy-SRU based time series prediction model outperforms the MLP, LSTM and attention-gated CNN-based time series models considered previously for inferring Granger causality. ", "pdf": "/pdf/b77f50150d3f24a52b70cfe3b4549acc0bda1e48.pdf", "code": "https://github.com/sakhanna/SRU_for_GCI", "paperhash": "khanna|economy_statistical_recurrent_units_for_inferring_nonlinear_granger_causality", "_bibtex": "@inproceedings{\nKhanna2020Economy,\ntitle={Economy Statistical Recurrent Units For Inferring Nonlinear Granger Causality},\nauthor={Saurabh Khanna and Vincent Y. F. Tan},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SyxV9ANFDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/9e458e303b22faf8a757dad874b88f02f1e5d5a7.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SyxV9ANFDH", "replyto": "SyxV9ANFDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1279/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1279/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576078599644, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1279/Reviewers"], "noninvitees": [], "tcdate": 1570237739690, "tmdate": 1576078599657, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1279/-/Official_Review"}}}], "count": 10}