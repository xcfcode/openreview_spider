{"notes": [{"id": "rke_YiRct7", "original": "r1gDn5xvYX", "number": 458, "cdate": 1538087807835, "ddate": null, "tcdate": 1538087807835, "tmdate": 1556908437576, "tddate": null, "forum": "rke_YiRct7", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Small nonlinearities in activation functions create bad local minima in neural networks", "abstract": "We investigate the loss surface of neural networks. We prove that even for one-hidden-layer networks with \"slightest\" nonlinearity, the empirical risks have spurious local minima in most cases. Our results thus indicate that in general \"no spurious local minim\" is a property limited to deep linear networks, and insights obtained from linear networks may not be robust. Specifically, for ReLU(-like) networks we constructively prove that for almost all practical datasets there exist infinitely many local minima. We also present a counterexample for more general activations (sigmoid, tanh, arctan, ReLU, etc.), for which there exists a bad local minimum. Our results make the least restrictive assumptions relative to existing results on spurious local optima in neural networks. We complete our discussion by presenting a comprehensive characterization of global optimality for deep linear networks, which unifies other results on this topic.", "keywords": ["spurious local minima", "loss surface", "optimization landscape", "neural network"], "authorids": ["chulheey@mit.edu", "suvrit@mit.edu", "jadbabai@mit.edu"], "authors": ["Chulhee Yun", "Suvrit Sra", "Ali Jadbabaie"], "TL;DR": "We constructively prove that even the slightest nonlinear activation functions introduce spurious local minima, for general datasets and activation functions.", "pdf": "/pdf/9dc34e786d0037c655b4e01acba62d2612460cf2.pdf", "paperhash": "yun|small_nonlinearities_in_activation_functions_create_bad_local_minima_in_neural_networks", "_bibtex": "@inproceedings{\nyun2018small,\ntitle={Small nonlinearities in activation functions create bad local minima in neural networks},\nauthor={Chulhee Yun and Suvrit Sra and Ali Jadbabaie},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rke_YiRct7},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "Hkg2uSrhvN", "original": null, "number": 2, "cdate": 1552860531828, "ddate": null, "tcdate": 1552860531828, "tmdate": 1552875895150, "tddate": null, "forum": "rke_YiRct7", "replyto": "r1xzVHt71E", "invitation": "ICLR.cc/2019/Conference/-/Paper458/Public_Comment", "content": {"comment": "A clarification: Liang et al. not only claimed that \"there can be counterexamples such that local minima can have *non-zero classification error*\", but also claimed that \"these local minima are spurious\" with high objective value (not just high classification error). In this sense, the nature of that paper is the same as your paper. \n\nThanks for pointing out that your paper constructed one dataset that works for many neurons. This is different from the mentioned paper. \n\nBTW: Liang et al. also pointed out that quadratic loss may not be a good metric for binary classification since there exists global minimum with high classification error. This claim is irrelevant for pure optimization purpose but important for machine learning purpose.  ", "title": "Liang et al. does construct spurious local minima"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Small nonlinearities in activation functions create bad local minima in neural networks", "abstract": "We investigate the loss surface of neural networks. We prove that even for one-hidden-layer networks with \"slightest\" nonlinearity, the empirical risks have spurious local minima in most cases. Our results thus indicate that in general \"no spurious local minim\" is a property limited to deep linear networks, and insights obtained from linear networks may not be robust. Specifically, for ReLU(-like) networks we constructively prove that for almost all practical datasets there exist infinitely many local minima. We also present a counterexample for more general activations (sigmoid, tanh, arctan, ReLU, etc.), for which there exists a bad local minimum. Our results make the least restrictive assumptions relative to existing results on spurious local optima in neural networks. We complete our discussion by presenting a comprehensive characterization of global optimality for deep linear networks, which unifies other results on this topic.", "keywords": ["spurious local minima", "loss surface", "optimization landscape", "neural network"], "authorids": ["chulheey@mit.edu", "suvrit@mit.edu", "jadbabai@mit.edu"], "authors": ["Chulhee Yun", "Suvrit Sra", "Ali Jadbabaie"], "TL;DR": "We constructively prove that even the slightest nonlinear activation functions introduce spurious local minima, for general datasets and activation functions.", "pdf": "/pdf/9dc34e786d0037c655b4e01acba62d2612460cf2.pdf", "paperhash": "yun|small_nonlinearities_in_activation_functions_create_bad_local_minima_in_neural_networks", "_bibtex": "@inproceedings{\nyun2018small,\ntitle={Small nonlinearities in activation functions create bad local minima in neural networks},\nauthor={Chulhee Yun and Suvrit Sra and Ali Jadbabaie},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rke_YiRct7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper458/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311835666, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "rke_YiRct7", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper458/Authors", "ICLR.cc/2019/Conference/Paper458/Reviewers", "ICLR.cc/2019/Conference/Paper458/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper458/Authors", "ICLR.cc/2019/Conference/Paper458/Reviewers", "ICLR.cc/2019/Conference/Paper458/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311835666}}}, {"id": "Byec_jnVlE", "original": null, "number": 1, "cdate": 1545026417870, "ddate": null, "tcdate": 1545026417870, "tmdate": 1545354473367, "tddate": null, "forum": "rke_YiRct7", "replyto": "rke_YiRct7", "invitation": "ICLR.cc/2019/Conference/-/Paper458/Meta_Review", "content": {"metareview": "This is an interesting paper that develops new techniques for analyzing the loss surface of deep networks, allowing the existence of spurious local minima to be established under fairly general conditions.  The reviewers responded with uniformly positive opinions.", "confidence": "5: The area chair is absolutely certain", "recommendation": "Accept (Poster)", "title": "Interesting investigation into the existence of spurious local minima in nonlinear networks"}, "signatures": ["ICLR.cc/2019/Conference/Paper458/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper458/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Small nonlinearities in activation functions create bad local minima in neural networks", "abstract": "We investigate the loss surface of neural networks. We prove that even for one-hidden-layer networks with \"slightest\" nonlinearity, the empirical risks have spurious local minima in most cases. Our results thus indicate that in general \"no spurious local minim\" is a property limited to deep linear networks, and insights obtained from linear networks may not be robust. Specifically, for ReLU(-like) networks we constructively prove that for almost all practical datasets there exist infinitely many local minima. We also present a counterexample for more general activations (sigmoid, tanh, arctan, ReLU, etc.), for which there exists a bad local minimum. Our results make the least restrictive assumptions relative to existing results on spurious local optima in neural networks. We complete our discussion by presenting a comprehensive characterization of global optimality for deep linear networks, which unifies other results on this topic.", "keywords": ["spurious local minima", "loss surface", "optimization landscape", "neural network"], "authorids": ["chulheey@mit.edu", "suvrit@mit.edu", "jadbabai@mit.edu"], "authors": ["Chulhee Yun", "Suvrit Sra", "Ali Jadbabaie"], "TL;DR": "We constructively prove that even the slightest nonlinear activation functions introduce spurious local minima, for general datasets and activation functions.", "pdf": "/pdf/9dc34e786d0037c655b4e01acba62d2612460cf2.pdf", "paperhash": "yun|small_nonlinearities_in_activation_functions_create_bad_local_minima_in_neural_networks", "_bibtex": "@inproceedings{\nyun2018small,\ntitle={Small nonlinearities in activation functions create bad local minima in neural networks},\nauthor={Chulhee Yun and Suvrit Sra and Ali Jadbabaie},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rke_YiRct7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper458/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353211296, "tddate": null, "super": null, "final": null, "reply": {"forum": "rke_YiRct7", "replyto": "rke_YiRct7", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper458/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper458/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper458/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353211296}}}, {"id": "r1xzVHt71E", "original": null, "number": 4, "cdate": 1543898410431, "ddate": null, "tcdate": 1543898410431, "tmdate": 1543898892425, "tddate": null, "forum": "rke_YiRct7", "replyto": "ryxPjBlZyE", "invitation": "ICLR.cc/2019/Conference/-/Paper458/Official_Comment", "content": {"title": "Reply to the comment", "comment": "Thank you for your interest in our paper, and also for pointing out a relevant result! The paper you noted is indeed related, but its focus is a bit different from our paper. The authors made assumptions on the loss function, data distribution, network structure, and activation function, and showed that all local minima of the empirical loss have zero classification error. Then, they presented counterexamples that if any of their assumptions are violated, there can be counterexamples such that local minima can have *non-zero classification error*. Please note that having nonzero classification error does not necessarily imply that the local minima are spurious, and having spurious local minima does not necessarily imply nonzero classification error either. Thus, the counterexamples do not directly relate to our work and the results are not directly comparable (as noted by the comment, the loss functions are also different). Having said that, we do appreciate the reference and we will add a citation in our next revision.\n\nIn comparison, our paper shows that even for a single dataset, not taylor-made for specific activation functions, there can be a spurious local minimum for a variety of activation functions. In fact, a draft version of our paper appeared earlier than the paper mentioned by the comment, which is one of the reasons why we missed it in our paper. We promise to cite this paper and explain the differences in detail, in the next version of our paper."}, "signatures": ["ICLR.cc/2019/Conference/Paper458/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper458/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper458/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Small nonlinearities in activation functions create bad local minima in neural networks", "abstract": "We investigate the loss surface of neural networks. We prove that even for one-hidden-layer networks with \"slightest\" nonlinearity, the empirical risks have spurious local minima in most cases. Our results thus indicate that in general \"no spurious local minim\" is a property limited to deep linear networks, and insights obtained from linear networks may not be robust. Specifically, for ReLU(-like) networks we constructively prove that for almost all practical datasets there exist infinitely many local minima. We also present a counterexample for more general activations (sigmoid, tanh, arctan, ReLU, etc.), for which there exists a bad local minimum. Our results make the least restrictive assumptions relative to existing results on spurious local optima in neural networks. We complete our discussion by presenting a comprehensive characterization of global optimality for deep linear networks, which unifies other results on this topic.", "keywords": ["spurious local minima", "loss surface", "optimization landscape", "neural network"], "authorids": ["chulheey@mit.edu", "suvrit@mit.edu", "jadbabai@mit.edu"], "authors": ["Chulhee Yun", "Suvrit Sra", "Ali Jadbabaie"], "TL;DR": "We constructively prove that even the slightest nonlinear activation functions introduce spurious local minima, for general datasets and activation functions.", "pdf": "/pdf/9dc34e786d0037c655b4e01acba62d2612460cf2.pdf", "paperhash": "yun|small_nonlinearities_in_activation_functions_create_bad_local_minima_in_neural_networks", "_bibtex": "@inproceedings{\nyun2018small,\ntitle={Small nonlinearities in activation functions create bad local minima in neural networks},\nauthor={Chulhee Yun and Suvrit Sra and Ali Jadbabaie},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rke_YiRct7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper458/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621623527, "tddate": null, "super": null, "final": null, "reply": {"forum": "rke_YiRct7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper458/Authors", "ICLR.cc/2019/Conference/Paper458/Reviewers", "ICLR.cc/2019/Conference/Paper458/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper458/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper458/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper458/Authors|ICLR.cc/2019/Conference/Paper458/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper458/Reviewers", "ICLR.cc/2019/Conference/Paper458/Authors", "ICLR.cc/2019/Conference/Paper458/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621623527}}}, {"id": "ryxPjBlZyE", "original": null, "number": 1, "cdate": 1543730590833, "ddate": null, "tcdate": 1543730590833, "tmdate": 1543730691792, "tddate": null, "forum": "rke_YiRct7", "replyto": "rke_YiRct7", "invitation": "ICLR.cc/2019/Conference/-/Paper458/Public_Comment", "content": {"comment": "For the realizable case, Liang et al. [1] have constructed spurious local minima for ReLU-class functions (sigma(x) = 0 for x <= 0, so include ELU), leaky-ReLU class functions (sigma(x)=x, for x>=0), quadratic-like functions and sigmoid-like functions. Each of them represents a class of activations that satisfying some conditions, with a similar flavor to (C2.1)-(C2.7)). Though the loss function is different (Liang et al. is for binary classification), this paper is not the first one to construct spurious local minima for many neuron activations. Could the authors comment on the differences?\n\n[1] Liang, S., Sun, R., Li, Y., & Srikant, R. (2018). Understanding the loss surface of neural networks for binary classification. arXiv preprint arXiv:1803.00909.", "title": "Important Related Work"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Small nonlinearities in activation functions create bad local minima in neural networks", "abstract": "We investigate the loss surface of neural networks. We prove that even for one-hidden-layer networks with \"slightest\" nonlinearity, the empirical risks have spurious local minima in most cases. Our results thus indicate that in general \"no spurious local minim\" is a property limited to deep linear networks, and insights obtained from linear networks may not be robust. Specifically, for ReLU(-like) networks we constructively prove that for almost all practical datasets there exist infinitely many local minima. We also present a counterexample for more general activations (sigmoid, tanh, arctan, ReLU, etc.), for which there exists a bad local minimum. Our results make the least restrictive assumptions relative to existing results on spurious local optima in neural networks. We complete our discussion by presenting a comprehensive characterization of global optimality for deep linear networks, which unifies other results on this topic.", "keywords": ["spurious local minima", "loss surface", "optimization landscape", "neural network"], "authorids": ["chulheey@mit.edu", "suvrit@mit.edu", "jadbabai@mit.edu"], "authors": ["Chulhee Yun", "Suvrit Sra", "Ali Jadbabaie"], "TL;DR": "We constructively prove that even the slightest nonlinear activation functions introduce spurious local minima, for general datasets and activation functions.", "pdf": "/pdf/9dc34e786d0037c655b4e01acba62d2612460cf2.pdf", "paperhash": "yun|small_nonlinearities_in_activation_functions_create_bad_local_minima_in_neural_networks", "_bibtex": "@inproceedings{\nyun2018small,\ntitle={Small nonlinearities in activation functions create bad local minima in neural networks},\nauthor={Chulhee Yun and Suvrit Sra and Ali Jadbabaie},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rke_YiRct7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper458/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311835666, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "rke_YiRct7", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper458/Authors", "ICLR.cc/2019/Conference/Paper458/Reviewers", "ICLR.cc/2019/Conference/Paper458/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper458/Authors", "ICLR.cc/2019/Conference/Paper458/Reviewers", "ICLR.cc/2019/Conference/Paper458/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311835666}}}, {"id": "SJx3vHAQCm", "original": null, "number": 3, "cdate": 1542870372490, "ddate": null, "tcdate": 1542870372490, "tmdate": 1542870372490, "tddate": null, "forum": "rke_YiRct7", "replyto": "H1gdFd3epm", "invitation": "ICLR.cc/2019/Conference/-/Paper458/Official_Comment", "content": {"title": "Reply to AnonReviewer2", "comment": "Thank you for your efforts in reviewing our paper. Below, we address the concerns raised, listed by the item number:\n\n(2) First, we point out a minor point: The dataset contains three data points. The input is two-dimensional, the output is one-dimensional, and there are three data points. We agree that suboptimality of neural nets with large sample size would be more interesting. While we are presenting a toy example to make a point, we would like to emphasize that this counterexample works for a *variety* of popular activation functions that are used in practice. \n\n(3) You are correct; a deep linear network is a purely theoretical subject of research, and it is not a practical model. However, the reason for studying it is that this simplified model can deliver some insights in understanding (nonlinear) neural networks. This model received a lot of attention since [Kawaguchi, NIPS\u201916] showed that linear neural networks with squared error loss have only global minima and saddle points, which implies that all local minima are globally optimal. There have been many efforts to extend this result to more general settings [e.g., Laurent & Brecht, ICML\u201918a,b]. Our paper settles the question of how far this property can be extended, and whether this can explain the fact that despite nonconvexity, gradient-based techniques can find weights that achieve near-zero loss. By providing a general result (Theorem 4 & Corollary 5) on linear neural networks for which this property holds, and also showing that this property fails to hold as soon as we add small nonlinearities to the hidden nodes (Theorem 1 & 2), we contribute to the growing body of literature that is trying to understand deep learning."}, "signatures": ["ICLR.cc/2019/Conference/Paper458/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper458/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper458/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Small nonlinearities in activation functions create bad local minima in neural networks", "abstract": "We investigate the loss surface of neural networks. We prove that even for one-hidden-layer networks with \"slightest\" nonlinearity, the empirical risks have spurious local minima in most cases. Our results thus indicate that in general \"no spurious local minim\" is a property limited to deep linear networks, and insights obtained from linear networks may not be robust. Specifically, for ReLU(-like) networks we constructively prove that for almost all practical datasets there exist infinitely many local minima. We also present a counterexample for more general activations (sigmoid, tanh, arctan, ReLU, etc.), for which there exists a bad local minimum. Our results make the least restrictive assumptions relative to existing results on spurious local optima in neural networks. We complete our discussion by presenting a comprehensive characterization of global optimality for deep linear networks, which unifies other results on this topic.", "keywords": ["spurious local minima", "loss surface", "optimization landscape", "neural network"], "authorids": ["chulheey@mit.edu", "suvrit@mit.edu", "jadbabai@mit.edu"], "authors": ["Chulhee Yun", "Suvrit Sra", "Ali Jadbabaie"], "TL;DR": "We constructively prove that even the slightest nonlinear activation functions introduce spurious local minima, for general datasets and activation functions.", "pdf": "/pdf/9dc34e786d0037c655b4e01acba62d2612460cf2.pdf", "paperhash": "yun|small_nonlinearities_in_activation_functions_create_bad_local_minima_in_neural_networks", "_bibtex": "@inproceedings{\nyun2018small,\ntitle={Small nonlinearities in activation functions create bad local minima in neural networks},\nauthor={Chulhee Yun and Suvrit Sra and Ali Jadbabaie},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rke_YiRct7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper458/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621623527, "tddate": null, "super": null, "final": null, "reply": {"forum": "rke_YiRct7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper458/Authors", "ICLR.cc/2019/Conference/Paper458/Reviewers", "ICLR.cc/2019/Conference/Paper458/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper458/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper458/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper458/Authors|ICLR.cc/2019/Conference/Paper458/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper458/Reviewers", "ICLR.cc/2019/Conference/Paper458/Authors", "ICLR.cc/2019/Conference/Paper458/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621623527}}}, {"id": "BJl5ES0mRm", "original": null, "number": 2, "cdate": 1542870322447, "ddate": null, "tcdate": 1542870322447, "tmdate": 1542870322447, "tddate": null, "forum": "rke_YiRct7", "replyto": "HkeS3_Q02X", "invitation": "ICLR.cc/2019/Conference/-/Paper458/Official_Comment", "content": {"title": "Reply to AnonReviewer3", "comment": "We appreciate the reviewer\u2019s time and thoughtful comments. We are glad that you found our paper easy to read. Below, we will provide answers to the reviewer\u2019s concerns, by the order they appear.\n\n* It is true that a non-convex problem in general has many spurious local minima, and it is also true that in practice, researchers sometimes don\u2019t observe such bad local minima. Understanding this seemingly contradictory phenomenon in its most general form is one of the fundamental open problems in deep learning.\nRecent results indicate that some important nonconvex problems satisfy this property, i.e., that all local minima are global. Examples include matrix completion, matrix sensing, PCA, etc. (cf. [1] and references therein). After the author in [2] showed that linear neural networks with squared error loss are examples of such problems, there have been efforts to extend this to more general neural networks. Our paper answers the question of \"how far can we extend the *local min is global* property,\" by providing a general result (Theorem 4 & Corollary 5) on linear networks for which this property holds. More importantly, we also show that this property is not robust and immediately breaks when we add nonlinearities to the network. (Theorem 1 & 2).\n\n* Our focus in this paper was about the properties of the loss surface of empirical risk, disjoint from algorithms or architectural choices. Of course, in practice, current training methods are able to find better solutions than convex models, even with existence of many spurious local minima. The results of our paper suggest that (as mentioned briefly in Section 5), the loss surface itself is not likely to provide sufficient explanation of why optimization works well in deep learning, and studying the effect of algorithms and other factors will be a valuable direction of future work.\n\n[1] Ge et al., No Spurious Local Minima in Nonconvex Low Rank Problems: A Unified Geometric Analysis, 2017\n[2] Kawaguchi, Deep learning without poor local minima, 2016"}, "signatures": ["ICLR.cc/2019/Conference/Paper458/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper458/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper458/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Small nonlinearities in activation functions create bad local minima in neural networks", "abstract": "We investigate the loss surface of neural networks. We prove that even for one-hidden-layer networks with \"slightest\" nonlinearity, the empirical risks have spurious local minima in most cases. Our results thus indicate that in general \"no spurious local minim\" is a property limited to deep linear networks, and insights obtained from linear networks may not be robust. Specifically, for ReLU(-like) networks we constructively prove that for almost all practical datasets there exist infinitely many local minima. We also present a counterexample for more general activations (sigmoid, tanh, arctan, ReLU, etc.), for which there exists a bad local minimum. Our results make the least restrictive assumptions relative to existing results on spurious local optima in neural networks. We complete our discussion by presenting a comprehensive characterization of global optimality for deep linear networks, which unifies other results on this topic.", "keywords": ["spurious local minima", "loss surface", "optimization landscape", "neural network"], "authorids": ["chulheey@mit.edu", "suvrit@mit.edu", "jadbabai@mit.edu"], "authors": ["Chulhee Yun", "Suvrit Sra", "Ali Jadbabaie"], "TL;DR": "We constructively prove that even the slightest nonlinear activation functions introduce spurious local minima, for general datasets and activation functions.", "pdf": "/pdf/9dc34e786d0037c655b4e01acba62d2612460cf2.pdf", "paperhash": "yun|small_nonlinearities_in_activation_functions_create_bad_local_minima_in_neural_networks", "_bibtex": "@inproceedings{\nyun2018small,\ntitle={Small nonlinearities in activation functions create bad local minima in neural networks},\nauthor={Chulhee Yun and Suvrit Sra and Ali Jadbabaie},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rke_YiRct7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper458/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621623527, "tddate": null, "super": null, "final": null, "reply": {"forum": "rke_YiRct7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper458/Authors", "ICLR.cc/2019/Conference/Paper458/Reviewers", "ICLR.cc/2019/Conference/Paper458/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper458/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper458/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper458/Authors|ICLR.cc/2019/Conference/Paper458/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper458/Reviewers", "ICLR.cc/2019/Conference/Paper458/Authors", "ICLR.cc/2019/Conference/Paper458/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621623527}}}, {"id": "HJg0brAXCm", "original": null, "number": 1, "cdate": 1542870278485, "ddate": null, "tcdate": 1542870278485, "tmdate": 1542870278485, "tddate": null, "forum": "rke_YiRct7", "replyto": "Hkgdn1QB3X", "invitation": "ICLR.cc/2019/Conference/-/Paper458/Official_Comment", "content": {"title": "Reply to AnonReviewer1", "comment": "We thank the reviewer for the enlightening feedback, and we are glad that the reviewer found our results/proof techniques interesting.\n\nWe will address the comments/concerns below:\n\n* Regarding realizable datasets:\nWe agree with the reviewer that proving existence of spurious local minima for realizable datasets is more difficult than non-realizable ones. When we discuss previous works, what we want to emphasize is that our Theorem 1 holds for *more general* datasets than existing results, without posing assumptions such as realizability. Our Theorem 1 holds for a set of datasets that do not fit linear models, which includes all non-realizable datasets and also *many* realizable datasets. To see why, note that perfect fit to linear models implies realizability, due to a construction similar to our Section 2.2, step 1.\n\n* Regarding other related results:\nFirst, we would like to emphasize that our construction of bad local minima doesn\u2019t require the hidden units to be zero at the same time. They were set to zero just to simplify exposition. As noted at the end of Section 2.1, as long as the bias vector of first hidden node is a large positive number so that $W_1 X + b_1 1_m^T > 0$ entry-wise, the network behaves as a \u201clocally\u201d linear neural network, so any local minima in such a region can only do as well as a linear model, being a bad local minimum. Note that for any one-hidden-layer neural network (no matter how wide), there is a nontrivial (measure > 0) set of such points in the parameter space. We will make this point more explicitly as we revise the paper.\nOur focus in this paper is the existence/nonexistence of spurious local minima in the whole parameter space. We fully agree that the optimization algorithms with proper initialization may not reach such \u201cbad\u201d regions, and that when we restrict the parameter space to some subsets, things may be different. As suggested by the reviewer, we will adjust the claims in our next version of the paper."}, "signatures": ["ICLR.cc/2019/Conference/Paper458/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper458/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper458/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Small nonlinearities in activation functions create bad local minima in neural networks", "abstract": "We investigate the loss surface of neural networks. We prove that even for one-hidden-layer networks with \"slightest\" nonlinearity, the empirical risks have spurious local minima in most cases. Our results thus indicate that in general \"no spurious local minim\" is a property limited to deep linear networks, and insights obtained from linear networks may not be robust. Specifically, for ReLU(-like) networks we constructively prove that for almost all practical datasets there exist infinitely many local minima. We also present a counterexample for more general activations (sigmoid, tanh, arctan, ReLU, etc.), for which there exists a bad local minimum. Our results make the least restrictive assumptions relative to existing results on spurious local optima in neural networks. We complete our discussion by presenting a comprehensive characterization of global optimality for deep linear networks, which unifies other results on this topic.", "keywords": ["spurious local minima", "loss surface", "optimization landscape", "neural network"], "authorids": ["chulheey@mit.edu", "suvrit@mit.edu", "jadbabai@mit.edu"], "authors": ["Chulhee Yun", "Suvrit Sra", "Ali Jadbabaie"], "TL;DR": "We constructively prove that even the slightest nonlinear activation functions introduce spurious local minima, for general datasets and activation functions.", "pdf": "/pdf/9dc34e786d0037c655b4e01acba62d2612460cf2.pdf", "paperhash": "yun|small_nonlinearities_in_activation_functions_create_bad_local_minima_in_neural_networks", "_bibtex": "@inproceedings{\nyun2018small,\ntitle={Small nonlinearities in activation functions create bad local minima in neural networks},\nauthor={Chulhee Yun and Suvrit Sra and Ali Jadbabaie},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rke_YiRct7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper458/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621623527, "tddate": null, "super": null, "final": null, "reply": {"forum": "rke_YiRct7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper458/Authors", "ICLR.cc/2019/Conference/Paper458/Reviewers", "ICLR.cc/2019/Conference/Paper458/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper458/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper458/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper458/Authors|ICLR.cc/2019/Conference/Paper458/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper458/Reviewers", "ICLR.cc/2019/Conference/Paper458/Authors", "ICLR.cc/2019/Conference/Paper458/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621623527}}}, {"id": "H1gdFd3epm", "original": null, "number": 3, "cdate": 1541617792014, "ddate": null, "tcdate": 1541617792014, "tmdate": 1541617792014, "tddate": null, "forum": "rke_YiRct7", "replyto": "rke_YiRct7", "invitation": "ICLR.cc/2019/Conference/-/Paper458/Official_Review", "content": {"title": "Review", "review": "The authors present some theoretical results on the loss surface of neural networks. Their main results are:\n\n(1) They consider a 1 layer hidden neural network where the single nonlinearity is ReLU / ReLU-like. Here they prove that as long as a linear model cannot fit the data, then there exits a local minimum strictly inferior to the global one (They can then scale the parameters to get infinitely many local optima).\n\nThe key idea is to construct a local minima whose risk value is the same as the local least squares solution. Then to construct a set of parameters that has smaller risk value than this local optima. The proof technique is interesting.\n\n(2) They construct a particular dataset for which a one hidden layer neural net with other nonlinear activations (sigmoid, tanh, etc.) also has local optima.\n\nI think this theorem is a bit less interesting since the dataset given has only two data points. I think it is less interesting to prove suboptimality of neural nets in small sample size settings. \n\n(3) Global optimailty of linear networks. The authors show that deep linear networks (i.e. y = W1 W2 W3...W5 x) have only global minima or saddle points.\n\nI'm not familiar enough with the field to know the significant of this result. The deep linear network  just seems like an artificial construction (i.e. in practice one would simply condense W1...W5 to one W) to study nonconvexity / local optima, no one would use it in practice.", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper458/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Small nonlinearities in activation functions create bad local minima in neural networks", "abstract": "We investigate the loss surface of neural networks. We prove that even for one-hidden-layer networks with \"slightest\" nonlinearity, the empirical risks have spurious local minima in most cases. Our results thus indicate that in general \"no spurious local minim\" is a property limited to deep linear networks, and insights obtained from linear networks may not be robust. Specifically, for ReLU(-like) networks we constructively prove that for almost all practical datasets there exist infinitely many local minima. We also present a counterexample for more general activations (sigmoid, tanh, arctan, ReLU, etc.), for which there exists a bad local minimum. Our results make the least restrictive assumptions relative to existing results on spurious local optima in neural networks. We complete our discussion by presenting a comprehensive characterization of global optimality for deep linear networks, which unifies other results on this topic.", "keywords": ["spurious local minima", "loss surface", "optimization landscape", "neural network"], "authorids": ["chulheey@mit.edu", "suvrit@mit.edu", "jadbabai@mit.edu"], "authors": ["Chulhee Yun", "Suvrit Sra", "Ali Jadbabaie"], "TL;DR": "We constructively prove that even the slightest nonlinear activation functions introduce spurious local minima, for general datasets and activation functions.", "pdf": "/pdf/9dc34e786d0037c655b4e01acba62d2612460cf2.pdf", "paperhash": "yun|small_nonlinearities_in_activation_functions_create_bad_local_minima_in_neural_networks", "_bibtex": "@inproceedings{\nyun2018small,\ntitle={Small nonlinearities in activation functions create bad local minima in neural networks},\nauthor={Chulhee Yun and Suvrit Sra and Ali Jadbabaie},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rke_YiRct7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper458/Official_Review", "cdate": 1542234456975, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "rke_YiRct7", "replyto": "rke_YiRct7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper458/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335728732, "tmdate": 1552335728732, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper458/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "HkeS3_Q02X", "original": null, "number": 2, "cdate": 1541449901334, "ddate": null, "tcdate": 1541449901334, "tmdate": 1541533978799, "tddate": null, "forum": "rke_YiRct7", "replyto": "rke_YiRct7", "invitation": "ICLR.cc/2019/Conference/-/Paper458/Official_Review", "content": {"title": "Review of \"Small nonlinearities in activation functions create bad local minima in neural networks\"", "review": "Paper represents theoretical analysis of the loss surface of neural networks. The authors supplied interesting results about local minima properties of neural networks. The paper is written quite well and easy to follow. Furthermore, authors made a comprehensive literature survey and connected their paper to already existing literature. The proofs seem correct (please note that paper is quite long and it requires more time then conference review period, hence, I stated \u201cseem correct\u201d). \nAlthough, paper provides novel theorems, I have several concerns, and these are:\n\u2022\tIsn\u2019t it clear that (generally speaking) a non-convex problem will have many local minima? Previous paper in neural network community is (in my opinion) not theorems. I believe one should read those statements such that in practice (please note here practice means that architectures, e.g. resnet50, inceptionv3, used in day to day life) researchers don\u2019t not observe those \u201creally bad\u201d local minima.\n\u2022\tIn ML literature, we have convex machines which are guaranteed to converge to global optimum. Given image dataset, or text datasets supervised deep learning in in general much better than convex methods e.g. SVMs. The paper clearly shows that there exist, in some cases, exponentially many local minima however  current training methods are able to find better solutions than convex methods (I am completely aware that functions classes are different however success metric, accuracy, is the same). Hence, how relevant are the results without taking in to account architectural choices or optimization methods for deep learning? May be structural risk minimisation is a better approach than empirical risk minimization for quantifying the performance of deep neural networks,\nIn conclusion, paper is interesting however I believe it need to be improved. \n", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper458/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Small nonlinearities in activation functions create bad local minima in neural networks", "abstract": "We investigate the loss surface of neural networks. We prove that even for one-hidden-layer networks with \"slightest\" nonlinearity, the empirical risks have spurious local minima in most cases. Our results thus indicate that in general \"no spurious local minim\" is a property limited to deep linear networks, and insights obtained from linear networks may not be robust. Specifically, for ReLU(-like) networks we constructively prove that for almost all practical datasets there exist infinitely many local minima. We also present a counterexample for more general activations (sigmoid, tanh, arctan, ReLU, etc.), for which there exists a bad local minimum. Our results make the least restrictive assumptions relative to existing results on spurious local optima in neural networks. We complete our discussion by presenting a comprehensive characterization of global optimality for deep linear networks, which unifies other results on this topic.", "keywords": ["spurious local minima", "loss surface", "optimization landscape", "neural network"], "authorids": ["chulheey@mit.edu", "suvrit@mit.edu", "jadbabai@mit.edu"], "authors": ["Chulhee Yun", "Suvrit Sra", "Ali Jadbabaie"], "TL;DR": "We constructively prove that even the slightest nonlinear activation functions introduce spurious local minima, for general datasets and activation functions.", "pdf": "/pdf/9dc34e786d0037c655b4e01acba62d2612460cf2.pdf", "paperhash": "yun|small_nonlinearities_in_activation_functions_create_bad_local_minima_in_neural_networks", "_bibtex": "@inproceedings{\nyun2018small,\ntitle={Small nonlinearities in activation functions create bad local minima in neural networks},\nauthor={Chulhee Yun and Suvrit Sra and Ali Jadbabaie},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rke_YiRct7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper458/Official_Review", "cdate": 1542234456975, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "rke_YiRct7", "replyto": "rke_YiRct7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper458/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335728732, "tmdate": 1552335728732, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper458/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "Hkgdn1QB3X", "original": null, "number": 1, "cdate": 1540857775675, "ddate": null, "tcdate": 1540857775675, "tmdate": 1541533978590, "tddate": null, "forum": "rke_YiRct7", "replyto": "rke_YiRct7", "invitation": "ICLR.cc/2019/Conference/-/Paper458/Official_Review", "content": {"title": "elegant construction; interesting phenomenon", "review": "\nThe authors provide a clean and easily understood sufficient\ncondition for spurious local minima to exist in networks with\na hidden layer using ReLUs or leaky ReLUs.  This condition,\nthat there is not linear transformation with zero loss,\nis satisfied for almost all inputs with more examples than\ninput variables.\n\nThe construction is elegant.  The mathematical writing in the paper,\nespecially describing the proof of Theorem 1, is very nice -- they\nexpose the main ideas effectively.\n\nI do not know of another paper using a similar proof, but I have not\nstudied the proofs of the most closely related papers prior to doing\nthis review, so I have limited ability to vouch for this paper's\ntechnical novelty.\n\nThe authors also show that networks using many other popular\nactivation functions have spurious local minima for a very\nsimple dataset.  All of these analysis are unified using a\nsimple, if technical, set of conditions on activation function.\n\nFinally, the authors prove a somewhat technical theorem about\noptima in deep linear networks, which generalizes some\nearlier treatments of this topic, providing an checkable\ncondition for global minimality.\n\nThere is extensive discussion of related work.  I am not aware of\nrelated work not covered by the authors.\n\nIn some cases, when the authors discuss previous work, they write as\nif restriction to the realizable case is an assumption, when it seems\nto me to be more of a constraint.  In other words, it seems harder to\nprove the existence of spurious minima in the realizable case.\nThey seem to acknowledge this after their statement of their Theorem 2,\nwhich also uses a realizable dataset.\n\nAlso, a few papers, including the Venturi, et al paper cited by\nthe authors, have analyzed whether spurious local minima exist\nin subsets of the parameter space, including those likely to\nbe reached during training with different sorts of initializations.\nIn light of this work, the authors might want to tone down claims\nabout how their work shows that results about linear networks do\nnot generalize to the non-linear case.  In particular, to make\ntheir construction work in the case of wide networks, they\nneed an overwhelming majority of the hidden units to be \"dead\",\nwhich seems as it is unlikely to arise from training with\ncommonly used initializations.\n\nOverall, I think that this paper makes an interesting and\nnon-obvious contribution on a hot topic.", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper458/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Small nonlinearities in activation functions create bad local minima in neural networks", "abstract": "We investigate the loss surface of neural networks. We prove that even for one-hidden-layer networks with \"slightest\" nonlinearity, the empirical risks have spurious local minima in most cases. Our results thus indicate that in general \"no spurious local minim\" is a property limited to deep linear networks, and insights obtained from linear networks may not be robust. Specifically, for ReLU(-like) networks we constructively prove that for almost all practical datasets there exist infinitely many local minima. We also present a counterexample for more general activations (sigmoid, tanh, arctan, ReLU, etc.), for which there exists a bad local minimum. Our results make the least restrictive assumptions relative to existing results on spurious local optima in neural networks. We complete our discussion by presenting a comprehensive characterization of global optimality for deep linear networks, which unifies other results on this topic.", "keywords": ["spurious local minima", "loss surface", "optimization landscape", "neural network"], "authorids": ["chulheey@mit.edu", "suvrit@mit.edu", "jadbabai@mit.edu"], "authors": ["Chulhee Yun", "Suvrit Sra", "Ali Jadbabaie"], "TL;DR": "We constructively prove that even the slightest nonlinear activation functions introduce spurious local minima, for general datasets and activation functions.", "pdf": "/pdf/9dc34e786d0037c655b4e01acba62d2612460cf2.pdf", "paperhash": "yun|small_nonlinearities_in_activation_functions_create_bad_local_minima_in_neural_networks", "_bibtex": "@inproceedings{\nyun2018small,\ntitle={Small nonlinearities in activation functions create bad local minima in neural networks},\nauthor={Chulhee Yun and Suvrit Sra and Ali Jadbabaie},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rke_YiRct7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper458/Official_Review", "cdate": 1542234456975, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "rke_YiRct7", "replyto": "rke_YiRct7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper458/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335728732, "tmdate": 1552335728732, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper458/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 11}