{"notes": [{"id": "AHm3dbp7D1D", "original": "4nfUEMfLWYhV", "number": 149, "cdate": 1601308025336, "ddate": null, "tcdate": 1601308025336, "tmdate": 1615515761182, "tddate": null, "forum": "AHm3dbp7D1D", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "SEED: Self-supervised Distillation For Visual Representation", "authorids": ["~Zhiyuan_Fang1", "~Jianfeng_Wang4", "~Lijuan_Wang1", "~Lei_Zhang23", "~Yezhou_Yang1", "~Zicheng_Liu1"], "authors": ["Zhiyuan Fang", "Jianfeng Wang", "Lijuan Wang", "Lei Zhang", "Yezhou Yang", "Zicheng Liu"], "keywords": ["Self Supervised Learning", "Knowledge Distillation", "Representation Learning"], "abstract": "This paper is concerned with self-supervised learning for small models. The problem is motivated by our empirical studies that while the widely used contrastive self-supervised learning method has shown great progress on large model training, it does not work well for small models. To address this problem, we propose a new learning paradigm, named $\\textbf{SE}$lf-Sup$\\textbf{E}$rvised $\\textbf{D}$istillation (${\\large S}$EED), where we leverage a larger network (as Teacher) to transfer its representational knowledge into a smaller architecture (as Student) in a self-supervised fashion. Instead of directly learning from unlabeled data, we train a student encoder to mimic the similarity score distribution inferred by a teacher over a set of instances. We show that ${\\large S}$EED dramatically boosts the performance of small networks on downstream tasks. Compared with self-supervised baselines, ${\\large S}$EED improves the top-1 accuracy from 42.2% to 67.6% on EfficientNet-B0 and from 36.3% to 68.2% on MobileNet-v3-Large on the ImageNet-1k dataset. ", "one-sentence_summary": "We propose ${\\large S}$EED, a self-supervised distillation technique for visual representation learning.", "pdf": "/pdf/1f85b790d971bca718badf148e1c89397c3c4d90.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "fang|seed_selfsupervised_distillation_for_visual_representation", "supplementary_material": "", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nfang2021seed,\ntitle={{\\{}SEED{\\}}: Self-supervised Distillation For Visual Representation},\nauthor={Zhiyuan Fang and Jianfeng Wang and Lijuan Wang and Lei Zhang and Yezhou Yang and Zicheng Liu},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=AHm3dbp7D1D}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "EumacoGzRvE", "original": null, "number": 1, "cdate": 1610040486177, "ddate": null, "tcdate": 1610040486177, "tmdate": 1610474091561, "tddate": null, "forum": "AHm3dbp7D1D", "replyto": "AHm3dbp7D1D", "invitation": "ICLR.cc/2021/Conference/Paper149/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Poster)", "comment": "There is definite consensus on this paper, with all reviewers expressing very favorable opinions. The author responses are very well articulated and address the main concerns expressed by the reviewers. The paper is very well-written and the ablation study well-executed. Some recent related work was missed in the original submission, but this was adequately addressed in rebuttal. The proposed approach is novel technique for feature representation learning. The clarifications to the manuscript and the new analyses are especially appreciated. "}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SEED: Self-supervised Distillation For Visual Representation", "authorids": ["~Zhiyuan_Fang1", "~Jianfeng_Wang4", "~Lijuan_Wang1", "~Lei_Zhang23", "~Yezhou_Yang1", "~Zicheng_Liu1"], "authors": ["Zhiyuan Fang", "Jianfeng Wang", "Lijuan Wang", "Lei Zhang", "Yezhou Yang", "Zicheng Liu"], "keywords": ["Self Supervised Learning", "Knowledge Distillation", "Representation Learning"], "abstract": "This paper is concerned with self-supervised learning for small models. The problem is motivated by our empirical studies that while the widely used contrastive self-supervised learning method has shown great progress on large model training, it does not work well for small models. To address this problem, we propose a new learning paradigm, named $\\textbf{SE}$lf-Sup$\\textbf{E}$rvised $\\textbf{D}$istillation (${\\large S}$EED), where we leverage a larger network (as Teacher) to transfer its representational knowledge into a smaller architecture (as Student) in a self-supervised fashion. Instead of directly learning from unlabeled data, we train a student encoder to mimic the similarity score distribution inferred by a teacher over a set of instances. We show that ${\\large S}$EED dramatically boosts the performance of small networks on downstream tasks. Compared with self-supervised baselines, ${\\large S}$EED improves the top-1 accuracy from 42.2% to 67.6% on EfficientNet-B0 and from 36.3% to 68.2% on MobileNet-v3-Large on the ImageNet-1k dataset. ", "one-sentence_summary": "We propose ${\\large S}$EED, a self-supervised distillation technique for visual representation learning.", "pdf": "/pdf/1f85b790d971bca718badf148e1c89397c3c4d90.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "fang|seed_selfsupervised_distillation_for_visual_representation", "supplementary_material": "", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nfang2021seed,\ntitle={{\\{}SEED{\\}}: Self-supervised Distillation For Visual Representation},\nauthor={Zhiyuan Fang and Jianfeng Wang and Lijuan Wang and Lei Zhang and Yezhou Yang and Zicheng Liu},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=AHm3dbp7D1D}\n}"}, "tags": [], "invitation": {"reply": {"forum": "AHm3dbp7D1D", "replyto": "AHm3dbp7D1D", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040486164, "tmdate": 1610474091546, "id": "ICLR.cc/2021/Conference/Paper149/-/Decision"}}}, {"id": "iKHBF4lZN-c", "original": null, "number": 4, "cdate": 1605680707135, "ddate": null, "tcdate": 1605680707135, "tmdate": 1605680707135, "tddate": null, "forum": "AHm3dbp7D1D", "replyto": "tN2Oe-tty0_", "invitation": "ICLR.cc/2021/Conference/Paper149/-/Official_Comment", "content": {"title": "We appreciate the reviewer for his/her comments. ", "comment": "We appreciate the reviewer for his/her comments.\n \n**Q13**: a notation change in equation (2).\n\n**A13**: We thank the reviewer for his/her careful reading, and we will address the confusion in our updated version accordingly.\n\n**Q14**: Please move the table\u2019s caption to the top of the table.\n\n**A14**: We will address the caption issue properly in our updated version, and we thank the reviewer for pointing this out."}, "signatures": ["ICLR.cc/2021/Conference/Paper149/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper149/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SEED: Self-supervised Distillation For Visual Representation", "authorids": ["~Zhiyuan_Fang1", "~Jianfeng_Wang4", "~Lijuan_Wang1", "~Lei_Zhang23", "~Yezhou_Yang1", "~Zicheng_Liu1"], "authors": ["Zhiyuan Fang", "Jianfeng Wang", "Lijuan Wang", "Lei Zhang", "Yezhou Yang", "Zicheng Liu"], "keywords": ["Self Supervised Learning", "Knowledge Distillation", "Representation Learning"], "abstract": "This paper is concerned with self-supervised learning for small models. The problem is motivated by our empirical studies that while the widely used contrastive self-supervised learning method has shown great progress on large model training, it does not work well for small models. To address this problem, we propose a new learning paradigm, named $\\textbf{SE}$lf-Sup$\\textbf{E}$rvised $\\textbf{D}$istillation (${\\large S}$EED), where we leverage a larger network (as Teacher) to transfer its representational knowledge into a smaller architecture (as Student) in a self-supervised fashion. Instead of directly learning from unlabeled data, we train a student encoder to mimic the similarity score distribution inferred by a teacher over a set of instances. We show that ${\\large S}$EED dramatically boosts the performance of small networks on downstream tasks. Compared with self-supervised baselines, ${\\large S}$EED improves the top-1 accuracy from 42.2% to 67.6% on EfficientNet-B0 and from 36.3% to 68.2% on MobileNet-v3-Large on the ImageNet-1k dataset. ", "one-sentence_summary": "We propose ${\\large S}$EED, a self-supervised distillation technique for visual representation learning.", "pdf": "/pdf/1f85b790d971bca718badf148e1c89397c3c4d90.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "fang|seed_selfsupervised_distillation_for_visual_representation", "supplementary_material": "", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nfang2021seed,\ntitle={{\\{}SEED{\\}}: Self-supervised Distillation For Visual Representation},\nauthor={Zhiyuan Fang and Jianfeng Wang and Lijuan Wang and Lei Zhang and Yezhou Yang and Zicheng Liu},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=AHm3dbp7D1D}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "AHm3dbp7D1D", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper149/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper149/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper149/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper149/Authors|ICLR.cc/2021/Conference/Paper149/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper149/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923874099, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper149/-/Official_Comment"}}}, {"id": "U69Xa6-H_4M", "original": null, "number": 2, "cdate": 1605678031361, "ddate": null, "tcdate": 1605678031361, "tmdate": 1605680518918, "tddate": null, "forum": "AHm3dbp7D1D", "replyto": "R7b7-3ILkl5", "invitation": "ICLR.cc/2021/Conference/Paper149/-/Official_Comment", "content": {"title": "We thank the reviewer for his/her comments and suggestions.", "comment": "We thank the reviewer for his/her comments and suggestions.  \n\n**Q1**: Performance gains on downstream tasks of detection and instance segmentation are much lower. How would the authors propose to improve these?\n\n**A1**: The detection/segmentation tasks are challenging for several reasons:\n* Following MoCo, we make the parameters of the backbone from self-supervised pre-training/distillation tunable for detection/segmentation fine-tuning. Accordingly, this makes the model initialization less important when the fine-tuning schedule is long enough or with a large number of annotations: e.g., performance gain on COCO is much less obvious than the gain on the VOC dataset. This trend also aligns with the observation in MoCo where limited improvement is obtained when trained under longer (schedule x 2) epochs on COCO: MoCo gains only +0.7 on AP_mk/bb, COCO dataset.\n* Domain gap between ImageNet and COCO. As the distillation is carried out on ImageNet-1k Dataset, we conjecture that the domain gap between pre-training and fine-tuning can yield a negative impact resulting in the diminishing gain.\n\nWe think this issue is indeed an interesting open challenge for future work: how can the generic SSL learned representations consistently perform well when transferred to other tasks/domains. We list several aspects that are potentially useful for addressing this:\n- The inconsistent optimization objectives for pre-training and fine-tuning. Current SSL supervision is solely about image-level discrimination without location information, i.e., image-level classification during SSL, versus object-level localization in fine-tuning. Thus, to design a task-specific objective that incorporates more location information for pre-training/distillation is necessary.\n- Conduct the self-supervised distillation at downstream. This might be helpful to mitigate the domain gaps.  \n\n\n**Q2**: Reviewer suggests to show more analysis on how different design choices of contrastive learning and settings affect performances of small models, and if these could aid performance improvement in addition to knowledge distillation.\n\n**A2**: We study how several different settings in contrastive learning affect SSL performances on small models. We find that expanding deeper MLP projection heads in small models improves the SSL performances (see Sec. A.1.2 of Supplementary materials). Specifically, SSL using MoCo-v2 on EfficientNet-b0 only yields a 39.1% linear evaluation Top-1 Acc. on ImageNet, which is increased to 42.2% after using a deeper MLP head. Compared to the improvement brought by SEED (over 25% Top-1 Acc. gain on EfficientNet), additional variations with other factors like larger memory queue, stronger augmentation do not bring obvious benefits.\nRecent concurrent work \u2018CompRess\u2019 as suggested by the reviewer also finds similar results, that small model suffers from self-supervised training. Notably, by using another state-of-the-art SSL method (i.e., SimCLR), small models still perform rather worse: AlexNet achieves only 46% Top-1 Acc., ResNet18 only achieves 51% Top-1 Acc. after SSL pre-training, which aligns with our observation using MoCo-v2: 52.5% Top-1 Acc using ResNet18.  \n\n\n\n**Q3**: Adding fully-supervised baselines for small models in table 1 will be useful in understanding the gap between full supervision and SSL for these models.\n\n**A3**: We thank the reviewer for pointing out this, and we now have added the fully-supervised baselines for smaller models in Table 1.  \n\n**Q4**: In figure 3, does 100% (green line) represent the student network trained with 100% of labeled imagenet supervised data? It is hard to interpret what these numbers represent.\n\n**A4**: Yes, the green line represents classification results trained with 100% labeled on ImageNet. We have added additional explanations in the caption to make it more clear.  \n\n\n**Q5**: Minor point: Some citations, which should not be in parentheses, are in parentheses (e.g., Romero et al. page 8). Please fix this in the revision.\n\n**A5**: We have addressed this accordingly in the updated version.  \n\n[1] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 9729\u20139738, 2020.2\n[2] CompRess: Self-Supervised Learning by Compressing Representations, Koohpayegani Soroush Abbasi, Tejankar Ajinkya and Pirsiavash Hamed.\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper149/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper149/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SEED: Self-supervised Distillation For Visual Representation", "authorids": ["~Zhiyuan_Fang1", "~Jianfeng_Wang4", "~Lijuan_Wang1", "~Lei_Zhang23", "~Yezhou_Yang1", "~Zicheng_Liu1"], "authors": ["Zhiyuan Fang", "Jianfeng Wang", "Lijuan Wang", "Lei Zhang", "Yezhou Yang", "Zicheng Liu"], "keywords": ["Self Supervised Learning", "Knowledge Distillation", "Representation Learning"], "abstract": "This paper is concerned with self-supervised learning for small models. The problem is motivated by our empirical studies that while the widely used contrastive self-supervised learning method has shown great progress on large model training, it does not work well for small models. To address this problem, we propose a new learning paradigm, named $\\textbf{SE}$lf-Sup$\\textbf{E}$rvised $\\textbf{D}$istillation (${\\large S}$EED), where we leverage a larger network (as Teacher) to transfer its representational knowledge into a smaller architecture (as Student) in a self-supervised fashion. Instead of directly learning from unlabeled data, we train a student encoder to mimic the similarity score distribution inferred by a teacher over a set of instances. We show that ${\\large S}$EED dramatically boosts the performance of small networks on downstream tasks. Compared with self-supervised baselines, ${\\large S}$EED improves the top-1 accuracy from 42.2% to 67.6% on EfficientNet-B0 and from 36.3% to 68.2% on MobileNet-v3-Large on the ImageNet-1k dataset. ", "one-sentence_summary": "We propose ${\\large S}$EED, a self-supervised distillation technique for visual representation learning.", "pdf": "/pdf/1f85b790d971bca718badf148e1c89397c3c4d90.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "fang|seed_selfsupervised_distillation_for_visual_representation", "supplementary_material": "", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nfang2021seed,\ntitle={{\\{}SEED{\\}}: Self-supervised Distillation For Visual Representation},\nauthor={Zhiyuan Fang and Jianfeng Wang and Lijuan Wang and Lei Zhang and Yezhou Yang and Zicheng Liu},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=AHm3dbp7D1D}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "AHm3dbp7D1D", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper149/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper149/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper149/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper149/Authors|ICLR.cc/2021/Conference/Paper149/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper149/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923874099, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper149/-/Official_Comment"}}}, {"id": "rcDd4pfFXf", "original": null, "number": 3, "cdate": 1605680385034, "ddate": null, "tcdate": 1605680385034, "tmdate": 1605680441691, "tddate": null, "forum": "AHm3dbp7D1D", "replyto": "1XoaxQiosYw", "invitation": "ICLR.cc/2021/Conference/Paper149/-/Official_Comment", "content": {"title": "We thank the reviewer for his/her constructive comments.", "comment": "We thank the reviewer for his/her constructive comments.\n\n**Q6**: It is not clear to me only distillation loss is applied or self-supervised learning loss is combined with distillation loss for learning student network.\n\n**A6**: We conduct experiments studying the effect of the original SSL (MoCo-V2) loss and find it does not bring additional benefits on top of self-supervised distillation loss. \nSpecifically, we set-up our experiment using a ResNet-18 distill from a MoCo-V2 pre-trained ResNet-50 for 200 epochs. We maintain two individual instance queues for SEED distillation and MoCo-V2 SSL on Student respectively. After evaluation, we find close results from these two strategies, SEED achieves 57.9%  (Top-1 linear Acc.), while SEED + MoCo-V2 achieves 57.6%. \nThis implies that the loss of SEED can to a large extent cover the original SSL loss, and it is not necessary to add SSL loss. Compared to SEED, SSL loss does not leverage any extra information as the optimization target. As representations from the Teacher have been well pre-trained by the SSL loss, representations from it already fit the principle of instance discrimination. SEED proposes to mimic the Teacher representations, which has the same effect as optimizing SSL objectives. \n\n**Q7**: The caption of Figure 3 is confusing, it would be good to explain it more clearly. \n\n**A7**: Figure 3 shows the semi-supervised linear evaluations on ImageNet, using 1% (red line), and 10% (blue line) of the labeled images, and compares with the full supervision linear evaluations (green line). This experiment shows that the representations are robust when only partial labels are available for fine-tuning. We will make the explanations more clear in our updated version.\n\n**Q8**: Improvements in object detection are small compared to other experiments. And is it possible to use smaller student networks here?\n\n**A8**: Several reasons explain the limited improvement in detection/segmentation tasks: \n- Experimental settings. We follow MoCo and make parameters on these two tasks tuneable during fine-tuning with fairly long training epochs (schedule X 2). This makes the model initialization less important when the fine-tuning schedule is long enough or with a large number of annotations. See more discussion in A1.\n- The domain gap between pre-training and fine-tuning can yield a negative impact resulting in a diminishing gain.\nTable 2 in the main paper actually shows the results of a relatively small detector (using ResNet18 as the backbone). We additionally show the results of larger architectures (ResNet34 and ResNet50) in the Appendix. We will try even smaller networks in our future version.\n\n**Q9**: In the experiment of different sizes of sample queue, does it mean that the larger the better? What is the intuition behind it? \n\n**A9**: We studied the effect of larger instance queues in Sec. A.1.8, Appendix, where we compare the different sizes of the queue from 128 to 65,536. Empirically, we find that the use of a larger queue can bring only marginal performance improvement: 63.3% for 65,536 and 61.7% for 128. Further enlarging the instance queue does not obviously contribute to the results. \nIntuitively, more samples in a queue mean more contrastiveness to help the representation to be discriminative to more negative samples while maintaining similarity with the positive sample. However, extra negative samples could also reduce the importance of being similar to the positive sample to some extent. Thus, an even larger queue does bring improvement, however is limited.\n\n\n**Q10**: Why does the paper decide to use the same data augmentation for SSL/SEED?\n\n**A10**: Intuitively, stronger augmentation provides the training with more diverse images, yielding representations with better generalization. From another aspect, SSL and SEED have much longer training epochs than the supervised training. In such a case, weaker augmentation can make the network overfit to the less-augmented image.\nWe find that it is important to maintain a consistent data distribution between the SSL pre-training and distillation as SEED has a similar optimization objective to SSL. \nFor these reasons, we choose to use identical augmentations as SSL pre-training, which is different from traditional distillation settings. We will attach additional ablations to study the effects of various augmentations in the future.\n\n**Q11**: Add related work of CompRess.\n\n**A11**: We thank the reviewer for bringing up CompRess as a concurrent work for us. We will add this as related work in our updated version.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper149/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper149/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SEED: Self-supervised Distillation For Visual Representation", "authorids": ["~Zhiyuan_Fang1", "~Jianfeng_Wang4", "~Lijuan_Wang1", "~Lei_Zhang23", "~Yezhou_Yang1", "~Zicheng_Liu1"], "authors": ["Zhiyuan Fang", "Jianfeng Wang", "Lijuan Wang", "Lei Zhang", "Yezhou Yang", "Zicheng Liu"], "keywords": ["Self Supervised Learning", "Knowledge Distillation", "Representation Learning"], "abstract": "This paper is concerned with self-supervised learning for small models. The problem is motivated by our empirical studies that while the widely used contrastive self-supervised learning method has shown great progress on large model training, it does not work well for small models. To address this problem, we propose a new learning paradigm, named $\\textbf{SE}$lf-Sup$\\textbf{E}$rvised $\\textbf{D}$istillation (${\\large S}$EED), where we leverage a larger network (as Teacher) to transfer its representational knowledge into a smaller architecture (as Student) in a self-supervised fashion. Instead of directly learning from unlabeled data, we train a student encoder to mimic the similarity score distribution inferred by a teacher over a set of instances. We show that ${\\large S}$EED dramatically boosts the performance of small networks on downstream tasks. Compared with self-supervised baselines, ${\\large S}$EED improves the top-1 accuracy from 42.2% to 67.6% on EfficientNet-B0 and from 36.3% to 68.2% on MobileNet-v3-Large on the ImageNet-1k dataset. ", "one-sentence_summary": "We propose ${\\large S}$EED, a self-supervised distillation technique for visual representation learning.", "pdf": "/pdf/1f85b790d971bca718badf148e1c89397c3c4d90.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "fang|seed_selfsupervised_distillation_for_visual_representation", "supplementary_material": "", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nfang2021seed,\ntitle={{\\{}SEED{\\}}: Self-supervised Distillation For Visual Representation},\nauthor={Zhiyuan Fang and Jianfeng Wang and Lijuan Wang and Lei Zhang and Yezhou Yang and Zicheng Liu},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=AHm3dbp7D1D}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "AHm3dbp7D1D", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper149/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper149/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper149/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper149/Authors|ICLR.cc/2021/Conference/Paper149/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper149/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923874099, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper149/-/Official_Comment"}}}, {"id": "tN2Oe-tty0_", "original": null, "number": 1, "cdate": 1603363088669, "ddate": null, "tcdate": 1603363088669, "tmdate": 1605024753317, "tddate": null, "forum": "AHm3dbp7D1D", "replyto": "AHm3dbp7D1D", "invitation": "ICLR.cc/2021/Conference/Paper149/-/Official_Review", "content": {"title": "Interesting work and solid results with various network architecture", "review": "Summary: \nThe paper address the problem of knowledge distillation in self-supervised learning, where the representational knowledge from the larger model (i.e., teacher) is used to guide the learning of a smaller model (i.e, student). To achieve this, an instance queue is used to compute the similarity score between teacher model features and the feature of a given image, and the learning objective is to minimise the cross-entropy loss of the similarity between teacher and student models. The paper provides comprehensive empirical results to justify the efficacy of the proposed approach.\n\nJustification of rating:\nThe paper provides a comprehensive evaluation of the proposed approach on various network architecture and downstream tasks. Overall, I feel this work does not have sufficient theoretical or algorithmic contributions. The key contribution is the idea of apply knowledge distillation for self-supervised learning.\n\nStrengths:\n+ This is the first work that addresses self-supervised learning (SSL) with knowledge distillation. It empirically shows SSL with a small model is challenging (consistent with finding from Chen et al., 2020a;b), and proposed a technique (SEED) to transfer knowledge.\n+ This paper provides comprehensive experiment on image classification task (self-supervised, semi-supervised and supervised), object detection/segmentation, domain transfer, as well as provide ablation studies on various model architecture and parameters. The results show knowledge distillation is effective for self-supervised learning. \n\nWeakness: \n- The core novelty of this work is the idea of conduct knowledge distillation in self-supervised learning. The key weakness is that the knowledge distillation approach and the instance queue approach are previously proposed and known to the research community. This work empirically shows how it can be combined for the task on hand.\n\nMinor comments: \n- In section 3.2, before eqn (2), it is best to change \u201cthe similarity score between $x_i$ and $\\vec{d}_j$\u2019s\u201d to \"the similarity between the extracted (teacher/student) feature $z_i$ and $\\vec{d}_j$\u2019s\u201d. This is to avoid confusion as one might wonder how can the similarity between the input image and the $\\vec{d}_j$ be computed.\n- Please move the table\u2019s caption to the top of the table.", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper149/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper149/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SEED: Self-supervised Distillation For Visual Representation", "authorids": ["~Zhiyuan_Fang1", "~Jianfeng_Wang4", "~Lijuan_Wang1", "~Lei_Zhang23", "~Yezhou_Yang1", "~Zicheng_Liu1"], "authors": ["Zhiyuan Fang", "Jianfeng Wang", "Lijuan Wang", "Lei Zhang", "Yezhou Yang", "Zicheng Liu"], "keywords": ["Self Supervised Learning", "Knowledge Distillation", "Representation Learning"], "abstract": "This paper is concerned with self-supervised learning for small models. The problem is motivated by our empirical studies that while the widely used contrastive self-supervised learning method has shown great progress on large model training, it does not work well for small models. To address this problem, we propose a new learning paradigm, named $\\textbf{SE}$lf-Sup$\\textbf{E}$rvised $\\textbf{D}$istillation (${\\large S}$EED), where we leverage a larger network (as Teacher) to transfer its representational knowledge into a smaller architecture (as Student) in a self-supervised fashion. Instead of directly learning from unlabeled data, we train a student encoder to mimic the similarity score distribution inferred by a teacher over a set of instances. We show that ${\\large S}$EED dramatically boosts the performance of small networks on downstream tasks. Compared with self-supervised baselines, ${\\large S}$EED improves the top-1 accuracy from 42.2% to 67.6% on EfficientNet-B0 and from 36.3% to 68.2% on MobileNet-v3-Large on the ImageNet-1k dataset. ", "one-sentence_summary": "We propose ${\\large S}$EED, a self-supervised distillation technique for visual representation learning.", "pdf": "/pdf/1f85b790d971bca718badf148e1c89397c3c4d90.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "fang|seed_selfsupervised_distillation_for_visual_representation", "supplementary_material": "", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nfang2021seed,\ntitle={{\\{}SEED{\\}}: Self-supervised Distillation For Visual Representation},\nauthor={Zhiyuan Fang and Jianfeng Wang and Lijuan Wang and Lei Zhang and Yezhou Yang and Zicheng Liu},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=AHm3dbp7D1D}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "AHm3dbp7D1D", "replyto": "AHm3dbp7D1D", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper149/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538149338, "tmdate": 1606915796431, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper149/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper149/-/Official_Review"}}}, {"id": "R7b7-3ILkl5", "original": null, "number": 3, "cdate": 1603947355650, "ddate": null, "tcdate": 1603947355650, "tmdate": 1605024753245, "tddate": null, "forum": "AHm3dbp7D1D", "replyto": "AHm3dbp7D1D", "invitation": "ICLR.cc/2021/Conference/Paper149/-/Official_Review", "content": {"title": "Simple approach for SSL knowledge distillation with good results", "review": "Summary: This paper proposes a  knowledge distillation (KD) approach for self-supervised learning (SSL) with small neural network models. The authors first observe that the state-of-the-art contrastive learning-based SSL does not obtain good performance on small models, due  to the larger  model capacity required for instance discrimination. To tackle this problem, they propose a SEED, a  KD method where the smaller student model learns to mimic its larger teacher model\u2019s similarity distribution between an instance and its augmented views, using a cross-entropy based objective. The authors perform various experiments to show that -- 1)  SEED obtains substantial improvement in SSL-based imagenet classification performance for small models as compared to SSL training without SEED, 2) the performance gains are also substantial for transfer learning on other classification tasks, 3) the performance gains are smaller for downstream tasks of object detection and instance segmentation, with performance gains reducing for the larger COCO dataset, as compared to VOC, 4) SEED is robust to choice of SSL method, and performs better than other KD approaches.\n\nStrengths: The paper is clearly written and well-organized. The SEED approach  is well-motivated and sensible. Experimental validation and the ablation studies are quite thorough. Performance gains on classification tasks are substantial. The method is simple to implement. \n\nWeaknesses: 1. Performance gains on downstream tasks of detection and instance segmentation are much lower -- how would the authors propose to improve these? 2. If the primary goal is to improve SSL performance on small models, I would have liked to see more analysis on how different design choices of setting up contrastive learning affect model performance and if these could aid performance improvement, in addition to knowledge distillation. \n\nQuestions and suggestions: 1. Adding fully-supervised baselines for small models in table 1 will be useful in understanding the gap between full supervision and SSL for these models. 2. In figure 3, does 100% (green line) represent the student network trained with 100% of labeled imagenet supervised data? It is hard to interpret what these numbers represent. 3. Minor point: Some citations, which should not be in parentheses, are in parentheses (e.g., Romero et al. page 8). Please fix this in the revision. \n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper149/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper149/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SEED: Self-supervised Distillation For Visual Representation", "authorids": ["~Zhiyuan_Fang1", "~Jianfeng_Wang4", "~Lijuan_Wang1", "~Lei_Zhang23", "~Yezhou_Yang1", "~Zicheng_Liu1"], "authors": ["Zhiyuan Fang", "Jianfeng Wang", "Lijuan Wang", "Lei Zhang", "Yezhou Yang", "Zicheng Liu"], "keywords": ["Self Supervised Learning", "Knowledge Distillation", "Representation Learning"], "abstract": "This paper is concerned with self-supervised learning for small models. The problem is motivated by our empirical studies that while the widely used contrastive self-supervised learning method has shown great progress on large model training, it does not work well for small models. To address this problem, we propose a new learning paradigm, named $\\textbf{SE}$lf-Sup$\\textbf{E}$rvised $\\textbf{D}$istillation (${\\large S}$EED), where we leverage a larger network (as Teacher) to transfer its representational knowledge into a smaller architecture (as Student) in a self-supervised fashion. Instead of directly learning from unlabeled data, we train a student encoder to mimic the similarity score distribution inferred by a teacher over a set of instances. We show that ${\\large S}$EED dramatically boosts the performance of small networks on downstream tasks. Compared with self-supervised baselines, ${\\large S}$EED improves the top-1 accuracy from 42.2% to 67.6% on EfficientNet-B0 and from 36.3% to 68.2% on MobileNet-v3-Large on the ImageNet-1k dataset. ", "one-sentence_summary": "We propose ${\\large S}$EED, a self-supervised distillation technique for visual representation learning.", "pdf": "/pdf/1f85b790d971bca718badf148e1c89397c3c4d90.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "fang|seed_selfsupervised_distillation_for_visual_representation", "supplementary_material": "", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nfang2021seed,\ntitle={{\\{}SEED{\\}}: Self-supervised Distillation For Visual Representation},\nauthor={Zhiyuan Fang and Jianfeng Wang and Lijuan Wang and Lei Zhang and Yezhou Yang and Zicheng Liu},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=AHm3dbp7D1D}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "AHm3dbp7D1D", "replyto": "AHm3dbp7D1D", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper149/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538149338, "tmdate": 1606915796431, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper149/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper149/-/Official_Review"}}}, {"id": "1XoaxQiosYw", "original": null, "number": 2, "cdate": 1603553907280, "ddate": null, "tcdate": 1603553907280, "tmdate": 1605024753184, "tddate": null, "forum": "AHm3dbp7D1D", "replyto": "AHm3dbp7D1D", "invitation": "ICLR.cc/2021/Conference/Paper149/-/Official_Review", "content": {"title": "It is interesting to see distillation applied to self-supervised learning. Experimental results are comprehensive. ", "review": "The paper proposes to distill knowledge from large teacher networks to small student networks in self-supervised learning. Experimental results show significant improvements on small networks.\n\nConcerns:\n\n- During the self-supervised distillation phase, it is not clear to me only distillation loss is applied or self-supervised learning loss is combined with distillation loss for learning student network. If only distillation loss is applied, does it make sense to train a student network using both self-supervised learning loss and distillation loss, such as MoCo-v2 used in most experiments? \n- The caption of Figure 3 is confusing, it would be good to explain it more clearly.\n- It seems that improvements on object detection and instance segmentation (Table 2) are relatively small compared to other experiments, are there any explanations? Could it be possible to use smaller student networks in this experiment as well? \n- In the experiment of different sizes of sample queue, does it mean that the larger the better? What is the intuition behind it?\n- Strong data augmentation is needed for most of self-supervised methods, but normally for distillation, it is not common to use very strong data augmentation, why does the paper decide to use the same data augmentation for both self-supervised learning and distillation learning?\n- It is quite similar to a very recent paper as shown in the following, it would be good to discuss the differences in the paper.\n\n@inproceedings{koohpayegani2020compress,\n  title={CompRess: Self-Supervised Learning by Compressing Representations},\n  author={Koohpayegani, Soroush Abbasi and Tejankar, Ajinkya and Pirsiavash, Hamed},\n  booktitle={NeurIPS},\n  year={2020}\n}", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper149/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper149/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SEED: Self-supervised Distillation For Visual Representation", "authorids": ["~Zhiyuan_Fang1", "~Jianfeng_Wang4", "~Lijuan_Wang1", "~Lei_Zhang23", "~Yezhou_Yang1", "~Zicheng_Liu1"], "authors": ["Zhiyuan Fang", "Jianfeng Wang", "Lijuan Wang", "Lei Zhang", "Yezhou Yang", "Zicheng Liu"], "keywords": ["Self Supervised Learning", "Knowledge Distillation", "Representation Learning"], "abstract": "This paper is concerned with self-supervised learning for small models. The problem is motivated by our empirical studies that while the widely used contrastive self-supervised learning method has shown great progress on large model training, it does not work well for small models. To address this problem, we propose a new learning paradigm, named $\\textbf{SE}$lf-Sup$\\textbf{E}$rvised $\\textbf{D}$istillation (${\\large S}$EED), where we leverage a larger network (as Teacher) to transfer its representational knowledge into a smaller architecture (as Student) in a self-supervised fashion. Instead of directly learning from unlabeled data, we train a student encoder to mimic the similarity score distribution inferred by a teacher over a set of instances. We show that ${\\large S}$EED dramatically boosts the performance of small networks on downstream tasks. Compared with self-supervised baselines, ${\\large S}$EED improves the top-1 accuracy from 42.2% to 67.6% on EfficientNet-B0 and from 36.3% to 68.2% on MobileNet-v3-Large on the ImageNet-1k dataset. ", "one-sentence_summary": "We propose ${\\large S}$EED, a self-supervised distillation technique for visual representation learning.", "pdf": "/pdf/1f85b790d971bca718badf148e1c89397c3c4d90.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "fang|seed_selfsupervised_distillation_for_visual_representation", "supplementary_material": "", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nfang2021seed,\ntitle={{\\{}SEED{\\}}: Self-supervised Distillation For Visual Representation},\nauthor={Zhiyuan Fang and Jianfeng Wang and Lijuan Wang and Lei Zhang and Yezhou Yang and Zicheng Liu},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=AHm3dbp7D1D}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "AHm3dbp7D1D", "replyto": "AHm3dbp7D1D", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper149/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538149338, "tmdate": 1606915796431, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper149/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper149/-/Official_Review"}}}], "count": 8}