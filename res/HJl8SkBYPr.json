{"notes": [{"id": "HJl8SkBYPr", "original": "Syl_fGpOwr", "number": 1693, "cdate": 1569439550340, "ddate": null, "tcdate": 1569439550340, "tmdate": 1577168222943, "tddate": null, "forum": "HJl8SkBYPr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["mgao@cs.umd.edu", "zizhaoz@google.com", "gy63@uw.edu", "soarik@google.com", "lsd@umiacs.umd.edu", "tpfister@google.com"], "title": "Consistency-Based Semi-Supervised Active Learning: Towards Minimizing Labeling Budget", "authors": ["Mingfei Gao", "Zizhao Zhang", "Guo Yu", "Sercan O. Arik", "Larry S. Davis", "Tomas Pfister"], "pdf": "/pdf/5d12c5148598d22be0d40a3eb8902c226f3c0cb9.pdf", "abstract": "Active learning (AL) aims to integrate data labeling and model training in a unified way, and to minimize the labeling budget by prioritizing the selection of high value data that can best improve model performance. Readily-available unlabeled data are used to evaluate selection mechanisms, but are not used for model training in conventional pool-based AL. To minimize the labeling budget, we unify unlabeled sample selection and model training based on two principles. First, we exploit both labeled and unlabeled data using semi-supervised learning (SSL) to distill information from unlabeled data that improves representation learning and sample selection. Second, we propose a simple yet effective selection metric that is coherent with the training objective such that the selected samples are effective at improving model performance. Our experimental results demonstrate superior performance with our proposed principles for limited labeled data compared to alternative AL and SSL combinations.  In addition, we study the AL phenomena of `cold start', which is becoming an increasingly more important factor to enable optimal unification of data labeling, model training and labeling budget minimization.  We propose a measure that is found to be empirically correlated with the AL target loss. This measure can be used to assist in determining the proper start size.", "keywords": ["Active learning", "semi-supervised learning"], "paperhash": "gao|consistencybased_semisupervised_active_learning_towards_minimizing_labeling_budget", "original_pdf": "/attachment/8375c90461c44de873d7b957ca66ffb9d3e39adf.pdf", "_bibtex": "@misc{\ngao2020consistencybased,\ntitle={Consistency-Based Semi-Supervised Active Learning: Towards Minimizing Labeling Budget},\nauthor={Mingfei Gao and Zizhao Zhang and Guo Yu and Sercan O. Arik and Larry S. Davis and Tomas Pfister},\nyear={2020},\nurl={https://openreview.net/forum?id=HJl8SkBYPr}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 12, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "LUTeBSo6ik", "original": null, "number": 1, "cdate": 1576798730035, "ddate": null, "tcdate": 1576798730035, "tmdate": 1576800906476, "tddate": null, "forum": "HJl8SkBYPr", "replyto": "HJl8SkBYPr", "invitation": "ICLR.cc/2020/Conference/Paper1693/-/Decision", "content": {"decision": "Reject", "comment": "The authors leverage advances in semi-supervised learning and data augmentation to propose a method for active learning. The AL method is based on the principle that a model should consistently label across perturbation/augmentations of examples, and thus propose to choose samples for active learning based on how much the estimated label distribution changes based on different perturbations of a given example. The method is intuitive and the experiments provide some evidence of efficacy. However, during discussion there was a lingering question of novelty that eventually swayed the group to reject this paper. ", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["mgao@cs.umd.edu", "zizhaoz@google.com", "gy63@uw.edu", "soarik@google.com", "lsd@umiacs.umd.edu", "tpfister@google.com"], "title": "Consistency-Based Semi-Supervised Active Learning: Towards Minimizing Labeling Budget", "authors": ["Mingfei Gao", "Zizhao Zhang", "Guo Yu", "Sercan O. Arik", "Larry S. Davis", "Tomas Pfister"], "pdf": "/pdf/5d12c5148598d22be0d40a3eb8902c226f3c0cb9.pdf", "abstract": "Active learning (AL) aims to integrate data labeling and model training in a unified way, and to minimize the labeling budget by prioritizing the selection of high value data that can best improve model performance. Readily-available unlabeled data are used to evaluate selection mechanisms, but are not used for model training in conventional pool-based AL. To minimize the labeling budget, we unify unlabeled sample selection and model training based on two principles. First, we exploit both labeled and unlabeled data using semi-supervised learning (SSL) to distill information from unlabeled data that improves representation learning and sample selection. Second, we propose a simple yet effective selection metric that is coherent with the training objective such that the selected samples are effective at improving model performance. Our experimental results demonstrate superior performance with our proposed principles for limited labeled data compared to alternative AL and SSL combinations.  In addition, we study the AL phenomena of `cold start', which is becoming an increasingly more important factor to enable optimal unification of data labeling, model training and labeling budget minimization.  We propose a measure that is found to be empirically correlated with the AL target loss. This measure can be used to assist in determining the proper start size.", "keywords": ["Active learning", "semi-supervised learning"], "paperhash": "gao|consistencybased_semisupervised_active_learning_towards_minimizing_labeling_budget", "original_pdf": "/attachment/8375c90461c44de873d7b957ca66ffb9d3e39adf.pdf", "_bibtex": "@misc{\ngao2020consistencybased,\ntitle={Consistency-Based Semi-Supervised Active Learning: Towards Minimizing Labeling Budget},\nauthor={Mingfei Gao and Zizhao Zhang and Guo Yu and Sercan O. Arik and Larry S. Davis and Tomas Pfister},\nyear={2020},\nurl={https://openreview.net/forum?id=HJl8SkBYPr}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "HJl8SkBYPr", "replyto": "HJl8SkBYPr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795712911, "tmdate": 1576800262401, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1693/-/Decision"}}}, {"id": "SkxcrNYsiS", "original": null, "number": 6, "cdate": 1573782593520, "ddate": null, "tcdate": 1573782593520, "tmdate": 1573782593520, "tddate": null, "forum": "HJl8SkBYPr", "replyto": "HJl8SkBYPr", "invitation": "ICLR.cc/2020/Conference/Paper1693/-/Official_Comment", "content": {"title": "Updates of the manuscript ", "comment": "Thanks again for all the valuable comments. We have updated our manuscript. \nThe current version includes the following changes.\n\n1. We improved our writing (fixed typos and grammar issues etc.). \n2. We revised some confusing statements.\n3. We revised the manuscript according to Q2, Q3 and Q5 of the Reviewer 1.\n4. We revised the manuscript according to Q1 of the Reviewer 2.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1693/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1693/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["mgao@cs.umd.edu", "zizhaoz@google.com", "gy63@uw.edu", "soarik@google.com", "lsd@umiacs.umd.edu", "tpfister@google.com"], "title": "Consistency-Based Semi-Supervised Active Learning: Towards Minimizing Labeling Budget", "authors": ["Mingfei Gao", "Zizhao Zhang", "Guo Yu", "Sercan O. Arik", "Larry S. Davis", "Tomas Pfister"], "pdf": "/pdf/5d12c5148598d22be0d40a3eb8902c226f3c0cb9.pdf", "abstract": "Active learning (AL) aims to integrate data labeling and model training in a unified way, and to minimize the labeling budget by prioritizing the selection of high value data that can best improve model performance. Readily-available unlabeled data are used to evaluate selection mechanisms, but are not used for model training in conventional pool-based AL. To minimize the labeling budget, we unify unlabeled sample selection and model training based on two principles. First, we exploit both labeled and unlabeled data using semi-supervised learning (SSL) to distill information from unlabeled data that improves representation learning and sample selection. Second, we propose a simple yet effective selection metric that is coherent with the training objective such that the selected samples are effective at improving model performance. Our experimental results demonstrate superior performance with our proposed principles for limited labeled data compared to alternative AL and SSL combinations.  In addition, we study the AL phenomena of `cold start', which is becoming an increasingly more important factor to enable optimal unification of data labeling, model training and labeling budget minimization.  We propose a measure that is found to be empirically correlated with the AL target loss. This measure can be used to assist in determining the proper start size.", "keywords": ["Active learning", "semi-supervised learning"], "paperhash": "gao|consistencybased_semisupervised_active_learning_towards_minimizing_labeling_budget", "original_pdf": "/attachment/8375c90461c44de873d7b957ca66ffb9d3e39adf.pdf", "_bibtex": "@misc{\ngao2020consistencybased,\ntitle={Consistency-Based Semi-Supervised Active Learning: Towards Minimizing Labeling Budget},\nauthor={Mingfei Gao and Zizhao Zhang and Guo Yu and Sercan O. Arik and Larry S. Davis and Tomas Pfister},\nyear={2020},\nurl={https://openreview.net/forum?id=HJl8SkBYPr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJl8SkBYPr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1693/Authors", "ICLR.cc/2020/Conference/Paper1693/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1693/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1693/Reviewers", "ICLR.cc/2020/Conference/Paper1693/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1693/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1693/Authors|ICLR.cc/2020/Conference/Paper1693/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504152262, "tmdate": 1576860558026, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1693/Authors", "ICLR.cc/2020/Conference/Paper1693/Reviewers", "ICLR.cc/2020/Conference/Paper1693/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1693/-/Official_Comment"}}}, {"id": "ByeD3J4SoB", "original": null, "number": 4, "cdate": 1573367727255, "ddate": null, "tcdate": 1573367727255, "tmdate": 1573782221056, "tddate": null, "forum": "HJl8SkBYPr", "replyto": "rkghWIQitS", "invitation": "ICLR.cc/2020/Conference/Paper1693/-/Official_Comment", "content": {"title": "(Updated) Thanks for your thoughtful suggestions.", "comment": "Thanks for your thoughtful comments. Please see our responses as follows.\n\nQ1. The consistency of a sample is measured based on the perturbed samples. How to generate these perturbed samples may have a great influence on the query results. In the paper, it said that these samples are generated by standard augmentation operations (e.g. random crops and horizontal flips for image data). This representation is hard to follow in the experiments. If possible, it is better to show in details.\n\nOur response: We augment the input images following (Berthelot et al., 2019). The input images are randomly flipped (left-right-random-flip code link: https://github.com/google-research/mixmatch/blob/1011a1d51eaa9ca6f5dba02096a848d1fe3fc38e/libml/data.py#L87) and then randomly cropped  (code link: https://github.com/google-research/mixmatch/blob/1011a1d51eaa9ca6f5dba02096a848d1fe3fc38e/libml/data.py#L91). \n\nWe have added these details in the updated version.\n\nQ2. In the uncertainty of active learning, the samples are selected from different distributions in the unlabeled data, for example, the marginal sampling selects the samples around the classification hyperlanes (Settles, Burr. \"Active learning.\" Synthesis Lectures on Artificial Intelligence and Machine Learning 6.1 (2012): 1-114.). Can you show which samples may be selected in the unlabeled data. In this way, the proposed criterion can be followed more easily.\n\nOur response: It is a great idea to analyze the selected samples. We analyze the selected samples and compare with those selected by other methods in Figure 2 and Figure 3. \n\nFigure 2 shows our selected samples tend to have high average entropy (uncertainty). On the other hand, uncertain samples are not always informative. For example, they may also promote focusing outlier samples outside the distribution of data which may harm the model performance in some cases. As shown in Figure 2 (middle), top-ranked samples chosen by our AL method can sometimes have low entropy as well - our metric considers uncertainty, but can also deviate from it in some cases. Figure 3 underlines that our method tends to focus on diversity in selection. These results suggest that the selected samples consider a combination of uncertainty and diversity attributes in different regimes.\n\nQ3. In the experiments, the proposed method can select a batch of samples at each iteration. How about the influence of the batch size. \n\nOur response: There is always a trade-off between a large AL batch size and a small AL batch size. Ideally, we would like to use AL as much as possible. Selecting a very large batch of samples will lead to insufficient usage of active learning given a limited budget. However, a very small AL batch size would lead to much more AL cycles which is computationally expensive.\n\nOur method is effective using reasonable AL batch sizes. We conducted more experiments on CIFAR-10 under the setting of Figure 1 (starting from 100 labeled samples), using different AL batch sizes. Our experiments show that when labeling 200 samples in total, we obtain accuracy of 89.5%, 89.2% and 89.3% when AL batch size is set to be 25, 50 and 100, respectively. These results suggest that, the performances are comparable using reasonable AL batch sizes.\n\n----------------------------------------------------------------------------------------------------------------                                  \n|                     AL batch size                                |        25       |         50        |       100       |\n----------------------------------------------------------------------------------------------------------------\n|# of AL cycles to reach 200 labeled data      |         4        |          2         |          1         |   \n----------------------------------------------------------------------------------------------------------------\n|                             Accuracy                                |      89.5%  |       89.2%    |        89.3%  |\n----------------------------------------------------------------------------------------------------------------\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1693/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1693/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["mgao@cs.umd.edu", "zizhaoz@google.com", "gy63@uw.edu", "soarik@google.com", "lsd@umiacs.umd.edu", "tpfister@google.com"], "title": "Consistency-Based Semi-Supervised Active Learning: Towards Minimizing Labeling Budget", "authors": ["Mingfei Gao", "Zizhao Zhang", "Guo Yu", "Sercan O. Arik", "Larry S. Davis", "Tomas Pfister"], "pdf": "/pdf/5d12c5148598d22be0d40a3eb8902c226f3c0cb9.pdf", "abstract": "Active learning (AL) aims to integrate data labeling and model training in a unified way, and to minimize the labeling budget by prioritizing the selection of high value data that can best improve model performance. Readily-available unlabeled data are used to evaluate selection mechanisms, but are not used for model training in conventional pool-based AL. To minimize the labeling budget, we unify unlabeled sample selection and model training based on two principles. First, we exploit both labeled and unlabeled data using semi-supervised learning (SSL) to distill information from unlabeled data that improves representation learning and sample selection. Second, we propose a simple yet effective selection metric that is coherent with the training objective such that the selected samples are effective at improving model performance. Our experimental results demonstrate superior performance with our proposed principles for limited labeled data compared to alternative AL and SSL combinations.  In addition, we study the AL phenomena of `cold start', which is becoming an increasingly more important factor to enable optimal unification of data labeling, model training and labeling budget minimization.  We propose a measure that is found to be empirically correlated with the AL target loss. This measure can be used to assist in determining the proper start size.", "keywords": ["Active learning", "semi-supervised learning"], "paperhash": "gao|consistencybased_semisupervised_active_learning_towards_minimizing_labeling_budget", "original_pdf": "/attachment/8375c90461c44de873d7b957ca66ffb9d3e39adf.pdf", "_bibtex": "@misc{\ngao2020consistencybased,\ntitle={Consistency-Based Semi-Supervised Active Learning: Towards Minimizing Labeling Budget},\nauthor={Mingfei Gao and Zizhao Zhang and Guo Yu and Sercan O. Arik and Larry S. Davis and Tomas Pfister},\nyear={2020},\nurl={https://openreview.net/forum?id=HJl8SkBYPr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJl8SkBYPr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1693/Authors", "ICLR.cc/2020/Conference/Paper1693/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1693/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1693/Reviewers", "ICLR.cc/2020/Conference/Paper1693/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1693/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1693/Authors|ICLR.cc/2020/Conference/Paper1693/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504152262, "tmdate": 1576860558026, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1693/Authors", "ICLR.cc/2020/Conference/Paper1693/Reviewers", "ICLR.cc/2020/Conference/Paper1693/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1693/-/Official_Comment"}}}, {"id": "SygZ4p7rsS", "original": null, "number": 3, "cdate": 1573367080721, "ddate": null, "tcdate": 1573367080721, "tmdate": 1573780599466, "tddate": null, "forum": "HJl8SkBYPr", "replyto": "HyeG10GntB", "invitation": "ICLR.cc/2020/Conference/Paper1693/-/Official_Comment", "content": {"title": "(Updated) Thanks for your valuable suggestions.", "comment": "Thanks for your valuable  comments. Please see our responses as follows.\n\nQ1. This paper compares in Table 1 the difference between just active learning vs active learning + SSL. I'm not sure this is a fair comparison. I think the better comparison is shown in Table 2. \n\nOur response: We agree with you that the Table 2 presents the fair comparison among different selection metrics. As we mentioned in the introduction, many existing active learning methods train their models using only labeled samples at each AL cycle. The purpose of Table 1 is to motivate our work by showing the effectiveness of involving SSL in active learning, in other words demonstrating the value of using unlabeled data for learning.\n\nQ2. The authors write that \"when only 100 samples are labeled, our method outperforms kcenter by 39.24% accuracy\". Do the authors mean after 100 additional labels are acquired (so 200 labels) or is this number off?\n\nOur response: Thanks for pointing this out. It is after 100 additional labels are acquired. We have clarified this in the updated version.\n\nQ3. Can the authors clarify what is meant by \"or some labels correspond to rare cases, as in self-driving cars\"? Why are such datasets more costly to acquire? Is it because of the size of the self-driving car datasets?\n\nOur response: Self-driving car datasets have long-tail distribution, generally consisting of vast amount of normal driving conditions, and various rare cases, e.g., children playing on the road and pedestrian lying on the street. When data samples are uniformly sampled to be labeled, to obtain enough samples to represent rare cases, one would need to label abundantly high number of examples for common cases. AL method is potentially useful in this scenario where it can make labeling suggestions prone to rare cases.\n\nThe original statement was confusing and did not convey much information, so we decided to remove the sentence in the current version.\n\nQ4. Although the method is more unified than some other AL + SSL approaches, I wonder if the L_u(x,M) can be made to look more like the C(B,M) = \\sum E(x,M). In particular, L_u(x,M) uses just a single perturbation and a different \"distance\" function than E(x,M) which uses N perturbations.\n\nOur response: We agree with the reviewer that C(B,M) = \\sum E(x,M) can be an interesting replacement for L_u(x,M) during model training to encourage unsupervised consistency. However, estimating accurate variance usually requires a large amount of samples (N=50 in our case). It will significantly increase the computation cost for model training (since every perturbation needs to forward the model once). We would like to discover this option in the future work.\n\nQ5. The authors state that they lose 1.26% accuracy to the fully supervised model. However, this is very much not within the margin of measurement error and 1.26% accuracy is rather significant for accuracies around 95%. Another way of putting it is that the method in the paper with 4K examples has 30% more error compared to the fully supervised method. Can the authors either change this claim or provide a number of labels where their method achieves the fully-supervised accuracy?\n\nOur response: Thanks for your suggestion! We have changed this claim as suggested to \u201cour method with 4K examples has 30% more error compared to the fully supervised method\u201d."}, "signatures": ["ICLR.cc/2020/Conference/Paper1693/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1693/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["mgao@cs.umd.edu", "zizhaoz@google.com", "gy63@uw.edu", "soarik@google.com", "lsd@umiacs.umd.edu", "tpfister@google.com"], "title": "Consistency-Based Semi-Supervised Active Learning: Towards Minimizing Labeling Budget", "authors": ["Mingfei Gao", "Zizhao Zhang", "Guo Yu", "Sercan O. Arik", "Larry S. Davis", "Tomas Pfister"], "pdf": "/pdf/5d12c5148598d22be0d40a3eb8902c226f3c0cb9.pdf", "abstract": "Active learning (AL) aims to integrate data labeling and model training in a unified way, and to minimize the labeling budget by prioritizing the selection of high value data that can best improve model performance. Readily-available unlabeled data are used to evaluate selection mechanisms, but are not used for model training in conventional pool-based AL. To minimize the labeling budget, we unify unlabeled sample selection and model training based on two principles. First, we exploit both labeled and unlabeled data using semi-supervised learning (SSL) to distill information from unlabeled data that improves representation learning and sample selection. Second, we propose a simple yet effective selection metric that is coherent with the training objective such that the selected samples are effective at improving model performance. Our experimental results demonstrate superior performance with our proposed principles for limited labeled data compared to alternative AL and SSL combinations.  In addition, we study the AL phenomena of `cold start', which is becoming an increasingly more important factor to enable optimal unification of data labeling, model training and labeling budget minimization.  We propose a measure that is found to be empirically correlated with the AL target loss. This measure can be used to assist in determining the proper start size.", "keywords": ["Active learning", "semi-supervised learning"], "paperhash": "gao|consistencybased_semisupervised_active_learning_towards_minimizing_labeling_budget", "original_pdf": "/attachment/8375c90461c44de873d7b957ca66ffb9d3e39adf.pdf", "_bibtex": "@misc{\ngao2020consistencybased,\ntitle={Consistency-Based Semi-Supervised Active Learning: Towards Minimizing Labeling Budget},\nauthor={Mingfei Gao and Zizhao Zhang and Guo Yu and Sercan O. Arik and Larry S. Davis and Tomas Pfister},\nyear={2020},\nurl={https://openreview.net/forum?id=HJl8SkBYPr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJl8SkBYPr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1693/Authors", "ICLR.cc/2020/Conference/Paper1693/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1693/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1693/Reviewers", "ICLR.cc/2020/Conference/Paper1693/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1693/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1693/Authors|ICLR.cc/2020/Conference/Paper1693/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504152262, "tmdate": 1576860558026, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1693/Authors", "ICLR.cc/2020/Conference/Paper1693/Reviewers", "ICLR.cc/2020/Conference/Paper1693/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1693/-/Official_Comment"}}}, {"id": "HJl9sGNHjr", "original": null, "number": 5, "cdate": 1573368482441, "ddate": null, "tcdate": 1573368482441, "tmdate": 1573368482441, "tddate": null, "forum": "HJl8SkBYPr", "replyto": "Syl2VCwWiH", "invitation": "ICLR.cc/2020/Conference/Paper1693/-/Official_Comment", "content": {"title": "Thanks for your interest.", "comment": "Thanks for your interest in several parts of our work and your kind suggestions to the possible future extensions of this work. Please see our responses as follows.\n\n1. I am wondering whether the idea of combining SSL and AL was not already introduced by others (Sener et al, CEAL from Wang et al.)? Nonetheless your performance is much better I guess mainly because of Mixmatch? How about other SSL techniques (mean teacher, VAT, SNTG or GAN based methods) does your approach also work there?\n\nOur response: As we mentioned in the related work, there are existing AL work incorporating SSL. Our contribution (novelty) is unifying the unlabeled sample selection and the SSL model training. Also, we propose consistency based AL criterion, which is simple and effective. In Tab. 2, we compared with other metrics to demonstrate the effectiveness of our proposed metric when all methods are based on Mixmatch.\n\nAs we mentioned in the section 5, regarding to other SSL methods that are not based on consistency principles, it might be necessary to redesign AL selection criteria aligned with their training objectives. We leave this investigation to future research.\n\n2. Does you k-center AL (Sener et al.) include the MIP (robust k-center) or is it just plain k-center? \n\nOur response: We used plain k-center.\n\n3. The consistency based AL criterion is indeed interesting but I think besides validating on SSL it should also be tested on standard supervised learning. I noticed such experiments in the supplementary is performs as good as entropy sampling. How does it compare to more recent approaches such as Learning Loss for Active Learning (CVPR19) or Bayesian Generative Active Deep Learning (ICML19). I think these experiments are very important for the proposed AL criterion to know in which setting it should be used.  Why are they not part of the main paper?\n\nOur response: Thanks for your interest in our consistency based AL criterion.\nIt is not part of the main paper, because using SSL in AL is much more effective compared to using supervised training (shown in Tab 1) and our contribution is proposing a framework which unifies unlabeled data selection and SSL. Testing our selection metric on different supervised models is not the focus of this work.  \n\n4. Another thing that I am wondering about is did you do experiments for Learning Loss for Active Learning or Bayesian Generative Active Deep Learning using a SSL approach to train the classifier? How do they perform in SSL scenario?\n\nOur response: Converting these supervised models to SSL approaches might be an interesting future work, but it is beyond the scope of this work. Our focus is unifying unlabeled data selection and SSL model training.\n\n5. I think the section 4 is interesting. However, I believe that it would be optimal to do active learning as early as possible optimally we should never use random sampling but select already samples at the beginning. The plots in Fig. 3. show that it the performance it not very good if we start too early with AL with too few labeled samples for the three strategies. So it means that if we are forced to start with 50 labeled samples it would be a good idea to select the next 50 samples randomly right? So I get the feeling that the studied three criteria are just bad in this situation but in general we should start as early as possible but we need other criteria. What are your thoughts about this? Maybe it would also be a good idea to vary the number of labeled samples that we add in each AL cycle.\n\nOur response: This is an interesting future work. A potential idea can be switching from AL methods to random whenever it is needed. Intuitively, random selection will be chosen more at the beginning and less afterwards. Another potential idea is AL with varying AL batch sizes (e.g. start with large batch size and then decrease for steep performance increase and then decrease again etc.).\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1693/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1693/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["mgao@cs.umd.edu", "zizhaoz@google.com", "gy63@uw.edu", "soarik@google.com", "lsd@umiacs.umd.edu", "tpfister@google.com"], "title": "Consistency-Based Semi-Supervised Active Learning: Towards Minimizing Labeling Budget", "authors": ["Mingfei Gao", "Zizhao Zhang", "Guo Yu", "Sercan O. Arik", "Larry S. Davis", "Tomas Pfister"], "pdf": "/pdf/5d12c5148598d22be0d40a3eb8902c226f3c0cb9.pdf", "abstract": "Active learning (AL) aims to integrate data labeling and model training in a unified way, and to minimize the labeling budget by prioritizing the selection of high value data that can best improve model performance. Readily-available unlabeled data are used to evaluate selection mechanisms, but are not used for model training in conventional pool-based AL. To minimize the labeling budget, we unify unlabeled sample selection and model training based on two principles. First, we exploit both labeled and unlabeled data using semi-supervised learning (SSL) to distill information from unlabeled data that improves representation learning and sample selection. Second, we propose a simple yet effective selection metric that is coherent with the training objective such that the selected samples are effective at improving model performance. Our experimental results demonstrate superior performance with our proposed principles for limited labeled data compared to alternative AL and SSL combinations.  In addition, we study the AL phenomena of `cold start', which is becoming an increasingly more important factor to enable optimal unification of data labeling, model training and labeling budget minimization.  We propose a measure that is found to be empirically correlated with the AL target loss. This measure can be used to assist in determining the proper start size.", "keywords": ["Active learning", "semi-supervised learning"], "paperhash": "gao|consistencybased_semisupervised_active_learning_towards_minimizing_labeling_budget", "original_pdf": "/attachment/8375c90461c44de873d7b957ca66ffb9d3e39adf.pdf", "_bibtex": "@misc{\ngao2020consistencybased,\ntitle={Consistency-Based Semi-Supervised Active Learning: Towards Minimizing Labeling Budget},\nauthor={Mingfei Gao and Zizhao Zhang and Guo Yu and Sercan O. Arik and Larry S. Davis and Tomas Pfister},\nyear={2020},\nurl={https://openreview.net/forum?id=HJl8SkBYPr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJl8SkBYPr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1693/Authors", "ICLR.cc/2020/Conference/Paper1693/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1693/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1693/Reviewers", "ICLR.cc/2020/Conference/Paper1693/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1693/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1693/Authors|ICLR.cc/2020/Conference/Paper1693/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504152262, "tmdate": 1576860558026, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1693/Authors", "ICLR.cc/2020/Conference/Paper1693/Reviewers", "ICLR.cc/2020/Conference/Paper1693/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1693/-/Official_Comment"}}}, {"id": "Syl2VCwWiH", "original": null, "number": 3, "cdate": 1573121587583, "ddate": null, "tcdate": 1573121587583, "tmdate": 1573121587583, "tddate": null, "forum": "HJl8SkBYPr", "replyto": "HJl8SkBYPr", "invitation": "ICLR.cc/2020/Conference/Paper1693/-/Public_Comment", "content": {"title": "Some thoughts and questions", "comment": "I got some questions about the novelty of the paper and about related work.\n\nI am wondering whether the idea of combining SSL and AL was not already introduced by others (Sener et al, CEAL from Wang et al.)? Nonetheless your performance is much better I guess mainly because of Mixmatch? How about other SSL techniques (mean teacher, VAT, SNTG or GAN based methods) does your approach also work there?\n\nDoes you k-center AL (Sener et al.) include the MIP (robust k-center) or is it just plain k-center? \n\nThe consistency based AL criterion is indeed interesting but I think besides validating on SSL it should also be tested on standard supervised learning. I noticed such experiments in the supplementary is performs as good as entropy sampling. How does it compare to more recent approaches such as Learning Loss for Active Learning (CVPR19) or Bayesian Generative Active Deep Learning (ICML19). I think these experiments are very important for the proposed AL criterion to know in which setting it should be used.  Why are they not part of the main paper?\n\nAnother thing that I am wondering about is did you do experiments for Learning Loss for Active Learning or Bayesian Generative Active Deep Learning using a SSL approach to train the classifier? How do they perform in SSL scenario?\n\nI think the section 4 is interesting. However, I believe that it would be optimal to do active learning as early as possible optimally we should never use random sampling but select already samples at the beginning. The plots in Fig. 3. show that it the performance it not very good if we start too early with AL with too few labeled samples for the three strategies. So it means that if we are forced to start with 50 labeled samples it would be a good idea to select the next 50 samples randomly right? So I get the feeling that the studied three criteria are just bad in this situation but in general we should start as early as possible but we need other criteria. What are your thoughts about this? Maybe it would also be a good idea to vary the number of labeled samples that we add in each AL cycle.\n\nThanks for your answer.\n\n\n\n"}, "signatures": ["~Christoph_Mayer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Christoph_Mayer1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["mgao@cs.umd.edu", "zizhaoz@google.com", "gy63@uw.edu", "soarik@google.com", "lsd@umiacs.umd.edu", "tpfister@google.com"], "title": "Consistency-Based Semi-Supervised Active Learning: Towards Minimizing Labeling Budget", "authors": ["Mingfei Gao", "Zizhao Zhang", "Guo Yu", "Sercan O. Arik", "Larry S. Davis", "Tomas Pfister"], "pdf": "/pdf/5d12c5148598d22be0d40a3eb8902c226f3c0cb9.pdf", "abstract": "Active learning (AL) aims to integrate data labeling and model training in a unified way, and to minimize the labeling budget by prioritizing the selection of high value data that can best improve model performance. Readily-available unlabeled data are used to evaluate selection mechanisms, but are not used for model training in conventional pool-based AL. To minimize the labeling budget, we unify unlabeled sample selection and model training based on two principles. First, we exploit both labeled and unlabeled data using semi-supervised learning (SSL) to distill information from unlabeled data that improves representation learning and sample selection. Second, we propose a simple yet effective selection metric that is coherent with the training objective such that the selected samples are effective at improving model performance. Our experimental results demonstrate superior performance with our proposed principles for limited labeled data compared to alternative AL and SSL combinations.  In addition, we study the AL phenomena of `cold start', which is becoming an increasingly more important factor to enable optimal unification of data labeling, model training and labeling budget minimization.  We propose a measure that is found to be empirically correlated with the AL target loss. This measure can be used to assist in determining the proper start size.", "keywords": ["Active learning", "semi-supervised learning"], "paperhash": "gao|consistencybased_semisupervised_active_learning_towards_minimizing_labeling_budget", "original_pdf": "/attachment/8375c90461c44de873d7b957ca66ffb9d3e39adf.pdf", "_bibtex": "@misc{\ngao2020consistencybased,\ntitle={Consistency-Based Semi-Supervised Active Learning: Towards Minimizing Labeling Budget},\nauthor={Mingfei Gao and Zizhao Zhang and Guo Yu and Sercan O. Arik and Larry S. Davis and Tomas Pfister},\nyear={2020},\nurl={https://openreview.net/forum?id=HJl8SkBYPr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJl8SkBYPr", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504191122, "tmdate": 1576860591134, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper1693/Authors", "ICLR.cc/2020/Conference/Paper1693/Reviewers", "ICLR.cc/2020/Conference/Paper1693/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1693/-/Public_Comment"}}}, {"id": "rkghWIQitS", "original": null, "number": 1, "cdate": 1571661316295, "ddate": null, "tcdate": 1571661316295, "tmdate": 1572972435338, "tddate": null, "forum": "HJl8SkBYPr", "replyto": "HJl8SkBYPr", "invitation": "ICLR.cc/2020/Conference/Paper1693/-/Official_Review", "content": {"rating": "6: Weak Accept", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper proposes a semi-supervised active learning method to reduce the labeling cost. In the proposed method, a selection criterion to better integrate AL selection mechanism in SSL training framework is designed. The simple metric that aims to measure the inconsistency across a certain number of meaningful perturbations. It considers N perturbed samples of the original input data x, which can be obtained by standard augmentation operations (e.g. random crops and horizontal flips for image data). Then the variance is adopted to quantify consistency.  In this way, the proposed method prefers data samples with high values, which may possess varying level of difficulty for the model to classify. To verify the effectiveness of the proposed method, several baseline methods are compared on several benchmark data sets, and the proposed method has achieved better performance. Meanwhile, to deal with the \u201ccold start\u201d problem, a measure that is found to be empirically correlated with the AL target loss is proposed, and this measure can be used to assist in determining the proper start size. However, there are some minor concerns:\n[1] The consistency of a sample is measured based on the perturbed samples. How to generate these perturbed samples may have a great influence on the query results. In the paper, it said that these samples are generated by standard augmentation operations (e.g. random crops and horizontal flips for image data). This representation is hard to follow in the experiments. If possible, it is better to show in details.\n[2] In the uncertainty of active learning, the samples are selected from different distributions in the unlabeled data, for example, the marginal sampling selects the samples around the classification hyperlanes (Settles, Burr. \"Active learning.\" Synthesis Lectures on Artificial Intelligence and Machine Learning 6.1 (2012): 1-114.). Can you show which samples may be selected in the unlabeled data. In this way, the proposed criterion can be followed more easily.\n[3] In the experiments, whether the proposed method can select a batch of samples at each iteration. How about the influence of the batch size. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1693/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1693/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["mgao@cs.umd.edu", "zizhaoz@google.com", "gy63@uw.edu", "soarik@google.com", "lsd@umiacs.umd.edu", "tpfister@google.com"], "title": "Consistency-Based Semi-Supervised Active Learning: Towards Minimizing Labeling Budget", "authors": ["Mingfei Gao", "Zizhao Zhang", "Guo Yu", "Sercan O. Arik", "Larry S. Davis", "Tomas Pfister"], "pdf": "/pdf/5d12c5148598d22be0d40a3eb8902c226f3c0cb9.pdf", "abstract": "Active learning (AL) aims to integrate data labeling and model training in a unified way, and to minimize the labeling budget by prioritizing the selection of high value data that can best improve model performance. Readily-available unlabeled data are used to evaluate selection mechanisms, but are not used for model training in conventional pool-based AL. To minimize the labeling budget, we unify unlabeled sample selection and model training based on two principles. First, we exploit both labeled and unlabeled data using semi-supervised learning (SSL) to distill information from unlabeled data that improves representation learning and sample selection. Second, we propose a simple yet effective selection metric that is coherent with the training objective such that the selected samples are effective at improving model performance. Our experimental results demonstrate superior performance with our proposed principles for limited labeled data compared to alternative AL and SSL combinations.  In addition, we study the AL phenomena of `cold start', which is becoming an increasingly more important factor to enable optimal unification of data labeling, model training and labeling budget minimization.  We propose a measure that is found to be empirically correlated with the AL target loss. This measure can be used to assist in determining the proper start size.", "keywords": ["Active learning", "semi-supervised learning"], "paperhash": "gao|consistencybased_semisupervised_active_learning_towards_minimizing_labeling_budget", "original_pdf": "/attachment/8375c90461c44de873d7b957ca66ffb9d3e39adf.pdf", "_bibtex": "@misc{\ngao2020consistencybased,\ntitle={Consistency-Based Semi-Supervised Active Learning: Towards Minimizing Labeling Budget},\nauthor={Mingfei Gao and Zizhao Zhang and Guo Yu and Sercan O. Arik and Larry S. Davis and Tomas Pfister},\nyear={2020},\nurl={https://openreview.net/forum?id=HJl8SkBYPr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJl8SkBYPr", "replyto": "HJl8SkBYPr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1693/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1693/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575666333777, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1693/Reviewers"], "noninvitees": [], "tcdate": 1570237733664, "tmdate": 1575666333790, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1693/-/Official_Review"}}}, {"id": "HyeG10GntB", "original": null, "number": 2, "cdate": 1571724762404, "ddate": null, "tcdate": 1571724762404, "tmdate": 1572972435296, "tddate": null, "forum": "HJl8SkBYPr", "replyto": "HJl8SkBYPr", "invitation": "ICLR.cc/2020/Conference/Paper1693/-/Official_Review", "content": {"rating": "6: Weak Accept", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper proposes a new combination method for active learning and semi-supervised learning, where the objective is to make predictions that are robust to perturbations (for SSL) and select points for labeling with labels that differ under perturbations. This technique achieves 2x label efficiency over SSL with uniform-random sampling. Additionally, the authors assess (at least for CIFAR-10 with batch size 50) the best starting random seed set as 100 labels, known as K_0 in this work. This work yields pretty good empirical results and has a conceptually unified approach to SSL and active learning building off of recent works. \n\n\nComments:\n\n - This paper compares in Table 1 the difference between just active learning vs active learning + SSL. I'm not sure this is a fair comparison. I think the better comparison is shown in Table 2. \n\n - The authors write that \"when only 100 samples are labeled, our method outperforms kcenter by 39.24% accuracy\". Do the authors mean after 100 additional labels are acquired (so 200 labels) or is this number off?\n\n - Can the authors clarify what is meant by \"or some labels correspond to rare cases, as in self-driving cars\"? Why are such datasets more costly to acquire? Is it because of the size of the self-driving car datasets?\n\n - Although the method is more unified than some other AL + SSL approaches, I wonder if the L_u(x,M) can be made to look more like the C(B,M) = \\sum E(x,M). In particular, L_u(x,M) uses just a single perturbation and a different \"distance\" function than E(x,M) which uses N perturbations.\n\n - The authors state that they lose 1.26% accuracy to the fully supervised model. However, this is very much not within the margin of measurement error and 1.26% accuracy is rather significant for accuracies around 95%. Another way of putting it is that the method in the paper with 4K examples has 30% more error compared to the fully supervised method. Can the authors either change this claim or provide a number of labels where their method achieves the fully-supervised accuracy?\n\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1693/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1693/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["mgao@cs.umd.edu", "zizhaoz@google.com", "gy63@uw.edu", "soarik@google.com", "lsd@umiacs.umd.edu", "tpfister@google.com"], "title": "Consistency-Based Semi-Supervised Active Learning: Towards Minimizing Labeling Budget", "authors": ["Mingfei Gao", "Zizhao Zhang", "Guo Yu", "Sercan O. Arik", "Larry S. Davis", "Tomas Pfister"], "pdf": "/pdf/5d12c5148598d22be0d40a3eb8902c226f3c0cb9.pdf", "abstract": "Active learning (AL) aims to integrate data labeling and model training in a unified way, and to minimize the labeling budget by prioritizing the selection of high value data that can best improve model performance. Readily-available unlabeled data are used to evaluate selection mechanisms, but are not used for model training in conventional pool-based AL. To minimize the labeling budget, we unify unlabeled sample selection and model training based on two principles. First, we exploit both labeled and unlabeled data using semi-supervised learning (SSL) to distill information from unlabeled data that improves representation learning and sample selection. Second, we propose a simple yet effective selection metric that is coherent with the training objective such that the selected samples are effective at improving model performance. Our experimental results demonstrate superior performance with our proposed principles for limited labeled data compared to alternative AL and SSL combinations.  In addition, we study the AL phenomena of `cold start', which is becoming an increasingly more important factor to enable optimal unification of data labeling, model training and labeling budget minimization.  We propose a measure that is found to be empirically correlated with the AL target loss. This measure can be used to assist in determining the proper start size.", "keywords": ["Active learning", "semi-supervised learning"], "paperhash": "gao|consistencybased_semisupervised_active_learning_towards_minimizing_labeling_budget", "original_pdf": "/attachment/8375c90461c44de873d7b957ca66ffb9d3e39adf.pdf", "_bibtex": "@misc{\ngao2020consistencybased,\ntitle={Consistency-Based Semi-Supervised Active Learning: Towards Minimizing Labeling Budget},\nauthor={Mingfei Gao and Zizhao Zhang and Guo Yu and Sercan O. Arik and Larry S. Davis and Tomas Pfister},\nyear={2020},\nurl={https://openreview.net/forum?id=HJl8SkBYPr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJl8SkBYPr", "replyto": "HJl8SkBYPr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1693/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1693/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575666333777, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1693/Reviewers"], "noninvitees": [], "tcdate": 1570237733664, "tmdate": 1575666333790, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1693/-/Official_Review"}}}, {"id": "BklkAy1pqH", "original": null, "number": 2, "cdate": 1572822983300, "ddate": null, "tcdate": 1572822983300, "tmdate": 1572822983300, "tddate": null, "forum": "HJl8SkBYPr", "replyto": "rylu0rBd5S", "invitation": "ICLR.cc/2020/Conference/Paper1693/-/Official_Comment", "content": {"title": "Reply to \"Does finetuning epochs affect the accuracy?\"", "comment": "Standard supervised training is easily overfitted on small labeled training data. Finetuning on an overfitted model can hurt the performance based on our empirical knowledge. Based on our observation, the advanced semi-supervised method is stable and robust to overfitting.  That is also our motivation, which is taking advantage of semi-supervised training to significantly improve active learning at early AL cycles. "}, "signatures": ["ICLR.cc/2020/Conference/Paper1693/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1693/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["mgao@cs.umd.edu", "zizhaoz@google.com", "gy63@uw.edu", "soarik@google.com", "lsd@umiacs.umd.edu", "tpfister@google.com"], "title": "Consistency-Based Semi-Supervised Active Learning: Towards Minimizing Labeling Budget", "authors": ["Mingfei Gao", "Zizhao Zhang", "Guo Yu", "Sercan O. Arik", "Larry S. Davis", "Tomas Pfister"], "pdf": "/pdf/5d12c5148598d22be0d40a3eb8902c226f3c0cb9.pdf", "abstract": "Active learning (AL) aims to integrate data labeling and model training in a unified way, and to minimize the labeling budget by prioritizing the selection of high value data that can best improve model performance. Readily-available unlabeled data are used to evaluate selection mechanisms, but are not used for model training in conventional pool-based AL. To minimize the labeling budget, we unify unlabeled sample selection and model training based on two principles. First, we exploit both labeled and unlabeled data using semi-supervised learning (SSL) to distill information from unlabeled data that improves representation learning and sample selection. Second, we propose a simple yet effective selection metric that is coherent with the training objective such that the selected samples are effective at improving model performance. Our experimental results demonstrate superior performance with our proposed principles for limited labeled data compared to alternative AL and SSL combinations.  In addition, we study the AL phenomena of `cold start', which is becoming an increasingly more important factor to enable optimal unification of data labeling, model training and labeling budget minimization.  We propose a measure that is found to be empirically correlated with the AL target loss. This measure can be used to assist in determining the proper start size.", "keywords": ["Active learning", "semi-supervised learning"], "paperhash": "gao|consistencybased_semisupervised_active_learning_towards_minimizing_labeling_budget", "original_pdf": "/attachment/8375c90461c44de873d7b957ca66ffb9d3e39adf.pdf", "_bibtex": "@misc{\ngao2020consistencybased,\ntitle={Consistency-Based Semi-Supervised Active Learning: Towards Minimizing Labeling Budget},\nauthor={Mingfei Gao and Zizhao Zhang and Guo Yu and Sercan O. Arik and Larry S. Davis and Tomas Pfister},\nyear={2020},\nurl={https://openreview.net/forum?id=HJl8SkBYPr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJl8SkBYPr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1693/Authors", "ICLR.cc/2020/Conference/Paper1693/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1693/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1693/Reviewers", "ICLR.cc/2020/Conference/Paper1693/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1693/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1693/Authors|ICLR.cc/2020/Conference/Paper1693/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504152262, "tmdate": 1576860558026, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1693/Authors", "ICLR.cc/2020/Conference/Paper1693/Reviewers", "ICLR.cc/2020/Conference/Paper1693/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1693/-/Official_Comment"}}}, {"id": "rylu0rBd5S", "original": null, "number": 2, "cdate": 1572521423665, "ddate": null, "tcdate": 1572521423665, "tmdate": 1572521423665, "tddate": null, "forum": "HJl8SkBYPr", "replyto": "HkgdYboDqS", "invitation": "ICLR.cc/2020/Conference/Paper1693/-/Public_Comment", "content": {"title": "Does finetuning epochs affect the accuracy?", "comment": "Many thanks for your response. I noticed that when the dataset is small, which is exactly the situation when active learning begins, training and finetuning on the small dataset might be easy to overfit. I'm confused that does the selection of the finetuning epochs number affect the accuracy on the test dataset?"}, "signatures": ["~Hao_Zhongkai1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Hao_Zhongkai1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["mgao@cs.umd.edu", "zizhaoz@google.com", "gy63@uw.edu", "soarik@google.com", "lsd@umiacs.umd.edu", "tpfister@google.com"], "title": "Consistency-Based Semi-Supervised Active Learning: Towards Minimizing Labeling Budget", "authors": ["Mingfei Gao", "Zizhao Zhang", "Guo Yu", "Sercan O. Arik", "Larry S. Davis", "Tomas Pfister"], "pdf": "/pdf/5d12c5148598d22be0d40a3eb8902c226f3c0cb9.pdf", "abstract": "Active learning (AL) aims to integrate data labeling and model training in a unified way, and to minimize the labeling budget by prioritizing the selection of high value data that can best improve model performance. Readily-available unlabeled data are used to evaluate selection mechanisms, but are not used for model training in conventional pool-based AL. To minimize the labeling budget, we unify unlabeled sample selection and model training based on two principles. First, we exploit both labeled and unlabeled data using semi-supervised learning (SSL) to distill information from unlabeled data that improves representation learning and sample selection. Second, we propose a simple yet effective selection metric that is coherent with the training objective such that the selected samples are effective at improving model performance. Our experimental results demonstrate superior performance with our proposed principles for limited labeled data compared to alternative AL and SSL combinations.  In addition, we study the AL phenomena of `cold start', which is becoming an increasingly more important factor to enable optimal unification of data labeling, model training and labeling budget minimization.  We propose a measure that is found to be empirically correlated with the AL target loss. This measure can be used to assist in determining the proper start size.", "keywords": ["Active learning", "semi-supervised learning"], "paperhash": "gao|consistencybased_semisupervised_active_learning_towards_minimizing_labeling_budget", "original_pdf": "/attachment/8375c90461c44de873d7b957ca66ffb9d3e39adf.pdf", "_bibtex": "@misc{\ngao2020consistencybased,\ntitle={Consistency-Based Semi-Supervised Active Learning: Towards Minimizing Labeling Budget},\nauthor={Mingfei Gao and Zizhao Zhang and Guo Yu and Sercan O. Arik and Larry S. Davis and Tomas Pfister},\nyear={2020},\nurl={https://openreview.net/forum?id=HJl8SkBYPr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJl8SkBYPr", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504191122, "tmdate": 1576860591134, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper1693/Authors", "ICLR.cc/2020/Conference/Paper1693/Reviewers", "ICLR.cc/2020/Conference/Paper1693/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1693/-/Public_Comment"}}}, {"id": "HkgdYboDqS", "original": null, "number": 1, "cdate": 1572479359561, "ddate": null, "tcdate": 1572479359561, "tmdate": 1572479359561, "tddate": null, "forum": "HJl8SkBYPr", "replyto": "SygeC5GPcB", "invitation": "ICLR.cc/2020/Conference/Paper1693/-/Official_Comment", "content": {"title": "Reply to \"Confused about the training strategy\"", "comment": "Thanks for your interest.\n\nWe finetune the model on the whole dataset. Our model is trained in the semi-supervised mode, so in each cycle both labeled and unlabeled data is used for training."}, "signatures": ["ICLR.cc/2020/Conference/Paper1693/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1693/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["mgao@cs.umd.edu", "zizhaoz@google.com", "gy63@uw.edu", "soarik@google.com", "lsd@umiacs.umd.edu", "tpfister@google.com"], "title": "Consistency-Based Semi-Supervised Active Learning: Towards Minimizing Labeling Budget", "authors": ["Mingfei Gao", "Zizhao Zhang", "Guo Yu", "Sercan O. Arik", "Larry S. Davis", "Tomas Pfister"], "pdf": "/pdf/5d12c5148598d22be0d40a3eb8902c226f3c0cb9.pdf", "abstract": "Active learning (AL) aims to integrate data labeling and model training in a unified way, and to minimize the labeling budget by prioritizing the selection of high value data that can best improve model performance. Readily-available unlabeled data are used to evaluate selection mechanisms, but are not used for model training in conventional pool-based AL. To minimize the labeling budget, we unify unlabeled sample selection and model training based on two principles. First, we exploit both labeled and unlabeled data using semi-supervised learning (SSL) to distill information from unlabeled data that improves representation learning and sample selection. Second, we propose a simple yet effective selection metric that is coherent with the training objective such that the selected samples are effective at improving model performance. Our experimental results demonstrate superior performance with our proposed principles for limited labeled data compared to alternative AL and SSL combinations.  In addition, we study the AL phenomena of `cold start', which is becoming an increasingly more important factor to enable optimal unification of data labeling, model training and labeling budget minimization.  We propose a measure that is found to be empirically correlated with the AL target loss. This measure can be used to assist in determining the proper start size.", "keywords": ["Active learning", "semi-supervised learning"], "paperhash": "gao|consistencybased_semisupervised_active_learning_towards_minimizing_labeling_budget", "original_pdf": "/attachment/8375c90461c44de873d7b957ca66ffb9d3e39adf.pdf", "_bibtex": "@misc{\ngao2020consistencybased,\ntitle={Consistency-Based Semi-Supervised Active Learning: Towards Minimizing Labeling Budget},\nauthor={Mingfei Gao and Zizhao Zhang and Guo Yu and Sercan O. Arik and Larry S. Davis and Tomas Pfister},\nyear={2020},\nurl={https://openreview.net/forum?id=HJl8SkBYPr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJl8SkBYPr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1693/Authors", "ICLR.cc/2020/Conference/Paper1693/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1693/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1693/Reviewers", "ICLR.cc/2020/Conference/Paper1693/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1693/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1693/Authors|ICLR.cc/2020/Conference/Paper1693/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504152262, "tmdate": 1576860558026, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1693/Authors", "ICLR.cc/2020/Conference/Paper1693/Reviewers", "ICLR.cc/2020/Conference/Paper1693/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1693/-/Official_Comment"}}}, {"id": "SygeC5GPcB", "original": null, "number": 1, "cdate": 1572444871845, "ddate": null, "tcdate": 1572444871845, "tmdate": 1572444871845, "tddate": null, "forum": "HJl8SkBYPr", "replyto": "HJl8SkBYPr", "invitation": "ICLR.cc/2020/Conference/Paper1693/-/Public_Comment", "content": {"title": "Confused about the training strategy", "comment": "I'm confused that when you select a new batch from the unlabeled data pool, you need to minimize the loss on the labeled dataset. Do you only finetune your neural network on the new batch or finetune on the whole labeled dataset, or completely retrain a model from random initialization? I notice that many works on active learning did not mention the training or finetuning strategy. When the dataset is large, completely retraining a deep neural network is time-consuming. \n"}, "signatures": ["~Hao_Zhongkai1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Hao_Zhongkai1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["mgao@cs.umd.edu", "zizhaoz@google.com", "gy63@uw.edu", "soarik@google.com", "lsd@umiacs.umd.edu", "tpfister@google.com"], "title": "Consistency-Based Semi-Supervised Active Learning: Towards Minimizing Labeling Budget", "authors": ["Mingfei Gao", "Zizhao Zhang", "Guo Yu", "Sercan O. Arik", "Larry S. Davis", "Tomas Pfister"], "pdf": "/pdf/5d12c5148598d22be0d40a3eb8902c226f3c0cb9.pdf", "abstract": "Active learning (AL) aims to integrate data labeling and model training in a unified way, and to minimize the labeling budget by prioritizing the selection of high value data that can best improve model performance. Readily-available unlabeled data are used to evaluate selection mechanisms, but are not used for model training in conventional pool-based AL. To minimize the labeling budget, we unify unlabeled sample selection and model training based on two principles. First, we exploit both labeled and unlabeled data using semi-supervised learning (SSL) to distill information from unlabeled data that improves representation learning and sample selection. Second, we propose a simple yet effective selection metric that is coherent with the training objective such that the selected samples are effective at improving model performance. Our experimental results demonstrate superior performance with our proposed principles for limited labeled data compared to alternative AL and SSL combinations.  In addition, we study the AL phenomena of `cold start', which is becoming an increasingly more important factor to enable optimal unification of data labeling, model training and labeling budget minimization.  We propose a measure that is found to be empirically correlated with the AL target loss. This measure can be used to assist in determining the proper start size.", "keywords": ["Active learning", "semi-supervised learning"], "paperhash": "gao|consistencybased_semisupervised_active_learning_towards_minimizing_labeling_budget", "original_pdf": "/attachment/8375c90461c44de873d7b957ca66ffb9d3e39adf.pdf", "_bibtex": "@misc{\ngao2020consistencybased,\ntitle={Consistency-Based Semi-Supervised Active Learning: Towards Minimizing Labeling Budget},\nauthor={Mingfei Gao and Zizhao Zhang and Guo Yu and Sercan O. Arik and Larry S. Davis and Tomas Pfister},\nyear={2020},\nurl={https://openreview.net/forum?id=HJl8SkBYPr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJl8SkBYPr", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504191122, "tmdate": 1576860591134, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper1693/Authors", "ICLR.cc/2020/Conference/Paper1693/Reviewers", "ICLR.cc/2020/Conference/Paper1693/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1693/-/Public_Comment"}}}], "count": 13}