{"notes": [{"id": "SyepHTNFDS", "original": "BJxEiVPwDH", "number": 541, "cdate": 1569439045362, "ddate": null, "tcdate": 1569439045362, "tmdate": 1577168216912, "tddate": null, "forum": "SyepHTNFDS", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Graph Residual Flow for Molecular Graph Generation", "authors": ["Shion Honda", "Hirotaka Akita", "Katsuhiko Ishiguro", "Toshiki Nakanishi", "Kenta Oono"], "authorids": ["26x.orc.ed5.1hs@gmail.com", "akita714@preferred.jp", "k.ishiguro.jp@ieee.org", "nakanishi@preferred.jp", "oono@preferred.jp"], "keywords": ["deep generative model", "normalizing flow", "graph generation", "cheminformatics"], "TL;DR": "We propose a residual flow model for molecular graphs, derive the conditions so that the flow is invertible, and show its efficacy in experiments.", "abstract": "Statistical generative models for molecular graphs attract attention from many researchers from the fields of bio- and chemo-informatics. Among these models, invertible flow-based approaches are not fully explored yet. In this paper, we propose a powerful invertible flow for molecular graphs, called Graph Residual Flow (GRF). The GRF is based on residual flows, which are known for more flexible and complex non-linear mappings than traditional coupling flows. We theoretically derive non-trivial conditions such that GRF is invertible, and present a way of keeping the entire flows invertible throughout the training and sampling. Experimental results show that a generative model based on the proposed GRF achieve comparable generation performance, with much smaller number of trainable parameters compared to the existing flow-based model. ", "pdf": "/pdf/2b4c83b9ad032d681365303fa6e8cb8a1ae8c95a.pdf", "paperhash": "honda|graph_residual_flow_for_molecular_graph_generation", "original_pdf": "/attachment/d5e9ab230ac7deb1c2e60c0c2e744b86d34f4c0d.pdf", "_bibtex": "@misc{\nhonda2020graph,\ntitle={Graph Residual Flow for Molecular Graph Generation},\nauthor={Shion Honda and Hirotaka Akita and Katsuhiko Ishiguro and Toshiki Nakanishi and Kenta Oono},\nyear={2020},\nurl={https://openreview.net/forum?id=SyepHTNFDS}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "-3fOKs3Ml", "original": null, "number": 1, "cdate": 1576798699255, "ddate": null, "tcdate": 1576798699255, "tmdate": 1576800936589, "tddate": null, "forum": "SyepHTNFDS", "replyto": "SyepHTNFDS", "invitation": "ICLR.cc/2020/Conference/Paper541/-/Decision", "content": {"decision": "Reject", "comment": "The authors propose a graph residual flow model for molecular generation.  Conceptual novelty is limited since it is simple extension and there isn't much improvement over state of art.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Graph Residual Flow for Molecular Graph Generation", "authors": ["Shion Honda", "Hirotaka Akita", "Katsuhiko Ishiguro", "Toshiki Nakanishi", "Kenta Oono"], "authorids": ["26x.orc.ed5.1hs@gmail.com", "akita714@preferred.jp", "k.ishiguro.jp@ieee.org", "nakanishi@preferred.jp", "oono@preferred.jp"], "keywords": ["deep generative model", "normalizing flow", "graph generation", "cheminformatics"], "TL;DR": "We propose a residual flow model for molecular graphs, derive the conditions so that the flow is invertible, and show its efficacy in experiments.", "abstract": "Statistical generative models for molecular graphs attract attention from many researchers from the fields of bio- and chemo-informatics. Among these models, invertible flow-based approaches are not fully explored yet. In this paper, we propose a powerful invertible flow for molecular graphs, called Graph Residual Flow (GRF). The GRF is based on residual flows, which are known for more flexible and complex non-linear mappings than traditional coupling flows. We theoretically derive non-trivial conditions such that GRF is invertible, and present a way of keeping the entire flows invertible throughout the training and sampling. Experimental results show that a generative model based on the proposed GRF achieve comparable generation performance, with much smaller number of trainable parameters compared to the existing flow-based model. ", "pdf": "/pdf/2b4c83b9ad032d681365303fa6e8cb8a1ae8c95a.pdf", "paperhash": "honda|graph_residual_flow_for_molecular_graph_generation", "original_pdf": "/attachment/d5e9ab230ac7deb1c2e60c0c2e744b86d34f4c0d.pdf", "_bibtex": "@misc{\nhonda2020graph,\ntitle={Graph Residual Flow for Molecular Graph Generation},\nauthor={Shion Honda and Hirotaka Akita and Katsuhiko Ishiguro and Toshiki Nakanishi and Kenta Oono},\nyear={2020},\nurl={https://openreview.net/forum?id=SyepHTNFDS}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "SyepHTNFDS", "replyto": "SyepHTNFDS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795719059, "tmdate": 1576800269649, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper541/-/Decision"}}}, {"id": "HJl9F8xKiH", "original": null, "number": 3, "cdate": 1573615234174, "ddate": null, "tcdate": 1573615234174, "tmdate": 1573615268489, "tddate": null, "forum": "SyepHTNFDS", "replyto": "HygVaGssYH", "invitation": "ICLR.cc/2020/Conference/Paper541/-/Official_Comment", "content": {"title": "Response to Reviewer #3", "comment": "We thank the reviewer for their comments and questions. In particular, thank you for precious suggestions on our writing. We revised our paper following your suggestions.\nExcept for these points, we address them in order.\n\n> Other than the above, GRF is a straightforward application of iResNet in molecule generation.\n\nUnlike images, graphs are discrete and do not have grid-like structures. Therefore, i-ResNet cannot be applied to graphs in a  straightforward way. To address these critical difference, we dequantise adjacency tensors and feature tensors and derive a condition where GRF is invertible reflecting topological information of graphs. From Lemma 1 and 2, the upper bound of the Lipschitz constant of graph residual layers can be decided only on the Lipschitz constant of the activation function (Theorem 1).\n\n>  Exact invertibility and smoothness are inherent properties of flow-based models that exist by design, so there isn't anything surprising about them.\n\nThe invertibility and smoothness of GRF is guaranteed in theory, but it is not in reality because fixed point iteration is truncated and some approximations are used in the computation. We conducted those experiments in order to demonstrate that the GRF can be appropriately trained with the derived condition for invertibility.\n\n> If GRF can achieve a performance similar to GraphNVP with fewer parameters, shouldn't it be able to achieve much better performance with more parameters? If GRF doesn't outperform GraphNVP even with more parameters, then I would expect to see a discussion explaining why.\n\nWe observed that increased parameters resulted in instability during training (especially in ZINC-250k). Specifically, with increased parameters, the learnt prior p(z) approached to the delta function rather than standard normal distribution. We assume that this is because the model was overfitting to maximize the log p(z) term of the objective function. Some regularization can alleviate this problem (e.g., adding KL divergence penalty), but we leave it for future work.\n\n>  What does \"kekulized\" mean?\n\n\u201ckekulize\u201d is to replace aromatic bonds in molecules with single/double bonds.\n\n> \"We adopt node-wise weight sharing for QM9 and low-rank approximation and multi-scale architecture for ZINC-250k.\" These are important aspects of the architecture that are nowhere else mentioned, and should be explained more thoroughly.\n\n\u201cNode-wise weight sharing\u201d and \u201clow-rank approximation\u201d is introduced to reduce parameters for fully-connected layers dealing with adjacency tensors. Namely, the weight matrix is expressed as:\n$$W \\in  \\mathbb{R}^{N \\times M} = AB$$\nwhere $A \\in \\mathbb{R}^{N \\times r}$ and $B \\in \\mathbb{R}^{r \\times M}$. When $r=1$, it is the same as node-wise weight sharing.\nFor multi-scale architecture, we refer NICE paper (Dinh et al., ICLR, 2015).\n\n\n>  It would be good to show how the metrics V, N, U, R in tables 1 and 2 vary with respect to the temperatures (for example, show curves of the four metrics as temperatures vary).\n\nWe have uploaded the figures here.\nhttps://drive.google.com/drive/folders/1XZEGyrIulVHZVXGzjaJW_M2Tbzcv7oGG?usp=sharing\nThese figures show how the metrics change against the temperature. Note that the temperature is different from that in 4.3. In ZINC-250k, the novelty line overlaps the uniqueness line.\n\n> The temperatures for ZINC-250k are really low (0.15 and 0.17), which to me is an indication that the flow model may have fitted the data quite badly.\n\nThis is also caused by the the learnt prior p(z) approaching to the delta function discussed above. Some regularization can alleviate this problem (e.g., adding KL divergence penalty), but we leave it for future work.\n\n> How does GRF compare to GraphNVP in terms of generation time? How slow is it to generate from GRF given that it takes 30 iterations for each residual block?\n\nIn the experiments GRF takes 100 iterations for each block and takes around 20 seconds to generate 1,000 graphs (GraphNVP takes around 5 seconds). Nevertheless, it is much faster than autoregression (e.g., JT-VAE takes around 200 seconds).\n\n> What is GraphLinear? Maybe give a citation?\n\nIn the revised version, we call it \u201cgraph convolution\u201d and please refer Neural Fingerprint (Duvenaud et al., NIPS, 2015).\n\n> Figure 3: Wouldn't it be more informative to centre the visualizations at the mean and use the two principal axes, rather than random mean and axes?\n\nWe apologize for the mistake in the caption. As described in 4.5, we use a random molecule as the centered one and two principal axes (fixed in the revised version).\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper541/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper541/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Graph Residual Flow for Molecular Graph Generation", "authors": ["Shion Honda", "Hirotaka Akita", "Katsuhiko Ishiguro", "Toshiki Nakanishi", "Kenta Oono"], "authorids": ["26x.orc.ed5.1hs@gmail.com", "akita714@preferred.jp", "k.ishiguro.jp@ieee.org", "nakanishi@preferred.jp", "oono@preferred.jp"], "keywords": ["deep generative model", "normalizing flow", "graph generation", "cheminformatics"], "TL;DR": "We propose a residual flow model for molecular graphs, derive the conditions so that the flow is invertible, and show its efficacy in experiments.", "abstract": "Statistical generative models for molecular graphs attract attention from many researchers from the fields of bio- and chemo-informatics. Among these models, invertible flow-based approaches are not fully explored yet. In this paper, we propose a powerful invertible flow for molecular graphs, called Graph Residual Flow (GRF). The GRF is based on residual flows, which are known for more flexible and complex non-linear mappings than traditional coupling flows. We theoretically derive non-trivial conditions such that GRF is invertible, and present a way of keeping the entire flows invertible throughout the training and sampling. Experimental results show that a generative model based on the proposed GRF achieve comparable generation performance, with much smaller number of trainable parameters compared to the existing flow-based model. ", "pdf": "/pdf/2b4c83b9ad032d681365303fa6e8cb8a1ae8c95a.pdf", "paperhash": "honda|graph_residual_flow_for_molecular_graph_generation", "original_pdf": "/attachment/d5e9ab230ac7deb1c2e60c0c2e744b86d34f4c0d.pdf", "_bibtex": "@misc{\nhonda2020graph,\ntitle={Graph Residual Flow for Molecular Graph Generation},\nauthor={Shion Honda and Hirotaka Akita and Katsuhiko Ishiguro and Toshiki Nakanishi and Kenta Oono},\nyear={2020},\nurl={https://openreview.net/forum?id=SyepHTNFDS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SyepHTNFDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper541/Authors", "ICLR.cc/2020/Conference/Paper541/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper541/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper541/Reviewers", "ICLR.cc/2020/Conference/Paper541/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper541/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper541/Authors|ICLR.cc/2020/Conference/Paper541/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504169909, "tmdate": 1576860560400, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper541/Authors", "ICLR.cc/2020/Conference/Paper541/Reviewers", "ICLR.cc/2020/Conference/Paper541/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper541/-/Official_Comment"}}}, {"id": "ByxLjllKoB", "original": null, "number": 2, "cdate": 1573613726133, "ddate": null, "tcdate": 1573613726133, "tmdate": 1573613726133, "tddate": null, "forum": "SyepHTNFDS", "replyto": "rkxxhCD0YH", "invitation": "ICLR.cc/2020/Conference/Paper541/-/Official_Comment", "content": {"title": "Response to Reviewer #2", "comment": "We thank the reviewer for their comments and questions. We address them in order.\n\n> Conceptual novelty seems to be limited. The method seems to be a basic modification of the GraphNVP approach by introducing ResNet-like skipped connections. Not entirely sure if this counts as a major contribution. \n\nGRF is not merely GraphNVP with skip connections. We construct a flow-based generative model for graphs with residual layers and by computing inverse operation through fixed-point iteration, while GraphNVP adopts affine coupling layers which are analytically invertible. \nAlso, it is not obvious that i-ResNet can be applied to graphs, which is discrete and not structured like images. We derive a condition where GRF is invertible reflecting topological information of graphs. From Lemma 1 and 2, the upper bound of the Lipschitz constant of graph residual layers can be decided only on the Lipschitz constant of the activation function (Theorem 1).\n\n> The smaller number of parameters in GRF (compared to GraphNVP) come with dramatically reduced performance measures pretty much across the board.\n\nWe observed that increased parameters resulted in instability during training (especially in ZINC-250k). We believe our contribution in terms of its performance is that GRF can achieve comparable scores with less parameters. Thanks to the reduced model size, GRF allows larger batch size than GraphNVP, which is preferable when the number of valid/unique/novel molecules matters rather than their probability.\n\n> Unclear why the reconstruction error in Fig 2 does not monotonically decrease for the ZINC dataset.\n\nWe conjecture that the fluctuation after 60 iterations of ZINC-250k is due to the numerical precision of float32 and the larger model size than that for QM9.\n\n> Unclear why Fig 3 suggests smooth variations in the learned latent spaces. I can only spot mode collapses and sudden jumps. It might help to plot VAE/GraphNVP embeddings here too.\n\nFirst, to the best of our knowledge, there is no standard method that quantitatively evaluates smoothness of the latent space. Thus, we adopted this qualitative evaluation. Though somewhat subjective, this kind of evaluation is common in this area (e.g., JT-VAE (Jin et al., 2018)). We claimed the smoothness of GRF\u2019s learned latent space looking at, for example, the lower right area of Figure 3 (a).\nMode collapse is a problematic phenomenon of GANs, which does not consider the empirical distribution. On the other hand, flow models in general do not suffer it because they are trained to maximize the likelihood of the whole training data. Some generated samples by GRF are the same because graphs are discrete; two slightly different latent variables can go to the same graphs.\nSudden jumps can also be attributed to the discreteness. GRF does not guarantee \u201cuniform smoothness.\u201d\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper541/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper541/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Graph Residual Flow for Molecular Graph Generation", "authors": ["Shion Honda", "Hirotaka Akita", "Katsuhiko Ishiguro", "Toshiki Nakanishi", "Kenta Oono"], "authorids": ["26x.orc.ed5.1hs@gmail.com", "akita714@preferred.jp", "k.ishiguro.jp@ieee.org", "nakanishi@preferred.jp", "oono@preferred.jp"], "keywords": ["deep generative model", "normalizing flow", "graph generation", "cheminformatics"], "TL;DR": "We propose a residual flow model for molecular graphs, derive the conditions so that the flow is invertible, and show its efficacy in experiments.", "abstract": "Statistical generative models for molecular graphs attract attention from many researchers from the fields of bio- and chemo-informatics. Among these models, invertible flow-based approaches are not fully explored yet. In this paper, we propose a powerful invertible flow for molecular graphs, called Graph Residual Flow (GRF). The GRF is based on residual flows, which are known for more flexible and complex non-linear mappings than traditional coupling flows. We theoretically derive non-trivial conditions such that GRF is invertible, and present a way of keeping the entire flows invertible throughout the training and sampling. Experimental results show that a generative model based on the proposed GRF achieve comparable generation performance, with much smaller number of trainable parameters compared to the existing flow-based model. ", "pdf": "/pdf/2b4c83b9ad032d681365303fa6e8cb8a1ae8c95a.pdf", "paperhash": "honda|graph_residual_flow_for_molecular_graph_generation", "original_pdf": "/attachment/d5e9ab230ac7deb1c2e60c0c2e744b86d34f4c0d.pdf", "_bibtex": "@misc{\nhonda2020graph,\ntitle={Graph Residual Flow for Molecular Graph Generation},\nauthor={Shion Honda and Hirotaka Akita and Katsuhiko Ishiguro and Toshiki Nakanishi and Kenta Oono},\nyear={2020},\nurl={https://openreview.net/forum?id=SyepHTNFDS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SyepHTNFDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper541/Authors", "ICLR.cc/2020/Conference/Paper541/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper541/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper541/Reviewers", "ICLR.cc/2020/Conference/Paper541/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper541/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper541/Authors|ICLR.cc/2020/Conference/Paper541/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504169909, "tmdate": 1576860560400, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper541/Authors", "ICLR.cc/2020/Conference/Paper541/Reviewers", "ICLR.cc/2020/Conference/Paper541/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper541/-/Official_Comment"}}}, {"id": "rJxVmxxtjB", "original": null, "number": 1, "cdate": 1573613595751, "ddate": null, "tcdate": 1573613595751, "tmdate": 1573613635511, "tddate": null, "forum": "SyepHTNFDS", "replyto": "Byg02cl9cS", "invitation": "ICLR.cc/2020/Conference/Paper541/-/Official_Comment", "content": {"title": "Response to Reviewer #1", "comment": "We thank the reviewer for their comments and questions. We address them in order.\n\n> The new model may output the same molecule for several times (shown by \u201cUniqueness\"), and it favors the straight-chain molecules.\n\nIt is true that our GRF suffers relatively low uniqueness, but GRF generates graphs in one-shot rather than node by node, so it takes less time for generation than autoregression. Compared to GraphNVP, GRF is more memory-efficient and allows larger batches thanks to the substantially reduced model size. If one desires unique molecules, then GRF can repeat graph generation with a larger batch size until the desired number of unique molecules are obtained. \nAlso, considering the real-world applications, there are some cases that do not require high uniqueness value. For example, lead optimization can be framed as latent variable optimization towards some molecular property, which is not about generating many unique molecules.\n\n> How does the method compared to methods which first generate SMILES string and then convert to molecule graph?\n\nSMILES-based approaches tend to have low validity and reconstruction, probably because the transformation between molecular structure and SMILES string is very hard. We refer to the Figure 1 of JT-VAE paper (Jin et al., 2018). This example clearly shows that a small difference in the structure can result in a large difference in SMILES. Vice versa.\nIncorporating SMILES syntax improves validity (e.g., Dai et al, 2018), while it involves complicated implementation of decoders/generators. On the other hand, GRF can decode latent variables just by the inverse operation. We can implement the model using modern deep learning frameworks such as PyTorch or TensorFlow."}, "signatures": ["ICLR.cc/2020/Conference/Paper541/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper541/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Graph Residual Flow for Molecular Graph Generation", "authors": ["Shion Honda", "Hirotaka Akita", "Katsuhiko Ishiguro", "Toshiki Nakanishi", "Kenta Oono"], "authorids": ["26x.orc.ed5.1hs@gmail.com", "akita714@preferred.jp", "k.ishiguro.jp@ieee.org", "nakanishi@preferred.jp", "oono@preferred.jp"], "keywords": ["deep generative model", "normalizing flow", "graph generation", "cheminformatics"], "TL;DR": "We propose a residual flow model for molecular graphs, derive the conditions so that the flow is invertible, and show its efficacy in experiments.", "abstract": "Statistical generative models for molecular graphs attract attention from many researchers from the fields of bio- and chemo-informatics. Among these models, invertible flow-based approaches are not fully explored yet. In this paper, we propose a powerful invertible flow for molecular graphs, called Graph Residual Flow (GRF). The GRF is based on residual flows, which are known for more flexible and complex non-linear mappings than traditional coupling flows. We theoretically derive non-trivial conditions such that GRF is invertible, and present a way of keeping the entire flows invertible throughout the training and sampling. Experimental results show that a generative model based on the proposed GRF achieve comparable generation performance, with much smaller number of trainable parameters compared to the existing flow-based model. ", "pdf": "/pdf/2b4c83b9ad032d681365303fa6e8cb8a1ae8c95a.pdf", "paperhash": "honda|graph_residual_flow_for_molecular_graph_generation", "original_pdf": "/attachment/d5e9ab230ac7deb1c2e60c0c2e744b86d34f4c0d.pdf", "_bibtex": "@misc{\nhonda2020graph,\ntitle={Graph Residual Flow for Molecular Graph Generation},\nauthor={Shion Honda and Hirotaka Akita and Katsuhiko Ishiguro and Toshiki Nakanishi and Kenta Oono},\nyear={2020},\nurl={https://openreview.net/forum?id=SyepHTNFDS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SyepHTNFDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper541/Authors", "ICLR.cc/2020/Conference/Paper541/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper541/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper541/Reviewers", "ICLR.cc/2020/Conference/Paper541/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper541/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper541/Authors|ICLR.cc/2020/Conference/Paper541/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504169909, "tmdate": 1576860560400, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper541/Authors", "ICLR.cc/2020/Conference/Paper541/Reviewers", "ICLR.cc/2020/Conference/Paper541/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper541/-/Official_Comment"}}}, {"id": "HygVaGssYH", "original": null, "number": 1, "cdate": 1571693243641, "ddate": null, "tcdate": 1571693243641, "tmdate": 1572972582575, "tddate": null, "forum": "SyepHTNFDS", "replyto": "SyepHTNFDS", "invitation": "ICLR.cc/2020/Conference/Paper541/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "Summary:\n\nThe paper proposes a flow-based model for generating molecules, called Graph Residual Flow (GRF). GRF is a version of iResNet that is tailored to molecule generation. The experiments show that GRF achieves similar performance to GraphNVP (a previous flow-based model for molecule generation), but has about 100 times fewer parameters.\n\nDecision:\n\nThis is interesting work, but in my opinion the current version of the paper needs significant improvement. For this reason, my recommendation is weak reject.\n\nContribution & originality:\n\nI think the application of flow models to the generation of molecules is an interesting research direction. Since molecules are structured objects that can be represented as graphs of connected atoms, care has to be taken to design suitable generative models for them, and this paper is a step in this direction.\n\nThe proposed model (GRF) is similar to iResNet in most aspects. The main differences are:\n- The linear layers in iResNet are modified to take into account the connectivity of the molecule.\n- Separate networks are used to generate the adjacency tensor and the feature matrix. The adjacency tensor is generated first, and then the feature-matrix network is conditioned on the generated adjacency matrix.\nOther than the above, GRF is a straightforward application of iResNet in molecule generation.\n\nTechnical & writing quality:\n\nThe experiments are interesting, and GRF is shown to perform similarly to GraphNVP while using 100 times fewer parameters. The comparison between GRF and GraphNVP is in my opinion the most interesting part of the paper. The experiments that show GRF can reconstruct molecules exactly and that generated molecules are a smooth function of the latent variables are less interesting in my opinion; exact invertibility and smoothness are inherent properties of flow-based models that exist by design, so there isn't anything surprising about them.\n\nThe main issue with the paper is that it's poorly written. In my opinion, the writing quality of the paper needs significant improvement for the paper to qualify for publication. The model is explained very poorly; if I weren't already familiar with iResNet I don't think I would be able to understand the model and be able to reproduce the results by reading the paper. Moreover, the use of English throughout the paper is poor: there are a lot of grammatical mistakes (such as incorrect usage of articles or lack of subject-verb agreement), and awkward expressions. In the following I will do my best to make concrete suggestions for improvement, but I would strongly encourage the authors to revise the paper thoroughly and have it checked for grammar and writing quality.\n\nSuggestions for improvement:\n\n\"The decoders of these graph generation models generates\" --> generate\n\n\"The state-of-the-art VAE-based models [...] have good generation performance but their decoding scheme is highly complicated\"\nIn what way is it complicated? Please provide more details.\n\n\"invertible flow-based statistical models [...] does\" --> do\n\n\"[flow-based models don't] require training for their decoders because the decoders are simply the inverse mapping of the\nencoders\"\nThis is an odd and unusual description of flow-based models. The way I see it, flow-based models aren't autoencoders, but functions that reparameterize data (here molecules) in terms of simpler random variables (here Gaussian noise). Saying that flow-based models don't require training for their decoders makes little sense.\n\n\"Liu et al. (2019) proposes\" -->  propose\n\n\"GraphNVP has a serious drawback when applied for sparse graphs\" --> applied to sparse graphs\n\n\"only one affine mapping is executed for each attribute of the latent representation\"\nThere are also coupling layers that are non-affine and therefore more flexible. For example:\n- Neural Autoregressive Flows, https://arxiv.org/abs/1804.00779\n- Sum-Of-Squares Polynomial Flow, https://arxiv.org/abs/1905.02325\n- Neural Spline Flows, https://arxiv.org/abs/1906.04032\n\n\"show that a generative model based on the GRF has much fewer trainable parameters compared to the GraphNVP, while still maintaining a comparable generation performance\"\nIf GRF can achieve a performance similar to GraphNVP with fewer parameters, shouldn't it be able to achieve much better performance with more parameters? If this is true, I would have expected to see an experiment were GRF uses more parameters and as a result outperforms GraphNVP. If GRF doesn't outperform GraphNVP even with more parameters, then I would expect to see a discussion explaining why.\n\nit would be good to explain fully the format of the data as early as in section 2.1. For example, it wasn't clear to me until much later how a flow-based model can be applied to discrete data, and whether the rows of the adjacency tensor and feature matrix are one-hot or not. Currently, the format of the data is partially explained three times (in section 2.1, section 3.1 and section 4.1) and only in section 4.1 things become clear for the reader.\n\nBeginning of page 3: the dequantized matrix X' is used before having been defined. This is an example of why the format of the data should be fully explained early on.\n\n\"The abovementioned formulations map only those variables [..] (Eqs.(2,5), and the remaining nodes [..] (Eqs.(3,5)\"\nThe first reference should be Eqs. (2,4). Also, the parentheses should close.\n\n\"ResNets [...], the golden standard for image recognition\"\nThe expression is \"gold standard\" not \"golden standard\". Also, it's too strong a statement and rather subjective to say that ResNets are the gold standard for image recognition; better say that ResNets have been successful in image recognition, which is an accurate and objective statement.\n\nSection 2.2 introduces iResNets, but it doesn't explain clearly that making the residual block contractive is a sufficient condition for invertibility. This is a crucial element for understanding iResNet that is used later on in the paper, so it should be clearly explained early on.\n\n\"i-ResNet [...] presents a constraint regarding the Lipschitz constant\"\nToo vague, and it doesn't explain what the constraint is.\n\n\"MintNet [...] derives the non-singularity requirements of the Jacobian of the (limited) residual block\"\nAlso vague and not informative.\n\nBeginning of section 3: GraphMVP --> GraphNVP\n\n\"we cannot directly apply the change-of-variables formula directly\"\nUses \"directly\" twice.\n\n\"we do not take it consideration\" --> we do not take into consideration\n\n\"an tensor entry X'[i, m] is a neighborhood\" --> a tensor entry X'[i, m] is a neighbor\n\n\"Similar discussion holds for A'\"\nIt's unclear to me what exactly holds for A', please be more specific. The previous statement mentioned neighbours, how does this apply to A'?\n\nIn the text after eq. (10), R_X uses a different font.\n\n\"is subject to update in every layers\" --> in every layer\n\nSection 3.3. is particularly poorly written. Given that this is one of the main contributions of the paper, it is really important that this section is written well and clearly. Currently, the section contains two one-sentence paragraphs that are clearly out of place. Also, eq. (12) makes little sense. In particular:\n- Isn't A a tensor of shape NxNxR? How did it become a matrix?\n- In what sense is X a matrix representation of z_x? How is X computed? Shouldn't X be of shape NxM? Also, please use a different symbol other than X, as X is already used for the feature matrix.\n- What is D and how is it defined?\n- Is vec() the vectorization operator? Does that mean that the input and output to the residual block has been flattened? This has not been explained or mentioned anywhere.\n- Is W a fully-connected linear layer or a convolution as in iResNet?\n- Where did the term D^{-0.5} A D^{-0.5} come from? How is it motivated / derived? Why does it make sense to use it? I understand that it's related to the graph Laplacian, but more information is needed for readers who are not familiar with graphs.\n\n\"is a Lipschitz-constant of for a certain function\" --> of a certain function\n\nThe explanation of eq. (13) is very poor, and I don't think a reader who is not already familiar with iResNet can follow it. Also, how was the infinite sum approximated? Did you use truncation or the Russian-roulette estimator?\n\nSection 3.4 introduces the term \"atomic feature tensors\" which is not defined. I presume it's the same as the feature matrix. Please be consistent with terminology throughout the paper.\n\n\"adjacent tensor\" --> adjacency tensor\n\n\"we have configure\" --> configured\n\nSection 3.4.2 finally explains the invertibility condition. Clearly, this is an important element of the algorithm, and should be explained fully and clearly early on. By this point, the invertibility condition should be clear, and no further explanation should be needed.\n\n\"we selecte\" --> selected\n\n\"satisfies the differentiability\" --> \"satisfies the differentiability condition\", or even better \"is differentiable\"\n\nWhat does \"kekulized\" mean?\n\n\"conmprises\" --> comprises\n\n\"we adopt node-wise weight sharing for QM9 and low-rank approximation and multi-scale architecture\"\nThese are important aspects of the architecture that are nowhere else mentioned, and should be explained more thoroughly.\n\n\"p_z(T_X, T_A; z)\"\nShouldn't this be p_z(z; T_X, T_A)?\n\nWhat does \"unique molecules\" mean? Are they the molecules that don't exist in the data? How do they differ from \"novel molecules\"?\n\nIt would be good to provide additional results. In particular:\n- It would be good to report training/validation/test log likelihood, or learning curves, that compare between GRF and GraphNVP.\n- It would be good to show how the metrics V, N, U, R in tables 1 and 2 vary with respect to the temperatures (for example, show curves of the four metrics as temperatures vary). Currently, results are reported for only one setting of temperatures, which doesn't show how sensitive the performance is to the temperatures.\n\nThe temperatures for ZINC-250k are really low (0.15 and 0.17), which to me is an indication that the flow model may have fitted the data quite badly.\n\nHow does GRF compare to GraphNVP in terms of generation time? How slow is it to generate from GRF given that it takes 30 iterations for each residual block?\n\nWhat is GraphLinear? Maybe give a citation?\n\n\"Since one of the most GPUs currently used\"\nThis phrase doesn't make much sense.\n\n\"principle components analysis\" --> principal components analysis\n\"Principle\" has a different meaning from \"principal\".\n\nFigure 3: Wouldn't it be more informative to centre the visualizations at the mean and use the two principal axes, rather than random mean and axes?"}, "signatures": ["ICLR.cc/2020/Conference/Paper541/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper541/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Graph Residual Flow for Molecular Graph Generation", "authors": ["Shion Honda", "Hirotaka Akita", "Katsuhiko Ishiguro", "Toshiki Nakanishi", "Kenta Oono"], "authorids": ["26x.orc.ed5.1hs@gmail.com", "akita714@preferred.jp", "k.ishiguro.jp@ieee.org", "nakanishi@preferred.jp", "oono@preferred.jp"], "keywords": ["deep generative model", "normalizing flow", "graph generation", "cheminformatics"], "TL;DR": "We propose a residual flow model for molecular graphs, derive the conditions so that the flow is invertible, and show its efficacy in experiments.", "abstract": "Statistical generative models for molecular graphs attract attention from many researchers from the fields of bio- and chemo-informatics. Among these models, invertible flow-based approaches are not fully explored yet. In this paper, we propose a powerful invertible flow for molecular graphs, called Graph Residual Flow (GRF). The GRF is based on residual flows, which are known for more flexible and complex non-linear mappings than traditional coupling flows. We theoretically derive non-trivial conditions such that GRF is invertible, and present a way of keeping the entire flows invertible throughout the training and sampling. Experimental results show that a generative model based on the proposed GRF achieve comparable generation performance, with much smaller number of trainable parameters compared to the existing flow-based model. ", "pdf": "/pdf/2b4c83b9ad032d681365303fa6e8cb8a1ae8c95a.pdf", "paperhash": "honda|graph_residual_flow_for_molecular_graph_generation", "original_pdf": "/attachment/d5e9ab230ac7deb1c2e60c0c2e744b86d34f4c0d.pdf", "_bibtex": "@misc{\nhonda2020graph,\ntitle={Graph Residual Flow for Molecular Graph Generation},\nauthor={Shion Honda and Hirotaka Akita and Katsuhiko Ishiguro and Toshiki Nakanishi and Kenta Oono},\nyear={2020},\nurl={https://openreview.net/forum?id=SyepHTNFDS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SyepHTNFDS", "replyto": "SyepHTNFDS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper541/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper541/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575771605313, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper541/Reviewers"], "noninvitees": [], "tcdate": 1570237750643, "tmdate": 1575771605327, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper541/-/Official_Review"}}}, {"id": "rkxxhCD0YH", "original": null, "number": 2, "cdate": 1571876519670, "ddate": null, "tcdate": 1571876519670, "tmdate": 1572972582535, "tddate": null, "forum": "SyepHTNFDS", "replyto": "SyepHTNFDS", "invitation": "ICLR.cc/2020/Conference/Paper541/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper introduces an invertible deep generative model architecture for modeling molecular graphs. The model is based on graph residual flows (GRF), which is a graph variant of normalizing flows. The GRF model is a refinement of the GraphNVP generative model, which is also invertible,  but which does not seem to work very well for sparse low-degree graphs (such as molecular graphs). \n\nThe main new idea seems to be to replace the regular coupling flows of GraphNVP with residual blocks: the residual blocks ensure invertibility while also helping to \"mix\" the information in the (tensor) representations in each layer better. This leads to more compact generative models. Due to the residual structure of the encoding model, the inversion (backward mapping) is a bit more expensive since it requires solving a fixed point problem in each layer, but in principle it can be done provided the layer weight matrices have small spectral norm.\n\nExperimental results on two molecular datasets seem to confirm the validity of the proposed architecture.\n\nThe paper is nicely written and the techniques/results are clearly described. However, I have two main concerns:\n\n- Conceptual novelty seems to be limited. The method seems to be a basic modification of the GraphNVP approach by introducing ResNet-like skipped connections. Not entirely sure if this counts as a major contribution. \n- Experimental results do not seem to suggest improvements over the state of the art. I am not an expert in molecular generation, but looking at Tables 1 and 2 seem to suggest that the smaller number of parameters in GRF (compared to GraphNVP) come with dramatically reduced performance measures pretty much across the board. This seems like a weak result So I do not know whether the tradeoffs are worth it or not.\n\nOther comments:\n- The detailed pseudocode of Algorithm 1 isn't really necessary considering it is just a fixed point iteration.\n- Unclear why the reconstruction error in Fig 2 does not monotonically decrease for the ZINC dataset.\n- Unclear why Fig 3 suggests smooth variations in the learned latent spaces. I can only spot mode collapses and sudden jumps. It might help to plot VAE/GraphNVP embeddings here too.\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper541/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper541/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Graph Residual Flow for Molecular Graph Generation", "authors": ["Shion Honda", "Hirotaka Akita", "Katsuhiko Ishiguro", "Toshiki Nakanishi", "Kenta Oono"], "authorids": ["26x.orc.ed5.1hs@gmail.com", "akita714@preferred.jp", "k.ishiguro.jp@ieee.org", "nakanishi@preferred.jp", "oono@preferred.jp"], "keywords": ["deep generative model", "normalizing flow", "graph generation", "cheminformatics"], "TL;DR": "We propose a residual flow model for molecular graphs, derive the conditions so that the flow is invertible, and show its efficacy in experiments.", "abstract": "Statistical generative models for molecular graphs attract attention from many researchers from the fields of bio- and chemo-informatics. Among these models, invertible flow-based approaches are not fully explored yet. In this paper, we propose a powerful invertible flow for molecular graphs, called Graph Residual Flow (GRF). The GRF is based on residual flows, which are known for more flexible and complex non-linear mappings than traditional coupling flows. We theoretically derive non-trivial conditions such that GRF is invertible, and present a way of keeping the entire flows invertible throughout the training and sampling. Experimental results show that a generative model based on the proposed GRF achieve comparable generation performance, with much smaller number of trainable parameters compared to the existing flow-based model. ", "pdf": "/pdf/2b4c83b9ad032d681365303fa6e8cb8a1ae8c95a.pdf", "paperhash": "honda|graph_residual_flow_for_molecular_graph_generation", "original_pdf": "/attachment/d5e9ab230ac7deb1c2e60c0c2e744b86d34f4c0d.pdf", "_bibtex": "@misc{\nhonda2020graph,\ntitle={Graph Residual Flow for Molecular Graph Generation},\nauthor={Shion Honda and Hirotaka Akita and Katsuhiko Ishiguro and Toshiki Nakanishi and Kenta Oono},\nyear={2020},\nurl={https://openreview.net/forum?id=SyepHTNFDS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SyepHTNFDS", "replyto": "SyepHTNFDS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper541/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper541/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575771605313, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper541/Reviewers"], "noninvitees": [], "tcdate": 1570237750643, "tmdate": 1575771605327, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper541/-/Official_Review"}}}, {"id": "Byg02cl9cS", "original": null, "number": 3, "cdate": 1572633270020, "ddate": null, "tcdate": 1572633270020, "tmdate": 1572972582490, "tddate": null, "forum": "SyepHTNFDS", "replyto": "SyepHTNFDS", "invitation": "ICLR.cc/2020/Conference/Paper541/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "GraphNVP is the first paper to introduce the concept of \"invertible flow\", that is to construct the invertible mapping from latent vector z to the graph G. By constructing the mapping from G to z, GraphNVP first changes the discrete feature vector into continuous variables, then update this matrix representation by scaling and transforming functions (Eq. (2)-(5) in this GRF paper). In each iteration the matrix is only updated by one row (one slice for the tensor), while keep other rows intact. Then for constructing the inverse mapping, we can first sample a random vector and then apply the \u201cinverse\u201d of the update rule to recover the edge matrix and node matrix respectively.\n\nThe main contribution for GRF paper is to find a new update rule from the idea of ResNet. The author thinks that GraphNVP only update one row each time, which is less efficient, and the model can only cover a limited number of mappings. Then he proposed a new function for update (Eq. (6)-(7)), which updates all rows each time. The author shows how to approximate the determinant of the Jacobian matrix, and how to construct the inverse mapping from fixed-point iteration, as the mapping is Lipschitz. Lastly the author shows that GRF uses much less parameters than GraphNVP both theoretically and practically, which means that the new model is more expressive. However, the new model may output the same molecule for several times (shown by \u201cUniqueness\"), and it favors the striaght-chain molecules.\n\nAnother question related to the experiment is that how does the method compared to methods which first generate SMILES string and then convert to molecule graph? Eg. Dai Et al. ICLR 2018, Syntax-directed generative model for structured data. "}, "signatures": ["ICLR.cc/2020/Conference/Paper541/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper541/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Graph Residual Flow for Molecular Graph Generation", "authors": ["Shion Honda", "Hirotaka Akita", "Katsuhiko Ishiguro", "Toshiki Nakanishi", "Kenta Oono"], "authorids": ["26x.orc.ed5.1hs@gmail.com", "akita714@preferred.jp", "k.ishiguro.jp@ieee.org", "nakanishi@preferred.jp", "oono@preferred.jp"], "keywords": ["deep generative model", "normalizing flow", "graph generation", "cheminformatics"], "TL;DR": "We propose a residual flow model for molecular graphs, derive the conditions so that the flow is invertible, and show its efficacy in experiments.", "abstract": "Statistical generative models for molecular graphs attract attention from many researchers from the fields of bio- and chemo-informatics. Among these models, invertible flow-based approaches are not fully explored yet. In this paper, we propose a powerful invertible flow for molecular graphs, called Graph Residual Flow (GRF). The GRF is based on residual flows, which are known for more flexible and complex non-linear mappings than traditional coupling flows. We theoretically derive non-trivial conditions such that GRF is invertible, and present a way of keeping the entire flows invertible throughout the training and sampling. Experimental results show that a generative model based on the proposed GRF achieve comparable generation performance, with much smaller number of trainable parameters compared to the existing flow-based model. ", "pdf": "/pdf/2b4c83b9ad032d681365303fa6e8cb8a1ae8c95a.pdf", "paperhash": "honda|graph_residual_flow_for_molecular_graph_generation", "original_pdf": "/attachment/d5e9ab230ac7deb1c2e60c0c2e744b86d34f4c0d.pdf", "_bibtex": "@misc{\nhonda2020graph,\ntitle={Graph Residual Flow for Molecular Graph Generation},\nauthor={Shion Honda and Hirotaka Akita and Katsuhiko Ishiguro and Toshiki Nakanishi and Kenta Oono},\nyear={2020},\nurl={https://openreview.net/forum?id=SyepHTNFDS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SyepHTNFDS", "replyto": "SyepHTNFDS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper541/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper541/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575771605313, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper541/Reviewers"], "noninvitees": [], "tcdate": 1570237750643, "tmdate": 1575771605327, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper541/-/Official_Review"}}}], "count": 8}