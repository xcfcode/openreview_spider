{"notes": [{"id": "rkx5oNQeOV", "original": "SyeDoVjYDN", "number": 30, "cdate": 1553114273932, "ddate": null, "tcdate": 1553114273932, "tmdate": 1562082118840, "tddate": null, "forum": "rkx5oNQeOV", "replyto": null, "invitation": "ICLR.cc/2019/Workshop/LLD/-/Blind_Submission", "content": {"title": "Adaptive Cross-Modal Few-Shot Learning", "authors": ["Chen Xing", "Negar Rostamzadeh", "Boris N. Oreshkin", "Pedro O. Pinheiro"], "authorids": ["xingchen1113@gmail.com", "negar.rostamzadeh@gmail.com", "boris@elementai.com", "pedro@opinheiro.com"], "keywords": ["few-shot learning", "cross-modality"], "abstract": "Metric-based meta-learning techniques have successfully been applied to few-shot classification problems. However, leveraging cross-modal information in a few-shot setting has yet to be explored. When the support from visual information is limited in few-shot image classification, semantic representations (learned from unsupervised text corpora) can provide strong prior knowledge and context to help learning. Based on this intuition, we design a model that is able to leverage visual and semantic features in the context of few-shot classification. We propose an adaptive mechanism that is able to effectively combine both modalities conditioned on categories. Through a series of experiments, we show that our method boosts the performance of metric-based approaches by effectively exploiting language structure. Using this extra modality, our model bypass current unimodal state-of-the-art methods by a large margin on miniImageNet. The improvement in performance is particularly large when the number of shots are small. ", "pdf": "/pdf/cbc354eebe1acb4ffdbb2193827fd3a73261f302.pdf", "paperhash": "xing|adaptive_crossmodal_fewshot_learning"}, "signatures": ["ICLR.cc/2019/Workshop/LLD"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/LLD"], "details": {"replyCount": 3, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/LLD/-/Blind_Submission", "cdate": 1548689671889, "reply": {"forum": null, "replyto": null, "readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2019/Workshop/LLD"]}, "signatures": {"values": ["ICLR.cc/2019/Workshop/LLD"]}, "content": {"authors": {"values-regex": ".*"}, "authorids": {"values-regex": ".*"}}}, "tcdate": 1548689671889, "tmdate": 1557933709646, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/LLD"], "invitees": ["~"], "signatures": ["ICLR.cc/2019/Workshop/LLD"], "details": {"writable": true}}}, "tauthor": "OpenReview.net"}, {"id": "BJxpo7eOYV", "original": null, "number": 1, "cdate": 1554674596909, "ddate": null, "tcdate": 1554674596909, "tmdate": 1555512016039, "tddate": null, "forum": "rkx5oNQeOV", "replyto": "rkx5oNQeOV", "invitation": "ICLR.cc/2019/Workshop/LLD/-/Paper30/Official_Review", "content": {"title": "Interesting new setup and model, but lacking important model details", "review": "Summary\nThis paper proposes an approach for leveraging additional semantic information for solving the recently well-studied task of (visual) few-shot classification. \n\nThe authors demonstrate their particular approach based on the Prototypical Network model. This model is meta-learned via a sequence of tasks, with the goal in each being to correctly classify a set of \u2018query\u2019 images belonging to one of N classes after conditioning on a small handful of \u2018support\u2019 images from the same classes. Prototypical Networks\u2019 mechanism of conditioning on the support set is to use it in order to create a prototype per class by averaging the corresponding support examples. Each query is then classified as the label of its nearest prototype. In this work, the prototype computation is modified to include an additional source of information: the word embedding of the corresponding class label. These two sources are combined via a convex combination with a learnable coefficient to decide the strength of each source. For the prototype of a particular class, this coefficient is defined as a sigmoid of the (transformed in a learnable way) word embedding of that class\u2019 label.\n\nThey show experimentally that a particular variant of their proposed approach is able to surpass the (single modality) state-of-the-art on mini-ImageNet and tiered-ImageNet, with the greatest gains obtained in the 1-shot case. \n\nReview\nPros:\n[+] The proposed problem is an appealing one to study, since in the non-low-shot scenario, analogous multi-modal approaches have shown to be beneficial. Further, the fact that the semantic information is obtained in an unsupervised fashion from word co-occurrences in text corpora makes this setup attractive as no additional labels are required.\n[+] The positive experimental results indeed confirm that semantic embeddings offer useful complementary information and can aid in visual few-shot meta-learning.\n\nCons:\n[-] While I understand that 4 pages is very little space, I found some important information missing pertaining to the models that are proposed and being compared here. In particular, I wasn\u2019t sure what ProtoNets++ is (no citation or explanation is included). Further, it seems that they implemented this approach on top of both ProtoNets++ (yielding AM3-ProtoNets++) and TADAM (yielding AM3-TADAM). I assume that the model they describe is the former. While I am familiar with TADAM, it\u2019s not obvious to me how exactly the semantic information is incorporated into that model. I feel it\u2019s better to sacrifice some space on a short explanation of this and cut space from somewhere else instead.\n[-] Another concern is regarding the potential leakage of information from the meta-test set into the meta-training phase through the word embeddings. Specifically, during the training of word embeddings on large corpora, it may be that the statistics of occurrence of words whose labels belong to the meta-test set of the visual task had influenced the shaping of the word embeddings of words whose labels are in the meta-training set. I understand that this might be hard to control, and I\u2019m not sure how large of a leakage effect there would be, but it would be useful to comment on it.\n\nOverall, I feel this is an interesting problem, and this seems to be an interesting approach for addressing it, so I will recommend acceptance. In future experiments it would be interesting to address situations where not all visual concepts have associated word embeddings. I\u2019m also curious if somehow episodically fine-tuning these word embeddings could yield additional performance gains.\n", "rating": "3: Marginally above acceptance threshold", "confidence": "3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Workshop/LLD/Paper30/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/LLD/Paper30/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive Cross-Modal Few-Shot Learning", "authors": ["Chen Xing", "Negar Rostamzadeh", "Boris N. Oreshkin", "Pedro O. Pinheiro"], "authorids": ["xingchen1113@gmail.com", "negar.rostamzadeh@gmail.com", "boris@elementai.com", "pedro@opinheiro.com"], "keywords": ["few-shot learning", "cross-modality"], "abstract": "Metric-based meta-learning techniques have successfully been applied to few-shot classification problems. However, leveraging cross-modal information in a few-shot setting has yet to be explored. When the support from visual information is limited in few-shot image classification, semantic representations (learned from unsupervised text corpora) can provide strong prior knowledge and context to help learning. Based on this intuition, we design a model that is able to leverage visual and semantic features in the context of few-shot classification. We propose an adaptive mechanism that is able to effectively combine both modalities conditioned on categories. Through a series of experiments, we show that our method boosts the performance of metric-based approaches by effectively exploiting language structure. Using this extra modality, our model bypass current unimodal state-of-the-art methods by a large margin on miniImageNet. The improvement in performance is particularly large when the number of shots are small. ", "pdf": "/pdf/cbc354eebe1acb4ffdbb2193827fd3a73261f302.pdf", "paperhash": "xing|adaptive_crossmodal_fewshot_learning"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/LLD/-/Paper30/Official_Review", "cdate": 1553713417104, "expdate": 1555718400000, "duedate": 1554681600000, "reply": {"forum": "rkx5oNQeOV", "replyto": "rkx5oNQeOV", "writers": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2019/Workshop/LLD/Paper30/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2019/Workshop/LLD/Paper30/AnonReviewer[0-9]+"}, "readers": {"values": ["everyone"], "description": "The users who will be allowed to read the above content."}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["5: Top 15% of accepted papers, strong accept", "4: Top 50% of accepted papers, clear accept", "3: Marginally above acceptance threshold", "2: Marginally below acceptance threshold", "1: Strong rejection"], "required": true}, "confidence": {"order": 4, "value-radio": ["3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "2: The reviewer is fairly confident that the evaluation is correct", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "tcdate": 1553713417104, "tmdate": 1555511827477, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/LLD"], "invitees": ["ICLR.cc/2019/Workshop/LLD/Paper30/Reviewers"], "signatures": ["ICLR.cc/2019/Workshop/LLD"]}}}, {"id": "SkgQRkbtKN", "original": null, "number": 2, "cdate": 1554743242719, "ddate": null, "tcdate": 1554743242719, "tmdate": 1555511882538, "tddate": null, "forum": "rkx5oNQeOV", "replyto": "rkx5oNQeOV", "invitation": "ICLR.cc/2019/Workshop/LLD/-/Paper30/Official_Review", "content": {"title": "Interesting idea, but not the right experiments to prove the point", "review": "This paper proposes an approach for performing multi-modal few shot learning. The main contribution is a new way of combining visual (images) and text features (word embeddings derived from text) which enables the use of existing meta-learning approaches. According to the authors, there are no other multi-modal approaches for few-shot classification.\n\nPros:\n- the problem seems important and useful.\n- the entire paper is described clearly.\n\nCons:\n- this paper is not really doing few-shot learning, because according to section 3.2. and the experiments, the authors use the test labels in order to know which word embeddings to assign to each sample: \"[...] containing label embeddings of all categories in D_train \u222a D_test\". In other words, the authors use the labels (which are the goal of the classification task) to find the match between the two input modalities (to know what Glove vector to assign to each image).\n- the experiments compare the results only between this multimodal approach and visual approaches. I believe using the Glove embeddings alone (no visual input) could give very good results on their own, and it is thus crucial for the authors to compare with this scenario too.\n- the explanation for why you chose this form for lambda_c is unclear: \"A very structured semantic space is a good choice for conditioning.\" \n\nOverall conclusion:\nWhile the tackled problem is important and the paper is very well written, I believe the setting and the chosen dataset are artificial, because it requires looking at the test labels to create their inputs. If the authors had chosen a different dataset where text and images come naturally together (e.g., image captioning tasks), this would indeed be a good contribution. Moreover, I believe the experiments do not cover an important setting that the authors should have compared with (i.e. using only word embeddings as input), to prove that their method gains benefits from both modalities.", "rating": "2: Marginally below acceptance threshold", "confidence": "2: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Workshop/LLD/Paper30/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/LLD/Paper30/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive Cross-Modal Few-Shot Learning", "authors": ["Chen Xing", "Negar Rostamzadeh", "Boris N. Oreshkin", "Pedro O. Pinheiro"], "authorids": ["xingchen1113@gmail.com", "negar.rostamzadeh@gmail.com", "boris@elementai.com", "pedro@opinheiro.com"], "keywords": ["few-shot learning", "cross-modality"], "abstract": "Metric-based meta-learning techniques have successfully been applied to few-shot classification problems. However, leveraging cross-modal information in a few-shot setting has yet to be explored. When the support from visual information is limited in few-shot image classification, semantic representations (learned from unsupervised text corpora) can provide strong prior knowledge and context to help learning. Based on this intuition, we design a model that is able to leverage visual and semantic features in the context of few-shot classification. We propose an adaptive mechanism that is able to effectively combine both modalities conditioned on categories. Through a series of experiments, we show that our method boosts the performance of metric-based approaches by effectively exploiting language structure. Using this extra modality, our model bypass current unimodal state-of-the-art methods by a large margin on miniImageNet. The improvement in performance is particularly large when the number of shots are small. ", "pdf": "/pdf/cbc354eebe1acb4ffdbb2193827fd3a73261f302.pdf", "paperhash": "xing|adaptive_crossmodal_fewshot_learning"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/LLD/-/Paper30/Official_Review", "cdate": 1553713417104, "expdate": 1555718400000, "duedate": 1554681600000, "reply": {"forum": "rkx5oNQeOV", "replyto": "rkx5oNQeOV", "writers": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2019/Workshop/LLD/Paper30/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2019/Workshop/LLD/Paper30/AnonReviewer[0-9]+"}, "readers": {"values": ["everyone"], "description": "The users who will be allowed to read the above content."}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["5: Top 15% of accepted papers, strong accept", "4: Top 50% of accepted papers, clear accept", "3: Marginally above acceptance threshold", "2: Marginally below acceptance threshold", "1: Strong rejection"], "required": true}, "confidence": {"order": 4, "value-radio": ["3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "2: The reviewer is fairly confident that the evaluation is correct", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "tcdate": 1553713417104, "tmdate": 1555511827477, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/LLD"], "invitees": ["ICLR.cc/2019/Workshop/LLD/Paper30/Reviewers"], "signatures": ["ICLR.cc/2019/Workshop/LLD"]}}}, {"id": "Syl-3kafqN", "original": null, "number": 1, "cdate": 1555382184765, "ddate": null, "tcdate": 1555382184765, "tmdate": 1555510975104, "tddate": null, "forum": "rkx5oNQeOV", "replyto": "rkx5oNQeOV", "invitation": "ICLR.cc/2019/Workshop/LLD/-/Paper30/Decision", "content": {"title": "Acceptance Decision", "decision": "Accept"}, "signatures": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive Cross-Modal Few-Shot Learning", "authors": ["Chen Xing", "Negar Rostamzadeh", "Boris N. Oreshkin", "Pedro O. Pinheiro"], "authorids": ["xingchen1113@gmail.com", "negar.rostamzadeh@gmail.com", "boris@elementai.com", "pedro@opinheiro.com"], "keywords": ["few-shot learning", "cross-modality"], "abstract": "Metric-based meta-learning techniques have successfully been applied to few-shot classification problems. However, leveraging cross-modal information in a few-shot setting has yet to be explored. When the support from visual information is limited in few-shot image classification, semantic representations (learned from unsupervised text corpora) can provide strong prior knowledge and context to help learning. Based on this intuition, we design a model that is able to leverage visual and semantic features in the context of few-shot classification. We propose an adaptive mechanism that is able to effectively combine both modalities conditioned on categories. Through a series of experiments, we show that our method boosts the performance of metric-based approaches by effectively exploiting language structure. Using this extra modality, our model bypass current unimodal state-of-the-art methods by a large margin on miniImageNet. The improvement in performance is particularly large when the number of shots are small. ", "pdf": "/pdf/cbc354eebe1acb4ffdbb2193827fd3a73261f302.pdf", "paperhash": "xing|adaptive_crossmodal_fewshot_learning"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/LLD/-/Paper30/Decision", "cdate": 1554736078763, "reply": {"forum": "rkx5oNQeOV", "replyto": "rkx5oNQeOV", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-regex": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "description": "How your identity will be displayed."}, "signatures": {"values": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"title": {"order": 1, "required": true, "value": "Acceptance Decision"}, "decision": {"order": 2, "required": true, "value-radio": ["Accept", "Reject"], "description": "Acceptance decision"}, "comment": {"order": 3, "required": false, "value-regex": "[\\S\\s]{0,5000}", "description": ""}}}, "tcdate": 1554736078763, "tmdate": 1555510960321, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/LLD"], "invitees": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "signatures": ["ICLR.cc/2019/Workshop/LLD"]}}}], "count": 4}