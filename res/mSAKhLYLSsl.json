{"notes": [{"id": "mSAKhLYLSsl", "original": "RqK0-PY9wJ", "number": 420, "cdate": 1601308054074, "ddate": null, "tcdate": 1601308054074, "tmdate": 1615207949731, "tddate": null, "forum": "mSAKhLYLSsl", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Dataset Condensation with Gradient Matching", "authorids": ["~Bo_Zhao4", "~Konda_Reddy_Mopuri3", "~Hakan_Bilen1"], "authors": ["Bo Zhao", "Konda Reddy Mopuri", "Hakan Bilen"], "keywords": ["dataset condensation", "data-efficient learning", "image generation"], "abstract": "As the state-of-the-art machine learning methods in many fields rely on larger datasets, storing datasets and training models on them become significantly more expensive. This paper proposes a training set synthesis technique for data-efficient learning, called Dataset Condensation, that learns to condense large dataset into a small set of informative synthetic samples for training deep neural networks from scratch. We formulate this goal as a gradient matching problem between the gradients of deep neural network weights that are trained on the original and our synthetic data. We rigorously evaluate its performance in several computer vision benchmarks and demonstrate that it significantly outperforms the state-of-the-art methods. Finally we explore the use of our method in continual learning and neural architecture search and report promising gains when limited memory and computations are available.", "one-sentence_summary": "This paper proposes a training set synthesis technique that learns to produce a small set of informative samples for training deep neural networks from scratch in a small fraction of computational cost while achieving as close results as possible.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhao|dataset_condensation_with_gradient_matching", "pdf": "/pdf/f6babe705b5d43a6a88ee3b553c209341b63a5e0.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhao2021dataset,\ntitle={Dataset Condensation with Gradient Matching},\nauthor={Bo Zhao and Konda Reddy Mopuri and Hakan Bilen},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=mSAKhLYLSsl}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 13, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "-Gt3NbnkW6c", "original": null, "number": 2, "cdate": 1614213826531, "ddate": null, "tcdate": 1614213826531, "tmdate": 1614534166757, "tddate": null, "forum": "mSAKhLYLSsl", "replyto": "v19JeswofRt", "invitation": "ICLR.cc/2021/Conference/Paper420/-/Comment", "content": {"title": "Hyper-parameters for extra experiments", "comment": "Dear Pau, thank you for being interested in our paper! As you have opened an issue for this question in our GitHub repository, we have provided detailed hyper-parameters for these extra experiments there. Please refer to the GitHub issue."}, "signatures": ["ICLR.cc/2021/Conference/Paper420/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper420/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dataset Condensation with Gradient Matching", "authorids": ["~Bo_Zhao4", "~Konda_Reddy_Mopuri3", "~Hakan_Bilen1"], "authors": ["Bo Zhao", "Konda Reddy Mopuri", "Hakan Bilen"], "keywords": ["dataset condensation", "data-efficient learning", "image generation"], "abstract": "As the state-of-the-art machine learning methods in many fields rely on larger datasets, storing datasets and training models on them become significantly more expensive. This paper proposes a training set synthesis technique for data-efficient learning, called Dataset Condensation, that learns to condense large dataset into a small set of informative synthetic samples for training deep neural networks from scratch. We formulate this goal as a gradient matching problem between the gradients of deep neural network weights that are trained on the original and our synthetic data. We rigorously evaluate its performance in several computer vision benchmarks and demonstrate that it significantly outperforms the state-of-the-art methods. Finally we explore the use of our method in continual learning and neural architecture search and report promising gains when limited memory and computations are available.", "one-sentence_summary": "This paper proposes a training set synthesis technique that learns to produce a small set of informative samples for training deep neural networks from scratch in a small fraction of computational cost while achieving as close results as possible.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhao|dataset_condensation_with_gradient_matching", "pdf": "/pdf/f6babe705b5d43a6a88ee3b553c209341b63a5e0.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhao2021dataset,\ntitle={Dataset Condensation with Gradient Matching},\nauthor={Bo Zhao and Konda Reddy Mopuri and Hakan Bilen},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=mSAKhLYLSsl}\n}"}, "tags": [], "invitation": {"reply": {"forum": "mSAKhLYLSsl", "readers": {"values": ["everyone"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2021/Conference/Paper420/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper420/Authors|ICLR.cc/2021/Conference/Paper420/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs"}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}}, "multiReply": true, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["everyone"], "tcdate": 1610649461936, "tmdate": 1610649461936, "id": "ICLR.cc/2021/Conference/Paper420/-/Comment"}}}, {"id": "v19JeswofRt", "original": null, "number": 1, "cdate": 1614083773662, "ddate": null, "tcdate": 1614083773662, "tmdate": 1614083773662, "tddate": null, "forum": "mSAKhLYLSsl", "replyto": "iMUcnCQVNhK", "invitation": "ICLR.cc/2021/Conference/Paper420/-/Comment", "content": {"title": "Hyperparams for large synthetic sets", "comment": "Dear authors, congratulations on this interesting work. I have looked at the code you released in https://github.com/VICO-UoE/DatasetCondensation\n\nAnd you define the hyperparams only for the dataset sizes that appear in the paper (up to 50 ipc), would it be possible to provide the hyperparams for the results reported in this comment? (with 50, 100, 200, 500, 1000 condensed images per class).\n\nThank you in advance!"}, "signatures": ["~Pau_de_Jorge1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "~Pau_de_Jorge1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dataset Condensation with Gradient Matching", "authorids": ["~Bo_Zhao4", "~Konda_Reddy_Mopuri3", "~Hakan_Bilen1"], "authors": ["Bo Zhao", "Konda Reddy Mopuri", "Hakan Bilen"], "keywords": ["dataset condensation", "data-efficient learning", "image generation"], "abstract": "As the state-of-the-art machine learning methods in many fields rely on larger datasets, storing datasets and training models on them become significantly more expensive. This paper proposes a training set synthesis technique for data-efficient learning, called Dataset Condensation, that learns to condense large dataset into a small set of informative synthetic samples for training deep neural networks from scratch. We formulate this goal as a gradient matching problem between the gradients of deep neural network weights that are trained on the original and our synthetic data. We rigorously evaluate its performance in several computer vision benchmarks and demonstrate that it significantly outperforms the state-of-the-art methods. Finally we explore the use of our method in continual learning and neural architecture search and report promising gains when limited memory and computations are available.", "one-sentence_summary": "This paper proposes a training set synthesis technique that learns to produce a small set of informative samples for training deep neural networks from scratch in a small fraction of computational cost while achieving as close results as possible.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhao|dataset_condensation_with_gradient_matching", "pdf": "/pdf/f6babe705b5d43a6a88ee3b553c209341b63a5e0.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhao2021dataset,\ntitle={Dataset Condensation with Gradient Matching},\nauthor={Bo Zhao and Konda Reddy Mopuri and Hakan Bilen},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=mSAKhLYLSsl}\n}"}, "tags": [], "invitation": {"reply": {"forum": "mSAKhLYLSsl", "readers": {"values": ["everyone"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2021/Conference/Paper420/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper420/Authors|ICLR.cc/2021/Conference/Paper420/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs"}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}}, "multiReply": true, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["everyone"], "tcdate": 1610649461936, "tmdate": 1610649461936, "id": "ICLR.cc/2021/Conference/Paper420/-/Comment"}}}, {"id": "00VbffDrrdg", "original": null, "number": 1, "cdate": 1610040472782, "ddate": null, "tcdate": 1610040472782, "tmdate": 1610474076946, "tddate": null, "forum": "mSAKhLYLSsl", "replyto": "mSAKhLYLSsl", "invitation": "ICLR.cc/2021/Conference/Paper420/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Oral)", "comment": "The paper introduces a novel dataset condensation technique that generates synthetic samples (images) by matching model gradients with those obtained on the original input samples (images). The authors also show that these synthetic images  are not architecture dependent and can be used to train different deep neural networks. The approach is validated on several smaller datasets like MNIST, SVHN and CIFAR10. This work is well-motivated and the methodological contributions convincing. All reviewers were enthusiastic and indicated that there were no flaws in this work. The rebuttal clarified outstanding questions and made the paper stronger."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dataset Condensation with Gradient Matching", "authorids": ["~Bo_Zhao4", "~Konda_Reddy_Mopuri3", "~Hakan_Bilen1"], "authors": ["Bo Zhao", "Konda Reddy Mopuri", "Hakan Bilen"], "keywords": ["dataset condensation", "data-efficient learning", "image generation"], "abstract": "As the state-of-the-art machine learning methods in many fields rely on larger datasets, storing datasets and training models on them become significantly more expensive. This paper proposes a training set synthesis technique for data-efficient learning, called Dataset Condensation, that learns to condense large dataset into a small set of informative synthetic samples for training deep neural networks from scratch. We formulate this goal as a gradient matching problem between the gradients of deep neural network weights that are trained on the original and our synthetic data. We rigorously evaluate its performance in several computer vision benchmarks and demonstrate that it significantly outperforms the state-of-the-art methods. Finally we explore the use of our method in continual learning and neural architecture search and report promising gains when limited memory and computations are available.", "one-sentence_summary": "This paper proposes a training set synthesis technique that learns to produce a small set of informative samples for training deep neural networks from scratch in a small fraction of computational cost while achieving as close results as possible.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhao|dataset_condensation_with_gradient_matching", "pdf": "/pdf/f6babe705b5d43a6a88ee3b553c209341b63a5e0.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhao2021dataset,\ntitle={Dataset Condensation with Gradient Matching},\nauthor={Bo Zhao and Konda Reddy Mopuri and Hakan Bilen},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=mSAKhLYLSsl}\n}"}, "tags": [], "invitation": {"reply": {"forum": "mSAKhLYLSsl", "replyto": "mSAKhLYLSsl", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040472769, "tmdate": 1610474076930, "id": "ICLR.cc/2021/Conference/Paper420/-/Decision"}}}, {"id": "uh5Ay9WH5Pm", "original": null, "number": 3, "cdate": 1603917662520, "ddate": null, "tcdate": 1603917662520, "tmdate": 1606786263612, "tddate": null, "forum": "mSAKhLYLSsl", "replyto": "mSAKhLYLSsl", "invitation": "ICLR.cc/2021/Conference/Paper420/-/Official_Review", "content": {"title": "Interesting idea with promising empirical results", "review": "##########################################################################\n\nSummary:\n \nThe paper proposes a novel dataset condensation technique that generates synthetic samples by matching model gradients with those obtained on the original input dataset. This technique is investigated empirically on several smaller datasets like MNIST, SVHN and CIFAR10. Two applications to continual learning and neural architecture search (NAS) are also explored and show some promising results.\n\n##########################################################################\n\nReasons for score: \n\nOverall, I vote for accepting this paper. The technique is intuitive and well-justified. Experimental results seem to suggest that it produces a synthetic set that compares favorably to those obtained using alternative methods. Also, additional applications of this technique to continual learning and NAS appear to be quite promising.\n\n##########################################################################\n\nPros: \n\n1. The paper is well written. The core idea is arrived at systematically and is carefully explained.\n\n2. The paper does a good job referencing prior work (with most papers that I knew of being included) and empirical results obtained by the authors appear to compare very favorably to this existing prior work. I do not think that presented empirical results are exhaustive, but they are definitely very promising.\n \n##########################################################################\n\nCons.\n\nI did not see any major problems with the paper. But wanted to make a few comments that could potentially be addressed:\n\n1. I found a sentence \"Note that each real and synthetic batch sampled from T and S contains samples from a single class and the synthetic data for each class are separately updated at each iteration\" a bit confusing. Specifically, does this mean that all samples in T and S have the same label? If so, does this mean that in the case when we have only one sample per class, S contains essentially the same input sample (with or without image augmentations depending on the experiment)?\n\n2. While S is built to match the gradients on the original architecture, it is a little counterintuitive that such a small training set would not cause dramatic overfitting on other (possibly \"heavier\") architectures. Do we need to use multiple synthetic sets in practice, or rely on heavy data augmentation to avoid overfitting? Or just limiting the number of training steps would be enough? It would be interesting to see a more detailed discussion of this aspect. While I am encouraged by NAS results, this remains one of my concerns.\n\n3. It would be interesting to see results on larger datasets like ImageNet or at least CIFAR-100. I understand that this exploration would be quite computationally intensive, but this would make the results much more convincing.\n\n##########################################################################\n\nQuestions during rebuttal period: \n\nPlease address and clarify the cons above. (I will update the score depending on the authors reply.)\n\n#########################################################################\n\nPost-rebuttal.\n\nThanks for a detailed response that clarified some questions and concerns that I previously had. I think the updated paper is stronger and I am inclined to raise the score to 8.\n", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper420/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper420/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dataset Condensation with Gradient Matching", "authorids": ["~Bo_Zhao4", "~Konda_Reddy_Mopuri3", "~Hakan_Bilen1"], "authors": ["Bo Zhao", "Konda Reddy Mopuri", "Hakan Bilen"], "keywords": ["dataset condensation", "data-efficient learning", "image generation"], "abstract": "As the state-of-the-art machine learning methods in many fields rely on larger datasets, storing datasets and training models on them become significantly more expensive. This paper proposes a training set synthesis technique for data-efficient learning, called Dataset Condensation, that learns to condense large dataset into a small set of informative synthetic samples for training deep neural networks from scratch. We formulate this goal as a gradient matching problem between the gradients of deep neural network weights that are trained on the original and our synthetic data. We rigorously evaluate its performance in several computer vision benchmarks and demonstrate that it significantly outperforms the state-of-the-art methods. Finally we explore the use of our method in continual learning and neural architecture search and report promising gains when limited memory and computations are available.", "one-sentence_summary": "This paper proposes a training set synthesis technique that learns to produce a small set of informative samples for training deep neural networks from scratch in a small fraction of computational cost while achieving as close results as possible.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhao|dataset_condensation_with_gradient_matching", "pdf": "/pdf/f6babe705b5d43a6a88ee3b553c209341b63a5e0.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhao2021dataset,\ntitle={Dataset Condensation with Gradient Matching},\nauthor={Bo Zhao and Konda Reddy Mopuri and Hakan Bilen},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=mSAKhLYLSsl}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "mSAKhLYLSsl", "replyto": "mSAKhLYLSsl", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper420/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538143552, "tmdate": 1606915792769, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper420/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper420/-/Official_Review"}}}, {"id": "Qpiz_1WvBbe", "original": null, "number": 1, "cdate": 1603601205188, "ddate": null, "tcdate": 1603601205188, "tmdate": 1606541158855, "tddate": null, "forum": "mSAKhLYLSsl", "replyto": "mSAKhLYLSsl", "invitation": "ICLR.cc/2021/Conference/Paper420/-/Official_Review", "content": {"title": "Good paper with extensive experiments", "review": "Summary: This paper tackles the challenging dataset condensation problem. The goal is to learn to synthesize a small dataset, so that a neural network trained on the small synthetic dataset can have similar performance as a network trained on the full dataset. The proposed method tackles the problem by gradient matching. The proposed method achieves state-of-the-art performance, and shows promising results on two other downstream tasks, continual learning and neural architecture search.\n\nStrength:\n+ Comparing to existing approaches, the proposed method is efficient and effective, achieving the state-of-the-art performance\n+ The authors use the synthetic dataset for two other downstream tasks and achieve promising results\n+ The authors conduct extensive experiments to study and analyze the proposed method\n+ The synthetic dataset trained on one architecture can be also used to train any other networks with different architectures, which makes to method more applicable \n\nWeakness:\n- In Section 2.3 \u201cGradient matching loss\u201d, the authors claim that the proposed distance metric is \u201ca better distance\u201d. It is probably better to use experiments or results to support this claim.\n\nOther comments:\n* Currently, the loss for the target task is cross-entropy loss only (Figure 1(b)). I wonder if this method can be used for other loss functions as well. Also, I wonder if the proposed method can be used for self-supervised tasks as well. Can the authors comment on these?\n* It seems that the authors do not have any special designs to make the condensed synthetic dataset cross-architecture generalizable. I wonder why the proposed method has such a good cross-architecture generalization.\n* In Figure 3, it seems that the performance on CIFAR10 saturates quickly as the number of images per class grows. Just curious, will the relative performance eventually reach 80~90% as in other datasets? If so, then what is the data ratio?\n\n---- Post-rebuttal comments----\n\nThanks for the response. After reading other reviews' comments and the rebuttal, I think this paper is in a good shape now. Thus, I am willing to increase my score to 8 and recommend acceptance.", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper420/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper420/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dataset Condensation with Gradient Matching", "authorids": ["~Bo_Zhao4", "~Konda_Reddy_Mopuri3", "~Hakan_Bilen1"], "authors": ["Bo Zhao", "Konda Reddy Mopuri", "Hakan Bilen"], "keywords": ["dataset condensation", "data-efficient learning", "image generation"], "abstract": "As the state-of-the-art machine learning methods in many fields rely on larger datasets, storing datasets and training models on them become significantly more expensive. This paper proposes a training set synthesis technique for data-efficient learning, called Dataset Condensation, that learns to condense large dataset into a small set of informative synthetic samples for training deep neural networks from scratch. We formulate this goal as a gradient matching problem between the gradients of deep neural network weights that are trained on the original and our synthetic data. We rigorously evaluate its performance in several computer vision benchmarks and demonstrate that it significantly outperforms the state-of-the-art methods. Finally we explore the use of our method in continual learning and neural architecture search and report promising gains when limited memory and computations are available.", "one-sentence_summary": "This paper proposes a training set synthesis technique that learns to produce a small set of informative samples for training deep neural networks from scratch in a small fraction of computational cost while achieving as close results as possible.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhao|dataset_condensation_with_gradient_matching", "pdf": "/pdf/f6babe705b5d43a6a88ee3b553c209341b63a5e0.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhao2021dataset,\ntitle={Dataset Condensation with Gradient Matching},\nauthor={Bo Zhao and Konda Reddy Mopuri and Hakan Bilen},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=mSAKhLYLSsl}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "mSAKhLYLSsl", "replyto": "mSAKhLYLSsl", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper420/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538143552, "tmdate": 1606915792769, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper420/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper420/-/Official_Review"}}}, {"id": "DYO5IP6Lhkt", "original": null, "number": 8, "cdate": 1605813949617, "ddate": null, "tcdate": 1605813949617, "tmdate": 1605813949617, "tddate": null, "forum": "mSAKhLYLSsl", "replyto": "3xjW198akhJ", "invitation": "ICLR.cc/2021/Conference/Paper420/-/Official_Comment", "content": {"title": "Thank you for the correction!", "comment": " "}, "signatures": ["ICLR.cc/2021/Conference/Paper420/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper420/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dataset Condensation with Gradient Matching", "authorids": ["~Bo_Zhao4", "~Konda_Reddy_Mopuri3", "~Hakan_Bilen1"], "authors": ["Bo Zhao", "Konda Reddy Mopuri", "Hakan Bilen"], "keywords": ["dataset condensation", "data-efficient learning", "image generation"], "abstract": "As the state-of-the-art machine learning methods in many fields rely on larger datasets, storing datasets and training models on them become significantly more expensive. This paper proposes a training set synthesis technique for data-efficient learning, called Dataset Condensation, that learns to condense large dataset into a small set of informative synthetic samples for training deep neural networks from scratch. We formulate this goal as a gradient matching problem between the gradients of deep neural network weights that are trained on the original and our synthetic data. We rigorously evaluate its performance in several computer vision benchmarks and demonstrate that it significantly outperforms the state-of-the-art methods. Finally we explore the use of our method in continual learning and neural architecture search and report promising gains when limited memory and computations are available.", "one-sentence_summary": "This paper proposes a training set synthesis technique that learns to produce a small set of informative samples for training deep neural networks from scratch in a small fraction of computational cost while achieving as close results as possible.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhao|dataset_condensation_with_gradient_matching", "pdf": "/pdf/f6babe705b5d43a6a88ee3b553c209341b63a5e0.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhao2021dataset,\ntitle={Dataset Condensation with Gradient Matching},\nauthor={Bo Zhao and Konda Reddy Mopuri and Hakan Bilen},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=mSAKhLYLSsl}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "mSAKhLYLSsl", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper420/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper420/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper420/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper420/Authors|ICLR.cc/2021/Conference/Paper420/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper420/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923871181, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper420/-/Official_Comment"}}}, {"id": "3xjW198akhJ", "original": null, "number": 7, "cdate": 1605813165312, "ddate": null, "tcdate": 1605813165312, "tmdate": 1605813165312, "tddate": null, "forum": "mSAKhLYLSsl", "replyto": "-L07ABoO8Dn", "invitation": "ICLR.cc/2021/Conference/Paper420/-/Official_Comment", "content": {"title": "Response to AnonReviewer3 - Removed the Linear Regression", "comment": "Thanks for your comment. We agree with your point. Thus we have removed the linear regression from Figure F5, instead we only report the Spearman\u2019s rank correlation coefficient. Please refer to the latest paper."}, "signatures": ["ICLR.cc/2021/Conference/Paper420/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper420/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dataset Condensation with Gradient Matching", "authorids": ["~Bo_Zhao4", "~Konda_Reddy_Mopuri3", "~Hakan_Bilen1"], "authors": ["Bo Zhao", "Konda Reddy Mopuri", "Hakan Bilen"], "keywords": ["dataset condensation", "data-efficient learning", "image generation"], "abstract": "As the state-of-the-art machine learning methods in many fields rely on larger datasets, storing datasets and training models on them become significantly more expensive. This paper proposes a training set synthesis technique for data-efficient learning, called Dataset Condensation, that learns to condense large dataset into a small set of informative synthetic samples for training deep neural networks from scratch. We formulate this goal as a gradient matching problem between the gradients of deep neural network weights that are trained on the original and our synthetic data. We rigorously evaluate its performance in several computer vision benchmarks and demonstrate that it significantly outperforms the state-of-the-art methods. Finally we explore the use of our method in continual learning and neural architecture search and report promising gains when limited memory and computations are available.", "one-sentence_summary": "This paper proposes a training set synthesis technique that learns to produce a small set of informative samples for training deep neural networks from scratch in a small fraction of computational cost while achieving as close results as possible.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhao|dataset_condensation_with_gradient_matching", "pdf": "/pdf/f6babe705b5d43a6a88ee3b553c209341b63a5e0.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhao2021dataset,\ntitle={Dataset Condensation with Gradient Matching},\nauthor={Bo Zhao and Konda Reddy Mopuri and Hakan Bilen},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=mSAKhLYLSsl}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "mSAKhLYLSsl", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper420/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper420/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper420/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper420/Authors|ICLR.cc/2021/Conference/Paper420/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper420/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923871181, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper420/-/Official_Comment"}}}, {"id": "-L07ABoO8Dn", "original": null, "number": 6, "cdate": 1605706480660, "ddate": null, "tcdate": 1605706480660, "tmdate": 1605706480660, "tddate": null, "forum": "mSAKhLYLSsl", "replyto": "FZLA3xuMTAG", "invitation": "ICLR.cc/2021/Conference/Paper420/-/Official_Comment", "content": {"title": "Re: A1", "comment": ">We agree that fitting a line to such data may not be very informative...\n>We followed the previous work...\n\nIf previous papers proposed some poor practice, the follow-up works should correct it, not repeat and reinforce it. "}, "signatures": ["ICLR.cc/2021/Conference/Paper420/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper420/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dataset Condensation with Gradient Matching", "authorids": ["~Bo_Zhao4", "~Konda_Reddy_Mopuri3", "~Hakan_Bilen1"], "authors": ["Bo Zhao", "Konda Reddy Mopuri", "Hakan Bilen"], "keywords": ["dataset condensation", "data-efficient learning", "image generation"], "abstract": "As the state-of-the-art machine learning methods in many fields rely on larger datasets, storing datasets and training models on them become significantly more expensive. This paper proposes a training set synthesis technique for data-efficient learning, called Dataset Condensation, that learns to condense large dataset into a small set of informative synthetic samples for training deep neural networks from scratch. We formulate this goal as a gradient matching problem between the gradients of deep neural network weights that are trained on the original and our synthetic data. We rigorously evaluate its performance in several computer vision benchmarks and demonstrate that it significantly outperforms the state-of-the-art methods. Finally we explore the use of our method in continual learning and neural architecture search and report promising gains when limited memory and computations are available.", "one-sentence_summary": "This paper proposes a training set synthesis technique that learns to produce a small set of informative samples for training deep neural networks from scratch in a small fraction of computational cost while achieving as close results as possible.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhao|dataset_condensation_with_gradient_matching", "pdf": "/pdf/f6babe705b5d43a6a88ee3b553c209341b63a5e0.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhao2021dataset,\ntitle={Dataset Condensation with Gradient Matching},\nauthor={Bo Zhao and Konda Reddy Mopuri and Hakan Bilen},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=mSAKhLYLSsl}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "mSAKhLYLSsl", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper420/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper420/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper420/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper420/Authors|ICLR.cc/2021/Conference/Paper420/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper420/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923871181, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper420/-/Official_Comment"}}}, {"id": "zPngfLS4lwn", "original": null, "number": 2, "cdate": 1603898539278, "ddate": null, "tcdate": 1603898539278, "tmdate": 1605706331508, "tddate": null, "forum": "mSAKhLYLSsl", "replyto": "mSAKhLYLSsl", "invitation": "ICLR.cc/2021/Conference/Paper420/-/Official_Review", "content": {"title": "Good paper", "review": "The paper presents a method for generating synthetic datasets from the large realworld datasets. The CNN trained on such synthetic dataset supposed to have similar accuracy on the realworld data, as trained on the original one. The benefit of a such procedure is reduced model training time and storage space (for data).\n\n\nThe method is built on the idea that the gradients of the network being trained on the real images should be similar to gradients, which were obtained by the training on the synthetic images.\n   \nThe method is validated on MNIST, SVHN, FashionMNIST and CIFAR-10 on several different architectures: MLP, AlexNet, VGG-like and ResNet architectures. \n\nMoreover, the paper compares the proposed method vs  many other baselines, e.g. methods, which select \"representative\" image from the dataset (coreset methods), as well as Dataset Distillation and cGAN. \n\n*****\nOverall I like the paper a lot. The method is well-motivated, shows good (sota) results and also often (for MNIST, SVHN, FashionMNIST)  produces human-recognizable examples, although there is no term/regularization directly encouraging this. \n\nThe paper also studies how architectural choices like normalization, pooling, etc. influence the generated samples and how samples generated for the architecture A are suitable for training architecture B.\n\nI don't see any major weakness in the paper.\n\n\nQuestions and comments.\n\n- In Figure 5, IMO it is bad practice to fit linear regression into blob-like data. (minor)\n- Have you tried to add some term, encouranding the diversity for the different synthetic samples belonging to the same class? \n\n********\n\nPost-rebuttal. \n\nThe rebuttal didn't raised any concerns and made the paper even stronger, thus I am keeping my score.\n", "rating": "9: Top 15% of accepted papers, strong accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper420/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper420/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dataset Condensation with Gradient Matching", "authorids": ["~Bo_Zhao4", "~Konda_Reddy_Mopuri3", "~Hakan_Bilen1"], "authors": ["Bo Zhao", "Konda Reddy Mopuri", "Hakan Bilen"], "keywords": ["dataset condensation", "data-efficient learning", "image generation"], "abstract": "As the state-of-the-art machine learning methods in many fields rely on larger datasets, storing datasets and training models on them become significantly more expensive. This paper proposes a training set synthesis technique for data-efficient learning, called Dataset Condensation, that learns to condense large dataset into a small set of informative synthetic samples for training deep neural networks from scratch. We formulate this goal as a gradient matching problem between the gradients of deep neural network weights that are trained on the original and our synthetic data. We rigorously evaluate its performance in several computer vision benchmarks and demonstrate that it significantly outperforms the state-of-the-art methods. Finally we explore the use of our method in continual learning and neural architecture search and report promising gains when limited memory and computations are available.", "one-sentence_summary": "This paper proposes a training set synthesis technique that learns to produce a small set of informative samples for training deep neural networks from scratch in a small fraction of computational cost while achieving as close results as possible.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhao|dataset_condensation_with_gradient_matching", "pdf": "/pdf/f6babe705b5d43a6a88ee3b553c209341b63a5e0.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhao2021dataset,\ntitle={Dataset Condensation with Gradient Matching},\nauthor={Bo Zhao and Konda Reddy Mopuri and Hakan Bilen},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=mSAKhLYLSsl}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "mSAKhLYLSsl", "replyto": "mSAKhLYLSsl", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper420/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538143552, "tmdate": 1606915792769, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper420/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper420/-/Official_Review"}}}, {"id": "iMUcnCQVNhK", "original": null, "number": 4, "cdate": 1605561642469, "ddate": null, "tcdate": 1605561642469, "tmdate": 1605562777400, "tddate": null, "forum": "mSAKhLYLSsl", "replyto": "Qpiz_1WvBbe", "invitation": "ICLR.cc/2021/Conference/Paper420/-/Official_Comment", "content": {"title": "Response to AnonReviewer2", "comment": "We thank the reviewer for his/her valuable comments and time.\n\nA1 \"Experimental justification for the gradient matching loss\": We agree with the reviewer that it is important to justify the decision. To this end, we compare the proposed distance metric to standard Euclidean distance and cosine distance after vectorising gradient tensors each layer and concatenating them as a single vector. By using each distance, we learn one condensed image per class on MNIST dataset and use them to train the same architecture from scratch. We repeat this experiment on different architectures (MLP, ConvNet, LeNet, AlexNet, VGG11 and ResNet18) and report their test accuracies below. The results show that the proposed distance metric obtains substantially better classification results than two baselines. We have added this ablation study in the supplementary material - Further Analysis - Ablation study on gradient distance metric. For more details, please refer to the revised paper. \n\n|           | MLP      | ConvNet  | LeNet    | AlexNet  | VGG      | ResNet   |\n|-----------------|----------------|----------------|----------------|----------------|----------------|----------------|\n| Euclidean | 69.3\u00b10.9 | 92.7\u00b10.3 | 65.0\u00b15.1 | 66.2\u00b15.6 | 57.1\u00b17.0 | 68.0\u00b15.2 |\n| Cosine    | 45.2\u00b13.6 | 69.2\u00b12.7 | 61.1\u00b18.2 | 58.3\u00b14.1 | 55.0\u00b15.0 | 68.8\u00b17.8 |\n| Ours      | 70.5\u00b11.2 | 91.7\u00b10.5 | 85.0\u00b11.7 | 82.7\u00b12.9 | 81.7\u00b12.6 | 89.4\u00b10.9 |\n\nA2 \"Other loss function than cross-entropy\": Although we have only trained our images on classification problems with cross-entropy loss, we cannot see any obvious reason that it should not work on a different loss function such as L1/L2 loss, given that it is differentiable. In future, we plan to test it on a regression problem such as facial landmark regression. \n\nA3 \"Self-supervised learning\": Our method could be used in self-supervised learning problems such as estimating rotation of an image (Gidaris et al 2018 ICLR) without any major modification. For more recent ones such as Chen et al 2020 ICML that uses contrastive loss over augmented data, the positive and negative data pairs are required to be constructed with the synthetic and real samples.\n\n[Gidaris et al 2018 ICLR]: Unsupervised Representation Learning by Predicting Image Rotations \n[Chen et al 2020 ICML]: A Simple Framework for Contrastive Learning of Visual Representations\n\nA4 \u201cWhy the proposed method has such a good cross-architecture generalization\u201d: While it is very difficult to provide a definite answer for the question, our intuition is that matching two sets of gradients with respect to convolutional kernels or fully connected weights enables the condensed images to encode discriminative information for the target task in a spatially structured way. At a high-level, the condensed images capture the correlation between pixels and high-dimensional deep representations in a very compressed way. We believe that such correlations are informative to the architectures that can relate these two in a similar way. For instance, our results show that condensed images learned in one convolutional architecture generalise well to another convolutional architecture but this is not the case from MLP to a convolutional network.\n\nA5 \u201dData ratio for reaching 80-90% of the upper-bound in CIFAR10\u201d: We evaluated the performance of our method on different numbers of condensed images per class in ConvNet architecture. Our method achieves 64%, 67%, 71%, 77%, 83% relative accuracy with 50, 100, 200, 500, 1000 condensed images per class, which are only 1%, 2%, 4%, 10%, 20% of the size of the original CIFAR10 dataset. We agree with the reviewer that pushing the results closer to the upper-bound is challenging and requires further improvements to our method. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper420/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper420/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dataset Condensation with Gradient Matching", "authorids": ["~Bo_Zhao4", "~Konda_Reddy_Mopuri3", "~Hakan_Bilen1"], "authors": ["Bo Zhao", "Konda Reddy Mopuri", "Hakan Bilen"], "keywords": ["dataset condensation", "data-efficient learning", "image generation"], "abstract": "As the state-of-the-art machine learning methods in many fields rely on larger datasets, storing datasets and training models on them become significantly more expensive. This paper proposes a training set synthesis technique for data-efficient learning, called Dataset Condensation, that learns to condense large dataset into a small set of informative synthetic samples for training deep neural networks from scratch. We formulate this goal as a gradient matching problem between the gradients of deep neural network weights that are trained on the original and our synthetic data. We rigorously evaluate its performance in several computer vision benchmarks and demonstrate that it significantly outperforms the state-of-the-art methods. Finally we explore the use of our method in continual learning and neural architecture search and report promising gains when limited memory and computations are available.", "one-sentence_summary": "This paper proposes a training set synthesis technique that learns to produce a small set of informative samples for training deep neural networks from scratch in a small fraction of computational cost while achieving as close results as possible.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhao|dataset_condensation_with_gradient_matching", "pdf": "/pdf/f6babe705b5d43a6a88ee3b553c209341b63a5e0.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhao2021dataset,\ntitle={Dataset Condensation with Gradient Matching},\nauthor={Bo Zhao and Konda Reddy Mopuri and Hakan Bilen},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=mSAKhLYLSsl}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "mSAKhLYLSsl", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper420/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper420/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper420/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper420/Authors|ICLR.cc/2021/Conference/Paper420/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper420/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923871181, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper420/-/Official_Comment"}}}, {"id": "vc97k1L7vqO", "original": null, "number": 5, "cdate": 1605562283015, "ddate": null, "tcdate": 1605562283015, "tmdate": 1605562283015, "tddate": null, "forum": "mSAKhLYLSsl", "replyto": "mSAKhLYLSsl", "invitation": "ICLR.cc/2021/Conference/Paper420/-/Official_Comment", "content": {"title": "Summary of changes in the revised paper.", "comment": "According to the reviews, we polished the paper in the following two aspects:\n\n1. To further clarify the mini-batch sampling process, we have added several notations and comments in Algorithm 1, and also give more details and discussion in Supplementary - Implementation Details - Dataset Condensation - Paragraph 2.\n\n2. To justify the effectiveness and robustness of the proposed gradient distance metric, we added the ablation study on the gradient distance metric in the supplementary material - Further Analysis - Ablation study on gradient distance metric."}, "signatures": ["ICLR.cc/2021/Conference/Paper420/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper420/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dataset Condensation with Gradient Matching", "authorids": ["~Bo_Zhao4", "~Konda_Reddy_Mopuri3", "~Hakan_Bilen1"], "authors": ["Bo Zhao", "Konda Reddy Mopuri", "Hakan Bilen"], "keywords": ["dataset condensation", "data-efficient learning", "image generation"], "abstract": "As the state-of-the-art machine learning methods in many fields rely on larger datasets, storing datasets and training models on them become significantly more expensive. This paper proposes a training set synthesis technique for data-efficient learning, called Dataset Condensation, that learns to condense large dataset into a small set of informative synthetic samples for training deep neural networks from scratch. We formulate this goal as a gradient matching problem between the gradients of deep neural network weights that are trained on the original and our synthetic data. We rigorously evaluate its performance in several computer vision benchmarks and demonstrate that it significantly outperforms the state-of-the-art methods. Finally we explore the use of our method in continual learning and neural architecture search and report promising gains when limited memory and computations are available.", "one-sentence_summary": "This paper proposes a training set synthesis technique that learns to produce a small set of informative samples for training deep neural networks from scratch in a small fraction of computational cost while achieving as close results as possible.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhao|dataset_condensation_with_gradient_matching", "pdf": "/pdf/f6babe705b5d43a6a88ee3b553c209341b63a5e0.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhao2021dataset,\ntitle={Dataset Condensation with Gradient Matching},\nauthor={Bo Zhao and Konda Reddy Mopuri and Hakan Bilen},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=mSAKhLYLSsl}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "mSAKhLYLSsl", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper420/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper420/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper420/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper420/Authors|ICLR.cc/2021/Conference/Paper420/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper420/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923871181, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper420/-/Official_Comment"}}}, {"id": "FZLA3xuMTAG", "original": null, "number": 3, "cdate": 1605560683319, "ddate": null, "tcdate": 1605560683319, "tmdate": 1605560683319, "tddate": null, "forum": "mSAKhLYLSsl", "replyto": "zPngfLS4lwn", "invitation": "ICLR.cc/2021/Conference/Paper420/-/Official_Comment", "content": {"title": "Response to AnonReviewer3", "comment": "We thank the reviewer for his/her valuable comments and time.\n\nA1 \u201cFit linear regression into blob-like data\u201d: We agree that fitting a line to such data may not be very informative, especially when the correlation is weak. Our main point was to only visualise the linear correlation between two sets of accuracies that are obtained with proxy dataset and full dataset training. We followed the previous work (Shleifer et al 2019 arXiv, Wang et al 2020 CVPR)  in the visualization.\n\n[Shleifer et al 2019 arXiv] Using Small Proxy Datasets to Accelerate Hyperparameter Search.\n[Wang et al 2020 CVPR] Nas-fcos: Fast neural architecture search for object detection.\n\nA2 \u201cSome term for encouraging diversity\u201d: While this is a very good suggestion, designing a good regularizer for encouraging diversity among condensed images is challenging. Naive solutions such as imposing orthogonality among the pixels of condensed images would not be meaningful. More promising solution may be encouraging orthogonality between the gradients of each synthetic sample. However, the gradient space is typically very high dimensional and a simple measure of orthogonality would be very weak. We currently work on grouping real images based on their appearance and learn a condensed image to match the average gradient of each such group.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper420/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper420/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dataset Condensation with Gradient Matching", "authorids": ["~Bo_Zhao4", "~Konda_Reddy_Mopuri3", "~Hakan_Bilen1"], "authors": ["Bo Zhao", "Konda Reddy Mopuri", "Hakan Bilen"], "keywords": ["dataset condensation", "data-efficient learning", "image generation"], "abstract": "As the state-of-the-art machine learning methods in many fields rely on larger datasets, storing datasets and training models on them become significantly more expensive. This paper proposes a training set synthesis technique for data-efficient learning, called Dataset Condensation, that learns to condense large dataset into a small set of informative synthetic samples for training deep neural networks from scratch. We formulate this goal as a gradient matching problem between the gradients of deep neural network weights that are trained on the original and our synthetic data. We rigorously evaluate its performance in several computer vision benchmarks and demonstrate that it significantly outperforms the state-of-the-art methods. Finally we explore the use of our method in continual learning and neural architecture search and report promising gains when limited memory and computations are available.", "one-sentence_summary": "This paper proposes a training set synthesis technique that learns to produce a small set of informative samples for training deep neural networks from scratch in a small fraction of computational cost while achieving as close results as possible.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhao|dataset_condensation_with_gradient_matching", "pdf": "/pdf/f6babe705b5d43a6a88ee3b553c209341b63a5e0.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhao2021dataset,\ntitle={Dataset Condensation with Gradient Matching},\nauthor={Bo Zhao and Konda Reddy Mopuri and Hakan Bilen},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=mSAKhLYLSsl}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "mSAKhLYLSsl", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper420/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper420/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper420/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper420/Authors|ICLR.cc/2021/Conference/Paper420/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper420/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923871181, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper420/-/Official_Comment"}}}, {"id": "qzeM2RB6QaW", "original": null, "number": 2, "cdate": 1605559980570, "ddate": null, "tcdate": 1605559980570, "tmdate": 1605560011614, "tddate": null, "forum": "mSAKhLYLSsl", "replyto": "uh5Ay9WH5Pm", "invitation": "ICLR.cc/2021/Conference/Paper420/-/Official_Comment", "content": {"title": "Response to AnonReviewer4", "comment": "We thank the reviewer for his/her valuable comments and time.\n\nA1: $\\mathcal{T}$ and $\\mathcal{S}$ contain images from all classes. At each training iteration, we sample single-class mini-batches at Step 6 of Algorithm 1 (see the revised paper). Specifically, we sample a mini-batch pair $B^\\mathcal{T}_c$ and $B^\\mathcal{S}_c$ that contain real and synthetic images from the same class $c$ at each inner iteration $t$. Then, the matching loss for each class is computed with the sampled mini-batch pair and used to update corresponding synthetic images $\\mathcal{S}_c$ by back-propagation (Step 7 and 8 of Algorithm 1 in the revised paper). This is repeated for every class. Training as such is not slower than using mixed-class batches, and  it can also be sped up by running them in parallel for each class. When we use mixed mini-batches, our method still works well. However, we found that separate-class strategy is faster to train as matching gradients w.r.t. data from single class is easier compared to those of multiple classes.\nWe have clarified these points in our notation and comments for Algorithm 1, and also give more details in Supplementary - Implementation Details - Dataset Condensation - Paragraph 2.\n\nA2: When we use condensed images to train heavy architectures (like AlexNet, VGG, ResNet), we indeed apply substantial data augmentation (crop, scale and rotate) to avoid overfitting. This is mentioned in Supplementary - Implementation Details \u2013 Dataset condensation - Paragraph 3. We don't need to use multiple synthetic sets to train a model. We have not applied early stopping and used a fixed number of training iterations. We follow the same practice for the baselines that we compare to.\n\nA3: Thank you for understanding that we are working on a largely-unexplored and challenging research problem. Here, we give some preliminary results on the CIFAR100 dataset in the following tables. The default ConvNet is used and all hyper-parameters are the same as corresponding ones for CIFAR10. The results show that our method remarkably outperforms the strong baseline - Herding in all settings.\n\n====================================================================\n\nAbsolute accuracy % on CIFAR100 dataset:\n\n|         | 1 image/class | 10 images/class | 20 images/class |\n|----------------|---------------|----------------|----------------|\n| Random     | 4.2\u00b10.3       | 14.6\u00b10.5       | 20.8\u00b10.6       |\n| Herding     | 8.4\u00b10.3       | 17.3\u00b10.3       |  21.5\u00b10.3       |\n| Ours           | 12.8\u00b10.3      | 25.2\u00b10.3       | 30.4\u00b10.3       |\n\n====================================================================\n\nRelative accuracy % (the ratio compared to its upper-bound 56.2$\\pm$0.3) on CIFAR100 dataset:\n\n|         | 1 image/class | 10 images/class | 20 images/class |\n|---------|---------------|----------------|----------------|\n| Random  | 7.5       | 26.0       | 37.0       | \n| Herding | 14.9       | 30.8      |  38.3       | \n| Ours    | 22.8      | 44.8       | 54.1       |\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper420/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper420/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dataset Condensation with Gradient Matching", "authorids": ["~Bo_Zhao4", "~Konda_Reddy_Mopuri3", "~Hakan_Bilen1"], "authors": ["Bo Zhao", "Konda Reddy Mopuri", "Hakan Bilen"], "keywords": ["dataset condensation", "data-efficient learning", "image generation"], "abstract": "As the state-of-the-art machine learning methods in many fields rely on larger datasets, storing datasets and training models on them become significantly more expensive. This paper proposes a training set synthesis technique for data-efficient learning, called Dataset Condensation, that learns to condense large dataset into a small set of informative synthetic samples for training deep neural networks from scratch. We formulate this goal as a gradient matching problem between the gradients of deep neural network weights that are trained on the original and our synthetic data. We rigorously evaluate its performance in several computer vision benchmarks and demonstrate that it significantly outperforms the state-of-the-art methods. Finally we explore the use of our method in continual learning and neural architecture search and report promising gains when limited memory and computations are available.", "one-sentence_summary": "This paper proposes a training set synthesis technique that learns to produce a small set of informative samples for training deep neural networks from scratch in a small fraction of computational cost while achieving as close results as possible.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhao|dataset_condensation_with_gradient_matching", "pdf": "/pdf/f6babe705b5d43a6a88ee3b553c209341b63a5e0.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhao2021dataset,\ntitle={Dataset Condensation with Gradient Matching},\nauthor={Bo Zhao and Konda Reddy Mopuri and Hakan Bilen},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=mSAKhLYLSsl}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "mSAKhLYLSsl", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper420/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper420/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper420/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper420/Authors|ICLR.cc/2021/Conference/Paper420/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper420/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923871181, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper420/-/Official_Comment"}}}], "count": 14}