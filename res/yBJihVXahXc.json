{"notes": [{"id": "yBJihVXahXc", "original": "4NeFKdNJ5aO", "number": 3096, "cdate": 1601308343532, "ddate": null, "tcdate": 1601308343532, "tmdate": 1614985689934, "tddate": null, "forum": "yBJihVXahXc", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Generalizing Graph Convolutional Networks via Heat Kernel", "authorids": ["~Jialin_Zhao1", "~Yuxiao_Dong1", "~Jie_Tang1", "dm18@mails.tsinghua.edu.cn", "~Kuansan_Wang1"], "authors": ["Jialin Zhao", "Yuxiao Dong", "Jie Tang", "Ming Ding", "Kuansan Wang"], "keywords": ["graph networks"], "abstract": "Graph convolutional networks (GCNs) have emerged as a powerful framework for mining and learning with graphs. A recent study shows that GCNs can be simplified as a linear model by removing nonlinearities and weight matrices across all consecutive layers, resulting the simple graph convolution (SGC) model. In this paper, we aim to understand GCNs and generalize SGC as a linear model via heat kernel (HKGCN), which acts as a low-pass filter on graphs and enables the aggregation of information from extremely large receptive fields. We theoretically show that HKGCN is in nature a continuous propagation model and GCNs without nonlinearities (i.e., SGC) are the discrete versions of it. Its low-pass filter and continuity properties facilitate the fast and smooth convergence of feature propagation. Experiments on million-scale networks show that the linear HKGCN model not only achieves consistently better results than SGC but also can match or even beat advanced GCN models, while maintaining SGC\u2019s superiority in efficiency.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhao|generalizing_graph_convolutional_networks_via_heat_kernel", "one-sentence_summary": "A continuous propagation model of GCNs with heat kernel.", "pdf": "/pdf/fcaaaad4d1105d321bc76f36d14137a1577cff52.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=So5grXZ6Hd", "_bibtex": "@misc{\nzhao2021generalizing,\ntitle={Generalizing Graph Convolutional Networks via Heat Kernel},\nauthor={Jialin Zhao and Yuxiao Dong and Jie Tang and Ming Ding and Kuansan Wang},\nyear={2021},\nurl={https://openreview.net/forum?id=yBJihVXahXc}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "Cg1tYKgnyGy", "original": null, "number": 1, "cdate": 1610040460150, "ddate": null, "tcdate": 1610040460150, "tmdate": 1610474063063, "tddate": null, "forum": "yBJihVXahXc", "replyto": "yBJihVXahXc", "invitation": "ICLR.cc/2021/Conference/Paper3096/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "This paper has been evaluated by four reviewers who overall hesitated between borderline reject/accept. In general, as Rev. 4 points out, this paper appears to cope with over-oscillation rather than over-smoothing aspect of GCN modeling (something worth clarifying). Rev. 3 also rightly points out that the connection between the heat kernel and GCN in fact was established in previous works. Also, the connection between SGC (polynomial filter) and  the heat diffusion (the spectral filter matrix) is hard to overlook (the impression that this work builds heavily on SGC). Therefore, while AC sympathizes with the idea, it is also difficult to overlook the incremental nature of the paper and therefore the paper cannot be accepted in its current form."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generalizing Graph Convolutional Networks via Heat Kernel", "authorids": ["~Jialin_Zhao1", "~Yuxiao_Dong1", "~Jie_Tang1", "dm18@mails.tsinghua.edu.cn", "~Kuansan_Wang1"], "authors": ["Jialin Zhao", "Yuxiao Dong", "Jie Tang", "Ming Ding", "Kuansan Wang"], "keywords": ["graph networks"], "abstract": "Graph convolutional networks (GCNs) have emerged as a powerful framework for mining and learning with graphs. A recent study shows that GCNs can be simplified as a linear model by removing nonlinearities and weight matrices across all consecutive layers, resulting the simple graph convolution (SGC) model. In this paper, we aim to understand GCNs and generalize SGC as a linear model via heat kernel (HKGCN), which acts as a low-pass filter on graphs and enables the aggregation of information from extremely large receptive fields. We theoretically show that HKGCN is in nature a continuous propagation model and GCNs without nonlinearities (i.e., SGC) are the discrete versions of it. Its low-pass filter and continuity properties facilitate the fast and smooth convergence of feature propagation. Experiments on million-scale networks show that the linear HKGCN model not only achieves consistently better results than SGC but also can match or even beat advanced GCN models, while maintaining SGC\u2019s superiority in efficiency.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhao|generalizing_graph_convolutional_networks_via_heat_kernel", "one-sentence_summary": "A continuous propagation model of GCNs with heat kernel.", "pdf": "/pdf/fcaaaad4d1105d321bc76f36d14137a1577cff52.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=So5grXZ6Hd", "_bibtex": "@misc{\nzhao2021generalizing,\ntitle={Generalizing Graph Convolutional Networks via Heat Kernel},\nauthor={Jialin Zhao and Yuxiao Dong and Jie Tang and Ming Ding and Kuansan Wang},\nyear={2021},\nurl={https://openreview.net/forum?id=yBJihVXahXc}\n}"}, "tags": [], "invitation": {"reply": {"forum": "yBJihVXahXc", "replyto": "yBJihVXahXc", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040460136, "tmdate": 1610474063043, "id": "ICLR.cc/2021/Conference/Paper3096/-/Decision"}}}, {"id": "rv5EwIWWhbo", "original": null, "number": 2, "cdate": 1603965542991, "ddate": null, "tcdate": 1603965542991, "tmdate": 1606797433622, "tddate": null, "forum": "yBJihVXahXc", "replyto": "yBJihVXahXc", "invitation": "ICLR.cc/2021/Conference/Paper3096/-/Official_Review", "content": {"title": "Simple Graph Convolution (SGC) + Heat Diffusion", "review": "This submission introduced a new graph convolutional operator based on heat diffusion, named heat kernel GCN (HKGCN). First, continuous-time heat diffusion on graphs is reviewed, where the solution is given by the heat equation (6). Then, the authors showed that classical GCN can be approximated in the same formulation through discretization.\n\nThe proposed method HKGCN is similar to the simplified GCN (SGC) by Wu et al. The learning procedure is quite straightforward: first, a heat diffusion is performed on the input features for a pre-specified time $t$; then, logic regression is performed on the diffused features to train the classifier. The SGC model performs several times of neighborhood averaging (as in vanilla GCN) based on a polynomial spectral filter of order one; while HKGCN performs heat diffusion with higher-order polynomial terms.\n\nMy main criticism is that applying heat diffusion on graph convolution is not new and the relationship with previous works is not clearly stated. See the cited (Xu et al 2019a) or not mentioned (Klicpera 2019) (Xu et al 2019c), where similar formulations and ideas already appeared. The main novelty of the proposed HKGCN, therefore, is on a combination of heat kernel and the SGC approach. This combination is not non-trivial enough and may not be significant enough to be published in ICLR.\n\nThe HKGCN method is motivated by the oscillation problem of GCN. How does the heat kernel help avoid oscillation? After the introduction of the heat equation, there should be some theoretical statements to provide a solution to this problem and to correspond to this motivation. Ideally, to show how HKGCN is different with the GCN approach. Instead, the oscillation problem is mainly solved by numerical simulation on the toy example and informal arguments.\n\nAs another novelty, the authors revealed that GCN can be approximated using heat diffusion under the same formulation. Again the connection although interesting is not a major contribution.\n\nEmpirically, the authors tested the HKGCN method on commonly used citation datasets and an OGB graph of arxiv articles, on both transitive and inductive learning tasks.\n\nThe huge speed improvement is mainly due to the same trick as SGC is used in HKGCN: there is no activation between the convolution layers which allows a pre-computation step followed by an extremely simplified learning step (logistic regression). This improvement is due to SGC and is expected.\n\nBy looking at the accuracy scores, the main comparison is HKGCN vs SGC because of their similarities. It is not convincing that using heat diffusion (HKGCN) instead of graph convolution (SGC) can bring a notable performance improvement. In most of the time, the improvement is quite marginal.\n\nOverall, this technical novelty and empirical significance are limited. There are not theoretical statements in this paper and I am evaluating it as an algorithmic contribution. I am recommending a weak rejection.\n\nMore comments:\n\nThe title is too broad. Please be more specific.\n\nToy example in the introduction: As this toy is mentioned again in later text, please explain in more detail the computation of GCN vs HKGCN.\n\nAs you started from GNN, it is good to cite some original GNN paper (Gori et al. 2005; Scarselli et al 09)\n\neq.(9) n_{iter} is hard to read\n\nin this template, most of the citations should use \\citep instead of \\cite\n\nReferences:\n\n(Klicpera 2019)\n\"Diffusion Improves Graph Learning\", Klicpera et al. 2019\n\n(Xu et al 2019c)\n\"graph wavelet neural network\" Xu et al. 2019\n\n---\nAfter rebuttal:\n\nThank you for the revision and the clarifications.\n\n\"no one has made a clear connection between GCN and the heat kernel.\" \nFor example, in (Klicpera 2019) section 2, the heat kernel is discussed as a special case.\n\n\"We don\u2019t simply combine SGC and heat kernel.\"\nClearly, the only difference between the proposed method in section 3.4 and SGC is that the authors used heat diffusion as the spectral filter matrix, while SGC used a polynomial filter (the K'th power of the normalized adjacency matrix).\n\nFurthermore, the other reviewers raised similar works such as graph ODE, which further reduces the novelty of this work.\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3096/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3096/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generalizing Graph Convolutional Networks via Heat Kernel", "authorids": ["~Jialin_Zhao1", "~Yuxiao_Dong1", "~Jie_Tang1", "dm18@mails.tsinghua.edu.cn", "~Kuansan_Wang1"], "authors": ["Jialin Zhao", "Yuxiao Dong", "Jie Tang", "Ming Ding", "Kuansan Wang"], "keywords": ["graph networks"], "abstract": "Graph convolutional networks (GCNs) have emerged as a powerful framework for mining and learning with graphs. A recent study shows that GCNs can be simplified as a linear model by removing nonlinearities and weight matrices across all consecutive layers, resulting the simple graph convolution (SGC) model. In this paper, we aim to understand GCNs and generalize SGC as a linear model via heat kernel (HKGCN), which acts as a low-pass filter on graphs and enables the aggregation of information from extremely large receptive fields. We theoretically show that HKGCN is in nature a continuous propagation model and GCNs without nonlinearities (i.e., SGC) are the discrete versions of it. Its low-pass filter and continuity properties facilitate the fast and smooth convergence of feature propagation. Experiments on million-scale networks show that the linear HKGCN model not only achieves consistently better results than SGC but also can match or even beat advanced GCN models, while maintaining SGC\u2019s superiority in efficiency.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhao|generalizing_graph_convolutional_networks_via_heat_kernel", "one-sentence_summary": "A continuous propagation model of GCNs with heat kernel.", "pdf": "/pdf/fcaaaad4d1105d321bc76f36d14137a1577cff52.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=So5grXZ6Hd", "_bibtex": "@misc{\nzhao2021generalizing,\ntitle={Generalizing Graph Convolutional Networks via Heat Kernel},\nauthor={Jialin Zhao and Yuxiao Dong and Jie Tang and Ming Ding and Kuansan Wang},\nyear={2021},\nurl={https://openreview.net/forum?id=yBJihVXahXc}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "yBJihVXahXc", "replyto": "yBJihVXahXc", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3096/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538082393, "tmdate": 1606915789455, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3096/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3096/-/Official_Review"}}}, {"id": "JZVlMesxb1p", "original": null, "number": 1, "cdate": 1603921958578, "ddate": null, "tcdate": 1603921958578, "tmdate": 1606091126884, "tddate": null, "forum": "yBJihVXahXc", "replyto": "yBJihVXahXc", "invitation": "ICLR.cc/2021/Conference/Paper3096/-/Official_Review", "content": {"title": "Heat-kernel GCN", "review": "This submission proposes to use heat kernel as the propagation matrix in graph convolutional networks. The authors show that heat kernels can induce more smooth propagation behavior than the commonly used discrete propagation. The submission also designs an efficient method to calculate the heat kernel (using Chebyshev expansion). \n\nStrength:\n- The proposed method is well motivated and heat kernel is (relatively) well-understood. \n- The writing is clear and easy-to-understand\n- The proposed method demonstrates improved performance on node classification datasets while inheriting the efficiency of simple graph convolution\n- The submission collects a larger scale arxiv graph dataset, which can be useful for the community. Will the authors release this dataset in the future?\n- This submission has done a detailed and thorough evaluation of the proposed method. I appreciate the effort to analyze $\\tilde{t}$, which should be helpful for practitioners.\n\nWeakness:\n- SGC is known to be ineffective for graph classification tasks. Does the proposed model also inherits this downside?\n- Table 3 lacks comparisons to more recent graph neural networks like Graph Markov Neural Networks. However, I don't think this is critical given the efficiency and strong performance of the proposed method on larger graph datasets. \n\n\n=== UPDATED ===\n\nGiven that the reviewers have not reached a consensus, I want to add more discussion to my review to facilitate the AC to make the decision. I would also include a few more quick TODOs for the authors and hope they can help add evidence to my argument.\n\n1. This submission proposes to replace the propagation in GNNs with heat kernel. The main motivation for this method is that the laplacian filters tend to oscillate, as illustrated by Figure 1. The heat-kernel provides a continuous convergence process and intuitively may address the oscillation process. Importantly, I believe the motivation of this method is *not* to prevent oversmoothing but to prevent over-oscillation. \n\nI believe this intuition is sound but encourage the authors to do more to validate this hypothesis. I appreciate the ablation study in Figure 6. As one more analysis, I suggest the authors to add the performance curves of SGC to Figure 6 (under the same setting). If this theoretical intuition is valid, we should expect HKGCN and SGC to behave more differently when the propagation degree is low (within 2-10). My understanding is that both HKGCN and SGC are efficient so this experiment shouldn't take long. Ideally the authors can update this result, at least on 1-2 datasets, within the discussion period.\n\n2. I am also very impressed by the efficiency of HKGCN. This submission has experimented with, to my knowledge, the largest publicly available graph dataset (arXiv), which contains more than one million nodes. According to the authors, HKGCN can be trained for this arXiv dataset in 48.5s, which is impressive. \n\nNote that here the heat diffusion matrix does not need to be computed/stored explicitly. Following the setup in SGC, HKGCN only needs to compute the propagated features in a preprocessing step.  \n\nI suggest the authors to give more concrete numbers to illustrate the efficiency of this method. For this arXiv dataset, what kind of hardware is required? How much actual RAM did you use to compute the preprocessing step?\n\n3. After reading other reviews, I now realize that this submission is not the first to introduce heat kernels into GCNs. Among the papers pointed out by other reviewers, I find [1] to be most relevant in that they also proposed the usage of heat kernels. Can the authors also clarify the difference between this submission and [2]?\n\nBased on this novelty concern, I have lowered my review score to 6. \n\n[1] Xu et al, graph wavelet neural network, ICLR 2019\n[2] Xu et al, Graph Convolutional Networks using Heat Kernel for Semi-supervised Learning, IJCAI 2019. ", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper3096/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3096/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generalizing Graph Convolutional Networks via Heat Kernel", "authorids": ["~Jialin_Zhao1", "~Yuxiao_Dong1", "~Jie_Tang1", "dm18@mails.tsinghua.edu.cn", "~Kuansan_Wang1"], "authors": ["Jialin Zhao", "Yuxiao Dong", "Jie Tang", "Ming Ding", "Kuansan Wang"], "keywords": ["graph networks"], "abstract": "Graph convolutional networks (GCNs) have emerged as a powerful framework for mining and learning with graphs. A recent study shows that GCNs can be simplified as a linear model by removing nonlinearities and weight matrices across all consecutive layers, resulting the simple graph convolution (SGC) model. In this paper, we aim to understand GCNs and generalize SGC as a linear model via heat kernel (HKGCN), which acts as a low-pass filter on graphs and enables the aggregation of information from extremely large receptive fields. We theoretically show that HKGCN is in nature a continuous propagation model and GCNs without nonlinearities (i.e., SGC) are the discrete versions of it. Its low-pass filter and continuity properties facilitate the fast and smooth convergence of feature propagation. Experiments on million-scale networks show that the linear HKGCN model not only achieves consistently better results than SGC but also can match or even beat advanced GCN models, while maintaining SGC\u2019s superiority in efficiency.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhao|generalizing_graph_convolutional_networks_via_heat_kernel", "one-sentence_summary": "A continuous propagation model of GCNs with heat kernel.", "pdf": "/pdf/fcaaaad4d1105d321bc76f36d14137a1577cff52.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=So5grXZ6Hd", "_bibtex": "@misc{\nzhao2021generalizing,\ntitle={Generalizing Graph Convolutional Networks via Heat Kernel},\nauthor={Jialin Zhao and Yuxiao Dong and Jie Tang and Ming Ding and Kuansan Wang},\nyear={2021},\nurl={https://openreview.net/forum?id=yBJihVXahXc}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "yBJihVXahXc", "replyto": "yBJihVXahXc", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3096/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538082393, "tmdate": 1606915789455, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3096/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3096/-/Official_Review"}}}, {"id": "NmZfjTIVr0", "original": null, "number": 9, "cdate": 1605914873160, "ddate": null, "tcdate": 1605914873160, "tmdate": 1605914873160, "tddate": null, "forum": "yBJihVXahXc", "replyto": "JZVlMesxb1p", "invitation": "ICLR.cc/2021/Conference/Paper3096/-/Official_Comment", "content": {"title": "Response to AnonReviewer4", "comment": "Thanks for your positive feedback! \n\n\n**Reviewer Comment: SGC is known to be ineffective for graph classification tasks. Does the proposed model also inherits this downside?**\n\nOur Response: For graph classification tasks, we usually need a graph readout layer to exact features from the whole graph. The ability to distinguish graphs mainly depends on this readout layer, instead of graph convolution. In this paper, we mainly target at node classification, under the setting of mean aggregator in GCN and SGC. Readout layer could be integrated to solve this problem.\n\n\n**Reviewer Comment: Table 3 lacks comparisons to more recent graph neural networks like Graph Markov Neural Networks.**\n\nOur Response: Thank you for giving this suggestion. More comparisons could be helpful."}, "signatures": ["ICLR.cc/2021/Conference/Paper3096/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3096/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generalizing Graph Convolutional Networks via Heat Kernel", "authorids": ["~Jialin_Zhao1", "~Yuxiao_Dong1", "~Jie_Tang1", "dm18@mails.tsinghua.edu.cn", "~Kuansan_Wang1"], "authors": ["Jialin Zhao", "Yuxiao Dong", "Jie Tang", "Ming Ding", "Kuansan Wang"], "keywords": ["graph networks"], "abstract": "Graph convolutional networks (GCNs) have emerged as a powerful framework for mining and learning with graphs. A recent study shows that GCNs can be simplified as a linear model by removing nonlinearities and weight matrices across all consecutive layers, resulting the simple graph convolution (SGC) model. In this paper, we aim to understand GCNs and generalize SGC as a linear model via heat kernel (HKGCN), which acts as a low-pass filter on graphs and enables the aggregation of information from extremely large receptive fields. We theoretically show that HKGCN is in nature a continuous propagation model and GCNs without nonlinearities (i.e., SGC) are the discrete versions of it. Its low-pass filter and continuity properties facilitate the fast and smooth convergence of feature propagation. Experiments on million-scale networks show that the linear HKGCN model not only achieves consistently better results than SGC but also can match or even beat advanced GCN models, while maintaining SGC\u2019s superiority in efficiency.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhao|generalizing_graph_convolutional_networks_via_heat_kernel", "one-sentence_summary": "A continuous propagation model of GCNs with heat kernel.", "pdf": "/pdf/fcaaaad4d1105d321bc76f36d14137a1577cff52.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=So5grXZ6Hd", "_bibtex": "@misc{\nzhao2021generalizing,\ntitle={Generalizing Graph Convolutional Networks via Heat Kernel},\nauthor={Jialin Zhao and Yuxiao Dong and Jie Tang and Ming Ding and Kuansan Wang},\nyear={2021},\nurl={https://openreview.net/forum?id=yBJihVXahXc}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "yBJihVXahXc", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3096/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3096/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3096/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3096/Authors|ICLR.cc/2021/Conference/Paper3096/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3096/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923841176, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3096/-/Official_Comment"}}}, {"id": "m9KA9adzip1", "original": null, "number": 8, "cdate": 1605914731739, "ddate": null, "tcdate": 1605914731739, "tmdate": 1605914731739, "tddate": null, "forum": "yBJihVXahXc", "replyto": "rv5EwIWWhbo", "invitation": "ICLR.cc/2021/Conference/Paper3096/-/Official_Comment", "content": {"title": "Response to AnonReviewer3", "comment": "Thanks for providing very valuable feedback regarding our submission and indicating specific points of uncertainty. Please see our response below:\n\n\n**Reviewer Comment: The relationship with previous works is not clearly stated.**\n\nOur Response: The previous works did not make a clear connection between GCN and the heat kernel, which is what HKGCN does. We will discuss the listed papers in details as follows.\n\n(Xu et al 2019c) proposed using graph wavelet transform as bases. It isn\u2019t quite relevant to our model which is focused on improving graph feature propagation. As discussed in the paper, (Xu et al 2019a) first introduced heat kernel into GCNs. (Klicpera 2019) generalized graph diffusion and used sparsification to decrease graph density. But we would argue that no one has made a clear connection between GCN and the heat kernel. We don\u2019t simply combine SGC and heat kernel. We use SGC as baseline because SGC removed nonlinear activation function, so it is a great reference model to show the ability of the heat kernel to generalize multi-layer GCN.\n\n\nThere are three main aspects which haven't been raised by previous works. First, we introduced an appropriate inductive bias that the propagation of features in graphs also follows Newton\u2019s law of cooling, which means the propagation rate on each edge is proportional to the difference between the features of nodes it connects. Second, we theoretically proved that the propagation matrix in GCN or SGC can be seen as a discrete version of the heat kernel. Third, although (Xu et al 2019a) (Klicpera 2019) also proposed that heat kernel is a low-pass filter. We go further to identify the reason why low-pass filter prevents oscillation, compared with band-stop filter(GCN).\n\n\n**Reviewer Comment: How does the heat kernel help avoid oscillation?**\n\nOur Response: The reason for how the heat kernel helps avoid oscillations can be summarized into two aspects, vertex domain and spectral domain.\n\nFirst, in vertex domain, heat kernel can be seen as GCN with step size of infinitely small. This helps avoid overshooting the convergence point, which is caused by too large step size. Just like we don\u2019t want to use a large learning rate in deep learning.\n\n\nSecond, in our spectral analysis section, we prove that high frequency spectrum will cause globally oscillating, which will lead to oscillations in features. So the low-pass filter (heat kernel) performs better in avoiding oscillations than the band-stop filter(GCN). \n\n\n**Reviewer Comment: The title is too broad. Please be more specific.**\n\nOur Response: We will change it to \u201cGeneralizing Graph Convolutional Networks via Heat Kernel\u201d in our updated version.\n\n\n**Reviewer Comment: Toy example in the introduction: As this toy is mentioned again in later text, please explain in more detail the computation of GCN vs HKGCN.**\n\nOur Response: The reason why we use this toy example is that we want to give an intuitive feeling about the ability of HKGCN to prevent oscillation. We also theoretically proved that in bothe vertex and spectral domain.\n\n\n**Reviewer Comment: As you started from GNN, it is good to cite some original GNN paper (Gori et al. 2005; Scarselli et al 09)**\n\nOur Response: Thank you. We will discuss them in our updated version.\n\n\n**Reviewer Comment: eq.(9) n_{iter} is hard to read. in this template, most of the citations should use \\citep instead of \\cite**\n\nOur Response: Thank you. We have fixed that in our updated version of paper."}, "signatures": ["ICLR.cc/2021/Conference/Paper3096/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3096/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generalizing Graph Convolutional Networks via Heat Kernel", "authorids": ["~Jialin_Zhao1", "~Yuxiao_Dong1", "~Jie_Tang1", "dm18@mails.tsinghua.edu.cn", "~Kuansan_Wang1"], "authors": ["Jialin Zhao", "Yuxiao Dong", "Jie Tang", "Ming Ding", "Kuansan Wang"], "keywords": ["graph networks"], "abstract": "Graph convolutional networks (GCNs) have emerged as a powerful framework for mining and learning with graphs. A recent study shows that GCNs can be simplified as a linear model by removing nonlinearities and weight matrices across all consecutive layers, resulting the simple graph convolution (SGC) model. In this paper, we aim to understand GCNs and generalize SGC as a linear model via heat kernel (HKGCN), which acts as a low-pass filter on graphs and enables the aggregation of information from extremely large receptive fields. We theoretically show that HKGCN is in nature a continuous propagation model and GCNs without nonlinearities (i.e., SGC) are the discrete versions of it. Its low-pass filter and continuity properties facilitate the fast and smooth convergence of feature propagation. Experiments on million-scale networks show that the linear HKGCN model not only achieves consistently better results than SGC but also can match or even beat advanced GCN models, while maintaining SGC\u2019s superiority in efficiency.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhao|generalizing_graph_convolutional_networks_via_heat_kernel", "one-sentence_summary": "A continuous propagation model of GCNs with heat kernel.", "pdf": "/pdf/fcaaaad4d1105d321bc76f36d14137a1577cff52.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=So5grXZ6Hd", "_bibtex": "@misc{\nzhao2021generalizing,\ntitle={Generalizing Graph Convolutional Networks via Heat Kernel},\nauthor={Jialin Zhao and Yuxiao Dong and Jie Tang and Ming Ding and Kuansan Wang},\nyear={2021},\nurl={https://openreview.net/forum?id=yBJihVXahXc}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "yBJihVXahXc", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3096/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3096/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3096/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3096/Authors|ICLR.cc/2021/Conference/Paper3096/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3096/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923841176, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3096/-/Official_Comment"}}}, {"id": "kf94dAj68Ob", "original": null, "number": 7, "cdate": 1605914418605, "ddate": null, "tcdate": 1605914418605, "tmdate": 1605914439639, "tddate": null, "forum": "yBJihVXahXc", "replyto": "w6uTpMBlGRv", "invitation": "ICLR.cc/2021/Conference/Paper3096/-/Official_Comment", "content": {"title": "Response to AnonReviewer2 Part2", "comment": "**Reviewer Comment: In Section 3.6, the authors mention that the time complexity of data processing is O(k|E|). What is k here?**\n\nOur Response: k is the Chebyshev expansion step in Equation 13. So as the Chebyshev expansion step increases, the time complexity will also increase linearly. Thanks for this advice. We emphasized this in our updated version of paper.\n\n[1] Poli, Michael, et al. \"Graph neural ordinary differential equations.\" arXiv preprint arXiv:1911.07532 (2019). [2] Deng, Zhiwei, et al. \"Continuous graph flow for flexible density estimation.\" arXiv preprint arXiv:1908.02436 (2019). [3] Zhuang, Juntang, et al. \"Ordinary differential equations on graph networks.\" (2019). [4] Xhonneux, Louis-Pascal AC, Meng Qu, and Jian Tang. \"Continuous Graph Neural Networks.\" arXiv preprint arXiv:1912.00967 (2019)."}, "signatures": ["ICLR.cc/2021/Conference/Paper3096/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3096/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generalizing Graph Convolutional Networks via Heat Kernel", "authorids": ["~Jialin_Zhao1", "~Yuxiao_Dong1", "~Jie_Tang1", "dm18@mails.tsinghua.edu.cn", "~Kuansan_Wang1"], "authors": ["Jialin Zhao", "Yuxiao Dong", "Jie Tang", "Ming Ding", "Kuansan Wang"], "keywords": ["graph networks"], "abstract": "Graph convolutional networks (GCNs) have emerged as a powerful framework for mining and learning with graphs. A recent study shows that GCNs can be simplified as a linear model by removing nonlinearities and weight matrices across all consecutive layers, resulting the simple graph convolution (SGC) model. In this paper, we aim to understand GCNs and generalize SGC as a linear model via heat kernel (HKGCN), which acts as a low-pass filter on graphs and enables the aggregation of information from extremely large receptive fields. We theoretically show that HKGCN is in nature a continuous propagation model and GCNs without nonlinearities (i.e., SGC) are the discrete versions of it. Its low-pass filter and continuity properties facilitate the fast and smooth convergence of feature propagation. Experiments on million-scale networks show that the linear HKGCN model not only achieves consistently better results than SGC but also can match or even beat advanced GCN models, while maintaining SGC\u2019s superiority in efficiency.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhao|generalizing_graph_convolutional_networks_via_heat_kernel", "one-sentence_summary": "A continuous propagation model of GCNs with heat kernel.", "pdf": "/pdf/fcaaaad4d1105d321bc76f36d14137a1577cff52.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=So5grXZ6Hd", "_bibtex": "@misc{\nzhao2021generalizing,\ntitle={Generalizing Graph Convolutional Networks via Heat Kernel},\nauthor={Jialin Zhao and Yuxiao Dong and Jie Tang and Ming Ding and Kuansan Wang},\nyear={2021},\nurl={https://openreview.net/forum?id=yBJihVXahXc}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "yBJihVXahXc", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3096/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3096/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3096/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3096/Authors|ICLR.cc/2021/Conference/Paper3096/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3096/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923841176, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3096/-/Official_Comment"}}}, {"id": "w6uTpMBlGRv", "original": null, "number": 5, "cdate": 1605914242277, "ddate": null, "tcdate": 1605914242277, "tmdate": 1605914431343, "tddate": null, "forum": "yBJihVXahXc", "replyto": "RBLU7p8zpR", "invitation": "ICLR.cc/2021/Conference/Paper3096/-/Official_Comment", "content": {"title": "Response to AnonReviewer2 Part1", "comment": "Thank you for your detailed and insightful comments! Please see our response below.\n\n**Reviewer Comment: Not novel, what is the differences between this work and [1,2,3,4]?**\n\nOur Response: [1,2,3] are different from HKGCN in both motivation and implementation. The formulas in [4] looks similar but different in meaning. We will discuss in details as follows:\n\n[2] use continuous flow on a generative model to solve generation tasks, but HKGCN aims to classification. [1, 3] use the Neural ODE framework and parametrize the derivative function using a 2 or 3 layer GNN directly. [4] improved ODE by developing a continuous message-passing layer.\n \nFrom a small beginning we can see how things will develop. We could find all their layers(nodes) have a self-loop. For example, in [1] equation (2), they turned GCN into the residual version, same as equation (3) in [3]. In equation (4) of [4], each node also needs to remember its original node features $H_0$. They do so because they couldn\u2019t make their models converge to a stable fixed point without adding a residual connection. This also shows why $t$ in [4] has quite a different meaning from ours $t$. They make their $t \\to \\infty$ to enable their networks to have an infinite number of \u2018layers\u2019. But our $t$ is a balance between local feature and global feature, so it shouldn\u2019t be too large nor too small. We didn\u2019t want to make our features converge to a stable point. We just want to use $t$ to control the propagation rate(time) of the nodes. \n\n\nA reviewer of [3] said it would be better if the authors could build a more detailed relationship between the neural ODEs framework and underlying GNN. He was also curious about how the topological information of graphs affects the graph ODEs via spectral-type graph convolution operations and what is the relationship to the oversmoothing phenomena. We think this is exactly what we do. We showed the relationship between heat kernel and gcn. We also analyzed from the spectral perspective and gave reasons why the low-pass filter will prevent oscillation. \n\nWe will add a more detailed discussion in the camera-ready version.\n\n**Reviewer Comment: The results are worse than SOTA methods.**\n\nOur Response: Our main contribution is to generalize GCNs into a continuous and linear propagation model with theoretical analyses. We are not focusing on getting the highest SOTA performance. We improve the propagation matrix in feature message passing. We fixed everything else in SGC, except replacing the feature propagation matrix with the heat kernel. Our theoretical analyses part is also focused on what makes the heat kernel a better feature propagation matrix. Our model hyperparameters are listed in the appendix. Please feel free to ask if you have any questions about model detail.\n\n\n**Reviewer Comment: Why large t won\u2019t cause low accuracy and how does the model avoid over-smoothing in practice?**\n\nOur Response: Thank you for pointing out this very interesting phenomenon. I think the reason behind this is still the continuous model. $H_t$ did tend to each element equals to $1/n$ as $t$ grows. And this makes the variation of nodes feature $var(X^{(t)})$ tends to zero. But because of the continuous model, the relationship between features of nodes are preserved. It\u2019s like we shift all nodes\u2019 features with the same rate toward the average feature of the whole graph as $t$ grows. This makes our linear logistic regression classifier still have the capability to distinguish different nodes. Our toy example in Figure 1 provides a more direct feeling, in which the node with larger features is always the same node in our model and GCN on the contrary.\n\n\n**Reviewer Comment: How to deal with computation cost when feature dimension is high?**\n\nOur Response: The size of matrix $H_t$ is $n*n$, n is the number of nodes in the graph. The way to calculate $H_t * X$ is by using Chebyshev expansion to convert $H_t$ into a polynomial of $A$, which is the adjacency matrix. Because $AX$ is a sparse matrix multiplication, the time complexity is proportional to the dimension of input feature, same as GCN. So computing $H_t X$ is proportional to the dimension of input feature, which won\u2019t cost a lot of time. Thanks for pointing out we missed calculating the dimension of input features in time complexity of the preprocessing step. We have changed it in our updated version of paper.\n\n\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3096/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3096/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generalizing Graph Convolutional Networks via Heat Kernel", "authorids": ["~Jialin_Zhao1", "~Yuxiao_Dong1", "~Jie_Tang1", "dm18@mails.tsinghua.edu.cn", "~Kuansan_Wang1"], "authors": ["Jialin Zhao", "Yuxiao Dong", "Jie Tang", "Ming Ding", "Kuansan Wang"], "keywords": ["graph networks"], "abstract": "Graph convolutional networks (GCNs) have emerged as a powerful framework for mining and learning with graphs. A recent study shows that GCNs can be simplified as a linear model by removing nonlinearities and weight matrices across all consecutive layers, resulting the simple graph convolution (SGC) model. In this paper, we aim to understand GCNs and generalize SGC as a linear model via heat kernel (HKGCN), which acts as a low-pass filter on graphs and enables the aggregation of information from extremely large receptive fields. We theoretically show that HKGCN is in nature a continuous propagation model and GCNs without nonlinearities (i.e., SGC) are the discrete versions of it. Its low-pass filter and continuity properties facilitate the fast and smooth convergence of feature propagation. Experiments on million-scale networks show that the linear HKGCN model not only achieves consistently better results than SGC but also can match or even beat advanced GCN models, while maintaining SGC\u2019s superiority in efficiency.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhao|generalizing_graph_convolutional_networks_via_heat_kernel", "one-sentence_summary": "A continuous propagation model of GCNs with heat kernel.", "pdf": "/pdf/fcaaaad4d1105d321bc76f36d14137a1577cff52.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=So5grXZ6Hd", "_bibtex": "@misc{\nzhao2021generalizing,\ntitle={Generalizing Graph Convolutional Networks via Heat Kernel},\nauthor={Jialin Zhao and Yuxiao Dong and Jie Tang and Ming Ding and Kuansan Wang},\nyear={2021},\nurl={https://openreview.net/forum?id=yBJihVXahXc}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "yBJihVXahXc", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3096/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3096/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3096/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3096/Authors|ICLR.cc/2021/Conference/Paper3096/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3096/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923841176, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3096/-/Official_Comment"}}}, {"id": "tE9fYEufWSi", "original": null, "number": 4, "cdate": 1605913878657, "ddate": null, "tcdate": 1605913878657, "tmdate": 1605913955305, "tddate": null, "forum": "yBJihVXahXc", "replyto": "rJoVdZ4Y4xn", "invitation": "ICLR.cc/2021/Conference/Paper3096/-/Official_Comment", "content": {"title": "Response to AnonReviewer1", "comment": "Thank you for the detailed comments. We provide detailed answers to each question as follows:\n\n\n**Reviewer Comment: To what extent GCN based on heat kernel formulation is able to mitigate over-smoothness?**\n\nOur Response: The experiment in Figure 3 showed our model is able to mitigate over-smoothness. Our features are still distinguishable and the model performs well even when \\Tilde{t} = 30 (\\Tilde{t}=30 is equivalent to a 15-layers GCN).\nIn [1], they said that the reason which causes over smoothing is that \u201crepeated applying Laplacian smoothing may mix the features of vertices from different clusters and make them indistinguishable\u201d. However, we improved Laplacian from discrete multiplications to continuous exponential. It\u2019s like we shift all nodes\u2019 features with the same rate toward the average feature of the whole graph as $t$ grows. This makes our linear logistic regression classifier still have the capability to distinguish different nodes. Our example in Figure 1 provides a more direct feeling, in which the node with larger features is always the same node in our model and GCN on the contrary. This shows how our model prevents oscillation and makes our features still distinguishable as \\Tilde{t} increases.\n\n[1] Li, Qimai, Zhichao Han, and Xiao-Ming Wu. \"Deeper insights into graph convolutional networks for semi-supervised learning.\" arXiv preprint arXiv:1801.07606 (2018).\n\n\n**Reviewer Comment: Is the proposed heat kernel function injective so that it is able to distinguish two non-isomorphic graphs?**\n\nOur Response: To distinguish two non-isomorphic graphs, or more generally the \"graph classification\" tasks, we usually need a graph readout layer to exact features from the whole graph. The ability to distinguish graphs mainly depends on this readout layer, instead of graph convolution. In this paper, we mainly target at node classification, under the setting of mean aggregator in GCN and SGC. Standard techniques for graph isomorphism, e.g. WL test, can be integrated into our method during the readout period to solve this task.\n\n\n**Reviewer Comment: It can be applicable only on graphs with fixed topology and size as it is the case for all spectral GCN (filters are not transferable across graphs since they are basis dependent). However, in real world problems graphs are irregular.**\n\nOur Response: The inductive results in Table 4 show that HKGCN generalize its parameter from a smaller subgraph which only contains training nodes to the whole graph, exhibiting its flexibility under **subgraph-level inductive learning**.\nHowever, just as you say, the graph-level inductive learning is a problem that nearly all GNNs are faced, if your \"irregular\" means that graphs will change greatly over the time. "}, "signatures": ["ICLR.cc/2021/Conference/Paper3096/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3096/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generalizing Graph Convolutional Networks via Heat Kernel", "authorids": ["~Jialin_Zhao1", "~Yuxiao_Dong1", "~Jie_Tang1", "dm18@mails.tsinghua.edu.cn", "~Kuansan_Wang1"], "authors": ["Jialin Zhao", "Yuxiao Dong", "Jie Tang", "Ming Ding", "Kuansan Wang"], "keywords": ["graph networks"], "abstract": "Graph convolutional networks (GCNs) have emerged as a powerful framework for mining and learning with graphs. A recent study shows that GCNs can be simplified as a linear model by removing nonlinearities and weight matrices across all consecutive layers, resulting the simple graph convolution (SGC) model. In this paper, we aim to understand GCNs and generalize SGC as a linear model via heat kernel (HKGCN), which acts as a low-pass filter on graphs and enables the aggregation of information from extremely large receptive fields. We theoretically show that HKGCN is in nature a continuous propagation model and GCNs without nonlinearities (i.e., SGC) are the discrete versions of it. Its low-pass filter and continuity properties facilitate the fast and smooth convergence of feature propagation. Experiments on million-scale networks show that the linear HKGCN model not only achieves consistently better results than SGC but also can match or even beat advanced GCN models, while maintaining SGC\u2019s superiority in efficiency.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhao|generalizing_graph_convolutional_networks_via_heat_kernel", "one-sentence_summary": "A continuous propagation model of GCNs with heat kernel.", "pdf": "/pdf/fcaaaad4d1105d321bc76f36d14137a1577cff52.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=So5grXZ6Hd", "_bibtex": "@misc{\nzhao2021generalizing,\ntitle={Generalizing Graph Convolutional Networks via Heat Kernel},\nauthor={Jialin Zhao and Yuxiao Dong and Jie Tang and Ming Ding and Kuansan Wang},\nyear={2021},\nurl={https://openreview.net/forum?id=yBJihVXahXc}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "yBJihVXahXc", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3096/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3096/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3096/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3096/Authors|ICLR.cc/2021/Conference/Paper3096/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3096/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923841176, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3096/-/Official_Comment"}}}, {"id": "RBLU7p8zpR", "original": null, "number": 3, "cdate": 1604190474571, "ddate": null, "tcdate": 1604190474571, "tmdate": 1605024069125, "tddate": null, "forum": "yBJihVXahXc", "replyto": "yBJihVXahXc", "invitation": "ICLR.cc/2021/Conference/Paper3096/-/Official_Review", "content": {"title": "Interesting work, but novelty is limited, and very relevant references are not cited and compared", "review": "This paper studies semi-supervised node classification in graph data. One powerful approach to the task is graph convolutional networks, which use discrete layers to perform information propagation. The paper generalizes GCNs into a continuous model via heat kernel, where the proposed model uses continuous layers for information propagation. The authors conduct both theoretical and empirical analysis of the proposed model. Experiments on several standard datasets show promising results. Overall, the paper studies an important problem in graph machine learning, and proposes a principled approach, which combines graph neural networks with heat kernels, and gives a new way of analyzing existing graph neural networks.\nHowever, the paper also has several weaknesses:\n1. The novelty is limited.\nAlthough the idea of developing continuous propagation layers is interesting, the idea has been explored by many recent works. For example, [1,2,3] use a graph neural network to define an ODE, which leads to a continuous feature propagation layer for node classification. [4] uses a linear ODE for feature propagation, which is very similar to the method proposed here, and the only difference is that the ODE in [4] also incorporates some constant term besides -LX. The authors should explain and clarify the difference between this work and existing works.\n\n2. The results are worse than SOTA methods.\nIn experiments, the authors conduct experiments in many standard datasets (e.g., Cora, Citeseer, Pubmed), and the proposed method shows promising results. However, the compared methods used in experiments are not competitive enough. The strongest baseline methods in Table 3 are GCN and GAT, and in Table 4 they are GraphSage and GCN. All these methods are proposed before 2017, and recently there are many more competitive methods proposed. To make the results more convincing, it is helpful to compare against some recent graph neural networks for node classification.\nBesides, I also have some questions regarding the model detail:\n1. About t in equation (6).\nIn Equation (6), the analytical form of X^{(t)} is given by X^{(t)}=e^{-Lt}X, where t can have a high impact on the results. If t is very small, then e^{-Lt} becomes an identity matrix, and hence H_t will be very close to X. If t is very large, then e^{-Lt} becomes an matrix whose elements are all close to 0, and thus all the rows in H_t will be almost the same, yielding an over-smoothing problem. In practice, what would be a proper value of t? Moreover, if we look at Figure (6), even when t is very large (e.g., t>20), the accuracy is still very high especially on Cora, which indicates that the model does not suffer from over-smoothing in practice. But if we check the analytical form X^{(t)}=e^{-Lt}X, when t is large, all the rows in H_t become very similar, which may lead to over-smoothing and a low accuracy. I wonder how does the proposed model manage to avoid over-smoothing in practice? Could the authors elaborate on that?\n2. About feature dimensionality.\nIn the propose method, the hidden matrix H_t has the same size as the feature matrix X. If the feature dimensionality of a dataset is very high, which is quite common in practice, then computing H^{(t)} can entail high cost. Is there a way to deal with the potential problem?\n3. About the time complexity.\nIn Section 3.6, the authors mention that the time complexity of data processing is O(k|E|). What is k here?\nReferences:\n[1] Poli, Michael, et al. \"Graph neural ordinary differential equations.\" arXiv preprint arXiv:1911.07532 (2019).\n[2] Deng, Zhiwei, et al. \"Continuous graph flow for flexible density estimation.\" arXiv preprint arXiv:1908.02436 (2019).\n[3] Zhuang, Juntang, et al. \"Ordinary differential equations on graph networks.\" (2019).\n[4] Xhonneux, Louis-Pascal AC, Meng Qu, and Jian Tang. \"Continuous Graph Neural Networks.\" arXiv preprint arXiv:1912.00967 (2019).", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper3096/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3096/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generalizing Graph Convolutional Networks via Heat Kernel", "authorids": ["~Jialin_Zhao1", "~Yuxiao_Dong1", "~Jie_Tang1", "dm18@mails.tsinghua.edu.cn", "~Kuansan_Wang1"], "authors": ["Jialin Zhao", "Yuxiao Dong", "Jie Tang", "Ming Ding", "Kuansan Wang"], "keywords": ["graph networks"], "abstract": "Graph convolutional networks (GCNs) have emerged as a powerful framework for mining and learning with graphs. A recent study shows that GCNs can be simplified as a linear model by removing nonlinearities and weight matrices across all consecutive layers, resulting the simple graph convolution (SGC) model. In this paper, we aim to understand GCNs and generalize SGC as a linear model via heat kernel (HKGCN), which acts as a low-pass filter on graphs and enables the aggregation of information from extremely large receptive fields. We theoretically show that HKGCN is in nature a continuous propagation model and GCNs without nonlinearities (i.e., SGC) are the discrete versions of it. Its low-pass filter and continuity properties facilitate the fast and smooth convergence of feature propagation. Experiments on million-scale networks show that the linear HKGCN model not only achieves consistently better results than SGC but also can match or even beat advanced GCN models, while maintaining SGC\u2019s superiority in efficiency.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhao|generalizing_graph_convolutional_networks_via_heat_kernel", "one-sentence_summary": "A continuous propagation model of GCNs with heat kernel.", "pdf": "/pdf/fcaaaad4d1105d321bc76f36d14137a1577cff52.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=So5grXZ6Hd", "_bibtex": "@misc{\nzhao2021generalizing,\ntitle={Generalizing Graph Convolutional Networks via Heat Kernel},\nauthor={Jialin Zhao and Yuxiao Dong and Jie Tang and Ming Ding and Kuansan Wang},\nyear={2021},\nurl={https://openreview.net/forum?id=yBJihVXahXc}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "yBJihVXahXc", "replyto": "yBJihVXahXc", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3096/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538082393, "tmdate": 1606915789455, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3096/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3096/-/Official_Review"}}}, {"id": "rJoVdZ4Y4xn", "original": null, "number": 4, "cdate": 1604247533532, "ddate": null, "tcdate": 1604247533532, "tmdate": 1605024069057, "tddate": null, "forum": "yBJihVXahXc", "replyto": "yBJihVXahXc", "invitation": "ICLR.cc/2021/Conference/Paper3096/-/Official_Review", "content": {"title": "Establishing link between heat kernels and linear GCN for more expressive graph representations", "review": "The authors  shed light on  linear GCNs models and propose  a new design which aim at generalizing  GCNs to continuous and  and a linear propagation model inspired by Newton's law. \nIt is based on the hypothesis that features propagation across nodes in a given graph follows the same process.\n To do so, the authors establish a link with heat kernels and formulate the problem as heat kernel learning within linear GCN model so that the network at the feature propagation step takes into consideration  multi-hop neighboring systems to refine the features of a given central node. \n\nIn this paper, the authors find out that features propagation based on heat kernel allows to control the oscillation between low and high frequencies. Controlling the appropriate level of granularities is quite a challenging task in deep learning mainly, as the convolutional filters are biased toward low frequencies. \nHowever,  important invariants and informative information for classification are within the chaos of high frequencies. In order to control that, the authors explain that  GCN based heat kernels can act as a low-pass filter cutoff. This combination of GCN and heat kernels are empirically validated considering node and graph classification tasks. The settings are clear and the comparison with related works is convincing.\n\nOne of the strong points of the paper is its capacity to provide comparable results state-of-the-art  with a reasonable complexity in space, with the advantage of being more simple and interpretable compared to existing (related) methods. Moreover a theoretical analysis from a spectral standpoint is introduced clearly.  It consists at setting link between Linear GCN and heat kernels, as well as with finite difference methods.\nHowever, it\u2019s not clear how the proposed GCN tackles the problem of over-smoothness and graph isomorphism. They are among the most challenging problems in graph learning. From that, l derive two questions :\n\n1- To what extent GCN based on heat kernel formulation is able to mitigate over-smoothness ?\n\n2- Is the proposed heat kernel function injective so that it is able to distinguish two non-isomorphic graphs ?\n\nOne possible weakness of the proposed design, is that it can be applicable only on graphs with fixed topology and size as it is the case for all spectral GCN (filters are not transferable across graphs since they are basis dependent). However, in real world problems  graphs are irregular.\n\nThe overall approach is  original, well placed in the litterature and the paper is well written. The authors conduct both theoretical and practical studies to show that this research direction could be important to improve existing GCN models. For that reason, l propose to accept the paper.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3096/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3096/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generalizing Graph Convolutional Networks via Heat Kernel", "authorids": ["~Jialin_Zhao1", "~Yuxiao_Dong1", "~Jie_Tang1", "dm18@mails.tsinghua.edu.cn", "~Kuansan_Wang1"], "authors": ["Jialin Zhao", "Yuxiao Dong", "Jie Tang", "Ming Ding", "Kuansan Wang"], "keywords": ["graph networks"], "abstract": "Graph convolutional networks (GCNs) have emerged as a powerful framework for mining and learning with graphs. A recent study shows that GCNs can be simplified as a linear model by removing nonlinearities and weight matrices across all consecutive layers, resulting the simple graph convolution (SGC) model. In this paper, we aim to understand GCNs and generalize SGC as a linear model via heat kernel (HKGCN), which acts as a low-pass filter on graphs and enables the aggregation of information from extremely large receptive fields. We theoretically show that HKGCN is in nature a continuous propagation model and GCNs without nonlinearities (i.e., SGC) are the discrete versions of it. Its low-pass filter and continuity properties facilitate the fast and smooth convergence of feature propagation. Experiments on million-scale networks show that the linear HKGCN model not only achieves consistently better results than SGC but also can match or even beat advanced GCN models, while maintaining SGC\u2019s superiority in efficiency.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhao|generalizing_graph_convolutional_networks_via_heat_kernel", "one-sentence_summary": "A continuous propagation model of GCNs with heat kernel.", "pdf": "/pdf/fcaaaad4d1105d321bc76f36d14137a1577cff52.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=So5grXZ6Hd", "_bibtex": "@misc{\nzhao2021generalizing,\ntitle={Generalizing Graph Convolutional Networks via Heat Kernel},\nauthor={Jialin Zhao and Yuxiao Dong and Jie Tang and Ming Ding and Kuansan Wang},\nyear={2021},\nurl={https://openreview.net/forum?id=yBJihVXahXc}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "yBJihVXahXc", "replyto": "yBJihVXahXc", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3096/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538082393, "tmdate": 1606915789455, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3096/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3096/-/Official_Review"}}}], "count": 11}