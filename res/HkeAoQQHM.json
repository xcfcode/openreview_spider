{"notes": [{"tddate": null, "ddate": null, "tmdate": 1521618344954, "tcdate": 1521618344954, "number": 1, "cdate": 1521618344954, "id": "SkWn6F15z", "invitation": "ICLR.cc/2018/Workshop/-/Paper1/Public_Comment", "forum": "HkeAoQQHM", "replyto": "Bko0sagYM", "signatures": ["~Yuechao_Gao1"], "readers": ["everyone"], "writers": ["~Yuechao_Gao1"], "content": {"title": "The impact of these ideas in real world applications", "comment": "Thanks for your comments. Actually we are working on an FPGA implimentation. Comparing with the state-of-the-art results, our methods achieve 2\u00d7 improvement for the computation efficiency per PE on most CONV and FC layers. Especially, our methods achieve 8\u00d7 improvement for the computation efficiency per PE on Alexnet layer CONV4 with 384 filters, and 11\u00d7 improvement on VGG16 layer CONV5-3 with 512 filters."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Stacked Filters Stationary Flow For Hardware-Oriented Acceleration Of Deep Convolutional Neural Networks", "abstract": "To address memory and computation resource limitations for hardware-oriented acceleration of deep convolutional neural networks(CNNs), we present a computation flow, stacked filters stationary flow (SFS), and a corresponding data encoding format, relative indexed compressed sparse filter format (CSF), to make the best of data sparsity, and simplify data handling at execution time. Comparing with the state-of-the-art result (Han et al., 2016b), our methods achieve 1.11x improvement in reducing the storage required by AlexNet, and 1.09x improvement in reducing the storage required by SqueezeNet, without loss of accuracy on the ImageNet dataset. Moreover, using these approaches, chip area for logics handling irregular sparse data access can be saved. Comparing with the 2D-SIMD processure structures in DVAS, ENVISION, etc., our methods achieve about 3.65x\u0002 processing element (PE) array utilization rate improvement (from 26.4% to 96.5%), using the data from Deep Compression on AlexNet.", "pdf": "/pdf/2aa396ef5420107b7465aa2725993da3952d346e.pdf", "TL;DR": "We present a computation flow, stacked filters stationary flow, and a corresponding data encoding format, relative indexed compressed sparse filter format,  for hardware acceleration of CNNs.", "paperhash": "yuechao|stacked_filters_stationary_flow_for_hardwareoriented_acceleration_of_deep_convolutional_neural_networks", "keywords": ["CNN hardware acceleration", "computation flow", "compressed sparse filter format", "3D-SIMD processor architecture"], "authors": ["Gao Yuechao", "Liu Nianhong", "Zhang Sheng"], "authorids": ["gyc15@mails.tsinghua.edu.cn", "lnh15@mails.tsinghua.edu.cn", "zhang_sh@tsinghua.edu.cn"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518712628264, "id": "ICLR.cc/2018/Workshop/-/Paper1/Public_Comment", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper1/Reviewers"], "reply": {"replyto": null, "forum": "HkeAoQQHM", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1518712628264}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582850497, "tcdate": 1520576811572, "number": 1, "cdate": 1520576811572, "id": "SyVNYsJYz", "invitation": "ICLR.cc/2018/Workshop/-/Paper1/Official_Review", "forum": "HkeAoQQHM", "replyto": "HkeAoQQHM", "signatures": ["ICLR.cc/2018/Workshop/Paper1/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper1/AnonReviewer1"], "content": {"title": "A method for reducing the space required for Hardware implementation of AlexNet", "rating": "6: Marginally above acceptance threshold", "review": "The authors present a new architecture for hardware implementation of AlexNet. The show a save in chip space (mainly the storage) and as a result in power.   \n\nThe paper is written clearly. While the subject is important, the save factor they show is nice but not dramatic. The encoding format is important since besides having new architectures for deep learning, it is important to also to have some discussion on new sparse schemes. \n\nIn the overall, I recommend to accept this paper. Although the contribution is incremental, this paper is important for the hardware community in order to continue and develop methods for hardware implementations.  ", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Stacked Filters Stationary Flow For Hardware-Oriented Acceleration Of Deep Convolutional Neural Networks", "abstract": "To address memory and computation resource limitations for hardware-oriented acceleration of deep convolutional neural networks(CNNs), we present a computation flow, stacked filters stationary flow (SFS), and a corresponding data encoding format, relative indexed compressed sparse filter format (CSF), to make the best of data sparsity, and simplify data handling at execution time. Comparing with the state-of-the-art result (Han et al., 2016b), our methods achieve 1.11x improvement in reducing the storage required by AlexNet, and 1.09x improvement in reducing the storage required by SqueezeNet, without loss of accuracy on the ImageNet dataset. Moreover, using these approaches, chip area for logics handling irregular sparse data access can be saved. Comparing with the 2D-SIMD processure structures in DVAS, ENVISION, etc., our methods achieve about 3.65x\u0002 processing element (PE) array utilization rate improvement (from 26.4% to 96.5%), using the data from Deep Compression on AlexNet.", "pdf": "/pdf/2aa396ef5420107b7465aa2725993da3952d346e.pdf", "TL;DR": "We present a computation flow, stacked filters stationary flow, and a corresponding data encoding format, relative indexed compressed sparse filter format,  for hardware acceleration of CNNs.", "paperhash": "yuechao|stacked_filters_stationary_flow_for_hardwareoriented_acceleration_of_deep_convolutional_neural_networks", "keywords": ["CNN hardware acceleration", "computation flow", "compressed sparse filter format", "3D-SIMD processor architecture"], "authors": ["Gao Yuechao", "Liu Nianhong", "Zhang Sheng"], "authorids": ["gyc15@mails.tsinghua.edu.cn", "lnh15@mails.tsinghua.edu.cn", "zhang_sh@tsinghua.edu.cn"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582850306, "id": "ICLR.cc/2018/Workshop/-/Paper1/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper1/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper1/AnonReviewer1", "ICLR.cc/2018/Workshop/Paper1/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper1/AnonReviewer3"], "reply": {"forum": "HkeAoQQHM", "replyto": "HkeAoQQHM", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper1/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper1/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582850306}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582776718, "tcdate": 1520635245647, "number": 2, "cdate": 1520635245647, "id": "SkL_6KeFG", "invitation": "ICLR.cc/2018/Workshop/-/Paper1/Official_Review", "forum": "HkeAoQQHM", "replyto": "HkeAoQQHM", "signatures": ["ICLR.cc/2018/Workshop/Paper1/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper1/AnonReviewer2"], "content": {"title": "Stacked filters for hardware acceleration", "rating": "6: Marginally above acceptance threshold", "review": "Stacked filters Stationary Flow For Hardware Oriented Acceleration of Deep Convolutional Neural Networks\nAuthors present a new stacked filter based architecture and a data encoding format for accelerating computations on CNNs.\n\nPositives:\nThe stacked filters are an intuitive way to parallelize the computation\nPaper emphasizes hardware efficiency:\nPE usage is increased\nChip design can be simplified w/o the need for complex logic\nIntroduction seems to emphasize the pros and cons of competing technologies\nResults show clear improvements\nNegatives:\nIt is not apparent to me that their storage improvements are massive (though this is probably a very incremental field)\nIt is not clear to me whether or not this technique will help anybody using anything other than their special architecture\nNotation in the math section is garbage\n3D-SIMD section should be cut - does not follow well and the figure 3 is useless \nFigure 1 is pretty non intuitive\nThe language is filled with run-ons and sentence fragments and generally unclear pronouns\nThe pseudocode is little more than a restatement of their horrendous math notation\nResults are very hard to read\nFigures are overall too small - need to focus or design better figures with this space limitation\n", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Stacked Filters Stationary Flow For Hardware-Oriented Acceleration Of Deep Convolutional Neural Networks", "abstract": "To address memory and computation resource limitations for hardware-oriented acceleration of deep convolutional neural networks(CNNs), we present a computation flow, stacked filters stationary flow (SFS), and a corresponding data encoding format, relative indexed compressed sparse filter format (CSF), to make the best of data sparsity, and simplify data handling at execution time. Comparing with the state-of-the-art result (Han et al., 2016b), our methods achieve 1.11x improvement in reducing the storage required by AlexNet, and 1.09x improvement in reducing the storage required by SqueezeNet, without loss of accuracy on the ImageNet dataset. Moreover, using these approaches, chip area for logics handling irregular sparse data access can be saved. Comparing with the 2D-SIMD processure structures in DVAS, ENVISION, etc., our methods achieve about 3.65x\u0002 processing element (PE) array utilization rate improvement (from 26.4% to 96.5%), using the data from Deep Compression on AlexNet.", "pdf": "/pdf/2aa396ef5420107b7465aa2725993da3952d346e.pdf", "TL;DR": "We present a computation flow, stacked filters stationary flow, and a corresponding data encoding format, relative indexed compressed sparse filter format,  for hardware acceleration of CNNs.", "paperhash": "yuechao|stacked_filters_stationary_flow_for_hardwareoriented_acceleration_of_deep_convolutional_neural_networks", "keywords": ["CNN hardware acceleration", "computation flow", "compressed sparse filter format", "3D-SIMD processor architecture"], "authors": ["Gao Yuechao", "Liu Nianhong", "Zhang Sheng"], "authorids": ["gyc15@mails.tsinghua.edu.cn", "lnh15@mails.tsinghua.edu.cn", "zhang_sh@tsinghua.edu.cn"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582850306, "id": "ICLR.cc/2018/Workshop/-/Paper1/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper1/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper1/AnonReviewer1", "ICLR.cc/2018/Workshop/Paper1/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper1/AnonReviewer3"], "reply": {"forum": "HkeAoQQHM", "replyto": "HkeAoQQHM", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper1/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper1/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582850306}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582753091, "tcdate": 1520651218863, "number": 3, "cdate": 1520651218863, "id": "Bko0sagYM", "invitation": "ICLR.cc/2018/Workshop/-/Paper1/Official_Review", "forum": "HkeAoQQHM", "replyto": "HkeAoQQHM", "signatures": ["ICLR.cc/2018/Workshop/Paper1/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper1/AnonReviewer3"], "content": {"title": "New ideas but lack real implementation ", "rating": "6: Marginally above acceptance threshold", "review": "The three ideas in the paper are new. The proposed sparse storage format CSF makes data access easier than CSC, etc. The simulation results are promising comparing to the work by Han (2016). The result numbers are theoretic numbers. Please correct me if I'm wrong, but it looks to me that the authors have't implemented those ideas into a software package. It will be good know what the impact of these ideas is when using in real world applications. ", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Stacked Filters Stationary Flow For Hardware-Oriented Acceleration Of Deep Convolutional Neural Networks", "abstract": "To address memory and computation resource limitations for hardware-oriented acceleration of deep convolutional neural networks(CNNs), we present a computation flow, stacked filters stationary flow (SFS), and a corresponding data encoding format, relative indexed compressed sparse filter format (CSF), to make the best of data sparsity, and simplify data handling at execution time. Comparing with the state-of-the-art result (Han et al., 2016b), our methods achieve 1.11x improvement in reducing the storage required by AlexNet, and 1.09x improvement in reducing the storage required by SqueezeNet, without loss of accuracy on the ImageNet dataset. Moreover, using these approaches, chip area for logics handling irregular sparse data access can be saved. Comparing with the 2D-SIMD processure structures in DVAS, ENVISION, etc., our methods achieve about 3.65x\u0002 processing element (PE) array utilization rate improvement (from 26.4% to 96.5%), using the data from Deep Compression on AlexNet.", "pdf": "/pdf/2aa396ef5420107b7465aa2725993da3952d346e.pdf", "TL;DR": "We present a computation flow, stacked filters stationary flow, and a corresponding data encoding format, relative indexed compressed sparse filter format,  for hardware acceleration of CNNs.", "paperhash": "yuechao|stacked_filters_stationary_flow_for_hardwareoriented_acceleration_of_deep_convolutional_neural_networks", "keywords": ["CNN hardware acceleration", "computation flow", "compressed sparse filter format", "3D-SIMD processor architecture"], "authors": ["Gao Yuechao", "Liu Nianhong", "Zhang Sheng"], "authorids": ["gyc15@mails.tsinghua.edu.cn", "lnh15@mails.tsinghua.edu.cn", "zhang_sh@tsinghua.edu.cn"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582850306, "id": "ICLR.cc/2018/Workshop/-/Paper1/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper1/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper1/AnonReviewer1", "ICLR.cc/2018/Workshop/Paper1/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper1/AnonReviewer3"], "reply": {"forum": "HkeAoQQHM", "replyto": "HkeAoQQHM", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper1/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper1/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582850306}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521573566422, "tcdate": 1521573566422, "number": 103, "cdate": 1521573566095, "id": "HJ8pACAtM", "invitation": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "forum": "HkeAoQQHM", "replyto": "HkeAoQQHM", "signatures": ["ICLR.cc/2018/Workshop/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Program_Chairs"], "content": {"decision": "Accept", "title": "ICLR 2018 Workshop Acceptance Decision", "comment": "Congratulations, your paper was accepted to the ICLR workshop."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Stacked Filters Stationary Flow For Hardware-Oriented Acceleration Of Deep Convolutional Neural Networks", "abstract": "To address memory and computation resource limitations for hardware-oriented acceleration of deep convolutional neural networks(CNNs), we present a computation flow, stacked filters stationary flow (SFS), and a corresponding data encoding format, relative indexed compressed sparse filter format (CSF), to make the best of data sparsity, and simplify data handling at execution time. Comparing with the state-of-the-art result (Han et al., 2016b), our methods achieve 1.11x improvement in reducing the storage required by AlexNet, and 1.09x improvement in reducing the storage required by SqueezeNet, without loss of accuracy on the ImageNet dataset. Moreover, using these approaches, chip area for logics handling irregular sparse data access can be saved. Comparing with the 2D-SIMD processure structures in DVAS, ENVISION, etc., our methods achieve about 3.65x\u0002 processing element (PE) array utilization rate improvement (from 26.4% to 96.5%), using the data from Deep Compression on AlexNet.", "pdf": "/pdf/2aa396ef5420107b7465aa2725993da3952d346e.pdf", "TL;DR": "We present a computation flow, stacked filters stationary flow, and a corresponding data encoding format, relative indexed compressed sparse filter format,  for hardware acceleration of CNNs.", "paperhash": "yuechao|stacked_filters_stationary_flow_for_hardwareoriented_acceleration_of_deep_convolutional_neural_networks", "keywords": ["CNN hardware acceleration", "computation flow", "compressed sparse filter format", "3D-SIMD processor architecture"], "authors": ["Gao Yuechao", "Liu Nianhong", "Zhang Sheng"], "authorids": ["gyc15@mails.tsinghua.edu.cn", "lnh15@mails.tsinghua.edu.cn", "zhang_sh@tsinghua.edu.cn"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518629844880, "id": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Program_Chairs"], "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Workshop/-/Submission", "writers": {"values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Workshop/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Workshop Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept", "Reject"]}}}, "nonreaders": [], "noninvitees": [], "cdate": 1518629844880}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1517888161012, "tcdate": 1516612552409, "number": 1, "cdate": 1516612552409, "id": "HkeAoQQHM", "invitation": "ICLR.cc/2018/Workshop/-/Submission", "forum": "HkeAoQQHM", "signatures": ["~Yuechao_Gao1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop"], "content": {"title": "Stacked Filters Stationary Flow For Hardware-Oriented Acceleration Of Deep Convolutional Neural Networks", "abstract": "To address memory and computation resource limitations for hardware-oriented acceleration of deep convolutional neural networks(CNNs), we present a computation flow, stacked filters stationary flow (SFS), and a corresponding data encoding format, relative indexed compressed sparse filter format (CSF), to make the best of data sparsity, and simplify data handling at execution time. Comparing with the state-of-the-art result (Han et al., 2016b), our methods achieve 1.11x improvement in reducing the storage required by AlexNet, and 1.09x improvement in reducing the storage required by SqueezeNet, without loss of accuracy on the ImageNet dataset. Moreover, using these approaches, chip area for logics handling irregular sparse data access can be saved. Comparing with the 2D-SIMD processure structures in DVAS, ENVISION, etc., our methods achieve about 3.65x\u0002 processing element (PE) array utilization rate improvement (from 26.4% to 96.5%), using the data from Deep Compression on AlexNet.", "pdf": "/pdf/2aa396ef5420107b7465aa2725993da3952d346e.pdf", "TL;DR": "We present a computation flow, stacked filters stationary flow, and a corresponding data encoding format, relative indexed compressed sparse filter format,  for hardware acceleration of CNNs.", "paperhash": "yuechao|stacked_filters_stationary_flow_for_hardwareoriented_acceleration_of_deep_convolutional_neural_networks", "keywords": ["CNN hardware acceleration", "computation flow", "compressed sparse filter format", "3D-SIMD processor architecture"], "authors": ["Gao Yuechao", "Liu Nianhong", "Zhang Sheng"], "authorids": ["gyc15@mails.tsinghua.edu.cn", "lnh15@mails.tsinghua.edu.cn", "zhang_sh@tsinghua.edu.cn"]}, "nonreaders": [], "details": {"replyCount": 5, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1518472800000, "tmdate": 1518474081690, "id": "ICLR.cc/2018/Workshop/-/Submission", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Workshop"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2018/Workshop", "description": "Your authorized identity to be associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 9, "value-regex": "upload", "description": "Upload a PDF file that ends with .pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 8, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names. Please provide real names; identities will be anonymized."}, "keywords": {"order": 6, "values-regex": "(^$)|[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of keywords."}, "TL;DR": {"required": false, "order": 7, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,500}"}, "authorids": {"required": true, "order": 3, "values-regex": "([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,},){0,}([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,})", "description": "Comma separated list of author email addresses, lowercased, in the same order as above. For authors with existing OpenReview accounts, please make sure that the provided email address(es) match those listed in the author's profile. Please provide real emails; identities will be anonymized."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1526248800000, "cdate": 1518474081690}}}], "count": 6}