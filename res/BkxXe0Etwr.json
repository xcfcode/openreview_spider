{"notes": [{"id": "BkxXe0Etwr", "original": "BkxVXE7_PS", "number": 923, "cdate": 1569439210946, "ddate": null, "tcdate": 1569439210946, "tmdate": 1583912048771, "tddate": null, "forum": "BkxXe0Etwr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "CAQL: Continuous Action Q-Learning", "authors": ["Moonkyung Ryu", "Yinlam Chow", "Ross Anderson", "Christian Tjandraatmadja", "Craig Boutilier"], "authorids": ["mkryu@google.com", "yinlamchow@google.com", "rander@google.com", "ctjandra@google.com", "cboutilier@google.com"], "keywords": ["Reinforcement learning (RL)", "DQN", "Continuous control", "Mixed-Integer Programming (MIP)"], "TL;DR": "A general framework of value-based reinforcement learning for continuous control", "abstract": "Reinforcement learning (RL) with value-based methods (e.g., Q-learning) has shown success in a variety of domains such as\ngames and recommender systems (RSs). When the action space is finite, these algorithms implicitly finds a policy by learning the optimal value function, which are often very efficient. \nHowever, one major challenge of extending Q-learning to tackle continuous-action RL problems is that obtaining optimal Bellman backup requires solving a continuous action-maximization (max-Q) problem. While it is common to restrict the parameterization of the Q-function to be concave in actions to simplify the max-Q problem, such a restriction might lead to performance degradation. Alternatively, when the Q-function is parameterized with a generic feed-forward neural network (NN), the max-Q problem can be NP-hard. In this work, we propose the CAQL method which minimizes the Bellman residual using Q-learning with one of several plug-and-play action optimizers. In particular, leveraging the strides of optimization theories in deep NN, we show that max-Q problem can be solved optimally with mixed-integer programming (MIP)---when the Q-function has sufficient representation power, this MIP-based optimization induces better policies and is more robust than counterparts, e.g., CEM or GA, that approximate the max-Q solution. To speed up training of CAQL, we develop three techniques, namely (i) dynamic tolerance, (ii) dual filtering, and (iii) clustering.\nTo speed up inference of CAQL, we introduce the action function that concurrently learns the optimal policy.\nTo demonstrate the efficiency of CAQL we compare it with state-of-the-art RL algorithms on benchmark continuous control problems that have different degrees of action constraints and show that CAQL significantly outperforms policy-based methods in heavily constrained environments.", "pdf": "/pdf/49cfb54fa89345e85f991893706d894f0d953510.pdf", "paperhash": "ryu|caql_continuous_action_qlearning", "_bibtex": "@inproceedings{\nRyu2020CAQL:,\ntitle={CAQL: Continuous Action Q-Learning},\nauthor={Moonkyung Ryu and Yinlam Chow and Ross Anderson and Christian Tjandraatmadja and Craig Boutilier},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BkxXe0Etwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/3b26c57bb8e7c649109748aefb71fd764b7c257e.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "_QI7ffqaiU", "original": null, "number": 1, "cdate": 1576798709850, "ddate": null, "tcdate": 1576798709850, "tmdate": 1576800926473, "tddate": null, "forum": "BkxXe0Etwr", "replyto": "BkxXe0Etwr", "invitation": "ICLR.cc/2020/Conference/Paper923/-/Decision", "content": {"decision": "Accept (Poster)", "comment": "All three reviewers gave scores of Weak Accept. AC has read the reviews and rebuttal and agrees that the paper makes a solid contribution and should be accepted.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "CAQL: Continuous Action Q-Learning", "authors": ["Moonkyung Ryu", "Yinlam Chow", "Ross Anderson", "Christian Tjandraatmadja", "Craig Boutilier"], "authorids": ["mkryu@google.com", "yinlamchow@google.com", "rander@google.com", "ctjandra@google.com", "cboutilier@google.com"], "keywords": ["Reinforcement learning (RL)", "DQN", "Continuous control", "Mixed-Integer Programming (MIP)"], "TL;DR": "A general framework of value-based reinforcement learning for continuous control", "abstract": "Reinforcement learning (RL) with value-based methods (e.g., Q-learning) has shown success in a variety of domains such as\ngames and recommender systems (RSs). When the action space is finite, these algorithms implicitly finds a policy by learning the optimal value function, which are often very efficient. \nHowever, one major challenge of extending Q-learning to tackle continuous-action RL problems is that obtaining optimal Bellman backup requires solving a continuous action-maximization (max-Q) problem. While it is common to restrict the parameterization of the Q-function to be concave in actions to simplify the max-Q problem, such a restriction might lead to performance degradation. Alternatively, when the Q-function is parameterized with a generic feed-forward neural network (NN), the max-Q problem can be NP-hard. In this work, we propose the CAQL method which minimizes the Bellman residual using Q-learning with one of several plug-and-play action optimizers. In particular, leveraging the strides of optimization theories in deep NN, we show that max-Q problem can be solved optimally with mixed-integer programming (MIP)---when the Q-function has sufficient representation power, this MIP-based optimization induces better policies and is more robust than counterparts, e.g., CEM or GA, that approximate the max-Q solution. To speed up training of CAQL, we develop three techniques, namely (i) dynamic tolerance, (ii) dual filtering, and (iii) clustering.\nTo speed up inference of CAQL, we introduce the action function that concurrently learns the optimal policy.\nTo demonstrate the efficiency of CAQL we compare it with state-of-the-art RL algorithms on benchmark continuous control problems that have different degrees of action constraints and show that CAQL significantly outperforms policy-based methods in heavily constrained environments.", "pdf": "/pdf/49cfb54fa89345e85f991893706d894f0d953510.pdf", "paperhash": "ryu|caql_continuous_action_qlearning", "_bibtex": "@inproceedings{\nRyu2020CAQL:,\ntitle={CAQL: Continuous Action Q-Learning},\nauthor={Moonkyung Ryu and Yinlam Chow and Ross Anderson and Christian Tjandraatmadja and Craig Boutilier},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BkxXe0Etwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/3b26c57bb8e7c649109748aefb71fd764b7c257e.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "BkxXe0Etwr", "replyto": "BkxXe0Etwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795728104, "tmdate": 1576800280448, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper923/-/Decision"}}}, {"id": "HylD-wzHor", "original": null, "number": 4, "cdate": 1573361406783, "ddate": null, "tcdate": 1573361406783, "tmdate": 1573512068617, "tddate": null, "forum": "BkxXe0Etwr", "replyto": "H1lX6OD6Kr", "invitation": "ICLR.cc/2020/Conference/Paper923/-/Official_Comment", "content": {"title": "Thanks for the helpful suggestions and questions on our paper. We provide brief responses to main issues/questions raised.", "comment": "Which approximation to use?: In Section 4 we provide three techniques---dynamic tolerance, dual filtering, and clustering---to accelerate training (reduce computational cost). Dynamic tolerance is useful in a very general sense, and can be applied to any problem. It speeds up the MIP solver by adjusting its tolerance w.r.t. TD error, and in practice, improves the performance of CAQL significantly (see Section 5 for CAQL-GA and CAQL-MIP). We recommend using this method by default. Dual filtering and clustering both trade off training speed with performance (except in the case of hinge-Q learning, which has no loss of optimality when using dual filtering)---this is because one removes the action optimization step that corresponds to inactive and \u201cless important\u201d next states. These are the practical options (with tunable parameters) that allow users or developers to explore specific trade-offs suitable to specific application domains. We will elaborate on these points in the revised paper. You raise an important question of under which conditions (or in which environments) might these methods work better or worse. We do not have definitive answers and hope to address this question in future research.\n\nLarge standard deviations: In general, model-free RL algorithms (including CAQL) indeed tend to have high variance during training. This also tends to obscure the significance of results in plots. Similar observations can be made of the results of other state-of-the-art methods such as DDPG and TD3. So to make the results more accessible and \u201cvisible\u201d to readers, various tricks can be used. For example, in the TD3 paper (https://arxiv.org/pdf/1802.09477.pdf), the authors specify the shaded region of the training curves (see, e.g., their Figure 5) to cover just HALF of the standard deviation, while the shaded regions in all our plots cover the full standard deviation. We will explore ways to make performance differences more visible in our revised paper.\n\nThat said, even with the current plots, we hope that it is evident that tasks with smaller action ranges (such as Hopper 0.25, HalfCheetah 0.25, Ant 0.1), CAQL-MIP has much lower variance in training than other methods (see Table 1 and the last paragraph on page 6 of the submission). This is a major benefit of using MIP-based solutions, despite the computational cost. For a clearer illustration, please find standalone graphs for CAQL-MIP at the following link: https://gofile.io/?c=Zno71b"}, "signatures": ["ICLR.cc/2020/Conference/Paper923/Authors"], "readers": ["ICLR.cc/2020/Conference/Paper923/Authors", "ICLR.cc/2020/Conference/Paper923/Reviewers", "ICLR.cc/2020/Conference/Paper923/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs", "everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper923/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "CAQL: Continuous Action Q-Learning", "authors": ["Moonkyung Ryu", "Yinlam Chow", "Ross Anderson", "Christian Tjandraatmadja", "Craig Boutilier"], "authorids": ["mkryu@google.com", "yinlamchow@google.com", "rander@google.com", "ctjandra@google.com", "cboutilier@google.com"], "keywords": ["Reinforcement learning (RL)", "DQN", "Continuous control", "Mixed-Integer Programming (MIP)"], "TL;DR": "A general framework of value-based reinforcement learning for continuous control", "abstract": "Reinforcement learning (RL) with value-based methods (e.g., Q-learning) has shown success in a variety of domains such as\ngames and recommender systems (RSs). When the action space is finite, these algorithms implicitly finds a policy by learning the optimal value function, which are often very efficient. \nHowever, one major challenge of extending Q-learning to tackle continuous-action RL problems is that obtaining optimal Bellman backup requires solving a continuous action-maximization (max-Q) problem. While it is common to restrict the parameterization of the Q-function to be concave in actions to simplify the max-Q problem, such a restriction might lead to performance degradation. Alternatively, when the Q-function is parameterized with a generic feed-forward neural network (NN), the max-Q problem can be NP-hard. In this work, we propose the CAQL method which minimizes the Bellman residual using Q-learning with one of several plug-and-play action optimizers. In particular, leveraging the strides of optimization theories in deep NN, we show that max-Q problem can be solved optimally with mixed-integer programming (MIP)---when the Q-function has sufficient representation power, this MIP-based optimization induces better policies and is more robust than counterparts, e.g., CEM or GA, that approximate the max-Q solution. To speed up training of CAQL, we develop three techniques, namely (i) dynamic tolerance, (ii) dual filtering, and (iii) clustering.\nTo speed up inference of CAQL, we introduce the action function that concurrently learns the optimal policy.\nTo demonstrate the efficiency of CAQL we compare it with state-of-the-art RL algorithms on benchmark continuous control problems that have different degrees of action constraints and show that CAQL significantly outperforms policy-based methods in heavily constrained environments.", "pdf": "/pdf/49cfb54fa89345e85f991893706d894f0d953510.pdf", "paperhash": "ryu|caql_continuous_action_qlearning", "_bibtex": "@inproceedings{\nRyu2020CAQL:,\ntitle={CAQL: Continuous Action Q-Learning},\nauthor={Moonkyung Ryu and Yinlam Chow and Ross Anderson and Christian Tjandraatmadja and Craig Boutilier},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BkxXe0Etwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/3b26c57bb8e7c649109748aefb71fd764b7c257e.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BkxXe0Etwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper923/Authors", "ICLR.cc/2020/Conference/Paper923/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper923/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper923/Reviewers", "ICLR.cc/2020/Conference/Paper923/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper923/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper923/Authors|ICLR.cc/2020/Conference/Paper923/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504164100, "tmdate": 1576860541722, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper923/Authors", "ICLR.cc/2020/Conference/Paper923/Reviewers", "ICLR.cc/2020/Conference/Paper923/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper923/-/Official_Comment"}}}, {"id": "ByxVH8MBiH", "original": null, "number": 3, "cdate": 1573361212064, "ddate": null, "tcdate": 1573361212064, "tmdate": 1573512037195, "tddate": null, "forum": "BkxXe0Etwr", "replyto": "S1lSZ2z0FH", "invitation": "ICLR.cc/2020/Conference/Paper923/-/Official_Comment", "content": {"title": "Thanks for the helpful suggestions and questions on our paper. We provide brief responses to main issues/questions raised.", "comment": "One high-level clarification: While the MIP formulation and the approximations we develop are important (and probably the most novel element of the paper), the general CAQL framework, which supports other optimizers (e.g., GA, CEM) is also an contribution that can serve to investigate alternative approaches to continuous-action Q-learning. We leave the choice of an optimizer and speed-up methods to users, and let them decide based on the specific trade-offs in their applications.\n\nOther network types: To simplify notation, we focus our presentation of fully-connected feedforward network representations of Q-functions. That said, extending the model (including the MIP formulation) to convolutional networks (with ReLU activation and max pooling) is straightforward (see reference Anderson et al. 2019, at https://arxiv.org/abs/1811.01988). Handling additional activation functions is not problematic for GA or CEM; but we agree that it is certainly not straightforward with MIP, where some approximation would be required (e.g., sigmoids can be approximated in a piece-wise-linear fashion and encoded in a MIP). We leave this to future work.\n\nHyper-parameter sweeps: Apologies and thanks for pointing this out. The values for the exploration decay hyper-parameter are given in Table 8, while there is no temperature parameter (the caption contains a typo). We will update the table/descriptions accordingly.\n\nIncreased action range: This is an interesting suggestion! Our conjecture is that increasing the action ranges (to be above the default values) will not significantly change the results from the default (non-constrained) case because the action constraint is almost always inactive in these cases. But we will explore this suggestion experimentally to verify.\n\nSymmetry: This is an excellent suggestion and could certainly enhance the scalability of MIP-based optimization. While our primary intent is to provide benchmark results on \u201cstandard\u201d MuJoCo experiments, we will extend our experiments using this idea. This should have an outsized impact on the MIP (vis-a-vis CEM, GA).\n\nFigure 3, Different values on x-axis, length of experiments: For the more difficult MuJoCo experiments (e.g., Ant, HalfCheetah, Humanoid), we set a longer training step of 500,000, while for simpler experiments (e.g., Pendulum, Hopper, Walker2D), we use a shorter training step of 200,000. In all the plots, performance is evaluated at every 1000 training steps.\n\nWhy length-200 episodes?: We scoped down episode length in these experiments due to the heavy computational requirements of the MIP-based action solver. For problems with longer horizons, we require a larger network for better Q-function approximation, which in turn significantly increases the computation time needed by the MIP solver. Additionally, with a longer horizon, we need to train the model longer (more training steps). In order to keep the MIP solution time manageable (and to compare it to CEM, GA), we shortened the horizon length to 200 and parameterized the Q-function with a relatively simple 32x16 feed-forward network. Some of this motivation can be found on page 6 of the submission, but admittedly, it is a bit terse. We will justify the short-horizon setting in more detail in the revision.\n\nFor a fairer comparison with state-of-the-art RL methods, we have also completed experimental results with the standard (horizon=1000) setup, comparing TD3, DDPG, and CAQL-GA (rather than CAQL MIP or CAQL-CEM). These results can be found at the following anonymous link: https://gofile.io/?c=x1iYJS  Similar to the setting in the TD3 paper (Fujimoto et al. 2018), we use a 400 x 300 feedforward network and observe a similar trend to that found with horizon 200.  In particular, with a constrained action range, CAQL-GA outperforms DDPG and TD3 for all Mujoco domains we test. For the default action range, CAQL-GA performs similar to TD3 in Hopper, Walker2D, HalfCheetah, while it outperforms the state-of-the-art in Ant and Humanoid by a significant margin."}, "signatures": ["ICLR.cc/2020/Conference/Paper923/Authors"], "readers": ["ICLR.cc/2020/Conference/Paper923/Authors", "ICLR.cc/2020/Conference/Paper923/Reviewers", "ICLR.cc/2020/Conference/Paper923/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs", "everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper923/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "CAQL: Continuous Action Q-Learning", "authors": ["Moonkyung Ryu", "Yinlam Chow", "Ross Anderson", "Christian Tjandraatmadja", "Craig Boutilier"], "authorids": ["mkryu@google.com", "yinlamchow@google.com", "rander@google.com", "ctjandra@google.com", "cboutilier@google.com"], "keywords": ["Reinforcement learning (RL)", "DQN", "Continuous control", "Mixed-Integer Programming (MIP)"], "TL;DR": "A general framework of value-based reinforcement learning for continuous control", "abstract": "Reinforcement learning (RL) with value-based methods (e.g., Q-learning) has shown success in a variety of domains such as\ngames and recommender systems (RSs). When the action space is finite, these algorithms implicitly finds a policy by learning the optimal value function, which are often very efficient. \nHowever, one major challenge of extending Q-learning to tackle continuous-action RL problems is that obtaining optimal Bellman backup requires solving a continuous action-maximization (max-Q) problem. While it is common to restrict the parameterization of the Q-function to be concave in actions to simplify the max-Q problem, such a restriction might lead to performance degradation. Alternatively, when the Q-function is parameterized with a generic feed-forward neural network (NN), the max-Q problem can be NP-hard. In this work, we propose the CAQL method which minimizes the Bellman residual using Q-learning with one of several plug-and-play action optimizers. In particular, leveraging the strides of optimization theories in deep NN, we show that max-Q problem can be solved optimally with mixed-integer programming (MIP)---when the Q-function has sufficient representation power, this MIP-based optimization induces better policies and is more robust than counterparts, e.g., CEM or GA, that approximate the max-Q solution. To speed up training of CAQL, we develop three techniques, namely (i) dynamic tolerance, (ii) dual filtering, and (iii) clustering.\nTo speed up inference of CAQL, we introduce the action function that concurrently learns the optimal policy.\nTo demonstrate the efficiency of CAQL we compare it with state-of-the-art RL algorithms on benchmark continuous control problems that have different degrees of action constraints and show that CAQL significantly outperforms policy-based methods in heavily constrained environments.", "pdf": "/pdf/49cfb54fa89345e85f991893706d894f0d953510.pdf", "paperhash": "ryu|caql_continuous_action_qlearning", "_bibtex": "@inproceedings{\nRyu2020CAQL:,\ntitle={CAQL: Continuous Action Q-Learning},\nauthor={Moonkyung Ryu and Yinlam Chow and Ross Anderson and Christian Tjandraatmadja and Craig Boutilier},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BkxXe0Etwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/3b26c57bb8e7c649109748aefb71fd764b7c257e.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BkxXe0Etwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper923/Authors", "ICLR.cc/2020/Conference/Paper923/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper923/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper923/Reviewers", "ICLR.cc/2020/Conference/Paper923/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper923/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper923/Authors|ICLR.cc/2020/Conference/Paper923/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504164100, "tmdate": 1576860541722, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper923/Authors", "ICLR.cc/2020/Conference/Paper923/Reviewers", "ICLR.cc/2020/Conference/Paper923/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper923/-/Official_Comment"}}}, {"id": "H1lX6OD6Kr", "original": null, "number": 1, "cdate": 1571809466643, "ddate": null, "tcdate": 1571809466643, "tmdate": 1572972535132, "tddate": null, "forum": "BkxXe0Etwr", "replyto": "BkxXe0Etwr", "invitation": "ICLR.cc/2020/Conference/Paper923/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "The paper proposes a novel value-based continuous control algorithm by formulating the problem as mixed-integer programming. With this formulation, the optimal action (corresponding to the maximum action value) can be found by solving the optimization problem at each time step. To reduce the time complexity of the optimization, the author proposes several variants to approximately solve the problem. Results on robotics control are presented. The proposed looks interesting and could be useful in practice. \n\n1. Section 4 of the paper can be improved. Although the author proposes several methods for approximating the optimal solution, it is unclear what message the author wants to convey. How to decide which approximation to use? Is there any situation where one of the approximations should be preferred? \n\n2. In the experiments, the standard deviation is very large, so it is hard to claim the proposed method is better."}, "signatures": ["ICLR.cc/2020/Conference/Paper923/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper923/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "CAQL: Continuous Action Q-Learning", "authors": ["Moonkyung Ryu", "Yinlam Chow", "Ross Anderson", "Christian Tjandraatmadja", "Craig Boutilier"], "authorids": ["mkryu@google.com", "yinlamchow@google.com", "rander@google.com", "ctjandra@google.com", "cboutilier@google.com"], "keywords": ["Reinforcement learning (RL)", "DQN", "Continuous control", "Mixed-Integer Programming (MIP)"], "TL;DR": "A general framework of value-based reinforcement learning for continuous control", "abstract": "Reinforcement learning (RL) with value-based methods (e.g., Q-learning) has shown success in a variety of domains such as\ngames and recommender systems (RSs). When the action space is finite, these algorithms implicitly finds a policy by learning the optimal value function, which are often very efficient. \nHowever, one major challenge of extending Q-learning to tackle continuous-action RL problems is that obtaining optimal Bellman backup requires solving a continuous action-maximization (max-Q) problem. While it is common to restrict the parameterization of the Q-function to be concave in actions to simplify the max-Q problem, such a restriction might lead to performance degradation. Alternatively, when the Q-function is parameterized with a generic feed-forward neural network (NN), the max-Q problem can be NP-hard. In this work, we propose the CAQL method which minimizes the Bellman residual using Q-learning with one of several plug-and-play action optimizers. In particular, leveraging the strides of optimization theories in deep NN, we show that max-Q problem can be solved optimally with mixed-integer programming (MIP)---when the Q-function has sufficient representation power, this MIP-based optimization induces better policies and is more robust than counterparts, e.g., CEM or GA, that approximate the max-Q solution. To speed up training of CAQL, we develop three techniques, namely (i) dynamic tolerance, (ii) dual filtering, and (iii) clustering.\nTo speed up inference of CAQL, we introduce the action function that concurrently learns the optimal policy.\nTo demonstrate the efficiency of CAQL we compare it with state-of-the-art RL algorithms on benchmark continuous control problems that have different degrees of action constraints and show that CAQL significantly outperforms policy-based methods in heavily constrained environments.", "pdf": "/pdf/49cfb54fa89345e85f991893706d894f0d953510.pdf", "paperhash": "ryu|caql_continuous_action_qlearning", "_bibtex": "@inproceedings{\nRyu2020CAQL:,\ntitle={CAQL: Continuous Action Q-Learning},\nauthor={Moonkyung Ryu and Yinlam Chow and Ross Anderson and Christian Tjandraatmadja and Craig Boutilier},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BkxXe0Etwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/3b26c57bb8e7c649109748aefb71fd764b7c257e.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BkxXe0Etwr", "replyto": "BkxXe0Etwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper923/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper923/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575404320178, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper923/Reviewers"], "noninvitees": [], "tcdate": 1570237744992, "tmdate": 1575404320189, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper923/-/Official_Review"}}}, {"id": "S1lSZ2z0FH", "original": null, "number": 2, "cdate": 1571855357123, "ddate": null, "tcdate": 1571855357123, "tmdate": 1572972535083, "tddate": null, "forum": "BkxXe0Etwr", "replyto": "BkxXe0Etwr", "invitation": "ICLR.cc/2020/Conference/Paper923/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "N/A", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "Summary: \nThis paper targets the maximization issue in continuous value based methods, especially Q Learning. The idea is to use Mixed Integer Programming (MIP) to solve the Q maximization step by formulating the neural network structure of the Q function as a constrained mixed integer program. Further improvements are made by approximating the MIP solution in order to make training/inference faster. The method is tested on tasks from the Mujoco domain and compared with other value based methods for continuous control. I found the paper simple to follow and well structured. The problem is well motivated too and the empirical analysis is quite rigorous.    \n\nThe obvious concerns are regarding scalability of the method; both in terms of 1) using other forms of neural network components (ex. Other activation functions, Convolutional networks in the case of vision based problems such as robotic manipulation) and 2) problems that are inherently less sample efficient, i.e. cases where shortening the sampling horizon is not a feasible option for learning meaningful policies. \n\nOverall, I feel the positive aspects more or less outweigh the drawbacks and therefore my vote is for a weak accept.\n\n\nComments/Questions: \n- Table 8 description says hyper-parameter sweeps were done for temperature and exploration noise decay values but the table is missing their values.\n- What happens when action range is increased from default? One of the reasons mentioned for constraining the action space is to validate how well policy-based methods work. To really take this point home, I feel it might be good to check with an increased action range.\n- Controlling w.r.t symmetry of the env (esp. in Mujoco domains), thus reducing the number of actions by half, might help faster MIP computation times. \n- Figure 3 x-axis runs till different values, but the description says training steps are 1000. How long are the experiments run for?\n- Can the authors elaborate on why the episode length is decreased from 1000 to 200?\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper923/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper923/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "CAQL: Continuous Action Q-Learning", "authors": ["Moonkyung Ryu", "Yinlam Chow", "Ross Anderson", "Christian Tjandraatmadja", "Craig Boutilier"], "authorids": ["mkryu@google.com", "yinlamchow@google.com", "rander@google.com", "ctjandra@google.com", "cboutilier@google.com"], "keywords": ["Reinforcement learning (RL)", "DQN", "Continuous control", "Mixed-Integer Programming (MIP)"], "TL;DR": "A general framework of value-based reinforcement learning for continuous control", "abstract": "Reinforcement learning (RL) with value-based methods (e.g., Q-learning) has shown success in a variety of domains such as\ngames and recommender systems (RSs). When the action space is finite, these algorithms implicitly finds a policy by learning the optimal value function, which are often very efficient. \nHowever, one major challenge of extending Q-learning to tackle continuous-action RL problems is that obtaining optimal Bellman backup requires solving a continuous action-maximization (max-Q) problem. While it is common to restrict the parameterization of the Q-function to be concave in actions to simplify the max-Q problem, such a restriction might lead to performance degradation. Alternatively, when the Q-function is parameterized with a generic feed-forward neural network (NN), the max-Q problem can be NP-hard. In this work, we propose the CAQL method which minimizes the Bellman residual using Q-learning with one of several plug-and-play action optimizers. In particular, leveraging the strides of optimization theories in deep NN, we show that max-Q problem can be solved optimally with mixed-integer programming (MIP)---when the Q-function has sufficient representation power, this MIP-based optimization induces better policies and is more robust than counterparts, e.g., CEM or GA, that approximate the max-Q solution. To speed up training of CAQL, we develop three techniques, namely (i) dynamic tolerance, (ii) dual filtering, and (iii) clustering.\nTo speed up inference of CAQL, we introduce the action function that concurrently learns the optimal policy.\nTo demonstrate the efficiency of CAQL we compare it with state-of-the-art RL algorithms on benchmark continuous control problems that have different degrees of action constraints and show that CAQL significantly outperforms policy-based methods in heavily constrained environments.", "pdf": "/pdf/49cfb54fa89345e85f991893706d894f0d953510.pdf", "paperhash": "ryu|caql_continuous_action_qlearning", "_bibtex": "@inproceedings{\nRyu2020CAQL:,\ntitle={CAQL: Continuous Action Q-Learning},\nauthor={Moonkyung Ryu and Yinlam Chow and Ross Anderson and Christian Tjandraatmadja and Craig Boutilier},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BkxXe0Etwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/3b26c57bb8e7c649109748aefb71fd764b7c257e.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BkxXe0Etwr", "replyto": "BkxXe0Etwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper923/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper923/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575404320178, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper923/Reviewers"], "noninvitees": [], "tcdate": 1570237744992, "tmdate": 1575404320189, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper923/-/Official_Review"}}}, {"id": "rkeMEqSY_S", "original": null, "number": 1, "cdate": 1570490921686, "ddate": null, "tcdate": 1570490921686, "tmdate": 1570490921686, "tddate": null, "forum": "BkxXe0Etwr", "replyto": "r1eqvrj_dH", "invitation": "ICLR.cc/2020/Conference/Paper923/-/Official_Comment", "content": {"comment": "Apologies for not citing these (and other) earlier works, we will fix this in revision --- space constraints cut into our broader discussion of related work more than it should have. ", "title": "Will cite the papers in revision."}, "signatures": ["ICLR.cc/2020/Conference/Paper923/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper923/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "CAQL: Continuous Action Q-Learning", "authors": ["Moonkyung Ryu", "Yinlam Chow", "Ross Anderson", "Christian Tjandraatmadja", "Craig Boutilier"], "authorids": ["mkryu@google.com", "yinlamchow@google.com", "rander@google.com", "ctjandra@google.com", "cboutilier@google.com"], "keywords": ["Reinforcement learning (RL)", "DQN", "Continuous control", "Mixed-Integer Programming (MIP)"], "TL;DR": "A general framework of value-based reinforcement learning for continuous control", "abstract": "Reinforcement learning (RL) with value-based methods (e.g., Q-learning) has shown success in a variety of domains such as\ngames and recommender systems (RSs). When the action space is finite, these algorithms implicitly finds a policy by learning the optimal value function, which are often very efficient. \nHowever, one major challenge of extending Q-learning to tackle continuous-action RL problems is that obtaining optimal Bellman backup requires solving a continuous action-maximization (max-Q) problem. While it is common to restrict the parameterization of the Q-function to be concave in actions to simplify the max-Q problem, such a restriction might lead to performance degradation. Alternatively, when the Q-function is parameterized with a generic feed-forward neural network (NN), the max-Q problem can be NP-hard. In this work, we propose the CAQL method which minimizes the Bellman residual using Q-learning with one of several plug-and-play action optimizers. In particular, leveraging the strides of optimization theories in deep NN, we show that max-Q problem can be solved optimally with mixed-integer programming (MIP)---when the Q-function has sufficient representation power, this MIP-based optimization induces better policies and is more robust than counterparts, e.g., CEM or GA, that approximate the max-Q solution. To speed up training of CAQL, we develop three techniques, namely (i) dynamic tolerance, (ii) dual filtering, and (iii) clustering.\nTo speed up inference of CAQL, we introduce the action function that concurrently learns the optimal policy.\nTo demonstrate the efficiency of CAQL we compare it with state-of-the-art RL algorithms on benchmark continuous control problems that have different degrees of action constraints and show that CAQL significantly outperforms policy-based methods in heavily constrained environments.", "pdf": "/pdf/49cfb54fa89345e85f991893706d894f0d953510.pdf", "paperhash": "ryu|caql_continuous_action_qlearning", "_bibtex": "@inproceedings{\nRyu2020CAQL:,\ntitle={CAQL: Continuous Action Q-Learning},\nauthor={Moonkyung Ryu and Yinlam Chow and Ross Anderson and Christian Tjandraatmadja and Craig Boutilier},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BkxXe0Etwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/3b26c57bb8e7c649109748aefb71fd764b7c257e.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BkxXe0Etwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper923/Authors", "ICLR.cc/2020/Conference/Paper923/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper923/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper923/Reviewers", "ICLR.cc/2020/Conference/Paper923/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper923/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper923/Authors|ICLR.cc/2020/Conference/Paper923/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504164100, "tmdate": 1576860541722, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper923/Authors", "ICLR.cc/2020/Conference/Paper923/Reviewers", "ICLR.cc/2020/Conference/Paper923/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper923/-/Official_Comment"}}}, {"id": "r1eqvrj_dH", "original": null, "number": 1, "cdate": 1570448738210, "ddate": null, "tcdate": 1570448738210, "tmdate": 1570448738210, "tddate": null, "forum": "BkxXe0Etwr", "replyto": "BkxXe0Etwr", "invitation": "ICLR.cc/2020/Conference/Paper923/-/Public_Comment", "content": {"comment": "Continuous Action Q-Learning is the title of a 2002 journal paper:\n\n@article{millan2002continuous,\n  title={Continuous-action Q-learning},\n  author={Mill{\\'a}n, Jos{\\'e} Del R and Posenato, Daniele and Dedieu, Eric},\n  journal={Machine Learning},\n  volume={49},\n  number={2-3},\n  pages={247--265},\n  year={2002},\n  publisher={Springer}\n}\n\nThere is also \n\n@inproceedings{gaskett1999q,\n  title={Q-learning in continuous state and action spaces},\n  author={Gaskett, Chris and Wettergreen, David and Zelinsky, Alexander},\n  booktitle={Australasian Joint Conference on Artificial Intelligence},\n  pages={417--428},\n  year={1999},\n  organization={Springer}\n}\n\nwhich directly points to Lemon Baird's wire fitting.\n\nOf course, these are not deep RL papers, but I think reading and properly citing these seminal papers is a good practice, as research in the domain did not start last year ;)\n\n\n", "title": "Missing reference to relevant papers"}, "signatures": ["~Olivier_Sigaud1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Olivier_Sigaud1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "CAQL: Continuous Action Q-Learning", "authors": ["Moonkyung Ryu", "Yinlam Chow", "Ross Anderson", "Christian Tjandraatmadja", "Craig Boutilier"], "authorids": ["mkryu@google.com", "yinlamchow@google.com", "rander@google.com", "ctjandra@google.com", "cboutilier@google.com"], "keywords": ["Reinforcement learning (RL)", "DQN", "Continuous control", "Mixed-Integer Programming (MIP)"], "TL;DR": "A general framework of value-based reinforcement learning for continuous control", "abstract": "Reinforcement learning (RL) with value-based methods (e.g., Q-learning) has shown success in a variety of domains such as\ngames and recommender systems (RSs). When the action space is finite, these algorithms implicitly finds a policy by learning the optimal value function, which are often very efficient. \nHowever, one major challenge of extending Q-learning to tackle continuous-action RL problems is that obtaining optimal Bellman backup requires solving a continuous action-maximization (max-Q) problem. While it is common to restrict the parameterization of the Q-function to be concave in actions to simplify the max-Q problem, such a restriction might lead to performance degradation. Alternatively, when the Q-function is parameterized with a generic feed-forward neural network (NN), the max-Q problem can be NP-hard. In this work, we propose the CAQL method which minimizes the Bellman residual using Q-learning with one of several plug-and-play action optimizers. In particular, leveraging the strides of optimization theories in deep NN, we show that max-Q problem can be solved optimally with mixed-integer programming (MIP)---when the Q-function has sufficient representation power, this MIP-based optimization induces better policies and is more robust than counterparts, e.g., CEM or GA, that approximate the max-Q solution. To speed up training of CAQL, we develop three techniques, namely (i) dynamic tolerance, (ii) dual filtering, and (iii) clustering.\nTo speed up inference of CAQL, we introduce the action function that concurrently learns the optimal policy.\nTo demonstrate the efficiency of CAQL we compare it with state-of-the-art RL algorithms on benchmark continuous control problems that have different degrees of action constraints and show that CAQL significantly outperforms policy-based methods in heavily constrained environments.", "pdf": "/pdf/49cfb54fa89345e85f991893706d894f0d953510.pdf", "paperhash": "ryu|caql_continuous_action_qlearning", "_bibtex": "@inproceedings{\nRyu2020CAQL:,\ntitle={CAQL: Continuous Action Q-Learning},\nauthor={Moonkyung Ryu and Yinlam Chow and Ross Anderson and Christian Tjandraatmadja and Craig Boutilier},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BkxXe0Etwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/3b26c57bb8e7c649109748aefb71fd764b7c257e.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BkxXe0Etwr", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504202244, "tmdate": 1576860575172, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper923/Authors", "ICLR.cc/2020/Conference/Paper923/Reviewers", "ICLR.cc/2020/Conference/Paper923/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper923/-/Public_Comment"}}}], "count": 8}