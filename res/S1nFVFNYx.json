{"notes": [{"tddate": null, "ddate": null, "cdate": null, "original": null, "tmdate": 1490028591933, "tcdate": 1490028591933, "number": 1, "id": "SkuVutTog", "invitation": "ICLR.cc/2017/workshop/-/paper89/acceptance", "forum": "S1nFVFNYx", "replyto": "S1nFVFNYx", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Accept", "title": "ICLR committee final decision"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "A Smooth Optimisation Perspective on Training Feedforward Neural Networks", "abstract": "We present a smooth optimisation perspective on training multilayer Feedforward Neural Networks (FNNs) in the supervised learning setting. By characterising the critical point conditions of an FNN based optimisation problem, we identify the conditions to eliminate local optima of the cost function. By studying the Hessian structure of the cost function at the global minima, we develop an approximate Newton FNN algorithm, which demonstrates promising convergence properties.", "pdf": "/pdf/4de0ae5e8be6303f846ff1dba180c7539c05e909.pdf", "paperhash": "shen|a_smooth_optimisation_perspective_on_training_feedforward_neural_networks", "conflicts": ["tum.de"], "authors": ["Hao Shen"], "authorids": ["hao.shen@tum.de"], "keywords": ["Theory", "Supervised Learning", "Optimization"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1490028592435, "id": "ICLR.cc/2017/workshop/-/paper89/acceptance", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "S1nFVFNYx", "replyto": "S1nFVFNYx", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept", "Reject"]}}}, "nonreaders": [], "cdate": 1490028592435}}}, {"tddate": null, "tmdate": 1489392228169, "tcdate": 1489392228169, "number": 2, "id": "rk3vGCmse", "invitation": "ICLR.cc/2017/workshop/-/paper89/public/comment", "forum": "S1nFVFNYx", "replyto": "ryBCmxfjg", "signatures": ["~Hao_Shen1"], "readers": ["everyone"], "writers": ["~Hao_Shen1"], "content": {"title": "Reply to AnonReviewer2", "comment": "The author appreciates the comments from the reviewer and his/her understanding about the challenge in squeezing such a tedious but straightforward analysis in three pages. Apparently, the author didn't succeed it. Nevertheless, these constructive comments will significantly improve the quality of future submissions. \n\nA brief technical reply to the reviewer's comments is that \n1) the condition $rank(P) = T * n_L$ has direct implications about the architecture of the network, and that\n2) the interpretation of the number T is more subtle than the sample size.\n\n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "A Smooth Optimisation Perspective on Training Feedforward Neural Networks", "abstract": "We present a smooth optimisation perspective on training multilayer Feedforward Neural Networks (FNNs) in the supervised learning setting. By characterising the critical point conditions of an FNN based optimisation problem, we identify the conditions to eliminate local optima of the cost function. By studying the Hessian structure of the cost function at the global minima, we develop an approximate Newton FNN algorithm, which demonstrates promising convergence properties.", "pdf": "/pdf/4de0ae5e8be6303f846ff1dba180c7539c05e909.pdf", "paperhash": "shen|a_smooth_optimisation_perspective_on_training_feedforward_neural_networks", "conflicts": ["tum.de"], "authors": ["Hao Shen"], "authorids": ["hao.shen@tum.de"], "keywords": ["Theory", "Supervised Learning", "Optimization"]}, "tags": [], "invitation": {"tddate": null, "tmdate": 1487340676682, "tcdate": 1487340676682, "id": "ICLR.cc/2017/workshop/-/paper89/public/comment", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2017/workshop/paper89/reviewers"], "reply": {"forum": "S1nFVFNYx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/workshop/reviewers", "ICLR.cc/2017/pcs"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1487340676682}}}, {"tddate": null, "nonreaders": null, "tmdate": 1489364473597, "tcdate": 1488939612857, "number": 1, "id": "HkBDqkp5g", "invitation": "ICLR.cc/2017/workshop/-/paper89/official/review", "forum": "S1nFVFNYx", "replyto": "S1nFVFNYx", "signatures": ["ICLR.cc/2017/workshop/paper89/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper89/AnonReviewer1"], "content": {"title": "Interesting approach, needs more support", "rating": "6: Marginally above acceptance threshold", "review": "The paper presents a smooth optimization perspective on feed forward neural networks and discusses a condition where the local optima does not exist in training. Next, by studying Hessian, it develops an approximate newton algorithm and provides an experiment showing the convergence attitude on the four regions classification benchmark.\n\nAlthough the approach is interesting, the paper lacks some important pieces: Theorem 1 relies on the matrix P to be full rank but it does not provide any cases or sufficient conditions when this holds. also Theorem 1 assumes the global minimum w* is reachable but does not provide any insights into when this holds. even a couple of examples would be good. I understand that this is a short version but the author could easily fit this in by reducing the introduction.\n\n", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "A Smooth Optimisation Perspective on Training Feedforward Neural Networks", "abstract": "We present a smooth optimisation perspective on training multilayer Feedforward Neural Networks (FNNs) in the supervised learning setting. By characterising the critical point conditions of an FNN based optimisation problem, we identify the conditions to eliminate local optima of the cost function. By studying the Hessian structure of the cost function at the global minima, we develop an approximate Newton FNN algorithm, which demonstrates promising convergence properties.", "pdf": "/pdf/4de0ae5e8be6303f846ff1dba180c7539c05e909.pdf", "paperhash": "shen|a_smooth_optimisation_perspective_on_training_feedforward_neural_networks", "conflicts": ["tum.de"], "authors": ["Hao Shen"], "authorids": ["hao.shen@tum.de"], "keywords": ["Theory", "Supervised Learning", "Optimization"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489269709862, "id": "ICLR.cc/2017/workshop/-/paper89/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper89/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper89/AnonReviewer1", "ICLR.cc/2017/workshop/paper89/AnonReviewer2"], "reply": {"forum": "S1nFVFNYx", "replyto": "S1nFVFNYx", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper89/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper89/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489269709862}}}, {"tddate": null, "tmdate": 1489269709160, "tcdate": 1489269709160, "number": 2, "id": "ryBCmxfjg", "invitation": "ICLR.cc/2017/workshop/-/paper89/official/review", "forum": "S1nFVFNYx", "replyto": "S1nFVFNYx", "signatures": ["ICLR.cc/2017/workshop/paper89/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper89/AnonReviewer2"], "content": {"title": "Review for smooth optimization perspective paper", "rating": "5: Marginally below acceptance threshold", "review": "The paper follows an interesting angle on optimizing neural networks. I think the write up can be improved considerably. I'm not sure if this is not the effect of having to restrict itself to 3 pages. E.g. after reading the paper, I'm not sure I know how to implement the proposed approximate Newton method in the paper.Section 3 provides some theoretical analysis of the optimization algorithm, but is not clear that those theorems sums to an algorithm that I could code up. \n\nI think (and maybe it is a bit harsh) that even as a workshop submission the paper is not yet clear enough (at least the pdf) and more effort needs to be put in explaining what the different constraints in the theorems mean (and how to achieve them) and in particular how do you get the algorithm.\n\nThe other aspect that I feel uncomfortable with, regarding the approach taken by the authors, is that it seems to me that the constraint of rank(P) = T * n_L, where T is the number of examples, spells out that the network memorized the training set rather than learned. IMHO, while pursuing this quest of either proving that the error surface has no \"bad\" local minima, or removing them is very valuable, is only so if we can get insights of why this works in a case where you *learn* a good solution, i.e. one that generalizes. I would like if the result is somehow independent of the dataset size, and has more something to do with the underlying structure of the data and nature of the network.  ", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "A Smooth Optimisation Perspective on Training Feedforward Neural Networks", "abstract": "We present a smooth optimisation perspective on training multilayer Feedforward Neural Networks (FNNs) in the supervised learning setting. By characterising the critical point conditions of an FNN based optimisation problem, we identify the conditions to eliminate local optima of the cost function. By studying the Hessian structure of the cost function at the global minima, we develop an approximate Newton FNN algorithm, which demonstrates promising convergence properties.", "pdf": "/pdf/4de0ae5e8be6303f846ff1dba180c7539c05e909.pdf", "paperhash": "shen|a_smooth_optimisation_perspective_on_training_feedforward_neural_networks", "conflicts": ["tum.de"], "authors": ["Hao Shen"], "authorids": ["hao.shen@tum.de"], "keywords": ["Theory", "Supervised Learning", "Optimization"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489269709862, "id": "ICLR.cc/2017/workshop/-/paper89/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper89/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper89/AnonReviewer1", "ICLR.cc/2017/workshop/paper89/AnonReviewer2"], "reply": {"forum": "S1nFVFNYx", "replyto": "S1nFVFNYx", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper89/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper89/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489269709862}}}, {"tddate": null, "tmdate": 1488970349419, "tcdate": 1488970349419, "number": 1, "id": "BJBOzPa5e", "invitation": "ICLR.cc/2017/workshop/-/paper89/public/comment", "forum": "S1nFVFNYx", "replyto": "HkBDqkp5g", "signatures": ["~Hao_Shen1"], "readers": ["everyone"], "writers": ["~Hao_Shen1"], "content": {"title": "Reply to AnonReviewer1", "comment": "1) I'd like to thank the reviewer for his/her interest in the proposed approach, as well as these constructive comments. In what follows, I address these points accordingly.\n\n2) The matrix P is a necessary condition to ensure local minima free in training FNNs. One simple case is the FNN architecture with only one hidden layer. If the number of processing units in the hidden layer is equal to the number of patterns, then the matrix P is guaranteed to be of full rank, i.e., $T \\cdot n_{L}$. However, in a general scenario, the rank of matrix P is dependent on the properties of the Khatri-Rao product of identically partitioned matrices. It is worth noticing that a form of column-wise Kronecker product of two matrices is also called the Khatri\u2013Rao product, which is not the case here. How to ensure matrix P to have a full rank in a general setting is still an open question.\n\n3) The assumption of the global minimum being reachable is based on the universal approximation (UA) theorem of FNNs. The UA theorem only guarantees the existence of an FNN, but is unfortunately not constructive. \n\n4) I'd be happy to provide more experiments in briefly addressing the reviewer's comments by reducing the introduction."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "A Smooth Optimisation Perspective on Training Feedforward Neural Networks", "abstract": "We present a smooth optimisation perspective on training multilayer Feedforward Neural Networks (FNNs) in the supervised learning setting. By characterising the critical point conditions of an FNN based optimisation problem, we identify the conditions to eliminate local optima of the cost function. By studying the Hessian structure of the cost function at the global minima, we develop an approximate Newton FNN algorithm, which demonstrates promising convergence properties.", "pdf": "/pdf/4de0ae5e8be6303f846ff1dba180c7539c05e909.pdf", "paperhash": "shen|a_smooth_optimisation_perspective_on_training_feedforward_neural_networks", "conflicts": ["tum.de"], "authors": ["Hao Shen"], "authorids": ["hao.shen@tum.de"], "keywords": ["Theory", "Supervised Learning", "Optimization"]}, "tags": [], "invitation": {"tddate": null, "tmdate": 1487340676682, "tcdate": 1487340676682, "id": "ICLR.cc/2017/workshop/-/paper89/public/comment", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2017/workshop/paper89/reviewers"], "reply": {"forum": "S1nFVFNYx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/workshop/reviewers", "ICLR.cc/2017/pcs"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1487340676682}}}, {"tddate": null, "replyto": null, "nonreaders": null, "ddate": null, "tmdate": 1487793789435, "tcdate": 1487340675686, "number": 89, "id": "S1nFVFNYx", "invitation": "ICLR.cc/2017/workshop/-/submission", "forum": "S1nFVFNYx", "signatures": ["~Hao_Shen1"], "readers": ["everyone"], "content": {"TL;DR": "", "title": "A Smooth Optimisation Perspective on Training Feedforward Neural Networks", "abstract": "We present a smooth optimisation perspective on training multilayer Feedforward Neural Networks (FNNs) in the supervised learning setting. By characterising the critical point conditions of an FNN based optimisation problem, we identify the conditions to eliminate local optima of the cost function. By studying the Hessian structure of the cost function at the global minima, we develop an approximate Newton FNN algorithm, which demonstrates promising convergence properties.", "pdf": "/pdf/4de0ae5e8be6303f846ff1dba180c7539c05e909.pdf", "paperhash": "shen|a_smooth_optimisation_perspective_on_training_feedforward_neural_networks", "conflicts": ["tum.de"], "authors": ["Hao Shen"], "authorids": ["hao.shen@tum.de"], "keywords": ["Theory", "Supervised Learning", "Optimization"]}, "writers": [], "details": {"replyCount": 5, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1487690420000, "tmdate": 1484242559574, "id": "ICLR.cc/2017/workshop/-/submission", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1495466420000, "cdate": 1484242559574}}}], "count": 6}