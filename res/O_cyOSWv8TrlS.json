{"notes": [{"ddate": null, "legacy_migration": true, "tmdate": 1392694980000, "tcdate": 1392694980000, "number": 8, "id": "ykGzyhr0mas8E", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "O_cyOSWv8TrlS", "replyto": "O_cyOSWv8TrlS", "signatures": ["David Reichert"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "We thank the reviewers for the fair reviews. For simplicity, below we refer to 'Anonymous 4c84', 'Anonymous ec9e', and 'Anonymous 0ae5' as reviewer 1-3, respectively.\r\n\r\nWe agree with the overall assessment of the reviewers. This is early work that intends to communicate a potentially powerful idea, backed up by simple experiments. There are several avenues for extending it towards principled theoretical frameworks and to address e.g. the need for learning, and we were hoping for feedback from the community to that end.\r\n\r\nPerhaps the main issue, as raised by reviewer 1, is whether this work would be better suited to a short workshop paper, given its early stage. This is a fair point. At the same time, given the amount of material we had, and given the solid 'story' that we wanted to communicate, it really didn't make sense to us not to write a full-length paper. For a conference track submission, we think the strengths of the work, such as its originality (to this audience) and quality of presentation, could overcome its shortcomings (but of course that's ultimately up to the reviewers and the chair to decide). We can also address several of the concrete concerns raised by the reviewers, and will do so in the following.\r\n\r\n\r\n****** Changes to paper ******\r\n\r\nWe are in the process of uploading an updated paper (v4, ETA Feb 18 7pm EST) to the arXiv, which takes the reviewers' comments into account. We clarified notation in the main text (now slightly breaking the 9 pages limit, which should be acceptable at this point) and wording in appendix B. We also added some of the main issues raised by the reviewers and our discussion thereof as appendix C, whenever it made sense to expand on points that were only briefly touched on in the main text.\r\n\r\n\r\n****** Reviewer 1 & Reviewer 2 ******\r\n\r\n'It is not clear that the segmentation is working for the bars experiment because multiple bars are colored by the same phase. What is the goal here? that each bar has a different, unique phase value? Does the underlying phase distribution effectively partition the bars? '\r\n\r\n'there is an issue of phase resolution: it appears that four different bars are all coded the same shade of green. Is this a problem? A readout mechanism might be confused and judge all these bars to be one object, even though bars occur independently in the training data.'\r\n\r\nThis is a very important issue that we are still considering. It is perhaps an issue more generally with the underlying biological theories rather than just our specific approach. As we noted in the paper, some theories pose that a limit on how many discrete objects can be represented in an oscillation cycle, without interference, explains certain capacity limits in cognition. The references we cited (Jensen & Lisman, 2005; Fell & Axmacher, 2011) refer to working memory as an example (often 4-7 items; note the number of peaks in Figure 3c -- obviously this needs more quantitative analysis). We would posit that, more generally, analysis of visual scenes requiring the concurrent separation of multiple objects is limited accordingly (one might call this a prediction -- or a `postdiction'? -- of our model). The question is then, how does the brain cope with this limitation? As usual in the face of perceptual capacity limits, the solution likely would involve attentional mechanisms. Such mechanisms might dynamically change the grouping of sensory inputs depending on task and context, such as whether questions are asked about individual parts and fine detail, or object groups and larger patterns. In the bars example, one might perceive the bars as a single group or texture, or focus on individual bars as capacity allows, perhaps relegating the rest of the image to a general background. \r\n\r\nDynamically changing phase assignments according to context, through top-down attentional input, should, in principle, be possible within the proposed framework: this is similar to grouping according to parts or wholes with top-down input, as in the experiment of Section 3.2.\r\n\r\n\r\n****** Reviewer 1 ******\r\n\r\nRegarding the similarity to Rao et al: as we've acknowledged in the paper, the work is similar in several points (we arrived at our framework and results independently and were not aware of Rao et al.'s work initially -- we do not think the latter is particularly well known in this community). However, we would want to counter the impression that our work does not provide additional contributions. First of all, to clarify the issue of training on multiple objects: in Rao et al.'s work, the training data consists of a small number of fixed 8x8 images (N <= 16 images *in total* for a dataset), containing simple patterns (one example has 4 small images with two faces instead). To demonstrate binding by synchrony, two of these patterns are superimposed during test time. We believe that going beyond this extremely constrained task, in particular showing that the binding can work when trained and tested on multiple objects, on multiple datasets including MNIST containing thousands of (if simple) images, is a valid contribution from our side, which is not diminished by the fact that this result relies on the capability of the DBM (indeed, showing that this works with the DBM is itself a contribution as it might tell us something interesting about the kind of representations learned by a DBM that is not usually made explicit). \r\n\r\nSimilarly, as far as we can see, Rao et al. do not discuss the gating aspect at all (as we mentioned in our paper), nor the specific issues with excitation and inhibition (Section 2.1) that we pointed out as motivation for using both classic and synchrony terms. Lastly, the following issues are addressed in our experiments only: network behavior on more than two objects; synchronization for objects that are not contiguous in the input images, as well as part vs. whole effects (Section 3.2); decoding distributed hidden representations according to phase (Section 3.3; in particular, it seems to be the case that Rao et al.s networks had a localist (single object<->single unit) representation in the top hidden layer in the majority of cases).\r\n\r\n'the introduction of phase is done in an ad-hoc way, without real justification from probabilistic goals [...]'\r\n\r\nWe agree that framing our approach as a proper probabilistic model would be helpful and perhaps more convincing to this audience (e.g. using an extension of the DUBM of Zemel et al., 1995, as discussed in the paper). At the same time, we think there is value to presenting the heuristic as is, based on a specific neuronal activation function, to emphasize that this idea could find application in neural networks more generally, not only those with a probabilistic interpretation/Boltzmann machines (that our approach is divorced from any one particular model is another difference when compared to Rao et al.'s work). In particular, we have performed exploratory experiments with networks trained (pretrained as real-valued nets or trained as complex-valued nets) with backprop, including (convolutional) feed-forward neural networks, autoencoders, or recurrent networks, as well as a biological model of lateral interactions in V1. We agree with the reviewer that a more rigorous mathematical and quantitative analysis is needed in any case.\r\n\r\n'the results in the appendix appear to indicate that the approach is not working very well in general, and the best results are the ones shown in the main text.' \r\n\r\nWe are not sure what exactly the reviewer is referring to here. If it is our statement that our approach of using pretrained real-valued networks does not always work, then yes, that is an issue that needs to be addressed. However, we should perhaps clarify that what we meant is: for some datasets and training parameters, models did not perform well; in cases where they did perform reasonably well however, that performance was relatively consistent across images, and the results show representative examples from those models. If, on the other hand, the reviewer is referring to results in the supplementary figure supposedly looking different from the figures in the main text, then no, other than perhaps with the schematic overview in Figure 6, we did not purposefully cherry-pick nicer looking results to display (most of the figures were actually cropped from the same larger figures simply for space reasons).\r\n\r\n'How is the phase distribution segmented? Phase is a continuous variable, and the segmentation/partitioning seems to be done by hand for the examples. This needs to be addressed.'\r\n\r\nPartitioning was done with k-means, not by hand...? Other options are possible (also depending on whether the aim is a principled machine learning framework or addressing questions about the brain with a biological model). It is true that phase is a continuous variable, however, our results indicate that there is a tendency to form discrete phase clusters, in line with biological models (e.g. the one of Miconi & VanRullen).\r\n\r\n'Also, what about the overlaps of the bars? these areas seem to be mis- or ambiguously labeled. is this a bug or a feature?' \r\n\r\nThis is more of a problem with the task itself being ill-defined on binary images, where an overlapping pixel cannot really be meaningfully said to belong to either object alone (as there is no occlusion as such). We plan to use (representations of) real-valued images in the future.\r\n\r\n\r\n****** Reviewer 2 ******\r\n\r\n'Comment: The text leading up to equation 1 is confusing regarding z. Is it an output or an input? It doesn't seem like we're dealing with a dynamical system, and the input was called x in the paragraph above. '\r\n\r\nWith x and z we just refer to, respectively, real-valued and complex-valued states in general. There are several notions of input here: the input (image) to the overall network, the units/states providing input to a specific unit, the total 'post-synaptic' input w . z, and the term that is ultimately used as input to the activation function with a real-valued domain (e.g. |w .  z|). We have attempted to clarify this in this revision of the paper by introducing some additional variables (perhaps the reviewer could check whether Section 2.1 is clearer now).\r\n\r\n'Comment: the use of |x| in equation 2 is confusing because presumably |w . x| is a vector norm whereas in w . |x| it denotes elementwise magnitude of the complex elements of x. Right?'\r\n\r\nAssuming the reviewer meant to write z not x: No, |w . z| is also the magnitude, and w . z happens to be a complex scalar; this is the input to a single unit, thus both w and z are vectors and this is a dot product. This should be clearer with the new notation.\r\n\r\n'Comment: the authors mention that the two terms in f() can be weighted, but don't include those weights in Eq. 2, as I have done above (alpha, \beta).'\r\n\r\nWe simply left this out for simplicity, because we do not actually explore unbalanced weightings in this paper and didn't want to introduce unnecessary notation.\r\n\r\n'A [conventional, real-valued] DBM was trained on small pictures with horizontal and vertical bars, and then 'converted' to a complex-valued network (and was the activation function changed to the one from Eq. 2? What does that mean in terms of inference in the DBM?)'\r\n\r\nYes the activation function changed; we essentially use the normal DBM training as a form of pretraining for the final, complex-valued architecture. The resulting neural network is likely not exactly to be interpreted as a probabilistic model. However, if such an interpretation is desired, our understanding is that running the network could be seen as an approximation of inference in a suitably extended DUBM (by adding an off state and a classic term; refer to Zemel et al., 1995, for comparison). For our experiments, we used two procedures (with similar outcomes) in analogy to inference in a DBM: either sampling a binary output magnitude from f(), or letting f() determine the output magnitude deterministically; the output phase was always set to the phase of the total input. The first procedure is similar to inference in such an extended DUBM, but, rather than sampling from a circular normal distribution on the unit circle when the unit is on, we simply take the mode of that distribution. The second procedure should qualitatively correspond to mean-field inference in an extended DUBM (see Eqs. 9 and 10 in the DUBM paper), using a slightly different output function.\r\n\r\nBy the way: perhaps we could have framed our work in such terms to begin with, but in a way that obscures what our original line of thinking was.\r\n\r\n'[...] 'sampling' (is this actually sampling from a probability distribution?)'\r\n\r\nNo, not exactly. We actually only used the term 'sampling' in the standard, real-valued case, other than in the caption of Figure 3. We will fix the latter.\r\n\r\n'Figure 3 illustrates what happens after 100 iterations of sampling, what happens after more iterations? Do the co-incidentally green bars change color independently of one another?'\r\n\r\nPhase assignments appear to be stable (see the supplementary movies), though we did not analyze this in detail. It should also be noted that the overall network is invariant to absolute phase, so only the relative phases matter.\r\n\r\n'I would strongly suggest that the authors upload their Pylearn2 code so that others can reproduce the effects presented in this paper [...]'\r\n\r\nWe are happy to publish the code either way, but it would unfortunately take some extra work to put it into a form that is accessible to others. We will do so if the paper gets accepted as a proper conference paper.\r\n\r\n\r\n****** Reviewer 3 ******\r\n\r\nUnfortunately, we certainly can't lay claim to being the first to explore this idea in a computational framework (see the references cited), though we are perhaps the first to make a connection to the types of deep networks that have recently been employed in the deep learning community (DBMs in this case; also, as stated, the framework could in principle be applied to other deep nets, such as ConvNets). Apart from that, we are of course happy to agree that this a fascinating idea and that it seems worthwhile to bring it to the attention of the ICLR community."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Neuronal Synchrony in Complex-Valued Deep Networks", "decision": "submitted, no decision", "abstract": "Deep learning has recently lead to great successes in tasks such as image recognition (e.g Krizhevsky et al., 2012). However, deep networks are still outmatched by the power and versatility of the brain, perhaps in part due to the richer neuronal computations available to real cortical circuits. The challenge is to identify which neural mechanisms are relevant, and to find suitable abstractions to model them. Here, we show how aspects of spike timing, long hypothesized to play a crucial role in cortical information processing, could be incorporated into deep networks to build richer, versatile deep representations. \r\nWe introduce a neural network formulation based on complex-valued neuronal units that is not only biologically meaningful but also amenable to a variety of deep learning frameworks. Here, units are attributed both a firing rate and a phase, the latter indicating properties of spike timing. We show how this formulation qualitatively captures several aspects thought to be related to neuronal synchrony, including gating of information processing and dynamic binding of distributed object representations. Focusing on the latter aspect, we demonstrate the potential of the approach in several simple experiments. Thus, synchrony could implement a flexible mechanism that fulfills multiple functional roles in deep networks.", "pdf": "https://arxiv.org/abs/1312.6115", "paperhash": "reichert|neuronal_synchrony_in_complexvalued_deep_networks", "keywords": [], "conflicts": [], "authors": ["David Reichert", "Thomas Serre"], "authorids": ["david_reichert@brown.edu", "thomas_serre@brown.edu"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392377100000, "tcdate": 1392377100000, "number": 7, "id": "9VimVyzHOKV8h", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "O_cyOSWv8TrlS", "replyto": "O_cyOSWv8TrlS", "signatures": ["anonymous reviewer ec9e"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Neuronal Synchrony in Complex-Valued Deep Networks", "review": "I find this paper deeply fascinating.  It illustrates - I believe for the first time - the utility of binding by synchrony in a deep network architecture.  Although the general idea of binding by synchrony is an old one, it is mostly a vague idea that has never, to my mind, been put into a concrete computational framework.  Here, the authors propose that the phase of oscillation in neural ensembles acts as a kind of 'label' for objects being represented in a distributed fashion.  That is, no single unit represents an object, but when an object is presented to the network it activates features at each level which synchronize via two-way communication between levels.  The result is a kind of segmentation of objects within the image.  This could be useful for example in separating objects from clutter, or possibly in resolving occlusion (although that has not been demonstrated here). \r\n\r\nAlthough mostly toy examples of patterns are used, I believe that this paper taps into a powerful idea, and that it will be of high interesting to the ICLR community, and certainly to the neuroscience community."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Neuronal Synchrony in Complex-Valued Deep Networks", "decision": "submitted, no decision", "abstract": "Deep learning has recently lead to great successes in tasks such as image recognition (e.g Krizhevsky et al., 2012). However, deep networks are still outmatched by the power and versatility of the brain, perhaps in part due to the richer neuronal computations available to real cortical circuits. The challenge is to identify which neural mechanisms are relevant, and to find suitable abstractions to model them. Here, we show how aspects of spike timing, long hypothesized to play a crucial role in cortical information processing, could be incorporated into deep networks to build richer, versatile deep representations. \r\nWe introduce a neural network formulation based on complex-valued neuronal units that is not only biologically meaningful but also amenable to a variety of deep learning frameworks. Here, units are attributed both a firing rate and a phase, the latter indicating properties of spike timing. We show how this formulation qualitatively captures several aspects thought to be related to neuronal synchrony, including gating of information processing and dynamic binding of distributed object representations. Focusing on the latter aspect, we demonstrate the potential of the approach in several simple experiments. Thus, synchrony could implement a flexible mechanism that fulfills multiple functional roles in deep networks.", "pdf": "https://arxiv.org/abs/1312.6115", "paperhash": "reichert|neuronal_synchrony_in_complexvalued_deep_networks", "keywords": [], "conflicts": [], "authors": ["David Reichert", "Thomas Serre"], "authorids": ["david_reichert@brown.edu", "thomas_serre@brown.edu"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392059880000, "tcdate": 1392059880000, "number": 6, "id": "HkA9HPn1mY7Ol", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "O_cyOSWv8TrlS", "replyto": "O_cyOSWv8TrlS", "signatures": ["anonymous reviewer 0ae5"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Neuronal Synchrony in Complex-Valued Deep Networks", "review": "SUMMARY                                                                                       \r\n  \r\nThe paper 'Neural Synchrony in Complex-Valued Deep Networks' tackles the important question of how it is that neural representations encode information about multiple objects independently and simultaneously.\r\nThe idea that the relative phase of periodic neural responses has been in circulation for some time (and this paper provides a good overview of relevant literature), but to date the principle has not gained traction in the pattern recognition side of neural modeling. This paper aims to change that by showing that a complex-valued Deep Boltzmann Machine can naturally segment images (in some simple synthetic cases) according to the various visible objects, through the phase of latent responses. \r\n \r\nThe basis for the technical contribution of this paper is a novel response function.  The [complex-valued] response function described by equations 1 and 2 is a function of a complex-valued weight vector applied to a complex-valued feature vector. Output z_j = r_j e^{i \theta_j} of each model neuron is determined by arg(z_j) = arg(w . x)\r\n  \r\n    f( alpha  |sum_j w_j . x_j | + \beta sum_j (w_j  |x_j|) )\r\n \r\nComment: The text leading up to equation 1 is confusing regarding z. Is it an output or an input?  It doesn't seem like we're dealing with a dynamical system, and the input was called x in the paragraph above.\r\n  \r\nComment: the use of |x| in equation 2 is confusing because presumably |w . x| is a vector norm whereas in w . |x| it denotes elementwise magnitude of the complex elements of x. Right?\r\n \r\nComment: the authos mention that the two terms in f() can be weighted, but don't include those weights in Eq. 2, as I have done above (alpha, \beta).\r\n  \r\nWhy use this function? The authors make an intuitive argument in the text that these two terms capture salient aspects of a more detailed spiking network based on Hodgkin-Huxley neurons, and Figure 1 illutrates the effect quantitatively for a specific, simple 2-to-1 feedforward network of rhythmic neurons. The value of this transfer function as a surrogate for detailed compartmental models is interesting, but is not the focus of the remainder of the paper.\r\n  \r\n \r\nThe paper's section 3 'Experiments: the case of binding by synchrony' was somewhat difficult for me to understand. A [conventional, real-valued] DBM was trained on small pictures with horizontal and vertical bars, and then 'converted' to a complex-valued network (and was the activation function changed to the one from Eq. 2? What does that mean in terms of inference in the DBM?) It was found that when clamping the visible-unit magnitudes to a particular picture, and 'sampling' (is this actually sampling from a probability distribution?) their phase and the hidden units' magnitude and phase, that there were groups of hidden units with phases that lined up with particular bars. This is good because it suggests a means of teasing apart the DBM's latent representation into groups that are 'working together' to represent something independently from other groups. (I wanted to see some sort of control trial, showing that a plain old real-valued DBM could not achieve the same thing, but I can't really think of the right thing to try.)\r\n \r\nThe demonstration in Figure 3 shows that already with this set of bars, that there is an issue of phase resolution: it appears that four different bars are all coded the same shade of green. Is this a problem? A readout mechanism might be confused and judge all these bars to be one object, even though bars occur independently in the training data. Figure 3 illustrates what happens after 100 iterations of sampling, what happens after more iterations? Do the co-incidentally green bars change color independently of one another?\r\n\r\nOverall, this research is highly relevant to the aims of the ICLR conference. It is at an early stage of development, in that no learning algorithm has been adapted to work with these complex-valued neurons (although the authors might consider adapting the ssRBM), the images used in the experiments are simple and synthetic, and the authors themselves lament that 'conversion' of a DBM is unreliable. Still, the idea of phase-based coding has a lot of potential, and it is worth exploring.  This paper would be an important step in that process. I would strongly suggest that the authors upload their Pylearn2 code so that others can reproduce the effects presented in this paper, especially if training and conversion of DBMs is as unreliable as they suggest.\r\n \r\n \r\nNOVELTY AND QUALITY\r\n \r\n- the use of complex-valued phase to perform segmentation in a DBM is novel\r\n\r\n- quality of presentation is very good\r\n \r\n \r\nPRO & CON\r\n \r\npro: phase-based segmentation is an intriguing idea from theoretical neuroscience, it's great to see it put to the test in engineering terms\r\n \r\ncon: the stimuli are quite simple compared with other deep learning and vision applications\r\n\r\ncon: the model is at an early proof-of-concept stage\r\n\r\ncon: no natural learning algorithm yet for the model"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Neuronal Synchrony in Complex-Valued Deep Networks", "decision": "submitted, no decision", "abstract": "Deep learning has recently lead to great successes in tasks such as image recognition (e.g Krizhevsky et al., 2012). However, deep networks are still outmatched by the power and versatility of the brain, perhaps in part due to the richer neuronal computations available to real cortical circuits. The challenge is to identify which neural mechanisms are relevant, and to find suitable abstractions to model them. Here, we show how aspects of spike timing, long hypothesized to play a crucial role in cortical information processing, could be incorporated into deep networks to build richer, versatile deep representations. \r\nWe introduce a neural network formulation based on complex-valued neuronal units that is not only biologically meaningful but also amenable to a variety of deep learning frameworks. Here, units are attributed both a firing rate and a phase, the latter indicating properties of spike timing. We show how this formulation qualitatively captures several aspects thought to be related to neuronal synchrony, including gating of information processing and dynamic binding of distributed object representations. Focusing on the latter aspect, we demonstrate the potential of the approach in several simple experiments. Thus, synchrony could implement a flexible mechanism that fulfills multiple functional roles in deep networks.", "pdf": "https://arxiv.org/abs/1312.6115", "paperhash": "reichert|neuronal_synchrony_in_complexvalued_deep_networks", "keywords": [], "conflicts": [], "authors": ["David Reichert", "Thomas Serre"], "authorids": ["david_reichert@brown.edu", "thomas_serre@brown.edu"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391115900000, "tcdate": 1391115900000, "number": 5, "id": "QuNxu31HJPlct", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "O_cyOSWv8TrlS", "replyto": "O_cyOSWv8TrlS", "signatures": ["anonymous reviewer 4c84"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Neuronal Synchrony in Complex-Valued Deep Networks", "review": "The paper describes a method to augment pre-trained DBMs with phase variables and shows some demonstrations of binding, segmentation, and partitioning (based on latent variables).\r\n\r\nPros:\r\nThe paper is very well written and introduces a number of concepts clearly.\r\nPhase is a curious neurophysiological phenomena and is deserving of modeling that addresses the representational consequences/implications.\r\nI could see how this ad-hoc approach could be used to understand DNNs (like extended Zeiler and Fergus' visualization work).\r\nThe paper may educate the ICLR community on the binding problem and proposals from the neuroscience community that argue for phase as a solution to this problem.\r\n\r\nCons:\r\nA major issue with the described work is its similarity to the work for Rao and colleagues.  The authors provide some comments about how their work is distinguished.  However these are merely rhetorical (your approach is general and theirs is not) or actually contributions that are not made by this paper but by previous work (DBMs can be trained to learn representations of multiple-simultaneously presented object/patterns).\r\n\r\nThe two major limitations of the paper in its current form:\r\n1. the introduction of phase is done in an ad-hoc way, without real justification from probabilistic goals.  The authors appear surprised that their hack worked at all.  It seems the more rigorous approach would be to either introduce phase as a proper latent variable and train the network to optimize the distribution to match the data distribution (the usual approach to modeling), or to explain more rigorously why this ad hoc extension does not interfere with the network (however it is not even clear from the experiments that the ad-hoc model preserves the properties of the original network).  A more rigorous mathematical approach might reveal that the basins of attraction are preserved with the introduction of phase, or that the phase variables are independent of the amplitude variables (I believe they are not).\r\n2. The results of the experiments are mostly just pictures and lack quantitative assessment or any controls.  The resulting work provides a demonstration of the phase idea for binding/grouping/segmentation, which are not new ideas (although they are probably new ideas to the ICLR community).  Furthermore, the results in the appendix appear to indicate that the approach is not working very well in general, and the best results are the ones shown in the main text.\r\n\r\nThe procedures avoid some obvious issues:\r\nHow is the phase distribution segmented?  Phase is a continuous variable, and the segmentation/partitioning seems to be done by hand for the examples.  This needs to be addressed.\r\nIt is not clear that the segmentation is working for the bars experiment because multiple bars are colored by the same phase.  What is the goal here?  that each bar has a different, unique phase value?  Does the underlying phase distribution effectively partition the bars?  Also, what about the overlaps of the bars?  these areas seem to be mis- or ambiguously labeled.  is this a bug or a feature?\r\n\r\nOverall, I think this is an interesting direction of research and the exposition is top notch, however the underlying work falls short of some obvious extensions and methodological rigor.  I think this would make for a nice workshop paper, so that it could receive some feedback from the community and educate the community of the phase binding idea, but it lacks some ingredients for a conference paper.\r\n\r\nSome other relevant references you might want to include:\r\nS. Jankowski, et al. 1996. Complex-valued multistate neural associative memory.\r\nT. Nitta. 2009. Complex-Valued Neural Networks: Utilizing High-Dimensional Parameters.\r\nC. Cadieu & K. Koepsell 2010. Modeling Image Structure with Factorized Phase-Coupled Boltzmann Machines."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Neuronal Synchrony in Complex-Valued Deep Networks", "decision": "submitted, no decision", "abstract": "Deep learning has recently lead to great successes in tasks such as image recognition (e.g Krizhevsky et al., 2012). However, deep networks are still outmatched by the power and versatility of the brain, perhaps in part due to the richer neuronal computations available to real cortical circuits. The challenge is to identify which neural mechanisms are relevant, and to find suitable abstractions to model them. Here, we show how aspects of spike timing, long hypothesized to play a crucial role in cortical information processing, could be incorporated into deep networks to build richer, versatile deep representations. \r\nWe introduce a neural network formulation based on complex-valued neuronal units that is not only biologically meaningful but also amenable to a variety of deep learning frameworks. Here, units are attributed both a firing rate and a phase, the latter indicating properties of spike timing. We show how this formulation qualitatively captures several aspects thought to be related to neuronal synchrony, including gating of information processing and dynamic binding of distributed object representations. Focusing on the latter aspect, we demonstrate the potential of the approach in several simple experiments. Thus, synchrony could implement a flexible mechanism that fulfills multiple functional roles in deep networks.", "pdf": "https://arxiv.org/abs/1312.6115", "paperhash": "reichert|neuronal_synchrony_in_complexvalued_deep_networks", "keywords": [], "conflicts": [], "authors": ["David Reichert", "Thomas Serre"], "authorids": ["david_reichert@brown.edu", "thomas_serre@brown.edu"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1390026840000, "tcdate": 1390026840000, "number": 3, "id": "bX1QeFiTtMb-r", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "O_cyOSWv8TrlS", "replyto": "O_cyOSWv8TrlS", "signatures": ["Sainbayar Sukhbaatar"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "I found this paper very interesting and inspiring."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Neuronal Synchrony in Complex-Valued Deep Networks", "decision": "submitted, no decision", "abstract": "Deep learning has recently lead to great successes in tasks such as image recognition (e.g Krizhevsky et al., 2012). However, deep networks are still outmatched by the power and versatility of the brain, perhaps in part due to the richer neuronal computations available to real cortical circuits. The challenge is to identify which neural mechanisms are relevant, and to find suitable abstractions to model them. Here, we show how aspects of spike timing, long hypothesized to play a crucial role in cortical information processing, could be incorporated into deep networks to build richer, versatile deep representations. \r\nWe introduce a neural network formulation based on complex-valued neuronal units that is not only biologically meaningful but also amenable to a variety of deep learning frameworks. Here, units are attributed both a firing rate and a phase, the latter indicating properties of spike timing. We show how this formulation qualitatively captures several aspects thought to be related to neuronal synchrony, including gating of information processing and dynamic binding of distributed object representations. Focusing on the latter aspect, we demonstrate the potential of the approach in several simple experiments. Thus, synchrony could implement a flexible mechanism that fulfills multiple functional roles in deep networks.", "pdf": "https://arxiv.org/abs/1312.6115", "paperhash": "reichert|neuronal_synchrony_in_complexvalued_deep_networks", "keywords": [], "conflicts": [], "authors": ["David Reichert", "Thomas Serre"], "authorids": ["david_reichert@brown.edu", "thomas_serre@brown.edu"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1390026840000, "tcdate": 1390026840000, "number": 4, "id": "au6kl4HEAJuS5", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "O_cyOSWv8TrlS", "replyto": "O_cyOSWv8TrlS", "signatures": ["Sainbayar Sukhbaatar"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "I found this paper very interesting and inspiring."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Neuronal Synchrony in Complex-Valued Deep Networks", "decision": "submitted, no decision", "abstract": "Deep learning has recently lead to great successes in tasks such as image recognition (e.g Krizhevsky et al., 2012). However, deep networks are still outmatched by the power and versatility of the brain, perhaps in part due to the richer neuronal computations available to real cortical circuits. The challenge is to identify which neural mechanisms are relevant, and to find suitable abstractions to model them. Here, we show how aspects of spike timing, long hypothesized to play a crucial role in cortical information processing, could be incorporated into deep networks to build richer, versatile deep representations. \r\nWe introduce a neural network formulation based on complex-valued neuronal units that is not only biologically meaningful but also amenable to a variety of deep learning frameworks. Here, units are attributed both a firing rate and a phase, the latter indicating properties of spike timing. We show how this formulation qualitatively captures several aspects thought to be related to neuronal synchrony, including gating of information processing and dynamic binding of distributed object representations. Focusing on the latter aspect, we demonstrate the potential of the approach in several simple experiments. Thus, synchrony could implement a flexible mechanism that fulfills multiple functional roles in deep networks.", "pdf": "https://arxiv.org/abs/1312.6115", "paperhash": "reichert|neuronal_synchrony_in_complexvalued_deep_networks", "keywords": [], "conflicts": [], "authors": ["David Reichert", "Thomas Serre"], "authorids": ["david_reichert@brown.edu", "thomas_serre@brown.edu"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1389649440000, "tcdate": 1389649440000, "number": 1, "id": "0oL8oJVsYd0Xl", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "O_cyOSWv8TrlS", "replyto": "O_cyOSWv8TrlS", "signatures": ["David Reichert"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "Thanks for the comment.\r\n\r\nJust to clarify, in the broader context there is plenty of relevant work that we did not discuss, due to limited space (we only discussed closely related work based on complex-valued nets). This includes models using coupled oscillators for segmentation. In particular, see also (and references therein):\r\n\r\nYu, G., & Slotine, J.-J. (2009). Visual Grouping by Neural Oscillator Networks. IEEE Transactions on Neural Networks, 20(12), 1871\u20131884. doi:10.1109/TNN.2009.2031678"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Neuronal Synchrony in Complex-Valued Deep Networks", "decision": "submitted, no decision", "abstract": "Deep learning has recently lead to great successes in tasks such as image recognition (e.g Krizhevsky et al., 2012). However, deep networks are still outmatched by the power and versatility of the brain, perhaps in part due to the richer neuronal computations available to real cortical circuits. The challenge is to identify which neural mechanisms are relevant, and to find suitable abstractions to model them. Here, we show how aspects of spike timing, long hypothesized to play a crucial role in cortical information processing, could be incorporated into deep networks to build richer, versatile deep representations. \r\nWe introduce a neural network formulation based on complex-valued neuronal units that is not only biologically meaningful but also amenable to a variety of deep learning frameworks. Here, units are attributed both a firing rate and a phase, the latter indicating properties of spike timing. We show how this formulation qualitatively captures several aspects thought to be related to neuronal synchrony, including gating of information processing and dynamic binding of distributed object representations. Focusing on the latter aspect, we demonstrate the potential of the approach in several simple experiments. Thus, synchrony could implement a flexible mechanism that fulfills multiple functional roles in deep networks.", "pdf": "https://arxiv.org/abs/1312.6115", "paperhash": "reichert|neuronal_synchrony_in_complexvalued_deep_networks", "keywords": [], "conflicts": [], "authors": ["David Reichert", "Thomas Serre"], "authorids": ["david_reichert@brown.edu", "thomas_serre@brown.edu"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1389121680000, "tcdate": 1389121680000, "number": 2, "id": "AAJOKj49HYATj", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "O_cyOSWv8TrlS", "replyto": "O_cyOSWv8TrlS", "signatures": ["Tapani Raiko"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "Thanks for a very interesting paper! I think this approach will have significant impact in future, although it is not ripe for even quantitative analysis yet.\r\n\r\nI wanted to point out our related work, that I have been planning to continue in a similar direction (as mentioned in our Discussion section):\r\nT. Raiko and H. Valpola. Chapter 7: Oscillatory Neural Network for Image Segmentation with Biased Competition for Attention. In From Brains to Systems: Brain-Inspired Cognitive Systems 2010 (ISBN 978-1-4614-0163-6), Advances in Experimental Medicine and Biology, Volume 718, pages 75-86, Springer New York, 2011. http://users.ics.aalto.fi/praiko/papers/bics_chapter.pdf"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Neuronal Synchrony in Complex-Valued Deep Networks", "decision": "submitted, no decision", "abstract": "Deep learning has recently lead to great successes in tasks such as image recognition (e.g Krizhevsky et al., 2012). However, deep networks are still outmatched by the power and versatility of the brain, perhaps in part due to the richer neuronal computations available to real cortical circuits. The challenge is to identify which neural mechanisms are relevant, and to find suitable abstractions to model them. Here, we show how aspects of spike timing, long hypothesized to play a crucial role in cortical information processing, could be incorporated into deep networks to build richer, versatile deep representations. \r\nWe introduce a neural network formulation based on complex-valued neuronal units that is not only biologically meaningful but also amenable to a variety of deep learning frameworks. Here, units are attributed both a firing rate and a phase, the latter indicating properties of spike timing. We show how this formulation qualitatively captures several aspects thought to be related to neuronal synchrony, including gating of information processing and dynamic binding of distributed object representations. Focusing on the latter aspect, we demonstrate the potential of the approach in several simple experiments. Thus, synchrony could implement a flexible mechanism that fulfills multiple functional roles in deep networks.", "pdf": "https://arxiv.org/abs/1312.6115", "paperhash": "reichert|neuronal_synchrony_in_complexvalued_deep_networks", "keywords": [], "conflicts": [], "authors": ["David Reichert", "Thomas Serre"], "authorids": ["david_reichert@brown.edu", "thomas_serre@brown.edu"]}, "tags": [], "invitation": {}}}, {"replyto": null, "ddate": null, "legacy_migration": true, "tmdate": 1387824240000, "tcdate": 1387824240000, "number": 38, "id": "O_cyOSWv8TrlS", "invitation": "ICLR.cc/2014/conference/-/submission", "forum": "O_cyOSWv8TrlS", "signatures": ["david_reichert@brown.edu"], "readers": ["everyone"], "content": {"title": "Neuronal Synchrony in Complex-Valued Deep Networks", "decision": "submitted, no decision", "abstract": "Deep learning has recently lead to great successes in tasks such as image recognition (e.g Krizhevsky et al., 2012). However, deep networks are still outmatched by the power and versatility of the brain, perhaps in part due to the richer neuronal computations available to real cortical circuits. The challenge is to identify which neural mechanisms are relevant, and to find suitable abstractions to model them. Here, we show how aspects of spike timing, long hypothesized to play a crucial role in cortical information processing, could be incorporated into deep networks to build richer, versatile deep representations. \r\nWe introduce a neural network formulation based on complex-valued neuronal units that is not only biologically meaningful but also amenable to a variety of deep learning frameworks. Here, units are attributed both a firing rate and a phase, the latter indicating properties of spike timing. We show how this formulation qualitatively captures several aspects thought to be related to neuronal synchrony, including gating of information processing and dynamic binding of distributed object representations. Focusing on the latter aspect, we demonstrate the potential of the approach in several simple experiments. Thus, synchrony could implement a flexible mechanism that fulfills multiple functional roles in deep networks.", "pdf": "https://arxiv.org/abs/1312.6115", "paperhash": "reichert|neuronal_synchrony_in_complexvalued_deep_networks", "keywords": [], "conflicts": [], "authors": ["David Reichert", "Thomas Serre"], "authorids": ["david_reichert@brown.edu", "thomas_serre@brown.edu"]}, "writers": [], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1369422751717, "tmdate": 1496674357195, "id": "ICLR.cc/2014/conference/-/submission", "writers": ["ICLR.cc/2014"], "signatures": ["OpenReview.net"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": []}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1377198751717, "cdate": 1496674357195}}}], "count": 9}