{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1488577232797, "tcdate": 1478281809810, "number": 263, "id": "Skq89Scxx", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "Skq89Scxx", "signatures": ["~Ilya_Loshchilov1"], "readers": ["everyone"], "content": {"title": "SGDR: Stochastic Gradient Descent with Warm Restarts", "abstract": "Restart techniques are common in gradient-free optimization to deal with multimodal functions. Partial warm restarts are also gaining popularity in gradient-based optimization to improve the rate of convergence in accelerated gradient schemes to deal with ill-conditioned functions. In this paper, we propose a simple warm restart technique for stochastic gradient descent to improve its anytime performance when training deep neural networks. We empirically study its performance on the CIFAR-10 and CIFAR-100 datasets,   \nwhere we demonstrate new state-of-the-art results at 3.14\\% and 16.21\\%, respectively. We also demonstrate its advantages on a dataset of EEG recordings and on a downsampled version of the ImageNet dataset. Our source code is available at \\\\ \\url{https://github.com/loshchil/SGDR}", "pdf": "/pdf/f1d82a22d77c21787940a33db6ce95a245c55eeb.pdf", "TL;DR": "We propose a simple warm restart technique for stochastic gradient descent to improve its anytime performance.", "paperhash": "loshchilov|sgdr_stochastic_gradient_descent_with_warm_restarts", "conflicts": ["uni-freiburg.de"], "keywords": ["Deep learning", "Optimization"], "authors": ["Ilya Loshchilov", "Frank Hutter"], "authorids": ["ilya@cs.uni-freiburg.de", "fh@cs.uni-freiburg.de"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396469068, "tcdate": 1486396469068, "number": 1, "id": "SypEhMUde", "invitation": "ICLR.cc/2017/conference/-/paper263/acceptance", "forum": "Skq89Scxx", "replyto": "Skq89Scxx", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"title": "ICLR committee final decision", "comment": "All reviewers viewed the paper favourably, with the only criticism being that seeing how the method complements other approaches (momentum, Adam) would make the paper more complete. We encourage the authors to include such a comparison in the camera ready version of their paper.", "decision": "Accept (Poster)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SGDR: Stochastic Gradient Descent with Warm Restarts", "abstract": "Restart techniques are common in gradient-free optimization to deal with multimodal functions. Partial warm restarts are also gaining popularity in gradient-based optimization to improve the rate of convergence in accelerated gradient schemes to deal with ill-conditioned functions. In this paper, we propose a simple warm restart technique for stochastic gradient descent to improve its anytime performance when training deep neural networks. We empirically study its performance on the CIFAR-10 and CIFAR-100 datasets,   \nwhere we demonstrate new state-of-the-art results at 3.14\\% and 16.21\\%, respectively. We also demonstrate its advantages on a dataset of EEG recordings and on a downsampled version of the ImageNet dataset. Our source code is available at \\\\ \\url{https://github.com/loshchil/SGDR}", "pdf": "/pdf/f1d82a22d77c21787940a33db6ce95a245c55eeb.pdf", "TL;DR": "We propose a simple warm restart technique for stochastic gradient descent to improve its anytime performance.", "paperhash": "loshchilov|sgdr_stochastic_gradient_descent_with_warm_restarts", "conflicts": ["uni-freiburg.de"], "keywords": ["Deep learning", "Optimization"], "authors": ["Ilya Loshchilov", "Frank Hutter"], "authorids": ["ilya@cs.uni-freiburg.de", "fh@cs.uni-freiburg.de"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396469551, "id": "ICLR.cc/2017/conference/-/paper263/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "Skq89Scxx", "replyto": "Skq89Scxx", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396469551}}}, {"tddate": null, "tmdate": 1484944250865, "tcdate": 1482253331725, "number": 3, "id": "SJ3MEJPEg", "invitation": "ICLR.cc/2017/conference/-/paper263/official/review", "forum": "Skq89Scxx", "replyto": "Skq89Scxx", "signatures": ["ICLR.cc/2017/conference/paper263/AnonReviewer4"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper263/AnonReviewer4"], "content": {"title": "Great trick worth publishing, but is there enough material for a full paper?", "rating": "7: Good paper, accept", "review": "This heuristic to improve gradient descent in image classification is simple and effective, but this looks to me more like a workshop track paper. Demonstration of the algorithm is limited to one task (CIFAR) and there is no theory to support it, so we do not know how it will generalize on other tasks\n\nWorking on DNNs for NLP, I find some observations in the paper opposite to my own experience. In particular, with architectures that combine a wide variety of layer types (embedding, RNN, CNN, gating), I found that ADAM-type techniques far outperform simple SGD with momentum, as they save searching for the right learning rate for each type of layer. But ADAM only works well combined with Poliak averaging, as it fluctuates a lot from one batch to another.\n\nRevision:\n-  the authors substantially improved the contents of the paper, including experiments on another set than Cifar\n-  the workshop track has been modified to breakthrough work, so my recommendation for it is not longer appropriate\nI have therefore improved my rating", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SGDR: Stochastic Gradient Descent with Warm Restarts", "abstract": "Restart techniques are common in gradient-free optimization to deal with multimodal functions. Partial warm restarts are also gaining popularity in gradient-based optimization to improve the rate of convergence in accelerated gradient schemes to deal with ill-conditioned functions. In this paper, we propose a simple warm restart technique for stochastic gradient descent to improve its anytime performance when training deep neural networks. We empirically study its performance on the CIFAR-10 and CIFAR-100 datasets,   \nwhere we demonstrate new state-of-the-art results at 3.14\\% and 16.21\\%, respectively. We also demonstrate its advantages on a dataset of EEG recordings and on a downsampled version of the ImageNet dataset. Our source code is available at \\\\ \\url{https://github.com/loshchil/SGDR}", "pdf": "/pdf/f1d82a22d77c21787940a33db6ce95a245c55eeb.pdf", "TL;DR": "We propose a simple warm restart technique for stochastic gradient descent to improve its anytime performance.", "paperhash": "loshchilov|sgdr_stochastic_gradient_descent_with_warm_restarts", "conflicts": ["uni-freiburg.de"], "keywords": ["Deep learning", "Optimization"], "authors": ["Ilya Loshchilov", "Frank Hutter"], "authorids": ["ilya@cs.uni-freiburg.de", "fh@cs.uni-freiburg.de"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512643801, "id": "ICLR.cc/2017/conference/-/paper263/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper263/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper263/AnonReviewer1", "ICLR.cc/2017/conference/paper263/AnonReviewer3", "ICLR.cc/2017/conference/paper263/AnonReviewer4"], "reply": {"forum": "Skq89Scxx", "replyto": "Skq89Scxx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper263/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper263/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512643801}}}, {"tddate": null, "tmdate": 1484405983406, "tcdate": 1484405983406, "number": 4, "id": "HJDyphPUg", "invitation": "ICLR.cc/2017/conference/-/paper263/public/comment", "forum": "Skq89Scxx", "replyto": "Skq89Scxx", "signatures": ["~Ilya_Loshchilov1"], "readers": ["everyone"], "writers": ["~Ilya_Loshchilov1"], "content": {"title": "Rebuttal", "comment": "We thank all reviewers for their positive evaluation and their valuable comments. We've uploaded a revision to address the issues raised and briefly reply to the reviewers' concerns here.\n\nExperiments on additional benchmarks \n==============================\n\nIn order to address the concerns of AnonReviewer4 about our method's generality, we added two more benchmarks to the main paper.\n\n1. We had already included additional results for another domain in the appendix of the original submission. That domain is motor-control decoding based on EEG data, which, due to the large noise present in brain signals, is very different from visual object recognition in general, and CIFAR in particular. We realize that it was not the best decision to only mention those results in the supplementary material, and we have now moved them to the main paper (new Section 4.4). In fact, our work is in part motivated and funded by a project aimed at accelerating the processing of this sort of brain data (see acknowledgments).\n\n2. We've now added a new experiment on a new downsampled version of the ImageNet dataset (new Section 4.5), which is much harder than the CIFAR benchmark because there are 1000 classes and because the target concept often only occupies a small part of the image. Due to the computational expense of training on 1 million images, these results are very preliminary -- e.g., we didn't tune hyperparameters yet. While the dataset is difficult for both the default learning rate schedule and SGDR, SGDR achieved much better results. An interpretation of this might be that, while the initial learning rate seems to be very important, SGDR reduces the problem of improper learning rate selection by quickly annealing from the initial learning rate to 0.\n\nWe would also like to point out that the simplicity and generality of SGDR let Huang et al use it to develop snapshot ensembles (https://openreview.net/forum?id=BJYwwY9ll) and show that the combination of SGDR and snapshot ensembles works very well on a variety of datasets.\n\nNew results for T_0=200\n===================\n\nWe added experiments for the experiment with annealing for 200 epochs (T_0=200) AnonReviewer3 suggested. The results show that longer annealing works great on CIFAR-10 but not that great on CIFAR-100. When considering both datasets, annealing with $T_0=200, T_{mult}=1$ is comparable to $T_0=10, T_{mult}=2$. However, the latter has a better any-time performance. Our initial text attempted to highlight the advantages of the annealing alone with \"Our results suggest that \\textit{even without any restarts} the proposed aggressive learning rate schedule given by eq. (5) is competitive w.r.t. the default schedule when training WRNs on the CIFAR-10 and CIFAR-100 datasets.\" We adjusted that sentence with \"(e.g., for $T_0=200, T_{mult}=1$)\" to point the reader towards the relevant result / setting. The results of $T_0=200, T_{mult}=1$ for WRN-28-20 are based on 1 run only because our computational resources were used for 5 runs of WRN-28-10 and new experiments with the downsampled ImageNet above. \n\nReadability of figures\n================\n\nFollowing the suggestions of AnonReviewer1 and AnonReviewer3, we improved the readability of some figures. It might be the case that the introduction of the curve with $T_0=200$ suggested by AnonReviewer3 slightly degraded the readability again, and we would be happy to move results with that setting (or any other settings the reviewers suggest) to the appendix. While the curves for SGDR tend to be lean, the ones for our baselines are very busy since they are quite noisy. If the reviewers preferred this, we would be open to the possibility of moving most plots that include many methods to the supplementary material and focussing on plots with a few select methods in the main paper.\n\n\nSummary\n=======\n\nWe are glad that the reviewers agree that our method is novel, simple, and effective, and we hope to have demonstrated its generality better now as well. We would also like to point out that it was already very useful, allowing Huang et al to develop snapshot ensembles, which directly use SGDR (as discussed in our Section 4.3, where we also present results with snapshot ensembles). We expect that the combination of SGDR and snapshot ensembles will be the new default strategy for many researchers, and we believe this is an additional point in favor of accepting our paper as a conference paper at ICLR. \n\nThank you again for your reviews!"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SGDR: Stochastic Gradient Descent with Warm Restarts", "abstract": "Restart techniques are common in gradient-free optimization to deal with multimodal functions. Partial warm restarts are also gaining popularity in gradient-based optimization to improve the rate of convergence in accelerated gradient schemes to deal with ill-conditioned functions. In this paper, we propose a simple warm restart technique for stochastic gradient descent to improve its anytime performance when training deep neural networks. We empirically study its performance on the CIFAR-10 and CIFAR-100 datasets,   \nwhere we demonstrate new state-of-the-art results at 3.14\\% and 16.21\\%, respectively. We also demonstrate its advantages on a dataset of EEG recordings and on a downsampled version of the ImageNet dataset. Our source code is available at \\\\ \\url{https://github.com/loshchil/SGDR}", "pdf": "/pdf/f1d82a22d77c21787940a33db6ce95a245c55eeb.pdf", "TL;DR": "We propose a simple warm restart technique for stochastic gradient descent to improve its anytime performance.", "paperhash": "loshchilov|sgdr_stochastic_gradient_descent_with_warm_restarts", "conflicts": ["uni-freiburg.de"], "keywords": ["Deep learning", "Optimization"], "authors": ["Ilya Loshchilov", "Frank Hutter"], "authorids": ["ilya@cs.uni-freiburg.de", "fh@cs.uni-freiburg.de"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287659082, "id": "ICLR.cc/2017/conference/-/paper263/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Skq89Scxx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper263/reviewers", "ICLR.cc/2017/conference/paper263/areachairs"], "cdate": 1485287659082}}}, {"tddate": null, "tmdate": 1482258837183, "tcdate": 1482258837183, "number": 3, "id": "B165YgvVg", "invitation": "ICLR.cc/2017/conference/-/paper263/public/comment", "forum": "Skq89Scxx", "replyto": "SJ3MEJPEg", "signatures": ["~Ilya_Loshchilov1"], "readers": ["everyone"], "writers": ["~Ilya_Loshchilov1"], "content": {"title": "quick reply", "comment": "Thank you for your review. Please consider this quick reply:\ni) We also show that SGDR works very well on a completely different dataset of EEG recordings (see section 7.1 in the Supplementary Material). We realize it was not the best decision to only mention this additional result in the supplementary material, and we'll pull it into the main part of the paper in the next revision.\nii) We do not counterpose ADAM to SGDR because the latter (warm restarts) could also be applied to the former. We focused on SGD with momentum since it dominates for optimizing state-of-the-art residual networks for image classification with large DNNs (e.g., on CIFAR, SVHN and ImageNet). \niii) Regarding the theoretical results, please consider our note given in Section 2.2:\n\"The authors showed that fixed warm restarts of the algorithm with a period proportional to the conditional number achieves the optimal linear convergence rate of the original accelerated gradient scheme. Since the condition number is\nan unknown parameter and its value may vary during the search, they proposed two adaptive warm restart techniques (O'Donoghue & Candes, 2012)\". Unfortunately, the results of (O'Donoghue & Candes, 2012) are limited to strongly convex functions and their extension to non-convex stochastic functions does not seem trivial.\"\nIn response to your review, and to demonstrate the generality of SGDR we will also try to get some compute resources to evaluate it on additional datasets over the holidays. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SGDR: Stochastic Gradient Descent with Warm Restarts", "abstract": "Restart techniques are common in gradient-free optimization to deal with multimodal functions. Partial warm restarts are also gaining popularity in gradient-based optimization to improve the rate of convergence in accelerated gradient schemes to deal with ill-conditioned functions. In this paper, we propose a simple warm restart technique for stochastic gradient descent to improve its anytime performance when training deep neural networks. We empirically study its performance on the CIFAR-10 and CIFAR-100 datasets,   \nwhere we demonstrate new state-of-the-art results at 3.14\\% and 16.21\\%, respectively. We also demonstrate its advantages on a dataset of EEG recordings and on a downsampled version of the ImageNet dataset. Our source code is available at \\\\ \\url{https://github.com/loshchil/SGDR}", "pdf": "/pdf/f1d82a22d77c21787940a33db6ce95a245c55eeb.pdf", "TL;DR": "We propose a simple warm restart technique for stochastic gradient descent to improve its anytime performance.", "paperhash": "loshchilov|sgdr_stochastic_gradient_descent_with_warm_restarts", "conflicts": ["uni-freiburg.de"], "keywords": ["Deep learning", "Optimization"], "authors": ["Ilya Loshchilov", "Frank Hutter"], "authorids": ["ilya@cs.uni-freiburg.de", "fh@cs.uni-freiburg.de"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287659082, "id": "ICLR.cc/2017/conference/-/paper263/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Skq89Scxx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper263/reviewers", "ICLR.cc/2017/conference/paper263/areachairs"], "cdate": 1485287659082}}}, {"tddate": null, "tmdate": 1481836196006, "tcdate": 1481836196002, "number": 2, "id": "By3oLYeNe", "invitation": "ICLR.cc/2017/conference/-/paper263/official/review", "forum": "Skq89Scxx", "replyto": "Skq89Scxx", "signatures": ["ICLR.cc/2017/conference/paper263/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper263/AnonReviewer3"], "content": {"title": "", "rating": "7: Good paper, accept", "review": "This an interesting investigation into learning rate schedules, bringing in the idea of restarts, often overlooked in deep learning. The paper does a thorough study on non-trivial datasets, and while the outcomes are not fully conclusive, the results are very good and the approach is novel enough to warrant publication. \n\nI thank the authors for revising the paper based on my concerns.\n\nTypos:\n- \u201cflesh\u201d -> \u201cflush\u201d", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SGDR: Stochastic Gradient Descent with Warm Restarts", "abstract": "Restart techniques are common in gradient-free optimization to deal with multimodal functions. Partial warm restarts are also gaining popularity in gradient-based optimization to improve the rate of convergence in accelerated gradient schemes to deal with ill-conditioned functions. In this paper, we propose a simple warm restart technique for stochastic gradient descent to improve its anytime performance when training deep neural networks. We empirically study its performance on the CIFAR-10 and CIFAR-100 datasets,   \nwhere we demonstrate new state-of-the-art results at 3.14\\% and 16.21\\%, respectively. We also demonstrate its advantages on a dataset of EEG recordings and on a downsampled version of the ImageNet dataset. Our source code is available at \\\\ \\url{https://github.com/loshchil/SGDR}", "pdf": "/pdf/f1d82a22d77c21787940a33db6ce95a245c55eeb.pdf", "TL;DR": "We propose a simple warm restart technique for stochastic gradient descent to improve its anytime performance.", "paperhash": "loshchilov|sgdr_stochastic_gradient_descent_with_warm_restarts", "conflicts": ["uni-freiburg.de"], "keywords": ["Deep learning", "Optimization"], "authors": ["Ilya Loshchilov", "Frank Hutter"], "authorids": ["ilya@cs.uni-freiburg.de", "fh@cs.uni-freiburg.de"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512643801, "id": "ICLR.cc/2017/conference/-/paper263/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper263/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper263/AnonReviewer1", "ICLR.cc/2017/conference/paper263/AnonReviewer3", "ICLR.cc/2017/conference/paper263/AnonReviewer4"], "reply": {"forum": "Skq89Scxx", "replyto": "Skq89Scxx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper263/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper263/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512643801}}}, {"tddate": null, "tmdate": 1481816186448, "tcdate": 1481816186443, "number": 1, "id": "SkMtuVxEx", "invitation": "ICLR.cc/2017/conference/-/paper263/official/review", "forum": "Skq89Scxx", "replyto": "Skq89Scxx", "signatures": ["ICLR.cc/2017/conference/paper263/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper263/AnonReviewer1"], "content": {"title": "An effective method to improve convergence of neural network training", "rating": "7: Good paper, accept", "review": "This paper describes a way to speed up convergence through sudden increases of otherwise monotonically decreasing learning rates. Several techniques are presented in a clear way and parameterized method is proposed and evaluated on the CIFAR task. The concept is easy to understand and the authors chose state-of-the-art models to show the performance of their algorithm. The relevance of these results goes beyond image classification.\n\n\nPros:\n\n- Simple and effective method to improve convergence\n- Good evaluation on well known database\n\n\nCons:\n\n- Connection of introduction and topic of the paper is a bit unclear\n- Fig 2, 4 and 5 are hard to read. Lines are out of bounds and maybe only the best setting for T_0 and T_mult would be clearer. The baseline also doesn't seem to converge\n\nRemarks:\nAn loss surface for T_0 against T_mult would be very helpful. Also understanding the relationship of network depth and the performance of this method would add value to this analysis.\n", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SGDR: Stochastic Gradient Descent with Warm Restarts", "abstract": "Restart techniques are common in gradient-free optimization to deal with multimodal functions. Partial warm restarts are also gaining popularity in gradient-based optimization to improve the rate of convergence in accelerated gradient schemes to deal with ill-conditioned functions. In this paper, we propose a simple warm restart technique for stochastic gradient descent to improve its anytime performance when training deep neural networks. We empirically study its performance on the CIFAR-10 and CIFAR-100 datasets,   \nwhere we demonstrate new state-of-the-art results at 3.14\\% and 16.21\\%, respectively. We also demonstrate its advantages on a dataset of EEG recordings and on a downsampled version of the ImageNet dataset. Our source code is available at \\\\ \\url{https://github.com/loshchil/SGDR}", "pdf": "/pdf/f1d82a22d77c21787940a33db6ce95a245c55eeb.pdf", "TL;DR": "We propose a simple warm restart technique for stochastic gradient descent to improve its anytime performance.", "paperhash": "loshchilov|sgdr_stochastic_gradient_descent_with_warm_restarts", "conflicts": ["uni-freiburg.de"], "keywords": ["Deep learning", "Optimization"], "authors": ["Ilya Loshchilov", "Frank Hutter"], "authorids": ["ilya@cs.uni-freiburg.de", "fh@cs.uni-freiburg.de"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512643801, "id": "ICLR.cc/2017/conference/-/paper263/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper263/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper263/AnonReviewer1", "ICLR.cc/2017/conference/paper263/AnonReviewer3", "ICLR.cc/2017/conference/paper263/AnonReviewer4"], "reply": {"forum": "Skq89Scxx", "replyto": "Skq89Scxx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper263/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper263/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512643801}}}, {"tddate": null, "tmdate": 1481727112325, "tcdate": 1480694153201, "number": 1, "id": "rJWctzyQe", "invitation": "ICLR.cc/2017/conference/-/paper263/public/comment", "forum": "Skq89Scxx", "replyto": "rJprQGy7g", "signatures": ["~Ilya_Loshchilov1"], "readers": ["everyone"], "writers": ["~Ilya_Loshchilov1"], "content": {"title": "a quick clarification about appendix 7.2", "comment": "All methods (SGDR and baselines) use the same number of samples/compute per epoch\n\nYou are right that when only the final results are considered, the annealing alone can do a great job. \nWe tried to emphasize it in our original discussion section:\n\"Our results suggest that \\textit{even without any restarts} the proposed aggressive learning rate schedule given by eq. (\\ref{eq:t}) is competitive w.r.t. the default schedule when training WRNs on the CIFAR-10 and CIFAR-100 datasets.\"\nWe are running experiments with T_0=200. We didn't do that before because our primary variant of SGDR with T_0=10, T_mult=2 (respectively, T0=1, T_mult=2) delivers its last (within 200 epochs) recommendation at epoch #150 (respectively, epoch #127). Still, it is interesting to see the results with T_0=200. \n\nWe modified Fig. 1 to make it more readable. We also removed yellow color from our plots. We fixed the y-axis of Fig. 4."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SGDR: Stochastic Gradient Descent with Warm Restarts", "abstract": "Restart techniques are common in gradient-free optimization to deal with multimodal functions. Partial warm restarts are also gaining popularity in gradient-based optimization to improve the rate of convergence in accelerated gradient schemes to deal with ill-conditioned functions. In this paper, we propose a simple warm restart technique for stochastic gradient descent to improve its anytime performance when training deep neural networks. We empirically study its performance on the CIFAR-10 and CIFAR-100 datasets,   \nwhere we demonstrate new state-of-the-art results at 3.14\\% and 16.21\\%, respectively. We also demonstrate its advantages on a dataset of EEG recordings and on a downsampled version of the ImageNet dataset. Our source code is available at \\\\ \\url{https://github.com/loshchil/SGDR}", "pdf": "/pdf/f1d82a22d77c21787940a33db6ce95a245c55eeb.pdf", "TL;DR": "We propose a simple warm restart technique for stochastic gradient descent to improve its anytime performance.", "paperhash": "loshchilov|sgdr_stochastic_gradient_descent_with_warm_restarts", "conflicts": ["uni-freiburg.de"], "keywords": ["Deep learning", "Optimization"], "authors": ["Ilya Loshchilov", "Frank Hutter"], "authorids": ["ilya@cs.uni-freiburg.de", "fh@cs.uni-freiburg.de"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287659082, "id": "ICLR.cc/2017/conference/-/paper263/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Skq89Scxx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper263/reviewers", "ICLR.cc/2017/conference/paper263/areachairs"], "cdate": 1485287659082}}}, {"tddate": null, "tmdate": 1481727073713, "tcdate": 1481203340372, "number": 2, "id": "HJEcA0LQg", "invitation": "ICLR.cc/2017/conference/-/paper263/public/comment", "forum": "Skq89Scxx", "replyto": "H1oEVueXl", "signatures": ["~Ilya_Loshchilov1"], "readers": ["everyone"], "writers": ["~Ilya_Loshchilov1"], "content": {"title": "reply: CIFAR evaluation", "comment": "Please note that if there are an even number of observations, we follow the convention to compute the median as the mean of the middle two numbers. The only reason we used only 2 runs of WRN-28-20 was the total cost of training for all listed scenarios. \nThank you for the remarks!\n\n- The missing full stop is fixed.\n- We modified Fig. 3 to make it lighter and hopefully more readable with a greater font size."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SGDR: Stochastic Gradient Descent with Warm Restarts", "abstract": "Restart techniques are common in gradient-free optimization to deal with multimodal functions. Partial warm restarts are also gaining popularity in gradient-based optimization to improve the rate of convergence in accelerated gradient schemes to deal with ill-conditioned functions. In this paper, we propose a simple warm restart technique for stochastic gradient descent to improve its anytime performance when training deep neural networks. We empirically study its performance on the CIFAR-10 and CIFAR-100 datasets,   \nwhere we demonstrate new state-of-the-art results at 3.14\\% and 16.21\\%, respectively. We also demonstrate its advantages on a dataset of EEG recordings and on a downsampled version of the ImageNet dataset. Our source code is available at \\\\ \\url{https://github.com/loshchil/SGDR}", "pdf": "/pdf/f1d82a22d77c21787940a33db6ce95a245c55eeb.pdf", "TL;DR": "We propose a simple warm restart technique for stochastic gradient descent to improve its anytime performance.", "paperhash": "loshchilov|sgdr_stochastic_gradient_descent_with_warm_restarts", "conflicts": ["uni-freiburg.de"], "keywords": ["Deep learning", "Optimization"], "authors": ["Ilya Loshchilov", "Frank Hutter"], "authorids": ["ilya@cs.uni-freiburg.de", "fh@cs.uni-freiburg.de"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287659082, "id": "ICLR.cc/2017/conference/-/paper263/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Skq89Scxx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper263/reviewers", "ICLR.cc/2017/conference/paper263/areachairs"], "cdate": 1485287659082}}}, {"tddate": null, "tmdate": 1480782898892, "tcdate": 1480782898887, "number": 2, "id": "H1oEVueXl", "invitation": "ICLR.cc/2017/conference/-/paper263/pre-review/question", "forum": "Skq89Scxx", "replyto": "Skq89Scxx", "signatures": ["ICLR.cc/2017/conference/paper263/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper263/AnonReviewer1"], "content": {"title": "CIFAR evaluation", "question": "Why did you only take the median of two runs in the last rows of Table 1 and is that the superior or inferior run?\n\nOther remarks:\n\n- Missing full stop at bottom of page 2 before \"Our empirical results\".\n- The black font on the dark blue in Fig 3. is hard to read on printouts\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SGDR: Stochastic Gradient Descent with Warm Restarts", "abstract": "Restart techniques are common in gradient-free optimization to deal with multimodal functions. Partial warm restarts are also gaining popularity in gradient-based optimization to improve the rate of convergence in accelerated gradient schemes to deal with ill-conditioned functions. In this paper, we propose a simple warm restart technique for stochastic gradient descent to improve its anytime performance when training deep neural networks. We empirically study its performance on the CIFAR-10 and CIFAR-100 datasets,   \nwhere we demonstrate new state-of-the-art results at 3.14\\% and 16.21\\%, respectively. We also demonstrate its advantages on a dataset of EEG recordings and on a downsampled version of the ImageNet dataset. Our source code is available at \\\\ \\url{https://github.com/loshchil/SGDR}", "pdf": "/pdf/f1d82a22d77c21787940a33db6ce95a245c55eeb.pdf", "TL;DR": "We propose a simple warm restart technique for stochastic gradient descent to improve its anytime performance.", "paperhash": "loshchilov|sgdr_stochastic_gradient_descent_with_warm_restarts", "conflicts": ["uni-freiburg.de"], "keywords": ["Deep learning", "Optimization"], "authors": ["Ilya Loshchilov", "Frank Hutter"], "authorids": ["ilya@cs.uni-freiburg.de", "fh@cs.uni-freiburg.de"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959372510, "id": "ICLR.cc/2017/conference/-/paper263/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper263/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper263/AnonReviewer3", "ICLR.cc/2017/conference/paper263/AnonReviewer1"], "reply": {"forum": "Skq89Scxx", "replyto": "Skq89Scxx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper263/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper263/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959372510}}}, {"tddate": null, "tmdate": 1480692548691, "tcdate": 1480692548685, "number": 1, "id": "rJprQGy7g", "invitation": "ICLR.cc/2017/conference/-/paper263/pre-review/question", "forum": "Skq89Scxx", "replyto": "Skq89Scxx", "signatures": ["ICLR.cc/2017/conference/paper263/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper263/AnonReviewer3"], "content": {"title": "restarts versus annealing", "question": "A quick clarification about appendix 7.2: does this mean that in the main paper, your method always uses twice the samples/compute per epoch compared to the baselines? Then all the learning curves (with epochs as x-axis) would be misleading?\n\nMy main question is about annealing versus restarts -- numerous papers have used various (linear/geometric/stepwise) learning-rate annealing schedules for deep learning (going back to at least Ciresan et al '12). I see the key innovation in your paper in its anytime performance, achieved via restarts+annealing. However, you mostly focus on final results, and on these the conclusions are less clear, also because I'm missing comparable annealing-no-restart baselines (e.g. T_0=200), and because some of the best results did very few restarts...\n\nAlso, Fig 1 is really difficult to read: the legend should not hide the data, and there are plotting tools that help disambiguate curves in better ways... in general the plotting quality is disappointing: it might be worth rethinking what you want to demonstrate and plot jsut the right information without the clutter (fig 4 with its actual anytime-test-errors seems much clearer, although there the y-axis is odd.)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SGDR: Stochastic Gradient Descent with Warm Restarts", "abstract": "Restart techniques are common in gradient-free optimization to deal with multimodal functions. Partial warm restarts are also gaining popularity in gradient-based optimization to improve the rate of convergence in accelerated gradient schemes to deal with ill-conditioned functions. In this paper, we propose a simple warm restart technique for stochastic gradient descent to improve its anytime performance when training deep neural networks. We empirically study its performance on the CIFAR-10 and CIFAR-100 datasets,   \nwhere we demonstrate new state-of-the-art results at 3.14\\% and 16.21\\%, respectively. We also demonstrate its advantages on a dataset of EEG recordings and on a downsampled version of the ImageNet dataset. Our source code is available at \\\\ \\url{https://github.com/loshchil/SGDR}", "pdf": "/pdf/f1d82a22d77c21787940a33db6ce95a245c55eeb.pdf", "TL;DR": "We propose a simple warm restart technique for stochastic gradient descent to improve its anytime performance.", "paperhash": "loshchilov|sgdr_stochastic_gradient_descent_with_warm_restarts", "conflicts": ["uni-freiburg.de"], "keywords": ["Deep learning", "Optimization"], "authors": ["Ilya Loshchilov", "Frank Hutter"], "authorids": ["ilya@cs.uni-freiburg.de", "fh@cs.uni-freiburg.de"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959372510, "id": "ICLR.cc/2017/conference/-/paper263/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper263/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper263/AnonReviewer3", "ICLR.cc/2017/conference/paper263/AnonReviewer1"], "reply": {"forum": "Skq89Scxx", "replyto": "Skq89Scxx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper263/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper263/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959372510}}}], "count": 11}