{"notes": [{"ddate": null, "legacy_migration": true, "tmdate": 1392729600000, "tcdate": 1392729600000, "number": 1, "id": "ZZ7TU_nWTrjcG", "invitation": "ICLR.cc/2014/-/submission/conference/reply", "forum": "u-IAYCzRsK-vN", "replyto": "mYLe3dIVG_lnh", "signatures": ["Jonathan Masci"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "The revised paper is now available."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Sparse similarity-preserving hashing", "decision": "submitted, no decision", "abstract": "In recent years, a lot of attention has been devoted to efficient nearest neighbor search by means of similarity-preserving hashing. One of the plights of existing hashing techniques is the intrinsic trade-off between performance and computational complexity: while longer hash codes allow for lower false positive rates, it is very difficult to increase the embedding dimensionality without incurring in very high false negatives rates or prohibiting computational costs. In this paper, we propose a way to overcome this limitation by enforcing the hash codes to be sparse. Sparse high-dimensional codes enjoy from the low false positive rates typical of long hashes, while keeping the false negative rates similar to those of a shorter dense hashing scheme with equal number of degrees of freedom. We use a tailored feed-forward neural network for the hashing function. Extensive experimental evaluation involving visual and multi-modal data shows the benefits of the proposed method.", "pdf": "https://arxiv.org/abs/1312.5479", "paperhash": "bronstein|sparse_similaritypreserving_hashing", "keywords": [], "conflicts": [], "authors": ["Alex M. Bronstein", "Guillermo Sapiro", "Pablo Sprechmann", "Jonathan Masci", "Michael M. Bronstein"], "authorids": ["alexbronst@gmail.com", "guillermo.sapiro@gmail.com", "pablo.sprechmann@duke.edu", "jonathan.masci@gmail.com", "michael.bronstein@usi.ch"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392652500000, "tcdate": 1392652500000, "number": 5, "id": "mYLe3dIVG_lnh", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "u-IAYCzRsK-vN", "replyto": "u-IAYCzRsK-vN", "signatures": ["Jonathan Masci"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "A new version (v3) of the paper will be available at Tue, 18 Feb 2014 01:00:00 GMT."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Sparse similarity-preserving hashing", "decision": "submitted, no decision", "abstract": "In recent years, a lot of attention has been devoted to efficient nearest neighbor search by means of similarity-preserving hashing. One of the plights of existing hashing techniques is the intrinsic trade-off between performance and computational complexity: while longer hash codes allow for lower false positive rates, it is very difficult to increase the embedding dimensionality without incurring in very high false negatives rates or prohibiting computational costs. In this paper, we propose a way to overcome this limitation by enforcing the hash codes to be sparse. Sparse high-dimensional codes enjoy from the low false positive rates typical of long hashes, while keeping the false negative rates similar to those of a shorter dense hashing scheme with equal number of degrees of freedom. We use a tailored feed-forward neural network for the hashing function. Extensive experimental evaluation involving visual and multi-modal data shows the benefits of the proposed method.", "pdf": "https://arxiv.org/abs/1312.5479", "paperhash": "bronstein|sparse_similaritypreserving_hashing", "keywords": [], "conflicts": [], "authors": ["Alex M. Bronstein", "Guillermo Sapiro", "Pablo Sprechmann", "Jonathan Masci", "Michael M. Bronstein"], "authorids": ["alexbronst@gmail.com", "guillermo.sapiro@gmail.com", "pablo.sprechmann@duke.edu", "jonathan.masci@gmail.com", "michael.bronstein@usi.ch"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392299400000, "tcdate": 1392299400000, "number": 1, "id": "GG8MxUz35hkXl", "invitation": "ICLR.cc/2014/-/submission/conference/reply", "forum": "u-IAYCzRsK-vN", "replyto": "XMixXVP-xZMUW", "signatures": ["Jonathan Masci"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "We are grateful to reviewer Anonymous a029 for his/her comments. We clarify below all the critical issues, and these points are addressed in the revision. We have produced additional evaluations to better convey our point; we invite all the reviewers to look at these results. Below are responses to the issues raised:\r\n\r\n\r\n1. Please note that we *do not* use sparsity in the retrieval. After the code is constructed, we use standard retrieval procedure (LUT for small r, brute force for large r, see our answers to the previous reviews) both for dense and sparse codes. Using small r guarantees fast retrieval; however, in the dense case, it comes at the expense of the recall. We show that introducing sparsity, we get high recall for small r, thus guaranteeing both fast search and high recall, which is usually impossible with standard methods. \r\n\r\nWe do believe, however, that it is also possible to take advantage of sparse codes to make the retrieval more efficient, and intend to explore this direction in future work. In particular, in our experiments we observed that the number of unique codes is significantly smaller for sparse hash compared to dense hash, which potentially allows an improvement of the data structure used for retrieval (an overwhelming number of LUT entries are empty). Here are results obtained on CIFAR10 datasets for m=10:\r\n\r\nAverage number of database elements mapped to the same code (collisions):\r\nSparseHash\t798.47\r\nKSH\t\t\t3.95\r\nNNHash\t\t4.83\r\nSSH\t\t\t1.01\r\nDH\t\t\t1.00\r\nAGH\t\t\t1.42\r\n\r\n\r\n2. \r\nAs requested by the reviewer, we have computed the timing for the experiments presented in the paper, and present the 3D plot of precision/recall/retrieval time for different methods for varying r:\r\n\r\nhttps://www.dropbox.com/s/td7pyc2hwwch2s1/sparsehash_timing_results.png\r\nAnnotation: o (r=0), triangle (r=1), + (r=2, implemented as brute force search)\r\n\r\nWe can conclude that:\r\n\r\n- search time is controlled mainly by the radius r, which also impacts the recall/precision. With dense methods, it is impossible to achieve fast search and high precision/recall. The use of sparsity makes this possible. \r\n- With sparse hash we are able to achieve orders of magnitude higher recall as well as an increase in precision for retrieval time comparable with the dense methods. \r\n- Our retrieval data structure does not currently take advantage of the code sparsity, suggesting a potentially significant reduction in search time. \r\n\r\n\r\n3. Evaluation methodology:\r\nWe find the reviewer's worry about the (m,r) being a poor predictor of the search time is very reasonable. However, we believe that the comparable search times reported for the same (m,r) settings in all methods suggests that it is not an issue in our specific experiments."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Sparse similarity-preserving hashing", "decision": "submitted, no decision", "abstract": "In recent years, a lot of attention has been devoted to efficient nearest neighbor search by means of similarity-preserving hashing. One of the plights of existing hashing techniques is the intrinsic trade-off between performance and computational complexity: while longer hash codes allow for lower false positive rates, it is very difficult to increase the embedding dimensionality without incurring in very high false negatives rates or prohibiting computational costs. In this paper, we propose a way to overcome this limitation by enforcing the hash codes to be sparse. Sparse high-dimensional codes enjoy from the low false positive rates typical of long hashes, while keeping the false negative rates similar to those of a shorter dense hashing scheme with equal number of degrees of freedom. We use a tailored feed-forward neural network for the hashing function. Extensive experimental evaluation involving visual and multi-modal data shows the benefits of the proposed method.", "pdf": "https://arxiv.org/abs/1312.5479", "paperhash": "bronstein|sparse_similaritypreserving_hashing", "keywords": [], "conflicts": [], "authors": ["Alex M. Bronstein", "Guillermo Sapiro", "Pablo Sprechmann", "Jonathan Masci", "Michael M. Bronstein"], "authorids": ["alexbronst@gmail.com", "guillermo.sapiro@gmail.com", "pablo.sprechmann@duke.edu", "jonathan.masci@gmail.com", "michael.bronstein@usi.ch"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391965500000, "tcdate": 1391965500000, "number": 1, "id": "LRcNRh6ZNB7T9", "invitation": "ICLR.cc/2014/-/submission/conference/reply", "forum": "u-IAYCzRsK-vN", "replyto": "TR-OT62E8KhmZ", "signatures": ["Jonathan Masci"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "We are grateful to Anonymous d060 for interesting comments and for appreciating our work. \r\nSome of the same comments are already addressed in the updated v2 of the arxiv report that should appear on Mon Feb 10. \r\nSince d060 has spotted the same issue as a636, we kindly ask the reviewer to also look at our previous response to a636. \r\nWe will incorporate the new comments and upload a new version to arxiv.\r\n\r\n \r\n1. Retrieval:\r\n\r\nFor large radius (r=m) the search is done exhaustively with complexity O(N), where N is the database size.  \r\nFor r=0 (exact collisions), the search is done as a LUT: the query code is fed into the lookup table, \r\ncontaining all entries in the database having the same code. \r\nThe complexity is O(m), independent of N.  For small values of r (partial collisions), \r\nthe search is done as for r=0 using perturbation of the query: at most r bits of the query are changed, and then it is fed into the LUT. The final result is the union of all the retrieval results. Complexity is O(r out of m). \r\nFor this reason, one seeks to use a small radius to obtain efficient search. With dense hash, this comes at the expense of\r\nvery low recall, as we explain theoretically and show experimentally. \r\nWith sparse hash, we are able to control the exponential growth of the hamming ball volume, thus resulting in much higher recall. \r\n\r\nIt is correctly noted by the reviewer that our method does not explicitly guarantee a fixed sparsity (it is possible to use a different NN architecture,\r\nderived from coordinate-descent pursuit CoD, to guarantee at most k non-zeros in the codes). However, we see in practice that the number of non-zeros in our codes is approximately fixed (e.g. for cifar10 sparse hash of length m=128 we get codes containing on average 7.6 +/- 2.3 non-zero bits), and the behavior of the codes is similar to the theoretical case with 'guaranteed' sparsity. To emphasize, sparsity is used to obtain codes that exhibit higher recall at low search r (in particular, r=0). The search is done in a standard way described above, without making a distinction between sparse and dense cases.  \r\nNote that lack of exact control of sparsity is common in l1 optimization problems, though as mentioned above the approximate control was found to be sufficient for this application as well.\r\n\r\n \r\n2. Formula 4: \r\nWe fixed formula (4) which missed the max(0,.) term.\r\n\r\n\r\n3. Calculating neighbors with large radii: \r\nIn our experiments, we used three radii: r=0 (collisions), r=2 and r=m (full radius). In the latter setting, we used 'brute force' search, going exhaustively through all the database.  \r\n\r\n\r\n4. Parameters setting:\r\nThe parameters were set empirically. We should stress we have not optimized these parameters, as we observed that setting them more or less arbitrarily provided performance significantly better than the competing dense hashing methods. \r\nLambda is initially set to 1 and after reduced to balance the positive and negative classes if needed.\r\nA value of 0.1 in CIFAR10 equally weights the positives and negatives for example.\r\nIn NUS we used 0.3 instead because each sample can belong to multiple classes and therefore distinction between pos and neg is not as clear as for CIFAR10.\r\nalpha is set to a small value such as 0.01 and decreased by a factor of 10 according to the desired sparsity level. \r\nThe margin M is usually 7 and we would suggest to use this as we did extensive evaluation in previous work. \r\nIn the experiments we increased it along with a higher alpha to check if this would allow better binarization. Ideally a large margin tends to saturate units and a large sparsity should further favor this phenomenon.\r\nIn the multimodal case mu_1 and mu_2 are set to 0.5 to use the respective modalities as regularization for the cross-modal embedding.\r\nWe would suggest to use this configuration and change it only if needed."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Sparse similarity-preserving hashing", "decision": "submitted, no decision", "abstract": "In recent years, a lot of attention has been devoted to efficient nearest neighbor search by means of similarity-preserving hashing. One of the plights of existing hashing techniques is the intrinsic trade-off between performance and computational complexity: while longer hash codes allow for lower false positive rates, it is very difficult to increase the embedding dimensionality without incurring in very high false negatives rates or prohibiting computational costs. In this paper, we propose a way to overcome this limitation by enforcing the hash codes to be sparse. Sparse high-dimensional codes enjoy from the low false positive rates typical of long hashes, while keeping the false negative rates similar to those of a shorter dense hashing scheme with equal number of degrees of freedom. We use a tailored feed-forward neural network for the hashing function. Extensive experimental evaluation involving visual and multi-modal data shows the benefits of the proposed method.", "pdf": "https://arxiv.org/abs/1312.5479", "paperhash": "bronstein|sparse_similaritypreserving_hashing", "keywords": [], "conflicts": [], "authors": ["Alex M. Bronstein", "Guillermo Sapiro", "Pablo Sprechmann", "Jonathan Masci", "Michael M. Bronstein"], "authorids": ["alexbronst@gmail.com", "guillermo.sapiro@gmail.com", "pablo.sprechmann@duke.edu", "jonathan.masci@gmail.com", "michael.bronstein@usi.ch"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391901780000, "tcdate": 1391901780000, "number": 4, "id": "XMixXVP-xZMUW", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "u-IAYCzRsK-vN", "replyto": "u-IAYCzRsK-vN", "signatures": ["anonymous reviewer a029"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Sparse similarity-preserving hashing", "review": "This paper builds upon Siamese neural networks [Hadsell et al, CVPR06] and (f)ISta networks [Gregor et al, ICML10] to learn to embed inputs into sparse code such that code distance reflect semantic similarity. The paper is clear and refers to related work appropriately. It is relevant to the conference. Extensive empirical comparisons over CIFAR-10 and NUS/Flickr are reported. I am mainly concerned by the computational efficiency motivation and the experimental methodology. I am also surprised that no attention to sparsity is given in the experiments given the paper title. \r\n\r\nThe introduction states that your motivation for sparse code mainly comes from computational efficiency. It seems of marginal importance. The r-radius search in an m dimensional space is nchoosek(m,r). With k sparse vectors, the same search now amount to flipping r/2 bits in the 1 bits and r/2 bits in the 0 bits in the query, i.e. nchoose(k,r/2)+nchoose(m-k, r/2). Both are comparable combinatorial problems. I feel that your motivation for sparse coding could come from the type of work you allude to at the end of column 1 in page 4 (by the way could you add some references there?).  \r\n\r\nI have concerns about the evaluation methodology. In particular, it is not clear to me why you compare different methods with a fixed code size and radius. It seems that, in an application setting, one might get some requirement in terms of mean average precision, recall at a given precision, precision at a fix recall, expected/worst search time, etc and would validate m and r to fit these performance requirement. Fixing m and r a priori and looking at the specific precision, recall point resulting from this arbitrary choice seems far from optimal for any methods. Moreover, I would also like to stress that m, r are also a poor predictor of a method running time given that different tree balance (number of points in each node and in particular the number of empty nodes) might yield very different search time at the same (m,r). In summary, I see little motivation for picking (m,r) a priori.  \r\n\r\nI am also surprised that no attention to sparsity is given in the experiments given the paper title. How was alpha validated? What is the impact of alpha on validation performance, in particular how does the validation error surface look like wrt alpha, m and r? It might also be interesting to look at the same type of results replacing L1 with L2 regularization of the representations to further justify your work. Also reporting the impact of sparsity on search time would be a must."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Sparse similarity-preserving hashing", "decision": "submitted, no decision", "abstract": "In recent years, a lot of attention has been devoted to efficient nearest neighbor search by means of similarity-preserving hashing. One of the plights of existing hashing techniques is the intrinsic trade-off between performance and computational complexity: while longer hash codes allow for lower false positive rates, it is very difficult to increase the embedding dimensionality without incurring in very high false negatives rates or prohibiting computational costs. In this paper, we propose a way to overcome this limitation by enforcing the hash codes to be sparse. Sparse high-dimensional codes enjoy from the low false positive rates typical of long hashes, while keeping the false negative rates similar to those of a shorter dense hashing scheme with equal number of degrees of freedom. We use a tailored feed-forward neural network for the hashing function. Extensive experimental evaluation involving visual and multi-modal data shows the benefits of the proposed method.", "pdf": "https://arxiv.org/abs/1312.5479", "paperhash": "bronstein|sparse_similaritypreserving_hashing", "keywords": [], "conflicts": [], "authors": ["Alex M. Bronstein", "Guillermo Sapiro", "Pablo Sprechmann", "Jonathan Masci", "Michael M. Bronstein"], "authorids": ["alexbronst@gmail.com", "guillermo.sapiro@gmail.com", "pablo.sprechmann@duke.edu", "jonathan.masci@gmail.com", "michael.bronstein@usi.ch"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391867160000, "tcdate": 1391867160000, "number": 3, "id": "TR-OT62E8KhmZ", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "u-IAYCzRsK-vN", "replyto": "u-IAYCzRsK-vN", "signatures": ["anonymous reviewer d060"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Sparse similarity-preserving hashing", "review": "The authors propose to use a to use a sparse locally sensitive hash along with an appropriate hashing function. The retrieval of sparse codes has different behaviour then dense code, which results in better performance then other other methods especially at low retrieval radius where computation is cheaper.\r\n\r\nNovelty: Good (as far as I know). \r\nQuality: Good, clearly explained except few details (see below), with experimental evaluation.\r\n\r\nDetails:\r\n- How do you define retrieval at a given radius for sparse codes? With two bit flips say, there are the same number of neighbours whether the code is dense or sparse. With the encoding proposed you don't have a guaranteed that only k values will be nonzero - it is not a strict bound. How do you define the neighbours - as only those that have the same or lower sparsity? \r\n \r\n- Is formula (4) correct, specifically line 2? I assume it should be more like the line 2 of eq. (3).\r\n\r\n- In the experiments, how did you calculate neighbours of such large radii - the number of neighbours grows as Choose(m, r). \r\n\r\n- There are a lot of hyper parameters: eq.4: lambda, alpha, M + eq. 5 mu_1, mu_2. How did you choose these? If your answer is 'cross validation' - how theses are a lot of parameters to cross validate. Do you have any good ways to set these?\r\n\r\n- Even though this is basic it would be good to explain in section 2(Efficient Retrieval) how is the retrieval defined - what is the situation?"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Sparse similarity-preserving hashing", "decision": "submitted, no decision", "abstract": "In recent years, a lot of attention has been devoted to efficient nearest neighbor search by means of similarity-preserving hashing. One of the plights of existing hashing techniques is the intrinsic trade-off between performance and computational complexity: while longer hash codes allow for lower false positive rates, it is very difficult to increase the embedding dimensionality without incurring in very high false negatives rates or prohibiting computational costs. In this paper, we propose a way to overcome this limitation by enforcing the hash codes to be sparse. Sparse high-dimensional codes enjoy from the low false positive rates typical of long hashes, while keeping the false negative rates similar to those of a shorter dense hashing scheme with equal number of degrees of freedom. We use a tailored feed-forward neural network for the hashing function. Extensive experimental evaluation involving visual and multi-modal data shows the benefits of the proposed method.", "pdf": "https://arxiv.org/abs/1312.5479", "paperhash": "bronstein|sparse_similaritypreserving_hashing", "keywords": [], "conflicts": [], "authors": ["Alex M. Bronstein", "Guillermo Sapiro", "Pablo Sprechmann", "Jonathan Masci", "Michael M. Bronstein"], "authorids": ["alexbronst@gmail.com", "guillermo.sapiro@gmail.com", "pablo.sprechmann@duke.edu", "jonathan.masci@gmail.com", "michael.bronstein@usi.ch"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391787780000, "tcdate": 1391787780000, "number": 2, "id": "XwIVamM38ga3N", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "u-IAYCzRsK-vN", "replyto": "u-IAYCzRsK-vN", "signatures": ["Jonathan Masci"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "We updated the paper and the new version will be available \r\non Mon, 10 Feb, 01:00 GMT."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Sparse similarity-preserving hashing", "decision": "submitted, no decision", "abstract": "In recent years, a lot of attention has been devoted to efficient nearest neighbor search by means of similarity-preserving hashing. One of the plights of existing hashing techniques is the intrinsic trade-off between performance and computational complexity: while longer hash codes allow for lower false positive rates, it is very difficult to increase the embedding dimensionality without incurring in very high false negatives rates or prohibiting computational costs. In this paper, we propose a way to overcome this limitation by enforcing the hash codes to be sparse. Sparse high-dimensional codes enjoy from the low false positive rates typical of long hashes, while keeping the false negative rates similar to those of a shorter dense hashing scheme with equal number of degrees of freedom. We use a tailored feed-forward neural network for the hashing function. Extensive experimental evaluation involving visual and multi-modal data shows the benefits of the proposed method.", "pdf": "https://arxiv.org/abs/1312.5479", "paperhash": "bronstein|sparse_similaritypreserving_hashing", "keywords": [], "conflicts": [], "authors": ["Alex M. Bronstein", "Guillermo Sapiro", "Pablo Sprechmann", "Jonathan Masci", "Michael M. Bronstein"], "authorids": ["alexbronst@gmail.com", "guillermo.sapiro@gmail.com", "pablo.sprechmann@duke.edu", "jonathan.masci@gmail.com", "michael.bronstein@usi.ch"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391604240000, "tcdate": 1391604240000, "number": 1, "id": "VVtOnEaB7_WSx", "invitation": "ICLR.cc/2014/-/submission/conference/reply", "forum": "u-IAYCzRsK-vN", "replyto": "BcgDqCiUDYcXQ", "signatures": ["Jonathan Masci"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "We are grateful to reviewer Anonymous a636 for constructive comments. We reply below to the points raised by the reviewer; we will post additional results requested here and eventually update the arxiv paper. \r\n\r\n\r\n1. We agree (and in fact note it in the paper) that when comparing dense and sparse hashes one has to compare the same number of **degrees of freedom** rather than **length**. Thus, comparing sparse and dense hashes of same length is actually less favorable to sparse hash (and despite that, we show to perform better even in this unfavorable comparison).\r\nReferring specifically to our results, closely comparable hashes would be sparse hash of length 128 and dense hash of length 48 (the exact number of degrees of freedom depends on the sparsity, which varies slightly). \r\nBecause of space limitations we removed the sparsity levels from our tables. They are as follows:\r\n\r\nCIFAR10\r\nm 48,  M 16, alpha 0.01,  lambda 0.1, L0 20.6%\r\nm 48,  M 7,  alpha 0.001, lambda 0.1, L0 39.1%\r\nm 48,  M 7,  alpha 0.001, lambda 0.1, L0 43.9%\r\nm 128, M 16, alpha 0.0,   lambda 0.1, L0 6.0%\r\n\r\nNUS\r\nm 64,   M 7,  alpha 0.05,  lambda 0.3, L0 17.4%\r\nm 64,   M 7,  alpha 0.05,  lambda 1.0, L0 20.1%\r\nm 64,   M 16, alpha 0.005, lambda 0.3, L0 21.7%\r\nm 256,  M 4,  alpha 0.05,  lambda 1.0, L0 6.6%\r\nm 256,  M 4,  alpha 0.05,  lambda 1.0, L0 9.4%\r\nm 256,  M 6,  alpha 0.005, lambda 0.3, L0 9.9%\r\n(here L0 means the number of non-zeros, in % of the hash length m)\r\n\r\n\r\n2. Retrieval is done as follows:\r\nFor large radius (r=m) the search is done exhaustively with complexity O(N), where N is the database size. \r\nFor r=0 (exact collisions), the search is done as a LUT: the query code is fed into the lookup table, containing all entries in the database having the same code. The complexity is O(m), independent of N. \r\nFor small values of r (partial collisions), the search is done as for r=0 using perturbation of the query: at most r bits of the query are changed, and then it is fed into the LUT. The final result is the union of all the retrieval results. Complexity is O(r out of m). \r\n\r\n\r\n3. Sign in ISTA-net: \r\nOur initial formulation converted the output to a ternary representation, doubling the number of bits, and explicitly coding for -1, 0 and +1. However, we found out that the difference of this more proper encoding vs the plain output of the net was negligible and therefore we opted for the simplest solution.\r\n\r\n\r\n4. Eq 4: \r\nIt is a typo, there is a max(0, .). Thanks for pointing it out.\r\n\r\n\r\n5. PR curves were generated using the ranking induced by the Hamming distance between the query and the database samples. In case of r<m we considered only the results falling into the Hamming ball of radius r. \r\n\r\n\r\n6: Table 4 last line: \r\nThe implementations of NN and NN-sparse hashes differ. For sparse hash, we use shrinkage and tanh whereas NN-hash uses only tanh.\r\nAdditionally the two losses also differ (for sparse hash, we measure the L1 distance which we found out already induces some sparsity). These are, in our opinion, the two main differences which explain the different performance and that sparse hash does not produce exactly the same results as NN-hash for alpha=0.\r\n\r\n\r\n7. Additional experiments: \r\nThanks for the suggestion. We will perform the requested experiments and will post them here at a later stage. \r\n\r\n\r\n8. We used a single iteration of ISTA-net in all experiments.\r\n\r\n\r\n9. Fig 6 left: \r\nthe dotted curve (m=128) for NN is there, right above the KSH (dotted purple).\r\nagh2 will be added in the updated version of the paper."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Sparse similarity-preserving hashing", "decision": "submitted, no decision", "abstract": "In recent years, a lot of attention has been devoted to efficient nearest neighbor search by means of similarity-preserving hashing. One of the plights of existing hashing techniques is the intrinsic trade-off between performance and computational complexity: while longer hash codes allow for lower false positive rates, it is very difficult to increase the embedding dimensionality without incurring in very high false negatives rates or prohibiting computational costs. In this paper, we propose a way to overcome this limitation by enforcing the hash codes to be sparse. Sparse high-dimensional codes enjoy from the low false positive rates typical of long hashes, while keeping the false negative rates similar to those of a shorter dense hashing scheme with equal number of degrees of freedom. We use a tailored feed-forward neural network for the hashing function. Extensive experimental evaluation involving visual and multi-modal data shows the benefits of the proposed method.", "pdf": "https://arxiv.org/abs/1312.5479", "paperhash": "bronstein|sparse_similaritypreserving_hashing", "keywords": [], "conflicts": [], "authors": ["Alex M. Bronstein", "Guillermo Sapiro", "Pablo Sprechmann", "Jonathan Masci", "Michael M. Bronstein"], "authorids": ["alexbronst@gmail.com", "guillermo.sapiro@gmail.com", "pablo.sprechmann@duke.edu", "jonathan.masci@gmail.com", "michael.bronstein@usi.ch"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391511480000, "tcdate": 1391511480000, "number": 1, "id": "BcgDqCiUDYcXQ", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "u-IAYCzRsK-vN", "replyto": "u-IAYCzRsK-vN", "signatures": ["anonymous reviewer a636"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Sparse similarity-preserving hashing", "review": "This work presents a similarity-preserving hashing scheme to produce binary codes that work well for small-radius hamming search.  The approach restricts hash codes to be sparse, thereby limiting the number of possible codes for a given length of m bits.  A small-radius search within the set of valid codes will therefore have more hits, yet the total bit length can be lengthened to allow better representation of similarities.  According to the authors, this is the first application of sparsity constraints in the context of binary similarity hashes.\r\n\r\nOverall I think this is a nice, well-motivated idea and corresponding implementation, with experiments on two datasets that demonstrate its effectiveness and mostly support the claims made in the motivation.  As a bonus, the authors describe a further adaptation to cross-modal data.\r\n\r\nI still feel there are some links missing between the analysis, implementation and evaluation that could be made more explicit, and have quite a few questions (see below).\r\n\r\n\r\nPros:\r\n\r\n- Well-argued idea for improving hashing by restricting the code set to enable targeting small search radii and large bit lengths\r\n\r\n- Experiments show good system performance\r\n\r\n\r\nCons:\r\n\r\n- Links between motivational analysis, implementation and evaluation could be made more explicit\r\n\r\n- Related to that, some claims alluded to in the motivation don't appear fully supported, e.g. more efficient search for k-sparse codes doesn't seem realized beyond keeping r small\r\n\r\n\r\nQuestions:\r\n\r\n- You mention comparing between k-sparse codes of length m and dense codes with the same degrees of freedom, i.e. of length log(m choose k).  This seems very appropriate, but the evaluation seems to compare same-length codes between methods.  Or, do the values of m reflect this comparison?  m=128 v. m=48 may work if k is around 10.  But I also don't see anything showing the distribution of nonzeros in the codes.\r\n\r\n- pg. 4:  'retrieving partial collisions of sparse binary vectors is ... less complex ... compared to their dense counterparts':   Could this be explained in more detail?  It seems a regular exhaustive hamming search is used in the implemented system (especially since there appears to be no hard limit on k, so any small change can valid for most codes).\r\n\r\n- The ISTA-net uses a sign-preserving shrink, and the outputs are binarized with tanh (also preserving sign) -- thus nonzeros of the ISTA-net can saturate to either +/- 1 depending on sign, while zeros are mapped to 0.  These are 3 values, not 2, so how are they converted to a binary vector, and how does this align with the xi(x) - xi(x') in the loss function (which seems to count a penalty for comparing +1 with 0 and a double-penalty for comparing +1 with -1)?\r\n\r\n- Eqn. 4:  Distance loss between codes is L1 instead of L2, and there is no max(0, .) on the negatives (but still a margin M).  Are these errors or intended?\r\n\r\n- I'm not sure how the PR curves were generated:  What ranking was used?\r\n\r\n- Table 4 last line: Says alpha=0; it seems the sparsity term would be disabled if alpha=0, so not sure why results here are better than NN-hash instead of about the same?\r\n\r\n\r\nMinor comments:\r\n\r\n- Would have liked to see more comparing level of sparsity vs. precision/recall for different small r.  There is a bit of this in the tables, but it would be interesting to see more comprehensive measurements here.  It would be great if there was a 2d array with e.g. r on the x axis and avg number of nonzeros on the y axis, for one or more fixed m.\r\n\r\n- How many layers/iterations were used in the ISTA-net?\r\n\r\n- Fig 6 left, curves for m=128 appear missing for nnhash and agh2"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Sparse similarity-preserving hashing", "decision": "submitted, no decision", "abstract": "In recent years, a lot of attention has been devoted to efficient nearest neighbor search by means of similarity-preserving hashing. One of the plights of existing hashing techniques is the intrinsic trade-off between performance and computational complexity: while longer hash codes allow for lower false positive rates, it is very difficult to increase the embedding dimensionality without incurring in very high false negatives rates or prohibiting computational costs. In this paper, we propose a way to overcome this limitation by enforcing the hash codes to be sparse. Sparse high-dimensional codes enjoy from the low false positive rates typical of long hashes, while keeping the false negative rates similar to those of a shorter dense hashing scheme with equal number of degrees of freedom. We use a tailored feed-forward neural network for the hashing function. Extensive experimental evaluation involving visual and multi-modal data shows the benefits of the proposed method.", "pdf": "https://arxiv.org/abs/1312.5479", "paperhash": "bronstein|sparse_similaritypreserving_hashing", "keywords": [], "conflicts": [], "authors": ["Alex M. Bronstein", "Guillermo Sapiro", "Pablo Sprechmann", "Jonathan Masci", "Michael M. Bronstein"], "authorids": ["alexbronst@gmail.com", "guillermo.sapiro@gmail.com", "pablo.sprechmann@duke.edu", "jonathan.masci@gmail.com", "michael.bronstein@usi.ch"]}, "tags": [], "invitation": {}}}, {"replyto": null, "ddate": null, "legacy_migration": true, "tmdate": 1387543560000, "tcdate": 1387543560000, "number": 16, "id": "u-IAYCzRsK-vN", "invitation": "ICLR.cc/2014/conference/-/submission", "forum": "u-IAYCzRsK-vN", "signatures": ["alexbronst@gmail.com"], "readers": ["everyone"], "content": {"title": "Sparse similarity-preserving hashing", "decision": "submitted, no decision", "abstract": "In recent years, a lot of attention has been devoted to efficient nearest neighbor search by means of similarity-preserving hashing. One of the plights of existing hashing techniques is the intrinsic trade-off between performance and computational complexity: while longer hash codes allow for lower false positive rates, it is very difficult to increase the embedding dimensionality without incurring in very high false negatives rates or prohibiting computational costs. In this paper, we propose a way to overcome this limitation by enforcing the hash codes to be sparse. Sparse high-dimensional codes enjoy from the low false positive rates typical of long hashes, while keeping the false negative rates similar to those of a shorter dense hashing scheme with equal number of degrees of freedom. We use a tailored feed-forward neural network for the hashing function. Extensive experimental evaluation involving visual and multi-modal data shows the benefits of the proposed method.", "pdf": "https://arxiv.org/abs/1312.5479", "paperhash": "bronstein|sparse_similaritypreserving_hashing", "keywords": [], "conflicts": [], "authors": ["Alex M. Bronstein", "Guillermo Sapiro", "Pablo Sprechmann", "Jonathan Masci", "Michael M. Bronstein"], "authorids": ["alexbronst@gmail.com", "guillermo.sapiro@gmail.com", "pablo.sprechmann@duke.edu", "jonathan.masci@gmail.com", "michael.bronstein@usi.ch"]}, "writers": [], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1369422751717, "tmdate": 1496674357195, "id": "ICLR.cc/2014/conference/-/submission", "writers": ["ICLR.cc/2014"], "signatures": ["OpenReview.net"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": []}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1377198751717, "cdate": 1496674357195}}}], "count": 10}