{"notes": [{"id": "H1gN6kSFwS", "original": "B1gDXEkFPS", "number": 1986, "cdate": 1569439676363, "ddate": null, "tcdate": 1569439676363, "tmdate": 1577168246652, "tddate": null, "forum": "H1gN6kSFwS", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["rosemary.nan.ke@gmail.com", "obilaniu@gmail.com", "anirudhgoyal9119@gmail.com", "stefan.a.bauer@gmail.com", "hugolarochelle@google.com", "chris.j.pal@gmail.com", "yoshua.bengio@mila.quebec"], "title": "Learning Neural Causal Models from Unknown Interventions", "authors": ["Nan Rosemary Ke", "Olexa Bilaniuk", "Anirudh Goyal", "Stephan Bauer", "Hugol Larochelle", "Chris Pal", "Yoshua Bengio"], "pdf": "/pdf/909851dd475666c92f50fb6a01f5c71b8b6432f3.pdf", "TL;DR": "Using end-to-end deep learning to discover the structure of a graphical model which is robust to interventions and trained without knowing what the interventions are", "abstract": "Meta-learning over a set of distributions can be interpreted as learning different types of parameters corresponding to short-term vs long-term aspects of the mechanisms underlying the generation of data. These are respectively captured by quickly-changing \\textit{parameters} and slowly-changing \\textit{meta-parameters}. We present a new framework for meta-learning causal models where the relationship between each variable and its parents is modeled by a neural network, modulated by structural meta-parameters which capture the overall topology of a directed graphical model. Our approach avoids a discrete search over models in favour of a continuous optimization procedure. We study a setting where interventional distributions are induced as a result of a random intervention on a single unknown variable of an unknown ground truth causal model, and the observations arising after such an intervention constitute one meta-example. To disentangle the slow-changing aspects of each conditional from the fast-changing adaptations to each intervention, we parametrize the neural network into fast parameters and slow meta-parameters. We introduce a meta-learning objective that favours solutions \\textit{robust} to frequent but sparse interventional distribution change, and which generalize well to previously unseen interventions. Optimizing this objective is shown experimentally to recover the structure of the causal graph. Finally, we find that when the learner is unaware of the intervention variable, it is able to infer that information, improving results further and focusing the parameter and meta-parameter updates where needed.", "keywords": ["deep learning", "graphical models", "meta learning"], "paperhash": "ke|learning_neural_causal_models_from_unknown_interventions", "original_pdf": "/attachment/46cdf6676b73a75437c5ddaca23f9b373e44a459.pdf", "_bibtex": "@misc{\nke2020learning,\ntitle={Learning Neural Causal Models from Unknown Interventions},\nauthor={Nan Rosemary Ke and Olexa Bilaniuk and Anirudh Goyal and Stephan Bauer and Hugol Larochelle and Chris Pal and Yoshua Bengio},\nyear={2020},\nurl={https://openreview.net/forum?id=H1gN6kSFwS}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 17, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "ERpMJZEjqI", "original": null, "number": 1, "cdate": 1576798737634, "ddate": null, "tcdate": 1576798737634, "tmdate": 1576800898737, "tddate": null, "forum": "H1gN6kSFwS", "replyto": "H1gN6kSFwS", "invitation": "ICLR.cc/2020/Conference/Paper1986/-/Decision", "content": {"decision": "Reject", "comment": "This paper proposes a metalearning objective to infer causal graphs from data based on masked neural networks to capture arbitrary conditional relationships. While the authors agree that the paper contains various interesting ideas, the theoretical and conceptual underpinnings of the proposed methodology are still lacking and the experiments cannot sufficiently make up for this. The method is definitely worth exploring more and a revision is likely to be accepted at another venue.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["rosemary.nan.ke@gmail.com", "obilaniu@gmail.com", "anirudhgoyal9119@gmail.com", "stefan.a.bauer@gmail.com", "hugolarochelle@google.com", "chris.j.pal@gmail.com", "yoshua.bengio@mila.quebec"], "title": "Learning Neural Causal Models from Unknown Interventions", "authors": ["Nan Rosemary Ke", "Olexa Bilaniuk", "Anirudh Goyal", "Stephan Bauer", "Hugol Larochelle", "Chris Pal", "Yoshua Bengio"], "pdf": "/pdf/909851dd475666c92f50fb6a01f5c71b8b6432f3.pdf", "TL;DR": "Using end-to-end deep learning to discover the structure of a graphical model which is robust to interventions and trained without knowing what the interventions are", "abstract": "Meta-learning over a set of distributions can be interpreted as learning different types of parameters corresponding to short-term vs long-term aspects of the mechanisms underlying the generation of data. These are respectively captured by quickly-changing \\textit{parameters} and slowly-changing \\textit{meta-parameters}. We present a new framework for meta-learning causal models where the relationship between each variable and its parents is modeled by a neural network, modulated by structural meta-parameters which capture the overall topology of a directed graphical model. Our approach avoids a discrete search over models in favour of a continuous optimization procedure. We study a setting where interventional distributions are induced as a result of a random intervention on a single unknown variable of an unknown ground truth causal model, and the observations arising after such an intervention constitute one meta-example. To disentangle the slow-changing aspects of each conditional from the fast-changing adaptations to each intervention, we parametrize the neural network into fast parameters and slow meta-parameters. We introduce a meta-learning objective that favours solutions \\textit{robust} to frequent but sparse interventional distribution change, and which generalize well to previously unseen interventions. Optimizing this objective is shown experimentally to recover the structure of the causal graph. Finally, we find that when the learner is unaware of the intervention variable, it is able to infer that information, improving results further and focusing the parameter and meta-parameter updates where needed.", "keywords": ["deep learning", "graphical models", "meta learning"], "paperhash": "ke|learning_neural_causal_models_from_unknown_interventions", "original_pdf": "/attachment/46cdf6676b73a75437c5ddaca23f9b373e44a459.pdf", "_bibtex": "@misc{\nke2020learning,\ntitle={Learning Neural Causal Models from Unknown Interventions},\nauthor={Nan Rosemary Ke and Olexa Bilaniuk and Anirudh Goyal and Stephan Bauer and Hugol Larochelle and Chris Pal and Yoshua Bengio},\nyear={2020},\nurl={https://openreview.net/forum?id=H1gN6kSFwS}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "H1gN6kSFwS", "replyto": "H1gN6kSFwS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795707417, "tmdate": 1576800255621, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1986/-/Decision"}}}, {"id": "rJlp-hUnjr", "original": null, "number": 19, "cdate": 1573837828629, "ddate": null, "tcdate": 1573837828629, "tmdate": 1573837828629, "tddate": null, "forum": "H1gN6kSFwS", "replyto": "SJlO234hsr", "invitation": "ICLR.cc/2020/Conference/Paper1986/-/Official_Comment", "content": {"title": "Rebuttal Discussion", "comment": "Dear Reviewer, \n\nCould you let us know if our response has addressed the concerns raised in your review? I think our response in point (b) above clarifies your main concern about insufficient comparisons (as in Asia graph, it was not simulated from MLP).\n\nWe would be happy to provide further revisions  to address any remaining issues and would appreciate a response from you on the points that we raised (as rebuttal period is going to end soonish).\n\nThanks for taking time and discussing with the authors. We appreciate it. :)"}, "signatures": ["ICLR.cc/2020/Conference/Paper1986/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1986/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["rosemary.nan.ke@gmail.com", "obilaniu@gmail.com", "anirudhgoyal9119@gmail.com", "stefan.a.bauer@gmail.com", "hugolarochelle@google.com", "chris.j.pal@gmail.com", "yoshua.bengio@mila.quebec"], "title": "Learning Neural Causal Models from Unknown Interventions", "authors": ["Nan Rosemary Ke", "Olexa Bilaniuk", "Anirudh Goyal", "Stephan Bauer", "Hugol Larochelle", "Chris Pal", "Yoshua Bengio"], "pdf": "/pdf/909851dd475666c92f50fb6a01f5c71b8b6432f3.pdf", "TL;DR": "Using end-to-end deep learning to discover the structure of a graphical model which is robust to interventions and trained without knowing what the interventions are", "abstract": "Meta-learning over a set of distributions can be interpreted as learning different types of parameters corresponding to short-term vs long-term aspects of the mechanisms underlying the generation of data. These are respectively captured by quickly-changing \\textit{parameters} and slowly-changing \\textit{meta-parameters}. We present a new framework for meta-learning causal models where the relationship between each variable and its parents is modeled by a neural network, modulated by structural meta-parameters which capture the overall topology of a directed graphical model. Our approach avoids a discrete search over models in favour of a continuous optimization procedure. We study a setting where interventional distributions are induced as a result of a random intervention on a single unknown variable of an unknown ground truth causal model, and the observations arising after such an intervention constitute one meta-example. To disentangle the slow-changing aspects of each conditional from the fast-changing adaptations to each intervention, we parametrize the neural network into fast parameters and slow meta-parameters. We introduce a meta-learning objective that favours solutions \\textit{robust} to frequent but sparse interventional distribution change, and which generalize well to previously unseen interventions. Optimizing this objective is shown experimentally to recover the structure of the causal graph. Finally, we find that when the learner is unaware of the intervention variable, it is able to infer that information, improving results further and focusing the parameter and meta-parameter updates where needed.", "keywords": ["deep learning", "graphical models", "meta learning"], "paperhash": "ke|learning_neural_causal_models_from_unknown_interventions", "original_pdf": "/attachment/46cdf6676b73a75437c5ddaca23f9b373e44a459.pdf", "_bibtex": "@misc{\nke2020learning,\ntitle={Learning Neural Causal Models from Unknown Interventions},\nauthor={Nan Rosemary Ke and Olexa Bilaniuk and Anirudh Goyal and Stephan Bauer and Hugol Larochelle and Chris Pal and Yoshua Bengio},\nyear={2020},\nurl={https://openreview.net/forum?id=H1gN6kSFwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1gN6kSFwS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1986/Authors", "ICLR.cc/2020/Conference/Paper1986/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1986/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1986/Reviewers", "ICLR.cc/2020/Conference/Paper1986/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1986/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1986/Authors|ICLR.cc/2020/Conference/Paper1986/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504147982, "tmdate": 1576860548484, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1986/Authors", "ICLR.cc/2020/Conference/Paper1986/Reviewers", "ICLR.cc/2020/Conference/Paper1986/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1986/-/Official_Comment"}}}, {"id": "SJlO234hsr", "original": null, "number": 17, "cdate": 1573829807985, "ddate": null, "tcdate": 1573829807985, "tmdate": 1573829807985, "tddate": null, "forum": "H1gN6kSFwS", "replyto": "BkgBSh0iiS", "invitation": "ICLR.cc/2020/Conference/Paper1986/-/Official_Comment", "content": {"title": "Response to Reviewer 1", "comment": "We thank the reviewer for his feedback. We would like to point out that: \n\n(a) We agree that the concern about hyper-parameter selection is valid for all neural network based methods but would like to point out that for each method we applied the same budget for hyper-parameter search. In addition, for full clarity and future benchmarking all our code will be released and made accessible. \n\n(b) We politely disagree with the reviewer that the comparison is inappropriate or unfair. The asia graph (which is defined in the BN Learn repository) dataset we use to evaluate all comparison methods is a real-world dataset and the underlying relationships are not known. In particular, they were not simulated from an MLP. \n\n(c) While we agree that the non-linearity in non-linear ICP adds flexible, the method has the fundamental problem that it relies on conditional independence testing, which is hard (Peters and Shah 2019). As pointed out by the author\u2019s of non-linear ICP themselves and referenced in our response Part 3, a comparison is not recommended \u201cIn practice\u201d (Conclusion p.24 in non-linear ICP, Heinze-Deml et.al 2018). Nevertheless, we will try to add a comparison to non-linear ICP. \n\n(d) We would like to point out that we already compare against 3 methods, in particular to the state-of-the art method for causal induction from interventional data, as noted in [1]. In general, we examined an array of causal learning methods where an open-source implementation is available. However, many of these methods can only handle continuous data (not discrete data) e.g. LinGAM, while many others do not handle interventions. We compare against all methods, which are applicable in our case, provide open-source code and were the authors themselves do not provide alternative recommendations on how to proceed in practice. \n\n[1] Versteg, Boosting Local Causal Discovery in High-Dimensional Expression Data https://arxiv.org/pdf/1910.02505v2.pdf\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1986/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1986/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["rosemary.nan.ke@gmail.com", "obilaniu@gmail.com", "anirudhgoyal9119@gmail.com", "stefan.a.bauer@gmail.com", "hugolarochelle@google.com", "chris.j.pal@gmail.com", "yoshua.bengio@mila.quebec"], "title": "Learning Neural Causal Models from Unknown Interventions", "authors": ["Nan Rosemary Ke", "Olexa Bilaniuk", "Anirudh Goyal", "Stephan Bauer", "Hugol Larochelle", "Chris Pal", "Yoshua Bengio"], "pdf": "/pdf/909851dd475666c92f50fb6a01f5c71b8b6432f3.pdf", "TL;DR": "Using end-to-end deep learning to discover the structure of a graphical model which is robust to interventions and trained without knowing what the interventions are", "abstract": "Meta-learning over a set of distributions can be interpreted as learning different types of parameters corresponding to short-term vs long-term aspects of the mechanisms underlying the generation of data. These are respectively captured by quickly-changing \\textit{parameters} and slowly-changing \\textit{meta-parameters}. We present a new framework for meta-learning causal models where the relationship between each variable and its parents is modeled by a neural network, modulated by structural meta-parameters which capture the overall topology of a directed graphical model. Our approach avoids a discrete search over models in favour of a continuous optimization procedure. We study a setting where interventional distributions are induced as a result of a random intervention on a single unknown variable of an unknown ground truth causal model, and the observations arising after such an intervention constitute one meta-example. To disentangle the slow-changing aspects of each conditional from the fast-changing adaptations to each intervention, we parametrize the neural network into fast parameters and slow meta-parameters. We introduce a meta-learning objective that favours solutions \\textit{robust} to frequent but sparse interventional distribution change, and which generalize well to previously unseen interventions. Optimizing this objective is shown experimentally to recover the structure of the causal graph. Finally, we find that when the learner is unaware of the intervention variable, it is able to infer that information, improving results further and focusing the parameter and meta-parameter updates where needed.", "keywords": ["deep learning", "graphical models", "meta learning"], "paperhash": "ke|learning_neural_causal_models_from_unknown_interventions", "original_pdf": "/attachment/46cdf6676b73a75437c5ddaca23f9b373e44a459.pdf", "_bibtex": "@misc{\nke2020learning,\ntitle={Learning Neural Causal Models from Unknown Interventions},\nauthor={Nan Rosemary Ke and Olexa Bilaniuk and Anirudh Goyal and Stephan Bauer and Hugol Larochelle and Chris Pal and Yoshua Bengio},\nyear={2020},\nurl={https://openreview.net/forum?id=H1gN6kSFwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1gN6kSFwS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1986/Authors", "ICLR.cc/2020/Conference/Paper1986/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1986/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1986/Reviewers", "ICLR.cc/2020/Conference/Paper1986/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1986/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1986/Authors|ICLR.cc/2020/Conference/Paper1986/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504147982, "tmdate": 1576860548484, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1986/Authors", "ICLR.cc/2020/Conference/Paper1986/Reviewers", "ICLR.cc/2020/Conference/Paper1986/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1986/-/Official_Comment"}}}, {"id": "BkgBSh0iiS", "original": null, "number": 16, "cdate": 1573805117311, "ddate": null, "tcdate": 1573805117311, "tmdate": 1573805117311, "tddate": null, "forum": "H1gN6kSFwS", "replyto": "B1gMQPO5jS", "invitation": "ICLR.cc/2020/Conference/Paper1986/-/Official_Comment", "content": {"title": "Comments re revision", "comment": "I applaud the authors for making good revisions to their paper.\n\nHowever, my main concern still stands.  Without any theoretical/conceptual underpinnings of when the proposed methodology will/won't work, all support for the proposed methodology must come from the empirical experiments.  Unfortunately, I still do not believe experiments are comprehensive enough to convince me of the\u00a0strength of the proposed methodology.  In particular, there are simply insufficient comparisons against other methods.  The proposed methodology does not appear better than Eaton & Murphy (2007), and the ICP method is clearly inappropriate as a linear model being applied in a setting where the underlying (simulated) relationships are known to be nonlinear (in fact match the MLP used in the authors method which is even more unfair).  Finally, the method of Zheng et al (2018) depends on various hyperparameters, and it is unclear whether these were set more or less favorably than the authors' method."}, "signatures": ["ICLR.cc/2020/Conference/Paper1986/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1986/AnonReviewer1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["rosemary.nan.ke@gmail.com", "obilaniu@gmail.com", "anirudhgoyal9119@gmail.com", "stefan.a.bauer@gmail.com", "hugolarochelle@google.com", "chris.j.pal@gmail.com", "yoshua.bengio@mila.quebec"], "title": "Learning Neural Causal Models from Unknown Interventions", "authors": ["Nan Rosemary Ke", "Olexa Bilaniuk", "Anirudh Goyal", "Stephan Bauer", "Hugol Larochelle", "Chris Pal", "Yoshua Bengio"], "pdf": "/pdf/909851dd475666c92f50fb6a01f5c71b8b6432f3.pdf", "TL;DR": "Using end-to-end deep learning to discover the structure of a graphical model which is robust to interventions and trained without knowing what the interventions are", "abstract": "Meta-learning over a set of distributions can be interpreted as learning different types of parameters corresponding to short-term vs long-term aspects of the mechanisms underlying the generation of data. These are respectively captured by quickly-changing \\textit{parameters} and slowly-changing \\textit{meta-parameters}. We present a new framework for meta-learning causal models where the relationship between each variable and its parents is modeled by a neural network, modulated by structural meta-parameters which capture the overall topology of a directed graphical model. Our approach avoids a discrete search over models in favour of a continuous optimization procedure. We study a setting where interventional distributions are induced as a result of a random intervention on a single unknown variable of an unknown ground truth causal model, and the observations arising after such an intervention constitute one meta-example. To disentangle the slow-changing aspects of each conditional from the fast-changing adaptations to each intervention, we parametrize the neural network into fast parameters and slow meta-parameters. We introduce a meta-learning objective that favours solutions \\textit{robust} to frequent but sparse interventional distribution change, and which generalize well to previously unseen interventions. Optimizing this objective is shown experimentally to recover the structure of the causal graph. Finally, we find that when the learner is unaware of the intervention variable, it is able to infer that information, improving results further and focusing the parameter and meta-parameter updates where needed.", "keywords": ["deep learning", "graphical models", "meta learning"], "paperhash": "ke|learning_neural_causal_models_from_unknown_interventions", "original_pdf": "/attachment/46cdf6676b73a75437c5ddaca23f9b373e44a459.pdf", "_bibtex": "@misc{\nke2020learning,\ntitle={Learning Neural Causal Models from Unknown Interventions},\nauthor={Nan Rosemary Ke and Olexa Bilaniuk and Anirudh Goyal and Stephan Bauer and Hugol Larochelle and Chris Pal and Yoshua Bengio},\nyear={2020},\nurl={https://openreview.net/forum?id=H1gN6kSFwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1gN6kSFwS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1986/Authors", "ICLR.cc/2020/Conference/Paper1986/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1986/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1986/Reviewers", "ICLR.cc/2020/Conference/Paper1986/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1986/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1986/Authors|ICLR.cc/2020/Conference/Paper1986/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504147982, "tmdate": 1576860548484, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1986/Authors", "ICLR.cc/2020/Conference/Paper1986/Reviewers", "ICLR.cc/2020/Conference/Paper1986/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1986/-/Official_Comment"}}}, {"id": "r1eGmbwssr", "original": null, "number": 14, "cdate": 1573773594007, "ddate": null, "tcdate": 1573773594007, "tmdate": 1573773594007, "tddate": null, "forum": "H1gN6kSFwS", "replyto": "HJxO6r5HYr", "invitation": "ICLR.cc/2020/Conference/Paper1986/-/Official_Comment", "content": {"title": "Response to \"Official Blind Review #2\"", "comment": "We are grateful to the reviewer for their enthusiastic feedback and comments!\n\nQ: \u201cIt would be informative if the paper had a paragraph discussing also the fundamental limitations of the approach more openly. For instance, the choice of the neural net architecture used for the structural assignment might have a huge impact on the outcome, especially because the same architecture is repetitively used for all variables of the SCM.\u201d\n\nWe thank the reviewer for pointing this out. We had chosen to implement all variables using the same neural network architecture for computational reasons (vectorization, batching), but it indeed might have had a significant impact on the learning process. A wider variety of architectures, incorporating heterogeneity in each variable\u2019s model, would strengthen the case for the approach.\n\nThere are, however, recent demonstrations that overparameterized neural networks can generalize well (with some regularization) [1]. This suggests that we may get away with deliberate over-parametrization, whether of each module separately or the whole network globally. The reviewer\u2019s proposal below to allow some cross-variable parameter sharing is compatible with the latter; The right capacity and level of sharing for each variable would then be allocated according to the different pressures from the data and the training objective.\n\n[1]. Belkin, Mikhail, Daniel Hsu, Siyuan Ma, and Soumik Mandal. \"Reconciling modern machine-learning practice and the classical bias\u2013variance trade-off.\" Proceedings of the National Academy of Sciences 116, no. 32 (2019): 15849-15854.\n\nQ. \u201cThe paper makes a strong scalability claim across the variable size thanks to independent Bernoullis assigned on the adjacency matrix entries. However, it reports results only for very small SCMs. It is understandable that given the premature stage of the causal inference research might not grant standardized data sets at a larger scale, but at least lack of this quantitative scalability test could be acknowledged and the related claims could be a little bit softened.\u201d\n\nWe thank the reviewer for pointing this out. We agree with the reviewer\u2019s point and a necessary continuation of our work is to demonstrate scaling to larger graphs available from e.g. the Bayesian Networks Repository. We will soften our scalability claims to better accord with the size of the problems solved in the paper.\n\nQ. \u201cI do not buy the argument in the first paragraph of Sec 3.5 about why the structural assignment functions need to be independent. As the model does not pose a distribution on neural net weights, sharing some weights (i.e. conditioning on them) would only bring conditional independence across the variables. I do not see a solid reason to try to avoid this. What is wrong for multiple variables to share some functional characteristics in their structural assignment? After all, some sort of conditional independence will be inevitable in modeling. If the variables share the same architecture, this is also conditional independence, not full independence. Relaxing the independence assumption and allowing some weight sharing could be beneficial at least for scalability of the model, could even bring about improved model fit due to cross-variable knowledge transfer.\u201d\n\nWe appreciate the thought-provoking idea from the reviewer of cross-variable knowledge transfer via sharing. This is especially applicable to the real world setting, where it is likely that functional characteristics will be shared between variables of a similar nature. We will gladly investigate further this issue.\n\nOverall, we would like to thank the reviewer for the positive feedback and comments. We will perform the changes the reviewer recommends and relax the enforced independences to see if scalability or performance gains materialize.\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1986/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1986/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["rosemary.nan.ke@gmail.com", "obilaniu@gmail.com", "anirudhgoyal9119@gmail.com", "stefan.a.bauer@gmail.com", "hugolarochelle@google.com", "chris.j.pal@gmail.com", "yoshua.bengio@mila.quebec"], "title": "Learning Neural Causal Models from Unknown Interventions", "authors": ["Nan Rosemary Ke", "Olexa Bilaniuk", "Anirudh Goyal", "Stephan Bauer", "Hugol Larochelle", "Chris Pal", "Yoshua Bengio"], "pdf": "/pdf/909851dd475666c92f50fb6a01f5c71b8b6432f3.pdf", "TL;DR": "Using end-to-end deep learning to discover the structure of a graphical model which is robust to interventions and trained without knowing what the interventions are", "abstract": "Meta-learning over a set of distributions can be interpreted as learning different types of parameters corresponding to short-term vs long-term aspects of the mechanisms underlying the generation of data. These are respectively captured by quickly-changing \\textit{parameters} and slowly-changing \\textit{meta-parameters}. We present a new framework for meta-learning causal models where the relationship between each variable and its parents is modeled by a neural network, modulated by structural meta-parameters which capture the overall topology of a directed graphical model. Our approach avoids a discrete search over models in favour of a continuous optimization procedure. We study a setting where interventional distributions are induced as a result of a random intervention on a single unknown variable of an unknown ground truth causal model, and the observations arising after such an intervention constitute one meta-example. To disentangle the slow-changing aspects of each conditional from the fast-changing adaptations to each intervention, we parametrize the neural network into fast parameters and slow meta-parameters. We introduce a meta-learning objective that favours solutions \\textit{robust} to frequent but sparse interventional distribution change, and which generalize well to previously unseen interventions. Optimizing this objective is shown experimentally to recover the structure of the causal graph. Finally, we find that when the learner is unaware of the intervention variable, it is able to infer that information, improving results further and focusing the parameter and meta-parameter updates where needed.", "keywords": ["deep learning", "graphical models", "meta learning"], "paperhash": "ke|learning_neural_causal_models_from_unknown_interventions", "original_pdf": "/attachment/46cdf6676b73a75437c5ddaca23f9b373e44a459.pdf", "_bibtex": "@misc{\nke2020learning,\ntitle={Learning Neural Causal Models from Unknown Interventions},\nauthor={Nan Rosemary Ke and Olexa Bilaniuk and Anirudh Goyal and Stephan Bauer and Hugol Larochelle and Chris Pal and Yoshua Bengio},\nyear={2020},\nurl={https://openreview.net/forum?id=H1gN6kSFwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1gN6kSFwS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1986/Authors", "ICLR.cc/2020/Conference/Paper1986/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1986/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1986/Reviewers", "ICLR.cc/2020/Conference/Paper1986/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1986/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1986/Authors|ICLR.cc/2020/Conference/Paper1986/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504147982, "tmdate": 1576860548484, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1986/Authors", "ICLR.cc/2020/Conference/Paper1986/Reviewers", "ICLR.cc/2020/Conference/Paper1986/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1986/-/Official_Comment"}}}, {"id": "B1gMQPO5jS", "original": null, "number": 12, "cdate": 1573713689962, "ddate": null, "tcdate": 1573713689962, "tmdate": 1573713891710, "tddate": null, "forum": "H1gN6kSFwS", "replyto": "Skx29AZdFH", "invitation": "ICLR.cc/2020/Conference/Paper1986/-/Official_Comment", "content": {"title": "Revised paper uploaded", "comment": "Dear reviewer #1, \n\nWe\u2019d like to thank you again for your review and feedback! We have updated our paper with your suggestions and those of others. In particular, we made the suggested citations to related work in section 4, included a section explaining the intervention in section 3.5, and began re-running the experiments with 5 random seeds each and reporting the error bars (Figure 5 Left is a beginning). We are also running experiments while varying the number of hidden states, as you have suggested. Would you have any other questions regarding the rebuttal?  We would be happy to provide further revisions or experiments to address any remaining issues. Many thanks again for your review and feedback.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1986/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1986/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["rosemary.nan.ke@gmail.com", "obilaniu@gmail.com", "anirudhgoyal9119@gmail.com", "stefan.a.bauer@gmail.com", "hugolarochelle@google.com", "chris.j.pal@gmail.com", "yoshua.bengio@mila.quebec"], "title": "Learning Neural Causal Models from Unknown Interventions", "authors": ["Nan Rosemary Ke", "Olexa Bilaniuk", "Anirudh Goyal", "Stephan Bauer", "Hugol Larochelle", "Chris Pal", "Yoshua Bengio"], "pdf": "/pdf/909851dd475666c92f50fb6a01f5c71b8b6432f3.pdf", "TL;DR": "Using end-to-end deep learning to discover the structure of a graphical model which is robust to interventions and trained without knowing what the interventions are", "abstract": "Meta-learning over a set of distributions can be interpreted as learning different types of parameters corresponding to short-term vs long-term aspects of the mechanisms underlying the generation of data. These are respectively captured by quickly-changing \\textit{parameters} and slowly-changing \\textit{meta-parameters}. We present a new framework for meta-learning causal models where the relationship between each variable and its parents is modeled by a neural network, modulated by structural meta-parameters which capture the overall topology of a directed graphical model. Our approach avoids a discrete search over models in favour of a continuous optimization procedure. We study a setting where interventional distributions are induced as a result of a random intervention on a single unknown variable of an unknown ground truth causal model, and the observations arising after such an intervention constitute one meta-example. To disentangle the slow-changing aspects of each conditional from the fast-changing adaptations to each intervention, we parametrize the neural network into fast parameters and slow meta-parameters. We introduce a meta-learning objective that favours solutions \\textit{robust} to frequent but sparse interventional distribution change, and which generalize well to previously unseen interventions. Optimizing this objective is shown experimentally to recover the structure of the causal graph. Finally, we find that when the learner is unaware of the intervention variable, it is able to infer that information, improving results further and focusing the parameter and meta-parameter updates where needed.", "keywords": ["deep learning", "graphical models", "meta learning"], "paperhash": "ke|learning_neural_causal_models_from_unknown_interventions", "original_pdf": "/attachment/46cdf6676b73a75437c5ddaca23f9b373e44a459.pdf", "_bibtex": "@misc{\nke2020learning,\ntitle={Learning Neural Causal Models from Unknown Interventions},\nauthor={Nan Rosemary Ke and Olexa Bilaniuk and Anirudh Goyal and Stephan Bauer and Hugol Larochelle and Chris Pal and Yoshua Bengio},\nyear={2020},\nurl={https://openreview.net/forum?id=H1gN6kSFwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1gN6kSFwS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1986/Authors", "ICLR.cc/2020/Conference/Paper1986/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1986/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1986/Reviewers", "ICLR.cc/2020/Conference/Paper1986/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1986/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1986/Authors|ICLR.cc/2020/Conference/Paper1986/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504147982, "tmdate": 1576860548484, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1986/Authors", "ICLR.cc/2020/Conference/Paper1986/Reviewers", "ICLR.cc/2020/Conference/Paper1986/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1986/-/Official_Comment"}}}, {"id": "rylbZRDpYS", "original": null, "number": 3, "cdate": 1571810809108, "ddate": null, "tcdate": 1571810809108, "tmdate": 1573704901609, "tddate": null, "forum": "H1gN6kSFwS", "replyto": "H1gN6kSFwS", "invitation": "ICLR.cc/2020/Conference/Paper1986/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #3", "review": "This paper proposes a MAML objective to learn causal graphs from data. The data in question is randomized but the algorithm does not have access to the identity of the intervention variable. So there is an added layer of complexity of deciphering which variable was intervened on. The MAML objective, in this case, links the causal structure to the slow-moving parameter theta_slow.\n\nThe novelty of the paper seems to be in the application of the MAML framework to causal discovery which is interesting to me. I think a little theory about the sensitivity of the claim of ' theta slow changes relate to the causal structure ' is important. Even showing empirically which sort of graphs and functions become issues for the model would be useful.\n\nHere are my issues with the paper:\n - No error bars for cross-entropy are reported in the experiments.\n - The acyclic regularizer does not reject large length cycles than 3.\n - The ability to predict interventions seems to drop off sharply as the number of nodes increases. This suggests an inability to scale to more than 20 variables.\n - The experimental setup of uniformly sampling an intervening variable seems artificial to me.\n - MLP-specification of the SCM also seemed a bit artificial to me. \n\nOverall, the experiments look reasonable and the method itself seems interesting although further work is needed to show it is useful.\n\n(writing comments) The paper could use a more structured re-write. I had trouble tracking terms around the paper. For example, there seems to be a difference between P_i and P because the former uses theta_i and the latter only uses theta_slow only.\n\n---------------------------------\n\nUpdated score to 6 after rebuttal.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper1986/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1986/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["rosemary.nan.ke@gmail.com", "obilaniu@gmail.com", "anirudhgoyal9119@gmail.com", "stefan.a.bauer@gmail.com", "hugolarochelle@google.com", "chris.j.pal@gmail.com", "yoshua.bengio@mila.quebec"], "title": "Learning Neural Causal Models from Unknown Interventions", "authors": ["Nan Rosemary Ke", "Olexa Bilaniuk", "Anirudh Goyal", "Stephan Bauer", "Hugol Larochelle", "Chris Pal", "Yoshua Bengio"], "pdf": "/pdf/909851dd475666c92f50fb6a01f5c71b8b6432f3.pdf", "TL;DR": "Using end-to-end deep learning to discover the structure of a graphical model which is robust to interventions and trained without knowing what the interventions are", "abstract": "Meta-learning over a set of distributions can be interpreted as learning different types of parameters corresponding to short-term vs long-term aspects of the mechanisms underlying the generation of data. These are respectively captured by quickly-changing \\textit{parameters} and slowly-changing \\textit{meta-parameters}. We present a new framework for meta-learning causal models where the relationship between each variable and its parents is modeled by a neural network, modulated by structural meta-parameters which capture the overall topology of a directed graphical model. Our approach avoids a discrete search over models in favour of a continuous optimization procedure. We study a setting where interventional distributions are induced as a result of a random intervention on a single unknown variable of an unknown ground truth causal model, and the observations arising after such an intervention constitute one meta-example. To disentangle the slow-changing aspects of each conditional from the fast-changing adaptations to each intervention, we parametrize the neural network into fast parameters and slow meta-parameters. We introduce a meta-learning objective that favours solutions \\textit{robust} to frequent but sparse interventional distribution change, and which generalize well to previously unseen interventions. Optimizing this objective is shown experimentally to recover the structure of the causal graph. Finally, we find that when the learner is unaware of the intervention variable, it is able to infer that information, improving results further and focusing the parameter and meta-parameter updates where needed.", "keywords": ["deep learning", "graphical models", "meta learning"], "paperhash": "ke|learning_neural_causal_models_from_unknown_interventions", "original_pdf": "/attachment/46cdf6676b73a75437c5ddaca23f9b373e44a459.pdf", "_bibtex": "@misc{\nke2020learning,\ntitle={Learning Neural Causal Models from Unknown Interventions},\nauthor={Nan Rosemary Ke and Olexa Bilaniuk and Anirudh Goyal and Stephan Bauer and Hugol Larochelle and Chris Pal and Yoshua Bengio},\nyear={2020},\nurl={https://openreview.net/forum?id=H1gN6kSFwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "H1gN6kSFwS", "replyto": "H1gN6kSFwS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1986/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1986/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575859551948, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1986/Reviewers"], "noninvitees": [], "tcdate": 1570237729418, "tmdate": 1575859551961, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1986/-/Official_Review"}}}, {"id": "BJlYsorcsS", "original": null, "number": 10, "cdate": 1573702560536, "ddate": null, "tcdate": 1573702560536, "tmdate": 1573702560536, "tddate": null, "forum": "H1gN6kSFwS", "replyto": "ByxAt1yFsH", "invitation": "ICLR.cc/2020/Conference/Paper1986/-/Official_Comment", "content": {"title": "Response to \"Opinion changed a little\"", "comment": "We thank the reviewer for the very prompt response and we thank the reviewer for noting our contributions. We have begun updating our paper\u2019s existing figures with error bars and are running the reviewer\u2019s suggested additional experiments.\n\nQ. \u201cError bars: Thank you for this. I am interested in seeing how well MAML does.\u201d\n\nA. We uploaded a new revision of the paper and updated Figure 5 Left in the paper to reflect the errors bars. We have conducted experiments for all 3-variable graphs with PRNG seeds 1 to 5. The graphs for remaining experiments will be updated in due course, but limited compute resources may delay them beyond the rebuttals deadline.\n\nQ. \u201dHowever, the idea of masking was used in https://arxiv.org/pdf/1803.04929.pdf which your method also depends on. I believe that this same technique gives you both the ability to model the causal structure and avoids exponential search, correct? Could you also clarify why the comparison against this work was not done? Unless I am missing something, while Kalainathan et al. learn from observational data, the method can be run on your setup. And they do not suffer from the exponential time-complexity.\u201d\n\nA. We appreciate the reviewer for pointing us to this work. It is true that this technique would also have the ability to model the causal structure and avoids the exponential search.\n\nThe main reason we did not compare to Kalainathan et al. was because their technique only handles continuous data and our method tackles discrete data. We settled on the discrete case because we needed datasets that are large and allow for interventions. We are aware of no large, multi-variable datasets for continuous variables, and an effort [1] to create such a dataset is only in its infancy: It supports only two variables (cause and effect pairs) and its authors themselves have made an urgent public call for far more validation pairs. By contrast, the Bayesian Networks Repository has a variety of discrete, multi-variable networks publicly available for benchmarking.\n\nAn additional complication is Kalainathan et al.\u2019s use of a GAN framework, which is not trivial to extend to the discrete case. The authors themselves admit as much in the conclusion of their paper: \u201cAn on-going extension regards the case of categorical and mixed variables, taking inspiration from discrete GANs (Hjelm et al., 2017).\u201d As of today, no such extension has been published.\n\nWe compared to 3 other methods. In particular, we compared to ICP, which is one of the state-of-the-art methods for causal induction from interventional data, as noted in [2]. In general, we examined an array of causal learning methods where an open-source implementation is available. However, many of these methods can only handle continuous data (not discrete data) e.g. LinGAM, while many others do not handle interventions.\n\n[1] J. M. Mooij, J. Peters, D. Janzing, J. Zscheischler, B. Schoelkopf: \"Distinguishing cause from effect using observational data: methods and benchmarks\", Journal of Machine Learning Research 17(32):1-102, 2016\n[2]. Versteg, Boosting Local Causal Discovery in High-Dimensional Expression Data https://arxiv.org/pdf/1910.02505v2.pdf\n\nQ. \u201cUniformly sampling of intervention:\nI think it is much more interesting to restrict the set of nodes you perform interventions on. This is sort of like a held-out intervention evaluation of your method. I think I was unclear about my issue here. It was not the uniformity, rather it was that the interventions were being done on all nodes.\u201d\n\nA. We thank the reviewer for clarifying this point. Although many SCMs\u2019 true causal structures can be recovered with only a restricted set of interventions, in the general case one needs the ability to intervene on all variables, and this is why we allow the method to do so. That being said, the reviewer\u2019s proposed experiments would be a valuable addition to the paper, and although limited computing resources and a backlog of other suggested experiments might prevent us from inserting these before the close of the rebuttal period, we will dedicate a series of experiments to this topic.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1986/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1986/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["rosemary.nan.ke@gmail.com", "obilaniu@gmail.com", "anirudhgoyal9119@gmail.com", "stefan.a.bauer@gmail.com", "hugolarochelle@google.com", "chris.j.pal@gmail.com", "yoshua.bengio@mila.quebec"], "title": "Learning Neural Causal Models from Unknown Interventions", "authors": ["Nan Rosemary Ke", "Olexa Bilaniuk", "Anirudh Goyal", "Stephan Bauer", "Hugol Larochelle", "Chris Pal", "Yoshua Bengio"], "pdf": "/pdf/909851dd475666c92f50fb6a01f5c71b8b6432f3.pdf", "TL;DR": "Using end-to-end deep learning to discover the structure of a graphical model which is robust to interventions and trained without knowing what the interventions are", "abstract": "Meta-learning over a set of distributions can be interpreted as learning different types of parameters corresponding to short-term vs long-term aspects of the mechanisms underlying the generation of data. These are respectively captured by quickly-changing \\textit{parameters} and slowly-changing \\textit{meta-parameters}. We present a new framework for meta-learning causal models where the relationship between each variable and its parents is modeled by a neural network, modulated by structural meta-parameters which capture the overall topology of a directed graphical model. Our approach avoids a discrete search over models in favour of a continuous optimization procedure. We study a setting where interventional distributions are induced as a result of a random intervention on a single unknown variable of an unknown ground truth causal model, and the observations arising after such an intervention constitute one meta-example. To disentangle the slow-changing aspects of each conditional from the fast-changing adaptations to each intervention, we parametrize the neural network into fast parameters and slow meta-parameters. We introduce a meta-learning objective that favours solutions \\textit{robust} to frequent but sparse interventional distribution change, and which generalize well to previously unseen interventions. Optimizing this objective is shown experimentally to recover the structure of the causal graph. Finally, we find that when the learner is unaware of the intervention variable, it is able to infer that information, improving results further and focusing the parameter and meta-parameter updates where needed.", "keywords": ["deep learning", "graphical models", "meta learning"], "paperhash": "ke|learning_neural_causal_models_from_unknown_interventions", "original_pdf": "/attachment/46cdf6676b73a75437c5ddaca23f9b373e44a459.pdf", "_bibtex": "@misc{\nke2020learning,\ntitle={Learning Neural Causal Models from Unknown Interventions},\nauthor={Nan Rosemary Ke and Olexa Bilaniuk and Anirudh Goyal and Stephan Bauer and Hugol Larochelle and Chris Pal and Yoshua Bengio},\nyear={2020},\nurl={https://openreview.net/forum?id=H1gN6kSFwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1gN6kSFwS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1986/Authors", "ICLR.cc/2020/Conference/Paper1986/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1986/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1986/Reviewers", "ICLR.cc/2020/Conference/Paper1986/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1986/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1986/Authors|ICLR.cc/2020/Conference/Paper1986/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504147982, "tmdate": 1576860548484, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1986/Authors", "ICLR.cc/2020/Conference/Paper1986/Reviewers", "ICLR.cc/2020/Conference/Paper1986/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1986/-/Official_Comment"}}}, {"id": "ByxAt1yFsH", "original": null, "number": 8, "cdate": 1573609350439, "ddate": null, "tcdate": 1573609350439, "tmdate": 1573635241887, "tddate": null, "forum": "H1gN6kSFwS", "replyto": "r1ekeUYOjH", "invitation": "ICLR.cc/2020/Conference/Paper1986/-/Official_Comment", "content": {"title": "Opinion changed a little", "comment": "Regarding contributions:\nI agree that these contributions are noteworthy. \n\nHowever, the idea of masking was used in https://arxiv.org/pdf/1803.04929.pdf which your method also depends on. I believe that this same technique gives you both the ability to model the causal structure and avoids exponential search, correct?\n\nCould you also clarify why the comparison against this work was not done? Unless I am missing something, while Kalainathan et al. learn from observational data, the method can be run on your setup. And they do not suffer from the exponential time-complexity.\n\nError bars:\nThank you for this. I am interested in seeing how well MAML does.\n\nAbout cyclic regularizer:\nThank you for pointing this out.\n\nAbout predicting interventions:\nYou claim in the paper that \"We find that ignoring this issue considerably hurts or slows down meta-learning, suggesting that we should try to infer on which variable the intervention took place.\"\nSo this seems strongly coupled with the ability of MAML to recover causal structure. Then, am I correct in saying that MAML's quality of causal discovery is maintained even when the intervention prediction quality goes down?\n\nMaybe I'm missing something but if the intervention cannot be predicted, does the setup not boil down to estimating structure from observational data?\n\nUniform sampling of intervention:\nI think it is more interesting to restrict the set of nodes you perform interventions on. This is sort of like a held-out intervention evaluation of your method. I think I was unclear about my issue here. It was not the uniformity, rather it was that the interventions were being done on all nodes.\n\nOverall, I believe this method shows promise but needs a little more evaluation and understanding."}, "signatures": ["ICLR.cc/2020/Conference/Paper1986/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1986/AnonReviewer3", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["rosemary.nan.ke@gmail.com", "obilaniu@gmail.com", "anirudhgoyal9119@gmail.com", "stefan.a.bauer@gmail.com", "hugolarochelle@google.com", "chris.j.pal@gmail.com", "yoshua.bengio@mila.quebec"], "title": "Learning Neural Causal Models from Unknown Interventions", "authors": ["Nan Rosemary Ke", "Olexa Bilaniuk", "Anirudh Goyal", "Stephan Bauer", "Hugol Larochelle", "Chris Pal", "Yoshua Bengio"], "pdf": "/pdf/909851dd475666c92f50fb6a01f5c71b8b6432f3.pdf", "TL;DR": "Using end-to-end deep learning to discover the structure of a graphical model which is robust to interventions and trained without knowing what the interventions are", "abstract": "Meta-learning over a set of distributions can be interpreted as learning different types of parameters corresponding to short-term vs long-term aspects of the mechanisms underlying the generation of data. These are respectively captured by quickly-changing \\textit{parameters} and slowly-changing \\textit{meta-parameters}. We present a new framework for meta-learning causal models where the relationship between each variable and its parents is modeled by a neural network, modulated by structural meta-parameters which capture the overall topology of a directed graphical model. Our approach avoids a discrete search over models in favour of a continuous optimization procedure. We study a setting where interventional distributions are induced as a result of a random intervention on a single unknown variable of an unknown ground truth causal model, and the observations arising after such an intervention constitute one meta-example. To disentangle the slow-changing aspects of each conditional from the fast-changing adaptations to each intervention, we parametrize the neural network into fast parameters and slow meta-parameters. We introduce a meta-learning objective that favours solutions \\textit{robust} to frequent but sparse interventional distribution change, and which generalize well to previously unseen interventions. Optimizing this objective is shown experimentally to recover the structure of the causal graph. Finally, we find that when the learner is unaware of the intervention variable, it is able to infer that information, improving results further and focusing the parameter and meta-parameter updates where needed.", "keywords": ["deep learning", "graphical models", "meta learning"], "paperhash": "ke|learning_neural_causal_models_from_unknown_interventions", "original_pdf": "/attachment/46cdf6676b73a75437c5ddaca23f9b373e44a459.pdf", "_bibtex": "@misc{\nke2020learning,\ntitle={Learning Neural Causal Models from Unknown Interventions},\nauthor={Nan Rosemary Ke and Olexa Bilaniuk and Anirudh Goyal and Stephan Bauer and Hugol Larochelle and Chris Pal and Yoshua Bengio},\nyear={2020},\nurl={https://openreview.net/forum?id=H1gN6kSFwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1gN6kSFwS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1986/Authors", "ICLR.cc/2020/Conference/Paper1986/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1986/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1986/Reviewers", "ICLR.cc/2020/Conference/Paper1986/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1986/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1986/Authors|ICLR.cc/2020/Conference/Paper1986/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504147982, "tmdate": 1576860548484, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1986/Authors", "ICLR.cc/2020/Conference/Paper1986/Reviewers", "ICLR.cc/2020/Conference/Paper1986/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1986/-/Official_Comment"}}}, {"id": "Hyg7Tw9OsH", "original": null, "number": 6, "cdate": 1573590971268, "ddate": null, "tcdate": 1573590971268, "tmdate": 1573590971268, "tddate": null, "forum": "H1gN6kSFwS", "replyto": "Skx29AZdFH", "invitation": "ICLR.cc/2020/Conference/Paper1986/-/Official_Comment", "content": {"title": "Response to Official Blind Review #1 (Part 4) ", "comment": "Q. \u201cWhy do the authors report cross entropy loss in Table 1?\u201d \n\nWe appreciate the reviewer for pointing this out. We reported cross entropy because we learn the likelihood of edges between variables rather than iterating through all possible graphs (which is typically done e.g. in ICP). Hence we maintain a distribution over graphs and we need to score how good that distribution is. This loss thus gives a better indication of how our method learned and converged over time.  There is also a direct comparison to the ground-truth graph and the cross entropy should converge close to 0 if our model has learned the correct structure. On top of reporting cross entropy, we also evaluate our model on predicted likelihood for out-of-distribution generalization as shown in Table 3. \n \nQ. \u201cInstead of ICP (which is constrained to be linear which is unrealistically simple), why don't the authors compare against nonlinearICP (which is more flexible like their neural networks): \u201c\n \nWhile we agree that the non-linearity in the non-linear version of ICP adds flexibility, it likewise increases the difficulty since it is unclear which non-linear and non-parameteric conditional independence test to use in practice. The performance of nonlinear ICP critically depends on the conditional independence tests. That is one reason why in the non-linear ICP paper it is explicitly recommended for practical purposes to use non-linear ICP (over its linear version, see discussion p24-p25 in (1)) only if all linear models are rejected. However, not all linear models were rejected by ICP in our case. Moreover conditional independence testing was shown to be hard [1] which might be one reason why our method shows superior performance over the state-of-the-art method. \n\n[1]. Shah, Rajen D., and Jonas Peters. \"The hardness of conditional independence testing and the generalised covariance measure.\" arXiv preprint arXiv:1804.07203 (2018).\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1986/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1986/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["rosemary.nan.ke@gmail.com", "obilaniu@gmail.com", "anirudhgoyal9119@gmail.com", "stefan.a.bauer@gmail.com", "hugolarochelle@google.com", "chris.j.pal@gmail.com", "yoshua.bengio@mila.quebec"], "title": "Learning Neural Causal Models from Unknown Interventions", "authors": ["Nan Rosemary Ke", "Olexa Bilaniuk", "Anirudh Goyal", "Stephan Bauer", "Hugol Larochelle", "Chris Pal", "Yoshua Bengio"], "pdf": "/pdf/909851dd475666c92f50fb6a01f5c71b8b6432f3.pdf", "TL;DR": "Using end-to-end deep learning to discover the structure of a graphical model which is robust to interventions and trained without knowing what the interventions are", "abstract": "Meta-learning over a set of distributions can be interpreted as learning different types of parameters corresponding to short-term vs long-term aspects of the mechanisms underlying the generation of data. These are respectively captured by quickly-changing \\textit{parameters} and slowly-changing \\textit{meta-parameters}. We present a new framework for meta-learning causal models where the relationship between each variable and its parents is modeled by a neural network, modulated by structural meta-parameters which capture the overall topology of a directed graphical model. Our approach avoids a discrete search over models in favour of a continuous optimization procedure. We study a setting where interventional distributions are induced as a result of a random intervention on a single unknown variable of an unknown ground truth causal model, and the observations arising after such an intervention constitute one meta-example. To disentangle the slow-changing aspects of each conditional from the fast-changing adaptations to each intervention, we parametrize the neural network into fast parameters and slow meta-parameters. We introduce a meta-learning objective that favours solutions \\textit{robust} to frequent but sparse interventional distribution change, and which generalize well to previously unseen interventions. Optimizing this objective is shown experimentally to recover the structure of the causal graph. Finally, we find that when the learner is unaware of the intervention variable, it is able to infer that information, improving results further and focusing the parameter and meta-parameter updates where needed.", "keywords": ["deep learning", "graphical models", "meta learning"], "paperhash": "ke|learning_neural_causal_models_from_unknown_interventions", "original_pdf": "/attachment/46cdf6676b73a75437c5ddaca23f9b373e44a459.pdf", "_bibtex": "@misc{\nke2020learning,\ntitle={Learning Neural Causal Models from Unknown Interventions},\nauthor={Nan Rosemary Ke and Olexa Bilaniuk and Anirudh Goyal and Stephan Bauer and Hugol Larochelle and Chris Pal and Yoshua Bengio},\nyear={2020},\nurl={https://openreview.net/forum?id=H1gN6kSFwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1gN6kSFwS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1986/Authors", "ICLR.cc/2020/Conference/Paper1986/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1986/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1986/Reviewers", "ICLR.cc/2020/Conference/Paper1986/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1986/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1986/Authors|ICLR.cc/2020/Conference/Paper1986/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504147982, "tmdate": 1576860548484, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1986/Authors", "ICLR.cc/2020/Conference/Paper1986/Reviewers", "ICLR.cc/2020/Conference/Paper1986/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1986/-/Official_Comment"}}}, {"id": "BJgyDD5dsS", "original": null, "number": 5, "cdate": 1573590870521, "ddate": null, "tcdate": 1573590870521, "tmdate": 1573590870521, "tddate": null, "forum": "H1gN6kSFwS", "replyto": "Skx29AZdFH", "invitation": "ICLR.cc/2020/Conference/Paper1986/-/Official_Comment", "content": {"title": "Response to Official Blind Review #1 (Part 3)", "comment": "Q. \u201cWhy does one even care about the graph being acyclic in this setting?\u201d\n\nWe defined our groundtruth SCM to be acyclic for the simplicity of sampling, otherwise we could not perform ancestral sampling. That being the case, an acyclic regularizer restricts the set of solutions, encouraging faster convergence of the model from a statistical point of view. Adding the regularizer speeds up convergence, but asymptotically both models with and without regularization converge towards the same point.\n\nQ. \u201cOne main reason for SCM modeling in science and policy-making is for analysts to better understand the data generating phenomena.  However your use of neural networks here seems to hamper interpretability, so how do you reconcile this issue?\u201d\n\nIn the foreword, we had lightly touched on the general concerns raised about neural networks and their interpretability. We will dive into greater detail here.\n\nThe (learned) structural parameters, which define the causal structure of the solution, are directly interpretable as an adjacency matrix. Examples of the learned adjacency matrix extracted from our model can be found in Figure 3 and 4 in the paper.\n\nAs regards the MLP-parametrized conditionals, they are as interpretable as conditional probability tables. This is because the MLP\u2019s learned functional parameters can always be reduced to such a table by querying the MLP for all possible discrete values of all possible ancestors.\n\nThere is a vast literature on interpretability of deep learning, of course, but we must admit that our main goal is to design better learning algorithms for autonomous intelligent systems (like robots) where the ability of those systems to understand the world is the primary goal (as opposed to extracting that knowledge for human consumption). Our neural networks\u2019 interpretability by analysts was therefore only a secondary objective, although in the end the model remains quite interpretable.\n\nQ. \u201cRelated papers that utilize the same idea of predicting a variable conditioned on subset of other variables via neural network + masking strategy:\u201d\n\nWe thank the reviewers for pointing such a complete list of papers, they are indeed relevant and we will update our relevant work section with the list of papers.\n\nQ \u201cour faith in the proposed methodology rests entirely on the empirical experiments.  However, I find these a bit too basic to be very convincing, and would at least like to see more methods being compared (in particular for the simulated graphs as well).\u201d\n\nWe thank the reviewers for pointing this out. There are several aspects that are relevant as listed below.  \n\n a. As mentioned in other recent works e.g. [1], ICP is one of the state-of-the-art algorithms. We would refer to the extensive experiments in their paper for additional baselines comparisons and while we can likewise add more baselines, we do not expect any changes in results. Please likewise see our answer for the comparison against non-linear ICP below. \n\n [1]. Versteg, Boosting Local Causal Discovery in High-Dimensional Expression Data https://arxiv.org/pdf/1910.02505v2.pdf\n\n  b. We have examined an array of causal learning methods where an open-source implementation is available. However, few of these are applicable to discrete data from interventions. Many of these methods can only handle continuous data (not discrete data) e.g. LinGAM and many others do not handle interventions. Hence we only compared to the ones that we had in the paper. If the reviewer is aware of an implementation of an algorithm applicable to our setup (leaving aside non-linear ICP, which we discuss in the answer below), we would be more than happy to run it.\n\n   c. We also present experiments aimed at measuring generalization (in terms of predictive power and likelihood) using the learned causal structure. Ideally, If the model has learned the right structure, it should generalize better. This is shown in Table 3 of our paper.\n\n\nQ. \u201cThe authors should describe what are the underlying interventions in each dataset a bit more.\u201d\n\nWe thank the reviewer for pointing this out, this is a good point and we will include this in the next revision.\n\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1986/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1986/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["rosemary.nan.ke@gmail.com", "obilaniu@gmail.com", "anirudhgoyal9119@gmail.com", "stefan.a.bauer@gmail.com", "hugolarochelle@google.com", "chris.j.pal@gmail.com", "yoshua.bengio@mila.quebec"], "title": "Learning Neural Causal Models from Unknown Interventions", "authors": ["Nan Rosemary Ke", "Olexa Bilaniuk", "Anirudh Goyal", "Stephan Bauer", "Hugol Larochelle", "Chris Pal", "Yoshua Bengio"], "pdf": "/pdf/909851dd475666c92f50fb6a01f5c71b8b6432f3.pdf", "TL;DR": "Using end-to-end deep learning to discover the structure of a graphical model which is robust to interventions and trained without knowing what the interventions are", "abstract": "Meta-learning over a set of distributions can be interpreted as learning different types of parameters corresponding to short-term vs long-term aspects of the mechanisms underlying the generation of data. These are respectively captured by quickly-changing \\textit{parameters} and slowly-changing \\textit{meta-parameters}. We present a new framework for meta-learning causal models where the relationship between each variable and its parents is modeled by a neural network, modulated by structural meta-parameters which capture the overall topology of a directed graphical model. Our approach avoids a discrete search over models in favour of a continuous optimization procedure. We study a setting where interventional distributions are induced as a result of a random intervention on a single unknown variable of an unknown ground truth causal model, and the observations arising after such an intervention constitute one meta-example. To disentangle the slow-changing aspects of each conditional from the fast-changing adaptations to each intervention, we parametrize the neural network into fast parameters and slow meta-parameters. We introduce a meta-learning objective that favours solutions \\textit{robust} to frequent but sparse interventional distribution change, and which generalize well to previously unseen interventions. Optimizing this objective is shown experimentally to recover the structure of the causal graph. Finally, we find that when the learner is unaware of the intervention variable, it is able to infer that information, improving results further and focusing the parameter and meta-parameter updates where needed.", "keywords": ["deep learning", "graphical models", "meta learning"], "paperhash": "ke|learning_neural_causal_models_from_unknown_interventions", "original_pdf": "/attachment/46cdf6676b73a75437c5ddaca23f9b373e44a459.pdf", "_bibtex": "@misc{\nke2020learning,\ntitle={Learning Neural Causal Models from Unknown Interventions},\nauthor={Nan Rosemary Ke and Olexa Bilaniuk and Anirudh Goyal and Stephan Bauer and Hugol Larochelle and Chris Pal and Yoshua Bengio},\nyear={2020},\nurl={https://openreview.net/forum?id=H1gN6kSFwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1gN6kSFwS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1986/Authors", "ICLR.cc/2020/Conference/Paper1986/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1986/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1986/Reviewers", "ICLR.cc/2020/Conference/Paper1986/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1986/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1986/Authors|ICLR.cc/2020/Conference/Paper1986/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504147982, "tmdate": 1576860548484, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1986/Authors", "ICLR.cc/2020/Conference/Paper1986/Reviewers", "ICLR.cc/2020/Conference/Paper1986/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1986/-/Official_Comment"}}}, {"id": "H1lA_85_oH", "original": null, "number": 4, "cdate": 1573590646246, "ddate": null, "tcdate": 1573590646246, "tmdate": 1573590646246, "tddate": null, "forum": "H1gN6kSFwS", "replyto": "Skx29AZdFH", "invitation": "ICLR.cc/2020/Conference/Paper1986/-/Official_Comment", "content": {"title": "Response to Official Blind Review #1 (Part 2) ", "comment": "Q. \u201dIn this setting, how do the authors propose selecting hyperparameter values? How does the reader know the authors did not simply tune their hyperparameters to best match the underlying ground truth (I assume the proposed methodology has many more hyperparameters and thus more degrees of freedom here).\u201d\n\nVery little effort was required for tuning the neural network hyperparameters:\n\n1. All of our experiments (synthetic and real data) use the same hyperparameters unless otherwise specified.\n2. We used common strategies for training a neural network and this does usually include several hyperparameters. Among others, there are the specific architecture, activation, number of hidden layers, size of hidden layers, learning rate and optimizer.\n     a.  The choice of the architecture was the simplest feedforward neural network, an MLP, with\n     b.  The smallest possible number of hidden layers which is 1\n     c.  The number of hidden neurons was designed only to be greater than the number of input or output neurons.\n     d. Given that ReLUs are standard in the literature, we selected a simple, well-known variant called LeakyReLU that avoids a common problem called the dying neuron problem [1].\n          i. The alpha parameter was arbitrarily set to 0.1 and never tuned.\n          ii. Since we are training (a set of) MLPs, we adapted some of the commonly used strategies for training MLPs. We used the Adam optimizer, one of the most successful ones in the literature [2] and selected the best learning rate from [0.01, 0.05, 0.001, 0.005].\n  3.  We are running additional experiments with various size of hidden units. We will update our paper with the new results once these experiments are completed.\n  4. For reproducibility, future benchmarking and baseline comparisons, all code will be released. \n       \n   [1]. Lu, Lu, Yeonjong Shin, Yanhui Su, and George Em Karniadakis. \"Dying ReLU and Initialization: Theory and Numerical Examples.\" arXiv preprint arXiv:1903.06733 (2019).\n\n   [2]. Kingma, Diederik P., and Jimmy Ba. \"Adam: A method for stochastic optimization.\" arXiv preprint arXiv:1412.6980 (2014).\n\nQ. \u201cWhy does one even care about the graph being acyclic in this setting?\u201d\n\nWe defined our groundtruth SCM to be acyclic for the simplicity of sampling, otherwise we could not perform ancestral sampling. That being the case, an acyclic regularizer restricts the set of solutions, encouraging faster convergence of the model from a statistical point of view. Adding the regularizer speeds up convergence, but asymptotically both models with and without regularization converge towards the same point.\n\nQ. \u201cOne main reason for SCM modeling in science and policy-making is for analysts to better understand the data generating phenomena.  However your use of neural networks here seems to hamper interpretability, so how do you reconcile this issue?\u201d\n\nIn the foreword, we had lightly touched on the general concerns raised about neural networks and their interpretability. We will dive into greater detail here.\n\nThe (learned) structural parameters, which define the causal structure of the solution, are directly interpretable as an adjacency matrix. Examples of the learned adjacency matrix extracted from our model can be found in Figure 3 and 4 in the paper.\n\nAs regards the MLP-parametrized conditionals, they are as interpretable as conditional probability tables. This is because the MLP\u2019s learned functional parameters can always be reduced to such a table by querying the MLP for all possible discrete values of all possible ancestors.\n\nThere is a vast literature on interpretability of deep learning, of course, but we must admit that our main goal is to design better learning algorithms for autonomous intelligent systems (like robots) where the ability of those systems to understand the world is the primary goal (as opposed to extracting that knowledge for human consumption). Our neural networks\u2019 interpretability by analysts was therefore only a secondary objective, although in the end the model remains quite interpretable.\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1986/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1986/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["rosemary.nan.ke@gmail.com", "obilaniu@gmail.com", "anirudhgoyal9119@gmail.com", "stefan.a.bauer@gmail.com", "hugolarochelle@google.com", "chris.j.pal@gmail.com", "yoshua.bengio@mila.quebec"], "title": "Learning Neural Causal Models from Unknown Interventions", "authors": ["Nan Rosemary Ke", "Olexa Bilaniuk", "Anirudh Goyal", "Stephan Bauer", "Hugol Larochelle", "Chris Pal", "Yoshua Bengio"], "pdf": "/pdf/909851dd475666c92f50fb6a01f5c71b8b6432f3.pdf", "TL;DR": "Using end-to-end deep learning to discover the structure of a graphical model which is robust to interventions and trained without knowing what the interventions are", "abstract": "Meta-learning over a set of distributions can be interpreted as learning different types of parameters corresponding to short-term vs long-term aspects of the mechanisms underlying the generation of data. These are respectively captured by quickly-changing \\textit{parameters} and slowly-changing \\textit{meta-parameters}. We present a new framework for meta-learning causal models where the relationship between each variable and its parents is modeled by a neural network, modulated by structural meta-parameters which capture the overall topology of a directed graphical model. Our approach avoids a discrete search over models in favour of a continuous optimization procedure. We study a setting where interventional distributions are induced as a result of a random intervention on a single unknown variable of an unknown ground truth causal model, and the observations arising after such an intervention constitute one meta-example. To disentangle the slow-changing aspects of each conditional from the fast-changing adaptations to each intervention, we parametrize the neural network into fast parameters and slow meta-parameters. We introduce a meta-learning objective that favours solutions \\textit{robust} to frequent but sparse interventional distribution change, and which generalize well to previously unseen interventions. Optimizing this objective is shown experimentally to recover the structure of the causal graph. Finally, we find that when the learner is unaware of the intervention variable, it is able to infer that information, improving results further and focusing the parameter and meta-parameter updates where needed.", "keywords": ["deep learning", "graphical models", "meta learning"], "paperhash": "ke|learning_neural_causal_models_from_unknown_interventions", "original_pdf": "/attachment/46cdf6676b73a75437c5ddaca23f9b373e44a459.pdf", "_bibtex": "@misc{\nke2020learning,\ntitle={Learning Neural Causal Models from Unknown Interventions},\nauthor={Nan Rosemary Ke and Olexa Bilaniuk and Anirudh Goyal and Stephan Bauer and Hugol Larochelle and Chris Pal and Yoshua Bengio},\nyear={2020},\nurl={https://openreview.net/forum?id=H1gN6kSFwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1gN6kSFwS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1986/Authors", "ICLR.cc/2020/Conference/Paper1986/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1986/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1986/Reviewers", "ICLR.cc/2020/Conference/Paper1986/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1986/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1986/Authors|ICLR.cc/2020/Conference/Paper1986/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504147982, "tmdate": 1576860548484, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1986/Authors", "ICLR.cc/2020/Conference/Paper1986/Reviewers", "ICLR.cc/2020/Conference/Paper1986/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1986/-/Official_Comment"}}}, {"id": "r1lSLH9diH", "original": null, "number": 3, "cdate": 1573590349431, "ddate": null, "tcdate": 1573590349431, "tmdate": 1573590349431, "tddate": null, "forum": "H1gN6kSFwS", "replyto": "Skx29AZdFH", "invitation": "ICLR.cc/2020/Conference/Paper1986/-/Official_Comment", "content": {"title": "Response to Official Blind Review #1 (Part 1) ", "comment": "We thank the reviewer for such detailed feedback. We are conducting additional experiments based on the feedback and will update the paper and rebuttal once the experiments are completed. \n\nThe reviewer expresses several general concerns about the use of neural networks for causal inference, focusing on attributes such as their large design space and their interpretability. We would like to underscore that this paper is intended as a step from today\u2019s completely non-causal neural networks towards incorporating more of the abilities required for handling causality. As such, our proposed method will indeed retain most of the benefits and limitations of neural networks, but improve on them by identifying causal structures.\n\nQ. \u201dHow come there is hardly any discussion of the identifiability issue beyond the few sentences in A.3. This is one of the key issues in learning SCMs and it is strange that the concept of \"faithfulness\" is not even mentioned in the paper. \u2026.In general, there is hardly any discussion of what conditions are required for the proposed estimates to even be valid. The authors seem to be optimistically assuming that their neural network + metalearning model will somehow pick up on the correct structure, without any actual conceptual investigation of this issue.\u201d\n\nBecause our task setup allows a random intervention over any variable, per (Eberhardt et al., 2012) it is at least in theory possible to identify the correct graph. The rest of the paper was mostly directed at showing that this is not only possible in theory but in practice as well.\n\nWe thank the reviewer for mentioning that some discussion of faithfulness would enhance the paper. There are several aspects that are relevant as listed below.  \nOur model does indeed assume faithfulness, however, this is not a limitation in practice. Because of the continuous evolution of the functional parameters for the conditional distributions MLP, we believe that occurrences of unfaithful populations will be extremely short-lived and exceedingly rare to begin with. Lastly, because our procedure invokes an outer-loop optimization procedure, gradient estimate errors induced by unfaithfulness can be compensated.\nThe faithfulness assumption (Pearl 2009, Peters et al. 2017) implies that any d-separation in the graph corresponds to a conditional independence in the data generating random variables. Under the assumption of faithfulness and a sufficiently large sample size, the Markov blanket can consistently be recovered given the availability of an efficient feature selection algorithm [1]. Neural Networks have been shown to be able to learn good features [2, 3] and require large datasets for training, which we assumed to be given here. We agree with the reviewer that similarly to the already mentioned assumptions of Markov equivalence and causal sufficiency (see A.3 PRELIMINARIES) we will add a discussion on faithfulness and the assumptions of the availability of large datasets to the manuscript. \n[1] J.-P. Pellet and A. Elisseeff. Using markov blankets for causal structure learning. Journal of Machine Learning Research, 2008\n[2] Bengio, Yoshua. Learning deep architectures for AI. Foundations and trends in Machine Learning, 2009\n[3] Bengio, Yoshua et. al, Representation learning: A review and new perspectives, arxiv 1206.5538, 2012\n\n\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1986/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1986/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["rosemary.nan.ke@gmail.com", "obilaniu@gmail.com", "anirudhgoyal9119@gmail.com", "stefan.a.bauer@gmail.com", "hugolarochelle@google.com", "chris.j.pal@gmail.com", "yoshua.bengio@mila.quebec"], "title": "Learning Neural Causal Models from Unknown Interventions", "authors": ["Nan Rosemary Ke", "Olexa Bilaniuk", "Anirudh Goyal", "Stephan Bauer", "Hugol Larochelle", "Chris Pal", "Yoshua Bengio"], "pdf": "/pdf/909851dd475666c92f50fb6a01f5c71b8b6432f3.pdf", "TL;DR": "Using end-to-end deep learning to discover the structure of a graphical model which is robust to interventions and trained without knowing what the interventions are", "abstract": "Meta-learning over a set of distributions can be interpreted as learning different types of parameters corresponding to short-term vs long-term aspects of the mechanisms underlying the generation of data. These are respectively captured by quickly-changing \\textit{parameters} and slowly-changing \\textit{meta-parameters}. We present a new framework for meta-learning causal models where the relationship between each variable and its parents is modeled by a neural network, modulated by structural meta-parameters which capture the overall topology of a directed graphical model. Our approach avoids a discrete search over models in favour of a continuous optimization procedure. We study a setting where interventional distributions are induced as a result of a random intervention on a single unknown variable of an unknown ground truth causal model, and the observations arising after such an intervention constitute one meta-example. To disentangle the slow-changing aspects of each conditional from the fast-changing adaptations to each intervention, we parametrize the neural network into fast parameters and slow meta-parameters. We introduce a meta-learning objective that favours solutions \\textit{robust} to frequent but sparse interventional distribution change, and which generalize well to previously unseen interventions. Optimizing this objective is shown experimentally to recover the structure of the causal graph. Finally, we find that when the learner is unaware of the intervention variable, it is able to infer that information, improving results further and focusing the parameter and meta-parameter updates where needed.", "keywords": ["deep learning", "graphical models", "meta learning"], "paperhash": "ke|learning_neural_causal_models_from_unknown_interventions", "original_pdf": "/attachment/46cdf6676b73a75437c5ddaca23f9b373e44a459.pdf", "_bibtex": "@misc{\nke2020learning,\ntitle={Learning Neural Causal Models from Unknown Interventions},\nauthor={Nan Rosemary Ke and Olexa Bilaniuk and Anirudh Goyal and Stephan Bauer and Hugol Larochelle and Chris Pal and Yoshua Bengio},\nyear={2020},\nurl={https://openreview.net/forum?id=H1gN6kSFwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1gN6kSFwS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1986/Authors", "ICLR.cc/2020/Conference/Paper1986/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1986/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1986/Reviewers", "ICLR.cc/2020/Conference/Paper1986/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1986/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1986/Authors|ICLR.cc/2020/Conference/Paper1986/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504147982, "tmdate": 1576860548484, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1986/Authors", "ICLR.cc/2020/Conference/Paper1986/Reviewers", "ICLR.cc/2020/Conference/Paper1986/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1986/-/Official_Comment"}}}, {"id": "SJlarIt_sS", "original": null, "number": 2, "cdate": 1573586500611, "ddate": null, "tcdate": 1573586500611, "tmdate": 1573586500611, "tddate": null, "forum": "H1gN6kSFwS", "replyto": "rylbZRDpYS", "invitation": "ICLR.cc/2020/Conference/Paper1986/-/Official_Comment", "content": {"title": "Response to Official Blind Review #3 (Part 2) ", "comment": "Q. \u201cMLP-specification of the SCM also seemed a bit artificial to me\u201d. \n\nBoth the groundtruth SCM and our model are parameterized by MLPs. \nCould we ask the reviewer to clarify if there is a specific aspect of this setup using an MLP the reviewer finds artificial here?  MLPs are used successfully in a large number of state-of-the-art solutions to many real-world ML problems. We think of our work as a step to bring causality to deep learning, which as Pearl would say, would be helpful to further climb the ladder of intelligence.\n\nWe chose to parameterize the ground truth SCM by MLPs for the ease of defining the conditional probability table (CPT), such that we do not have to exhaustively define the CPT for different variables and graphs, something very convenient  as the number of variables increases (and the size of a full CPT would grow exponentially). \nAs for our model, one of the key contributions is to parametrize a learned SCM using a neural network. It is suggested by the recent review paper on causal structural learning. The paper [1] concluded that \"more efficient algorithms are needed\". One possibility of a more efficient algorithm is one that avoids an explicit exponential search over all possible DAGs and our framework of learning a SCM parameterized by a neural network using a meta-learning approach is a step towards this goal. \n\nAnother contribution is that our framework/ method of MLP specification of the SCM generalizes well to the challenge of out-of-distribution interventions.\n\n[1]. Heinze-Deml, Christina, Marloes H. Maathuis, and Nicolai Meinshausen. \"Causal structure learning.\" Annual Review of Statistics and Its Application 5 (2018): 371-391.\n\nQ. \u201c had trouble tracking terms around the paper.\u201d\n\nWe thank the reviewer for pointing this out. We have double checked our use of terms and updated the paper to improve clarity in this regard.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1986/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1986/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["rosemary.nan.ke@gmail.com", "obilaniu@gmail.com", "anirudhgoyal9119@gmail.com", "stefan.a.bauer@gmail.com", "hugolarochelle@google.com", "chris.j.pal@gmail.com", "yoshua.bengio@mila.quebec"], "title": "Learning Neural Causal Models from Unknown Interventions", "authors": ["Nan Rosemary Ke", "Olexa Bilaniuk", "Anirudh Goyal", "Stephan Bauer", "Hugol Larochelle", "Chris Pal", "Yoshua Bengio"], "pdf": "/pdf/909851dd475666c92f50fb6a01f5c71b8b6432f3.pdf", "TL;DR": "Using end-to-end deep learning to discover the structure of a graphical model which is robust to interventions and trained without knowing what the interventions are", "abstract": "Meta-learning over a set of distributions can be interpreted as learning different types of parameters corresponding to short-term vs long-term aspects of the mechanisms underlying the generation of data. These are respectively captured by quickly-changing \\textit{parameters} and slowly-changing \\textit{meta-parameters}. We present a new framework for meta-learning causal models where the relationship between each variable and its parents is modeled by a neural network, modulated by structural meta-parameters which capture the overall topology of a directed graphical model. Our approach avoids a discrete search over models in favour of a continuous optimization procedure. We study a setting where interventional distributions are induced as a result of a random intervention on a single unknown variable of an unknown ground truth causal model, and the observations arising after such an intervention constitute one meta-example. To disentangle the slow-changing aspects of each conditional from the fast-changing adaptations to each intervention, we parametrize the neural network into fast parameters and slow meta-parameters. We introduce a meta-learning objective that favours solutions \\textit{robust} to frequent but sparse interventional distribution change, and which generalize well to previously unseen interventions. Optimizing this objective is shown experimentally to recover the structure of the causal graph. Finally, we find that when the learner is unaware of the intervention variable, it is able to infer that information, improving results further and focusing the parameter and meta-parameter updates where needed.", "keywords": ["deep learning", "graphical models", "meta learning"], "paperhash": "ke|learning_neural_causal_models_from_unknown_interventions", "original_pdf": "/attachment/46cdf6676b73a75437c5ddaca23f9b373e44a459.pdf", "_bibtex": "@misc{\nke2020learning,\ntitle={Learning Neural Causal Models from Unknown Interventions},\nauthor={Nan Rosemary Ke and Olexa Bilaniuk and Anirudh Goyal and Stephan Bauer and Hugol Larochelle and Chris Pal and Yoshua Bengio},\nyear={2020},\nurl={https://openreview.net/forum?id=H1gN6kSFwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1gN6kSFwS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1986/Authors", "ICLR.cc/2020/Conference/Paper1986/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1986/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1986/Reviewers", "ICLR.cc/2020/Conference/Paper1986/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1986/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1986/Authors|ICLR.cc/2020/Conference/Paper1986/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504147982, "tmdate": 1576860548484, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1986/Authors", "ICLR.cc/2020/Conference/Paper1986/Reviewers", "ICLR.cc/2020/Conference/Paper1986/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1986/-/Official_Comment"}}}, {"id": "r1ekeUYOjH", "original": null, "number": 1, "cdate": 1573586406577, "ddate": null, "tcdate": 1573586406577, "tmdate": 1573586406577, "tddate": null, "forum": "H1gN6kSFwS", "replyto": "rylbZRDpYS", "invitation": "ICLR.cc/2020/Conference/Paper1986/-/Official_Comment", "content": {"title": "Response to Official Blind Review #3 (Part 1)", "comment": "We thank the reviewer for the feedback. We have conducted additional experiments based on the feedback and will update our paper once the experiments are completed.\n\nOur paper is related to MAML like procedures for meta-learning, but goes beyond the usual setting, making a significant contribution through developing more sophisticated algorithms that enable causal structure learning.\nThe difficulties those changes addressed are intrinsic to causal structure learning, especially in the challenging unknown-intervention scenario that we have set ourselves. The challenges we solve are 1) how to handle unknown interventions, 2) how to avoid an exponential search over all possible DAGs, 3) how to model the effect of the intervention, and finally 4) how to model the underlying causal structure.\n\nQ. \u201cNo error bars for cross-entropy are reported in the experiments.\u201d\nWe thank the reviewer for pointing this out. We have conducted additional experiments and will update our paper once the experiments have been completed.\n\nQ. \u201cThe acyclic regularizer does not reject large length cycles than 3.\u2018 \nWe appreciate the reviewer\u2019s concern. The regularizer can be extended to length-n cycles, however, this becomes more computationally demanding as n increases. However, we found that a smaller n does not affect our model empirically.  As shown in Figure 2, 3 and 4, our model did not learn cycles of any length greater than 2. In fact, we have found that even completely removing this regularizer does not hurt the asymptotic performance of our model. The regularizer helps the model to converge faster, however, the model still converges reasonably fast without the regularizer, as shown in Figure 6 Right.\n\nQ. \u201cThe ability to predict interventions seems to drop off sharply as the number of nodes increases.\u201d\nWe are aware of this limitation. It makes sense that  guessing which node has been intervened becomes harder as the number of nodes increases and we find that empirically, without surprise. We agree  that it is a challenge to scale to larger graphs (namely graphs with more than 20 variables), however even for the sizes of graphs we consider our paper finds greatly improved solutions and this is already a significant advance over past work.  One extension we hope will help to overcome this difficulty would be to perform a soft prediction of the interventional nodes, instead of the hard decision that we have now. We also would like to highlight that the intervention prediction performs significantly better than random at all times.\n\nOne note on the recent papers on this topic: although ICP and non-linear ICP consider a larger number of covariates, they only aim to identify the causal parents of one variable. This task alone already has exponential cost, which would be further increased if the algorithm were applied for reconstructing the whole graph by applying it  iteratively to each node. Due to the computational cost, this is infeasible for larger graphs.. Other recent papers e.g. [1] likewise only consider similar number of variables given the computational cost of the proposed algorithms. In contrast one contribution of our paper is a proposal how to avoid an exponential search over all possible DAGs. \n\n[1]. Ghassami, AmirEmad, Saber Salehkaleybar, Negar Kiyavash, and Kun Zhang. \"Learning causal structures using regression invariance.\" In Advances in Neural Information Processing Systems, pp. 3011-3021. 2017.\n\nQ. \u201cThe experimental setup of uniformly sampling an intervening variable seems artificial to me.\u201d\n\nWe thank the reviewer for pointing this out. We agree that in the real world, interventions rarely appear to be chosen uniformly randomly. However, given the lack of better real-world causal structures than those from the BNLearn graph repository, and the lack of a commonly-agreed intervention probability on each node, uniform sampling seemed reasonable. Doing otherwise would have required us to justify why we picked those specific intervention probabilities. However, if the reviewer has suggestions for specific non-uniform intervention probabilities, we will be happy to perform additional experiments with them.\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1986/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1986/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["rosemary.nan.ke@gmail.com", "obilaniu@gmail.com", "anirudhgoyal9119@gmail.com", "stefan.a.bauer@gmail.com", "hugolarochelle@google.com", "chris.j.pal@gmail.com", "yoshua.bengio@mila.quebec"], "title": "Learning Neural Causal Models from Unknown Interventions", "authors": ["Nan Rosemary Ke", "Olexa Bilaniuk", "Anirudh Goyal", "Stephan Bauer", "Hugol Larochelle", "Chris Pal", "Yoshua Bengio"], "pdf": "/pdf/909851dd475666c92f50fb6a01f5c71b8b6432f3.pdf", "TL;DR": "Using end-to-end deep learning to discover the structure of a graphical model which is robust to interventions and trained without knowing what the interventions are", "abstract": "Meta-learning over a set of distributions can be interpreted as learning different types of parameters corresponding to short-term vs long-term aspects of the mechanisms underlying the generation of data. These are respectively captured by quickly-changing \\textit{parameters} and slowly-changing \\textit{meta-parameters}. We present a new framework for meta-learning causal models where the relationship between each variable and its parents is modeled by a neural network, modulated by structural meta-parameters which capture the overall topology of a directed graphical model. Our approach avoids a discrete search over models in favour of a continuous optimization procedure. We study a setting where interventional distributions are induced as a result of a random intervention on a single unknown variable of an unknown ground truth causal model, and the observations arising after such an intervention constitute one meta-example. To disentangle the slow-changing aspects of each conditional from the fast-changing adaptations to each intervention, we parametrize the neural network into fast parameters and slow meta-parameters. We introduce a meta-learning objective that favours solutions \\textit{robust} to frequent but sparse interventional distribution change, and which generalize well to previously unseen interventions. Optimizing this objective is shown experimentally to recover the structure of the causal graph. Finally, we find that when the learner is unaware of the intervention variable, it is able to infer that information, improving results further and focusing the parameter and meta-parameter updates where needed.", "keywords": ["deep learning", "graphical models", "meta learning"], "paperhash": "ke|learning_neural_causal_models_from_unknown_interventions", "original_pdf": "/attachment/46cdf6676b73a75437c5ddaca23f9b373e44a459.pdf", "_bibtex": "@misc{\nke2020learning,\ntitle={Learning Neural Causal Models from Unknown Interventions},\nauthor={Nan Rosemary Ke and Olexa Bilaniuk and Anirudh Goyal and Stephan Bauer and Hugol Larochelle and Chris Pal and Yoshua Bengio},\nyear={2020},\nurl={https://openreview.net/forum?id=H1gN6kSFwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1gN6kSFwS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1986/Authors", "ICLR.cc/2020/Conference/Paper1986/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1986/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1986/Reviewers", "ICLR.cc/2020/Conference/Paper1986/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1986/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1986/Authors|ICLR.cc/2020/Conference/Paper1986/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504147982, "tmdate": 1576860548484, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1986/Authors", "ICLR.cc/2020/Conference/Paper1986/Reviewers", "ICLR.cc/2020/Conference/Paper1986/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1986/-/Official_Comment"}}}, {"id": "HJxO6r5HYr", "original": null, "number": 1, "cdate": 1571296704372, "ddate": null, "tcdate": 1571296704372, "tmdate": 1572972398039, "tddate": null, "forum": "H1gN6kSFwS", "replyto": "H1gN6kSFwS", "invitation": "ICLR.cc/2020/Conference/Paper1986/-/Official_Review", "content": {"rating": "8: Accept", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "The paper develops a learning-based causal inference method that performs multiple tasks jointly: \n\ni) scalable discovery of the underlying Structured Causal Model (SCM) by modeling both structural assignments and the SCM as a continuously parameterized chain of distributions, \n\nii) identification of the intervened variables, which are not known to the model a-priori unlike the mainstream causal inference setups,\n\niii) achieving the two aforementioned goals using meta-learning driven heuristics, i.e. interventions cause distributional shifts. \n\nWhile the paper adopts the core design choices from recent prior art (Bengio et al., 2019), the proposed methodology (especially ii)) is sufficiently novel to be published as a main-track conference paper. The paper is very well-written, follows a concrete and easy-to-follow story line. It solves multiple ambitious problems end-to-end and justifies the methodological novelty claims by a properly conducted set of experiments. The paper also successfully employs simple and useful but forgotten old techniques such as fast/slow parameter decomposition in the proposed model pipeline.\n\nThe intervention prediction heuristic is splendid. It is simple, sensible, and has been proven by experiments to be very effective. I would rate this as the primary novelty presented in this paper.\n\nThe paper can be improved if the below relatively minor concerns are addressed:\n\n i) It would be informative if the paper had a paragraph discussing also the fundamental limitations of the approach more openly. For instance, the choice of the neural net architecture used for the structural assignment might have a huge impact on the outcome, especially because the same architecture is repetitively used for all variables of the SCM. Furthermore, treatment of each variable with a fully independent neural net could cause overparameterization as the SCM grows in number of variables.\n\n ii) The paper makes a strong scalability claim across the variable size thanks to independent Bernoullis assigned on the adjacency matrix entries. However, it reports results only for very small SCMs. It is understandable that given the premature stage of the causal inference research might not grant standardized data sets at a larger scale, but at least lack of this quantitative scalability test could be acknowledged and the related claims could be a little bit softened.\n\n iii) I do not buy the argument in the first paragraph of Sec 3.5 about why the structural assignment functions need to be independent. As the model does not pose a distribution on neural net weights, sharing some weights (i.e. conditioning on them) would only bring conditional independence across the variables. I do not see a solid reason to try to avoid this. What is wrong for multiple variables to share some functional characteristics in their structural assignment? After all, some sort of conditional independence will be inevitable in modeling. If the variables share the same architecture, this is also conditional independence, not full independence. Relaxing the independence assumption and allowing some weight sharing could be beneficial at least for scalability of the model, could even bring about improved model fit due to cross-variable knowledge transfer.\n\nOverall, none of the aforementioned three weaknesses is fundamental. In the status-quo, this is a spectactular research paper and my initial vote is an accept."}, "signatures": ["ICLR.cc/2020/Conference/Paper1986/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1986/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["rosemary.nan.ke@gmail.com", "obilaniu@gmail.com", "anirudhgoyal9119@gmail.com", "stefan.a.bauer@gmail.com", "hugolarochelle@google.com", "chris.j.pal@gmail.com", "yoshua.bengio@mila.quebec"], "title": "Learning Neural Causal Models from Unknown Interventions", "authors": ["Nan Rosemary Ke", "Olexa Bilaniuk", "Anirudh Goyal", "Stephan Bauer", "Hugol Larochelle", "Chris Pal", "Yoshua Bengio"], "pdf": "/pdf/909851dd475666c92f50fb6a01f5c71b8b6432f3.pdf", "TL;DR": "Using end-to-end deep learning to discover the structure of a graphical model which is robust to interventions and trained without knowing what the interventions are", "abstract": "Meta-learning over a set of distributions can be interpreted as learning different types of parameters corresponding to short-term vs long-term aspects of the mechanisms underlying the generation of data. These are respectively captured by quickly-changing \\textit{parameters} and slowly-changing \\textit{meta-parameters}. We present a new framework for meta-learning causal models where the relationship between each variable and its parents is modeled by a neural network, modulated by structural meta-parameters which capture the overall topology of a directed graphical model. Our approach avoids a discrete search over models in favour of a continuous optimization procedure. We study a setting where interventional distributions are induced as a result of a random intervention on a single unknown variable of an unknown ground truth causal model, and the observations arising after such an intervention constitute one meta-example. To disentangle the slow-changing aspects of each conditional from the fast-changing adaptations to each intervention, we parametrize the neural network into fast parameters and slow meta-parameters. We introduce a meta-learning objective that favours solutions \\textit{robust} to frequent but sparse interventional distribution change, and which generalize well to previously unseen interventions. Optimizing this objective is shown experimentally to recover the structure of the causal graph. Finally, we find that when the learner is unaware of the intervention variable, it is able to infer that information, improving results further and focusing the parameter and meta-parameter updates where needed.", "keywords": ["deep learning", "graphical models", "meta learning"], "paperhash": "ke|learning_neural_causal_models_from_unknown_interventions", "original_pdf": "/attachment/46cdf6676b73a75437c5ddaca23f9b373e44a459.pdf", "_bibtex": "@misc{\nke2020learning,\ntitle={Learning Neural Causal Models from Unknown Interventions},\nauthor={Nan Rosemary Ke and Olexa Bilaniuk and Anirudh Goyal and Stephan Bauer and Hugol Larochelle and Chris Pal and Yoshua Bengio},\nyear={2020},\nurl={https://openreview.net/forum?id=H1gN6kSFwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "H1gN6kSFwS", "replyto": "H1gN6kSFwS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1986/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1986/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575859551948, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1986/Reviewers"], "noninvitees": [], "tcdate": 1570237729418, "tmdate": 1575859551961, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1986/-/Official_Review"}}}, {"id": "Skx29AZdFH", "original": null, "number": 2, "cdate": 1571458707959, "ddate": null, "tcdate": 1571458707959, "tmdate": 1572972398003, "tddate": null, "forum": "H1gN6kSFwS", "replyto": "H1gN6kSFwS", "invitation": "ICLR.cc/2020/Conference/Paper1986/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper proposes an SCM-model based on masked neural networks to capture arbitrary conditional relationships combined with meta-learning-style adaptation to reflect the effects of various unknown interventions. Overall the paper is well written and easy to follow, but some conceptual issues remain.\n\n\n- How come there is hardly any discussion of the identifiability issue beyond the few sentences in A.3. This is one of the key issues in learning SCMs and it is strange that the concept of \"faithfulness\" is not even mentioned in the paper.\n\nIn general, there is hardly any discussion of what conditions are required for the proposed estimates to even be valid. The authors seem to be optimistically assuming that their neural network + metalearning model will\u00a0somehow pick up on the correct structure, without any actual conceptual investigation of this issue.\n\n- The massive downside of neural nets is all the various hyperparameters one has to set (eg. architecture, optimizer, activations, etc). In this setting, how do the authors propose selecting hyperparameter values? How does the reader know the authors did not simply tune their hyperparameters to best match the underlying ground truth (I assume the proposed methodology has many more\u00a0hyperparameters and thus more degrees of freedom here).\nI would like to see the empirical performance of different variants of your model with different hyperparameter values to assess its sensitivity to these choices. \n\n- Why does one even care about the graph being acylic in this setting?\nThe mere fact that the authors require a regularizer to ensure acylicity suggests this approach is prone to mis-identifying the ground truth structure (which is always acyclic in the experiments).\n\n- One main reason for SCM modeling in science and policy-making is for analysts to better understand the data generating phenomena.  However your use of neural networks here seems to hamper interpretability, so how do you reconcile this issue? Also is your sparsity regularizer satisfactory to confidently diagnose presence/absence of an edge (in constrast to statistical hypothesis tests, say based on conditional independence). Isn't this heavily influenced by the particular sparsity-regularizer value that happened to be selected?\n\n\n- Related papers that utilize the same idea of predicting a variable conditioned on subset of other variables via neural network + masking strategy:\n\nIvanov et al (2019). VARIATIONAL AUTOENCODER WITH ARBITRARY CONDITIONING. \nhttps://openreview.net/pdf?id=SyxtJh0qYm\n\nLi et al (2019). Flow Models for Arbitrary Conditional Likelihoods.\nhttps://arxiv.org/abs/1909.06319\n\nYoon et al. GAIN: Missing data imputation using generative adversarial nets. Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, 2018. http://proceedings.mlr.press/v80/yoon18a.html\n\nDouglas et al. A universal marginalizer for amortized inference in generative\nmodels. arXiv preprint arXiv:1711.00695, 2017\n\nFor clarity, the authors should highlight the differences of their approach from these works (beyond the causal setting).\n\n- Given the lack of theoretical / conceptual guarantees that the methodology will work, our faith in the proposed methodology rests entirely on the empirical experiments.  However, I find these a bit too basic to be very convincing, and would at least like to see more methods being compared (in particular for the simulated graphs as well).\n\n- The authors should describe what are the underlying interventions in each dataset a bit more.\n\n- The Figures should be better explained (took me a while to figure out what dots/colors represent).\n\n- Why do the authors report cross entropy loss in Table 1? To my knowledge this is not a standard metric for measuring the quality of structure-estimates.\n \n- Instead of ICP (which is constrained to be linear which is unrealistically simple), why don't the authors compare against nonlinearICP (which is more flexible like their neural networks):  \n\nHeinze-Deml et al (2018). Invariant Causal Prediction for Nonlinear Models. https://arxiv.org/pdf/1706.08576.pdf"}, "signatures": ["ICLR.cc/2020/Conference/Paper1986/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1986/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["rosemary.nan.ke@gmail.com", "obilaniu@gmail.com", "anirudhgoyal9119@gmail.com", "stefan.a.bauer@gmail.com", "hugolarochelle@google.com", "chris.j.pal@gmail.com", "yoshua.bengio@mila.quebec"], "title": "Learning Neural Causal Models from Unknown Interventions", "authors": ["Nan Rosemary Ke", "Olexa Bilaniuk", "Anirudh Goyal", "Stephan Bauer", "Hugol Larochelle", "Chris Pal", "Yoshua Bengio"], "pdf": "/pdf/909851dd475666c92f50fb6a01f5c71b8b6432f3.pdf", "TL;DR": "Using end-to-end deep learning to discover the structure of a graphical model which is robust to interventions and trained without knowing what the interventions are", "abstract": "Meta-learning over a set of distributions can be interpreted as learning different types of parameters corresponding to short-term vs long-term aspects of the mechanisms underlying the generation of data. These are respectively captured by quickly-changing \\textit{parameters} and slowly-changing \\textit{meta-parameters}. We present a new framework for meta-learning causal models where the relationship between each variable and its parents is modeled by a neural network, modulated by structural meta-parameters which capture the overall topology of a directed graphical model. Our approach avoids a discrete search over models in favour of a continuous optimization procedure. We study a setting where interventional distributions are induced as a result of a random intervention on a single unknown variable of an unknown ground truth causal model, and the observations arising after such an intervention constitute one meta-example. To disentangle the slow-changing aspects of each conditional from the fast-changing adaptations to each intervention, we parametrize the neural network into fast parameters and slow meta-parameters. We introduce a meta-learning objective that favours solutions \\textit{robust} to frequent but sparse interventional distribution change, and which generalize well to previously unseen interventions. Optimizing this objective is shown experimentally to recover the structure of the causal graph. Finally, we find that when the learner is unaware of the intervention variable, it is able to infer that information, improving results further and focusing the parameter and meta-parameter updates where needed.", "keywords": ["deep learning", "graphical models", "meta learning"], "paperhash": "ke|learning_neural_causal_models_from_unknown_interventions", "original_pdf": "/attachment/46cdf6676b73a75437c5ddaca23f9b373e44a459.pdf", "_bibtex": "@misc{\nke2020learning,\ntitle={Learning Neural Causal Models from Unknown Interventions},\nauthor={Nan Rosemary Ke and Olexa Bilaniuk and Anirudh Goyal and Stephan Bauer and Hugol Larochelle and Chris Pal and Yoshua Bengio},\nyear={2020},\nurl={https://openreview.net/forum?id=H1gN6kSFwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "H1gN6kSFwS", "replyto": "H1gN6kSFwS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1986/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1986/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575859551948, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1986/Reviewers"], "noninvitees": [], "tcdate": 1570237729418, "tmdate": 1575859551961, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1986/-/Official_Review"}}}], "count": 18}