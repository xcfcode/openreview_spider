{"notes": [{"tddate": null, "ddate": null, "cdate": null, "original": null, "tmdate": 1490028597413, "tcdate": 1490028597413, "number": 1, "id": "Syg6VOYpje", "invitation": "ICLR.cc/2017/workshop/-/paper98/acceptance", "forum": "r1HUjsVFg", "replyto": "r1HUjsVFg", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Consistent Alignment of Word Embedding Models", "abstract": "Word embedding models offer continuous vector representations that can capture rich contextual semantics based on their word co-occurrence patterns. While these word vectors can provide very effective features used in many NLP tasks such as clustering similar words and inferring learning relationships, many challenges and open research questions remain. In this paper, we propose a solution that aligns variations of the same model (or different models) in a joint low-dimensional latent space leveraging carefully generated synthetic data points. This generative process is inspired by the observation that a variety of linguistic relationships is captured by simple linear operations in embedded space. We demonstrate that our approach can lead to substantial improvements in recovering embeddings of local neighborhoods.", "pdf": "/pdf/79ce267d9927943a83ab06d05d9ea6dd48a41d8c.pdf", "TL;DR": "Improving consistency and alignment of word embedding models via injection of synthetic data points", "paperhash": "sahin|consistent_alignment_of_word_embedding_models", "conflicts": ["ll.mit.edu"], "keywords": ["Natural language processing", "Transfer Learning", "Unsupervised Learning", "Applications"], "authors": ["Cem Safak Sahin", "Rajmonda S. Caceres", "Brandon Oselio", "William M. Campbell"], "authorids": ["cem.sahin@ll.mit.edu", "rajmonda.caceres@ll.mit.edu", "brandon.oselio@ll.mit.edu", "wcampbell@ll.mit.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1490028597950, "id": "ICLR.cc/2017/workshop/-/paper98/acceptance", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "r1HUjsVFg", "replyto": "r1HUjsVFg", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept", "Reject"]}}}, "nonreaders": [], "cdate": 1490028597950}}}, {"tddate": null, "tmdate": 1489506440077, "tcdate": 1489506440077, "number": 2, "id": "Skl9lqSjg", "invitation": "ICLR.cc/2017/workshop/-/paper98/official/review", "forum": "r1HUjsVFg", "replyto": "r1HUjsVFg", "signatures": ["ICLR.cc/2017/workshop/paper98/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper98/AnonReviewer1"], "content": {"title": "Interesting idea, lacking in clarity", "rating": "3: Clear rejection", "review": "This paper introduces a method for aligning the representations of different word embedding models by leveraging synthetic data points. Even after reading the paper several times, and reading the authors' response to another reviewer's questions, I still struggle to say how exactly the authors achieve this alignment and precisely what problem the technique is supposed to resolve.\n\nIf I understand correctly, synthetic data points are generated by combining (with coefficient +/- 1) embeddings from within the same neighborhood, and if the result also falls within the neighborhood it is retained as a \"latent word\". These latent words help anchor the alignment process. Intuitively, having more points to anchor a neighborhood makes some sense, but I don't understand the details of how the alignment is actually being implemented, so it is hard to say anything more concrete.\n\nIt seems like the major novelty of the proposed approach is in the generation of the latent words. So, in addition to a more detailed explanation of the alignment process, I would have liked to see more analysis or discussion related to the methodology for choosing the latent words. In particular, an experiment in which the latent words are just random points within the neighborhood (rather than linear combinations of existing embeddings) would be very important.\n\nOverall, I think this paper needs considerable work to improve the clarity of exposition, and could use some additional experiments to support the proposed method for choosing latent words.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Consistent Alignment of Word Embedding Models", "abstract": "Word embedding models offer continuous vector representations that can capture rich contextual semantics based on their word co-occurrence patterns. While these word vectors can provide very effective features used in many NLP tasks such as clustering similar words and inferring learning relationships, many challenges and open research questions remain. In this paper, we propose a solution that aligns variations of the same model (or different models) in a joint low-dimensional latent space leveraging carefully generated synthetic data points. This generative process is inspired by the observation that a variety of linguistic relationships is captured by simple linear operations in embedded space. We demonstrate that our approach can lead to substantial improvements in recovering embeddings of local neighborhoods.", "pdf": "/pdf/79ce267d9927943a83ab06d05d9ea6dd48a41d8c.pdf", "TL;DR": "Improving consistency and alignment of word embedding models via injection of synthetic data points", "paperhash": "sahin|consistent_alignment_of_word_embedding_models", "conflicts": ["ll.mit.edu"], "keywords": ["Natural language processing", "Transfer Learning", "Unsupervised Learning", "Applications"], "authors": ["Cem Safak Sahin", "Rajmonda S. Caceres", "Brandon Oselio", "William M. Campbell"], "authorids": ["cem.sahin@ll.mit.edu", "rajmonda.caceres@ll.mit.edu", "brandon.oselio@ll.mit.edu", "wcampbell@ll.mit.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489506440793, "id": "ICLR.cc/2017/workshop/-/paper98/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper98/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper98/AnonReviewer2", "ICLR.cc/2017/workshop/paper98/AnonReviewer1"], "reply": {"forum": "r1HUjsVFg", "replyto": "r1HUjsVFg", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper98/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper98/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489506440793}}}, {"tddate": null, "tmdate": 1489279786249, "tcdate": 1489279786249, "number": 1, "id": "ByGNizfig", "invitation": "ICLR.cc/2017/workshop/-/paper98/public/comment", "forum": "r1HUjsVFg", "replyto": "B1VajElil", "signatures": ["~Cem_Safak_Sahin1"], "readers": ["everyone"], "writers": ["~Cem_Safak_Sahin1"], "content": {"title": "Response to AnonReviewer2", "comment": "Thanks for your review and your time. Our\u00a0response\u00a0for the\u00a0reviewer\u2019s\u00a0comments is below. In general, we agree that the concepts questioned by the reviewer could be explained better in plain english (rather\u00a0than defined as\u00a0equations). However, the 3-page limit made it a\u00a0challenging task to include both. We would be happy to offer a revised version.\n\nA1: The reviewer expressed confusion about our definition of \u00a0\u201cunstable\u201d / \u201cinconsistent\u201d word2vec models. What we mean by unstable or inconsistent is that we observe word2vec models to not always preserve local neighborhood structure even in learning scenarios when we should expect them to. For some given parameter k, that controls what we consider as local, we expect k semantically similar words to be consistently embedded close to each other in high dimensional space. As presented in Fig. 1, word2vec can generate inconsistent embeddings of similar words \u00a0even for the same input data and the same model with the same training parameters. Note that this inconsistency can be due to various reasons such as usage of more\u00a0than one workers\u00a0(i.e., multi-threading)\u00a0during modeling (e.g., , down sampling, size of the input dataset etc). We wanted to highlight in a quantitative way how different these embeddings can be. Furthermore, the inconsistency might be a really important issue if a downstream task requires\u00a0real-time stream output from the word embedding model (e.g., collecting data from chat rooms in\u00a0real time and training word2vec models for some specific down stream task.)\u00a0\n\nA2. We agree with the reviewer that the definitions for the\u00a0\u201cfixed local neighborhood\u201d\u00a0and\u00a0\u201cneighborhood overlap\u201d\u00a0could be explained in a much simpler way than the definitions we give at\u00a0the end of page 2. We would be happy to do so in a revised version of the paper. By a fixed local neighborhood we mean the word and the k-nearest neighboring words in the representation generated by the word2vec model. Note that k is a given parameter. By neighborhood overlap we mean the fraction of common words between the neighborhoods of the same word in two different representations generated by two word2vec models.\n\nA3. We agree with the reviewer about the large body of work around bilingual / multilingual word embedding and their alignments. Our motivation for this paper is not to introduce manifold alignment as a new methodology for improving word embedding problems. Our contribution is to suggest a data imputation technique that together with alignment techniques leads to word representations where similar words are consistently placed near each other. \u00a0Because of this, we believe that our method can further improve the alignment quality and other downstream tasks.\u00a0Another important distinction of our method is that our method is unsupervised in contrast with other existing methods that are either supervised or semi-supervised.\u00a0\n\nA4. The reviewer expressed confusion about the definition of latent words. We define a latent word as synthetically generated points in high dimensional space (page 1). \u00a0We use the coordinates of existing data points (words) to generate the coordinates of these synthetic data points. The formal definition of a latent word is given at page 3 as \u201cA latent word $w^i_\uff0a$ can be generated by $w^i_\uff0a$ = P(\u03b1_n \u00d7 w^i_{r_n} )$, where $\u03b1_n$ is a randomly chosen integer from $[-1, +1]$ and $w^i_{r_n}\u00a0\u2208\u00a0n^i_{\\epsilon} |_{w^i_l}$ .\u201d We use these synthetically generated words to increase the quality of alignment. We agree that using \u201clatent\u201d might be confusing and would be happy to change this reference in a revised version of our paper.\n\nA5. Reviewer asks about the goal of our paper. Our goal is to improve the quality of alignment for two word embedding models by carefully injecting words in a similar fashion that statistical data imputation techniques handle missing data\u00a0under sampled datasets. We provide \u00a0initial, very promising results that such an approach can improve the quality of alignment of different models and because of this we believe that our method would also improve other down stream tasks.\u00a0"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Consistent Alignment of Word Embedding Models", "abstract": "Word embedding models offer continuous vector representations that can capture rich contextual semantics based on their word co-occurrence patterns. While these word vectors can provide very effective features used in many NLP tasks such as clustering similar words and inferring learning relationships, many challenges and open research questions remain. In this paper, we propose a solution that aligns variations of the same model (or different models) in a joint low-dimensional latent space leveraging carefully generated synthetic data points. This generative process is inspired by the observation that a variety of linguistic relationships is captured by simple linear operations in embedded space. We demonstrate that our approach can lead to substantial improvements in recovering embeddings of local neighborhoods.", "pdf": "/pdf/79ce267d9927943a83ab06d05d9ea6dd48a41d8c.pdf", "TL;DR": "Improving consistency and alignment of word embedding models via injection of synthetic data points", "paperhash": "sahin|consistent_alignment_of_word_embedding_models", "conflicts": ["ll.mit.edu"], "keywords": ["Natural language processing", "Transfer Learning", "Unsupervised Learning", "Applications"], "authors": ["Cem Safak Sahin", "Rajmonda S. Caceres", "Brandon Oselio", "William M. Campbell"], "authorids": ["cem.sahin@ll.mit.edu", "rajmonda.caceres@ll.mit.edu", "brandon.oselio@ll.mit.edu", "wcampbell@ll.mit.edu"]}, "tags": [], "invitation": {"tddate": null, "tmdate": 1487350605743, "tcdate": 1487350605743, "id": "ICLR.cc/2017/workshop/-/paper98/public/comment", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2017/workshop/paper98/reviewers"], "reply": {"forum": "r1HUjsVFg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/workshop/reviewers", "ICLR.cc/2017/pcs"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1487350605743}}}, {"tddate": null, "tmdate": 1489157052110, "tcdate": 1489157052110, "number": 1, "id": "B1VajElil", "invitation": "ICLR.cc/2017/workshop/-/paper98/official/review", "forum": "r1HUjsVFg", "replyto": "r1HUjsVFg", "signatures": ["ICLR.cc/2017/workshop/paper98/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper98/AnonReviewer2"], "content": {"title": "Not sure what is being proposed", "rating": "3: Clear rejection", "review": "This paper argues that word embedding algorithms are \"unstable\" and \"inconsistent\", although I am not sure what is meant by these terms, and proposes an alignment solution, although I'm not sure how this solution really works.  The authors state: \n\n\"For a fixed local neighborhood size, we re-train the same model, using the same parameters, on the same training dataset. We then measure model stability as a function of neighborhood overlap across consecutive re-trained model instances.\"\n\nI'm not sure what is meant by a \"fixed local neighbourhood\" (do you mean a fixed region in the induced embedding space? how is this chosen?). Nor do I understand what \"neighborhood overlap\" means (do you mean the fraction of overlapping words found in the same fixed regions in the two different embedding spaces?) \n\nMy best guess is that the authors train two word embedding models initialized differently on the same data, and then measure the fraction of common words that are embedded in the same \"local neighborhood\", i.e. same volume of the induced embedding space.\n\nThat different models learn different embeddings which are not aligned is not at all a surprising finding. Furthermore, that these embedding spaces of different models can be aligned to transfer information from one space to the other is also not surprising. This has motivated a large amount of work on bilingual / multilingual word embeddings (see [1] for just one example), and semi-supervised word embeddings (see [2] for one example).\n\nI was hoping that the mention of \"latent words\" meant that the authors had some kind of latent-variable approach to automatically learn the alignments, but I couldn't understand how the \"latent words\" are generated nor how they are used in the alignment process?\n\nOverall, I am not sure what problem the authors are solving and I'm not sure what their proposed solution really involves, and lastly, I'm not sure in what way the intrinsic evaluation results shown are meant to be interpreted as a measure of success (does the proposed method actually improve results in a real task?). \n\n[1] \"BilBOWA: Fast Bilingual Distributed Word Representations without Word Alignments\", Gouws et al., ICML 2014, https://arxiv.org/pdf/1410.2455.pdf.\n[2] \"Retrofitting Word Vectors to Semantic Lexicons\", Faruqui et al., 2014, https://arxiv.org/pdf/1411.4166.pdf.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Consistent Alignment of Word Embedding Models", "abstract": "Word embedding models offer continuous vector representations that can capture rich contextual semantics based on their word co-occurrence patterns. While these word vectors can provide very effective features used in many NLP tasks such as clustering similar words and inferring learning relationships, many challenges and open research questions remain. In this paper, we propose a solution that aligns variations of the same model (or different models) in a joint low-dimensional latent space leveraging carefully generated synthetic data points. This generative process is inspired by the observation that a variety of linguistic relationships is captured by simple linear operations in embedded space. We demonstrate that our approach can lead to substantial improvements in recovering embeddings of local neighborhoods.", "pdf": "/pdf/79ce267d9927943a83ab06d05d9ea6dd48a41d8c.pdf", "TL;DR": "Improving consistency and alignment of word embedding models via injection of synthetic data points", "paperhash": "sahin|consistent_alignment_of_word_embedding_models", "conflicts": ["ll.mit.edu"], "keywords": ["Natural language processing", "Transfer Learning", "Unsupervised Learning", "Applications"], "authors": ["Cem Safak Sahin", "Rajmonda S. Caceres", "Brandon Oselio", "William M. Campbell"], "authorids": ["cem.sahin@ll.mit.edu", "rajmonda.caceres@ll.mit.edu", "brandon.oselio@ll.mit.edu", "wcampbell@ll.mit.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489506440793, "id": "ICLR.cc/2017/workshop/-/paper98/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper98/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper98/AnonReviewer2", "ICLR.cc/2017/workshop/paper98/AnonReviewer1"], "reply": {"forum": "r1HUjsVFg", "replyto": "r1HUjsVFg", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper98/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper98/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489506440793}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1487350605094, "tcdate": 1487350605094, "number": 98, "id": "r1HUjsVFg", "invitation": "ICLR.cc/2017/workshop/-/submission", "forum": "r1HUjsVFg", "signatures": ["~Cem_Safak_Sahin1"], "readers": ["everyone"], "content": {"title": "Consistent Alignment of Word Embedding Models", "abstract": "Word embedding models offer continuous vector representations that can capture rich contextual semantics based on their word co-occurrence patterns. While these word vectors can provide very effective features used in many NLP tasks such as clustering similar words and inferring learning relationships, many challenges and open research questions remain. In this paper, we propose a solution that aligns variations of the same model (or different models) in a joint low-dimensional latent space leveraging carefully generated synthetic data points. This generative process is inspired by the observation that a variety of linguistic relationships is captured by simple linear operations in embedded space. We demonstrate that our approach can lead to substantial improvements in recovering embeddings of local neighborhoods.", "pdf": "/pdf/79ce267d9927943a83ab06d05d9ea6dd48a41d8c.pdf", "TL;DR": "Improving consistency and alignment of word embedding models via injection of synthetic data points", "paperhash": "sahin|consistent_alignment_of_word_embedding_models", "conflicts": ["ll.mit.edu"], "keywords": ["Natural language processing", "Transfer Learning", "Unsupervised Learning", "Applications"], "authors": ["Cem Safak Sahin", "Rajmonda S. Caceres", "Brandon Oselio", "William M. Campbell"], "authorids": ["cem.sahin@ll.mit.edu", "rajmonda.caceres@ll.mit.edu", "brandon.oselio@ll.mit.edu", "wcampbell@ll.mit.edu"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1487690420000, "tmdate": 1484242559574, "id": "ICLR.cc/2017/workshop/-/submission", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1495466420000, "cdate": 1484242559574}}}], "count": 5}