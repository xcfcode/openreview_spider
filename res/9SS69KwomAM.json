{"notes": [{"id": "9SS69KwomAM", "original": "bL2zfZ4G0BB", "number": 2829, "cdate": 1601308314019, "ddate": null, "tcdate": 1601308314019, "tmdate": 1616117303929, "tddate": null, "forum": "9SS69KwomAM", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Solving Compositional Reinforcement Learning Problems via Task Reduction", "authorids": ["~Yunfei_Li1", "wuyilin98@gmail.com", "~Huazhe_Xu1", "~Xiaolong_Wang3", "~Yi_Wu1"], "authors": ["Yunfei Li", "Yilin Wu", "Huazhe Xu", "Xiaolong Wang", "Yi Wu"], "keywords": ["compositional task", "sparse reward", "reinforcement learning", "task reduction", "imitation learning"], "abstract": "We propose a novel learning paradigm, Self-Imitation via Reduction (SIR), for solving compositional reinforcement learning problems. SIR is based on two core ideas: task reduction and self-imitation. Task reduction tackles a hard-to-solve task by actively reducing it to an easier task whose solution is known by the RL agent. Once the original hard task is successfully solved by task reduction, the agent naturally obtains a self-generated solution trajectory to imitate. By continuously collecting and imitating such demonstrations, the agent is able to progressively expand the solved subspace in the entire task space. Experiment results show that SIR can significantly accelerate and improve learning on a variety of challenging sparse-reward continuous-control problems with compositional structures. Code and videos are available at https://sites.google.com/view/sir-compositional.", "one-sentence_summary": "We propose a deep RL algorithm for learning compositional strategies to solve sparse-reward continuous-control problems.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|solving_compositional_reinforcement_learning_problems_via_task_reduction", "supplementary_material": "/attachment/54b84ef85ce580bdd6d9d567ff3dabf2d3fcc004.zip", "pdf": "/pdf/77f78b692f36356e5e5bbddd012a3367bd821b29.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nli2021solving,\ntitle={Solving Compositional Reinforcement Learning Problems via Task Reduction},\nauthor={Yunfei Li and Yilin Wu and Huazhe Xu and Xiaolong Wang and Yi Wu},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=9SS69KwomAM}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 13, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "Ah8mGqhKd8f", "original": null, "number": 1, "cdate": 1610040351562, "ddate": null, "tcdate": 1610040351562, "tmdate": 1610473940573, "tddate": null, "forum": "9SS69KwomAM", "replyto": "9SS69KwomAM", "invitation": "ICLR.cc/2021/Conference/Paper2829/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Poster)", "comment": "The paper proposes a method for solving challenging sparse reward problems by performing task reduction followed by self-imitation learning from solution trajectories to the reduced tasks.  The core innovation seems to me to be the uses of the reduction search, which is essentially a form of recursive subgoal selection, but where the subgoals are sure to be achievable as assessed by leveraging the learned value function.  This idea seems rather general, though its use is strongly facilitated in this paper by definition of the space (i.e. that object target is the space, is pre-specified; there is only one, rather limited result on a pixel-based task). \n\nNote: Another submission to this conference also explores a quite similar idea to the task reduction proposed in this paper -- see \"Divide-and-Conquer Monte Carlo Tree Search\".  The breaking down of the problem into sub-problems using the value function is similar, but the details of how the papers proceed from there is quite distinct.\n\nThis is a difficult meta-review decision due to the fairly mixed reviews, coupled with limited engagement in the discussion phase.  Two reviewers felt the paper was solid and could be accepted (R1 and R2 with scores 7 and 6 respectively).  R3 gave a borderline review that leaned towards reject (score 5).  R3 replied to the initial author response, which provided helpful feedback to the authors. Ultimately, in my assessment, the authors did a fairly thorough job of addressing some of the points raised by R3, including by adding an additional comparison even where they didn't agree with the reviewer. R4 assigned the paper the lowest score of 3.  The authors provided a lengthy reply to this review asserting that the review may have reflected misunderstanding of paper details, but the reviewer did not respond to the authors.    \n\nTwo core issues raised about this paper relate to the definition of the space for subgoals and the limited difficulty of the tasks. However, this method does not claim to be entirely ignorant of the task space so I don't see the fact that they do include some domain knowledge in designing the goal space to be totally undermining of the method.  They focus on the complementary issue of how to break down difficult problems into sub-problems.  While it would be considerably more impressive if the goal space were learned, I think this harder version of the problem remains a fundamental and deep problem within AI, so it seems to me too much to ask of the present paper (especially given that it was not the stated focus of the paper).  And while the tasks explored in the paper are a little contrived (some repetitive motifs and designed with a relatively small search space over subtasks), these problems do have some complex structure.  Compared to many works in this field, I applaud the authors for engaging with problems with both long-horizon task structure as well as complex high-DoF continuous control component.\n\nWhile I agree with some of the concerns raised, my overall assessment is that I find the contributions sufficiently innovative and substantial to justify acceptance.  The authors proposed a specific innovation and evaluated that innovation.  Insofar as their innovation is somewhat general, I don't think this paper can be the last word on how well it compares with the diverse approaches it could be set against.  And while the experiments are not definitive, I do think they do constitute a fairly ambitious initial validation of the core idea.  \n"}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Solving Compositional Reinforcement Learning Problems via Task Reduction", "authorids": ["~Yunfei_Li1", "wuyilin98@gmail.com", "~Huazhe_Xu1", "~Xiaolong_Wang3", "~Yi_Wu1"], "authors": ["Yunfei Li", "Yilin Wu", "Huazhe Xu", "Xiaolong Wang", "Yi Wu"], "keywords": ["compositional task", "sparse reward", "reinforcement learning", "task reduction", "imitation learning"], "abstract": "We propose a novel learning paradigm, Self-Imitation via Reduction (SIR), for solving compositional reinforcement learning problems. SIR is based on two core ideas: task reduction and self-imitation. Task reduction tackles a hard-to-solve task by actively reducing it to an easier task whose solution is known by the RL agent. Once the original hard task is successfully solved by task reduction, the agent naturally obtains a self-generated solution trajectory to imitate. By continuously collecting and imitating such demonstrations, the agent is able to progressively expand the solved subspace in the entire task space. Experiment results show that SIR can significantly accelerate and improve learning on a variety of challenging sparse-reward continuous-control problems with compositional structures. Code and videos are available at https://sites.google.com/view/sir-compositional.", "one-sentence_summary": "We propose a deep RL algorithm for learning compositional strategies to solve sparse-reward continuous-control problems.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|solving_compositional_reinforcement_learning_problems_via_task_reduction", "supplementary_material": "/attachment/54b84ef85ce580bdd6d9d567ff3dabf2d3fcc004.zip", "pdf": "/pdf/77f78b692f36356e5e5bbddd012a3367bd821b29.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nli2021solving,\ntitle={Solving Compositional Reinforcement Learning Problems via Task Reduction},\nauthor={Yunfei Li and Yilin Wu and Huazhe Xu and Xiaolong Wang and Yi Wu},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=9SS69KwomAM}\n}"}, "tags": [], "invitation": {"reply": {"forum": "9SS69KwomAM", "replyto": "9SS69KwomAM", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040351548, "tmdate": 1610473940555, "id": "ICLR.cc/2021/Conference/Paper2829/-/Decision"}}}, {"id": "mgJT6z6L0Uz", "original": null, "number": 10, "cdate": 1606306796163, "ddate": null, "tcdate": 1606306796163, "tmdate": 1606307228318, "tddate": null, "forum": "9SS69KwomAM", "replyto": "3OwFwHv0jTj", "invitation": "ICLR.cc/2021/Conference/Paper2829/-/Official_Comment", "content": {"title": "We have further updated our paper", "comment": "Thanks for your valuable comments.\n\n**Regarding curriculum learning baseline**\n1. We have updated our paper to include a discussion with the curriculum learning method in Sec 2. We emphasize that despite the conceptual similarity, our method focuses on solving a given task while curriculum learning methods focus on task generation and typically treat policy learning as a black box.\n2. We included a comparison with GoalGAN (reference [1] as mentioned) at the end of Sec 5.3 (Fig 10). Since GoalGAN needs to construct a GAN to generate training tasks, it takes a large number of samples for GAN training. By contrast, our method does not explicitly generate a training task curriculum and therefore learns much faster (even faster than a manually designed training curriculum).\n\n**Regarding the heuristics in Stacking**\nWe have updated our paper and now the stacking experiment *does not use ANY* specialized heuristics. The performance almost remains the same.\n\n**Regarding SAC v.s. PPO**\n1. We add an explanation of this at the beginning of Sec 5. \n2. We want to clarify that we use PPO since it is much *faster w.r.t. wall-clock time* (Fig 17 in Appendix C). Although SAC is typically more sample-efficient than PPO, it runs much more policy optimization steps (i.e., a policy update every a few samples collected) than PPO (i.e., only 1 policy update after a large batch of samples collected). In 4-Room Maze, SAC does not converge after 3 days of training.\n3. We also want to emphasize that we have presented SAC results in a simplified 3-Room Maze in Fig 18 Appendix C. We also run SAC in the comparison with HRL methods."}, "signatures": ["ICLR.cc/2021/Conference/Paper2829/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2829/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Solving Compositional Reinforcement Learning Problems via Task Reduction", "authorids": ["~Yunfei_Li1", "wuyilin98@gmail.com", "~Huazhe_Xu1", "~Xiaolong_Wang3", "~Yi_Wu1"], "authors": ["Yunfei Li", "Yilin Wu", "Huazhe Xu", "Xiaolong Wang", "Yi Wu"], "keywords": ["compositional task", "sparse reward", "reinforcement learning", "task reduction", "imitation learning"], "abstract": "We propose a novel learning paradigm, Self-Imitation via Reduction (SIR), for solving compositional reinforcement learning problems. SIR is based on two core ideas: task reduction and self-imitation. Task reduction tackles a hard-to-solve task by actively reducing it to an easier task whose solution is known by the RL agent. Once the original hard task is successfully solved by task reduction, the agent naturally obtains a self-generated solution trajectory to imitate. By continuously collecting and imitating such demonstrations, the agent is able to progressively expand the solved subspace in the entire task space. Experiment results show that SIR can significantly accelerate and improve learning on a variety of challenging sparse-reward continuous-control problems with compositional structures. Code and videos are available at https://sites.google.com/view/sir-compositional.", "one-sentence_summary": "We propose a deep RL algorithm for learning compositional strategies to solve sparse-reward continuous-control problems.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|solving_compositional_reinforcement_learning_problems_via_task_reduction", "supplementary_material": "/attachment/54b84ef85ce580bdd6d9d567ff3dabf2d3fcc004.zip", "pdf": "/pdf/77f78b692f36356e5e5bbddd012a3367bd821b29.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nli2021solving,\ntitle={Solving Compositional Reinforcement Learning Problems via Task Reduction},\nauthor={Yunfei Li and Yilin Wu and Huazhe Xu and Xiaolong Wang and Yi Wu},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=9SS69KwomAM}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "9SS69KwomAM", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2829/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2829/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2829/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2829/Authors|ICLR.cc/2021/Conference/Paper2829/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2829/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923844080, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2829/-/Official_Comment"}}}, {"id": "SYG1VuUV9O0", "original": null, "number": 2, "cdate": 1605289131667, "ddate": null, "tcdate": 1605289131667, "tmdate": 1606307178144, "tddate": null, "forum": "9SS69KwomAM", "replyto": "QPRiuFPuULK", "invitation": "ICLR.cc/2021/Conference/Paper2829/-/Official_Comment", "content": {"title": "Thanks for your comments", "comment": "We would like to thank reviewer 1 for your valuable comments and acknowledgement on this work.\n\n3: \u201cIs it possible to extend the current approach to a recursive decomposition for harder tasks?\u201d  Yes, it is possible to perform multi-step planning to select multiple subgoals at a time but it makes the reduction search a substantially harder planning problem. Only requiring a 1-step planning operator for solving complex compositional tasks is an important property of our method and makes our method easy to work in practice. We also believe this is one of the reasons why our method outperforms HRL methods, which requires multiple-step planning. Also, task reduction offers an alternative perspective on solving compositional problems by only performing 1-step planning at a time but incorporating compositionality through time with self-imitation. \n\n5 and 6: We are working towards this direction that performs task reduction over the full state in visual domains. In the preliminary U-Wall maze experiment, our algorithm plans with CEM over all dimensions of 16-d VAE latent spaces.\n\n7: \u201cMore detailed discussion on how to mitigate arbitrary reset\u201d:  We can track previous solutions to the trajectories and first return to the step following the tracked solution then perform task reduction, similar to the \u201creturn-and-explore\u201d trick in [1]. Also, we only use reset when performing task reduction at training time. During task-time task reduction, we simply start planning from the initial state, as what we have done in Sec. 5.1 \u201cTask Reduction as a Planner\u201d. \n\n[1] Adrien Ecoffet, Joost Huizinga, Joel Lehman, Kenneth O. Stanley and Jeff Clune. Go-Explore: a New Approach for Hard-Exploration Problems. CoRR, abs/1901.10995, 2019.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2829/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2829/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Solving Compositional Reinforcement Learning Problems via Task Reduction", "authorids": ["~Yunfei_Li1", "wuyilin98@gmail.com", "~Huazhe_Xu1", "~Xiaolong_Wang3", "~Yi_Wu1"], "authors": ["Yunfei Li", "Yilin Wu", "Huazhe Xu", "Xiaolong Wang", "Yi Wu"], "keywords": ["compositional task", "sparse reward", "reinforcement learning", "task reduction", "imitation learning"], "abstract": "We propose a novel learning paradigm, Self-Imitation via Reduction (SIR), for solving compositional reinforcement learning problems. SIR is based on two core ideas: task reduction and self-imitation. Task reduction tackles a hard-to-solve task by actively reducing it to an easier task whose solution is known by the RL agent. Once the original hard task is successfully solved by task reduction, the agent naturally obtains a self-generated solution trajectory to imitate. By continuously collecting and imitating such demonstrations, the agent is able to progressively expand the solved subspace in the entire task space. Experiment results show that SIR can significantly accelerate and improve learning on a variety of challenging sparse-reward continuous-control problems with compositional structures. Code and videos are available at https://sites.google.com/view/sir-compositional.", "one-sentence_summary": "We propose a deep RL algorithm for learning compositional strategies to solve sparse-reward continuous-control problems.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|solving_compositional_reinforcement_learning_problems_via_task_reduction", "supplementary_material": "/attachment/54b84ef85ce580bdd6d9d567ff3dabf2d3fcc004.zip", "pdf": "/pdf/77f78b692f36356e5e5bbddd012a3367bd821b29.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nli2021solving,\ntitle={Solving Compositional Reinforcement Learning Problems via Task Reduction},\nauthor={Yunfei Li and Yilin Wu and Huazhe Xu and Xiaolong Wang and Yi Wu},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=9SS69KwomAM}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "9SS69KwomAM", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2829/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2829/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2829/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2829/Authors|ICLR.cc/2021/Conference/Paper2829/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2829/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923844080, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2829/-/Official_Comment"}}}, {"id": "JHbBGAoV4s1", "original": null, "number": 7, "cdate": 1605289984151, "ddate": null, "tcdate": 1605289984151, "tmdate": 1606307080714, "tddate": null, "forum": "9SS69KwomAM", "replyto": "9SS69KwomAM", "invitation": "ICLR.cc/2021/Conference/Paper2829/-/Official_Comment", "content": {"title": "We have further updated our paper with more results", "comment": "We have uploaded a new version of our paper, with all differences marked in blue.\n\nThe changes are listed as follows:\n1. we include discussions with curriculum learning in Sec 2. \n2. we clarify that we use PPO for maze navigation because SAC is much slower than PPO w.r.t. wall-clock time. We explain this in Sec. 5 and Fig 17 in Appendix C. We also include additional SAC results in Fig 18 in Appendix C.\n3. We updated the results of the Stacking experiment. So now the task reduction search for stacking does not involve any pruning. The performance of SIR remains unchanged. \n4. We include additional results with curriculum learning methods in Sec 5.3 (Fig. 10)."}, "signatures": ["ICLR.cc/2021/Conference/Paper2829/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2829/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Solving Compositional Reinforcement Learning Problems via Task Reduction", "authorids": ["~Yunfei_Li1", "wuyilin98@gmail.com", "~Huazhe_Xu1", "~Xiaolong_Wang3", "~Yi_Wu1"], "authors": ["Yunfei Li", "Yilin Wu", "Huazhe Xu", "Xiaolong Wang", "Yi Wu"], "keywords": ["compositional task", "sparse reward", "reinforcement learning", "task reduction", "imitation learning"], "abstract": "We propose a novel learning paradigm, Self-Imitation via Reduction (SIR), for solving compositional reinforcement learning problems. SIR is based on two core ideas: task reduction and self-imitation. Task reduction tackles a hard-to-solve task by actively reducing it to an easier task whose solution is known by the RL agent. Once the original hard task is successfully solved by task reduction, the agent naturally obtains a self-generated solution trajectory to imitate. By continuously collecting and imitating such demonstrations, the agent is able to progressively expand the solved subspace in the entire task space. Experiment results show that SIR can significantly accelerate and improve learning on a variety of challenging sparse-reward continuous-control problems with compositional structures. Code and videos are available at https://sites.google.com/view/sir-compositional.", "one-sentence_summary": "We propose a deep RL algorithm for learning compositional strategies to solve sparse-reward continuous-control problems.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|solving_compositional_reinforcement_learning_problems_via_task_reduction", "supplementary_material": "/attachment/54b84ef85ce580bdd6d9d567ff3dabf2d3fcc004.zip", "pdf": "/pdf/77f78b692f36356e5e5bbddd012a3367bd821b29.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nli2021solving,\ntitle={Solving Compositional Reinforcement Learning Problems via Task Reduction},\nauthor={Yunfei Li and Yilin Wu and Huazhe Xu and Xiaolong Wang and Yi Wu},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=9SS69KwomAM}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "9SS69KwomAM", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2829/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2829/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2829/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2829/Authors|ICLR.cc/2021/Conference/Paper2829/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2829/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923844080, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2829/-/Official_Comment"}}}, {"id": "DXgYU7IgajI", "original": null, "number": 6, "cdate": 1605289855090, "ddate": null, "tcdate": 1605289855090, "tmdate": 1606303217719, "tddate": null, "forum": "9SS69KwomAM", "replyto": "Rws4AloDp5w", "invitation": "ICLR.cc/2021/Conference/Paper2829/-/Official_Comment", "content": {"title": "Some important properties of our algorithm might be misunderstood (2) ", "comment": "### Responses to specific questions ###\n\n**Regarding \u201cthe library of tasks\u201d**\n\nThe task space is continuously-parameterized (i.e., there are multiple continuous parameters). This setting is different from conventional multi-task learning, which typically assumes a fixed, discrete set of tasks. By contrast, our setting is typically considered as goal-conditioned learning [1][2]. Due to this continuous parameterization, our task space contains an infinite number of possible configurations and therefore it is non-trivial to discover the optimal subgoal. To tackle this challenge, we conduct two solutions in this paper: (1) directly search over the entire space with standard planning techniques such as CEM, as what we did in the extension to visual domain and in comparison with HRL on the relatively simple maze with only the agent itself (Sec. 5.3). Note that, the planning problems we consider (i.e., 1-step planning) are even *much simplified* comparing with standard motion planning or HRL literature; (2) we propose an effective and *generic* heuristic for scenarios with an object-centric state representation, i.e., to search for the states with only one object different from the original state (e.g., what we did in the 4-room maze). Note that even with this heuristic the search space remains *huge* since we still need to search over the entire location space of that object.\n\n**Regarding \u201ca measure of how expensive this offline component is\u201d**\n\nThe offline component in our algorithm is just querying a learned value function when evaluating the sub-goal candidates. The cost of this reduction search is not a bottleneck in our method, since all the queries can be fully paralleled as a single input batch to the value network.  We profiled the computing cost w.r.t. wall-clock time in the \u201cPush\u201d scenario and find that reduction search only accounts for approximately 1% of the whole algorithm. \n\n\n**Regarding comparison with methods that also use \u201cinterim policies\u201d**\n\nWe do conduct experiments with baselines having fair access to interim policies. \n1. In Sec 5.3 with HRL. The higher-level policy in HRL methods does have direct access to ''interim policies'' by treating ''interim policies'' as low-level policies and *explicitly* proposes sub-tasks to them. While our method *implicitly* learns multiple steps of composition with repeated task reduction and self imitation, which is much lightweight and outperforms HRL baselines. \n2. We also consider domains where the interim policies *do not exist at all* for all the methods such as \u201cU-Shape maze\u201d in Fig. 9a. This maze contains no other movable objects except the agent itself. All the algorithms work equally on the observation space that only contains the agent state. So the performances solely depend on how the planning quality of each RL algorithm over the space.\n\n**Regarding DS baseline**\nThe comparison to SAC + DS aims to show these problems are challenging, i.e., simple reward engineering might not be sufficient to solve the problem.\n\n**Regarding \u201cno evidence of implementation\u201d of recursive reduction**\n\n''Recursion'' *implicitly* occurs when task reduction is performed. Fig 6. shows that our agent masters a complex strategy that interacts with multiple objects to solve a task. Learning such a policy requires multiple task reductions, as we described in the 4-Room maze example. It is a major benefit of our method, i.e., it can implicitly learn increasingly more complex policies via self-imitation without the need of explicitly running multiple task reductions in a single trajectory. Since task reduction only succeeds when both subtasks are solved, the agent must have learned to tackle tasks with (N-1) objects before it learns to handle N-object cases. The recursion happens naturally as training proceeds when previously learned policies can be executed to form part(s) of the composite trajectories in task reduction, thus leading to more complex compositional behaviors. \n\n**Regarding \u201cextend hierarchical policies to other domains\u201d**\n\nWe directly follow their original setting (Fig. 9a/b) for a fair comparison. We did try to run skill chaining in our \u201cPush\u201d scenario, but it struggled to make progress. We also noticed that most HRL works are validated in maze navigation domains and focused on navigating the agent itself to the target position without manipulating other objects. We remark that manipulating multiple objects is very challenging: to our knowledge, there is no other effective deep RL method that solves the sparse-reward stacking problem. \n\n \n[1] Leslie Pack Kaelbling.  Hierarchical learning in stochastic domains:  Preliminary results.  In Proceedings of the tenth international conference on machine learning, volume 951, pp. 167\u2013173, 1993.\n\n[2] Tom Schaul, Daniel Horgan, Karol Gregor, and David Silver. Universal value function approximators. In International conference on machine learning, pp. 1312\u20131320, 2015.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2829/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2829/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Solving Compositional Reinforcement Learning Problems via Task Reduction", "authorids": ["~Yunfei_Li1", "wuyilin98@gmail.com", "~Huazhe_Xu1", "~Xiaolong_Wang3", "~Yi_Wu1"], "authors": ["Yunfei Li", "Yilin Wu", "Huazhe Xu", "Xiaolong Wang", "Yi Wu"], "keywords": ["compositional task", "sparse reward", "reinforcement learning", "task reduction", "imitation learning"], "abstract": "We propose a novel learning paradigm, Self-Imitation via Reduction (SIR), for solving compositional reinforcement learning problems. SIR is based on two core ideas: task reduction and self-imitation. Task reduction tackles a hard-to-solve task by actively reducing it to an easier task whose solution is known by the RL agent. Once the original hard task is successfully solved by task reduction, the agent naturally obtains a self-generated solution trajectory to imitate. By continuously collecting and imitating such demonstrations, the agent is able to progressively expand the solved subspace in the entire task space. Experiment results show that SIR can significantly accelerate and improve learning on a variety of challenging sparse-reward continuous-control problems with compositional structures. Code and videos are available at https://sites.google.com/view/sir-compositional.", "one-sentence_summary": "We propose a deep RL algorithm for learning compositional strategies to solve sparse-reward continuous-control problems.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|solving_compositional_reinforcement_learning_problems_via_task_reduction", "supplementary_material": "/attachment/54b84ef85ce580bdd6d9d567ff3dabf2d3fcc004.zip", "pdf": "/pdf/77f78b692f36356e5e5bbddd012a3367bd821b29.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nli2021solving,\ntitle={Solving Compositional Reinforcement Learning Problems via Task Reduction},\nauthor={Yunfei Li and Yilin Wu and Huazhe Xu and Xiaolong Wang and Yi Wu},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=9SS69KwomAM}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "9SS69KwomAM", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2829/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2829/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2829/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2829/Authors|ICLR.cc/2021/Conference/Paper2829/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2829/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923844080, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2829/-/Official_Comment"}}}, {"id": "Rws4AloDp5w", "original": null, "number": 5, "cdate": 1605289750946, "ddate": null, "tcdate": 1605289750946, "tmdate": 1606301424128, "tddate": null, "forum": "9SS69KwomAM", "replyto": "lqMvAFdXO2", "invitation": "ICLR.cc/2021/Conference/Paper2829/-/Official_Comment", "content": {"title": "Some important properties of our algorithm might be misunderstood (1)", "comment": "Dear reviewer,\n\nWe appreciate your comments and suggestions. \n\nIn order to better answer your questions, we first explain in detail how our method works in a concrete setting, the 4-Room maze (Fig.6 & Fig.7 in our paper), which we believe can clarify many of your concerns. We then provide detailed answers to your specific questions in the following thread.\n\n**How SIR works in 4-Room Maze:**\n\n1. We have a *continuous* distribution of tasks. Each task is specified by an agent's initial location, box locations (there are 3 elongated boxes and 1 cubic box),  a target object id (i.e., which object to move), and the goal location. All the locations are specified by 2 coordinates, so a task is parameterized by 12 real parameters and an integer parameter (object id).\n\n2. For each training episode, a *random* task is selected. If the agent fails to complete the task, we search for an intermediate goal $s_b$, which is specified by the object id (which object to move) and the target position for the object (2 coordinates). *Offline* queries to the learned value function are performed to select the best sub-goal (Eq.1). \n\n3. We collect successful composite trajectories and run imitation learning on them in addition to RL training. Hence, when the agent learns to manipulate different objects, task reduction helps the agent quickly learn a composite strategy of first moving an elongated box to clear a blocked door and then moving the cubic box. Once the agent learns to move an object across a blocked door, task reduction further helps the agent quickly learn to clear two elongated boxes to clear two blocked doors. Subsequently, the agent gradually learns to manipulate a growing number of objects to accomplish a task.\n\n4. Finally, the agent learns a complex strategy for an extremely challenging task (Fig. 6), where the agent clears all the 3 elongated boxes before it moves the cubic box to the goal. "}, "signatures": ["ICLR.cc/2021/Conference/Paper2829/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2829/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Solving Compositional Reinforcement Learning Problems via Task Reduction", "authorids": ["~Yunfei_Li1", "wuyilin98@gmail.com", "~Huazhe_Xu1", "~Xiaolong_Wang3", "~Yi_Wu1"], "authors": ["Yunfei Li", "Yilin Wu", "Huazhe Xu", "Xiaolong Wang", "Yi Wu"], "keywords": ["compositional task", "sparse reward", "reinforcement learning", "task reduction", "imitation learning"], "abstract": "We propose a novel learning paradigm, Self-Imitation via Reduction (SIR), for solving compositional reinforcement learning problems. SIR is based on two core ideas: task reduction and self-imitation. Task reduction tackles a hard-to-solve task by actively reducing it to an easier task whose solution is known by the RL agent. Once the original hard task is successfully solved by task reduction, the agent naturally obtains a self-generated solution trajectory to imitate. By continuously collecting and imitating such demonstrations, the agent is able to progressively expand the solved subspace in the entire task space. Experiment results show that SIR can significantly accelerate and improve learning on a variety of challenging sparse-reward continuous-control problems with compositional structures. Code and videos are available at https://sites.google.com/view/sir-compositional.", "one-sentence_summary": "We propose a deep RL algorithm for learning compositional strategies to solve sparse-reward continuous-control problems.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|solving_compositional_reinforcement_learning_problems_via_task_reduction", "supplementary_material": "/attachment/54b84ef85ce580bdd6d9d567ff3dabf2d3fcc004.zip", "pdf": "/pdf/77f78b692f36356e5e5bbddd012a3367bd821b29.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nli2021solving,\ntitle={Solving Compositional Reinforcement Learning Problems via Task Reduction},\nauthor={Yunfei Li and Yilin Wu and Huazhe Xu and Xiaolong Wang and Yi Wu},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=9SS69KwomAM}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "9SS69KwomAM", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2829/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2829/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2829/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2829/Authors|ICLR.cc/2021/Conference/Paper2829/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2829/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923844080, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2829/-/Official_Comment"}}}, {"id": "3OwFwHv0jTj", "original": null, "number": 9, "cdate": 1606243259786, "ddate": null, "tcdate": 1606243259786, "tmdate": 1606243259786, "tddate": null, "forum": "9SS69KwomAM", "replyto": "soENv938V_L", "invitation": "ICLR.cc/2021/Conference/Paper2829/-/Official_Comment", "content": {"title": "Feedback", "comment": "Thank you for the rebuttal. \nIt is appreciated that sometimes different researchers will have different views on what a method represents.\nTo clarify, you propose a method that searches for a good task decomposition such that an intermediate task can be solved to simplify solving the final task. Even if the terminology is not used, this has a lot in common with curriculum learning (i.e. finding easier tasks to simplify learning harder tasks). And because of this similarity, a comparison with methods which include steps for finding simpler tasks (see e.g. the previous reference [1]), such as your algorithm does, would be natural. Instead this type of comparison is missing in the submission. While including any additional algorithm clearly constitutes a good share of additional work, here it is justified given limited baselines baselines.\n\nIn addition, the strongest benefits are obtained in the stacking task where highly important, additional information is only provided to the proposed method. Here, I appreciate the provision of the additional baseline with reduced privileged information. My suggestion would be to only include the additional curve (also in the main paper) and remove the results with privileged information as these mostly hold information about the quality of the additional information and not the proposed algorithm.\n\nThe outperforming of SAC by PPO still remains a big question and I expect that this could be addressed with sufficient hyperparameter tuning for the off-policy algorithm, but regarding the paper's focus this aspect can be seen as less important.\n\nOverall, this remains an interesting paper with a flawed experimental section, which however has been slightly improved by the additional baseline."}, "signatures": ["ICLR.cc/2021/Conference/Paper2829/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2829/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Solving Compositional Reinforcement Learning Problems via Task Reduction", "authorids": ["~Yunfei_Li1", "wuyilin98@gmail.com", "~Huazhe_Xu1", "~Xiaolong_Wang3", "~Yi_Wu1"], "authors": ["Yunfei Li", "Yilin Wu", "Huazhe Xu", "Xiaolong Wang", "Yi Wu"], "keywords": ["compositional task", "sparse reward", "reinforcement learning", "task reduction", "imitation learning"], "abstract": "We propose a novel learning paradigm, Self-Imitation via Reduction (SIR), for solving compositional reinforcement learning problems. SIR is based on two core ideas: task reduction and self-imitation. Task reduction tackles a hard-to-solve task by actively reducing it to an easier task whose solution is known by the RL agent. Once the original hard task is successfully solved by task reduction, the agent naturally obtains a self-generated solution trajectory to imitate. By continuously collecting and imitating such demonstrations, the agent is able to progressively expand the solved subspace in the entire task space. Experiment results show that SIR can significantly accelerate and improve learning on a variety of challenging sparse-reward continuous-control problems with compositional structures. Code and videos are available at https://sites.google.com/view/sir-compositional.", "one-sentence_summary": "We propose a deep RL algorithm for learning compositional strategies to solve sparse-reward continuous-control problems.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|solving_compositional_reinforcement_learning_problems_via_task_reduction", "supplementary_material": "/attachment/54b84ef85ce580bdd6d9d567ff3dabf2d3fcc004.zip", "pdf": "/pdf/77f78b692f36356e5e5bbddd012a3367bd821b29.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nli2021solving,\ntitle={Solving Compositional Reinforcement Learning Problems via Task Reduction},\nauthor={Yunfei Li and Yilin Wu and Huazhe Xu and Xiaolong Wang and Yi Wu},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=9SS69KwomAM}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "9SS69KwomAM", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2829/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2829/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2829/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2829/Authors|ICLR.cc/2021/Conference/Paper2829/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2829/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923844080, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2829/-/Official_Comment"}}}, {"id": "soENv938V_L", "original": null, "number": 4, "cdate": 1605289396834, "ddate": null, "tcdate": 1605289396834, "tmdate": 1605289396834, "tddate": null, "forum": "9SS69KwomAM", "replyto": "R8XdVWbPadl", "invitation": "ICLR.cc/2021/Conference/Paper2829/-/Official_Comment", "content": {"title": "We did not propose a curriculum learning algorithm", "comment": "Dear reviewer, \n\nWe really appreciate your comments. We want to clarify that our paper has been substantially updated since NeurIPS. Our method is NOT a curriculum learning method and the term curriculum learning is not even mentioned before Sec. 4. Our method purely tackles compositional RL problems. We appreciate your NeurIPS review and did agree that the previous version made some inaccurate and confusing claims. We believe the current version describes our contributions much more precisely.\n\nRegarding the space for searching intermediate states during task reduction, we want to clarify that we did not use any heuristics in our visual RL task and the navigation without boxes task compared with HRL methods (Sec. 5.3 Fig. 8 & Fig. 9a). In the visual RL task, we follow the standard setting [1] to first learn a VAE latent space and then use the standard planning method CEM to search/plan over the entire latent space. In the navigation experiment without boxes, we similarly search over the entire observation space. In both cases, our method outperforms baseline methods. \n\nFor RL problems with a lot of objects and an object-centric state representation, we propose a generic heuristic for efficient search (see Sec. 4.4 \u201cReduction-Target Search), that is to search for the states with one object different from the current state. In object-centric domains, we find an effective but still general heuristic to reduce the search space, \n\nIn \u201cStack\u201d scenario, the only additional heuristic we use is to exclude the objects already stacked from the reduction target search space. Note that this is just object-level pruning and we still need to search over the entire location space of each remaining object for a good reduction target. In fact, this pruning is not necessary: we include the new stacking results without pruning during reduction target search in Fig. 15 appendix C.  Our method still performs the best and is only slightly less sample-efficient than the original one with pruning. Lastly, we want to emphasize that even the one-object-difference heuristic is not a strong inductive bias empirically: we noticed that the agent even discovered a strategy to manipulate two objects together as demonstrated in the learned behavior (Fig. 4).\n\nRegarding curriculum learning baselines, as we have explained that this work is simply an RL algorithm for solving compositional tasks. Therefore we think it is not necessary to compare with curriculum learning algorithms.\n\nRegarding \u201coff-policy SAC underperforms on-policy PPO on the navigation task\u201d, this is based on our empirical finding that SAC requires a large number of samples to train is a magnitude slower than PPO w.r.t. wall time: SAC requires ~3e5 seconds to achieve success rate 0.6 while PPO trained for 6e4 seconds can already beat it (see Figure 16  in appendix).  We also want to clarify that we did conduct SAC-based experiments in navigation scenarios. We compared SAC-based SIR with other HRL baselines in relatively simple maze navigation domains in the \u201ccomparison with HRL\u201d subsection where our method significantly outperforms HRL baselines. For the more complex Room maze, we also add the result of SAC-based experiments (Figure 17) into the appendix, where SIR still performs the best compared with SAC and SAC+SIL. \n\n[1] Soroush Nasiriany, Vitchyr Pong, Steven Lin, and Sergey Levine. Planning with goal-conditioned policies. In Advances in Neural Information Processing Systems, pp. 14814\u201314825, 2019.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2829/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2829/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Solving Compositional Reinforcement Learning Problems via Task Reduction", "authorids": ["~Yunfei_Li1", "wuyilin98@gmail.com", "~Huazhe_Xu1", "~Xiaolong_Wang3", "~Yi_Wu1"], "authors": ["Yunfei Li", "Yilin Wu", "Huazhe Xu", "Xiaolong Wang", "Yi Wu"], "keywords": ["compositional task", "sparse reward", "reinforcement learning", "task reduction", "imitation learning"], "abstract": "We propose a novel learning paradigm, Self-Imitation via Reduction (SIR), for solving compositional reinforcement learning problems. SIR is based on two core ideas: task reduction and self-imitation. Task reduction tackles a hard-to-solve task by actively reducing it to an easier task whose solution is known by the RL agent. Once the original hard task is successfully solved by task reduction, the agent naturally obtains a self-generated solution trajectory to imitate. By continuously collecting and imitating such demonstrations, the agent is able to progressively expand the solved subspace in the entire task space. Experiment results show that SIR can significantly accelerate and improve learning on a variety of challenging sparse-reward continuous-control problems with compositional structures. Code and videos are available at https://sites.google.com/view/sir-compositional.", "one-sentence_summary": "We propose a deep RL algorithm for learning compositional strategies to solve sparse-reward continuous-control problems.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|solving_compositional_reinforcement_learning_problems_via_task_reduction", "supplementary_material": "/attachment/54b84ef85ce580bdd6d9d567ff3dabf2d3fcc004.zip", "pdf": "/pdf/77f78b692f36356e5e5bbddd012a3367bd821b29.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nli2021solving,\ntitle={Solving Compositional Reinforcement Learning Problems via Task Reduction},\nauthor={Yunfei Li and Yilin Wu and Huazhe Xu and Xiaolong Wang and Yi Wu},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=9SS69KwomAM}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "9SS69KwomAM", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2829/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2829/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2829/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2829/Authors|ICLR.cc/2021/Conference/Paper2829/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2829/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923844080, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2829/-/Official_Comment"}}}, {"id": "nEYl2P8vX6n", "original": null, "number": 3, "cdate": 1605289228687, "ddate": null, "tcdate": 1605289228687, "tmdate": 1605289228687, "tddate": null, "forum": "9SS69KwomAM", "replyto": "szHrlzWnrBO", "invitation": "ICLR.cc/2021/Conference/Paper2829/-/Official_Comment", "content": {"title": "Citation added and discussed", "comment": "Thank you for pointing out this paper. We have cited it in the related work. This paper considers tabular RL cases and transfers previous experiences to new goals by exploiting a triangularity constraint on value functions. The concept of reusing previous knowledge is similar to ours, but we focus on a different problem that tackles compositional problems (in continuous control).\n \nRegarding the limitation on computationally tractable search spaces, we have applied our method to the visual domain where task reduction is performed over the entire latent space with CEM.\n\nRegarding \u201cit would be more convincing and interesting to show that the proposed method can do deeper planning by applying task reduction recursively\u201d, only requiring a 1-step planning operator for solving complex compositional tasks is an important advantage of our method and makes our method easy to work in practice. We also believe this is one of the reasons why our method outperforms HRL methods, since HRL requires multiple-step planning at a time, which is a substantially harder planning problem. With the help of imitation learning, our policy can still gradually learn increasingly more complex strategies which demonstrate strong compositionality,  for example learning to manipulate multiple boxes in the Stack scenario (Sec. 5.2 Fig. 4) and sequentially pushing away all the elongated boxes that block the door in 4-Room maze scenario (Sec. 5.3 Fig. 6). \n\n\u201cRationale behind product of values\u201d: in binary sparse reward setting, value function approximates the probability of successfully reaching the goal. Product of two values gives the approximate probability that both reduction trajectories can succeed. In the context of goal-conditioned RL, we believe the binary sparse reward is one of the most general forms of reward functions that does not require any engineering.\n\nSome points to clarify: As we state in the last paragraph of Sec. 5.1, all the rollout timesteps during task reduction are already included in the x-axis of our plots to make a fair comparison. HER is applied in all the SAC-based algorithms (SIR, SAC, SIL). \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2829/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2829/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Solving Compositional Reinforcement Learning Problems via Task Reduction", "authorids": ["~Yunfei_Li1", "wuyilin98@gmail.com", "~Huazhe_Xu1", "~Xiaolong_Wang3", "~Yi_Wu1"], "authors": ["Yunfei Li", "Yilin Wu", "Huazhe Xu", "Xiaolong Wang", "Yi Wu"], "keywords": ["compositional task", "sparse reward", "reinforcement learning", "task reduction", "imitation learning"], "abstract": "We propose a novel learning paradigm, Self-Imitation via Reduction (SIR), for solving compositional reinforcement learning problems. SIR is based on two core ideas: task reduction and self-imitation. Task reduction tackles a hard-to-solve task by actively reducing it to an easier task whose solution is known by the RL agent. Once the original hard task is successfully solved by task reduction, the agent naturally obtains a self-generated solution trajectory to imitate. By continuously collecting and imitating such demonstrations, the agent is able to progressively expand the solved subspace in the entire task space. Experiment results show that SIR can significantly accelerate and improve learning on a variety of challenging sparse-reward continuous-control problems with compositional structures. Code and videos are available at https://sites.google.com/view/sir-compositional.", "one-sentence_summary": "We propose a deep RL algorithm for learning compositional strategies to solve sparse-reward continuous-control problems.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|solving_compositional_reinforcement_learning_problems_via_task_reduction", "supplementary_material": "/attachment/54b84ef85ce580bdd6d9d567ff3dabf2d3fcc004.zip", "pdf": "/pdf/77f78b692f36356e5e5bbddd012a3367bd821b29.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nli2021solving,\ntitle={Solving Compositional Reinforcement Learning Problems via Task Reduction},\nauthor={Yunfei Li and Yilin Wu and Huazhe Xu and Xiaolong Wang and Yi Wu},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=9SS69KwomAM}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "9SS69KwomAM", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2829/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2829/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2829/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2829/Authors|ICLR.cc/2021/Conference/Paper2829/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2829/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923844080, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2829/-/Official_Comment"}}}, {"id": "lqMvAFdXO2", "original": null, "number": 1, "cdate": 1603862297637, "ddate": null, "tcdate": 1603862297637, "tmdate": 1605024123433, "tddate": null, "forum": "9SS69KwomAM", "replyto": "9SS69KwomAM", "invitation": "ICLR.cc/2021/Conference/Paper2829/-/Official_Review", "content": {"title": "Review: Solving Compositional RL problems via task reduction: Some key omissions", "review": "#######################################################################\n\nSummary:\n\nIn this paper the authors propose a method for solving compositional tasks in the RL setting. The method (Self-Imitation via Reduction), is a 2-step method in which the agent first reduces the target task into two simpler tasks, and then solves the full task using as a demonstration the composite task uncovered in step-1.\n\n#######################################################################\n\nReasons for score:\n\nI am currently voting for a rejection based on what seem to be two key omissions from the paper:\n\n1. The way in which the set of 'library' tasks is selected, and\n2. A measure of how expensive this offline component of the algorithm is\n\nThe experimental comparisons could also have been stronger.\n\n#######################################################################\nPros:\n\n1. I think the paper is reasonable well written. I found just a few typos, and I think the key conceptual ideas are reasonably easy to follow. \n\n2. The 2-step implementation of some sort of \"policy reduction\" followed by an imitation learning step would seem to be a good in principle idea that merits further exploration\n\n#######################################################################\n\nCons:\n\n1. In Sec 3 the authors talk explicitly about a multi-task RL learning setting, and indeed a central part of the method is a search step over interim states $s_\\beta$ , but I do not see explicitly how this set is constructed. This is of course a critical consideration for a number of reasons:\n\n(1)  If this set of tasks is large, then you are likely to find a good $s_\\beta$, but the search space increases (as does the memory footprint)\n\n(2) If this set is small (and perhaps curated), then the subsequent result is weak (if the agent need only search over a small handful of interim tasks which already include moving the elongated box say, then of course the subsequent learning is rapid). \n\n2. The experimental results would be more compelling if comparisons were made to methods that similarly had some access to interim policies. The comparisons to SAC for example are valid in that they provide a lower bound, but there is additional information available to SIR. Similar it is unclear what the comparison to SAC with a dense rewards shows exactly.\n\n3. The authors mention that the method is extensible even though \"... tasks reduction only performs 1-step planning... SIR still retains the capability of learning an arbitrarily complex policy by alternating between imitation and reduction: as more tasks are solved, these learned tasks can recursive further serve as new reductions...\" - this is a nice idea, but I saw no evidence of this implementation in the paper.\n\n#######################################################################\n\nQuestions during rebuttal period:\n\nPlease address the concerns above. Also, you have some comparison with hierarchical policy algorithms in 9.a/b - is it possible to extend these to the other domains?\n\n#######################################################################\n\nSome general comments:\n\n- I think the results for these multi-step methods are often easier to digest and understand if the phases are presented separately:\n    - Phase 1, reduction:\n        - for each experiment, which task reduction was uncovered by the agent?\n        - as you know, in many cases there are multiple valid reductions, which does your algorithm find and why?\n        - since your reduction phase is 1-step and greedy, in which situations might it not work so well\n        - etc.\n    - Phase 2, imitation:\n        - comparison to other methods\n        - effect of parameter choices\n        - etc.\n- A little more justification for you particular experimental comparisons can be helpful. It seems like there are a few potential avenues you might have liked to explore:\n    - comparison to an information impoverished baseline (like SAC)\n    - comparison to different task distributions\n    - comparison to other subtask methods\n    - etc.", "rating": "3: Clear rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2829/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2829/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Solving Compositional Reinforcement Learning Problems via Task Reduction", "authorids": ["~Yunfei_Li1", "wuyilin98@gmail.com", "~Huazhe_Xu1", "~Xiaolong_Wang3", "~Yi_Wu1"], "authors": ["Yunfei Li", "Yilin Wu", "Huazhe Xu", "Xiaolong Wang", "Yi Wu"], "keywords": ["compositional task", "sparse reward", "reinforcement learning", "task reduction", "imitation learning"], "abstract": "We propose a novel learning paradigm, Self-Imitation via Reduction (SIR), for solving compositional reinforcement learning problems. SIR is based on two core ideas: task reduction and self-imitation. Task reduction tackles a hard-to-solve task by actively reducing it to an easier task whose solution is known by the RL agent. Once the original hard task is successfully solved by task reduction, the agent naturally obtains a self-generated solution trajectory to imitate. By continuously collecting and imitating such demonstrations, the agent is able to progressively expand the solved subspace in the entire task space. Experiment results show that SIR can significantly accelerate and improve learning on a variety of challenging sparse-reward continuous-control problems with compositional structures. Code and videos are available at https://sites.google.com/view/sir-compositional.", "one-sentence_summary": "We propose a deep RL algorithm for learning compositional strategies to solve sparse-reward continuous-control problems.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|solving_compositional_reinforcement_learning_problems_via_task_reduction", "supplementary_material": "/attachment/54b84ef85ce580bdd6d9d567ff3dabf2d3fcc004.zip", "pdf": "/pdf/77f78b692f36356e5e5bbddd012a3367bd821b29.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nli2021solving,\ntitle={Solving Compositional Reinforcement Learning Problems via Task Reduction},\nauthor={Yunfei Li and Yilin Wu and Huazhe Xu and Xiaolong Wang and Yi Wu},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=9SS69KwomAM}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "9SS69KwomAM", "replyto": "9SS69KwomAM", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2829/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538087869, "tmdate": 1606915758814, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2829/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2829/-/Official_Review"}}}, {"id": "R8XdVWbPadl", "original": null, "number": 2, "cdate": 1603912498008, "ddate": null, "tcdate": 1603912498008, "tmdate": 1605024123374, "tddate": null, "forum": "9SS69KwomAM", "replyto": "9SS69KwomAM", "invitation": "ICLR.cc/2021/Conference/Paper2829/-/Official_Review", "content": {"title": "Relevant method, good performance, very limited baselines, and some question remain open.", "review": "The submission proposes an intuitive curriculum learning method which focuses on sparse reward tasks in RL and uses universal value function approximators. \nIt has 3 explicit steps: \n1. identifying a state to decompose the one task into two \n2. solve these new tasks \n3. solve the complete task by imitating the trajectories from both subtasks.\n \nOn the positive side, the paper is overall clearly written and easy to follow for most parts and performs commensurate or better to baselines on a set of simulated manipulation and locomotion domains.\n\nOn the negative side, the method often only performs commensurate or close to the baselines while introducing significant added complexity and additional hyperparameters. The stacking task, which demonstrates the strongest benefit for SIR, requires strong domain knowledge as the search for intermediate states is highly constrained.  Constraining the search for intermediate states to positions with blocks under the required stacking position is significant domain knowledge unavailable to the other methods. The minimum requirement for a fairer comparison would be to include a version of SIR without this constraint. \n\nMore generally, aspects regarding the specifics of the space in which we search for intermediate states and the baselines remain unclear (e.g. in terms of the search space since according to the appendix the space for states and goals is not the same and e.g. for stacking other constraints exist).\n\nThe final problem regarding the evaluation is that while presenting essentially a curriculum learning method, the paper does not compare against other work in curriculum learning as baseline (e.g. [1,2]).\n\nOther questions remain such as the surprising statement that off-policy SAC underperforms on-policy PPO on the navigation task. Statements that are counter to intuition and existing comparisons between SAC and PPO should be supported with experimental results.\n\nOverall, the introduced method follows a valuable direction for curriculum learning in RL but the submission demonstrates significant weaknesses regarding fair evaluation.\n\n[1] Florensa, Carlos, et al. Automatic goal generation  for reinforcement learning agents. In International Conference on Machine Learning 2018\n[2] Racaniere, Sebastien et al. Automated curriculum generation through setter-solver interactions. In International Conference on Learning Representations 2020.\n\n\n(Disclaimer: I have reviewed a previously submitted version of this work and a big share of critical points remains the same between both reviews including domain knowledge unavailable to baselines and comparison to other curriculum learning methods.)\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2829/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2829/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Solving Compositional Reinforcement Learning Problems via Task Reduction", "authorids": ["~Yunfei_Li1", "wuyilin98@gmail.com", "~Huazhe_Xu1", "~Xiaolong_Wang3", "~Yi_Wu1"], "authors": ["Yunfei Li", "Yilin Wu", "Huazhe Xu", "Xiaolong Wang", "Yi Wu"], "keywords": ["compositional task", "sparse reward", "reinforcement learning", "task reduction", "imitation learning"], "abstract": "We propose a novel learning paradigm, Self-Imitation via Reduction (SIR), for solving compositional reinforcement learning problems. SIR is based on two core ideas: task reduction and self-imitation. Task reduction tackles a hard-to-solve task by actively reducing it to an easier task whose solution is known by the RL agent. Once the original hard task is successfully solved by task reduction, the agent naturally obtains a self-generated solution trajectory to imitate. By continuously collecting and imitating such demonstrations, the agent is able to progressively expand the solved subspace in the entire task space. Experiment results show that SIR can significantly accelerate and improve learning on a variety of challenging sparse-reward continuous-control problems with compositional structures. Code and videos are available at https://sites.google.com/view/sir-compositional.", "one-sentence_summary": "We propose a deep RL algorithm for learning compositional strategies to solve sparse-reward continuous-control problems.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|solving_compositional_reinforcement_learning_problems_via_task_reduction", "supplementary_material": "/attachment/54b84ef85ce580bdd6d9d567ff3dabf2d3fcc004.zip", "pdf": "/pdf/77f78b692f36356e5e5bbddd012a3367bd821b29.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nli2021solving,\ntitle={Solving Compositional Reinforcement Learning Problems via Task Reduction},\nauthor={Yunfei Li and Yilin Wu and Huazhe Xu and Xiaolong Wang and Yi Wu},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=9SS69KwomAM}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "9SS69KwomAM", "replyto": "9SS69KwomAM", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2829/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538087869, "tmdate": 1606915758814, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2829/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2829/-/Official_Review"}}}, {"id": "szHrlzWnrBO", "original": null, "number": 3, "cdate": 1603915394896, "ddate": null, "tcdate": 1603915394896, "tmdate": 1605024123305, "tddate": null, "forum": "9SS69KwomAM", "replyto": "9SS69KwomAM", "invitation": "ICLR.cc/2021/Conference/Paper2829/-/Official_Review", "content": {"title": "Review", "review": "# Summary \nThis paper proposes a new method that combines task reduction and self-imitation learning for goal-based reinforcement learning problems. The idea is to decompose a hard task into two subtasks (subgoals) such that the solution to one of them is already known. Self-imitation learning is used to quickly learn to reproduce such successful trajectories. The experimental result shows that the proposed method outperforms the baseline SAC + HER and SAC + SIL as well as hierarchical architectures such as HIRO and DSC.\n\n## Pros\n* The idea is interesting and novel.\n* The empirical results are good.\n\n## Cons\n* Limited to (a subset of) goal-based RL problems.\n\n# Novelty\nThe proposed idea of decomposing a task into two easy tasks is novel and interesting. It would be worth citing and discussing a relevant prior work [1], which also proposes such a decomposition for goal-based RL. \n\n# Quality\n* The empirical results are good. Specifically, it is interesting that the proposed method outperforms hierarchical RL methods without being explicitly hierarchical.\n* At the same time, the proposed method seems very specific to a subset of goal-based RL problems, where searching the goal space is computationally tractable. \n* Though I appreciate the extension to visual domains using $\\beta$-VAE to remedy the aforementioned limitation, the U-Wall maze doesn\u2019t seem like visually complex, and the result (Figure 8) is not very strong. Either showing much better results on Figure 8 or showing results on complex visual domains would strengthen the claim. \n* It would be more convincing and interesting to show that the proposed method can do deeper planning by applying task reduction recursively.\n\n# Clarity\n* The paper is easy to follow, and the figures are well-presented.  \n* What is the rationale behind $V(s_a, s_b, g_a) = V(s_a, s_b) * V(s_b, g_a)$? This seems quite specific to \u201cgoal-reaching\u201d 1-or-0 reward structures. Do you have an idea how to generalize this to more general reward structures? \n* In Algorithm 1, do you generate a new trajectory to get $\\tau\u2019$? If this is the case, is this taken into account as the number of steps (x-axis) in the learning curves? Otherwise, they are not fair comparisons. \n* Just to check if they are apples-to-apples comparisons, do you use HER across all methods (yours and baselines)?\nIt would be good to mention that this paper considers goal-based RL problems early in the paper (in abstract or introduction). \n\n[1] Floyd-Warshall Reinforcement Learning: Learning from Past Experiences to Reach New Goals, Vikas Dhiman et al.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2829/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2829/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Solving Compositional Reinforcement Learning Problems via Task Reduction", "authorids": ["~Yunfei_Li1", "wuyilin98@gmail.com", "~Huazhe_Xu1", "~Xiaolong_Wang3", "~Yi_Wu1"], "authors": ["Yunfei Li", "Yilin Wu", "Huazhe Xu", "Xiaolong Wang", "Yi Wu"], "keywords": ["compositional task", "sparse reward", "reinforcement learning", "task reduction", "imitation learning"], "abstract": "We propose a novel learning paradigm, Self-Imitation via Reduction (SIR), for solving compositional reinforcement learning problems. SIR is based on two core ideas: task reduction and self-imitation. Task reduction tackles a hard-to-solve task by actively reducing it to an easier task whose solution is known by the RL agent. Once the original hard task is successfully solved by task reduction, the agent naturally obtains a self-generated solution trajectory to imitate. By continuously collecting and imitating such demonstrations, the agent is able to progressively expand the solved subspace in the entire task space. Experiment results show that SIR can significantly accelerate and improve learning on a variety of challenging sparse-reward continuous-control problems with compositional structures. Code and videos are available at https://sites.google.com/view/sir-compositional.", "one-sentence_summary": "We propose a deep RL algorithm for learning compositional strategies to solve sparse-reward continuous-control problems.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|solving_compositional_reinforcement_learning_problems_via_task_reduction", "supplementary_material": "/attachment/54b84ef85ce580bdd6d9d567ff3dabf2d3fcc004.zip", "pdf": "/pdf/77f78b692f36356e5e5bbddd012a3367bd821b29.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nli2021solving,\ntitle={Solving Compositional Reinforcement Learning Problems via Task Reduction},\nauthor={Yunfei Li and Yilin Wu and Huazhe Xu and Xiaolong Wang and Yi Wu},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=9SS69KwomAM}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "9SS69KwomAM", "replyto": "9SS69KwomAM", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2829/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538087869, "tmdate": 1606915758814, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2829/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2829/-/Official_Review"}}}, {"id": "QPRiuFPuULK", "original": null, "number": 4, "cdate": 1604009000803, "ddate": null, "tcdate": 1604009000803, "tmdate": 1605024123245, "tddate": null, "forum": "9SS69KwomAM", "replyto": "9SS69KwomAM", "invitation": "ICLR.cc/2021/Conference/Paper2829/-/Official_Review", "content": {"title": "Task reduction and self imitiation; simple approach with nice empirical results", "review": "This paper presents Self Imitation via Reduction (SIR) an approach to learning long-horizon tasks by successively reducing it to easy to solve tasks, generating solutions to these easier tasks and self-imitation on successful task solutions. This is done by training a goal-conditioned policy together with a universal value function. When given a task that cannot be solved via the current policy, SIR searches for an intermediate state that divides the task into two easily solvable sub-tasks; this search is carried out by maximising the (composed) value of the sub-trajectories under the current policy. The policy is then executed for these sub-tasks in sequence; success in both these sub-tasks means a solution for the full task is now found. This solution is used as a demonstration that the policy can use for self-imitation via an advantage weighted behavioural cloning objective. This is combined with the policy loss of standard actor-critic algorithms such as SAC and PPO in both the off-policy and on-policy settings respectively and shows significant performance improvements compared to baselines on several long-horizon tasks such as robotic pushing, stacking 3 blocks and a multi-room maze task. \n\nA few points:\n1. This approach tackles an important problem of effectively learning policies for long-horizon tasks taking advantage of compositional structure of sub-problems. It nicely combines several ideas from prior work such as self-imitation and universal value functions to present a method that is conceptual simple but performs well empirically on complex tasks.\n2. SIR seems quite related to the two-level architecture in standard hierarchical RL approaches \u2014 the planner for task reduction is the top level and the low level being the policy. There is a few key difference though: SIR uses this structure primarily for learning and over time the knowledge in this bi-level structure is distilled into the low level policy. It would be interesting if this is discussed a bit further in the paper.\n3. Unlike traditional sub-goal selection methods which recursively decompose the problem into sub-problems, SIR does a single reduction step to decompose the task into two sub-tasks. This works well under the assumption that the goal-conditioned policy has sufficient representational capacity to capture the variety of tasks in the environment. Is it possible to extend the current approach to a recursive decomposition for harder tasks?\n4. The paper is very well written. There is a clear motivation, contributions and a thorough overview of the related work in this area. The discussions are well structured and together with the appendix a lot of detail is provided on the experiments and methods.\n5. A crucial component of the proposed approach is the search for possible reductions. In the proposed approach this is highly structured and limited to very few dimensions of the actual observation space (e.g. only considering object translations in the pushing task, only considering moving unstacked blocks in the stacking task). This provides a key advantage to SIR compared to baselines as it significantly reduces the branching factor of search and consequently can lead to many successful reductions early on during training. This somewhat reduces the strength of the proposed results. As an additional baseline, it would be good to see the performance achieved by SIR when search is not structured and allowed to explore all dimensions of the state space. Does this reduce the performance and/or learning speed of SIR?\n6. The paper presents initial results on a vision-based task where a VAE representation is used as state. What is the dimensionality of this representation? As mentioned above, this can have a significant impact on the learning performance (while CEM should do better compared to random search it is not clear if this can mitigate the issue by itself).\n7. Another key limitation of the proposed approach is the reliance on arbitrary resets which is not feasible in the real world. While this is briefly discussed in the paper it is not clear how this can be mitigated easily. A more detailed discussion would be useful.\n\nOverall, the approach is quite nice and the initial results are encouraging. I would suggest a weak accept.", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2829/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2829/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Solving Compositional Reinforcement Learning Problems via Task Reduction", "authorids": ["~Yunfei_Li1", "wuyilin98@gmail.com", "~Huazhe_Xu1", "~Xiaolong_Wang3", "~Yi_Wu1"], "authors": ["Yunfei Li", "Yilin Wu", "Huazhe Xu", "Xiaolong Wang", "Yi Wu"], "keywords": ["compositional task", "sparse reward", "reinforcement learning", "task reduction", "imitation learning"], "abstract": "We propose a novel learning paradigm, Self-Imitation via Reduction (SIR), for solving compositional reinforcement learning problems. SIR is based on two core ideas: task reduction and self-imitation. Task reduction tackles a hard-to-solve task by actively reducing it to an easier task whose solution is known by the RL agent. Once the original hard task is successfully solved by task reduction, the agent naturally obtains a self-generated solution trajectory to imitate. By continuously collecting and imitating such demonstrations, the agent is able to progressively expand the solved subspace in the entire task space. Experiment results show that SIR can significantly accelerate and improve learning on a variety of challenging sparse-reward continuous-control problems with compositional structures. Code and videos are available at https://sites.google.com/view/sir-compositional.", "one-sentence_summary": "We propose a deep RL algorithm for learning compositional strategies to solve sparse-reward continuous-control problems.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|solving_compositional_reinforcement_learning_problems_via_task_reduction", "supplementary_material": "/attachment/54b84ef85ce580bdd6d9d567ff3dabf2d3fcc004.zip", "pdf": "/pdf/77f78b692f36356e5e5bbddd012a3367bd821b29.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nli2021solving,\ntitle={Solving Compositional Reinforcement Learning Problems via Task Reduction},\nauthor={Yunfei Li and Yilin Wu and Huazhe Xu and Xiaolong Wang and Yi Wu},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=9SS69KwomAM}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "9SS69KwomAM", "replyto": "9SS69KwomAM", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2829/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538087869, "tmdate": 1606915758814, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2829/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2829/-/Official_Review"}}}], "count": 14}