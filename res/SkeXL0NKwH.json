{"notes": [{"id": "SkeXL0NKwH", "original": "SkgxqkD_Pr", "number": 1133, "cdate": 1569439307144, "ddate": null, "tcdate": 1569439307144, "tmdate": 1577168277816, "tddate": null, "forum": "SkeXL0NKwH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["agural@stanford.edu", "phillip.nadeau@analog.com", "mehul.tikekar@analog.com", "murmann@stanford.edu"], "title": "Low Rank Training of Deep Neural Networks for Emerging Memory Technology", "authors": ["Albert Gural", "Phillip Nadeau", "Mehul Tikekar", "Boris Murmann"], "pdf": "/pdf/ded4869c017f6803939dcc2ebf18c9cca5342392.pdf", "TL;DR": "We use Kronecker sum approximations for low-rank training to address challenges in training neural networks on edge devices that utilize emerging memory technologies.", "abstract": "The recent success of neural networks for solving difficult decision tasks has incentivized incorporating smart decision making \"at the edge.\" However, this work has traditionally focused on neural network inference, rather than training, due to memory and compute limitations, especially in emerging non-volatile memory systems, where writes are energetically costly and reduce lifespan. Yet, the ability to train at the edge is becoming increasingly important as it enables applications such as real-time adaptability to device drift and environmental variation, user customization, and federated learning across devices. In this work, we address four key challenges for training on edge devices with non-volatile memory: low weight update density, weight quantization, low auxiliary memory, and online learning. We present a low-rank training scheme that addresses these four challenges while maintaining computational efficiency. We then demonstrate the technique on a representative convolutional neural network across several adaptation problems, where it out-performs standard SGD both in accuracy and in number of weight updates.", "code": "https://anonymous.4open.science/r/77ebbbb0-45c7-4944-a594-3dd742b7ca07/", "keywords": ["low rank training", "kronecker sum", "emerging memory", "non-volatile memory", "rram", "reram", "federated learning"], "paperhash": "gural|low_rank_training_of_deep_neural_networks_for_emerging_memory_technology", "original_pdf": "/attachment/1ad0a36618589c6a7bda1630c177b35a17a8732e.pdf", "_bibtex": "@misc{\ngural2020low,\ntitle={Low Rank Training of Deep Neural Networks for Emerging Memory Technology},\nauthor={Albert Gural and Phillip Nadeau and Mehul Tikekar and Boris Murmann},\nyear={2020},\nurl={https://openreview.net/forum?id=SkeXL0NKwH}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "SS4yLLIFT", "original": null, "number": 1, "cdate": 1576798715358, "ddate": null, "tcdate": 1576798715358, "tmdate": 1576800921167, "tddate": null, "forum": "SkeXL0NKwH", "replyto": "SkeXL0NKwH", "invitation": "ICLR.cc/2020/Conference/Paper1133/-/Decision", "content": {"decision": "Reject", "comment": "The reviewers generally agreed that the novelty of the work was very limited. This is not necessarily a deal-breaker for a largely applied contribution, but for an applied paper, the evaluation of the actual application on edge devices is not present. So if the main contribution is the application, and there is no evaluation of this application, then it does not seem like the paper is really complete. As such, I cannot recommend it for acceptance.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["agural@stanford.edu", "phillip.nadeau@analog.com", "mehul.tikekar@analog.com", "murmann@stanford.edu"], "title": "Low Rank Training of Deep Neural Networks for Emerging Memory Technology", "authors": ["Albert Gural", "Phillip Nadeau", "Mehul Tikekar", "Boris Murmann"], "pdf": "/pdf/ded4869c017f6803939dcc2ebf18c9cca5342392.pdf", "TL;DR": "We use Kronecker sum approximations for low-rank training to address challenges in training neural networks on edge devices that utilize emerging memory technologies.", "abstract": "The recent success of neural networks for solving difficult decision tasks has incentivized incorporating smart decision making \"at the edge.\" However, this work has traditionally focused on neural network inference, rather than training, due to memory and compute limitations, especially in emerging non-volatile memory systems, where writes are energetically costly and reduce lifespan. Yet, the ability to train at the edge is becoming increasingly important as it enables applications such as real-time adaptability to device drift and environmental variation, user customization, and federated learning across devices. In this work, we address four key challenges for training on edge devices with non-volatile memory: low weight update density, weight quantization, low auxiliary memory, and online learning. We present a low-rank training scheme that addresses these four challenges while maintaining computational efficiency. We then demonstrate the technique on a representative convolutional neural network across several adaptation problems, where it out-performs standard SGD both in accuracy and in number of weight updates.", "code": "https://anonymous.4open.science/r/77ebbbb0-45c7-4944-a594-3dd742b7ca07/", "keywords": ["low rank training", "kronecker sum", "emerging memory", "non-volatile memory", "rram", "reram", "federated learning"], "paperhash": "gural|low_rank_training_of_deep_neural_networks_for_emerging_memory_technology", "original_pdf": "/attachment/1ad0a36618589c6a7bda1630c177b35a17a8732e.pdf", "_bibtex": "@misc{\ngural2020low,\ntitle={Low Rank Training of Deep Neural Networks for Emerging Memory Technology},\nauthor={Albert Gural and Phillip Nadeau and Mehul Tikekar and Boris Murmann},\nyear={2020},\nurl={https://openreview.net/forum?id=SkeXL0NKwH}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "SkeXL0NKwH", "replyto": "SkeXL0NKwH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795704413, "tmdate": 1576800251984, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1133/-/Decision"}}}, {"id": "Hkeyp1RFqB", "original": null, "number": 4, "cdate": 1572622263227, "ddate": null, "tcdate": 1572622263227, "tmdate": 1574713858275, "tddate": null, "forum": "SkeXL0NKwH", "replyto": "SkeXL0NKwH", "invitation": "ICLR.cc/2020/Conference/Paper1133/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #4", "review": "This paper proposes a low-rank training method targeting for edge devices. The main contribution is an algorithm called Streaming Kronecker-Sum Approximation. The authors claim that the proposed method addresses four key challenges of low weight update density, weight quantization, low auxiliary memory, and online learning.\n\nThe paper should be rejected because of the following reasons:\n(1) The paper is a little hard to follow and the writing can be significantly improved. In particular, the authors introduce four main challenges in section 3. However, I found they are not that accessible and hard to understand. In section 4.4.2, the objective is to get a minimum variance rank-r approximation to the diagonal matrix \\Sigma, but I think the authors mix \"m\" up with \"r\".\n(2) The novelty of the algorithm is limited. From section 4.1 to 4.5, most discussions are about previously proposed methods. The algorithm proposed by the author (i.e., SKS) only involves some basic manipulations of linear algebra. I don't think it's novel enough to be a new algorithm.\n(3) Experimental results are limited. The authors spent a lot of time discussing on-device computing, but all their experiments are just simulations on standard benchmarks. For such a paper concerning training on edge devices, I would expect to see some experiments on real edge devices.\n\nOverall, I think the paper needs further improvements to be qualified for being accepted.\n\n-----------------------------------------------------------------------------------------------------------------------------------------------------------------\npost rebuttal:\n\nI've read the authors' responses and the updated paper. Though my concern on writing has been resolved to some extent, I'm still unsatisfied with the empirical experiments. I believe the authors need to do experiments on edge devices since they have emphasized a lot about on-device computing. That being said, I'm not an expert in hardware and have no idea how hard it is to conduct those experiments. I've increased the score to 3 but still vote for rejection.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper1133/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1133/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["agural@stanford.edu", "phillip.nadeau@analog.com", "mehul.tikekar@analog.com", "murmann@stanford.edu"], "title": "Low Rank Training of Deep Neural Networks for Emerging Memory Technology", "authors": ["Albert Gural", "Phillip Nadeau", "Mehul Tikekar", "Boris Murmann"], "pdf": "/pdf/ded4869c017f6803939dcc2ebf18c9cca5342392.pdf", "TL;DR": "We use Kronecker sum approximations for low-rank training to address challenges in training neural networks on edge devices that utilize emerging memory technologies.", "abstract": "The recent success of neural networks for solving difficult decision tasks has incentivized incorporating smart decision making \"at the edge.\" However, this work has traditionally focused on neural network inference, rather than training, due to memory and compute limitations, especially in emerging non-volatile memory systems, where writes are energetically costly and reduce lifespan. Yet, the ability to train at the edge is becoming increasingly important as it enables applications such as real-time adaptability to device drift and environmental variation, user customization, and federated learning across devices. In this work, we address four key challenges for training on edge devices with non-volatile memory: low weight update density, weight quantization, low auxiliary memory, and online learning. We present a low-rank training scheme that addresses these four challenges while maintaining computational efficiency. We then demonstrate the technique on a representative convolutional neural network across several adaptation problems, where it out-performs standard SGD both in accuracy and in number of weight updates.", "code": "https://anonymous.4open.science/r/77ebbbb0-45c7-4944-a594-3dd742b7ca07/", "keywords": ["low rank training", "kronecker sum", "emerging memory", "non-volatile memory", "rram", "reram", "federated learning"], "paperhash": "gural|low_rank_training_of_deep_neural_networks_for_emerging_memory_technology", "original_pdf": "/attachment/1ad0a36618589c6a7bda1630c177b35a17a8732e.pdf", "_bibtex": "@misc{\ngural2020low,\ntitle={Low Rank Training of Deep Neural Networks for Emerging Memory Technology},\nauthor={Albert Gural and Phillip Nadeau and Mehul Tikekar and Boris Murmann},\nyear={2020},\nurl={https://openreview.net/forum?id=SkeXL0NKwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SkeXL0NKwH", "replyto": "SkeXL0NKwH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1133/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1133/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575067312969, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1133/Reviewers"], "noninvitees": [], "tcdate": 1570237741882, "tmdate": 1575067312981, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1133/-/Official_Review"}}}, {"id": "BJlMoRsWqB", "original": null, "number": 3, "cdate": 1572089497857, "ddate": null, "tcdate": 1572089497857, "tmdate": 1574488872672, "tddate": null, "forum": "SkeXL0NKwH", "replyto": "SkeXL0NKwH", "invitation": "ICLR.cc/2020/Conference/Paper1133/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #1", "review": "While inference on edge devices is a popular and well-studied problem in recent days, training on these devices comes with many challenges. This paper proposes a low-rank training schema that helps mitigate some of the critical challenges that occur during training models on NVM memory-based edge devices. Additionally, two techniques, namely streaming batch norm and gradient max norm, are proposed to help training in an online setting. The proposed method is mainly based on approximating the Kronecker sum and is largely inspired by (Benzing et al, ICML 2019, Optimal Kronecker-sum approximation of real time recurrent learning). The proposed approach provides a few optimizations that improves this performance further, and outperforms SGD in terms of accuracy and the number of weights updates in a limited experimental setting.\n\n+ves:\n+ Training on an edge device is a relevant setting, where there is very little work so far, and this is a useful objective.\n+ Focusing on the Kronecker sum and speeding it seems like an interesting solution to solving this problem.\n+ The experimental settings considered (e.g. flipping bits and adding Gaussian noise to weights) is interesting, and perhaps of larger relevance to other work in the area.\n+ The ablation studies provided in the appendix are useful and appreciable.\n\nConcerns:\n- The key concern is that most of the proposed method is built upon Benzing et al\u2019s work (ICML 2019), and the original contribution seems limited. While the paper introduces a few optimizations further, this seems to be incremental than originally novel.\n\n- The problem is formulated for linear regression with least-squares loss, and the experimentation is carried out on the MNIST dataset (classification setting). How is the methodology relevant to the cross-entropy loss, typically used for classification? The paper does not talk about this.\n\n- The results are shown for a network with four 3 x 3 convolution layers and two fully connected layers on the MNIST dataset. Results with different architectures on a few other datasets (at least CIFAR-10) would be necessary to assess the usefulness of this method. Some discussion on what would be the maximum depth of the network that can be trained using this training schema and hardware would be very useful. \n\n- Do all deep neural network architectures (and loss functions) admit a Kronecker sum representation? What class of models can benefit from this method? Factorizations such as Cholesky allow to interpolate between computational complexity and decomposition rank by tuning a rank hyperparameter. Why would such factorizations not be better for memory-constrained settings?\n\n- It would be interesting to see the results using the proposed algorithm on standard hardware, will it provide the same performance as SGD when the scale of the problem increases in terms of layers and other regularization techniques, etc.? This would help understand the performance of the algorithm in a standard-setting.\n\nMinor issues:\n- The paper could have been organized better. Most of the main paper is used to describe Benzing et al\u2019s method, and a lot of the details of the original contributions are in the Appendix. While the equations discussed in Section 4 hold for Linear Regression, the same cannot be directly extended for other networks like CNNs. Considering all the experiments are on CNNs, Sec B.2 in the Appendix should have been in the main paper to help a reader follow the experiments. \n\n=====POST-REBUTTAL COMMENTS========\nI thank the authors for the rebuttal. The paper's organization is much better now. Unfortunately, there are many concerns which are still not convincingly answered: (i) Considering the focus of this work (as admitted by the authors in the rebuttal) is empirical and with limited technical novelty, more comprehensive results on many datasets are required. To some extent, transfer to ImageNet addresses this partially, but not convincingly enough; (ii) It is not clear from the authors' rebuttal why testing this on standard hardware is not possible/appropriate. The rebuttal says \"hard to quantify due to a variety of factors\" but that is vague without stating the factors.\n\nI am willing to increase the rating from 3 to 4, but am unfortunately still towards rejecting the current shape of this work. I believe the work will benefit from considering all reviewers' comments more comprehensively to have a more impactful paper.\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper1133/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1133/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["agural@stanford.edu", "phillip.nadeau@analog.com", "mehul.tikekar@analog.com", "murmann@stanford.edu"], "title": "Low Rank Training of Deep Neural Networks for Emerging Memory Technology", "authors": ["Albert Gural", "Phillip Nadeau", "Mehul Tikekar", "Boris Murmann"], "pdf": "/pdf/ded4869c017f6803939dcc2ebf18c9cca5342392.pdf", "TL;DR": "We use Kronecker sum approximations for low-rank training to address challenges in training neural networks on edge devices that utilize emerging memory technologies.", "abstract": "The recent success of neural networks for solving difficult decision tasks has incentivized incorporating smart decision making \"at the edge.\" However, this work has traditionally focused on neural network inference, rather than training, due to memory and compute limitations, especially in emerging non-volatile memory systems, where writes are energetically costly and reduce lifespan. Yet, the ability to train at the edge is becoming increasingly important as it enables applications such as real-time adaptability to device drift and environmental variation, user customization, and federated learning across devices. In this work, we address four key challenges for training on edge devices with non-volatile memory: low weight update density, weight quantization, low auxiliary memory, and online learning. We present a low-rank training scheme that addresses these four challenges while maintaining computational efficiency. We then demonstrate the technique on a representative convolutional neural network across several adaptation problems, where it out-performs standard SGD both in accuracy and in number of weight updates.", "code": "https://anonymous.4open.science/r/77ebbbb0-45c7-4944-a594-3dd742b7ca07/", "keywords": ["low rank training", "kronecker sum", "emerging memory", "non-volatile memory", "rram", "reram", "federated learning"], "paperhash": "gural|low_rank_training_of_deep_neural_networks_for_emerging_memory_technology", "original_pdf": "/attachment/1ad0a36618589c6a7bda1630c177b35a17a8732e.pdf", "_bibtex": "@misc{\ngural2020low,\ntitle={Low Rank Training of Deep Neural Networks for Emerging Memory Technology},\nauthor={Albert Gural and Phillip Nadeau and Mehul Tikekar and Boris Murmann},\nyear={2020},\nurl={https://openreview.net/forum?id=SkeXL0NKwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SkeXL0NKwH", "replyto": "SkeXL0NKwH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1133/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1133/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575067312969, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1133/Reviewers"], "noninvitees": [], "tcdate": 1570237741882, "tmdate": 1575067312981, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1133/-/Official_Review"}}}, {"id": "Hyxzg_vAFS", "original": null, "number": 2, "cdate": 1571874793597, "ddate": null, "tcdate": 1571874793597, "tmdate": 1574135976686, "tddate": null, "forum": "SkeXL0NKwH", "replyto": "SkeXL0NKwH", "invitation": "ICLR.cc/2020/Conference/Paper1133/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "title": "Official Blind Review #3", "review": "This paper proposes a low rank training method called the Streaming Kronecker Sum approximation (SKS algorithm) for training low precision models on edge devices. The authors compare their method to SGD for convolutional networks on MNIST and demonstrate improvements in terms of accuracy. The authors make use of the Optimal Kronecker-sum algorithm of Benzing et al and propose further improvements to it in the form of the SKS algorithm.\n\nThe main weakness of the paper seems to me limited experimental results - they mainly show improvements for CNNs on MNIST. The gains from their method would be made more  convincing by doing more large scale experiments on other larger datasets such as CIFAR-10/100, Imagenet and text datasets. \n\nOverall, I recommend acceptance based on the thoroughness of the work and the authors open-sourcing their code which would additionally help with reproducibility of their results.\n\n[Edit: Upon reading discussion among other reviewers, I have updated my score to reject.]", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper1133/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1133/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["agural@stanford.edu", "phillip.nadeau@analog.com", "mehul.tikekar@analog.com", "murmann@stanford.edu"], "title": "Low Rank Training of Deep Neural Networks for Emerging Memory Technology", "authors": ["Albert Gural", "Phillip Nadeau", "Mehul Tikekar", "Boris Murmann"], "pdf": "/pdf/ded4869c017f6803939dcc2ebf18c9cca5342392.pdf", "TL;DR": "We use Kronecker sum approximations for low-rank training to address challenges in training neural networks on edge devices that utilize emerging memory technologies.", "abstract": "The recent success of neural networks for solving difficult decision tasks has incentivized incorporating smart decision making \"at the edge.\" However, this work has traditionally focused on neural network inference, rather than training, due to memory and compute limitations, especially in emerging non-volatile memory systems, where writes are energetically costly and reduce lifespan. Yet, the ability to train at the edge is becoming increasingly important as it enables applications such as real-time adaptability to device drift and environmental variation, user customization, and federated learning across devices. In this work, we address four key challenges for training on edge devices with non-volatile memory: low weight update density, weight quantization, low auxiliary memory, and online learning. We present a low-rank training scheme that addresses these four challenges while maintaining computational efficiency. We then demonstrate the technique on a representative convolutional neural network across several adaptation problems, where it out-performs standard SGD both in accuracy and in number of weight updates.", "code": "https://anonymous.4open.science/r/77ebbbb0-45c7-4944-a594-3dd742b7ca07/", "keywords": ["low rank training", "kronecker sum", "emerging memory", "non-volatile memory", "rram", "reram", "federated learning"], "paperhash": "gural|low_rank_training_of_deep_neural_networks_for_emerging_memory_technology", "original_pdf": "/attachment/1ad0a36618589c6a7bda1630c177b35a17a8732e.pdf", "_bibtex": "@misc{\ngural2020low,\ntitle={Low Rank Training of Deep Neural Networks for Emerging Memory Technology},\nauthor={Albert Gural and Phillip Nadeau and Mehul Tikekar and Boris Murmann},\nyear={2020},\nurl={https://openreview.net/forum?id=SkeXL0NKwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SkeXL0NKwH", "replyto": "SkeXL0NKwH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1133/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1133/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575067312969, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1133/Reviewers"], "noninvitees": [], "tcdate": 1570237741882, "tmdate": 1575067312981, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1133/-/Official_Review"}}}, {"id": "HJlTK8qhjr", "original": null, "number": 5, "cdate": 1573852805393, "ddate": null, "tcdate": 1573852805393, "tmdate": 1573852870627, "tddate": null, "forum": "SkeXL0NKwH", "replyto": "r1liBXXTKB", "invitation": "ICLR.cc/2020/Conference/Paper1133/-/Official_Comment", "content": {"title": "Response", "comment": "Thank you for your detailed review. We will address eight questions/suggestions/comments you raised below, in order.\n(a) optLR - We have completely reorganized the paper (see global response, point (1)). Hopefully this clears up the confusion.\n(b) Remove 4.3 - We agreed that this background was superfluous and removed it, as per global response, point (1).\n(c) Intuition for 4.4 - This is a bit difficult to do in such a short space. Our breakdown of the section into two components ((1) computation of the SVD; (2) min-var unbiased estimator of Sigma) was an attempt to provide some intuition for the result.\n(d) SVD - There is no straightforward answer to this. SVD has lower variance so in certain cases, optimization may in fact exhibit better convergence early on, as shown in the newly added convergence experiments. In deep learning experiments, however, SKS seems to have better empirical performance.\n(e) Contributions - These are not the only contributions. See global response, point (2).\n(f) Quantization - See global response, point (1). Our reorganized paper should be clearer.\n(g) Chip/UORO - See global response, point (3).\n(h) Explanation/Additional Comparisons - See global response, points (1) and (3)."}, "signatures": ["ICLR.cc/2020/Conference/Paper1133/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1133/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["agural@stanford.edu", "phillip.nadeau@analog.com", "mehul.tikekar@analog.com", "murmann@stanford.edu"], "title": "Low Rank Training of Deep Neural Networks for Emerging Memory Technology", "authors": ["Albert Gural", "Phillip Nadeau", "Mehul Tikekar", "Boris Murmann"], "pdf": "/pdf/ded4869c017f6803939dcc2ebf18c9cca5342392.pdf", "TL;DR": "We use Kronecker sum approximations for low-rank training to address challenges in training neural networks on edge devices that utilize emerging memory technologies.", "abstract": "The recent success of neural networks for solving difficult decision tasks has incentivized incorporating smart decision making \"at the edge.\" However, this work has traditionally focused on neural network inference, rather than training, due to memory and compute limitations, especially in emerging non-volatile memory systems, where writes are energetically costly and reduce lifespan. Yet, the ability to train at the edge is becoming increasingly important as it enables applications such as real-time adaptability to device drift and environmental variation, user customization, and federated learning across devices. In this work, we address four key challenges for training on edge devices with non-volatile memory: low weight update density, weight quantization, low auxiliary memory, and online learning. We present a low-rank training scheme that addresses these four challenges while maintaining computational efficiency. We then demonstrate the technique on a representative convolutional neural network across several adaptation problems, where it out-performs standard SGD both in accuracy and in number of weight updates.", "code": "https://anonymous.4open.science/r/77ebbbb0-45c7-4944-a594-3dd742b7ca07/", "keywords": ["low rank training", "kronecker sum", "emerging memory", "non-volatile memory", "rram", "reram", "federated learning"], "paperhash": "gural|low_rank_training_of_deep_neural_networks_for_emerging_memory_technology", "original_pdf": "/attachment/1ad0a36618589c6a7bda1630c177b35a17a8732e.pdf", "_bibtex": "@misc{\ngural2020low,\ntitle={Low Rank Training of Deep Neural Networks for Emerging Memory Technology},\nauthor={Albert Gural and Phillip Nadeau and Mehul Tikekar and Boris Murmann},\nyear={2020},\nurl={https://openreview.net/forum?id=SkeXL0NKwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SkeXL0NKwH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1133/Authors", "ICLR.cc/2020/Conference/Paper1133/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1133/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1133/Reviewers", "ICLR.cc/2020/Conference/Paper1133/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1133/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1133/Authors|ICLR.cc/2020/Conference/Paper1133/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504160730, "tmdate": 1576860536391, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1133/Authors", "ICLR.cc/2020/Conference/Paper1133/Reviewers", "ICLR.cc/2020/Conference/Paper1133/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1133/-/Official_Comment"}}}, {"id": "Hygld8c2iH", "original": null, "number": 4, "cdate": 1573852775807, "ddate": null, "tcdate": 1573852775807, "tmdate": 1573852775807, "tddate": null, "forum": "SkeXL0NKwH", "replyto": "Hyxzg_vAFS", "invitation": "ICLR.cc/2020/Conference/Paper1133/-/Official_Comment", "content": {"title": "Response", "comment": "Thank you for your detailed review. We have addressed your concern in our global response, point (3)."}, "signatures": ["ICLR.cc/2020/Conference/Paper1133/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1133/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["agural@stanford.edu", "phillip.nadeau@analog.com", "mehul.tikekar@analog.com", "murmann@stanford.edu"], "title": "Low Rank Training of Deep Neural Networks for Emerging Memory Technology", "authors": ["Albert Gural", "Phillip Nadeau", "Mehul Tikekar", "Boris Murmann"], "pdf": "/pdf/ded4869c017f6803939dcc2ebf18c9cca5342392.pdf", "TL;DR": "We use Kronecker sum approximations for low-rank training to address challenges in training neural networks on edge devices that utilize emerging memory technologies.", "abstract": "The recent success of neural networks for solving difficult decision tasks has incentivized incorporating smart decision making \"at the edge.\" However, this work has traditionally focused on neural network inference, rather than training, due to memory and compute limitations, especially in emerging non-volatile memory systems, where writes are energetically costly and reduce lifespan. Yet, the ability to train at the edge is becoming increasingly important as it enables applications such as real-time adaptability to device drift and environmental variation, user customization, and federated learning across devices. In this work, we address four key challenges for training on edge devices with non-volatile memory: low weight update density, weight quantization, low auxiliary memory, and online learning. We present a low-rank training scheme that addresses these four challenges while maintaining computational efficiency. We then demonstrate the technique on a representative convolutional neural network across several adaptation problems, where it out-performs standard SGD both in accuracy and in number of weight updates.", "code": "https://anonymous.4open.science/r/77ebbbb0-45c7-4944-a594-3dd742b7ca07/", "keywords": ["low rank training", "kronecker sum", "emerging memory", "non-volatile memory", "rram", "reram", "federated learning"], "paperhash": "gural|low_rank_training_of_deep_neural_networks_for_emerging_memory_technology", "original_pdf": "/attachment/1ad0a36618589c6a7bda1630c177b35a17a8732e.pdf", "_bibtex": "@misc{\ngural2020low,\ntitle={Low Rank Training of Deep Neural Networks for Emerging Memory Technology},\nauthor={Albert Gural and Phillip Nadeau and Mehul Tikekar and Boris Murmann},\nyear={2020},\nurl={https://openreview.net/forum?id=SkeXL0NKwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SkeXL0NKwH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1133/Authors", "ICLR.cc/2020/Conference/Paper1133/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1133/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1133/Reviewers", "ICLR.cc/2020/Conference/Paper1133/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1133/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1133/Authors|ICLR.cc/2020/Conference/Paper1133/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504160730, "tmdate": 1576860536391, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1133/Authors", "ICLR.cc/2020/Conference/Paper1133/Reviewers", "ICLR.cc/2020/Conference/Paper1133/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1133/-/Official_Comment"}}}, {"id": "BJx9BI5nsH", "original": null, "number": 3, "cdate": 1573852738062, "ddate": null, "tcdate": 1573852738062, "tmdate": 1573852738062, "tddate": null, "forum": "SkeXL0NKwH", "replyto": "BJlMoRsWqB", "invitation": "ICLR.cc/2020/Conference/Paper1133/-/Official_Comment", "content": {"title": "Response", "comment": "Thank you for your detailed review. We will address the six concerns you raised (including one minor issue) below, in order.\n(a) We address this in the global response, point (2).\n(b) The original linear regression example was more meant to point out that the gradients of the weight matrix are sums of outer products. However, we decided that this way of organizing/motivating the paper was confusing and overly circuitous. Our new paper organization gets straight to the point in Section 3.\n(c) We address this in the global response, point (3). Also on the issue of saying anything analytically interesting, we partially address that in the global response, point (2).\n(d) Anywhere where there is a matrix-vector product should admit a Kronecker sum representation for the derivatives, and it is therefore applicable to a wide variety of deep learning models. Because Kronecker sums are the form the derivative takes, it is natural to base our low-rank representation off of this form. Keep in mind that for our application, having low-rank gradients is good, but the weights themselves do not need to be low-rank since the NVM we are using to store weights is spatially dense.\n(e) I agree this would be interesting, although hard to quantify due to the variety of factors affecting algorithm performance on any specific hardware. Our global response, point (3) may partially address this concern, since we test both SGD and SKS on a larger problem.\n(f) We address this in the global response, point (1)."}, "signatures": ["ICLR.cc/2020/Conference/Paper1133/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1133/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["agural@stanford.edu", "phillip.nadeau@analog.com", "mehul.tikekar@analog.com", "murmann@stanford.edu"], "title": "Low Rank Training of Deep Neural Networks for Emerging Memory Technology", "authors": ["Albert Gural", "Phillip Nadeau", "Mehul Tikekar", "Boris Murmann"], "pdf": "/pdf/ded4869c017f6803939dcc2ebf18c9cca5342392.pdf", "TL;DR": "We use Kronecker sum approximations for low-rank training to address challenges in training neural networks on edge devices that utilize emerging memory technologies.", "abstract": "The recent success of neural networks for solving difficult decision tasks has incentivized incorporating smart decision making \"at the edge.\" However, this work has traditionally focused on neural network inference, rather than training, due to memory and compute limitations, especially in emerging non-volatile memory systems, where writes are energetically costly and reduce lifespan. Yet, the ability to train at the edge is becoming increasingly important as it enables applications such as real-time adaptability to device drift and environmental variation, user customization, and federated learning across devices. In this work, we address four key challenges for training on edge devices with non-volatile memory: low weight update density, weight quantization, low auxiliary memory, and online learning. We present a low-rank training scheme that addresses these four challenges while maintaining computational efficiency. We then demonstrate the technique on a representative convolutional neural network across several adaptation problems, where it out-performs standard SGD both in accuracy and in number of weight updates.", "code": "https://anonymous.4open.science/r/77ebbbb0-45c7-4944-a594-3dd742b7ca07/", "keywords": ["low rank training", "kronecker sum", "emerging memory", "non-volatile memory", "rram", "reram", "federated learning"], "paperhash": "gural|low_rank_training_of_deep_neural_networks_for_emerging_memory_technology", "original_pdf": "/attachment/1ad0a36618589c6a7bda1630c177b35a17a8732e.pdf", "_bibtex": "@misc{\ngural2020low,\ntitle={Low Rank Training of Deep Neural Networks for Emerging Memory Technology},\nauthor={Albert Gural and Phillip Nadeau and Mehul Tikekar and Boris Murmann},\nyear={2020},\nurl={https://openreview.net/forum?id=SkeXL0NKwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SkeXL0NKwH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1133/Authors", "ICLR.cc/2020/Conference/Paper1133/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1133/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1133/Reviewers", "ICLR.cc/2020/Conference/Paper1133/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1133/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1133/Authors|ICLR.cc/2020/Conference/Paper1133/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504160730, "tmdate": 1576860536391, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1133/Authors", "ICLR.cc/2020/Conference/Paper1133/Reviewers", "ICLR.cc/2020/Conference/Paper1133/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1133/-/Official_Comment"}}}, {"id": "SyeTz8c3jH", "original": null, "number": 2, "cdate": 1573852693163, "ddate": null, "tcdate": 1573852693163, "tmdate": 1573852693163, "tddate": null, "forum": "SkeXL0NKwH", "replyto": "Hkeyp1RFqB", "invitation": "ICLR.cc/2020/Conference/Paper1133/-/Official_Comment", "content": {"title": "Response", "comment": "Thank you for your detailed review. We have addressed your points (1-3) in the global response (1-3), respectively.\n\nOne additional point you raised is that there may be a mix up of \"m\" with \"r\". m is meant to represent the singular value index (1, 2, ..., m-1, m, m+1, ..., r, q) where we should start mixing the singular values (m, m+1, ..., r, q) and therefore is distinct from r. However, you may have been referring to a symbol clash between this definition of m and the m found in the dimensions (m x n) of the weight matrix. We have relabeled some variables to avoid this confusion (m => n_i; n => n_o representing the input and output dimensions, respectively)."}, "signatures": ["ICLR.cc/2020/Conference/Paper1133/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1133/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["agural@stanford.edu", "phillip.nadeau@analog.com", "mehul.tikekar@analog.com", "murmann@stanford.edu"], "title": "Low Rank Training of Deep Neural Networks for Emerging Memory Technology", "authors": ["Albert Gural", "Phillip Nadeau", "Mehul Tikekar", "Boris Murmann"], "pdf": "/pdf/ded4869c017f6803939dcc2ebf18c9cca5342392.pdf", "TL;DR": "We use Kronecker sum approximations for low-rank training to address challenges in training neural networks on edge devices that utilize emerging memory technologies.", "abstract": "The recent success of neural networks for solving difficult decision tasks has incentivized incorporating smart decision making \"at the edge.\" However, this work has traditionally focused on neural network inference, rather than training, due to memory and compute limitations, especially in emerging non-volatile memory systems, where writes are energetically costly and reduce lifespan. Yet, the ability to train at the edge is becoming increasingly important as it enables applications such as real-time adaptability to device drift and environmental variation, user customization, and federated learning across devices. In this work, we address four key challenges for training on edge devices with non-volatile memory: low weight update density, weight quantization, low auxiliary memory, and online learning. We present a low-rank training scheme that addresses these four challenges while maintaining computational efficiency. We then demonstrate the technique on a representative convolutional neural network across several adaptation problems, where it out-performs standard SGD both in accuracy and in number of weight updates.", "code": "https://anonymous.4open.science/r/77ebbbb0-45c7-4944-a594-3dd742b7ca07/", "keywords": ["low rank training", "kronecker sum", "emerging memory", "non-volatile memory", "rram", "reram", "federated learning"], "paperhash": "gural|low_rank_training_of_deep_neural_networks_for_emerging_memory_technology", "original_pdf": "/attachment/1ad0a36618589c6a7bda1630c177b35a17a8732e.pdf", "_bibtex": "@misc{\ngural2020low,\ntitle={Low Rank Training of Deep Neural Networks for Emerging Memory Technology},\nauthor={Albert Gural and Phillip Nadeau and Mehul Tikekar and Boris Murmann},\nyear={2020},\nurl={https://openreview.net/forum?id=SkeXL0NKwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SkeXL0NKwH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1133/Authors", "ICLR.cc/2020/Conference/Paper1133/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1133/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1133/Reviewers", "ICLR.cc/2020/Conference/Paper1133/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1133/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1133/Authors|ICLR.cc/2020/Conference/Paper1133/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504160730, "tmdate": 1576860536391, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1133/Authors", "ICLR.cc/2020/Conference/Paper1133/Reviewers", "ICLR.cc/2020/Conference/Paper1133/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1133/-/Official_Comment"}}}, {"id": "S1eXiBc3sr", "original": null, "number": 1, "cdate": 1573852571403, "ddate": null, "tcdate": 1573852571403, "tmdate": 1573852571403, "tddate": null, "forum": "SkeXL0NKwH", "replyto": "SkeXL0NKwH", "invitation": "ICLR.cc/2020/Conference/Paper1133/-/Official_Comment", "content": {"title": "Global Response to Reviewers", "comment": "We would like to thank the reviewers for their constructive criticism which has been instrumental in our paper revision process. We hope that these revisions address most of the reviewer's concerns. To reduce redundancy, we will discuss the key changes here and leave reviewer-specific comments below their respective comments.\n\n1) Paper organization - A number of reviewers found the paper organization confusing. We have addressed this by cutting the less important background sections and being upfront (in Section 3) with the key observation that makes low rank training so well suited for NVM systems. We have also simplified this explanation significantly by focusing on two key challenges (instead of four).\n\n2) Limited novelty - The main purpose of our paper is to make the argument that the low-rank training methods developed for RNN training applications can be used in training NVM systems. As such, it should be expected that algorithmic innovations would be more modest, in comparison to empirical tests. However, another source of novelty can come from analyzing why (or under what conditions) the technique would transfer to this new problem domain successfully. In our paper revision, we added a section analyzing convergence in the online convex setting, which can provide intuition for the key design knobs that allow effective optimization.\n\n3) Limited experiments - Our original experiments tested performance of different algorithms across a variety of different adaptation problems. However, they were all done with the MNIST dataset. At the time, we expected this might represent a realistic use case for the first generation of test chips. However, it has since become clear that larger problems may be of interest and realistically implementable in the near future. To address these more ambitious use cases, we added a suite of transfer learning-like experiments for ImageNet. Addressing one reviewer's concerns, these experiments are swept over different algorithms, ranks, and learning rates to get a better sense of relative algorithm performance. Some reviewers also commented on the lack of on-device experiments. However, since NVM is still in its infancy, we are not currently capable of testing on physical devices. Instead, our paper aims to straddle the boundary between algorithms and hardware."}, "signatures": ["ICLR.cc/2020/Conference/Paper1133/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1133/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["agural@stanford.edu", "phillip.nadeau@analog.com", "mehul.tikekar@analog.com", "murmann@stanford.edu"], "title": "Low Rank Training of Deep Neural Networks for Emerging Memory Technology", "authors": ["Albert Gural", "Phillip Nadeau", "Mehul Tikekar", "Boris Murmann"], "pdf": "/pdf/ded4869c017f6803939dcc2ebf18c9cca5342392.pdf", "TL;DR": "We use Kronecker sum approximations for low-rank training to address challenges in training neural networks on edge devices that utilize emerging memory technologies.", "abstract": "The recent success of neural networks for solving difficult decision tasks has incentivized incorporating smart decision making \"at the edge.\" However, this work has traditionally focused on neural network inference, rather than training, due to memory and compute limitations, especially in emerging non-volatile memory systems, where writes are energetically costly and reduce lifespan. Yet, the ability to train at the edge is becoming increasingly important as it enables applications such as real-time adaptability to device drift and environmental variation, user customization, and federated learning across devices. In this work, we address four key challenges for training on edge devices with non-volatile memory: low weight update density, weight quantization, low auxiliary memory, and online learning. We present a low-rank training scheme that addresses these four challenges while maintaining computational efficiency. We then demonstrate the technique on a representative convolutional neural network across several adaptation problems, where it out-performs standard SGD both in accuracy and in number of weight updates.", "code": "https://anonymous.4open.science/r/77ebbbb0-45c7-4944-a594-3dd742b7ca07/", "keywords": ["low rank training", "kronecker sum", "emerging memory", "non-volatile memory", "rram", "reram", "federated learning"], "paperhash": "gural|low_rank_training_of_deep_neural_networks_for_emerging_memory_technology", "original_pdf": "/attachment/1ad0a36618589c6a7bda1630c177b35a17a8732e.pdf", "_bibtex": "@misc{\ngural2020low,\ntitle={Low Rank Training of Deep Neural Networks for Emerging Memory Technology},\nauthor={Albert Gural and Phillip Nadeau and Mehul Tikekar and Boris Murmann},\nyear={2020},\nurl={https://openreview.net/forum?id=SkeXL0NKwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SkeXL0NKwH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1133/Authors", "ICLR.cc/2020/Conference/Paper1133/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1133/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1133/Reviewers", "ICLR.cc/2020/Conference/Paper1133/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1133/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1133/Authors|ICLR.cc/2020/Conference/Paper1133/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504160730, "tmdate": 1576860536391, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1133/Authors", "ICLR.cc/2020/Conference/Paper1133/Reviewers", "ICLR.cc/2020/Conference/Paper1133/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1133/-/Official_Comment"}}}, {"id": "r1liBXXTKB", "original": null, "number": 1, "cdate": 1571791683436, "ddate": null, "tcdate": 1571791683436, "tmdate": 1572972508521, "tddate": null, "forum": "SkeXL0NKwH", "replyto": "SkeXL0NKwH", "invitation": "ICLR.cc/2020/Conference/Paper1133/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "This work presents a new online training scheme which is amenable to non volatile memories and particularly applicable to smart edge devices. This deals with 4 challenges with smart edge learning paradigms - low weight update density, weight quantization, low auxiliary memory, online learning and show experiments to understand benefits. \n\nDisclaimer: I am far from an expert on this domain, so my review is not very well calibrated or informative. \n\nHow exactly are you implementing optLR in Section 4.2?\n\nMaybe the section 4.3 can instead go to prior work/related work rather than be described in this paper if it\u2019s not being used very much. It seems important for the mixing of the lower SVD elements in the OK method but perhaps can be removed. \n\nI think for people less familiar with the area, a more intuitive explanation of section 4.4 would be helpful. \n\nGenerally how much does the algorithm suffer from using an SVD type approximation for learning?\n\nThe new contributions of this paper are to use a running estimate of Q_L, Q_R and weightings using modified Gram Schmidt. \n\nNot immediately clear how the scheme described related to the quantization point in the challenges. \n\nExperiments seem to show the proposed benefits but are done with artificial models/simulations. Would it be easy to implement this on chip and try it on actual hardware? Can we also compare to things like OK/UORO in the experiments?\n\nOverall seems like a novel and interesting way to do SGD on the edge, I think that a little bit simpler explanation would make the paper more readable and some additional comparisons would also benefit the paper. "}, "signatures": ["ICLR.cc/2020/Conference/Paper1133/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1133/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["agural@stanford.edu", "phillip.nadeau@analog.com", "mehul.tikekar@analog.com", "murmann@stanford.edu"], "title": "Low Rank Training of Deep Neural Networks for Emerging Memory Technology", "authors": ["Albert Gural", "Phillip Nadeau", "Mehul Tikekar", "Boris Murmann"], "pdf": "/pdf/ded4869c017f6803939dcc2ebf18c9cca5342392.pdf", "TL;DR": "We use Kronecker sum approximations for low-rank training to address challenges in training neural networks on edge devices that utilize emerging memory technologies.", "abstract": "The recent success of neural networks for solving difficult decision tasks has incentivized incorporating smart decision making \"at the edge.\" However, this work has traditionally focused on neural network inference, rather than training, due to memory and compute limitations, especially in emerging non-volatile memory systems, where writes are energetically costly and reduce lifespan. Yet, the ability to train at the edge is becoming increasingly important as it enables applications such as real-time adaptability to device drift and environmental variation, user customization, and federated learning across devices. In this work, we address four key challenges for training on edge devices with non-volatile memory: low weight update density, weight quantization, low auxiliary memory, and online learning. We present a low-rank training scheme that addresses these four challenges while maintaining computational efficiency. We then demonstrate the technique on a representative convolutional neural network across several adaptation problems, where it out-performs standard SGD both in accuracy and in number of weight updates.", "code": "https://anonymous.4open.science/r/77ebbbb0-45c7-4944-a594-3dd742b7ca07/", "keywords": ["low rank training", "kronecker sum", "emerging memory", "non-volatile memory", "rram", "reram", "federated learning"], "paperhash": "gural|low_rank_training_of_deep_neural_networks_for_emerging_memory_technology", "original_pdf": "/attachment/1ad0a36618589c6a7bda1630c177b35a17a8732e.pdf", "_bibtex": "@misc{\ngural2020low,\ntitle={Low Rank Training of Deep Neural Networks for Emerging Memory Technology},\nauthor={Albert Gural and Phillip Nadeau and Mehul Tikekar and Boris Murmann},\nyear={2020},\nurl={https://openreview.net/forum?id=SkeXL0NKwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SkeXL0NKwH", "replyto": "SkeXL0NKwH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1133/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1133/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575067312969, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1133/Reviewers"], "noninvitees": [], "tcdate": 1570237741882, "tmdate": 1575067312981, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1133/-/Official_Review"}}}], "count": 11}