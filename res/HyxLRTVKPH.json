{"notes": [{"id": "HyxLRTVKPH", "original": "Byer_FZOvS", "number": 856, "cdate": 1569439181747, "ddate": null, "tcdate": 1569439181747, "tmdate": 1583912043333, "tddate": null, "forum": "HyxLRTVKPH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["mtli@cs.cmu.edu", "meyumer@gmail.com", "deva@cs.cmu.edu"], "title": "Budgeted Training: Rethinking Deep Neural Network Training Under Resource Constraints", "authors": ["Mengtian Li", "Ersin Yumer", "Deva Ramanan"], "pdf": "/pdf/bc6efbe44896472edd87187c87d89ea3d4b0ed22.pdf", "TL;DR": "Introduce a formal setting for budgeted training and propose a budget-aware linear learning rate schedule", "abstract": "In most practical settings and theoretical analyses, one assumes that a model can be trained until convergence. However, the growing complexity of machine learning datasets and models may violate such assumptions. Indeed, current approaches for hyper-parameter tuning and neural architecture search tend to be limited by practical resource constraints. Therefore, we introduce a formal setting for studying training under the non-asymptotic, resource-constrained regime, i.e., budgeted training. We analyze the following problem: \"given a dataset, algorithm, and fixed resource budget, what is the best achievable performance?\" We focus on the number of optimization iterations as the representative resource. Under such a setting, we show that it is critical to adjust the learning rate schedule according to the given budget. Among budget-aware learning schedules, we find simple linear decay to be both robust and high-performing. We support our claim through extensive experiments with state-of-the-art models on ImageNet (image classification), Kinetics (video classification), MS COCO (object detection and instance segmentation), and Cityscapes (semantic segmentation). We also analyze our results and find that the key to a good schedule is budgeted convergence, a phenomenon whereby the gradient vanishes at the end of each allowed budget. We also revisit existing approaches for fast convergence and show that budget-aware learning schedules readily outperform such approaches under (the practical but under-explored) budgeted training setting.", "keywords": ["budgeted training", "learning rate schedule", "linear schedule", "annealing", "learning rate decay"], "paperhash": "li|budgeted_training_rethinking_deep_neural_network_training_under_resource_constraints", "_bibtex": "@inproceedings{\nLi2020Budgeted,\ntitle={Budgeted Training: Rethinking Deep Neural Network Training Under Resource Constraints},\nauthor={Mengtian Li and Ersin Yumer and Deva Ramanan},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HyxLRTVKPH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/cd7f54b5f71631164abf7d4129285f849a388cd1.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "Q1tVFYLr57", "original": null, "number": 8, "cdate": 1580028533421, "ddate": null, "tcdate": 1580028533421, "tmdate": 1580028533421, "tddate": null, "forum": "HyxLRTVKPH", "replyto": "SJgRJmmbjB", "invitation": "ICLR.cc/2020/Conference/Paper856/-/Official_Comment", "content": {"title": "Early Stopping and Variance in Table 12", "comment": "We agree that the proposed method is not compatible with early stopping, which might limit its applicability to certain training schemes. Ideally, one would like to train for enough epochs and choose the best performing checkpoint. However, the training budget can be limited sometimes. As mentioned in the introduction, budgeted training is useful for future datasets which can be magnitudes larger, and for research development stage and neural architecture search (NAS) where only the relative performance is of interest. Our experiments suggest that in those cases, budget-aware schedules might be preferred over early stopping.  Though not directly related, hopefully, our NAS experiments in Appendix A address some of R2's concerns on HPO.\n\nAlso, thanks for pointing out the mistake in Table 12 (now Table 3)! We have fixed it."}, "signatures": ["ICLR.cc/2020/Conference/Paper856/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper856/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["mtli@cs.cmu.edu", "meyumer@gmail.com", "deva@cs.cmu.edu"], "title": "Budgeted Training: Rethinking Deep Neural Network Training Under Resource Constraints", "authors": ["Mengtian Li", "Ersin Yumer", "Deva Ramanan"], "pdf": "/pdf/bc6efbe44896472edd87187c87d89ea3d4b0ed22.pdf", "TL;DR": "Introduce a formal setting for budgeted training and propose a budget-aware linear learning rate schedule", "abstract": "In most practical settings and theoretical analyses, one assumes that a model can be trained until convergence. However, the growing complexity of machine learning datasets and models may violate such assumptions. Indeed, current approaches for hyper-parameter tuning and neural architecture search tend to be limited by practical resource constraints. Therefore, we introduce a formal setting for studying training under the non-asymptotic, resource-constrained regime, i.e., budgeted training. We analyze the following problem: \"given a dataset, algorithm, and fixed resource budget, what is the best achievable performance?\" We focus on the number of optimization iterations as the representative resource. Under such a setting, we show that it is critical to adjust the learning rate schedule according to the given budget. Among budget-aware learning schedules, we find simple linear decay to be both robust and high-performing. We support our claim through extensive experiments with state-of-the-art models on ImageNet (image classification), Kinetics (video classification), MS COCO (object detection and instance segmentation), and Cityscapes (semantic segmentation). We also analyze our results and find that the key to a good schedule is budgeted convergence, a phenomenon whereby the gradient vanishes at the end of each allowed budget. We also revisit existing approaches for fast convergence and show that budget-aware learning schedules readily outperform such approaches under (the practical but under-explored) budgeted training setting.", "keywords": ["budgeted training", "learning rate schedule", "linear schedule", "annealing", "learning rate decay"], "paperhash": "li|budgeted_training_rethinking_deep_neural_network_training_under_resource_constraints", "_bibtex": "@inproceedings{\nLi2020Budgeted,\ntitle={Budgeted Training: Rethinking Deep Neural Network Training Under Resource Constraints},\nauthor={Mengtian Li and Ersin Yumer and Deva Ramanan},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HyxLRTVKPH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/cd7f54b5f71631164abf7d4129285f849a388cd1.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HyxLRTVKPH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper856/Authors", "ICLR.cc/2020/Conference/Paper856/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper856/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper856/Reviewers", "ICLR.cc/2020/Conference/Paper856/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper856/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper856/Authors|ICLR.cc/2020/Conference/Paper856/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504165152, "tmdate": 1576860560179, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper856/Authors", "ICLR.cc/2020/Conference/Paper856/Reviewers", "ICLR.cc/2020/Conference/Paper856/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper856/-/Official_Comment"}}}, {"id": "13QDi3MegV", "original": null, "number": 1, "cdate": 1576798707911, "ddate": null, "tcdate": 1576798707911, "tmdate": 1576800928441, "tddate": null, "forum": "HyxLRTVKPH", "replyto": "HyxLRTVKPH", "invitation": "ICLR.cc/2020/Conference/Paper856/-/Decision", "content": {"decision": "Accept (Poster)", "comment": "This paper formalizes the problem of training deep networks in the presence of a budget, expressed here as a maximum total number of optimization iterations, and evaluates various budget-aware learning schedules, finding simple linear decay to work well. \n\nPost-discussion, the reviewers all felt that this was a good paper. There were some concerns about the lack of theoretical justification for linear decay, but these were overruled by the practical use of these papers to the community. Therefore I am recommending it be accepted.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["mtli@cs.cmu.edu", "meyumer@gmail.com", "deva@cs.cmu.edu"], "title": "Budgeted Training: Rethinking Deep Neural Network Training Under Resource Constraints", "authors": ["Mengtian Li", "Ersin Yumer", "Deva Ramanan"], "pdf": "/pdf/bc6efbe44896472edd87187c87d89ea3d4b0ed22.pdf", "TL;DR": "Introduce a formal setting for budgeted training and propose a budget-aware linear learning rate schedule", "abstract": "In most practical settings and theoretical analyses, one assumes that a model can be trained until convergence. However, the growing complexity of machine learning datasets and models may violate such assumptions. Indeed, current approaches for hyper-parameter tuning and neural architecture search tend to be limited by practical resource constraints. Therefore, we introduce a formal setting for studying training under the non-asymptotic, resource-constrained regime, i.e., budgeted training. We analyze the following problem: \"given a dataset, algorithm, and fixed resource budget, what is the best achievable performance?\" We focus on the number of optimization iterations as the representative resource. Under such a setting, we show that it is critical to adjust the learning rate schedule according to the given budget. Among budget-aware learning schedules, we find simple linear decay to be both robust and high-performing. We support our claim through extensive experiments with state-of-the-art models on ImageNet (image classification), Kinetics (video classification), MS COCO (object detection and instance segmentation), and Cityscapes (semantic segmentation). We also analyze our results and find that the key to a good schedule is budgeted convergence, a phenomenon whereby the gradient vanishes at the end of each allowed budget. We also revisit existing approaches for fast convergence and show that budget-aware learning schedules readily outperform such approaches under (the practical but under-explored) budgeted training setting.", "keywords": ["budgeted training", "learning rate schedule", "linear schedule", "annealing", "learning rate decay"], "paperhash": "li|budgeted_training_rethinking_deep_neural_network_training_under_resource_constraints", "_bibtex": "@inproceedings{\nLi2020Budgeted,\ntitle={Budgeted Training: Rethinking Deep Neural Network Training Under Resource Constraints},\nauthor={Mengtian Li and Ersin Yumer and Deva Ramanan},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HyxLRTVKPH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/cd7f54b5f71631164abf7d4129285f849a388cd1.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "HyxLRTVKPH", "replyto": "HyxLRTVKPH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795717095, "tmdate": 1576800267311, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper856/-/Decision"}}}, {"id": "SJgRJmmbjB", "original": null, "number": 3, "cdate": 1573102309863, "ddate": null, "tcdate": 1573102309863, "tmdate": 1574649348452, "tddate": null, "forum": "HyxLRTVKPH", "replyto": "HyxLRTVKPH", "invitation": "ICLR.cc/2020/Conference/Paper856/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #2", "review": "Pros:\nThe paper is clearly written. It provides an interesting perspective for training neural networks under resource constraints. The problem setting is novel. The proposed solution is simply decaying learning rate linearly from the initial value to zero during training, which is parameter free. \n\nCons:\n\n- As the authors are advocating using linear scaling schedule, I would like to see whether it has some clear advantages over other schedules, but it is not quite clear. For example, we can still see step based schedule has better performance in 2 of the 4 tasks in Table 2. Poly and Cosine schedule is also better in some of the budgets in Figure 2. \n\n- The comparison in Figure 2 and Table 2 is not very convincing without considering the variance of different trails. It is not clear whether the advantage is caused by learning rate schedule or randomness. It is better to report the mean and variance for multiple trials. Ideally, it would be better to performance significance test.\n\n- I would like to see other lr schedules in Table 2. As shown in Figure 2, step based schedule is not the top3 schedules for CIFAR-10.\n\n- As shown in Table 3, the proposed method has to wait until the end of training to get the best performing model, while step based schedule can find the best model around 90% training. The author argues that the proposed method does not need to perform validation test for each checkpoint and reduce the computation cost,  however, on the other side, this means early stopping is not able to use for linear scaling based schedule, which could be very useful when the training budget is large enough and evaluation is cheap.\n\n- My major concern for this work is a lack of deeper understanding about the reason why linear LR schedule works better, if any. It would be stronger with such understandings. The authors try to provide an explanation from the relationship between learning rate and gradient magnitudes, but no clear conclusion is given. As noted in [1,2,3], when weight decay is used in training and BN layers are used, the weight magnitude is also decreasing, so is the gradient norms. But the weight norms or gradient norms does not mean too much due to scale invariance. I would like to see when no weight decay is used and whether there is any correlation between the learning rate and gradient norms. \n\n- What is the lr decay unit for linear schedule? Is it decaying per epoch or per mini-batch? If epoch based lr decay is used, it is essentially step-based lr decay with many steps. Then when the number of epochs is three, the step decay method (lr decays at epoch 1 and 2) and the linear decay method are actually almost equivalent. \n\n- I would expect the convergence to be related with number of iterations. When the number of iterations is not long enough, neither linear lr decay or step based decay will work. It would be better if the author can investigate when linear schedule starts to outperforms step based decay in terms of epochs or iterations. Since different batch size results in different number of iterations, I would expect the difference between two schedules for small batch size at the early stage of training would be less in comparison with large batch training, especially when the number of iterations is enough.\n\n- Different initial learning rate may also results in different behaviour. We often see some learning curves with larger initial learning rate converges faster at the beginning but yields similar generalization error at the end of training. On the other hand, the author only compared different schedules with single initial learning rate. Image when the initial learning rate is small, there would not be too much difference for different schedules. Actually the linear decay schedule changes may simply find a good learning rate during training as long as the initial learning rate is larger than the optimal one.\n\n\n[1] Dinh et al, Sharp minima can generalize for deep nets, ICML 2017\n[2] Li et al, Visualizing the Loss Landscape of Neural Nets, NIPS 2018\n[3] van Laarhoven, L2 regularization versus batch and weight normalization, NIPS 2017\n\n\n----- update after rebuttal ------\nThe authors' rebuttal addressed some of my concerns.  I think early stopping is still an important feature to save compute, especially for HPO, which could limit the usage of the proposed method (cannot stop earlier). If the authors believe practitioners are used to train model in full budget without early stopping to guarantee the best performance, then why would people try budged training with worse performance? I hope the authors could make the limitation clear. My major concern about the reason for why linear scaling schedule is better is still not clear, which makes the contribution kind of weak. Nevertheless,  the problem setting and the observations could be beneficial to the community for further discussion, So I raised my score to weak accept.\n\nminor: I see exactly the same variance values for step and linear methods int the top rows of Table 12, is this a mistake?\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"}, "signatures": ["ICLR.cc/2020/Conference/Paper856/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper856/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["mtli@cs.cmu.edu", "meyumer@gmail.com", "deva@cs.cmu.edu"], "title": "Budgeted Training: Rethinking Deep Neural Network Training Under Resource Constraints", "authors": ["Mengtian Li", "Ersin Yumer", "Deva Ramanan"], "pdf": "/pdf/bc6efbe44896472edd87187c87d89ea3d4b0ed22.pdf", "TL;DR": "Introduce a formal setting for budgeted training and propose a budget-aware linear learning rate schedule", "abstract": "In most practical settings and theoretical analyses, one assumes that a model can be trained until convergence. However, the growing complexity of machine learning datasets and models may violate such assumptions. Indeed, current approaches for hyper-parameter tuning and neural architecture search tend to be limited by practical resource constraints. Therefore, we introduce a formal setting for studying training under the non-asymptotic, resource-constrained regime, i.e., budgeted training. We analyze the following problem: \"given a dataset, algorithm, and fixed resource budget, what is the best achievable performance?\" We focus on the number of optimization iterations as the representative resource. Under such a setting, we show that it is critical to adjust the learning rate schedule according to the given budget. Among budget-aware learning schedules, we find simple linear decay to be both robust and high-performing. We support our claim through extensive experiments with state-of-the-art models on ImageNet (image classification), Kinetics (video classification), MS COCO (object detection and instance segmentation), and Cityscapes (semantic segmentation). We also analyze our results and find that the key to a good schedule is budgeted convergence, a phenomenon whereby the gradient vanishes at the end of each allowed budget. We also revisit existing approaches for fast convergence and show that budget-aware learning schedules readily outperform such approaches under (the practical but under-explored) budgeted training setting.", "keywords": ["budgeted training", "learning rate schedule", "linear schedule", "annealing", "learning rate decay"], "paperhash": "li|budgeted_training_rethinking_deep_neural_network_training_under_resource_constraints", "_bibtex": "@inproceedings{\nLi2020Budgeted,\ntitle={Budgeted Training: Rethinking Deep Neural Network Training Under Resource Constraints},\nauthor={Mengtian Li and Ersin Yumer and Deva Ramanan},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HyxLRTVKPH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/cd7f54b5f71631164abf7d4129285f849a388cd1.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HyxLRTVKPH", "replyto": "HyxLRTVKPH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper856/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper856/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575694110143, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper856/Reviewers"], "noninvitees": [], "tcdate": 1570237745992, "tmdate": 1575694110158, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper856/-/Official_Review"}}}, {"id": "S1xyJwsnsB", "original": null, "number": 4, "cdate": 1573856983447, "ddate": null, "tcdate": 1573856983447, "tmdate": 1573856983447, "tddate": null, "forum": "HyxLRTVKPH", "replyto": "HyxLRTVKPH", "invitation": "ICLR.cc/2020/Conference/Paper856/-/Official_Comment", "content": {"title": "Summary of changes for the revised version", "comment": "In the revision, the main text remains the same, and additional statistics and experiments are provided in the appendix. These additions are requested by the reviewers.\n\n- We added mean and std tables (Table 11 and 12) in Appendix I for our main results (Fig 2 and Table 2). In the main text, we report the median of 3 runs.\n- We added the weight norm plot in Fig 9 corresponding to Fig 3 (the gradient norm and the learning rate).\n- We plotted Fig 3 & 9 in the setting with weight decay disabled in Fig 10 & 11.\n- We added ablation studies about the batch size and the initial learning rate in Tab 13 & 14 respectively."}, "signatures": ["ICLR.cc/2020/Conference/Paper856/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper856/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["mtli@cs.cmu.edu", "meyumer@gmail.com", "deva@cs.cmu.edu"], "title": "Budgeted Training: Rethinking Deep Neural Network Training Under Resource Constraints", "authors": ["Mengtian Li", "Ersin Yumer", "Deva Ramanan"], "pdf": "/pdf/bc6efbe44896472edd87187c87d89ea3d4b0ed22.pdf", "TL;DR": "Introduce a formal setting for budgeted training and propose a budget-aware linear learning rate schedule", "abstract": "In most practical settings and theoretical analyses, one assumes that a model can be trained until convergence. However, the growing complexity of machine learning datasets and models may violate such assumptions. Indeed, current approaches for hyper-parameter tuning and neural architecture search tend to be limited by practical resource constraints. Therefore, we introduce a formal setting for studying training under the non-asymptotic, resource-constrained regime, i.e., budgeted training. We analyze the following problem: \"given a dataset, algorithm, and fixed resource budget, what is the best achievable performance?\" We focus on the number of optimization iterations as the representative resource. Under such a setting, we show that it is critical to adjust the learning rate schedule according to the given budget. Among budget-aware learning schedules, we find simple linear decay to be both robust and high-performing. We support our claim through extensive experiments with state-of-the-art models on ImageNet (image classification), Kinetics (video classification), MS COCO (object detection and instance segmentation), and Cityscapes (semantic segmentation). We also analyze our results and find that the key to a good schedule is budgeted convergence, a phenomenon whereby the gradient vanishes at the end of each allowed budget. We also revisit existing approaches for fast convergence and show that budget-aware learning schedules readily outperform such approaches under (the practical but under-explored) budgeted training setting.", "keywords": ["budgeted training", "learning rate schedule", "linear schedule", "annealing", "learning rate decay"], "paperhash": "li|budgeted_training_rethinking_deep_neural_network_training_under_resource_constraints", "_bibtex": "@inproceedings{\nLi2020Budgeted,\ntitle={Budgeted Training: Rethinking Deep Neural Network Training Under Resource Constraints},\nauthor={Mengtian Li and Ersin Yumer and Deva Ramanan},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HyxLRTVKPH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/cd7f54b5f71631164abf7d4129285f849a388cd1.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HyxLRTVKPH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper856/Authors", "ICLR.cc/2020/Conference/Paper856/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper856/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper856/Reviewers", "ICLR.cc/2020/Conference/Paper856/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper856/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper856/Authors|ICLR.cc/2020/Conference/Paper856/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504165152, "tmdate": 1576860560179, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper856/Authors", "ICLR.cc/2020/Conference/Paper856/Reviewers", "ICLR.cc/2020/Conference/Paper856/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper856/-/Official_Comment"}}}, {"id": "B1lG-xMcoB", "original": null, "number": 3, "cdate": 1573687289561, "ddate": null, "tcdate": 1573687289561, "tmdate": 1573687289561, "tddate": null, "forum": "HyxLRTVKPH", "replyto": "BygJ8bHctB", "invitation": "ICLR.cc/2020/Conference/Paper856/-/Official_Comment", "content": {"title": "Response for R3", "comment": "Thanks for reviewing our paper!\n\n1. The effects of a \u201cvanishing gradient\u201d may be more obvious in Fig 5 (right), where a small loss is achieved only when the gradient magnitude is small. We will work on better explanations and improve the clarity here.\n\n2. We agree that it would be more comprehensive to include algorithms like Adam. However, for most experiments, we focus on comparing budget-aware linear schedules with off-the-shelf defaults (which we assume have already been aggressively tuned in the state-of-the-art codebases for each of our benchmark tasks). For NAS, most of the papers we found adopt SGD. And for all practical vision benchmarks, momentum SGD is used. There is no particular reason not to include Adam except keeping our experiments to a manageable size. As we argue for R2, our evaluations are significantly larger scale than other papers in our peer group. Finally, we point out that we have included AMSGrad in Fig 4 & 6, which is designed to improve upon Adam itself.\n\n3. We have fixed these typos. Thanks for pointing out!\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper856/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper856/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["mtli@cs.cmu.edu", "meyumer@gmail.com", "deva@cs.cmu.edu"], "title": "Budgeted Training: Rethinking Deep Neural Network Training Under Resource Constraints", "authors": ["Mengtian Li", "Ersin Yumer", "Deva Ramanan"], "pdf": "/pdf/bc6efbe44896472edd87187c87d89ea3d4b0ed22.pdf", "TL;DR": "Introduce a formal setting for budgeted training and propose a budget-aware linear learning rate schedule", "abstract": "In most practical settings and theoretical analyses, one assumes that a model can be trained until convergence. However, the growing complexity of machine learning datasets and models may violate such assumptions. Indeed, current approaches for hyper-parameter tuning and neural architecture search tend to be limited by practical resource constraints. Therefore, we introduce a formal setting for studying training under the non-asymptotic, resource-constrained regime, i.e., budgeted training. We analyze the following problem: \"given a dataset, algorithm, and fixed resource budget, what is the best achievable performance?\" We focus on the number of optimization iterations as the representative resource. Under such a setting, we show that it is critical to adjust the learning rate schedule according to the given budget. Among budget-aware learning schedules, we find simple linear decay to be both robust and high-performing. We support our claim through extensive experiments with state-of-the-art models on ImageNet (image classification), Kinetics (video classification), MS COCO (object detection and instance segmentation), and Cityscapes (semantic segmentation). We also analyze our results and find that the key to a good schedule is budgeted convergence, a phenomenon whereby the gradient vanishes at the end of each allowed budget. We also revisit existing approaches for fast convergence and show that budget-aware learning schedules readily outperform such approaches under (the practical but under-explored) budgeted training setting.", "keywords": ["budgeted training", "learning rate schedule", "linear schedule", "annealing", "learning rate decay"], "paperhash": "li|budgeted_training_rethinking_deep_neural_network_training_under_resource_constraints", "_bibtex": "@inproceedings{\nLi2020Budgeted,\ntitle={Budgeted Training: Rethinking Deep Neural Network Training Under Resource Constraints},\nauthor={Mengtian Li and Ersin Yumer and Deva Ramanan},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HyxLRTVKPH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/cd7f54b5f71631164abf7d4129285f849a388cd1.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HyxLRTVKPH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper856/Authors", "ICLR.cc/2020/Conference/Paper856/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper856/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper856/Reviewers", "ICLR.cc/2020/Conference/Paper856/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper856/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper856/Authors|ICLR.cc/2020/Conference/Paper856/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504165152, "tmdate": 1576860560179, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper856/Authors", "ICLR.cc/2020/Conference/Paper856/Reviewers", "ICLR.cc/2020/Conference/Paper856/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper856/-/Official_Comment"}}}, {"id": "S1lJo1f9sB", "original": null, "number": 2, "cdate": 1573687190591, "ddate": null, "tcdate": 1573687190591, "tmdate": 1573687190591, "tddate": null, "forum": "HyxLRTVKPH", "replyto": "SyeXsBqpKS", "invitation": "ICLR.cc/2020/Conference/Paper856/-/Official_Comment", "content": {"title": "Response for R1", "comment": "Thanks for reviewing our paper!\n\nIn the deep learning era, it might often be the case that practice precedes theory. We hope that our budgeted setting allows new optimization algorithms to appear. Many existing optimizers focus on the goal of decreasing the training loss as fast as possible, which is not required in the budgeted setting. We are also open to any suggestions on how to develop a sound theory to explain the findings in this paper."}, "signatures": ["ICLR.cc/2020/Conference/Paper856/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper856/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["mtli@cs.cmu.edu", "meyumer@gmail.com", "deva@cs.cmu.edu"], "title": "Budgeted Training: Rethinking Deep Neural Network Training Under Resource Constraints", "authors": ["Mengtian Li", "Ersin Yumer", "Deva Ramanan"], "pdf": "/pdf/bc6efbe44896472edd87187c87d89ea3d4b0ed22.pdf", "TL;DR": "Introduce a formal setting for budgeted training and propose a budget-aware linear learning rate schedule", "abstract": "In most practical settings and theoretical analyses, one assumes that a model can be trained until convergence. However, the growing complexity of machine learning datasets and models may violate such assumptions. Indeed, current approaches for hyper-parameter tuning and neural architecture search tend to be limited by practical resource constraints. Therefore, we introduce a formal setting for studying training under the non-asymptotic, resource-constrained regime, i.e., budgeted training. We analyze the following problem: \"given a dataset, algorithm, and fixed resource budget, what is the best achievable performance?\" We focus on the number of optimization iterations as the representative resource. Under such a setting, we show that it is critical to adjust the learning rate schedule according to the given budget. Among budget-aware learning schedules, we find simple linear decay to be both robust and high-performing. We support our claim through extensive experiments with state-of-the-art models on ImageNet (image classification), Kinetics (video classification), MS COCO (object detection and instance segmentation), and Cityscapes (semantic segmentation). We also analyze our results and find that the key to a good schedule is budgeted convergence, a phenomenon whereby the gradient vanishes at the end of each allowed budget. We also revisit existing approaches for fast convergence and show that budget-aware learning schedules readily outperform such approaches under (the practical but under-explored) budgeted training setting.", "keywords": ["budgeted training", "learning rate schedule", "linear schedule", "annealing", "learning rate decay"], "paperhash": "li|budgeted_training_rethinking_deep_neural_network_training_under_resource_constraints", "_bibtex": "@inproceedings{\nLi2020Budgeted,\ntitle={Budgeted Training: Rethinking Deep Neural Network Training Under Resource Constraints},\nauthor={Mengtian Li and Ersin Yumer and Deva Ramanan},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HyxLRTVKPH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/cd7f54b5f71631164abf7d4129285f849a388cd1.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HyxLRTVKPH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper856/Authors", "ICLR.cc/2020/Conference/Paper856/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper856/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper856/Reviewers", "ICLR.cc/2020/Conference/Paper856/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper856/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper856/Authors|ICLR.cc/2020/Conference/Paper856/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504165152, "tmdate": 1576860560179, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper856/Authors", "ICLR.cc/2020/Conference/Paper856/Reviewers", "ICLR.cc/2020/Conference/Paper856/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper856/-/Official_Comment"}}}, {"id": "Hkx2U1zcir", "original": null, "number": 1, "cdate": 1573687124102, "ddate": null, "tcdate": 1573687124102, "tmdate": 1573687124102, "tddate": null, "forum": "HyxLRTVKPH", "replyto": "SJgRJmmbjB", "invitation": "ICLR.cc/2020/Conference/Paper856/-/Official_Comment", "content": {"title": "Response and additional experiments for R2", "comment": "Thanks for reviewing our paper!\n\nAs requested, we added mean and std tables (Table 11 and 12) in Appendix I. For reference, we previously reported the median in Fig 2 and Table 2. R2\u2019s first concern is full-budget training: we find that the improvement of linear decay over step decay is sometimes not statistically significant (e.g., both are nearly identical for ImagetNet training with the full-budget). However, at budget-constraints settings (our focus! - e.g., than 50% iterations), linear definitely and consistently outperforms step-decay.\n\nThe second concern regards the relative performance between linear, cosine and poly. Our primary empirical observation, illustrated in Fig 1, is that budget-aware, smoothly decaying schedules dramatically outperform other schedules in low-budget settings. We agree entirely that budget-aware poly and cosine curves, when plotted in Fig. 1, would look similar to linear. That is why we state in the second last paragraph on page 6:  \u201cusing cosine and poly will achieve similar performance as linear\u201d. Linear schedules are not our primary contribution; rather budget-aware training is. That said, we choose to emphasize linear as the practical \u201cgo-to\u201d budget-aware schedule because of its simplicity arising from the lack of parameters.\n\nWe also agree with R2 that having other learning rate schedules in Table 2 would be more informative. But we simply ran out of resources to do so in the rebuttal period (each individual run takes 1 week on a 4 or 8 GPU machine). We emphasize that most papers in our peer group analyze performance on CIFAR, and perhaps ImageNet. Our analysis on multiple large-scale vision benchmarks is unique, and (in our opinion), makes our results particularly relevant. But it also makes it difficult to perform exhaustive ablations. Instead, we make the assumption that state-of-the-art codebases for such benchmarks are already well-tuned, and we \u201cswap out\u201d default learning-rates holding all else fixed. That said, we posit that cosine, poly, and linear will behave similarly, and all outperform step decay.\n\nIt is true that our method cannot take advantage of early stopping. R2 is correct that, in theory, early stopping can be used to save computational resources. But our experience is that early stopping in practice, is not used to reduce compute (e.g., most practitioners still finish the full training run and apply early-stopping post-hoc for model selection to reduce overfitting).\n\nR3 points an alternative explanation to the decreasing gradient magnitude in Fig 3:  when weight decay and BN are used, the weight magnitude is also decreasing. We plotted more statistics in Appendix J to address this issue, including the requested no weight decay setting. First, for Fig 3-5, we plot the full gradient of the cross-entropy loss, excluding the regularization loss. For completeness, we plot the weight norm in Fig 9 (same setting with weight decay of 5e-4). We can see that there is a similar correlation between weight norm and learning rate. Note that here we swap out poly in Fig 3 with the SGDR in Fig 6 since poly exhibits similar trends as linear. To see the effect of weight decay, we re-run the experiments without weight decay and plot both the full gradient norm (of the cross-entropy loss only) and the weight norm in Fig 10 and Fig 11. We see the weight magnitude is increasing and there is still some though weaker correlation between the learning rate and the full gradient norm (most obviously in the SGDR plot). Therefore, our observation still holds and is consistent with the reference provided by R3. We hesitate in drawing a clear conclusion since we don\u2019t want to over-claim. However, we do believe this \u201cvanishing gradient\u201d phenomenon is interesting and worth studying.\n\nIn practice, the minimum budget we explore is 1 epoch. We find that linear outperforms step decay in this setting. R2 is correct that this may change for even smaller budgets, but such resource constraints seem artificially-limiting given the sizes of current datasets. For completeness, we added experiments of different batch sizes. The experiments suggest that R2\u2019s hypothesis is correct: the gap between the two schedules is smaller with a small batch size than with a large one.\n\nWe also added experiments about initial learning rates in Appendix K. We find the initial learning rate is a hyperparameter mostly orthogonal to the learning rate schedule studied in this paper. While our conclusion of linear schedule outperforming step decay still holds for different initial learning rates, the importance of initial learning rates still outweighs that of learning rate schedules. Specifically, It is not the case that the linear learning rate can work with larger initial learning rates.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper856/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper856/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["mtli@cs.cmu.edu", "meyumer@gmail.com", "deva@cs.cmu.edu"], "title": "Budgeted Training: Rethinking Deep Neural Network Training Under Resource Constraints", "authors": ["Mengtian Li", "Ersin Yumer", "Deva Ramanan"], "pdf": "/pdf/bc6efbe44896472edd87187c87d89ea3d4b0ed22.pdf", "TL;DR": "Introduce a formal setting for budgeted training and propose a budget-aware linear learning rate schedule", "abstract": "In most practical settings and theoretical analyses, one assumes that a model can be trained until convergence. However, the growing complexity of machine learning datasets and models may violate such assumptions. Indeed, current approaches for hyper-parameter tuning and neural architecture search tend to be limited by practical resource constraints. Therefore, we introduce a formal setting for studying training under the non-asymptotic, resource-constrained regime, i.e., budgeted training. We analyze the following problem: \"given a dataset, algorithm, and fixed resource budget, what is the best achievable performance?\" We focus on the number of optimization iterations as the representative resource. Under such a setting, we show that it is critical to adjust the learning rate schedule according to the given budget. Among budget-aware learning schedules, we find simple linear decay to be both robust and high-performing. We support our claim through extensive experiments with state-of-the-art models on ImageNet (image classification), Kinetics (video classification), MS COCO (object detection and instance segmentation), and Cityscapes (semantic segmentation). We also analyze our results and find that the key to a good schedule is budgeted convergence, a phenomenon whereby the gradient vanishes at the end of each allowed budget. We also revisit existing approaches for fast convergence and show that budget-aware learning schedules readily outperform such approaches under (the practical but under-explored) budgeted training setting.", "keywords": ["budgeted training", "learning rate schedule", "linear schedule", "annealing", "learning rate decay"], "paperhash": "li|budgeted_training_rethinking_deep_neural_network_training_under_resource_constraints", "_bibtex": "@inproceedings{\nLi2020Budgeted,\ntitle={Budgeted Training: Rethinking Deep Neural Network Training Under Resource Constraints},\nauthor={Mengtian Li and Ersin Yumer and Deva Ramanan},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HyxLRTVKPH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/cd7f54b5f71631164abf7d4129285f849a388cd1.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HyxLRTVKPH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper856/Authors", "ICLR.cc/2020/Conference/Paper856/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper856/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper856/Reviewers", "ICLR.cc/2020/Conference/Paper856/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper856/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper856/Authors|ICLR.cc/2020/Conference/Paper856/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504165152, "tmdate": 1576860560179, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper856/Authors", "ICLR.cc/2020/Conference/Paper856/Reviewers", "ICLR.cc/2020/Conference/Paper856/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper856/-/Official_Comment"}}}, {"id": "BygJ8bHctB", "original": null, "number": 1, "cdate": 1571602759105, "ddate": null, "tcdate": 1571602759105, "tmdate": 1572972543619, "tddate": null, "forum": "HyxLRTVKPH", "replyto": "HyxLRTVKPH", "invitation": "ICLR.cc/2020/Conference/Paper856/-/Official_Review", "content": {"rating": "6: Weak Accept", "experience_assessment": "I do not know much about this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This work presents a simple technique for tuning the learning rate for Neural Network training when under a \"budget\" -- the budget here is specified as a fixed number of epochs that is expected to be a small fraction of the total number of epochs required to achieve maximum accuracy. The main contribution of this paper is in showing that a simpler linear decay schedule that goes to zero at the end of the proposed budget achieves good performance. The paper proposes a framework called budget-aware schedule which represents any learning rate schedule where the ratio of learning rate at time `'t' base learning rate is only a function of the ratio of 't' to total budget 'T'. In this family of schedules, the paper shows that a simple linear decay works best for all budgets. In the appendix, the authors compare their proposed schedule with adaptive techniques and show that under a given budget, it outperforms latets adaptive techniques like adabound, amsgrad, etc.\n\nPros:\n1. This paper presents a simple technique for a problem that is impactful namely performing training under a small budget presumably as an approximation during neural architecture search or hyperparameter tuning. The technique is empirically shown to be effective for many computer vision benchmarks.\n2. The paper presents extensive experimental results comparing linear decay with other budget-aware schedules. The accuracy comparisons are performed under different budgets as well as for neural architecture ranking while selecting architecture with budgeted training.\n3. Overall, I think this paper can be generally useful for many practitioners.\n\nCons:\n1. The paper makes claims around the phenomena of gradient magnitude vanishing as well as its effectiveness. E.g. in section 5, authors state \"We call this \u201cvanishing gradient\u201d phenomenon budgeted convergence. This correlation suggests that decaying schedules to near-zero rates (and using BAC) may be more effective than early stopping.\". This is not clear from the paper as the paper merely shows gradient magnitude decreasing with learning rate. This claim appear like an overreach to me.\n2. The key motivating use cases for budget-aware training is providing approximations for problems like neural architecture search and hyper parameter tuning. However, for these use cases, the paper does not perform extensive comparisons for commonly used algorithms like Adam. Why?\n\nnits:\n1. In section 2, various -> varies\n2. Right above equation 1, budge -> budget\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper856/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper856/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["mtli@cs.cmu.edu", "meyumer@gmail.com", "deva@cs.cmu.edu"], "title": "Budgeted Training: Rethinking Deep Neural Network Training Under Resource Constraints", "authors": ["Mengtian Li", "Ersin Yumer", "Deva Ramanan"], "pdf": "/pdf/bc6efbe44896472edd87187c87d89ea3d4b0ed22.pdf", "TL;DR": "Introduce a formal setting for budgeted training and propose a budget-aware linear learning rate schedule", "abstract": "In most practical settings and theoretical analyses, one assumes that a model can be trained until convergence. However, the growing complexity of machine learning datasets and models may violate such assumptions. Indeed, current approaches for hyper-parameter tuning and neural architecture search tend to be limited by practical resource constraints. Therefore, we introduce a formal setting for studying training under the non-asymptotic, resource-constrained regime, i.e., budgeted training. We analyze the following problem: \"given a dataset, algorithm, and fixed resource budget, what is the best achievable performance?\" We focus on the number of optimization iterations as the representative resource. Under such a setting, we show that it is critical to adjust the learning rate schedule according to the given budget. Among budget-aware learning schedules, we find simple linear decay to be both robust and high-performing. We support our claim through extensive experiments with state-of-the-art models on ImageNet (image classification), Kinetics (video classification), MS COCO (object detection and instance segmentation), and Cityscapes (semantic segmentation). We also analyze our results and find that the key to a good schedule is budgeted convergence, a phenomenon whereby the gradient vanishes at the end of each allowed budget. We also revisit existing approaches for fast convergence and show that budget-aware learning schedules readily outperform such approaches under (the practical but under-explored) budgeted training setting.", "keywords": ["budgeted training", "learning rate schedule", "linear schedule", "annealing", "learning rate decay"], "paperhash": "li|budgeted_training_rethinking_deep_neural_network_training_under_resource_constraints", "_bibtex": "@inproceedings{\nLi2020Budgeted,\ntitle={Budgeted Training: Rethinking Deep Neural Network Training Under Resource Constraints},\nauthor={Mengtian Li and Ersin Yumer and Deva Ramanan},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HyxLRTVKPH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/cd7f54b5f71631164abf7d4129285f849a388cd1.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HyxLRTVKPH", "replyto": "HyxLRTVKPH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper856/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper856/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575694110143, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper856/Reviewers"], "noninvitees": [], "tcdate": 1570237745992, "tmdate": 1575694110158, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper856/-/Official_Review"}}}, {"id": "SyeXsBqpKS", "original": null, "number": 2, "cdate": 1571820954682, "ddate": null, "tcdate": 1571820954682, "tmdate": 1572972543584, "tddate": null, "forum": "HyxLRTVKPH", "replyto": "HyxLRTVKPH", "invitation": "ICLR.cc/2020/Conference/Paper856/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper analyzed which learning rate schedule (LRS) should be used when the budget (number of iteration) is limited. First, the authors have introduced the concept of BAS (Budget-Aware Schedule). Various LRSs are classified, and it is experimentally shown that the LRSs based on BAS performed better. Among them, the performance of the linear decay method was shown to be simple and robust.\n\nPros\n\n1. Formally define important and well-motivated issues to improve performance on a limited budget.\n2. Various experimental results show that the simple linear decay method works well, and it might become a baseline method for future budgeted training solutions (assuming there is no similar work with the same purpose).\n3. It seems to be easy to apply to the NAS,  and the experiments in Appendix A look a big plus. This section can be put into the main paper.\n\nCons\n\n1. No sound theory as to why linear decay or other smooth decay methods work well.\n2. As Mishkin et al. [1] have already experimented with linear decay, the novelty of the methodology proposed by the authors might be limited.\n\nWhile there is some concern regarding the significance of novelty,  the paper seems meaningful enough to be accepted.\n\n[1] Dmytro Mishkin, Nikolay Sergievskiy, and Jiri Matas. Systematic evaluation of convolution neural network advances on the imagenet. Computer Vision and Image Understanding, 161: 11\u201319, 2017."}, "signatures": ["ICLR.cc/2020/Conference/Paper856/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper856/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["mtli@cs.cmu.edu", "meyumer@gmail.com", "deva@cs.cmu.edu"], "title": "Budgeted Training: Rethinking Deep Neural Network Training Under Resource Constraints", "authors": ["Mengtian Li", "Ersin Yumer", "Deva Ramanan"], "pdf": "/pdf/bc6efbe44896472edd87187c87d89ea3d4b0ed22.pdf", "TL;DR": "Introduce a formal setting for budgeted training and propose a budget-aware linear learning rate schedule", "abstract": "In most practical settings and theoretical analyses, one assumes that a model can be trained until convergence. However, the growing complexity of machine learning datasets and models may violate such assumptions. Indeed, current approaches for hyper-parameter tuning and neural architecture search tend to be limited by practical resource constraints. Therefore, we introduce a formal setting for studying training under the non-asymptotic, resource-constrained regime, i.e., budgeted training. We analyze the following problem: \"given a dataset, algorithm, and fixed resource budget, what is the best achievable performance?\" We focus on the number of optimization iterations as the representative resource. Under such a setting, we show that it is critical to adjust the learning rate schedule according to the given budget. Among budget-aware learning schedules, we find simple linear decay to be both robust and high-performing. We support our claim through extensive experiments with state-of-the-art models on ImageNet (image classification), Kinetics (video classification), MS COCO (object detection and instance segmentation), and Cityscapes (semantic segmentation). We also analyze our results and find that the key to a good schedule is budgeted convergence, a phenomenon whereby the gradient vanishes at the end of each allowed budget. We also revisit existing approaches for fast convergence and show that budget-aware learning schedules readily outperform such approaches under (the practical but under-explored) budgeted training setting.", "keywords": ["budgeted training", "learning rate schedule", "linear schedule", "annealing", "learning rate decay"], "paperhash": "li|budgeted_training_rethinking_deep_neural_network_training_under_resource_constraints", "_bibtex": "@inproceedings{\nLi2020Budgeted,\ntitle={Budgeted Training: Rethinking Deep Neural Network Training Under Resource Constraints},\nauthor={Mengtian Li and Ersin Yumer and Deva Ramanan},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HyxLRTVKPH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/cd7f54b5f71631164abf7d4129285f849a388cd1.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HyxLRTVKPH", "replyto": "HyxLRTVKPH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper856/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper856/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575694110143, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper856/Reviewers"], "noninvitees": [], "tcdate": 1570237745992, "tmdate": 1575694110158, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper856/-/Official_Review"}}}], "count": 10}