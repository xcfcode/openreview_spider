{"notes": [{"tddate": null, "ddate": null, "cdate": null, "original": null, "tmdate": 1490028555013, "tcdate": 1490028555013, "number": 1, "id": "B1XMuFpjl", "invitation": "ICLR.cc/2017/workshop/-/paper19/acceptance", "forum": "H1PMaa1Yg", "replyto": "H1PMaa1Yg", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Exploring loss function topology with cyclical learning rates", "abstract": "We present observations and discussion of previously unreported phenomena discovered while training residual networks.  The goal of this work is to better understand the nature of neural networks through the examination of these new empirical results.   These behaviors were identified through the application of Cyclical Learning Rates (CLR)  and linear network interpolation.  Among these behaviors are counterintuitive increases and decreases in training loss and instances of rapid training.   For example, we demonstrate how CLR can produce greater testing accuracy than traditional training despite using large learning rates. ", "pdf": "/pdf/33317e3020e8039bee78093e64f8c13ecd62b2b3.pdf", "TL;DR": "We present empirical observations and discussion of unusual phenomena discovered while training Residual networks.", "paperhash": "smith|exploring_loss_function_topology_with_cyclical_learning_rates", "keywords": ["Deep learning"], "conflicts": ["nrl.navy.mil", "umbc.edu"], "authors": ["Leslie N. Smith", "Nicholay Topin"], "authorids": ["leslie.smith@nrl.navy.mil", "ntopin1@umbc.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1490028555565, "id": "ICLR.cc/2017/workshop/-/paper19/acceptance", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "H1PMaa1Yg", "replyto": "H1PMaa1Yg", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept", "Reject"]}}}, "nonreaders": [], "cdate": 1490028555565}}}, {"tddate": null, "tmdate": 1489426384527, "tcdate": 1489426384527, "number": 1, "id": "HJ_Rv84jx", "invitation": "ICLR.cc/2017/workshop/-/paper19/public/comment", "forum": "H1PMaa1Yg", "replyto": "Hku-l_lse", "signatures": ["~Leslie_N_Smith1"], "readers": ["everyone"], "writers": ["~Leslie_N_Smith1"], "content": {"title": "Reply to AnonReviewers", "comment": "We believe the intent of the ICLR workshop is to provide a forum for late-breaking results, even if a paper hasn't been fully developed into what one expects for a conference paper.  Our workshop paper is such a paper, providing experimental results that have not been seen before, though it isn't as fully developed as a conference paper.\n\nUnfortunately, the 3-page limit meant not showing the control experiments and many of the other results we've obtained.  We ran experiments with a variety of learning rate schedules, architectures, solvers, and hyper-parameters. We mentioned some of the other results very briefly in our conclusion but could not include them fully due to space limitations.\n\nThe original cyclical learning rate paper (Smith 2015, Smith 2017) discusses that the current scheme was compared with many other cyclic methods and the linear scheme was chosen because the more complex methods provided no additional benefit.  Please skim the earlier paper for more details.  The purpose of this current paper was not to introduce cyclical learning rate as a practical tool but to show it is also an experimental tool that demonstrates the new phenomena described.\n\nRegarding Figure 2a, some simple explanations are possible, and there are certainly many examples in the literature where SGD becomes unstable and diverges.  However, to our knowledge, the literature does not show examples where SGD becomes unstable, diverges, and then starts converging (note that the test accuracy falls slightly and recovers quickly), especially while the learning rate continues to increase.  This is why we include this as a novel phenomenon.  Furthermore, from a geometric perspective, one can imagine that the increasing learning rate causes the solution to jump out of a local minimum and hence the sudden jump but, if so, why would it continue to converge while learning rate increases?  We believe these phenomena are unusual and are providing some insight into the loss function topology.\n\nIn addition, Figure 1 shows the plots that started our investigation and we don't think your explanation holds for this example. These plots show test accuracy during regular training (not using cyclical learning rates), so the learning rate is monotonically decreasing. Furthermore, the dip in test accuracy happens for an initial learning rate of 0.14 but not for 0.24 or 0.35.\n\nRegarding Figure 2b, it does show the cyclical learning rate result compared to an optimal monotonically decreasing schedule.  The point is that within 20,000 iterations it produced a better solution than the optimal schedule could in 80,000 - 100,000 iterations.  We also feel it is interesting that such high performance is possible when the smallest value used for the learning rate is 0.1, which is commonly considered large.\n\nAs we say in the Conclusions, we are actively searching for a collaborator who can provide a theoretical analysis for a full follow-up paper. We welcome any readers who feel they understand the theoretical causes for these phenomena to please contact me.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Exploring loss function topology with cyclical learning rates", "abstract": "We present observations and discussion of previously unreported phenomena discovered while training residual networks.  The goal of this work is to better understand the nature of neural networks through the examination of these new empirical results.   These behaviors were identified through the application of Cyclical Learning Rates (CLR)  and linear network interpolation.  Among these behaviors are counterintuitive increases and decreases in training loss and instances of rapid training.   For example, we demonstrate how CLR can produce greater testing accuracy than traditional training despite using large learning rates. ", "pdf": "/pdf/33317e3020e8039bee78093e64f8c13ecd62b2b3.pdf", "TL;DR": "We present empirical observations and discussion of unusual phenomena discovered while training Residual networks.", "paperhash": "smith|exploring_loss_function_topology_with_cyclical_learning_rates", "keywords": ["Deep learning"], "conflicts": ["nrl.navy.mil", "umbc.edu"], "authors": ["Leslie N. Smith", "Nicholay Topin"], "authorids": ["leslie.smith@nrl.navy.mil", "ntopin1@umbc.edu"]}, "tags": [], "invitation": {"tddate": null, "tmdate": 1487031567986, "tcdate": 1487031567986, "id": "ICLR.cc/2017/workshop/-/paper19/public/comment", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2017/workshop/paper19/reviewers"], "reply": {"forum": "H1PMaa1Yg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/workshop/reviewers", "ICLR.cc/2017/pcs"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1487031567986}}}, {"tddate": null, "tmdate": 1489170432487, "tcdate": 1489170432487, "number": 2, "id": "Hku-l_lse", "invitation": "ICLR.cc/2017/workshop/-/paper19/official/review", "forum": "H1PMaa1Yg", "replyto": "H1PMaa1Yg", "signatures": ["ICLR.cc/2017/workshop/paper19/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper19/AnonReviewer1"], "content": {"title": "Interesting phenomena, but more experiments needed to rule out less interesting explanations", "rating": "4: Ok but not good enough - rejection", "review": "This paper discusses several interesting phenomena regarding the training and testing error curves over the course of training deep network models on image classification tasks. Among the findings are that test error performance can be nonmonotonic with certain learning rates, and imposing a cyclic alternation between low and high learning rates can speed learning. \n\n-While these results may point to something deeper, additional control experiments would greatly strengthen the paper. The finding that a cyclic learning schedule can speed learning would be potentially of practical interest, but the experiments compare just one particular cyclic scheme to one particular fixed learning rate. Does a carefully optimized fixed learning rate match the cyclic performance? Is a cycle really necessary, or can the learning rate just decrease monotonically over the course of learning?\n\n-There may be simple standard explanations of these phenomena. The test error spikes up on each cycle as the learning rate crosses some threshold, which seems a straightforward case of SGD becoming unstable and diverging when the learning rate is made too high. After taking a giant bad step, higher learning rates can make progress because the network is terrible and fine adjustments are not necessary. More is necessary to back up the claim that these results provide insight into the \"loss function topology.\"\n\n+The finding of faster convergence with cyclic learning rate schedules, if it remains faster than the optimal fixed or monotonically decreasing schedule, would be very interesting and merits more investigation.\n\n+The suggestion of interpolating many models to yield higher generalization performance is also a potentially interesting direction. ", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Exploring loss function topology with cyclical learning rates", "abstract": "We present observations and discussion of previously unreported phenomena discovered while training residual networks.  The goal of this work is to better understand the nature of neural networks through the examination of these new empirical results.   These behaviors were identified through the application of Cyclical Learning Rates (CLR)  and linear network interpolation.  Among these behaviors are counterintuitive increases and decreases in training loss and instances of rapid training.   For example, we demonstrate how CLR can produce greater testing accuracy than traditional training despite using large learning rates. ", "pdf": "/pdf/33317e3020e8039bee78093e64f8c13ecd62b2b3.pdf", "TL;DR": "We present empirical observations and discussion of unusual phenomena discovered while training Residual networks.", "paperhash": "smith|exploring_loss_function_topology_with_cyclical_learning_rates", "keywords": ["Deep learning"], "conflicts": ["nrl.navy.mil", "umbc.edu"], "authors": ["Leslie N. Smith", "Nicholay Topin"], "authorids": ["leslie.smith@nrl.navy.mil", "ntopin1@umbc.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489170433307, "id": "ICLR.cc/2017/workshop/-/paper19/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper19/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper19/AnonReviewer2", "ICLR.cc/2017/workshop/paper19/AnonReviewer1"], "reply": {"forum": "H1PMaa1Yg", "replyto": "H1PMaa1Yg", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper19/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper19/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489170433307}}}, {"tddate": null, "tmdate": 1488916370729, "tcdate": 1488916370729, "number": 1, "id": "S1i9J52cx", "invitation": "ICLR.cc/2017/workshop/-/paper19/official/review", "forum": "H1PMaa1Yg", "replyto": "H1PMaa1Yg", "signatures": ["ICLR.cc/2017/workshop/paper19/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper19/AnonReviewer2"], "content": {"title": "Official Review", "rating": "4: Ok but not good enough - rejection", "review": "This work presents a series of observations gleaned from training a ResNet at different learning rates and schedules. While in general this sort of empirical analysis is a good thing, the paper does not put forward any novel explanation or theory based on these observations. Overall the paper is reasonably well written but lacks clear motivation or take-aways. The techniques in this paper are not novel but the analysis is interesting. I would recommend rejection at this time but encourage the authors to see if they can further explore possible insights their experiments may have uncovered. ", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Exploring loss function topology with cyclical learning rates", "abstract": "We present observations and discussion of previously unreported phenomena discovered while training residual networks.  The goal of this work is to better understand the nature of neural networks through the examination of these new empirical results.   These behaviors were identified through the application of Cyclical Learning Rates (CLR)  and linear network interpolation.  Among these behaviors are counterintuitive increases and decreases in training loss and instances of rapid training.   For example, we demonstrate how CLR can produce greater testing accuracy than traditional training despite using large learning rates. ", "pdf": "/pdf/33317e3020e8039bee78093e64f8c13ecd62b2b3.pdf", "TL;DR": "We present empirical observations and discussion of unusual phenomena discovered while training Residual networks.", "paperhash": "smith|exploring_loss_function_topology_with_cyclical_learning_rates", "keywords": ["Deep learning"], "conflicts": ["nrl.navy.mil", "umbc.edu"], "authors": ["Leslie N. Smith", "Nicholay Topin"], "authorids": ["leslie.smith@nrl.navy.mil", "ntopin1@umbc.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489170433307, "id": "ICLR.cc/2017/workshop/-/paper19/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper19/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper19/AnonReviewer2", "ICLR.cc/2017/workshop/paper19/AnonReviewer1"], "reply": {"forum": "H1PMaa1Yg", "replyto": "H1PMaa1Yg", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper19/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper19/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489170433307}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1487248649811, "tcdate": 1487031567164, "number": 19, "id": "H1PMaa1Yg", "invitation": "ICLR.cc/2017/workshop/-/submission", "forum": "H1PMaa1Yg", "signatures": ["~Leslie_N_Smith1"], "readers": ["everyone"], "content": {"title": "Exploring loss function topology with cyclical learning rates", "abstract": "We present observations and discussion of previously unreported phenomena discovered while training residual networks.  The goal of this work is to better understand the nature of neural networks through the examination of these new empirical results.   These behaviors were identified through the application of Cyclical Learning Rates (CLR)  and linear network interpolation.  Among these behaviors are counterintuitive increases and decreases in training loss and instances of rapid training.   For example, we demonstrate how CLR can produce greater testing accuracy than traditional training despite using large learning rates. ", "pdf": "/pdf/33317e3020e8039bee78093e64f8c13ecd62b2b3.pdf", "TL;DR": "We present empirical observations and discussion of unusual phenomena discovered while training Residual networks.", "paperhash": "smith|exploring_loss_function_topology_with_cyclical_learning_rates", "keywords": ["Deep learning"], "conflicts": ["nrl.navy.mil", "umbc.edu"], "authors": ["Leslie N. Smith", "Nicholay Topin"], "authorids": ["leslie.smith@nrl.navy.mil", "ntopin1@umbc.edu"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1487690420000, "tmdate": 1484242559574, "id": "ICLR.cc/2017/workshop/-/submission", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1495466420000, "cdate": 1484242559574}}}], "count": 5}