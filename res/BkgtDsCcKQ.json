{"notes": [{"id": "BkgtDsCcKQ", "original": "B1lB6lvcK7", "number": 282, "cdate": 1538087776871, "ddate": null, "tcdate": 1538087776871, "tmdate": 1550482104109, "tddate": null, "forum": "BkgtDsCcKQ", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Function Space Particle Optimization for Bayesian Neural Networks", "abstract": "While Bayesian neural networks (BNNs) have drawn increasing attention, their posterior inference remains challenging, due to the high-dimensional and over-parameterized nature. To address this issue, several highly flexible and scalable variational inference procedures based on the idea of particle optimization have been proposed. These methods directly optimize a set of particles to approximate the target posterior. However, their application to BNNs often yields sub-optimal performance, as such methods have a particular failure mode on over-parameterized models. In this paper, we propose to solve this issue by performing particle optimization directly in the space of regression functions. We demonstrate through extensive experiments that our method successfully overcomes this issue, and outperforms strong baselines in a variety of tasks including prediction, defense against adversarial examples, and reinforcement learning.", "keywords": ["Bayesian neural networks", "uncertainty estimation", "variational inference"], "authorids": ["wzy196@gmail.com", "rtz19970824@gmail.com", "dcszj@mail.tsinghua.edu.cn", "dcszb@mail.tsinghua.edu.cn"], "authors": ["Ziyu Wang", "Tongzheng Ren", "Jun Zhu", "Bo Zhang"], "pdf": "/pdf/fef83ae6842b76284e2de4d04977d1575d0efdf4.pdf", "paperhash": "wang|function_space_particle_optimization_for_bayesian_neural_networks", "_bibtex": "@inproceedings{\nwang2018function,\ntitle={Function Space Particle Optimization for Bayesian Neural Networks},\nauthor={Ziyu Wang and Tongzheng Ren and Jun Zhu and Bo Zhang},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BkgtDsCcKQ},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 16, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "B1gdhGdvME", "original": null, "number": 12, "cdate": 1547301551954, "ddate": null, "tcdate": 1547301551954, "tmdate": 1547301551954, "tddate": null, "forum": "BkgtDsCcKQ", "replyto": "S1epmLv8z4", "invitation": "ICLR.cc/2019/Conference/-/Paper282/Official_Comment", "content": {"title": "Thanks for the pointers", "comment": "Hi Roman,\n\nThanks for the pointers! I just went through these papers and, indeed, it was an enjoyable read. We will add citations in the camera-ready version. \n\nBest,\nZiyu Wang"}, "signatures": ["ICLR.cc/2019/Conference/Paper282/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper282/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper282/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Function Space Particle Optimization for Bayesian Neural Networks", "abstract": "While Bayesian neural networks (BNNs) have drawn increasing attention, their posterior inference remains challenging, due to the high-dimensional and over-parameterized nature. To address this issue, several highly flexible and scalable variational inference procedures based on the idea of particle optimization have been proposed. These methods directly optimize a set of particles to approximate the target posterior. However, their application to BNNs often yields sub-optimal performance, as such methods have a particular failure mode on over-parameterized models. In this paper, we propose to solve this issue by performing particle optimization directly in the space of regression functions. We demonstrate through extensive experiments that our method successfully overcomes this issue, and outperforms strong baselines in a variety of tasks including prediction, defense against adversarial examples, and reinforcement learning.", "keywords": ["Bayesian neural networks", "uncertainty estimation", "variational inference"], "authorids": ["wzy196@gmail.com", "rtz19970824@gmail.com", "dcszj@mail.tsinghua.edu.cn", "dcszb@mail.tsinghua.edu.cn"], "authors": ["Ziyu Wang", "Tongzheng Ren", "Jun Zhu", "Bo Zhang"], "pdf": "/pdf/fef83ae6842b76284e2de4d04977d1575d0efdf4.pdf", "paperhash": "wang|function_space_particle_optimization_for_bayesian_neural_networks", "_bibtex": "@inproceedings{\nwang2018function,\ntitle={Function Space Particle Optimization for Bayesian Neural Networks},\nauthor={Ziyu Wang and Tongzheng Ren and Jun Zhu and Bo Zhang},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BkgtDsCcKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper282/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621616180, "tddate": null, "super": null, "final": null, "reply": {"forum": "BkgtDsCcKQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper282/Authors", "ICLR.cc/2019/Conference/Paper282/Reviewers", "ICLR.cc/2019/Conference/Paper282/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper282/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper282/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper282/Authors|ICLR.cc/2019/Conference/Paper282/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper282/Reviewers", "ICLR.cc/2019/Conference/Paper282/Authors", "ICLR.cc/2019/Conference/Paper282/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621616180}}}, {"id": "S1epmLv8z4", "original": null, "number": 1, "cdate": 1547232805326, "ddate": null, "tcdate": 1547232805326, "tmdate": 1547232875248, "tddate": null, "forum": "BkgtDsCcKQ", "replyto": "BkgtDsCcKQ", "invitation": "ICLR.cc/2019/Conference/-/Paper282/Public_Comment", "content": {"comment": "Dear Colleagues,\n\nCongratulations on having your impressive work accepted! \n\nWe similarly consider BNNs to be a very important and exciting field of study, and investigate them in the wide regime, where equivalence to the GP prior arises. As you reference this line of work in section 3.2, we would like to point out relevant concurrent work:\n\n1) https://openreview.net/forum?id=B1EA-M-0Z derived the NN-GP correspondence concurrently with Matthews et al, 2018, and\n2) https://openreview.net/forum?id=B1g30j0qF7 similarly derived the CNN-GP correspondence concurrently with Garriga-Alonso et al, 2018.\n\nIn addition to being concurrent derivations, both works provide a lot of complementary findings. I hope you find these references useful!\n\nBest,\nRoman.", "title": "On function-space priors for wide BNNs"}, "signatures": ["~Roman_Novak2"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Roman_Novak2", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Function Space Particle Optimization for Bayesian Neural Networks", "abstract": "While Bayesian neural networks (BNNs) have drawn increasing attention, their posterior inference remains challenging, due to the high-dimensional and over-parameterized nature. To address this issue, several highly flexible and scalable variational inference procedures based on the idea of particle optimization have been proposed. These methods directly optimize a set of particles to approximate the target posterior. However, their application to BNNs often yields sub-optimal performance, as such methods have a particular failure mode on over-parameterized models. In this paper, we propose to solve this issue by performing particle optimization directly in the space of regression functions. We demonstrate through extensive experiments that our method successfully overcomes this issue, and outperforms strong baselines in a variety of tasks including prediction, defense against adversarial examples, and reinforcement learning.", "keywords": ["Bayesian neural networks", "uncertainty estimation", "variational inference"], "authorids": ["wzy196@gmail.com", "rtz19970824@gmail.com", "dcszj@mail.tsinghua.edu.cn", "dcszb@mail.tsinghua.edu.cn"], "authors": ["Ziyu Wang", "Tongzheng Ren", "Jun Zhu", "Bo Zhang"], "pdf": "/pdf/fef83ae6842b76284e2de4d04977d1575d0efdf4.pdf", "paperhash": "wang|function_space_particle_optimization_for_bayesian_neural_networks", "_bibtex": "@inproceedings{\nwang2018function,\ntitle={Function Space Particle Optimization for Bayesian Neural Networks},\nauthor={Ziyu Wang and Tongzheng Ren and Jun Zhu and Bo Zhang},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BkgtDsCcKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper282/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311876508, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "BkgtDsCcKQ", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper282/Authors", "ICLR.cc/2019/Conference/Paper282/Reviewers", "ICLR.cc/2019/Conference/Paper282/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper282/Authors", "ICLR.cc/2019/Conference/Paper282/Reviewers", "ICLR.cc/2019/Conference/Paper282/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311876508}}}, {"id": "B1xOH5nWe4", "original": null, "number": 1, "cdate": 1544829503715, "ddate": null, "tcdate": 1544829503715, "tmdate": 1545354530556, "tddate": null, "forum": "BkgtDsCcKQ", "replyto": "BkgtDsCcKQ", "invitation": "ICLR.cc/2019/Conference/-/Paper282/Meta_Review", "content": {"metareview": "Reviewers are in a consensus and recommended to accept after engaging with the authors. Please take reviewers' comments into consideration to improve your submission for the camera ready.", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Accept (Poster)", "title": "Paper decision"}, "signatures": ["ICLR.cc/2019/Conference/Paper282/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper282/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Function Space Particle Optimization for Bayesian Neural Networks", "abstract": "While Bayesian neural networks (BNNs) have drawn increasing attention, their posterior inference remains challenging, due to the high-dimensional and over-parameterized nature. To address this issue, several highly flexible and scalable variational inference procedures based on the idea of particle optimization have been proposed. These methods directly optimize a set of particles to approximate the target posterior. However, their application to BNNs often yields sub-optimal performance, as such methods have a particular failure mode on over-parameterized models. In this paper, we propose to solve this issue by performing particle optimization directly in the space of regression functions. We demonstrate through extensive experiments that our method successfully overcomes this issue, and outperforms strong baselines in a variety of tasks including prediction, defense against adversarial examples, and reinforcement learning.", "keywords": ["Bayesian neural networks", "uncertainty estimation", "variational inference"], "authorids": ["wzy196@gmail.com", "rtz19970824@gmail.com", "dcszj@mail.tsinghua.edu.cn", "dcszb@mail.tsinghua.edu.cn"], "authors": ["Ziyu Wang", "Tongzheng Ren", "Jun Zhu", "Bo Zhang"], "pdf": "/pdf/fef83ae6842b76284e2de4d04977d1575d0efdf4.pdf", "paperhash": "wang|function_space_particle_optimization_for_bayesian_neural_networks", "_bibtex": "@inproceedings{\nwang2018function,\ntitle={Function Space Particle Optimization for Bayesian Neural Networks},\nauthor={Ziyu Wang and Tongzheng Ren and Jun Zhu and Bo Zhang},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BkgtDsCcKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper282/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353271649, "tddate": null, "super": null, "final": null, "reply": {"forum": "BkgtDsCcKQ", "replyto": "BkgtDsCcKQ", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper282/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper282/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper282/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353271649}}}, {"id": "SJgtpDLf1E", "original": null, "number": 11, "cdate": 1543821249143, "ddate": null, "tcdate": 1543821249143, "tmdate": 1543821249143, "tddate": null, "forum": "BkgtDsCcKQ", "replyto": "ByeUSNVqCX", "invitation": "ICLR.cc/2019/Conference/-/Paper282/Official_Comment", "content": {"title": "Thanks!", "comment": "Thanks for acknowledging our contribution and revising the rating."}, "signatures": ["ICLR.cc/2019/Conference/Paper282/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper282/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper282/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Function Space Particle Optimization for Bayesian Neural Networks", "abstract": "While Bayesian neural networks (BNNs) have drawn increasing attention, their posterior inference remains challenging, due to the high-dimensional and over-parameterized nature. To address this issue, several highly flexible and scalable variational inference procedures based on the idea of particle optimization have been proposed. These methods directly optimize a set of particles to approximate the target posterior. However, their application to BNNs often yields sub-optimal performance, as such methods have a particular failure mode on over-parameterized models. In this paper, we propose to solve this issue by performing particle optimization directly in the space of regression functions. We demonstrate through extensive experiments that our method successfully overcomes this issue, and outperforms strong baselines in a variety of tasks including prediction, defense against adversarial examples, and reinforcement learning.", "keywords": ["Bayesian neural networks", "uncertainty estimation", "variational inference"], "authorids": ["wzy196@gmail.com", "rtz19970824@gmail.com", "dcszj@mail.tsinghua.edu.cn", "dcszb@mail.tsinghua.edu.cn"], "authors": ["Ziyu Wang", "Tongzheng Ren", "Jun Zhu", "Bo Zhang"], "pdf": "/pdf/fef83ae6842b76284e2de4d04977d1575d0efdf4.pdf", "paperhash": "wang|function_space_particle_optimization_for_bayesian_neural_networks", "_bibtex": "@inproceedings{\nwang2018function,\ntitle={Function Space Particle Optimization for Bayesian Neural Networks},\nauthor={Ziyu Wang and Tongzheng Ren and Jun Zhu and Bo Zhang},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BkgtDsCcKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper282/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621616180, "tddate": null, "super": null, "final": null, "reply": {"forum": "BkgtDsCcKQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper282/Authors", "ICLR.cc/2019/Conference/Paper282/Reviewers", "ICLR.cc/2019/Conference/Paper282/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper282/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper282/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper282/Authors|ICLR.cc/2019/Conference/Paper282/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper282/Reviewers", "ICLR.cc/2019/Conference/Paper282/Authors", "ICLR.cc/2019/Conference/Paper282/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621616180}}}, {"id": "HJlcUPIfkV", "original": null, "number": 10, "cdate": 1543821137614, "ddate": null, "tcdate": 1543821137614, "tmdate": 1543821137614, "tddate": null, "forum": "BkgtDsCcKQ", "replyto": "HJlsKYptRX", "invitation": "ICLR.cc/2019/Conference/-/Paper282/Official_Comment", "content": {"title": "Thanks & Response", "comment": "Thank you for acknowledging our contributions, revising the ratings and the nice comments. We appreciate that. Below, we briefly address the extra questions.\n\nQ1: Regarding the impact of B' on performance, and values we have used in the experiments\nA1: We will make them clearer in the final version. In fact, (1) we did not tune B' thoroughly, and only experimented with B'<=B/2; inside this range, increasing B' improves predictive performance, although improvement becomes marginal when B' is large. E.g. on the Concrete dataset in Section 5.2.1, setting B'=100 improves the average NLL by 0.04 compared to B'=10, and by 0.08 compared to B'=0; the standard deviation of NLL on this dataset is 0.04.\n(2) In the synthetic experiment in Section 5.1, varying B' does not have a significant impact on the quality of uncertainty estimation. A possible explanation is that the smoothness constraint encoded in the function-space prior \"propagates\" uncertainty in q[f(X_{train})] to q[f(x)] for nearby x.\n(3) The value of B-B' is given in the text. For all experiments on feed-forward networks, we have set B' to min(100, B/2). This value is determined by grid search (in {1, 10, 100}) on a UCI regression dataset. In the ResNet experiment we used B'=4, and we did not experiment with other values.\n\nQ2: On downsampling f(x) in classification\nA2: We apologize that the phrase \"for a single data point x\" might be misleading here. We will make it clearer in the final version. In fact, denote the batch size (used for prior estimation) as B and number of classes as C, then without down-sampling, we will need to approximate the prior distribution of a B*C dimensional vector, the concatenated function values for all data points in the batch. This is high-dimensional compared to the 1d-regression case, where the dimension of the concatenated function values is B.\nSo we choose to down-sample the indices of this vector, i.e. to down-sample the set of classes for which we will take the corresponding logits, concatenate them for all data points in a batch, and estimate the prior distribution. It is a type of stochastic approximation similar to mini-batching, and is not necessary: alternatively we can use a smaller B.\n\nQ3-i: On the accuracy on clean data in the adversarial robustness experiment:\nA3-i: We will clarify this in more detail in the final version. As widely observed in the literature (e.g., Liao et al., 2017), it is common to sacrifice some (often tiny) performance on clean samples in order to obtain significant improvement in adversarial robustness. For BNNs, under certain priors, the posterior mean estimate could be sub-optimal on iid test samples, compared to an ensemble of MAP estimates; and it is possible the prior we used in the ResNet experiment falls into this case. However, such a prior can still be useful for adversarial defense, because, as we have reviewed in Section 5.3, the latter task involves prediction on uncertain inputs. As we focused on adversarial robustness, we did not adjust the prior specifically to optimize accuracy on clean data, and used the Gaussian prior corresponding to the original L2 regularizer in ResNet instead. We leave the search of a more sensible prior as future work.\n\nQ3-ii: On the possibility to adjust \\mu for robustness applications:\nA3-ii: Thanks for the suggestion. We agree that it will be an interesting direction of future work. E.g. we could add to \\mu a component that focuses on the regions of adversarial examples, similar to what we proposed for domain adaptation; and to improve performance on general CV tasks, in KDE we could specify better kernels than isotropic Gaussian kernels. It is possible that a careful adaptation of our method could further improve adversarial robustness, or performance on general CV tasks."}, "signatures": ["ICLR.cc/2019/Conference/Paper282/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper282/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper282/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Function Space Particle Optimization for Bayesian Neural Networks", "abstract": "While Bayesian neural networks (BNNs) have drawn increasing attention, their posterior inference remains challenging, due to the high-dimensional and over-parameterized nature. To address this issue, several highly flexible and scalable variational inference procedures based on the idea of particle optimization have been proposed. These methods directly optimize a set of particles to approximate the target posterior. However, their application to BNNs often yields sub-optimal performance, as such methods have a particular failure mode on over-parameterized models. In this paper, we propose to solve this issue by performing particle optimization directly in the space of regression functions. We demonstrate through extensive experiments that our method successfully overcomes this issue, and outperforms strong baselines in a variety of tasks including prediction, defense against adversarial examples, and reinforcement learning.", "keywords": ["Bayesian neural networks", "uncertainty estimation", "variational inference"], "authorids": ["wzy196@gmail.com", "rtz19970824@gmail.com", "dcszj@mail.tsinghua.edu.cn", "dcszb@mail.tsinghua.edu.cn"], "authors": ["Ziyu Wang", "Tongzheng Ren", "Jun Zhu", "Bo Zhang"], "pdf": "/pdf/fef83ae6842b76284e2de4d04977d1575d0efdf4.pdf", "paperhash": "wang|function_space_particle_optimization_for_bayesian_neural_networks", "_bibtex": "@inproceedings{\nwang2018function,\ntitle={Function Space Particle Optimization for Bayesian Neural Networks},\nauthor={Ziyu Wang and Tongzheng Ren and Jun Zhu and Bo Zhang},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BkgtDsCcKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper282/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621616180, "tddate": null, "super": null, "final": null, "reply": {"forum": "BkgtDsCcKQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper282/Authors", "ICLR.cc/2019/Conference/Paper282/Reviewers", "ICLR.cc/2019/Conference/Paper282/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper282/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper282/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper282/Authors|ICLR.cc/2019/Conference/Paper282/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper282/Reviewers", "ICLR.cc/2019/Conference/Paper282/Authors", "ICLR.cc/2019/Conference/Paper282/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621616180}}}, {"id": "ByeUSNVqCX", "original": null, "number": 9, "cdate": 1543287870358, "ddate": null, "tcdate": 1543287870358, "tmdate": 1543287870358, "tddate": null, "forum": "BkgtDsCcKQ", "replyto": "SJlJ9HiQCQ", "invitation": "ICLR.cc/2019/Conference/-/Paper282/Official_Comment", "content": {"title": "Response to the revision", "comment": "Thank you for the detailed response. The clarity of the paper has been improved significantly and all my questions have been reasonably addressed. I have upgraded my rating accordingly."}, "signatures": ["ICLR.cc/2019/Conference/Paper282/AnonReviewer3"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper282/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper282/AnonReviewer3", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Function Space Particle Optimization for Bayesian Neural Networks", "abstract": "While Bayesian neural networks (BNNs) have drawn increasing attention, their posterior inference remains challenging, due to the high-dimensional and over-parameterized nature. To address this issue, several highly flexible and scalable variational inference procedures based on the idea of particle optimization have been proposed. These methods directly optimize a set of particles to approximate the target posterior. However, their application to BNNs often yields sub-optimal performance, as such methods have a particular failure mode on over-parameterized models. In this paper, we propose to solve this issue by performing particle optimization directly in the space of regression functions. We demonstrate through extensive experiments that our method successfully overcomes this issue, and outperforms strong baselines in a variety of tasks including prediction, defense against adversarial examples, and reinforcement learning.", "keywords": ["Bayesian neural networks", "uncertainty estimation", "variational inference"], "authorids": ["wzy196@gmail.com", "rtz19970824@gmail.com", "dcszj@mail.tsinghua.edu.cn", "dcszb@mail.tsinghua.edu.cn"], "authors": ["Ziyu Wang", "Tongzheng Ren", "Jun Zhu", "Bo Zhang"], "pdf": "/pdf/fef83ae6842b76284e2de4d04977d1575d0efdf4.pdf", "paperhash": "wang|function_space_particle_optimization_for_bayesian_neural_networks", "_bibtex": "@inproceedings{\nwang2018function,\ntitle={Function Space Particle Optimization for Bayesian Neural Networks},\nauthor={Ziyu Wang and Tongzheng Ren and Jun Zhu and Bo Zhang},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BkgtDsCcKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper282/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621616180, "tddate": null, "super": null, "final": null, "reply": {"forum": "BkgtDsCcKQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper282/Authors", "ICLR.cc/2019/Conference/Paper282/Reviewers", "ICLR.cc/2019/Conference/Paper282/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper282/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper282/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper282/Authors|ICLR.cc/2019/Conference/Paper282/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper282/Reviewers", "ICLR.cc/2019/Conference/Paper282/Authors", "ICLR.cc/2019/Conference/Paper282/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621616180}}}, {"id": "Hyx6dqfunQ", "original": null, "number": 1, "cdate": 1541053045182, "ddate": null, "tcdate": 1541053045182, "tmdate": 1543287618281, "tddate": null, "forum": "BkgtDsCcKQ", "replyto": "BkgtDsCcKQ", "invitation": "ICLR.cc/2019/Conference/-/Paper282/Official_Review", "content": {"title": "This is an interesting paper that seems to make an important contribution but its technical presentation needs to be improved. As of now, it is somewhat hard to appreciate how significant the proposed solution is. I hope the authors could help me clarify the below points so I can converge to a final rating.", "review": "PAPER SUMMARY:\n\nThis paper proposes a new POVI method for posterior inference in BNN. Unlike existing POVI techniques that optimize particles in the weight space which often yields sub-optimal results on BNN due to its over-parameterized nature, the new POVI method aims to maintain and update particles  directly on the space of regression functions to overcome this sub-optimal issue.\n\nNOVELTY & SIGNIFICANCE:\n\nIn general, I am inclined to think that this paper has made an important contribution with very promising results but I still have doubts in the proposed solution technique (as detailed below) and am not able to converge to a final rating at this point.\n\nTECHNICAL SOUNDNESS:\n\nThe authors claim that the new POVI technique operates directly on the function-space posterior to sidestep the over-parameterized issue of BNN but ultimately each function particle is still identified by a weight particle (as detailed in Eq. (2)). In terms of high-level ideas, I am not sure I understand the implied fundamental differences between this work and SVGD and how significant is it.\n\nOn the technical level, the key difference between the proposed work and SVGD seems to be the particle update equation in (2): The gradient flow is multiplied with the derivative of the BNN evaluated at the corresponding weight particle (in SVGD, the gradient flow was used alone). The authors then mentioned that this update rule results from minimizing the difference between f(X, theta) and f(X, theta) + \\epsilon * v(f(., theta))(X). I do not follow this step -- please elaborate.\n\nThe theoretical justification that follows Eq. (3) is somewhat incoherent: What is \\Epsilon(q(f(x)))? This has not been defined before or anywhere in the main text. Furthermore, the paragraph that follows the theoretical justification implies the computation of the gradient flow in (3) involves the likelihood term -- why is that?\n\nIn Algorithm 1, why do we sample from both the training set and some measure \\mu? I am sure there must be a reason for this but I could not find it anywhere except for a short statement that \"for convenience, we choose \\mu in such a way that samples from \\mu always consists a mini-batch from X\". Please elaborate.\n\nWill the proposed POVI converge?\n\nCLARITY:\n\nI think this paper has clarity issue with the technical exposition. The explanation tends to be very limited and even appear coherent at important points. For example, see\nmy 3rd point above. \n", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper282/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Function Space Particle Optimization for Bayesian Neural Networks", "abstract": "While Bayesian neural networks (BNNs) have drawn increasing attention, their posterior inference remains challenging, due to the high-dimensional and over-parameterized nature. To address this issue, several highly flexible and scalable variational inference procedures based on the idea of particle optimization have been proposed. These methods directly optimize a set of particles to approximate the target posterior. However, their application to BNNs often yields sub-optimal performance, as such methods have a particular failure mode on over-parameterized models. In this paper, we propose to solve this issue by performing particle optimization directly in the space of regression functions. We demonstrate through extensive experiments that our method successfully overcomes this issue, and outperforms strong baselines in a variety of tasks including prediction, defense against adversarial examples, and reinforcement learning.", "keywords": ["Bayesian neural networks", "uncertainty estimation", "variational inference"], "authorids": ["wzy196@gmail.com", "rtz19970824@gmail.com", "dcszj@mail.tsinghua.edu.cn", "dcszb@mail.tsinghua.edu.cn"], "authors": ["Ziyu Wang", "Tongzheng Ren", "Jun Zhu", "Bo Zhang"], "pdf": "/pdf/fef83ae6842b76284e2de4d04977d1575d0efdf4.pdf", "paperhash": "wang|function_space_particle_optimization_for_bayesian_neural_networks", "_bibtex": "@inproceedings{\nwang2018function,\ntitle={Function Space Particle Optimization for Bayesian Neural Networks},\nauthor={Ziyu Wang and Tongzheng Ren and Jun Zhu and Bo Zhang},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BkgtDsCcKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper282/Official_Review", "cdate": 1542234497255, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "BkgtDsCcKQ", "replyto": "BkgtDsCcKQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper282/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335689485, "tmdate": 1552335689485, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper282/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "HyloUTy5nQ", "original": null, "number": 3, "cdate": 1541172563026, "ddate": null, "tcdate": 1541172563026, "tmdate": 1543260666350, "tddate": null, "forum": "BkgtDsCcKQ", "replyto": "BkgtDsCcKQ", "invitation": "ICLR.cc/2019/Conference/-/Paper282/Official_Review", "content": {"title": "Very promising technique, but requires clarification", "review": "Based on the revision, I am willing to raise the score from 5 to 7.\n\n========================================== \n\nThe authors address the problems of variational inference in over-parameterized models and the problem of the collapse of particle-optimization-based variational inference methods (POVI). The authors propose to solve these problems by performing POVI in the space of functions instead of the weight space and propose a heuristic approximation to POVI in function spaces.\n\nPros:\n1) I believe that this work is of great importance to the Bayesian deep learning community, and may cause a paradigm shift in this area.\n2) The method performs well in practice, and alleviates the over-parameterization problem, as shown in Appendix A.\n3) It seems scalable and easy to implement (and is similar to SVGD in this regard), however, some necessary details are omitted.\n\nCons:\n1) The paper is structured nicely, but the central part of the paper, Section 3, is written poorly; many necessary details are omitted.\n2) The use of proposed approximations is not justified\n\nIn order to be able to perform POVI in function-space, the authors use 4 different approximations in succession. The authors do not check the impact of those approximations empirically, and only assess the performance of the final procedure. I believe it would be beneficial to see the impact of those approximations on simple toy tasks where function-space POVI can be performed directly. Only two approximations are well-motivated (mini-batching and approximation of the prior distribution), whereas the translation of the function-space update and the choice of mu (the distribution, from which we sample mini-batches) are stated without any details.\n\nMajor concerns:\n1) As far as I understand, one can see the translation of the function-space update to the weight-space update (2) as one step of SGD for the minimization of the MSE \\sum_x (f(x; \\theta^i) - f^i_l(x) - \\eps v(f^i_l)(x))^2, where the sum is taken over the whole space X if it is finite, or over the current mini-batch otherwise. The learning rate of such update is fixed at 1. This should be clearly stated in the paper, as for now the update (2) is given without any explanation.\n\n2) I am concerned with the theoretical justification paragraph for the update rule (3) (mini-batching). It is clear that if each marginal is matched exactly, the full posterior is also exactly matched. However, it would usually not be possible to match all marginals using parametric approximations for f(x). Moreover, it is not clear why would updates (3) even converge at all or converge to the desired point, as it is essentially the update for an optimization problem (minimization of the MSE done by SGD with a fixed learning rate), nested into a simulation problem (function-space POVI). This paragraph provides a nice intuition to why the procedure works, but theoretical justification would require more rigor.\n\n3) Another approximation that is left unnoted is the choice of mu (the distribution over mini-batches). It seems to me from the definition of function-space POVI that we need to use the uniform distribution over the whole object space X (or, if we do not do mini-batching, we need to use the full space X). However, the choice of X seems arbitrary. For example, for MNIST data we may consider all real-values 28x28 matrices, where all elements lie on the segment [0,1]. Or, we could use the full space R^28x28. Or, we could use only the support of the empirical distribution. I have several concerns here:\n3.1) If the particles are parametric, the solution may greatly depend on the choice of X. As the empirical distribution has a finite support, it would be dominated by other points unless the data points are reweighted. And as the likelihood does not depend on the out-of-dataset samples, all particles f^i would collapse into prior, completely ignoring the training data.\n3.2) If the prior is non-parametric, f(x) for all out-of-dataset objects x would collapse to the prior, whereas the f(x) for all the training objects would perfectly match the training data. Therefore we would not be able to make non-trivial predictions for the objects that are not contained in the training set unless the function-space kernel of the function-space prior somehow prevents it. This poses a question: how can we ensure the ability of our particles to interpolate and extrapolate without making them parametric? Even in the parametric case, if we have no additional regularization and flexible enough models, they could overfit and have a similar problem.\nThese two concerns may be wrong, as I did not fully understand how the function-space prior distribution works, and how the function-space kernel is defined (see concern 4).\n\n4) Finally, it is not stated how the kernels for function-space POVI are defined. Therefore, it is not clear how to implement the proposed technique, and how to reproduce the results. Also, without the full expression for the weight-space update, it is difficult to relate the proposed procedure to the plain weight-space POVI with the function value kernel, discussed in Appendix B.\n\nMinor comments:\n1) It is hard to see the initial accuracy of different models from Figure 3 (accuracy without adversarial examples). Also, what is the test log-likelihood of these models?\n2) It seems that sign in line 5 on page 4 should be '-'\n\nI believe that this could be a very strong paper. Unfortunately, the paper lacks a lot of important details, and I do not think that it is ready for publication in its current form.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper282/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Function Space Particle Optimization for Bayesian Neural Networks", "abstract": "While Bayesian neural networks (BNNs) have drawn increasing attention, their posterior inference remains challenging, due to the high-dimensional and over-parameterized nature. To address this issue, several highly flexible and scalable variational inference procedures based on the idea of particle optimization have been proposed. These methods directly optimize a set of particles to approximate the target posterior. However, their application to BNNs often yields sub-optimal performance, as such methods have a particular failure mode on over-parameterized models. In this paper, we propose to solve this issue by performing particle optimization directly in the space of regression functions. We demonstrate through extensive experiments that our method successfully overcomes this issue, and outperforms strong baselines in a variety of tasks including prediction, defense against adversarial examples, and reinforcement learning.", "keywords": ["Bayesian neural networks", "uncertainty estimation", "variational inference"], "authorids": ["wzy196@gmail.com", "rtz19970824@gmail.com", "dcszj@mail.tsinghua.edu.cn", "dcszb@mail.tsinghua.edu.cn"], "authors": ["Ziyu Wang", "Tongzheng Ren", "Jun Zhu", "Bo Zhang"], "pdf": "/pdf/fef83ae6842b76284e2de4d04977d1575d0efdf4.pdf", "paperhash": "wang|function_space_particle_optimization_for_bayesian_neural_networks", "_bibtex": "@inproceedings{\nwang2018function,\ntitle={Function Space Particle Optimization for Bayesian Neural Networks},\nauthor={Ziyu Wang and Tongzheng Ren and Jun Zhu and Bo Zhang},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BkgtDsCcKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper282/Official_Review", "cdate": 1542234497255, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "BkgtDsCcKQ", "replyto": "BkgtDsCcKQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper282/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335689485, "tmdate": 1552335689485, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper282/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "HJlsKYptRX", "original": null, "number": 7, "cdate": 1543260547436, "ddate": null, "tcdate": 1543260547436, "tmdate": 1543260547436, "tddate": null, "forum": "BkgtDsCcKQ", "replyto": "BJeUUXsQ0X", "invitation": "ICLR.cc/2019/Conference/-/Paper282/Official_Comment", "content": {"title": "Response to the revision", "comment": "Thank you for clarification; the paper has become much more clear after the revision, and the extended experiments provide additional motivation/justification for the proposed approximations. Most of my concerns have been answered.\n\nI still have several questions and comments:\n\n1) I am still convinced that the predictive performance of the model would greatly depend on the choice of mu, although this choice *seems* to be arbitrary. However, now I see it differently: as we are using the parametric approximation, it is not possible to perfectly approximate the posterior on the full set of X. By choosing different distributions mu, we can choose the regions of the input space in which we would like to approximate the posterior better. For example, if we seek better predictive performance, it makes perfect sense to use the KDE of the training data and use unlabeled data to gather more information about the domain. If one is interested in good out-of-domain uncertainty, it would make sense to add some out-of-domain data. I believe that the careful choice of mu is a nice way to incorporate the domain information in the trained discriminative model.\n\n2) How would one balance B' and B-B'? How does it impact the uncertainty and predictive performance? What values of B and B' do you use in the experiments?\n\n3) What do you mean by \"f(x) is already high-dimensional for a single data point x, one should down-sample f(x) within each x\"? If I understand correctly, dim f(x) is equal to the number of classes, which is typically not very high. Could you elaborate more on the mentioned down-sampling? How exactly is it performed, and why is it needed?\n\n4) I am not sure whether the accuracy degradation in Table 10 can be attributed to adversarial robustness. If I understand correctly, you do not perform adversarial training or otherwise address adversarial examples explicitly. Still, it would be interesting to see how the choice of B' and the parameters of KDE would influence the prediction performance and the robustness to adversarial examples. I suspect that it is possible to obtain some trade-off.\n\nTo sum up:\n+ The paper provides an elegant way to perform function-space posterior inference that does not suffer from overparameterization\n+ The paper provides a nice way to explicitly choose the regions in the input space in which the posterior should be approximated better\n- Although the approximations are well-motivated, there are many of them. It is not clear how well does the proposed procedure correspond to the true posterior inference, or how different parameters of the procedure impact the inference."}, "signatures": ["ICLR.cc/2019/Conference/Paper282/AnonReviewer2"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper282/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper282/AnonReviewer2", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Function Space Particle Optimization for Bayesian Neural Networks", "abstract": "While Bayesian neural networks (BNNs) have drawn increasing attention, their posterior inference remains challenging, due to the high-dimensional and over-parameterized nature. To address this issue, several highly flexible and scalable variational inference procedures based on the idea of particle optimization have been proposed. These methods directly optimize a set of particles to approximate the target posterior. However, their application to BNNs often yields sub-optimal performance, as such methods have a particular failure mode on over-parameterized models. In this paper, we propose to solve this issue by performing particle optimization directly in the space of regression functions. We demonstrate through extensive experiments that our method successfully overcomes this issue, and outperforms strong baselines in a variety of tasks including prediction, defense against adversarial examples, and reinforcement learning.", "keywords": ["Bayesian neural networks", "uncertainty estimation", "variational inference"], "authorids": ["wzy196@gmail.com", "rtz19970824@gmail.com", "dcszj@mail.tsinghua.edu.cn", "dcszb@mail.tsinghua.edu.cn"], "authors": ["Ziyu Wang", "Tongzheng Ren", "Jun Zhu", "Bo Zhang"], "pdf": "/pdf/fef83ae6842b76284e2de4d04977d1575d0efdf4.pdf", "paperhash": "wang|function_space_particle_optimization_for_bayesian_neural_networks", "_bibtex": "@inproceedings{\nwang2018function,\ntitle={Function Space Particle Optimization for Bayesian Neural Networks},\nauthor={Ziyu Wang and Tongzheng Ren and Jun Zhu and Bo Zhang},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BkgtDsCcKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper282/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621616180, "tddate": null, "super": null, "final": null, "reply": {"forum": "BkgtDsCcKQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper282/Authors", "ICLR.cc/2019/Conference/Paper282/Reviewers", "ICLR.cc/2019/Conference/Paper282/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper282/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper282/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper282/Authors|ICLR.cc/2019/Conference/Paper282/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper282/Reviewers", "ICLR.cc/2019/Conference/Paper282/Authors", "ICLR.cc/2019/Conference/Paper282/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621616180}}}, {"id": "SylRwHjQAm", "original": null, "number": 5, "cdate": 1542858086253, "ddate": null, "tcdate": 1542858086253, "tmdate": 1542858531733, "tddate": null, "forum": "BkgtDsCcKQ", "replyto": "Hyx6dqfunQ", "invitation": "ICLR.cc/2019/Conference/-/Paper282/Official_Comment", "content": {"title": "Thanks for your feedback; summary of high-level idea and response to individual questions below (1/2)", "comment": "Thank you for your positive and constructive feedback. We have incorporated them in the revision. As you expressed some confusion about our proposed method and contribution, we first present an informal but more high-level explanation; responses to your individual questions are presented following that.\n\n####### our high-level idea, its difference to weight-space SVGD, and contribution #######\n\nAs stated in Section 2 of the revision, the vanilla SVGD performs Bayesian inference of BNNs in the space of network weights (namely \u201cweight-space inference\u201d). While such a view is natural, our key observation is that it can make the problem unnecessarily hard. Namely, SVGD works by placing particles in high-probability regions, and at the same time making sure they are distant from each other, so they constitute a representative sample of the posterior. But in the weight-space posterior of BNN, there exists (at least) exponentially many numbers of posterior maximas that can be distant from each other, but corresponding to the same function. So a possible convergence point is where each SVGD particle occupies one of them; and in prediction, this posterior approximation does not improve over a single point estimate. In other words, a good approximation of the weight-space posterior isn\u2019t necessarily good in function space.\n\nWe propose to do SVGD in function space. The key difference is that distance metrics are now defined on functions, so networks corresponding to the same function are far less likely to be represented by multiple particles (see Remark 3.2 and Appendix D in our revision). This is non-trivial as the function space is infinite-dimensional, but we have built an effective approximation that is very easy to implement.\n\nWe believe our contribution is significant, as\n(1) our proposal is very easy to implement, and performs extremely well in practice;\n(2) inference in over-parameterized models has attracted attention for long (see the first paragraph of Section 4). By shifting the focus to the prediction function instead of its specific parameterization, our work presents a novel solution to this problem."}, "signatures": ["ICLR.cc/2019/Conference/Paper282/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper282/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper282/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Function Space Particle Optimization for Bayesian Neural Networks", "abstract": "While Bayesian neural networks (BNNs) have drawn increasing attention, their posterior inference remains challenging, due to the high-dimensional and over-parameterized nature. To address this issue, several highly flexible and scalable variational inference procedures based on the idea of particle optimization have been proposed. These methods directly optimize a set of particles to approximate the target posterior. However, their application to BNNs often yields sub-optimal performance, as such methods have a particular failure mode on over-parameterized models. In this paper, we propose to solve this issue by performing particle optimization directly in the space of regression functions. We demonstrate through extensive experiments that our method successfully overcomes this issue, and outperforms strong baselines in a variety of tasks including prediction, defense against adversarial examples, and reinforcement learning.", "keywords": ["Bayesian neural networks", "uncertainty estimation", "variational inference"], "authorids": ["wzy196@gmail.com", "rtz19970824@gmail.com", "dcszj@mail.tsinghua.edu.cn", "dcszb@mail.tsinghua.edu.cn"], "authors": ["Ziyu Wang", "Tongzheng Ren", "Jun Zhu", "Bo Zhang"], "pdf": "/pdf/fef83ae6842b76284e2de4d04977d1575d0efdf4.pdf", "paperhash": "wang|function_space_particle_optimization_for_bayesian_neural_networks", "_bibtex": "@inproceedings{\nwang2018function,\ntitle={Function Space Particle Optimization for Bayesian Neural Networks},\nauthor={Ziyu Wang and Tongzheng Ren and Jun Zhu and Bo Zhang},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BkgtDsCcKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper282/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621616180, "tddate": null, "super": null, "final": null, "reply": {"forum": "BkgtDsCcKQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper282/Authors", "ICLR.cc/2019/Conference/Paper282/Reviewers", "ICLR.cc/2019/Conference/Paper282/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper282/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper282/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper282/Authors|ICLR.cc/2019/Conference/Paper282/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper282/Reviewers", "ICLR.cc/2019/Conference/Paper282/Authors", "ICLR.cc/2019/Conference/Paper282/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621616180}}}, {"id": "SJlJ9HiQCQ", "original": null, "number": 6, "cdate": 1542858118763, "ddate": null, "tcdate": 1542858118763, "tmdate": 1542858404590, "tddate": null, "forum": "BkgtDsCcKQ", "replyto": "SylRwHjQAm", "invitation": "ICLR.cc/2019/Conference/-/Paper282/Official_Comment", "content": {"title": "Response to individual questions (2/2)", "comment": "####### Response to Technical Questions #######\n\nQ1. Elaboration of the update rule (2) (now Eq. (3) in revision):\nA1: The update rule (3) corresponds to doing one step of gradient descent to minimize the squared distance between f(X;theta) and f(X;theta)+\\epsilon*v(f(X;\\theta)). We clarified this in Remark 3.1 in our revision.\n\nQ2. What is \\Epsilon(q(f(x)))?\nA2: Thanks for pointing out. We revised the text to clarify it. \\mathcal{E}(q[f(x)]) is an energy functional defined on the space of (variational) distributions, which achieves its unique minimum at the true posterior p. Its exact form varies across different POVI methods. Please refer to [Chen et al., 2018] for the exact form.\n\nQ3. Why (3) (now Eq (4) in revision) involves the likelihood term:\nA3: In fact, any kinds of gradient flow (GF) that corresponds to a Bayesian inference procedure will include model posterior, and subsequently through the Bayes' rule, the likelihood term. The GFs considered in this work are defined in the newly added Eq. (1), in which v is defined in Table 1. You can see there that they all include the posterior term. We have revised the paragraph you mentioned to make this clearer.\n\nQ4. why we sampled from both the training set and some measure \\mu:\nA4: Thanks for pointing out. In fact, this is a typo. The correct statement is that we only need to sample from \\mu, which includes samples from the training set. We sincerely apologize for the confusion, and have fixed it in the revision. We have also revised Section 3.1.2 to clarify why we need samples from both the training set and another distribution with a full support. An intuitive explanation is:\n(1) firstly, to make sure prediction interpolates and extrapolates, at least some components of \\mu (recall it is defined on X^B, and a sample from \\mu contains B points in input space) must visit the entire input space, i.e. have a full support. So these components can\u2019t only include the training points, and we incorporated a full-support component for this purpose.\n(2) if samples from \\mu does not contain training points with a non-zero probability, we will have trouble in computing the likelihood, as it is only defined (in closed form) on training samples. Therefore, we added the training sample component.\n\nQ5. On whether the proposed method will converge:\nA5: We have added convergence plots in Appendix A.2.4 and Appendix C. We can see that in practice, the proposed procedure is fairly robust, and has no convergence issues. Moreover, our algorithm is an approximation to an idealized algorithm: the simulation of the ``averaged gradient flow\u2019\u2019 in paragraph \u201cTheoretical Justification\u201d of Section 3.1.2. As we discussed there, simulation of the averaged GF converges to a unique optima (i.e., the true posterior).\nFinally, like much previous work, we do introduce some approximations (e.g., stochastic approximations to the averaged GF and parametric approximation to particle functions). Developing a theory that simultaneously considers all approximations is very hard, so we present thorough empirical evidence: in Section 5.1, the approximate posterior obtained by our algorithm is of high quality; and in the rest of Section 5, the approximation works well in real-world. Furthermore, we have added numerical studies on the impact of these approximations in Appendix C. These results suggest that our method will be of much value to our community.\n\n####### Clarity issues #######\n\nThanks for your suggestion. We revised the paper thoroughly to clarify some ideas that may confuse the potential audience.\n\n####### References #######\n\n[Chen et al., 2018]: Changyou Chen, Ruiyi Zhang, Wenlin Wang, Bai Li, and Liqun Chen. A unified particleoptimization framework for scalable bayesian sampling. arXiv preprint arXiv:1805.11659, 2018.\n[Ambrosio et al., 2008]: Luigi Ambrosio, Nicola Gigli, and Giuseppe Savare \u0301. Gradient flows: in metric spaces and in the space of probability measures. Springer Science & Business Media, 2008."}, "signatures": ["ICLR.cc/2019/Conference/Paper282/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper282/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper282/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Function Space Particle Optimization for Bayesian Neural Networks", "abstract": "While Bayesian neural networks (BNNs) have drawn increasing attention, their posterior inference remains challenging, due to the high-dimensional and over-parameterized nature. To address this issue, several highly flexible and scalable variational inference procedures based on the idea of particle optimization have been proposed. These methods directly optimize a set of particles to approximate the target posterior. However, their application to BNNs often yields sub-optimal performance, as such methods have a particular failure mode on over-parameterized models. In this paper, we propose to solve this issue by performing particle optimization directly in the space of regression functions. We demonstrate through extensive experiments that our method successfully overcomes this issue, and outperforms strong baselines in a variety of tasks including prediction, defense against adversarial examples, and reinforcement learning.", "keywords": ["Bayesian neural networks", "uncertainty estimation", "variational inference"], "authorids": ["wzy196@gmail.com", "rtz19970824@gmail.com", "dcszj@mail.tsinghua.edu.cn", "dcszb@mail.tsinghua.edu.cn"], "authors": ["Ziyu Wang", "Tongzheng Ren", "Jun Zhu", "Bo Zhang"], "pdf": "/pdf/fef83ae6842b76284e2de4d04977d1575d0efdf4.pdf", "paperhash": "wang|function_space_particle_optimization_for_bayesian_neural_networks", "_bibtex": "@inproceedings{\nwang2018function,\ntitle={Function Space Particle Optimization for Bayesian Neural Networks},\nauthor={Ziyu Wang and Tongzheng Ren and Jun Zhu and Bo Zhang},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BkgtDsCcKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper282/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621616180, "tddate": null, "super": null, "final": null, "reply": {"forum": "BkgtDsCcKQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper282/Authors", "ICLR.cc/2019/Conference/Paper282/Reviewers", "ICLR.cc/2019/Conference/Paper282/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper282/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper282/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper282/Authors|ICLR.cc/2019/Conference/Paper282/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper282/Reviewers", "ICLR.cc/2019/Conference/Paper282/Authors", "ICLR.cc/2019/Conference/Paper282/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621616180}}}, {"id": "Hyxehfi70Q", "original": null, "number": 2, "cdate": 1542857383646, "ddate": null, "tcdate": 1542857383646, "tmdate": 1542857926844, "tddate": null, "forum": "BkgtDsCcKQ", "replyto": "rJepoAru2m", "invitation": "ICLR.cc/2019/Conference/-/Paper282/Official_Comment", "content": {"title": "Thanks for your feedback; response to individual questions below", "comment": "Thank you for your positive and constructive feedback, and we have incorporated them into the revision. We address individual questions below.\n\nQ1: On the need of architectural search:\nA1: We address this concern from three aspects, as detailed below.\n(1) It is true that the degeneracy of weight-space POVI worsens as architecture complexity increases. We presented such an evaluation in Fig. 5, Appendix A.1. In the revision, we also added experiments on some UCI datasets using narrower network architectures in Appendix A.2.4, in which the performance of weight-space POVI methods improves, but is still outperformed by function-space methods.\n(2) We note that our previous experiment setups are standard as in BNN literature, and are fair to all baselines: in comparisons to almost all baselines, we followed the setup in their original paper. The only exceptions are the synthetic data experiments in Appendix A.1, which explicitly evaluate the impact of model complexity; and the ResNet-32 experiment, which, to our knowledge, is not considered by any previous work in BNN inference.\n(3) Finally, to model high-dimensional datasets, it is necessary to use large networks. For example, ResNet-32 has 0.4 million parameters; and the RNNs used in language modeling has hundreds of hidden units in each layer. Complex models lead to more severe over-parameterization, and BNN benchmarks should reflect this property. While we agree on the importance of evaluating different BNN architectures, e.g., for regression tasks, it requires a huge amount of work, given the number of baselines we considered, and such an evaluation should be a separate effort.\n\nQ2: On the possibility of introducing parameter constraints to improve weight space VI:\nA2: We agree that this is a good idea. However, it is hard to implement in practice, because of two reasons: 1) First, it is hard to cover all sources of unidentifiability by imposing parameter constraints. As an example, we show that order constraints alone cannot ensure identifiability: suppose we are to learn the ReLU function with a single hidden layer and a single hidden unit. We can scale the weights of the two connections in the network appropriately, and obtain an infinite number of modes; 2) Second, it is often non-trivial to apply gradient-based optimization under such constraints, and even if such an optimization scheme is possible, its impact on the learning dynamics (of variational parameters) is not clear.\nIn contrast, our method eliminates all sources of unidentifiability, has a simple form similar to (unconstrained) gradient descent, and converges robustly in practice. We revised the paper to include further discussions on this, in Remark 3.1 and 3.2 where we show the resemblence of our algorithm to gradient descent; and in Appendix A.2.4 where we presented empirical evidence that our algorithm converges robustly in practice. In conclusion, we believe our method is a more practical solution to unidentifiability.\n\nQ3: On the number of particles needed, and scalability:\nA3: We address this concern from three aspects, as detailed below: \n(1) We chose to use 20 particles following the original SVGD paper, so the results are comparable; and as we have discussed in the text (Remark 3.2), our method is far more particle-efficient compared to the weight-space POVI methods.\n(2) We hypothesize that even with a small number of particles, function-space methods could produce posterior approximations that are useful in practice. A reason is that the structure of \u201cfunction-space posterior\u201d (at least in the finite-dimensional case, in which its density is well-defined) is often strikingly simple: for GP regression with conjugate likelihood, for example, the posterior is essentially uni-modal (Rasmussen, 2004). Empirically this hypothesis is supported by our experiments on CIFAR-10, in which a posterior approximation using merely 8 particles is shown to be more robust against adversarial examples, which is presumably due to the improved representation of epistemic uncertainty.\n(3) Finally, in terms of scalability, our method can be easily parallelized using model parallelism, as the only communication needed in each iteration is to broadcast the top-layer activations (function evaluations on the mini-batch). This cost is negligible compared to sending all network weights, which is needed in data-parallel training and weight-space POVI methods. We added the discussion on scalability in Appendix D.\n\nReferences:\n[Rasmussen, 2004]: Carl Edward Rasmussen. Gaussian processes in machine learning. In Advanced lectures on machine learning, pp. 63\u201371. Springer, 2004."}, "signatures": ["ICLR.cc/2019/Conference/Paper282/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper282/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper282/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Function Space Particle Optimization for Bayesian Neural Networks", "abstract": "While Bayesian neural networks (BNNs) have drawn increasing attention, their posterior inference remains challenging, due to the high-dimensional and over-parameterized nature. To address this issue, several highly flexible and scalable variational inference procedures based on the idea of particle optimization have been proposed. These methods directly optimize a set of particles to approximate the target posterior. However, their application to BNNs often yields sub-optimal performance, as such methods have a particular failure mode on over-parameterized models. In this paper, we propose to solve this issue by performing particle optimization directly in the space of regression functions. We demonstrate through extensive experiments that our method successfully overcomes this issue, and outperforms strong baselines in a variety of tasks including prediction, defense against adversarial examples, and reinforcement learning.", "keywords": ["Bayesian neural networks", "uncertainty estimation", "variational inference"], "authorids": ["wzy196@gmail.com", "rtz19970824@gmail.com", "dcszj@mail.tsinghua.edu.cn", "dcszb@mail.tsinghua.edu.cn"], "authors": ["Ziyu Wang", "Tongzheng Ren", "Jun Zhu", "Bo Zhang"], "pdf": "/pdf/fef83ae6842b76284e2de4d04977d1575d0efdf4.pdf", "paperhash": "wang|function_space_particle_optimization_for_bayesian_neural_networks", "_bibtex": "@inproceedings{\nwang2018function,\ntitle={Function Space Particle Optimization for Bayesian Neural Networks},\nauthor={Ziyu Wang and Tongzheng Ren and Jun Zhu and Bo Zhang},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BkgtDsCcKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper282/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621616180, "tddate": null, "super": null, "final": null, "reply": {"forum": "BkgtDsCcKQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper282/Authors", "ICLR.cc/2019/Conference/Paper282/Reviewers", "ICLR.cc/2019/Conference/Paper282/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper282/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper282/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper282/Authors|ICLR.cc/2019/Conference/Paper282/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper282/Reviewers", "ICLR.cc/2019/Conference/Paper282/Authors", "ICLR.cc/2019/Conference/Paper282/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621616180}}}, {"id": "H1lfE7o7Cm", "original": null, "number": 3, "cdate": 1542857514342, "ddate": null, "tcdate": 1542857514342, "tmdate": 1542857894548, "tddate": null, "forum": "BkgtDsCcKQ", "replyto": "HyloUTy5nQ", "invitation": "ICLR.cc/2019/Conference/-/Paper282/Official_Comment", "content": {"title": "Thanks for your review; response and clarification (1/2)", "comment": "Thank you for your positive and constructive feedbacks. We revised the presentation thoroughly following them. However, there are also some misunderstandings that we hope to clarify.\n\nQ: On lack of details:\nA: We clarified the kernel specification issue you mentioned in Section 3.1.2. Our implementation will also be made public after the review process to make everything reproducible. \n\nQ: On the justification of approximations:\nA: Thanks for the suggestions. First, as you suggested, we have added simulations evaluating the impact of the parametric approximation and mini-batching in Appendix C. The result shows they do not influence convergence. Second, we have thoroughly revised the justification for stochastic approximation in Section 3.1.2. \nFinally, please note that your comment on the choice of \\mu being an approximation could be a major understanding, which we clarify below.\n\nRegarding your major concerns:\n\nQ1. Regarding the weight space update (2) (now Eq.(3)):\nA1: Thank you for the comments. Yes, your explanation is correct. We have revised Section 3.1.1 to clarify it. In brief, such an update is easy to implement, relates to ensemble training, and does not impact convergence empirically. Please read the revision for full details.\n\nQ2. Regarding the theoretical justification of (3) (now Eq.(4)):\nA2: Thanks for the comments. We clarify from three points: \n(1) On your concern that marginals could not be matched exactly: we added a paragraph in the `Justificiation of (4)` part in Sec 3.1.2. The point is that even in that case, the average energy is an excellent choice of variational objective, as in practice we only care about average approximation error of lower-order moments; and in the definition of the averaged energy, \\mu could be specified to incorporate distributional assumptions about the test set.\n(2) On convergence: in Appendix C, we added synthetic experiments specifically verifying that parametric approximation plus mini-batching does not impact convergence. We also added a convergence plot for a real-world dataset in Appendix A.2.4, which shows our algorithm is fairly stable in practice.\n(3) However, a rigorous theory that simultaneously addresses both approximations could be hard to develop, as our procedure works on the infinite-dimensional Wasserstein spaces, a Riemannian manifold with a non-Euclidean metric; and optimization (i.e. simulation of gradient flows) on Riemannian manifolds is less well-studied than their Euclidean counterparts. We are not aware of any immediate results to this answer.\nAlthough a bit heuristic, our method is well-motivated, works well empirically, and has an elegant, easy-to-implement form. We hope you agree that such a procedure could be of value to the community.\n\nQ3. On your belief that we need a uniform \\mu:\nA3: There is some misunderstanding that might be caused by previous typos and over-restrictive conditions in Section 3.1.2. We apologize for the confusion; the text has been revised, and we also clarify this issue below. In fact, the only requirement on \\mu is that\n\n  if q(f(x))=p(f(x)|X_{train},Y_{train}) almost everywhere w.r.t. \\mu(x), then q(f) and p(f|X_{train},Y_{train}) defines the same stochastic process.\n\nFor example, if the posterior is a GP, a sufficient condition is that B>=2, and a single sample from \\mu consists of samples from a continuous measure supported on the entire X, as well as samples from the training set. According to the condition above, \\mu don\u2019t need to have identically distributed components (recall it is a distribution on X^B, and a sample from \\mu contains B samples in the input space), or follow the uniform distribution.\nAs mentioned in the second to last paragraph in Section 3.1.2, our choice of \\mu is the product measure of mini-batch from the training set, and samples from its kernel density estimation.\n\nQ3.1. On your concern that training set could be ignored:\nA3.1: As clarified above in our response to Q3, a single sample from \\mu contains both training data and samples from a continuous distribution, so training data is not ignored."}, "signatures": ["ICLR.cc/2019/Conference/Paper282/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper282/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper282/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Function Space Particle Optimization for Bayesian Neural Networks", "abstract": "While Bayesian neural networks (BNNs) have drawn increasing attention, their posterior inference remains challenging, due to the high-dimensional and over-parameterized nature. To address this issue, several highly flexible and scalable variational inference procedures based on the idea of particle optimization have been proposed. These methods directly optimize a set of particles to approximate the target posterior. However, their application to BNNs often yields sub-optimal performance, as such methods have a particular failure mode on over-parameterized models. In this paper, we propose to solve this issue by performing particle optimization directly in the space of regression functions. We demonstrate through extensive experiments that our method successfully overcomes this issue, and outperforms strong baselines in a variety of tasks including prediction, defense against adversarial examples, and reinforcement learning.", "keywords": ["Bayesian neural networks", "uncertainty estimation", "variational inference"], "authorids": ["wzy196@gmail.com", "rtz19970824@gmail.com", "dcszj@mail.tsinghua.edu.cn", "dcszb@mail.tsinghua.edu.cn"], "authors": ["Ziyu Wang", "Tongzheng Ren", "Jun Zhu", "Bo Zhang"], "pdf": "/pdf/fef83ae6842b76284e2de4d04977d1575d0efdf4.pdf", "paperhash": "wang|function_space_particle_optimization_for_bayesian_neural_networks", "_bibtex": "@inproceedings{\nwang2018function,\ntitle={Function Space Particle Optimization for Bayesian Neural Networks},\nauthor={Ziyu Wang and Tongzheng Ren and Jun Zhu and Bo Zhang},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BkgtDsCcKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper282/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621616180, "tddate": null, "super": null, "final": null, "reply": {"forum": "BkgtDsCcKQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper282/Authors", "ICLR.cc/2019/Conference/Paper282/Reviewers", "ICLR.cc/2019/Conference/Paper282/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper282/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper282/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper282/Authors|ICLR.cc/2019/Conference/Paper282/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper282/Reviewers", "ICLR.cc/2019/Conference/Paper282/Authors", "ICLR.cc/2019/Conference/Paper282/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621616180}}}, {"id": "BJeUUXsQ0X", "original": null, "number": 4, "cdate": 1542857550508, "ddate": null, "tcdate": 1542857550508, "tmdate": 1542857550508, "tddate": null, "forum": "BkgtDsCcKQ", "replyto": "H1lfE7o7Cm", "invitation": "ICLR.cc/2019/Conference/-/Paper282/Official_Comment", "content": {"title": "Response and Clarification (2/2)", "comment": "Q3.2-i: On your concern that f(x) could collapse to prior:\nA3.2-i: You are correct that the function-space prior prevents it: while priors used in practice (like the common weak Gaussian prior on NN weights) are flexible, their flexibility is far from letting out-of-sample prediction collapse to the prior, as they all encode smoothness in some sense (See the example in [^1] below). The particle functions in your example are a.s. not continuous, and are not inside the support of the prior. As the support of posterior must be the subset of that of prior, the situation your described *will not happen*. You can also see it from the synthetic experiments in Section 5.1.\n[^1]: E.g. for GP with a RBF kernel, samples from prior are a.s. continuous; the BNN with truncated priors only represents functions with a bounded Lipschitz constant. (It is possible to do inference using non-smooth priors, but they must also encode correlation, and can be analyzed similarly. It seems that the only prior satisfying your description is an i.i.d. noise process, which should not be used in practice anyway.)\n\nQ3.2-ii: A further clarification on overfitting in parametric models:\nA3.2-ii: (1) It is still possible that with a pathological prior (or one that is too weak), prediction based on full posterior still overfits (See this blog article and references therein: http://www.nowozin.net/sebastian/blog/do-bayesians-overfit.html). However, this is not a problem that can be solved by the inference side, but rather the problem of specifying right priors. In other words, if the prior is pathological, a faithful inference procedure should make the user aware of that. \n(2) Also, even for imperfect priors, Bayesian inference guards against overfitting much better than the MAP estimate, and the observation that frequentist inference for NN overfits does not directly transfer to the Bayesian case. An intuitive explanation is that Bayesian inference performs model averaging, which weights each model point based on its complexity.\n\nQ4. On the definition of kernels in function-space POVI, and our method\u2019s relation to the function value kernel:\nA6: (1) We apologize for the mistake; we added it back to Section 3.1.2. Please notice that gradient flows are defined on B-dimensional marginal distributions, thus kernels in our algorithm are defined on finite-dimensional spaces, and can be easily specified.\n(2) As for the relation between our proposal and the function-value kernel, we have added a new Appendix D, in which we derived the detailed update rules and compared them. We hope it clarifies your concerns.\n\nFinally, for your minor comments, we address them as detailed below:\n\n1. For the adversarial defense experiment, we have added the initial accuracy and log likelihood in Appendix A.3. \n2. We have fixed the typo on Page 5, line 4. Thanks for pointing it out."}, "signatures": ["ICLR.cc/2019/Conference/Paper282/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper282/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper282/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Function Space Particle Optimization for Bayesian Neural Networks", "abstract": "While Bayesian neural networks (BNNs) have drawn increasing attention, their posterior inference remains challenging, due to the high-dimensional and over-parameterized nature. To address this issue, several highly flexible and scalable variational inference procedures based on the idea of particle optimization have been proposed. These methods directly optimize a set of particles to approximate the target posterior. However, their application to BNNs often yields sub-optimal performance, as such methods have a particular failure mode on over-parameterized models. In this paper, we propose to solve this issue by performing particle optimization directly in the space of regression functions. We demonstrate through extensive experiments that our method successfully overcomes this issue, and outperforms strong baselines in a variety of tasks including prediction, defense against adversarial examples, and reinforcement learning.", "keywords": ["Bayesian neural networks", "uncertainty estimation", "variational inference"], "authorids": ["wzy196@gmail.com", "rtz19970824@gmail.com", "dcszj@mail.tsinghua.edu.cn", "dcszb@mail.tsinghua.edu.cn"], "authors": ["Ziyu Wang", "Tongzheng Ren", "Jun Zhu", "Bo Zhang"], "pdf": "/pdf/fef83ae6842b76284e2de4d04977d1575d0efdf4.pdf", "paperhash": "wang|function_space_particle_optimization_for_bayesian_neural_networks", "_bibtex": "@inproceedings{\nwang2018function,\ntitle={Function Space Particle Optimization for Bayesian Neural Networks},\nauthor={Ziyu Wang and Tongzheng Ren and Jun Zhu and Bo Zhang},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BkgtDsCcKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper282/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621616180, "tddate": null, "super": null, "final": null, "reply": {"forum": "BkgtDsCcKQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper282/Authors", "ICLR.cc/2019/Conference/Paper282/Reviewers", "ICLR.cc/2019/Conference/Paper282/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper282/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper282/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper282/Authors|ICLR.cc/2019/Conference/Paper282/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper282/Reviewers", "ICLR.cc/2019/Conference/Paper282/Authors", "ICLR.cc/2019/Conference/Paper282/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621616180}}}, {"id": "SklEAZjXR7", "original": null, "number": 1, "cdate": 1542857163862, "ddate": null, "tcdate": 1542857163862, "tmdate": 1542857163862, "tddate": null, "forum": "BkgtDsCcKQ", "replyto": "BkgtDsCcKQ", "invitation": "ICLR.cc/2019/Conference/-/Paper282/Official_Comment", "content": {"title": "Summary of the Revision", "comment": "- We revised Section 3 thoroughly. Please notice the change of equation numbering.\n- We added three remarks in Section 3.1.1, discussing the motivation of the parametric update rule (2) (i.e., Eq (3) in the revision); comparing it with weight-space POVI and ensemble training; and noting that it does not impact convergence in synthetic experiments.\n- We revised Section 3.1.2 to clarify a few possible misunderstandings, including the requirements of the sampling distribution \\mu and the specification of the kernel.\n- In Appendix A (experimental details and further results), we added convergence plots and benchmark for narrower network architectures on UCI datasets in Section A.2.4; and accuracy and log likelihood on clean MNIST and CIFAR-10 data in Section A.3.\n- We added a new appendix, Appendix C, where we evaluated the impact of several approximations in our algorithm.\n- We added a new appendix, Appendix D, where we further clarified the connection and differences between our algorithm, weight-space POVI, and ensemble methods.\n- We revised the language and fixed a few typos, most notably:\n    - In Algorithm 1, we fixed a typo where previously, we erroneously required to sample from \\mu *and* a batch from the training set. In the corrected version, the training-set batch is *part of* samples from \\mu.\n    - In Appendix A.2.3, we fixed the results of the \u201cprotein\u201d dataset in Table 7 (comparison with Ma et al. (2018)). The previous result was obtained using an incorrect configuration, due to a scripting error. We apologize for the inconvenience; however, *neither the comparison result on that specific dataset, or the conclusion of the corresponding experiment is influenced*.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper282/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper282/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper282/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Function Space Particle Optimization for Bayesian Neural Networks", "abstract": "While Bayesian neural networks (BNNs) have drawn increasing attention, their posterior inference remains challenging, due to the high-dimensional and over-parameterized nature. To address this issue, several highly flexible and scalable variational inference procedures based on the idea of particle optimization have been proposed. These methods directly optimize a set of particles to approximate the target posterior. However, their application to BNNs often yields sub-optimal performance, as such methods have a particular failure mode on over-parameterized models. In this paper, we propose to solve this issue by performing particle optimization directly in the space of regression functions. We demonstrate through extensive experiments that our method successfully overcomes this issue, and outperforms strong baselines in a variety of tasks including prediction, defense against adversarial examples, and reinforcement learning.", "keywords": ["Bayesian neural networks", "uncertainty estimation", "variational inference"], "authorids": ["wzy196@gmail.com", "rtz19970824@gmail.com", "dcszj@mail.tsinghua.edu.cn", "dcszb@mail.tsinghua.edu.cn"], "authors": ["Ziyu Wang", "Tongzheng Ren", "Jun Zhu", "Bo Zhang"], "pdf": "/pdf/fef83ae6842b76284e2de4d04977d1575d0efdf4.pdf", "paperhash": "wang|function_space_particle_optimization_for_bayesian_neural_networks", "_bibtex": "@inproceedings{\nwang2018function,\ntitle={Function Space Particle Optimization for Bayesian Neural Networks},\nauthor={Ziyu Wang and Tongzheng Ren and Jun Zhu and Bo Zhang},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BkgtDsCcKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper282/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621616180, "tddate": null, "super": null, "final": null, "reply": {"forum": "BkgtDsCcKQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper282/Authors", "ICLR.cc/2019/Conference/Paper282/Reviewers", "ICLR.cc/2019/Conference/Paper282/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper282/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper282/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper282/Authors|ICLR.cc/2019/Conference/Paper282/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper282/Reviewers", "ICLR.cc/2019/Conference/Paper282/Authors", "ICLR.cc/2019/Conference/Paper282/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621616180}}}, {"id": "rJepoAru2m", "original": null, "number": 2, "cdate": 1541066405075, "ddate": null, "tcdate": 1541066405075, "tmdate": 1541534126205, "tddate": null, "forum": "BkgtDsCcKQ", "replyto": "BkgtDsCcKQ", "invitation": "ICLR.cc/2019/Conference/-/Paper282/Official_Review", "content": {"title": "Function Space Particle Optimization for Bayesian Neural Networks", "review": "This paper considers particle optimization variational inference methods for Bayesian neural networks.  To avoid degeneracies which arise when these algorithms are applied to the weight space posterior, the authors consider applying the approach in the function space.  A heuristic motivation is given for their algorithm and it seems to have good empirical performance.\n\nI find the paper well-motivated and the suggested algorithm original and interesting.  As the authors mention at one point the derivation is rather heuristic, so much depends on the empirical assessment of their approach.  I was wondering if it was worthwhile to include an architecture search of some kind in the empirical comparisons in the examples?  This is because if wider than needed hidden layers are used this will worsen some of the degeneracies of the weight space posterior which could make the weight space algorithms perform worse.  Also the authors use a Gaussian process approximation in part of their algorithm and wide hidden layers make that approximation more reasonable and may advantage their approach for that reason too.  The authors discuss in Appendix B other approaches to improving weight space POVI.  I wonder also if parameter constraints would be helpful for improving the performance of the weight space methods, such as order constraints on the hidden layer biases for example to remove at least some of the sources of unidentifiability.  The authors talk in the introduction about the difficulties of exploring a complex high-dimensional posterior, the curse of dimensionality, and the limitations of current variational families but only 20 points are used to represent the posterior in the examples.   Are many more particles required to obtain good performance in more complex models and does the approach scale well in terms of its computational requirements in that sense?", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper282/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Function Space Particle Optimization for Bayesian Neural Networks", "abstract": "While Bayesian neural networks (BNNs) have drawn increasing attention, their posterior inference remains challenging, due to the high-dimensional and over-parameterized nature. To address this issue, several highly flexible and scalable variational inference procedures based on the idea of particle optimization have been proposed. These methods directly optimize a set of particles to approximate the target posterior. However, their application to BNNs often yields sub-optimal performance, as such methods have a particular failure mode on over-parameterized models. In this paper, we propose to solve this issue by performing particle optimization directly in the space of regression functions. We demonstrate through extensive experiments that our method successfully overcomes this issue, and outperforms strong baselines in a variety of tasks including prediction, defense against adversarial examples, and reinforcement learning.", "keywords": ["Bayesian neural networks", "uncertainty estimation", "variational inference"], "authorids": ["wzy196@gmail.com", "rtz19970824@gmail.com", "dcszj@mail.tsinghua.edu.cn", "dcszb@mail.tsinghua.edu.cn"], "authors": ["Ziyu Wang", "Tongzheng Ren", "Jun Zhu", "Bo Zhang"], "pdf": "/pdf/fef83ae6842b76284e2de4d04977d1575d0efdf4.pdf", "paperhash": "wang|function_space_particle_optimization_for_bayesian_neural_networks", "_bibtex": "@inproceedings{\nwang2018function,\ntitle={Function Space Particle Optimization for Bayesian Neural Networks},\nauthor={Ziyu Wang and Tongzheng Ren and Jun Zhu and Bo Zhang},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BkgtDsCcKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper282/Official_Review", "cdate": 1542234497255, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "BkgtDsCcKQ", "replyto": "BkgtDsCcKQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper282/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335689485, "tmdate": 1552335689485, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper282/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 17}