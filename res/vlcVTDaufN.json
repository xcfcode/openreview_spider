{"notes": [{"id": "vlcVTDaufN", "original": "ajTcIgHh9h1", "number": 1783, "cdate": 1601308196773, "ddate": null, "tcdate": 1601308196773, "tmdate": 1614985773686, "tddate": null, "forum": "vlcVTDaufN", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Differentiable Combinatorial Losses through Generalized Gradients of Linear Programs", "authorids": ["gaox2@vcu.edu", "~Han_Zhang6", "~Aliakbar_Panahi1", "~Tom_Arodz1"], "authors": ["Xi Gao", "Han Zhang", "Aliakbar Panahi", "Tom Arodz"], "keywords": ["combinatorial optimization", "linear programs", "generalized gradient"], "abstract": "Combinatorial problems with linear objective function play a central role in many computer science applications, and efficient algorithms for solving them are well known. However, the solutions to these problems are not differentiable with respect to the parameters specifying the problem instance \u2013 for example, shortest distance between two nodes in a graph is not a differentiable function of graph edge weights. Recently, attempts to integrate combinatorial and, more broadly, convex optimization solvers into gradient-trained models resulted in several approaches for differentiating over the solution vector to the optimization problem. However, in many cases, the interest is in differentiating over only the objective value, not the solution vector, and using existing approaches introduces unnecessary overhead. Here, we show how to perform gradient descent directly over the objective value of the solution to combinatorial problems. We demonstrate advantage of the approach in examples involving sequence-to-sequence modeling using differentiable encoder-decoder architecture with softmax or Gumbel-softmax, and in weakly supervised learning involving a convolutional, residual feed-forward network for image classification.\n", "one-sentence_summary": "We show how to differentiate over the objective value of the optimal solution to a combinatorial problem, using a single run to a black-box combinatorial solver.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "gao|differentiable_combinatorial_losses_through_generalized_gradients_of_linear_programs", "supplementary_material": "/attachment/e56fe0ccd0473786e8e37abb77bcdd1e242c9b31.zip", "pdf": "/pdf/5f56218dc9cfdf7fd981ed0dbc31be519e834c14.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=8pd5Gf2fs", "_bibtex": "@misc{\ngao2021differentiable,\ntitle={Differentiable Combinatorial Losses through Generalized Gradients of Linear Programs},\nauthor={Xi Gao and Han Zhang and Aliakbar Panahi and Tom Arodz},\nyear={2021},\nurl={https://openreview.net/forum?id=vlcVTDaufN}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 11, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "oYZxMW8REAW", "original": null, "number": 1, "cdate": 1610040357690, "ddate": null, "tcdate": 1610040357690, "tmdate": 1610473947503, "tddate": null, "forum": "vlcVTDaufN", "replyto": "vlcVTDaufN", "invitation": "ICLR.cc/2021/Conference/Paper1783/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "This paper received high variance in the reviews.\n\nI personally agree with AnonReviewer4 that the theoretical results presented in this paper are well-known results on the sensitivity analysis of linear programs. See for instance \"Introduction to linear optimization\" by Bertsimas and Tsitsiklis, Chapter 5.\n\nMore generally, these results are a special case of Danskin's theorem and the envelope theorem:\nhttps://en.wikipedia.org/wiki/Danskin%27s_theorem\nhttps://en.wikipedia.org/wiki/Envelope_theorem\n\nClarke's generalized gradients are just subgradients in the case of convex functions, which is the case here.\n\nMy recommentation to the authors if they want to publish their work is to focus on the applications and to stop claiming novelty on the theoretical side."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Differentiable Combinatorial Losses through Generalized Gradients of Linear Programs", "authorids": ["gaox2@vcu.edu", "~Han_Zhang6", "~Aliakbar_Panahi1", "~Tom_Arodz1"], "authors": ["Xi Gao", "Han Zhang", "Aliakbar Panahi", "Tom Arodz"], "keywords": ["combinatorial optimization", "linear programs", "generalized gradient"], "abstract": "Combinatorial problems with linear objective function play a central role in many computer science applications, and efficient algorithms for solving them are well known. However, the solutions to these problems are not differentiable with respect to the parameters specifying the problem instance \u2013 for example, shortest distance between two nodes in a graph is not a differentiable function of graph edge weights. Recently, attempts to integrate combinatorial and, more broadly, convex optimization solvers into gradient-trained models resulted in several approaches for differentiating over the solution vector to the optimization problem. However, in many cases, the interest is in differentiating over only the objective value, not the solution vector, and using existing approaches introduces unnecessary overhead. Here, we show how to perform gradient descent directly over the objective value of the solution to combinatorial problems. We demonstrate advantage of the approach in examples involving sequence-to-sequence modeling using differentiable encoder-decoder architecture with softmax or Gumbel-softmax, and in weakly supervised learning involving a convolutional, residual feed-forward network for image classification.\n", "one-sentence_summary": "We show how to differentiate over the objective value of the optimal solution to a combinatorial problem, using a single run to a black-box combinatorial solver.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "gao|differentiable_combinatorial_losses_through_generalized_gradients_of_linear_programs", "supplementary_material": "/attachment/e56fe0ccd0473786e8e37abb77bcdd1e242c9b31.zip", "pdf": "/pdf/5f56218dc9cfdf7fd981ed0dbc31be519e834c14.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=8pd5Gf2fs", "_bibtex": "@misc{\ngao2021differentiable,\ntitle={Differentiable Combinatorial Losses through Generalized Gradients of Linear Programs},\nauthor={Xi Gao and Han Zhang and Aliakbar Panahi and Tom Arodz},\nyear={2021},\nurl={https://openreview.net/forum?id=vlcVTDaufN}\n}"}, "tags": [], "invitation": {"reply": {"forum": "vlcVTDaufN", "replyto": "vlcVTDaufN", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040357676, "tmdate": 1610473947485, "id": "ICLR.cc/2021/Conference/Paper1783/-/Decision"}}}, {"id": "hmI4mUbQyEt", "original": null, "number": 3, "cdate": 1606269906057, "ddate": null, "tcdate": 1606269906057, "tmdate": 1606302326917, "tddate": null, "forum": "vlcVTDaufN", "replyto": "Y_4QitrxrTY", "invitation": "ICLR.cc/2021/Conference/Paper1783/-/Official_Comment", "content": {"title": "Reply to reviewer 5", "comment": "Thank you for your comments and suggestions for improvements. Below, we address each of them:\n\n>> ... practitioners that do not have a background in combinatorial optimization may want to use it. The paper does not provide enough details to do so. I'd replace Algorithm 1 with a box specific to bipartite matching.\n\nWe have added Algorithm 2 with the outline of the procedure for bipartite matching example. In addition, in the Supplementary Material, we show the pytorch code for the Autograd class implementing differentiation of the optimal matching cost. We are preparing a github repository that will include the full code and experiments, not only for the bipartite matching but also for the language modeling experiments.\n\n\n>> You need to provide far more detail/background on differentiable decoding for the secnod set of experiments. It was unclear what Softmax/ Gumbel-Softmax meant. Is Softmax not the same as MLE?\n\nWe edited the manuscript to clarify the difference between these two approaches for providing the input to the next iteration of an RNN in a differentiable way. While the softmax approach directly uses the distribution over the vocabulary from the softmax output layer of the RNN as the input to the next step in the RNN for next word decoding, Gumbel-softmax uses instead a differentiable sample from that softmax-based output distribution. In both cases, for a single predicted word, we use a standard MLE/softmax cross-entropy approach for assessing the prediction loss.\n\n>> You write \"while this architecture is no longer the top performer in terms of ROUGE metric \u2013currently, large pre-trained self-attention models are the state-of-the-art \u2013 it is much more efficient intraining, allowing for experimenting with different loss functions.\" Is the speed difference really that much? \n\nLarge seq2seq models like GPT-2 that are pre-trained on the task of predicting the next word (i.e., the desired output is a shifted sequence) might benefit from using sentence-level pre-training via global sequence alignment and the combinatorial gradients, but our computational budget did not allow us to perform GPT-2 pre-training to test this hypothesis. \n\n>> Figure 1: why is cvxpy timing u-shaped?\n\nWe expanded the manuscript to provide an explanation of this phenomenon. For a sample bag of size b = 4,...,32, in each epoch involving m=50,000 images there are m/b individual bags, that is, b-to-b bipartite matching problems to be solved. For all methods, we see an initial drop in total time as the number of individual problems drops when b increases, and the time to solve each of the problems increases only moderately. But for large values of b, the time to solve each combinatorial problems grows more steeply for cvxpylayers, overtaking any gains from reducing the number of individual problems. This is not the case for the combinatorial solver, where increase in time to solve one problem is offset by the reduction in the number of problems, and the total time stays stable in the analyzed b=4,...,32 range. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1783/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1783/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Differentiable Combinatorial Losses through Generalized Gradients of Linear Programs", "authorids": ["gaox2@vcu.edu", "~Han_Zhang6", "~Aliakbar_Panahi1", "~Tom_Arodz1"], "authors": ["Xi Gao", "Han Zhang", "Aliakbar Panahi", "Tom Arodz"], "keywords": ["combinatorial optimization", "linear programs", "generalized gradient"], "abstract": "Combinatorial problems with linear objective function play a central role in many computer science applications, and efficient algorithms for solving them are well known. However, the solutions to these problems are not differentiable with respect to the parameters specifying the problem instance \u2013 for example, shortest distance between two nodes in a graph is not a differentiable function of graph edge weights. Recently, attempts to integrate combinatorial and, more broadly, convex optimization solvers into gradient-trained models resulted in several approaches for differentiating over the solution vector to the optimization problem. However, in many cases, the interest is in differentiating over only the objective value, not the solution vector, and using existing approaches introduces unnecessary overhead. Here, we show how to perform gradient descent directly over the objective value of the solution to combinatorial problems. We demonstrate advantage of the approach in examples involving sequence-to-sequence modeling using differentiable encoder-decoder architecture with softmax or Gumbel-softmax, and in weakly supervised learning involving a convolutional, residual feed-forward network for image classification.\n", "one-sentence_summary": "We show how to differentiate over the objective value of the optimal solution to a combinatorial problem, using a single run to a black-box combinatorial solver.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "gao|differentiable_combinatorial_losses_through_generalized_gradients_of_linear_programs", "supplementary_material": "/attachment/e56fe0ccd0473786e8e37abb77bcdd1e242c9b31.zip", "pdf": "/pdf/5f56218dc9cfdf7fd981ed0dbc31be519e834c14.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=8pd5Gf2fs", "_bibtex": "@misc{\ngao2021differentiable,\ntitle={Differentiable Combinatorial Losses through Generalized Gradients of Linear Programs},\nauthor={Xi Gao and Han Zhang and Aliakbar Panahi and Tom Arodz},\nyear={2021},\nurl={https://openreview.net/forum?id=vlcVTDaufN}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "vlcVTDaufN", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1783/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1783/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1783/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1783/Authors|ICLR.cc/2021/Conference/Paper1783/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1783/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923855753, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1783/-/Official_Comment"}}}, {"id": "7jSyT4CsZ1V", "original": null, "number": 7, "cdate": 1606270092326, "ddate": null, "tcdate": 1606270092326, "tmdate": 1606270092326, "tddate": null, "forum": "vlcVTDaufN", "replyto": "MOJj-92FseJ", "invitation": "ICLR.cc/2021/Conference/Paper1783/-/Official_Comment", "content": {"title": "Reply to reviewer 4", "comment": "Thank you for a detailed critique; we have performed additional experiments to address your comment about the performance comparisons.\n\nTo show performance benefits beyond the current implementations of differentiable convex programming layers, we have added the recently proposed approach for gradients of the solution vector of a combinatorial solver [Vlastelica Pogan\u010di\u0107\u202c et al., 2020] as another baseline. As expected, the proposed approach for differentiating the objective value is twice as fast as this baseline: it only solves the combinatorial problem once (in the forward pass), while the baseline method also solves a modified instance of the combinatorial problem in the backward pass, which they need to have the gradient of the optimal solution vector. \n\nThese results confirm that narrowing the focus from the more general problem of solution vector gradient that has received attention recently to the objective value gradient and to existing mathematical techniques for obtaining it results in performance gains, even when in both cases combinatorial, non-LP-based solvers are being used."}, "signatures": ["ICLR.cc/2021/Conference/Paper1783/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1783/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Differentiable Combinatorial Losses through Generalized Gradients of Linear Programs", "authorids": ["gaox2@vcu.edu", "~Han_Zhang6", "~Aliakbar_Panahi1", "~Tom_Arodz1"], "authors": ["Xi Gao", "Han Zhang", "Aliakbar Panahi", "Tom Arodz"], "keywords": ["combinatorial optimization", "linear programs", "generalized gradient"], "abstract": "Combinatorial problems with linear objective function play a central role in many computer science applications, and efficient algorithms for solving them are well known. However, the solutions to these problems are not differentiable with respect to the parameters specifying the problem instance \u2013 for example, shortest distance between two nodes in a graph is not a differentiable function of graph edge weights. Recently, attempts to integrate combinatorial and, more broadly, convex optimization solvers into gradient-trained models resulted in several approaches for differentiating over the solution vector to the optimization problem. However, in many cases, the interest is in differentiating over only the objective value, not the solution vector, and using existing approaches introduces unnecessary overhead. Here, we show how to perform gradient descent directly over the objective value of the solution to combinatorial problems. We demonstrate advantage of the approach in examples involving sequence-to-sequence modeling using differentiable encoder-decoder architecture with softmax or Gumbel-softmax, and in weakly supervised learning involving a convolutional, residual feed-forward network for image classification.\n", "one-sentence_summary": "We show how to differentiate over the objective value of the optimal solution to a combinatorial problem, using a single run to a black-box combinatorial solver.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "gao|differentiable_combinatorial_losses_through_generalized_gradients_of_linear_programs", "supplementary_material": "/attachment/e56fe0ccd0473786e8e37abb77bcdd1e242c9b31.zip", "pdf": "/pdf/5f56218dc9cfdf7fd981ed0dbc31be519e834c14.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=8pd5Gf2fs", "_bibtex": "@misc{\ngao2021differentiable,\ntitle={Differentiable Combinatorial Losses through Generalized Gradients of Linear Programs},\nauthor={Xi Gao and Han Zhang and Aliakbar Panahi and Tom Arodz},\nyear={2021},\nurl={https://openreview.net/forum?id=vlcVTDaufN}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "vlcVTDaufN", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1783/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1783/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1783/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1783/Authors|ICLR.cc/2021/Conference/Paper1783/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1783/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923855753, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1783/-/Official_Comment"}}}, {"id": "pk32YXrcDhD", "original": null, "number": 6, "cdate": 1606270035488, "ddate": null, "tcdate": 1606270035488, "tmdate": 1606270035488, "tddate": null, "forum": "vlcVTDaufN", "replyto": "sOfvPLz93I", "invitation": "ICLR.cc/2021/Conference/Paper1783/-/Official_Comment", "content": {"title": "Reply to reviewer 3", "comment": "Thank you for your comments and suggestions. We address them below:\n\n>> On the other hand, the novelty of the method may be a little overstated. In particular, it is known that using a generic LP solver is oftentimes not the most efficient way of computing the gradient, and that specialized combinatorial solver should be used. (...) For the problem of GSA, which corresponds to using a dynamic time warping loss, solving the small LP is done using dynamic programming. Using a DTW loss on top of a deep neural network has already been studied (see e.g. the cited Mensch and Blondel paper, where the authors solve the LP using DP). Using a generic LP solver such as the one in cvxpy is a little naive in that case, and it not surprising that it performs poorly.\n\nThe method by Mench and Blondel relies on re-implementing dynamic programming with the maximum operation replaced by its smoothed version. The method we propose works with existing combinatorial or LP solvers. To provide a comparison beyond LP solvers implemented in cvxpylayers, in the updated manuscript we added experiments with a recently proposed method for differentiating the combinatorial problem's solution vector using a combinatorial solver [Vlastelica Pogan\u010di\u0107\u202c et al., 2020]. To find the gradient of the solution vector, that approach requires solving two instances of the combinatorial problem, one in the forward phase, one in the backwards phase. Our method, by focusing on the gradient of the objective value only, which is what we need in the analyzed use cases, requires only one call to the combinatorial solver; experiments conclude that it is twice as fast. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1783/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1783/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Differentiable Combinatorial Losses through Generalized Gradients of Linear Programs", "authorids": ["gaox2@vcu.edu", "~Han_Zhang6", "~Aliakbar_Panahi1", "~Tom_Arodz1"], "authors": ["Xi Gao", "Han Zhang", "Aliakbar Panahi", "Tom Arodz"], "keywords": ["combinatorial optimization", "linear programs", "generalized gradient"], "abstract": "Combinatorial problems with linear objective function play a central role in many computer science applications, and efficient algorithms for solving them are well known. However, the solutions to these problems are not differentiable with respect to the parameters specifying the problem instance \u2013 for example, shortest distance between two nodes in a graph is not a differentiable function of graph edge weights. Recently, attempts to integrate combinatorial and, more broadly, convex optimization solvers into gradient-trained models resulted in several approaches for differentiating over the solution vector to the optimization problem. However, in many cases, the interest is in differentiating over only the objective value, not the solution vector, and using existing approaches introduces unnecessary overhead. Here, we show how to perform gradient descent directly over the objective value of the solution to combinatorial problems. We demonstrate advantage of the approach in examples involving sequence-to-sequence modeling using differentiable encoder-decoder architecture with softmax or Gumbel-softmax, and in weakly supervised learning involving a convolutional, residual feed-forward network for image classification.\n", "one-sentence_summary": "We show how to differentiate over the objective value of the optimal solution to a combinatorial problem, using a single run to a black-box combinatorial solver.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "gao|differentiable_combinatorial_losses_through_generalized_gradients_of_linear_programs", "supplementary_material": "/attachment/e56fe0ccd0473786e8e37abb77bcdd1e242c9b31.zip", "pdf": "/pdf/5f56218dc9cfdf7fd981ed0dbc31be519e834c14.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=8pd5Gf2fs", "_bibtex": "@misc{\ngao2021differentiable,\ntitle={Differentiable Combinatorial Losses through Generalized Gradients of Linear Programs},\nauthor={Xi Gao and Han Zhang and Aliakbar Panahi and Tom Arodz},\nyear={2021},\nurl={https://openreview.net/forum?id=vlcVTDaufN}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "vlcVTDaufN", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1783/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1783/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1783/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1783/Authors|ICLR.cc/2021/Conference/Paper1783/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1783/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923855753, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1783/-/Official_Comment"}}}, {"id": "yVhPBDFXBxj", "original": null, "number": 5, "cdate": 1606269990329, "ddate": null, "tcdate": 1606269990329, "tmdate": 1606269990329, "tddate": null, "forum": "vlcVTDaufN", "replyto": "a3fg_-gk_oR", "invitation": "ICLR.cc/2021/Conference/Paper1783/-/Official_Comment", "content": {"title": "Reply to reviewer 1", "comment": "Thank you for critique and suggestions for improvements. We address them below:\n\n>> The biggest weakness is the lack of a comparison with related approaches for differentiating through combinatorial losses, such as some of the approaches discussed in the introduction as [Pogancic 2020] that consider similar problems. (...) it would be significantly more convincing to empirically compare to approaches that differentiate through the combinatorial losses.\n\nWe have expanded the manuscript to include experiments with the approach proposed in [Pogancic 2020]. As expected, their approach is twice as slow, due to the fact that it requires two calls to the combinatorial solver instead of one in our approach. The need for the second call in [Pogancic 2020] arises from the difference in the goals of the two approaches: ours is focused on the gradient of the objective value directly, while theirs is focused on the gradient of the solution vector, from which the objective value can be obtained if needed via a differentiable operation. \n\n>> Are the approaches using the same definition of a derivative?  [Pogancic 2020] discusses an issue with the real derivative through combinatorial optimization being uninformative or near-zero everywhere, is this also an issue in the setting here? Can this approach be seen as an approximation or surrogate to the derivative of the combinatorial problem as the other approaches?\n\nThe gradient of the solution vector is null everywhere, and requires interpolation as in [Pogancic 2020]. On the other hand, the gradient of the objective value is not constant, and we do not need any interpolation/approximation. \n\n>> How should the gradients of the continuous baselines (with CVXPY) compare to the method being proposed in the experiments? If they're using the ideal formulation LP, should they be the same in theory (as Figure 1 validates), but in practice due to solver errors, gives suboptimal directions (as Table 1 shows)?\n\nIndeed, the gradient resulting from our approach and the gradient resulting from the use of cvxpylayers should in principle be the same.\n\n>> If I understand correctly, this approach requires a known mapping from the combinatorial problem to the ILP, and from the ILP to the LP, which could make it more involved to apply than some of the related methods that don't require knowing this information.\n\nThe mapping of the combinatorial problem to a corresponding LP is indeed needed, conceptually, to specify how the parameters defining the combinatorial problem relate to the vectors/matrices specifying the LP and, for the gradients, how the combinatorial solution vector relates to the primal/dual solution vectors. That does not mean, however, the detailed LP for a specific instance needs to be spelled out.\n\n>> The last paragraph of the introduction presents a form of the criterion with a loss and the combinatorial objective value with notation that's not used later on in the paper. (...) Page 2, second paragraph: The last sentence on differentiable continuous LPs/QPs seems separate from the rest of paragraph on combinatorial solvers.\n\nThank you for pointing it out - we have edited the manuscript to remove these inconsistencies. "}, "signatures": ["ICLR.cc/2021/Conference/Paper1783/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1783/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Differentiable Combinatorial Losses through Generalized Gradients of Linear Programs", "authorids": ["gaox2@vcu.edu", "~Han_Zhang6", "~Aliakbar_Panahi1", "~Tom_Arodz1"], "authors": ["Xi Gao", "Han Zhang", "Aliakbar Panahi", "Tom Arodz"], "keywords": ["combinatorial optimization", "linear programs", "generalized gradient"], "abstract": "Combinatorial problems with linear objective function play a central role in many computer science applications, and efficient algorithms for solving them are well known. However, the solutions to these problems are not differentiable with respect to the parameters specifying the problem instance \u2013 for example, shortest distance between two nodes in a graph is not a differentiable function of graph edge weights. Recently, attempts to integrate combinatorial and, more broadly, convex optimization solvers into gradient-trained models resulted in several approaches for differentiating over the solution vector to the optimization problem. However, in many cases, the interest is in differentiating over only the objective value, not the solution vector, and using existing approaches introduces unnecessary overhead. Here, we show how to perform gradient descent directly over the objective value of the solution to combinatorial problems. We demonstrate advantage of the approach in examples involving sequence-to-sequence modeling using differentiable encoder-decoder architecture with softmax or Gumbel-softmax, and in weakly supervised learning involving a convolutional, residual feed-forward network for image classification.\n", "one-sentence_summary": "We show how to differentiate over the objective value of the optimal solution to a combinatorial problem, using a single run to a black-box combinatorial solver.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "gao|differentiable_combinatorial_losses_through_generalized_gradients_of_linear_programs", "supplementary_material": "/attachment/e56fe0ccd0473786e8e37abb77bcdd1e242c9b31.zip", "pdf": "/pdf/5f56218dc9cfdf7fd981ed0dbc31be519e834c14.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=8pd5Gf2fs", "_bibtex": "@misc{\ngao2021differentiable,\ntitle={Differentiable Combinatorial Losses through Generalized Gradients of Linear Programs},\nauthor={Xi Gao and Han Zhang and Aliakbar Panahi and Tom Arodz},\nyear={2021},\nurl={https://openreview.net/forum?id=vlcVTDaufN}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "vlcVTDaufN", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1783/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1783/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1783/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1783/Authors|ICLR.cc/2021/Conference/Paper1783/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1783/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923855753, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1783/-/Official_Comment"}}}, {"id": "7qSezy0tElK", "original": null, "number": 4, "cdate": 1606269942336, "ddate": null, "tcdate": 1606269942336, "tmdate": 1606269942336, "tddate": null, "forum": "vlcVTDaufN", "replyto": "O1D80eB4gwi", "invitation": "ICLR.cc/2021/Conference/Paper1783/-/Official_Comment", "content": {"title": "Reply to reviewer 2", "comment": "Thank you for your comments. \n\nWe have added a comparison with a recently proposed method for differentiating the solution vector using combinatorial solvers, to further illustrate the performance benefits resulting from the proposed approach."}, "signatures": ["ICLR.cc/2021/Conference/Paper1783/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1783/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Differentiable Combinatorial Losses through Generalized Gradients of Linear Programs", "authorids": ["gaox2@vcu.edu", "~Han_Zhang6", "~Aliakbar_Panahi1", "~Tom_Arodz1"], "authors": ["Xi Gao", "Han Zhang", "Aliakbar Panahi", "Tom Arodz"], "keywords": ["combinatorial optimization", "linear programs", "generalized gradient"], "abstract": "Combinatorial problems with linear objective function play a central role in many computer science applications, and efficient algorithms for solving them are well known. However, the solutions to these problems are not differentiable with respect to the parameters specifying the problem instance \u2013 for example, shortest distance between two nodes in a graph is not a differentiable function of graph edge weights. Recently, attempts to integrate combinatorial and, more broadly, convex optimization solvers into gradient-trained models resulted in several approaches for differentiating over the solution vector to the optimization problem. However, in many cases, the interest is in differentiating over only the objective value, not the solution vector, and using existing approaches introduces unnecessary overhead. Here, we show how to perform gradient descent directly over the objective value of the solution to combinatorial problems. We demonstrate advantage of the approach in examples involving sequence-to-sequence modeling using differentiable encoder-decoder architecture with softmax or Gumbel-softmax, and in weakly supervised learning involving a convolutional, residual feed-forward network for image classification.\n", "one-sentence_summary": "We show how to differentiate over the objective value of the optimal solution to a combinatorial problem, using a single run to a black-box combinatorial solver.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "gao|differentiable_combinatorial_losses_through_generalized_gradients_of_linear_programs", "supplementary_material": "/attachment/e56fe0ccd0473786e8e37abb77bcdd1e242c9b31.zip", "pdf": "/pdf/5f56218dc9cfdf7fd981ed0dbc31be519e834c14.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=8pd5Gf2fs", "_bibtex": "@misc{\ngao2021differentiable,\ntitle={Differentiable Combinatorial Losses through Generalized Gradients of Linear Programs},\nauthor={Xi Gao and Han Zhang and Aliakbar Panahi and Tom Arodz},\nyear={2021},\nurl={https://openreview.net/forum?id=vlcVTDaufN}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "vlcVTDaufN", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1783/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1783/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1783/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1783/Authors|ICLR.cc/2021/Conference/Paper1783/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1783/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923855753, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1783/-/Official_Comment"}}}, {"id": "MOJj-92FseJ", "original": null, "number": 1, "cdate": 1603758127316, "ddate": null, "tcdate": 1603758127316, "tmdate": 1605024358308, "tddate": null, "forum": "vlcVTDaufN", "replyto": "vlcVTDaufN", "invitation": "ICLR.cc/2021/Conference/Paper1783/-/Official_Review", "content": {"title": "Differentiating over the objective of a linear (integer) program is not an interesting problem", "review": "The value of the optimal objective as a function of the cost vector $c$ can be written as $z^*(c) = c^T u^*(c)$ where the optimal solution $u^*$ also depends on $c$. The function $u^*(c)$ is piecewise constant -- there are finitely (resp. countably) many feasible solutions; candidates for $u^*$ -- and so the function $z^*(c)$ is a piecewise linear function of $c$, with gradient $u^*(c)$, wherever it exists (otherwise there is analogous subgradient). Obviously, all it takes for computing $u^*(c)$ is solving -- anyhow -- the combinatorial problem. This is all trivial and well-known, yet the authors do precisely that.\n\nCan it be saved by proposing gradients also of w.r.t. constraints? No. These results are (slightly) less trivial but -- as authors admit -- are known since 1975. Moreover, the gradient with respect to $c$ is the only one used in experiments, as far as I understand.\n\nIs there independent value in Theorem 1? I do not see it. It seems to be a bulky wrapper around the classical result. It only introduces some sort of transition from a vector specifying a combinatorial problem to a collection of vectors/matrices specifying an integer program. Also, the central concept of generalized gradient merely provides a formal framework to talk about non-unique gradients at boundary regions -- similarly to subgradient, subdifferential -- for the method itself, it has no specific relevance.\n\nThe claims of better performance compared to cvxpy are also absolutely non-surprising -- cvxpy currently uses a slightly suboptimal -- and a very expensive -- solver for linear programs. That is all.\n", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper1783/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1783/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Differentiable Combinatorial Losses through Generalized Gradients of Linear Programs", "authorids": ["gaox2@vcu.edu", "~Han_Zhang6", "~Aliakbar_Panahi1", "~Tom_Arodz1"], "authors": ["Xi Gao", "Han Zhang", "Aliakbar Panahi", "Tom Arodz"], "keywords": ["combinatorial optimization", "linear programs", "generalized gradient"], "abstract": "Combinatorial problems with linear objective function play a central role in many computer science applications, and efficient algorithms for solving them are well known. However, the solutions to these problems are not differentiable with respect to the parameters specifying the problem instance \u2013 for example, shortest distance between two nodes in a graph is not a differentiable function of graph edge weights. Recently, attempts to integrate combinatorial and, more broadly, convex optimization solvers into gradient-trained models resulted in several approaches for differentiating over the solution vector to the optimization problem. However, in many cases, the interest is in differentiating over only the objective value, not the solution vector, and using existing approaches introduces unnecessary overhead. Here, we show how to perform gradient descent directly over the objective value of the solution to combinatorial problems. We demonstrate advantage of the approach in examples involving sequence-to-sequence modeling using differentiable encoder-decoder architecture with softmax or Gumbel-softmax, and in weakly supervised learning involving a convolutional, residual feed-forward network for image classification.\n", "one-sentence_summary": "We show how to differentiate over the objective value of the optimal solution to a combinatorial problem, using a single run to a black-box combinatorial solver.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "gao|differentiable_combinatorial_losses_through_generalized_gradients_of_linear_programs", "supplementary_material": "/attachment/e56fe0ccd0473786e8e37abb77bcdd1e242c9b31.zip", "pdf": "/pdf/5f56218dc9cfdf7fd981ed0dbc31be519e834c14.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=8pd5Gf2fs", "_bibtex": "@misc{\ngao2021differentiable,\ntitle={Differentiable Combinatorial Losses through Generalized Gradients of Linear Programs},\nauthor={Xi Gao and Han Zhang and Aliakbar Panahi and Tom Arodz},\nyear={2021},\nurl={https://openreview.net/forum?id=vlcVTDaufN}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "vlcVTDaufN", "replyto": "vlcVTDaufN", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1783/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538110738, "tmdate": 1606915760563, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1783/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1783/-/Official_Review"}}}, {"id": "sOfvPLz93I", "original": null, "number": 2, "cdate": 1603838328306, "ddate": null, "tcdate": 1603838328306, "tmdate": 1605024358244, "tddate": null, "forum": "vlcVTDaufN", "replyto": "vlcVTDaufN", "invitation": "ICLR.cc/2021/Conference/Paper1783/-/Official_Review", "content": {"title": "Well presented generic method for efficiently using combinatorial LP layers - albeit not completely novel", "review": "Summary\n-------\n\nThe authors propose a simple method to optimize objective values defined as the\noptimal value of a combinatorial integer linear program, whose parameter depends on the\noutput of a certain model. \n\nFor this, they note that generalized gradient of such objective values are\nefficiently computed using the primal and dual solution of the ILP itself. In\nparticular, the ILP can be solved using specialized and efficient solver,\ninstead of solving a generic LP, as proposed in concurrent work.\n\nThe authors propose two example applications, that are described precisely, and validated against generic LP solving approaches. Using combinatorial specialized solvers outperform generic LP solving approaches in term of computation, and in term of validation metrics, as generic LP solving is hindered by errors which makes the learning process diverge.\n\nReview\n------\n\nThe paper is well written and well organized. The theoretical aspects are well documented, and the examples are introduced precisely and pedagogically.\n\nThe method itself is interesting as it ensures that the generalized gradient of\nmany ILP problems are computable efficiently. Theorem 1 states those guarantees, and many examples are discussed.\n\nOn the other hand, the novelty of the method may be a little overstated. In\nparticular, it is known that using a generic LP solver is oftentimes not the\nmost efficient way of computing the gradient, and that specialized combinatorial\nsolver should be used. \n\nFor the problem of GSA, which corresponds to using a dynamic time warping loss,\nsolving the small LP is done using dynamic programming. Using a DTW loss on top\nof a deep neural network has already been studied (see e.g. the cited Mensch and\nBlondel paper, where the authors solve the LP using DP). Using a generic LP\nsolver such as the one in cvxpy is a little naive in that case, and it not\nsurprising that it performs poorly. \n\nIn this example, we only require the gradient with respect to the cost (P is\n\"primal-pdfiff-efficient\"). Arguably, this manuscript also enable us to\nbackpropagate through parametrized constraints, using the formula proposed in\nTheorem 1, which is little known in this community.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1783/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1783/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Differentiable Combinatorial Losses through Generalized Gradients of Linear Programs", "authorids": ["gaox2@vcu.edu", "~Han_Zhang6", "~Aliakbar_Panahi1", "~Tom_Arodz1"], "authors": ["Xi Gao", "Han Zhang", "Aliakbar Panahi", "Tom Arodz"], "keywords": ["combinatorial optimization", "linear programs", "generalized gradient"], "abstract": "Combinatorial problems with linear objective function play a central role in many computer science applications, and efficient algorithms for solving them are well known. However, the solutions to these problems are not differentiable with respect to the parameters specifying the problem instance \u2013 for example, shortest distance between two nodes in a graph is not a differentiable function of graph edge weights. Recently, attempts to integrate combinatorial and, more broadly, convex optimization solvers into gradient-trained models resulted in several approaches for differentiating over the solution vector to the optimization problem. However, in many cases, the interest is in differentiating over only the objective value, not the solution vector, and using existing approaches introduces unnecessary overhead. Here, we show how to perform gradient descent directly over the objective value of the solution to combinatorial problems. We demonstrate advantage of the approach in examples involving sequence-to-sequence modeling using differentiable encoder-decoder architecture with softmax or Gumbel-softmax, and in weakly supervised learning involving a convolutional, residual feed-forward network for image classification.\n", "one-sentence_summary": "We show how to differentiate over the objective value of the optimal solution to a combinatorial problem, using a single run to a black-box combinatorial solver.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "gao|differentiable_combinatorial_losses_through_generalized_gradients_of_linear_programs", "supplementary_material": "/attachment/e56fe0ccd0473786e8e37abb77bcdd1e242c9b31.zip", "pdf": "/pdf/5f56218dc9cfdf7fd981ed0dbc31be519e834c14.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=8pd5Gf2fs", "_bibtex": "@misc{\ngao2021differentiable,\ntitle={Differentiable Combinatorial Losses through Generalized Gradients of Linear Programs},\nauthor={Xi Gao and Han Zhang and Aliakbar Panahi and Tom Arodz},\nyear={2021},\nurl={https://openreview.net/forum?id=vlcVTDaufN}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "vlcVTDaufN", "replyto": "vlcVTDaufN", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1783/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538110738, "tmdate": 1606915760563, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1783/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1783/-/Official_Review"}}}, {"id": "a3fg_-gk_oR", "original": null, "number": 3, "cdate": 1603892756936, "ddate": null, "tcdate": 1603892756936, "tmdate": 1605024358179, "tddate": null, "forum": "vlcVTDaufN", "replyto": "vlcVTDaufN", "invitation": "ICLR.cc/2021/Conference/Paper1783/-/Official_Review", "content": {"title": "Novel approach, comparisons don't compare to other combinatorial optimization differentiation methods", "review": "This paper shows how to differentiate through combinatorial\nlosses by differentiating through the ideal formulation LP.\nUnderstanding how to differentiate through combinatorial\noptimization so that it can be used as part of the model or\nloss is important as it captures many natural operations.\nI am giving this a weak accept as it is a novel approach\nfor differentiation that the community can build on,\nbut the positioning and relation to prior work and\nempirical comparisons could be stronger (more details below).\n\n# Strengths\nTo the best of my knowledge this is a novel approach that\nmakes the elegant and natural connections going from\na combinatorial problem to an ILP to an LP\nto differentiating through the LP using known methods.\n\n# Weaknesses\nThe biggest weakness is the lack of a comparison with related\napproaches for differentiating through combinatorial losses,\nsuch as some of the approaches discussed in the introduction\nas [Pogancic 2020] that consider similar problems.\nThe experimental settings considered in this paper compare to\nbaselines that *don't* differentiate through the combinatorial\naspect of the problem. While this is a great step of validating\nthe power of these approaches, I think that it would be significantly\nmore convincing to empirically compare to approaches that\ndifferentiate through the combinatorial losses.\n\nI also think it's important to discuss the comparisons to the\nrelated approaches for differentiating through parameterized\ncombinatorial optimization. Are the approaches using the same\ndefinition of a derivative? [Pogancic 2020] discusses an issue\nwith the real derivative through combinatorial optimization being\nuninformative or near-zero everywhere, is this also an issue in\nthe setting here?\nCan this approach be seen as an approximation or surrogate to\nthe derivative of the combinatorial problem as the other approaches?\n\nIf I understand correctly, this approach requires a known mapping from\nthe combinatorial problem to the ILP, and from the ILP to the LP,\nwhich could make it more involved to apply than some of the related\nmethods that don't require knowing this information.\n\n# Other questions and comments\nHow should the gradients of the continuous baselines (with CVXPY)\ncompare to the method being proposed in the experiments?\nIf they're using the ideal formulation LP, should they be the\nsame in theory (as Figure 1 validates), but in practice due\nto solver errors, gives suboptimal directions (as Table 1 shows)?\n\nThe last paragraph of the introduction presents a form of the\ncriterion with a loss and the combinatorial objective value\nwith notation that's not used later on in the paper.\n\nPage 2, second paragraph: The last sentence on differentiable\ncontinuous LPs/QPs seems separate from the rest of paragraph\non combinatorial solvers.", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1783/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1783/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Differentiable Combinatorial Losses through Generalized Gradients of Linear Programs", "authorids": ["gaox2@vcu.edu", "~Han_Zhang6", "~Aliakbar_Panahi1", "~Tom_Arodz1"], "authors": ["Xi Gao", "Han Zhang", "Aliakbar Panahi", "Tom Arodz"], "keywords": ["combinatorial optimization", "linear programs", "generalized gradient"], "abstract": "Combinatorial problems with linear objective function play a central role in many computer science applications, and efficient algorithms for solving them are well known. However, the solutions to these problems are not differentiable with respect to the parameters specifying the problem instance \u2013 for example, shortest distance between two nodes in a graph is not a differentiable function of graph edge weights. Recently, attempts to integrate combinatorial and, more broadly, convex optimization solvers into gradient-trained models resulted in several approaches for differentiating over the solution vector to the optimization problem. However, in many cases, the interest is in differentiating over only the objective value, not the solution vector, and using existing approaches introduces unnecessary overhead. Here, we show how to perform gradient descent directly over the objective value of the solution to combinatorial problems. We demonstrate advantage of the approach in examples involving sequence-to-sequence modeling using differentiable encoder-decoder architecture with softmax or Gumbel-softmax, and in weakly supervised learning involving a convolutional, residual feed-forward network for image classification.\n", "one-sentence_summary": "We show how to differentiate over the objective value of the optimal solution to a combinatorial problem, using a single run to a black-box combinatorial solver.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "gao|differentiable_combinatorial_losses_through_generalized_gradients_of_linear_programs", "supplementary_material": "/attachment/e56fe0ccd0473786e8e37abb77bcdd1e242c9b31.zip", "pdf": "/pdf/5f56218dc9cfdf7fd981ed0dbc31be519e834c14.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=8pd5Gf2fs", "_bibtex": "@misc{\ngao2021differentiable,\ntitle={Differentiable Combinatorial Losses through Generalized Gradients of Linear Programs},\nauthor={Xi Gao and Han Zhang and Aliakbar Panahi and Tom Arodz},\nyear={2021},\nurl={https://openreview.net/forum?id=vlcVTDaufN}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "vlcVTDaufN", "replyto": "vlcVTDaufN", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1783/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538110738, "tmdate": 1606915760563, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1783/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1783/-/Official_Review"}}}, {"id": "O1D80eB4gwi", "original": null, "number": 4, "cdate": 1604274715703, "ddate": null, "tcdate": 1604274715703, "tmdate": 1605024358095, "tddate": null, "forum": "vlcVTDaufN", "replyto": "vlcVTDaufN", "invitation": "ICLR.cc/2021/Conference/Paper1783/-/Official_Review", "content": {"title": "review for \"Differentiable Combinatorial Losses through Generalized Gradients of Linear Programs\"", "review": "The authors present a technique to integrate combinatorial optimization sub-problems into a gradient descent based application. The approach they describe relies only on differentiation of the value of the combinatorial program (instead of the solution vector), and can be done with relatively low overhead (compared to techniques that involve modifying combinatorial algorithms to differentiable elements, or the use of differentiable linear/quadratic programming layers)\n\nThey motivate and show the advantages of their approach using two natural and useful examples. The experimental results show promise, and the paper is well written, and motivated.\n\n", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1783/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1783/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Differentiable Combinatorial Losses through Generalized Gradients of Linear Programs", "authorids": ["gaox2@vcu.edu", "~Han_Zhang6", "~Aliakbar_Panahi1", "~Tom_Arodz1"], "authors": ["Xi Gao", "Han Zhang", "Aliakbar Panahi", "Tom Arodz"], "keywords": ["combinatorial optimization", "linear programs", "generalized gradient"], "abstract": "Combinatorial problems with linear objective function play a central role in many computer science applications, and efficient algorithms for solving them are well known. However, the solutions to these problems are not differentiable with respect to the parameters specifying the problem instance \u2013 for example, shortest distance between two nodes in a graph is not a differentiable function of graph edge weights. Recently, attempts to integrate combinatorial and, more broadly, convex optimization solvers into gradient-trained models resulted in several approaches for differentiating over the solution vector to the optimization problem. However, in many cases, the interest is in differentiating over only the objective value, not the solution vector, and using existing approaches introduces unnecessary overhead. Here, we show how to perform gradient descent directly over the objective value of the solution to combinatorial problems. We demonstrate advantage of the approach in examples involving sequence-to-sequence modeling using differentiable encoder-decoder architecture with softmax or Gumbel-softmax, and in weakly supervised learning involving a convolutional, residual feed-forward network for image classification.\n", "one-sentence_summary": "We show how to differentiate over the objective value of the optimal solution to a combinatorial problem, using a single run to a black-box combinatorial solver.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "gao|differentiable_combinatorial_losses_through_generalized_gradients_of_linear_programs", "supplementary_material": "/attachment/e56fe0ccd0473786e8e37abb77bcdd1e242c9b31.zip", "pdf": "/pdf/5f56218dc9cfdf7fd981ed0dbc31be519e834c14.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=8pd5Gf2fs", "_bibtex": "@misc{\ngao2021differentiable,\ntitle={Differentiable Combinatorial Losses through Generalized Gradients of Linear Programs},\nauthor={Xi Gao and Han Zhang and Aliakbar Panahi and Tom Arodz},\nyear={2021},\nurl={https://openreview.net/forum?id=vlcVTDaufN}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "vlcVTDaufN", "replyto": "vlcVTDaufN", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1783/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538110738, "tmdate": 1606915760563, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1783/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1783/-/Official_Review"}}}, {"id": "Y_4QitrxrTY", "original": null, "number": 5, "cdate": 1604677679978, "ddate": null, "tcdate": 1604677679978, "tmdate": 1605024358026, "tddate": null, "forum": "vlcVTDaufN", "replyto": "vlcVTDaufN", "invitation": "ICLR.cc/2021/Conference/Paper1783/-/Official_Review", "content": {"title": "Interesting general method, but needs far more details about the resulting algorithm for concrete use-cases.", "review": "=quality=Proposed method is significantly more practical, both in terms of ease of implementation and speed, than prior related work for minimizing combinatorial losses. The paper's exposition needs to be improved considerably, though (see below).\n\n=originality=The paper draws on advanced concepts from combinatorial optimization that may be unfamiliar to many ICLR readers, but that have the potential for large impact.\n\n= significance=The paper's proposed method is practical for very common ML setups in NLP and computer vision that are used day-to-day.\n\n= Pros =\u00a0Proposed method is interesting, practical, and relatively easy to implement.\n\n= Cons =\u00a0Paper writing omits many key details necessary to use the method in practice. Experiments build the method on top of out-\ndated models and do not demonstrate that the method could be used with modern models (e.g. attention-based decoders).\n\n==Comments==I appreciate that you provide a very general recipe for constructing the differentiable combinatorial layers. However, the paper provides far too few details for the particular problems (bipartite matching and sequence alignment) that appear in the experiments. The supplementary material does not help. Your method is promising, and practitioners that do not have a background in combinatorial optimization may want to use it. The paper does not provide enough details to do so. I'd replace Algorithm 1 with a box specific to bipartite matching.\n\nYou need to provide far more detail/background on differentiable decoding for the secnod set of experiments. It was unclear what Softmax/ Gumbel-Softmax meant. Is Softmax not the same as MLE?\n\nWhile the second set of experiments provides useful ablation analysis of the impact of your method, it builds on an out-dated model. You write \"while this architecture is no longer the top performer in terms of ROUGE metric \u2013currently, large pre-trained self-attention models are the state-of-the-art \u2013 it is much more efficient intraining, allowing for experimenting with different loss functions.\" Is the speed difference really that much? I'm surprised that that makes a difference in terms of which experiments are feasible.\n\nFigure 1: why is cvxpy timing u-shaped?", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1783/AnonReviewer5"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1783/AnonReviewer5"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Differentiable Combinatorial Losses through Generalized Gradients of Linear Programs", "authorids": ["gaox2@vcu.edu", "~Han_Zhang6", "~Aliakbar_Panahi1", "~Tom_Arodz1"], "authors": ["Xi Gao", "Han Zhang", "Aliakbar Panahi", "Tom Arodz"], "keywords": ["combinatorial optimization", "linear programs", "generalized gradient"], "abstract": "Combinatorial problems with linear objective function play a central role in many computer science applications, and efficient algorithms for solving them are well known. However, the solutions to these problems are not differentiable with respect to the parameters specifying the problem instance \u2013 for example, shortest distance between two nodes in a graph is not a differentiable function of graph edge weights. Recently, attempts to integrate combinatorial and, more broadly, convex optimization solvers into gradient-trained models resulted in several approaches for differentiating over the solution vector to the optimization problem. However, in many cases, the interest is in differentiating over only the objective value, not the solution vector, and using existing approaches introduces unnecessary overhead. Here, we show how to perform gradient descent directly over the objective value of the solution to combinatorial problems. We demonstrate advantage of the approach in examples involving sequence-to-sequence modeling using differentiable encoder-decoder architecture with softmax or Gumbel-softmax, and in weakly supervised learning involving a convolutional, residual feed-forward network for image classification.\n", "one-sentence_summary": "We show how to differentiate over the objective value of the optimal solution to a combinatorial problem, using a single run to a black-box combinatorial solver.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "gao|differentiable_combinatorial_losses_through_generalized_gradients_of_linear_programs", "supplementary_material": "/attachment/e56fe0ccd0473786e8e37abb77bcdd1e242c9b31.zip", "pdf": "/pdf/5f56218dc9cfdf7fd981ed0dbc31be519e834c14.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=8pd5Gf2fs", "_bibtex": "@misc{\ngao2021differentiable,\ntitle={Differentiable Combinatorial Losses through Generalized Gradients of Linear Programs},\nauthor={Xi Gao and Han Zhang and Aliakbar Panahi and Tom Arodz},\nyear={2021},\nurl={https://openreview.net/forum?id=vlcVTDaufN}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "vlcVTDaufN", "replyto": "vlcVTDaufN", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1783/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538110738, "tmdate": 1606915760563, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1783/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1783/-/Official_Review"}}}], "count": 12}