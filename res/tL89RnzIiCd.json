{"notes": [{"id": "tL89RnzIiCd", "original": "TbWbwAsplhu", "number": 3489, "cdate": 1601308387179, "ddate": null, "tcdate": 1601308387179, "tmdate": 1615989928657, "tddate": null, "forum": "tL89RnzIiCd", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Hopfield Networks is All You Need", "authorids": ["~Hubert_Ramsauer2", "~Bernhard_Sch\u00e4fl1", "~Johannes_Lehner1", "~Philipp_Seidl1", "~Michael_Widrich2", "~Lukas_Gruber2", "~Markus_Holzleitner1", "~Thomas_Adler1", "openreview20@kreil.org", "~Michael_K_Kopp1", "~G\u00fcnter_Klambauer1", "~Johannes_Brandstetter1", "~Sepp_Hochreiter1"], "authors": ["Hubert Ramsauer", "Bernhard Sch\u00e4fl", "Johannes Lehner", "Philipp Seidl", "Michael Widrich", "Lukas Gruber", "Markus Holzleitner", "Thomas Adler", "David Kreil", "Michael K Kopp", "G\u00fcnter Klambauer", "Johannes Brandstetter", "Sepp Hochreiter"], "keywords": ["Modern Hopfield Network", "Energy", "Attention", "Convergence", "Storage Capacity", "Hopfield layer", "Associative Memory"], "abstract": "We introduce a modern Hopfield network with continuous states and a corresponding update rule. The new Hopfield network can store exponentially (with the dimension of the associative space) many patterns, retrieves the pattern with one update, and has exponentially small retrieval errors. It has three types of energy minima (fixed points of the update): (1) global fixed point averaging over all patterns, (2) metastable states averaging over a subset of patterns, and (3) fixed points which store a single pattern. The new update rule is equivalent to the attention mechanism used in transformers. This equivalence enables a characterization of the heads of transformer models. These heads perform in the first layers preferably global averaging and in higher layers partial averaging via metastable states. The new modern Hopfield network can be integrated  into deep learning architectures as layers to allow the storage of and access to raw input data, intermediate results, or learned prototypes.\nThese Hopfield layers enable new ways of deep learning, beyond fully-connected, convolutional, or recurrent networks, and provide pooling, memory, association, and attention mechanisms. We demonstrate the broad applicability of the Hopfield layers\nacross various domains. Hopfield layers improved state-of-the-art on three out of four considered multiple instance learning problems as well as on immune repertoire classification with several hundreds of thousands of instances. On the UCI benchmark collections of small classification tasks, where deep learning methods typically struggle, Hopfield layers yielded a new state-of-the-art when compared to different machine learning methods. Finally, Hopfield layers achieved state-of-the-art on two drug design datasets. The implementation is available at: \\url{https://github.com/ml-jku/hopfield-layers}", "one-sentence_summary": "A novel continuous Hopfield network is proposed whose update rule is the attention mechanism of the transformer model and which can be integrated into deep learning architectures.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ramsauer|hopfield_networks_is_all_you_need", "supplementary_material": "/attachment/33ec53f091d7d7bbce2c7d7f8c5a630db50e4ebb.zip", "pdf": "/pdf/4dfbed3a6ececb7282dfef90fd6c03812ae0da7b.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nramsauer2021hopfield,\ntitle={Hopfield Networks is All You Need},\nauthor={Hubert Ramsauer and Bernhard Sch{\\\"a}fl and Johannes Lehner and Philipp Seidl and Michael Widrich and Lukas Gruber and Markus Holzleitner and Thomas Adler and David Kreil and Michael K Kopp and G{\\\"u}nter Klambauer and Johannes Brandstetter and Sepp Hochreiter},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=tL89RnzIiCd}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "WLfnFWzoqnp", "original": null, "number": 1, "cdate": 1610040493188, "ddate": null, "tcdate": 1610040493188, "tmdate": 1610474099234, "tddate": null, "forum": "tL89RnzIiCd", "replyto": "tL89RnzIiCd", "invitation": "ICLR.cc/2021/Conference/Paper3489/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Poster)", "comment": "The novelty of the paper are:\n+ introduces a new Hopfield network with continuous states, hence can be learned end-to-end differentiation and back propagation.\n+ derives efficient update rules\n+ reveals a connection between the update rules and transformers\n+ illustrate how the network can be used as a layer in deep neural network that can perform different functions\n\nThe presentation was clear enough for the reviewers to understand and appreciate the novelty, although there were a few points of confusion. I would recommend the authors to address several suggestions that came up in the discussions including:\n- additional analysis to highlight when and how the networks is able to outperform other competing models\n- intuitions about the proofs for the theorems (okay to leave the detailed derivation in the appendix)\n\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hopfield Networks is All You Need", "authorids": ["~Hubert_Ramsauer2", "~Bernhard_Sch\u00e4fl1", "~Johannes_Lehner1", "~Philipp_Seidl1", "~Michael_Widrich2", "~Lukas_Gruber2", "~Markus_Holzleitner1", "~Thomas_Adler1", "openreview20@kreil.org", "~Michael_K_Kopp1", "~G\u00fcnter_Klambauer1", "~Johannes_Brandstetter1", "~Sepp_Hochreiter1"], "authors": ["Hubert Ramsauer", "Bernhard Sch\u00e4fl", "Johannes Lehner", "Philipp Seidl", "Michael Widrich", "Lukas Gruber", "Markus Holzleitner", "Thomas Adler", "David Kreil", "Michael K Kopp", "G\u00fcnter Klambauer", "Johannes Brandstetter", "Sepp Hochreiter"], "keywords": ["Modern Hopfield Network", "Energy", "Attention", "Convergence", "Storage Capacity", "Hopfield layer", "Associative Memory"], "abstract": "We introduce a modern Hopfield network with continuous states and a corresponding update rule. The new Hopfield network can store exponentially (with the dimension of the associative space) many patterns, retrieves the pattern with one update, and has exponentially small retrieval errors. It has three types of energy minima (fixed points of the update): (1) global fixed point averaging over all patterns, (2) metastable states averaging over a subset of patterns, and (3) fixed points which store a single pattern. The new update rule is equivalent to the attention mechanism used in transformers. This equivalence enables a characterization of the heads of transformer models. These heads perform in the first layers preferably global averaging and in higher layers partial averaging via metastable states. The new modern Hopfield network can be integrated  into deep learning architectures as layers to allow the storage of and access to raw input data, intermediate results, or learned prototypes.\nThese Hopfield layers enable new ways of deep learning, beyond fully-connected, convolutional, or recurrent networks, and provide pooling, memory, association, and attention mechanisms. We demonstrate the broad applicability of the Hopfield layers\nacross various domains. Hopfield layers improved state-of-the-art on three out of four considered multiple instance learning problems as well as on immune repertoire classification with several hundreds of thousands of instances. On the UCI benchmark collections of small classification tasks, where deep learning methods typically struggle, Hopfield layers yielded a new state-of-the-art when compared to different machine learning methods. Finally, Hopfield layers achieved state-of-the-art on two drug design datasets. The implementation is available at: \\url{https://github.com/ml-jku/hopfield-layers}", "one-sentence_summary": "A novel continuous Hopfield network is proposed whose update rule is the attention mechanism of the transformer model and which can be integrated into deep learning architectures.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ramsauer|hopfield_networks_is_all_you_need", "supplementary_material": "/attachment/33ec53f091d7d7bbce2c7d7f8c5a630db50e4ebb.zip", "pdf": "/pdf/4dfbed3a6ececb7282dfef90fd6c03812ae0da7b.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nramsauer2021hopfield,\ntitle={Hopfield Networks is All You Need},\nauthor={Hubert Ramsauer and Bernhard Sch{\\\"a}fl and Johannes Lehner and Philipp Seidl and Michael Widrich and Lukas Gruber and Markus Holzleitner and Thomas Adler and David Kreil and Michael K Kopp and G{\\\"u}nter Klambauer and Johannes Brandstetter and Sepp Hochreiter},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=tL89RnzIiCd}\n}"}, "tags": [], "invitation": {"reply": {"forum": "tL89RnzIiCd", "replyto": "tL89RnzIiCd", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040493172, "tmdate": 1610474099215, "id": "ICLR.cc/2021/Conference/Paper3489/-/Decision"}}}, {"id": "d0DwejhNgq", "original": null, "number": 1, "cdate": 1603768277433, "ddate": null, "tcdate": 1603768277433, "tmdate": 1606501348435, "tddate": null, "forum": "tL89RnzIiCd", "replyto": "tL89RnzIiCd", "invitation": "ICLR.cc/2021/Conference/Paper3489/-/Official_Review", "content": {"title": "Very interesting but missing critical content", "review": "This work extends the binary Hopfield network (Demircigil et al., 2017) to continuous patterns and states. Connections are drawn between the result model to the attention layers of the transformers, the pooling operation of LSTM, similarity search, and fully connected layers. Experimental results are briefly described for analyzing the attention of Bert models, multiple instance learning, and small UCI classification tasks.\n\nThe proposed model seems very interesting, and the proposed applications seem reasonable at a very high level. However, there is just not enough detail in this paper for me to understand how the models are implemented or why the model works better than other approaches.\n\nFor example, section 3 declared 3 types of Hopfield layers, but without any formal definitions to them, or how they are integrated to the proposed models. The experiment section compares performances with existing models, but lacks any analysis of why the proposed models work better. Similarly, there is a lack of motivation/intuition in the introduction section.\n\n## After author feedback ##\nThanks for the paper update, and now I have a better understanding of the proposed approach. I have updated my review to the following:\n\nPreviously Widrich+ (2020) showed that integrating transformer-like attention (or equivalently modern Hopfield networks based on softmax)  into deep learning architectures outperforms existing methods (kNN and logistic regression) for massive MIL such as immune repertoire classification. More specifically a pooling layer can be formed by attending over a repertoire of instances with a fixed (but learnable) query vector.\n\nThis work provides theoretical analysis of such a layer for its energy function, convergence of updates, and storage capacity, and points to directions of how such a layer can be understood and controlled. It extends the previous experiment:\n1) apply HopfieldPooling (attention with fixed learnable query Q) to more MIL datasets (animal image and breast cancer)  and achieve state of the art results. \n2) apply Hopfield (attention) to 75 small UCI benchmarks replacing feedforward nets. Here Selu units (Klambauer+ 2017) are used to map input to storage Y  and query R. The result is quite positive beating previous approaches including SVM, random forest, and SNN (Klambauer+ 2017)\n3) apply HopfieldLayer (attention with fixed training data Y as storage) to 4 drug design tasks  acting as an instance-based learning approach.\n\nThe result seems quite interesting indicating that general purpose layers such as  feedforward, pooling and nearest neighbors can be improved (in terms of robustness, learnability, or controllability) by adding attention like operations.\n\nI think the paper can talk less about existing results, and focus more on the new results and their analysis:\n- remove [Immune Repertoire Classification] result since it is from previous work.\n- move the Drug Design experiment details to the main text, and add some comment about under what condition Hopfield outperforms/underperforms RF.\n- for the UCI benchmark experiment the transformer layer (Vaswani+ 2017) seems to be a natural baseline and should be compared to. \n\nSuggestions for the presentation:\n- Should only in the future work section state that Hopfield can potentially substitute LSTMs or GRUs, since it is all hypothetical with no experiment result at this point.\n- The word \"implemented\"  in Section 4 seems misleading as there is nothing changed in the Bert model structure? \"Transformer and BERT models can be implemented by the layer Hopfield.\"  \n- Can be more specific in descriptions. For example in the description of (2) Layer HopfieldPooling and (3) Layer HopfieldLayer in Section 3, R and  W_K can be  referenced again for \"state (query) patterns \" and \"The stored (key) patterns\" respectively.\n- It is probably more informative to replace figure 1 with a table to directly compare the energy function and updating rules of different Hopfield nets--i.e., classical, exponential and attention.\n- Avoid using \"x\" in equation 1, since the symbol has already been used for the stored patterns.\n- \"HopfieldLayer\" seems to be a very strange name.\n\n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3489/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3489/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hopfield Networks is All You Need", "authorids": ["~Hubert_Ramsauer2", "~Bernhard_Sch\u00e4fl1", "~Johannes_Lehner1", "~Philipp_Seidl1", "~Michael_Widrich2", "~Lukas_Gruber2", "~Markus_Holzleitner1", "~Thomas_Adler1", "openreview20@kreil.org", "~Michael_K_Kopp1", "~G\u00fcnter_Klambauer1", "~Johannes_Brandstetter1", "~Sepp_Hochreiter1"], "authors": ["Hubert Ramsauer", "Bernhard Sch\u00e4fl", "Johannes Lehner", "Philipp Seidl", "Michael Widrich", "Lukas Gruber", "Markus Holzleitner", "Thomas Adler", "David Kreil", "Michael K Kopp", "G\u00fcnter Klambauer", "Johannes Brandstetter", "Sepp Hochreiter"], "keywords": ["Modern Hopfield Network", "Energy", "Attention", "Convergence", "Storage Capacity", "Hopfield layer", "Associative Memory"], "abstract": "We introduce a modern Hopfield network with continuous states and a corresponding update rule. The new Hopfield network can store exponentially (with the dimension of the associative space) many patterns, retrieves the pattern with one update, and has exponentially small retrieval errors. It has three types of energy minima (fixed points of the update): (1) global fixed point averaging over all patterns, (2) metastable states averaging over a subset of patterns, and (3) fixed points which store a single pattern. The new update rule is equivalent to the attention mechanism used in transformers. This equivalence enables a characterization of the heads of transformer models. These heads perform in the first layers preferably global averaging and in higher layers partial averaging via metastable states. The new modern Hopfield network can be integrated  into deep learning architectures as layers to allow the storage of and access to raw input data, intermediate results, or learned prototypes.\nThese Hopfield layers enable new ways of deep learning, beyond fully-connected, convolutional, or recurrent networks, and provide pooling, memory, association, and attention mechanisms. We demonstrate the broad applicability of the Hopfield layers\nacross various domains. Hopfield layers improved state-of-the-art on three out of four considered multiple instance learning problems as well as on immune repertoire classification with several hundreds of thousands of instances. On the UCI benchmark collections of small classification tasks, where deep learning methods typically struggle, Hopfield layers yielded a new state-of-the-art when compared to different machine learning methods. Finally, Hopfield layers achieved state-of-the-art on two drug design datasets. The implementation is available at: \\url{https://github.com/ml-jku/hopfield-layers}", "one-sentence_summary": "A novel continuous Hopfield network is proposed whose update rule is the attention mechanism of the transformer model and which can be integrated into deep learning architectures.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ramsauer|hopfield_networks_is_all_you_need", "supplementary_material": "/attachment/33ec53f091d7d7bbce2c7d7f8c5a630db50e4ebb.zip", "pdf": "/pdf/4dfbed3a6ececb7282dfef90fd6c03812ae0da7b.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nramsauer2021hopfield,\ntitle={Hopfield Networks is All You Need},\nauthor={Hubert Ramsauer and Bernhard Sch{\\\"a}fl and Johannes Lehner and Philipp Seidl and Michael Widrich and Lukas Gruber and Markus Holzleitner and Thomas Adler and David Kreil and Michael K Kopp and G{\\\"u}nter Klambauer and Johannes Brandstetter and Sepp Hochreiter},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=tL89RnzIiCd}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "tL89RnzIiCd", "replyto": "tL89RnzIiCd", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3489/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538074900, "tmdate": 1606915798292, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3489/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3489/-/Official_Review"}}}, {"id": "NIctU6e5MxV", "original": null, "number": 5, "cdate": 1605631185562, "ddate": null, "tcdate": 1605631185562, "tmdate": 1605631185562, "tddate": null, "forum": "tL89RnzIiCd", "replyto": "d0DwejhNgq", "invitation": "ICLR.cc/2021/Conference/Paper3489/-/Official_Comment", "content": {"title": "Response to Reviewer 3", "comment": "Thank you for your helpful review. Please, let us expand a bit on the weaknesses you pointed out.\n\n\n* \u201cnot enough detail in this paper for me to understand how the models are implemented or why the model works better than other approaches.\u201d & \u201cFor example, section 3 declared 3 types of Hopfield layers, but without any formal definitions to them\u201d: Sorry for the bad description. We massively extended Section 3, to describe (i) our main goal, (ii) how Hopfield networks are integrated into deep learning architectures, and (iii) how the layers are designed. We also give possible applications of the layers. Then we refer to the experiments, where the layers are used.\nSince gradients have to be propagated through these layers, we aim at obtaining continuous Hopfield networks that are differentiable and can retrieve by one update step. One update is equivalent to updating a layer in a neural network.\n\n* \u201cwhy the model works better than other approaches.\u201d & \u201cbut lacks any analysis of why the proposed models work better\u201d:\n We now give explanations why the proposed models work better. In particular, we show that Hopfield layers can realize k-nearest neighbor (with a learned distance metric), SVM-like methods (storing support vectors or reference vectors), similarity-based methods (similarity to stored patterns), learning vector quantization (stored patterns are the centers of clusters), etc. We also show that the transformer model can readily be implemented. Furthermore, the layers can perform simple pooling operations or store the elements of a time series, therefore, can replace LSTM or GRU layers.\n\n* \u201clack of motivation in the introduction section.\u201d: \nWe added a contribution paragraph to the introduction and massively extended Section 3. We now write in the abstract: \u201cThese Hopfield layers enable new ways of deep learning, beyond fully-connected, convolutional, or recurrent networks, and provide pooling, memory, association, and attention mechanisms.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3489/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3489/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hopfield Networks is All You Need", "authorids": ["~Hubert_Ramsauer2", "~Bernhard_Sch\u00e4fl1", "~Johannes_Lehner1", "~Philipp_Seidl1", "~Michael_Widrich2", "~Lukas_Gruber2", "~Markus_Holzleitner1", "~Thomas_Adler1", "openreview20@kreil.org", "~Michael_K_Kopp1", "~G\u00fcnter_Klambauer1", "~Johannes_Brandstetter1", "~Sepp_Hochreiter1"], "authors": ["Hubert Ramsauer", "Bernhard Sch\u00e4fl", "Johannes Lehner", "Philipp Seidl", "Michael Widrich", "Lukas Gruber", "Markus Holzleitner", "Thomas Adler", "David Kreil", "Michael K Kopp", "G\u00fcnter Klambauer", "Johannes Brandstetter", "Sepp Hochreiter"], "keywords": ["Modern Hopfield Network", "Energy", "Attention", "Convergence", "Storage Capacity", "Hopfield layer", "Associative Memory"], "abstract": "We introduce a modern Hopfield network with continuous states and a corresponding update rule. The new Hopfield network can store exponentially (with the dimension of the associative space) many patterns, retrieves the pattern with one update, and has exponentially small retrieval errors. It has three types of energy minima (fixed points of the update): (1) global fixed point averaging over all patterns, (2) metastable states averaging over a subset of patterns, and (3) fixed points which store a single pattern. The new update rule is equivalent to the attention mechanism used in transformers. This equivalence enables a characterization of the heads of transformer models. These heads perform in the first layers preferably global averaging and in higher layers partial averaging via metastable states. The new modern Hopfield network can be integrated  into deep learning architectures as layers to allow the storage of and access to raw input data, intermediate results, or learned prototypes.\nThese Hopfield layers enable new ways of deep learning, beyond fully-connected, convolutional, or recurrent networks, and provide pooling, memory, association, and attention mechanisms. We demonstrate the broad applicability of the Hopfield layers\nacross various domains. Hopfield layers improved state-of-the-art on three out of four considered multiple instance learning problems as well as on immune repertoire classification with several hundreds of thousands of instances. On the UCI benchmark collections of small classification tasks, where deep learning methods typically struggle, Hopfield layers yielded a new state-of-the-art when compared to different machine learning methods. Finally, Hopfield layers achieved state-of-the-art on two drug design datasets. The implementation is available at: \\url{https://github.com/ml-jku/hopfield-layers}", "one-sentence_summary": "A novel continuous Hopfield network is proposed whose update rule is the attention mechanism of the transformer model and which can be integrated into deep learning architectures.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ramsauer|hopfield_networks_is_all_you_need", "supplementary_material": "/attachment/33ec53f091d7d7bbce2c7d7f8c5a630db50e4ebb.zip", "pdf": "/pdf/4dfbed3a6ececb7282dfef90fd6c03812ae0da7b.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nramsauer2021hopfield,\ntitle={Hopfield Networks is All You Need},\nauthor={Hubert Ramsauer and Bernhard Sch{\\\"a}fl and Johannes Lehner and Philipp Seidl and Michael Widrich and Lukas Gruber and Markus Holzleitner and Thomas Adler and David Kreil and Michael K Kopp and G{\\\"u}nter Klambauer and Johannes Brandstetter and Sepp Hochreiter},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=tL89RnzIiCd}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "tL89RnzIiCd", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3489/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3489/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3489/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3489/Authors|ICLR.cc/2021/Conference/Paper3489/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3489/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923837007, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3489/-/Official_Comment"}}}, {"id": "e432F1tQpzC", "original": null, "number": 4, "cdate": 1605631048112, "ddate": null, "tcdate": 1605631048112, "tmdate": 1605631048112, "tddate": null, "forum": "tL89RnzIiCd", "replyto": "z4pH0FlkxHI", "invitation": "ICLR.cc/2021/Conference/Paper3489/-/Official_Comment", "content": {"title": "Response to Reviewer 4", "comment": "Thank you for a very insightful review that helps us to improve our paper.\n\n\n* \u201cclarity about the optimization in the new proposed variant of hopfield networks\u201d:\n We massively extended Section 3, to describe (i) our main goal, (ii) how Hopfield networks are integrated into deep learning architectures, and (iii) how the layers are designed.\n\n* \u201cmotivation behind update equations\u201d: \nIn the appendix \u201cA.1.3 NEW UPDATE RULE\u201c, we give in Eq. (29) for comparison, the synchronous update rule for the classical Hopfield network with threshold zero, which is very similar to our update rule but without the softmax. In appendix \u201cLemma A18\u201d, we show that the softmax is the derivative of the Log-Sum-Exp Function.\n\n* \u201cintuition behind convergence in one update\u201d: \nSorry, we mixed up mathematical convergence and retrieval, which is the convergence in praxis. We now separated mathematical convergence from retrieval (being close to a fixed point). We now define retrieval by an update that comes epsilon-close to the fixed point. Random patterns are mentioned.\n\n* What happens to the updates / optimization when the patterns are not well separated?: \nWe discuss this case in the paragraph \u201cMetastable states and one global fixed point.\u201d We write: \u201cIf some vectors are similar to each other and well separated from all other vectors, then a metastable state near the similar vectors exists. Iterates that start near the metastable state converge to this metastable state, also if initialized by one of the similar patterns.\u201d In this case these similar patterns are retrieved collectively. We write further: \u201cIf no pattern is well separated from the others, then the iteration converges to a global fixed point close to the arithmetic mean of the vectors.\u201d \n\n* \u201cIs the trend in Fig 2 observed across more or less across all datasets?\u201d: \nFig. 2 (now moved into the appendix) is specific to the transformer architecture and exemplary NLP tasks. We give in the appendix additional examples for this trend.\n\n* Other comments \u201cmax-margin classifiers / kernel methods\u201d: \nWe now give the connections to SVMs as the stored patterns can serve as support vectors. However, we do not see an obvious relation to max-margin classifiers.\n\n* Other comments \u201cnon-linear transformations\u201d: \nNon-linear activation functions been used for the experiments in immune repertoire classification. We clarify that more, in particular in the appendix.\n\n* \u201cusing them in large scale language modeling tasks where transformers are popular right now.\u201d: \nWe have shown that the transformer attention mechanism is exactly the update rule of a modern Hopfield network. The transformer architecture is one example of applying our approach with modern Hopfield networks. The layer Hopfield together with residual connections (skip connections) gives the self-attention layer of the transformer. Also the encoder-decoder attention layer of the transformer can be realized, where Y comes from the encoder and R from the decoder. Also the layernorm is supplied automatically for the Hopfied layer by our Pytorch implementation. Since there are already many experiments with transformers, we focus on new tasks that can be solved with Hopfield networks.   \nThe attention mechanism of transformers is just an associative memory, where queries serve to retrieve keys that are stored. However, we can supply more functionalities by other Hopfield layers. Therefore, we focus on experiments, where these new architectures have not been tested. We achieved many new state-of-the-art for different datasets.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3489/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3489/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hopfield Networks is All You Need", "authorids": ["~Hubert_Ramsauer2", "~Bernhard_Sch\u00e4fl1", "~Johannes_Lehner1", "~Philipp_Seidl1", "~Michael_Widrich2", "~Lukas_Gruber2", "~Markus_Holzleitner1", "~Thomas_Adler1", "openreview20@kreil.org", "~Michael_K_Kopp1", "~G\u00fcnter_Klambauer1", "~Johannes_Brandstetter1", "~Sepp_Hochreiter1"], "authors": ["Hubert Ramsauer", "Bernhard Sch\u00e4fl", "Johannes Lehner", "Philipp Seidl", "Michael Widrich", "Lukas Gruber", "Markus Holzleitner", "Thomas Adler", "David Kreil", "Michael K Kopp", "G\u00fcnter Klambauer", "Johannes Brandstetter", "Sepp Hochreiter"], "keywords": ["Modern Hopfield Network", "Energy", "Attention", "Convergence", "Storage Capacity", "Hopfield layer", "Associative Memory"], "abstract": "We introduce a modern Hopfield network with continuous states and a corresponding update rule. The new Hopfield network can store exponentially (with the dimension of the associative space) many patterns, retrieves the pattern with one update, and has exponentially small retrieval errors. It has three types of energy minima (fixed points of the update): (1) global fixed point averaging over all patterns, (2) metastable states averaging over a subset of patterns, and (3) fixed points which store a single pattern. The new update rule is equivalent to the attention mechanism used in transformers. This equivalence enables a characterization of the heads of transformer models. These heads perform in the first layers preferably global averaging and in higher layers partial averaging via metastable states. The new modern Hopfield network can be integrated  into deep learning architectures as layers to allow the storage of and access to raw input data, intermediate results, or learned prototypes.\nThese Hopfield layers enable new ways of deep learning, beyond fully-connected, convolutional, or recurrent networks, and provide pooling, memory, association, and attention mechanisms. We demonstrate the broad applicability of the Hopfield layers\nacross various domains. Hopfield layers improved state-of-the-art on three out of four considered multiple instance learning problems as well as on immune repertoire classification with several hundreds of thousands of instances. On the UCI benchmark collections of small classification tasks, where deep learning methods typically struggle, Hopfield layers yielded a new state-of-the-art when compared to different machine learning methods. Finally, Hopfield layers achieved state-of-the-art on two drug design datasets. The implementation is available at: \\url{https://github.com/ml-jku/hopfield-layers}", "one-sentence_summary": "A novel continuous Hopfield network is proposed whose update rule is the attention mechanism of the transformer model and which can be integrated into deep learning architectures.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ramsauer|hopfield_networks_is_all_you_need", "supplementary_material": "/attachment/33ec53f091d7d7bbce2c7d7f8c5a630db50e4ebb.zip", "pdf": "/pdf/4dfbed3a6ececb7282dfef90fd6c03812ae0da7b.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nramsauer2021hopfield,\ntitle={Hopfield Networks is All You Need},\nauthor={Hubert Ramsauer and Bernhard Sch{\\\"a}fl and Johannes Lehner and Philipp Seidl and Michael Widrich and Lukas Gruber and Markus Holzleitner and Thomas Adler and David Kreil and Michael K Kopp and G{\\\"u}nter Klambauer and Johannes Brandstetter and Sepp Hochreiter},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=tL89RnzIiCd}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "tL89RnzIiCd", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3489/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3489/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3489/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3489/Authors|ICLR.cc/2021/Conference/Paper3489/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3489/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923837007, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3489/-/Official_Comment"}}}, {"id": "01xnbsXe2b1", "original": null, "number": 3, "cdate": 1605630856360, "ddate": null, "tcdate": 1605630856360, "tmdate": 1605630856360, "tddate": null, "forum": "tL89RnzIiCd", "replyto": "DzD5Lil_BJT", "invitation": "ICLR.cc/2021/Conference/Paper3489/-/Official_Comment", "content": {"title": "Response to Reviewer 1", "comment": "Thank you for a very elaborate review that helps us to improve our paper. We address the points individually.\n\n\n* \u201cI was left me wondering about the added value of this new model\u201d: \nThe main goal of the paper is to integrate associative memories (the Hopfield layers) into deep learning architectures as layers. Therefore each layer can store and access raw input data, reference data, intermediate results, or (learned) prototypes. These Hopfield layers enable new ways of deep learning, beyond e.g. CNNs or recurrent networks. Hopfield layers can be used for multiple instance learning, point sets, or learning to process sequences. They enable the substitution of k-nearest neighbor, support vector machine models, or learning vector quantization in each layer separately. Also the transformer\u2019s  self-attention and encoder-decoder attention are examples of Hopfield layers, but the interpretation as an associative memory is novel.\n\n* \u201cIt was not clear to me what is gained by this greater complexity and whether the gains justify the larger complexity.\u201d:\nA continuous Hopfield network (not discrete) is necessary in order to enable end-to-end differentiable models. The continuous Hopfield network is integrated as a special layer in deep learning architectures, where backpropagation requires this layer to be differentiable. Therefore, we also investigate if one update is sufficient for being close to the fixed point. Integrated into a deep learning architecture, only one Hopfield update step should be performed, which is equivalent to updating a layer in a neural network. The reviewer might be right in their assumption that discrete networks might also do the job and the continuous models are the discrete models in disguise. However, it is not clear how to learn the weights that map to the embedding space, where the Hopfield network stores and retrieves patterns. We massively extended Section 3, to describe (i) our main goal, (ii) how Hopfield networks are integrated into deep learning architectures, and (iii) how the layers are designed.\n\n* \u201cbreaking their long paper to two different sections, one presenting the theoretical advantages of their new model and the other focusing on practical benefits\u201d:  \nThanks for this advice. We do that: Section 2 is dedicated to theoretical considerations and Section 3 to practical / implementation details. We now massively extended Section 3.\n\n* \u201cComment 2\u201d and \u201cthe nature of convergence to a fixed point wasn't clear to me\u201d & \u201cconverge in one update step\u201d: \nSorry, we mixed up mathematical convergence and retrieval, which is the convergence in praxis. We now separated mathematical convergence from retrieval (being close to a fixed point). We now define retrieval by an update that comes epsilon-close to the fixed point. Random patterns are mentioned.\n\n* \"Comment 3: proven for c= 1.37 and c= 3.15 in Theorem 3\": Sorry for the ambiguous formulation. It is proven if the assumptions in Theorem 3 are fulfilled, reasonable settings which fulfill the assumptions are given for c=1.37 and c=3.15. This has been corrected.\n\n* \u201cComment 4: true for random patterns\u201d:  Yes. We mention that now.\n\n* \u201cComment 5: Is beta>0\u201d: Yes. We mention that now."}, "signatures": ["ICLR.cc/2021/Conference/Paper3489/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3489/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hopfield Networks is All You Need", "authorids": ["~Hubert_Ramsauer2", "~Bernhard_Sch\u00e4fl1", "~Johannes_Lehner1", "~Philipp_Seidl1", "~Michael_Widrich2", "~Lukas_Gruber2", "~Markus_Holzleitner1", "~Thomas_Adler1", "openreview20@kreil.org", "~Michael_K_Kopp1", "~G\u00fcnter_Klambauer1", "~Johannes_Brandstetter1", "~Sepp_Hochreiter1"], "authors": ["Hubert Ramsauer", "Bernhard Sch\u00e4fl", "Johannes Lehner", "Philipp Seidl", "Michael Widrich", "Lukas Gruber", "Markus Holzleitner", "Thomas Adler", "David Kreil", "Michael K Kopp", "G\u00fcnter Klambauer", "Johannes Brandstetter", "Sepp Hochreiter"], "keywords": ["Modern Hopfield Network", "Energy", "Attention", "Convergence", "Storage Capacity", "Hopfield layer", "Associative Memory"], "abstract": "We introduce a modern Hopfield network with continuous states and a corresponding update rule. The new Hopfield network can store exponentially (with the dimension of the associative space) many patterns, retrieves the pattern with one update, and has exponentially small retrieval errors. It has three types of energy minima (fixed points of the update): (1) global fixed point averaging over all patterns, (2) metastable states averaging over a subset of patterns, and (3) fixed points which store a single pattern. The new update rule is equivalent to the attention mechanism used in transformers. This equivalence enables a characterization of the heads of transformer models. These heads perform in the first layers preferably global averaging and in higher layers partial averaging via metastable states. The new modern Hopfield network can be integrated  into deep learning architectures as layers to allow the storage of and access to raw input data, intermediate results, or learned prototypes.\nThese Hopfield layers enable new ways of deep learning, beyond fully-connected, convolutional, or recurrent networks, and provide pooling, memory, association, and attention mechanisms. We demonstrate the broad applicability of the Hopfield layers\nacross various domains. Hopfield layers improved state-of-the-art on three out of four considered multiple instance learning problems as well as on immune repertoire classification with several hundreds of thousands of instances. On the UCI benchmark collections of small classification tasks, where deep learning methods typically struggle, Hopfield layers yielded a new state-of-the-art when compared to different machine learning methods. Finally, Hopfield layers achieved state-of-the-art on two drug design datasets. The implementation is available at: \\url{https://github.com/ml-jku/hopfield-layers}", "one-sentence_summary": "A novel continuous Hopfield network is proposed whose update rule is the attention mechanism of the transformer model and which can be integrated into deep learning architectures.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ramsauer|hopfield_networks_is_all_you_need", "supplementary_material": "/attachment/33ec53f091d7d7bbce2c7d7f8c5a630db50e4ebb.zip", "pdf": "/pdf/4dfbed3a6ececb7282dfef90fd6c03812ae0da7b.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nramsauer2021hopfield,\ntitle={Hopfield Networks is All You Need},\nauthor={Hubert Ramsauer and Bernhard Sch{\\\"a}fl and Johannes Lehner and Philipp Seidl and Michael Widrich and Lukas Gruber and Markus Holzleitner and Thomas Adler and David Kreil and Michael K Kopp and G{\\\"u}nter Klambauer and Johannes Brandstetter and Sepp Hochreiter},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=tL89RnzIiCd}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "tL89RnzIiCd", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3489/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3489/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3489/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3489/Authors|ICLR.cc/2021/Conference/Paper3489/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3489/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923837007, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3489/-/Official_Comment"}}}, {"id": "m4JXwVy-pCR", "original": null, "number": 2, "cdate": 1605630702819, "ddate": null, "tcdate": 1605630702819, "tmdate": 1605630702819, "tddate": null, "forum": "tL89RnzIiCd", "replyto": "tL89RnzIiCd", "invitation": "ICLR.cc/2021/Conference/Paper3489/-/Official_Comment", "content": {"title": "Thank you for your feedback!", "comment": "We thank all reviewers for their time and for their constructive feedback. It helped us a lot to improve our paper. We hope to answer all questions and provide clarifications in individual responses to the respective reviewers. Further, we uploaded a rebuttal revision of our paper incorporating your sound suggestions. Concretely, we massively extended Section 3, to describe (i) our main goal, (ii) how Hopfield networks are integrated into deep learning architectures, and (iii) how the layers are designed."}, "signatures": ["ICLR.cc/2021/Conference/Paper3489/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3489/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hopfield Networks is All You Need", "authorids": ["~Hubert_Ramsauer2", "~Bernhard_Sch\u00e4fl1", "~Johannes_Lehner1", "~Philipp_Seidl1", "~Michael_Widrich2", "~Lukas_Gruber2", "~Markus_Holzleitner1", "~Thomas_Adler1", "openreview20@kreil.org", "~Michael_K_Kopp1", "~G\u00fcnter_Klambauer1", "~Johannes_Brandstetter1", "~Sepp_Hochreiter1"], "authors": ["Hubert Ramsauer", "Bernhard Sch\u00e4fl", "Johannes Lehner", "Philipp Seidl", "Michael Widrich", "Lukas Gruber", "Markus Holzleitner", "Thomas Adler", "David Kreil", "Michael K Kopp", "G\u00fcnter Klambauer", "Johannes Brandstetter", "Sepp Hochreiter"], "keywords": ["Modern Hopfield Network", "Energy", "Attention", "Convergence", "Storage Capacity", "Hopfield layer", "Associative Memory"], "abstract": "We introduce a modern Hopfield network with continuous states and a corresponding update rule. The new Hopfield network can store exponentially (with the dimension of the associative space) many patterns, retrieves the pattern with one update, and has exponentially small retrieval errors. It has three types of energy minima (fixed points of the update): (1) global fixed point averaging over all patterns, (2) metastable states averaging over a subset of patterns, and (3) fixed points which store a single pattern. The new update rule is equivalent to the attention mechanism used in transformers. This equivalence enables a characterization of the heads of transformer models. These heads perform in the first layers preferably global averaging and in higher layers partial averaging via metastable states. The new modern Hopfield network can be integrated  into deep learning architectures as layers to allow the storage of and access to raw input data, intermediate results, or learned prototypes.\nThese Hopfield layers enable new ways of deep learning, beyond fully-connected, convolutional, or recurrent networks, and provide pooling, memory, association, and attention mechanisms. We demonstrate the broad applicability of the Hopfield layers\nacross various domains. Hopfield layers improved state-of-the-art on three out of four considered multiple instance learning problems as well as on immune repertoire classification with several hundreds of thousands of instances. On the UCI benchmark collections of small classification tasks, where deep learning methods typically struggle, Hopfield layers yielded a new state-of-the-art when compared to different machine learning methods. Finally, Hopfield layers achieved state-of-the-art on two drug design datasets. The implementation is available at: \\url{https://github.com/ml-jku/hopfield-layers}", "one-sentence_summary": "A novel continuous Hopfield network is proposed whose update rule is the attention mechanism of the transformer model and which can be integrated into deep learning architectures.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ramsauer|hopfield_networks_is_all_you_need", "supplementary_material": "/attachment/33ec53f091d7d7bbce2c7d7f8c5a630db50e4ebb.zip", "pdf": "/pdf/4dfbed3a6ececb7282dfef90fd6c03812ae0da7b.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nramsauer2021hopfield,\ntitle={Hopfield Networks is All You Need},\nauthor={Hubert Ramsauer and Bernhard Sch{\\\"a}fl and Johannes Lehner and Philipp Seidl and Michael Widrich and Lukas Gruber and Markus Holzleitner and Thomas Adler and David Kreil and Michael K Kopp and G{\\\"u}nter Klambauer and Johannes Brandstetter and Sepp Hochreiter},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=tL89RnzIiCd}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "tL89RnzIiCd", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3489/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3489/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3489/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3489/Authors|ICLR.cc/2021/Conference/Paper3489/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3489/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923837007, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3489/-/Official_Comment"}}}, {"id": "z4pH0FlkxHI", "original": null, "number": 2, "cdate": 1604079601211, "ddate": null, "tcdate": 1604079601211, "tmdate": 1605023990977, "tddate": null, "forum": "tL89RnzIiCd", "replyto": "tL89RnzIiCd", "invitation": "ICLR.cc/2021/Conference/Paper3489/-/Official_Review", "content": {"title": "Paper makes good technical contribution draws interesting connections between classical Hopfield networks and Attention Mechanism in transformers", "review": "The paper introduces a new Hopfield network which have continuous states and propose update rules for optimizing it. It also draws connections between the new model and attention mechanism used in transformers. Small scale empirical study is presented.\n\nOverall I like the technical contribution of the work but feel the paper could be revised to improve clarity about the optimization in the new proposed variant of hopfield networks. Below some specific comments:\n\nPros:\n- connecting hopfield networks to attention mechanism and drawing out the variants in section 3 (as hopfield layers) is useful\n- The exposition in section 1 and 2 where the authors describe the hopfield network with continuous states is written well (although I do feel the motivation behind update equations could be explained a bit better)\n\nCons:\n- As I mentioned earlier, I don't fully understand the intuition behind convergence in one update. Can the authors clarify this? Also the paper mentions update rule in eqn (5) converges after one update for well separated patterns. What happens to the updates / optimization when the patterns are not well separated? This should be discussed after equation (5). Maybe present different scenarios to make it clear.\n\n- Empirical study is limited in my opinion and can be improved. Is the trend in Fig 2 observed across more or less across all datasets? Can the authors comment on this? I like the visualization in the figure but it is bit hard to interpret (perhaps a more clearer label for it could help with that). \n\nOther comments:\n- The idea of separated patterns leads me to ask this question: is there any connection of this work to max-margin classifiers / kernel methods?\n\n- Did the authors consider what would happen if non-linear transformations (e.g. activation functions in DNNs) are applied on top of the inputs? How does the existing network change in that case?\n\n- Can the authors comment on the utility / challenges in applying their proposed method on datasets / tasks beyond the small scale UCI datasets used in their experiments? e.g. using them in large scale language modeling tasks where transformers are popular right now.\n\n", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3489/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3489/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hopfield Networks is All You Need", "authorids": ["~Hubert_Ramsauer2", "~Bernhard_Sch\u00e4fl1", "~Johannes_Lehner1", "~Philipp_Seidl1", "~Michael_Widrich2", "~Lukas_Gruber2", "~Markus_Holzleitner1", "~Thomas_Adler1", "openreview20@kreil.org", "~Michael_K_Kopp1", "~G\u00fcnter_Klambauer1", "~Johannes_Brandstetter1", "~Sepp_Hochreiter1"], "authors": ["Hubert Ramsauer", "Bernhard Sch\u00e4fl", "Johannes Lehner", "Philipp Seidl", "Michael Widrich", "Lukas Gruber", "Markus Holzleitner", "Thomas Adler", "David Kreil", "Michael K Kopp", "G\u00fcnter Klambauer", "Johannes Brandstetter", "Sepp Hochreiter"], "keywords": ["Modern Hopfield Network", "Energy", "Attention", "Convergence", "Storage Capacity", "Hopfield layer", "Associative Memory"], "abstract": "We introduce a modern Hopfield network with continuous states and a corresponding update rule. The new Hopfield network can store exponentially (with the dimension of the associative space) many patterns, retrieves the pattern with one update, and has exponentially small retrieval errors. It has three types of energy minima (fixed points of the update): (1) global fixed point averaging over all patterns, (2) metastable states averaging over a subset of patterns, and (3) fixed points which store a single pattern. The new update rule is equivalent to the attention mechanism used in transformers. This equivalence enables a characterization of the heads of transformer models. These heads perform in the first layers preferably global averaging and in higher layers partial averaging via metastable states. The new modern Hopfield network can be integrated  into deep learning architectures as layers to allow the storage of and access to raw input data, intermediate results, or learned prototypes.\nThese Hopfield layers enable new ways of deep learning, beyond fully-connected, convolutional, or recurrent networks, and provide pooling, memory, association, and attention mechanisms. We demonstrate the broad applicability of the Hopfield layers\nacross various domains. Hopfield layers improved state-of-the-art on three out of four considered multiple instance learning problems as well as on immune repertoire classification with several hundreds of thousands of instances. On the UCI benchmark collections of small classification tasks, where deep learning methods typically struggle, Hopfield layers yielded a new state-of-the-art when compared to different machine learning methods. Finally, Hopfield layers achieved state-of-the-art on two drug design datasets. The implementation is available at: \\url{https://github.com/ml-jku/hopfield-layers}", "one-sentence_summary": "A novel continuous Hopfield network is proposed whose update rule is the attention mechanism of the transformer model and which can be integrated into deep learning architectures.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ramsauer|hopfield_networks_is_all_you_need", "supplementary_material": "/attachment/33ec53f091d7d7bbce2c7d7f8c5a630db50e4ebb.zip", "pdf": "/pdf/4dfbed3a6ececb7282dfef90fd6c03812ae0da7b.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nramsauer2021hopfield,\ntitle={Hopfield Networks is All You Need},\nauthor={Hubert Ramsauer and Bernhard Sch{\\\"a}fl and Johannes Lehner and Philipp Seidl and Michael Widrich and Lukas Gruber and Markus Holzleitner and Thomas Adler and David Kreil and Michael K Kopp and G{\\\"u}nter Klambauer and Johannes Brandstetter and Sepp Hochreiter},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=tL89RnzIiCd}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "tL89RnzIiCd", "replyto": "tL89RnzIiCd", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3489/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538074900, "tmdate": 1606915798292, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3489/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3489/-/Official_Review"}}}, {"id": "DzD5Lil_BJT", "original": null, "number": 3, "cdate": 1604181668882, "ddate": null, "tcdate": 1604181668882, "tmdate": 1605023990913, "tddate": null, "forum": "tL89RnzIiCd", "replyto": "tL89RnzIiCd", "invitation": "ICLR.cc/2021/Conference/Paper3489/-/Official_Review", "content": {"title": "Interesting results but some questions arise", "review": "This paper considers a continuous version of the classical Hopfield network (HN) model.In contrast to well studied discrete models where the patterns (vectors) that are \nstored are discrete, this paper studied continuous vectors and a new continuous energy function.\nConvergence results to a fixed point are proven for the new rule, and it is shown that for the case of random patterns, the Hopfield network can memorize exponentially many patterns (with high probability).\u00a0 Finally several implementations are given showing how incorporating the new Hopfield net in classification tasks can improve classification accuracy in regimes where \ndata is scarce and where neural networks do not fare well. \n\nThe paper is rather long and I did not verify all results. The description appears sound.The proofs appear non-trivial and rather technical. While the results here are nontrivial I was left me wondering about the \nadded value of this new model. One of the biggest advantages of HN was its simplicity and elegance. More recent results of Hopfield and others with higher degree energy functions managed to maintain this clarity and brevity. The new model however is significantly more involved. It was not clear to me what is gained by this greater complexity and whether the gains \njustify the larger complexity. In actual implementations very limited precision is often necessary.How does this discretization influence the continuous model? How robust is it to rounding errors? Don't we get \"old\" discrete models in disguise? \n\nThe (impressive) empirical results raise similar questions. Can't we use old discrete HN instead of the new model and achieve similar results? It would be perhaps more informative to compare different HN to the new model presented in this paper. It seems a bit strange that previous uses of HN (discrete ) did not achieve such an improvement in previous studies. It would be beneficial to add more on related work in this area. \n\n The authors might consider breaking their long paper to two different sections, one presenting the theoretical advantages of their new model and the other focusing on practical benefits. \n\nFinally, the nature of convergence to a fixed point wasn't clear to me. It seems likely that if patterns are not random convergence can take a long time as is the case for discrete HN. \nSome recent work about the complexity of finding fixed points of continuous functions may be relevant here:A converse to Banach's fixed point theorem and its CLS-completeness.\nMore specific comments:\n1) The paper starts with a rather lengthy discussion of previous work. \nI would recommend outlining the contributions of this paper earlier on. \n2) \"converge in one update step with exponentially low error and have storage capacity proportional to...\" It was not clear to me that random patterns are considered here. \n3) \"proven for c= 1.37andc= 3.15 in Theorem 3\" for what c exactly is the result proven? \n4) \"Furthermore, with a single update, the fixed point recovered with high probability\"I presume this is true for random patterns? \n5) Is beta>0?", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3489/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3489/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hopfield Networks is All You Need", "authorids": ["~Hubert_Ramsauer2", "~Bernhard_Sch\u00e4fl1", "~Johannes_Lehner1", "~Philipp_Seidl1", "~Michael_Widrich2", "~Lukas_Gruber2", "~Markus_Holzleitner1", "~Thomas_Adler1", "openreview20@kreil.org", "~Michael_K_Kopp1", "~G\u00fcnter_Klambauer1", "~Johannes_Brandstetter1", "~Sepp_Hochreiter1"], "authors": ["Hubert Ramsauer", "Bernhard Sch\u00e4fl", "Johannes Lehner", "Philipp Seidl", "Michael Widrich", "Lukas Gruber", "Markus Holzleitner", "Thomas Adler", "David Kreil", "Michael K Kopp", "G\u00fcnter Klambauer", "Johannes Brandstetter", "Sepp Hochreiter"], "keywords": ["Modern Hopfield Network", "Energy", "Attention", "Convergence", "Storage Capacity", "Hopfield layer", "Associative Memory"], "abstract": "We introduce a modern Hopfield network with continuous states and a corresponding update rule. The new Hopfield network can store exponentially (with the dimension of the associative space) many patterns, retrieves the pattern with one update, and has exponentially small retrieval errors. It has three types of energy minima (fixed points of the update): (1) global fixed point averaging over all patterns, (2) metastable states averaging over a subset of patterns, and (3) fixed points which store a single pattern. The new update rule is equivalent to the attention mechanism used in transformers. This equivalence enables a characterization of the heads of transformer models. These heads perform in the first layers preferably global averaging and in higher layers partial averaging via metastable states. The new modern Hopfield network can be integrated  into deep learning architectures as layers to allow the storage of and access to raw input data, intermediate results, or learned prototypes.\nThese Hopfield layers enable new ways of deep learning, beyond fully-connected, convolutional, or recurrent networks, and provide pooling, memory, association, and attention mechanisms. We demonstrate the broad applicability of the Hopfield layers\nacross various domains. Hopfield layers improved state-of-the-art on three out of four considered multiple instance learning problems as well as on immune repertoire classification with several hundreds of thousands of instances. On the UCI benchmark collections of small classification tasks, where deep learning methods typically struggle, Hopfield layers yielded a new state-of-the-art when compared to different machine learning methods. Finally, Hopfield layers achieved state-of-the-art on two drug design datasets. The implementation is available at: \\url{https://github.com/ml-jku/hopfield-layers}", "one-sentence_summary": "A novel continuous Hopfield network is proposed whose update rule is the attention mechanism of the transformer model and which can be integrated into deep learning architectures.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ramsauer|hopfield_networks_is_all_you_need", "supplementary_material": "/attachment/33ec53f091d7d7bbce2c7d7f8c5a630db50e4ebb.zip", "pdf": "/pdf/4dfbed3a6ececb7282dfef90fd6c03812ae0da7b.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nramsauer2021hopfield,\ntitle={Hopfield Networks is All You Need},\nauthor={Hubert Ramsauer and Bernhard Sch{\\\"a}fl and Johannes Lehner and Philipp Seidl and Michael Widrich and Lukas Gruber and Markus Holzleitner and Thomas Adler and David Kreil and Michael K Kopp and G{\\\"u}nter Klambauer and Johannes Brandstetter and Sepp Hochreiter},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=tL89RnzIiCd}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "tL89RnzIiCd", "replyto": "tL89RnzIiCd", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3489/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538074900, "tmdate": 1606915798292, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3489/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3489/-/Official_Review"}}}], "count": 9}