{"notes": [{"tddate": null, "ddate": null, "cdate": null, "original": null, "tmdate": 1490028593579, "tcdate": 1490028593579, "number": 1, "id": "BJ54dtpix", "invitation": "ICLR.cc/2017/workshop/-/paper92/acceptance", "forum": "SkaxnKEYg", "replyto": "SkaxnKEYg", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Accept", "title": "ICLR committee final decision"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Emergence of Language with Multi-agent Games: Learning to Communicate with Sequences of Symbols", "abstract": "Learning to communicate through interaction, rather than relying on explicit supervision, is often considered a prerequisite for developing a general AI. We study a setting where two agents engage in playing a referential game and, from scratch, develop a communication protocol necessary to succeed in this game. We require that messages they exchange, both at train and test time, are in the form of a language (i.e. sequences of discrete symbols). As the ultimate goal is to ensure that communication is accomplished in natural language, we perform preliminary experiments where we inject prior information about natural language into our model and study properties of the resulting protocol.", "pdf": "/pdf/bb5181d2caf01da2808e9a0a99274b4da70b20d4.pdf", "TL;DR": "We proposed an efficient learning strategy for developing communication protocol(variable-length sequences of discrete symbols) between two agents for playing a referential game.  ", "paperhash": "havrylov|emergence_of_language_with_multiagent_games_learning_to_communicate_with_sequences_of_symbols", "conflicts": ["uva.nl"], "keywords": ["Natural language processing", "Deep learning", "Multi-modal learning", "Games"], "authors": ["Serhii Havrylov", "Ivan Titov"], "authorids": ["s.havrylov@uva.nl", "titov@uva.nl"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1490028594145, "id": "ICLR.cc/2017/workshop/-/paper92/acceptance", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "SkaxnKEYg", "replyto": "SkaxnKEYg", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept", "Reject"]}}}, "nonreaders": [], "cdate": 1490028594145}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1490021701189, "tcdate": 1487342581528, "number": 92, "id": "SkaxnKEYg", "invitation": "ICLR.cc/2017/workshop/-/submission", "forum": "SkaxnKEYg", "signatures": ["~Serhii_Havrylov1"], "readers": ["everyone"], "content": {"title": "Emergence of Language with Multi-agent Games: Learning to Communicate with Sequences of Symbols", "abstract": "Learning to communicate through interaction, rather than relying on explicit supervision, is often considered a prerequisite for developing a general AI. We study a setting where two agents engage in playing a referential game and, from scratch, develop a communication protocol necessary to succeed in this game. We require that messages they exchange, both at train and test time, are in the form of a language (i.e. sequences of discrete symbols). As the ultimate goal is to ensure that communication is accomplished in natural language, we perform preliminary experiments where we inject prior information about natural language into our model and study properties of the resulting protocol.", "pdf": "/pdf/bb5181d2caf01da2808e9a0a99274b4da70b20d4.pdf", "TL;DR": "We proposed an efficient learning strategy for developing communication protocol(variable-length sequences of discrete symbols) between two agents for playing a referential game.  ", "paperhash": "havrylov|emergence_of_language_with_multiagent_games_learning_to_communicate_with_sequences_of_symbols", "conflicts": ["uva.nl"], "keywords": ["Natural language processing", "Deep learning", "Multi-modal learning", "Games"], "authors": ["Serhii Havrylov", "Ivan Titov"], "authorids": ["s.havrylov@uva.nl", "titov@uva.nl"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1487690420000, "tmdate": 1484242559574, "id": "ICLR.cc/2017/workshop/-/submission", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1495466420000, "cdate": 1484242559574}}}, {"tddate": null, "tmdate": 1489758110082, "tcdate": 1489758110082, "number": 3, "id": "HyLswwKjl", "invitation": "ICLR.cc/2017/workshop/-/paper92/public/comment", "forum": "SkaxnKEYg", "replyto": "SyHbXgGig", "signatures": ["~Serhii_Havrylov1"], "readers": ["everyone"], "writers": ["~Serhii_Havrylov1"], "content": {"title": "Re: No Title", "comment": "Thank you very much for your comments.\n\nWe updated the paper and made the following changes:\n\n1. We added clarification regarding grounding process.\n2. We added more examples of qualitative analysis into the appendix for images from the different domain."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Emergence of Language with Multi-agent Games: Learning to Communicate with Sequences of Symbols", "abstract": "Learning to communicate through interaction, rather than relying on explicit supervision, is often considered a prerequisite for developing a general AI. We study a setting where two agents engage in playing a referential game and, from scratch, develop a communication protocol necessary to succeed in this game. We require that messages they exchange, both at train and test time, are in the form of a language (i.e. sequences of discrete symbols). As the ultimate goal is to ensure that communication is accomplished in natural language, we perform preliminary experiments where we inject prior information about natural language into our model and study properties of the resulting protocol.", "pdf": "/pdf/bb5181d2caf01da2808e9a0a99274b4da70b20d4.pdf", "TL;DR": "We proposed an efficient learning strategy for developing communication protocol(variable-length sequences of discrete symbols) between two agents for playing a referential game.  ", "paperhash": "havrylov|emergence_of_language_with_multiagent_games_learning_to_communicate_with_sequences_of_symbols", "conflicts": ["uva.nl"], "keywords": ["Natural language processing", "Deep learning", "Multi-modal learning", "Games"], "authors": ["Serhii Havrylov", "Ivan Titov"], "authorids": ["s.havrylov@uva.nl", "titov@uva.nl"]}, "tags": [], "invitation": {"tddate": null, "tmdate": 1487342582599, "tcdate": 1487342582599, "id": "ICLR.cc/2017/workshop/-/paper92/public/comment", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2017/workshop/paper92/reviewers"], "reply": {"forum": "SkaxnKEYg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/workshop/reviewers", "ICLR.cc/2017/pcs"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1487342582599}}}, {"tddate": null, "nonreaders": null, "tmdate": 1489498722314, "tcdate": 1489498383931, "number": 2, "id": "HkdzZ_Hsx", "invitation": "ICLR.cc/2017/workshop/-/paper92/public/comment", "forum": "SkaxnKEYg", "replyto": "ByDGJgEil", "signatures": ["~Serhii_Havrylov1"], "readers": ["everyone"], "writers": ["~Serhii_Havrylov1"], "content": {"title": "Re: Regarding grounding", "comment": "Thank you for your feedback again.\n\n>> Given the authors comments about the discriminativeness of MSCOCO, wouldn't it be possible to induce a language model from a different dataset (say Wikipedia) and see how this compares to the one induced from MSCOCO?\n\nIt is possible to use a different dataset for learning language model, but we leave this experiment for future work.\n\n\n>> ... was there really a need to test the hypothesis of naturalness in the proposed way?\n\nAt first, we tested proposed model with KL regularization. It had worse performance in comparison to unregularized model. To understand why it is the case, we trained Imaginet model (is it an intrinsic property of language or proposed method just failed to discover right protocol?). Without this benchmark, it is hard to determine the reason for the lower performance. \n\nAssuming that it is easier to acquire unannotated sentences we believe that proposed regularization still does make sense. We suppose that with this indirect form of supervision model will require a smaller amount of data for direct supervision."}, "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Emergence of Language with Multi-agent Games: Learning to Communicate with Sequences of Symbols", "abstract": "Learning to communicate through interaction, rather than relying on explicit supervision, is often considered a prerequisite for developing a general AI. We study a setting where two agents engage in playing a referential game and, from scratch, develop a communication protocol necessary to succeed in this game. We require that messages they exchange, both at train and test time, are in the form of a language (i.e. sequences of discrete symbols). As the ultimate goal is to ensure that communication is accomplished in natural language, we perform preliminary experiments where we inject prior information about natural language into our model and study properties of the resulting protocol.", "pdf": "/pdf/bb5181d2caf01da2808e9a0a99274b4da70b20d4.pdf", "TL;DR": "We proposed an efficient learning strategy for developing communication protocol(variable-length sequences of discrete symbols) between two agents for playing a referential game.  ", "paperhash": "havrylov|emergence_of_language_with_multiagent_games_learning_to_communicate_with_sequences_of_symbols", "conflicts": ["uva.nl"], "keywords": ["Natural language processing", "Deep learning", "Multi-modal learning", "Games"], "authors": ["Serhii Havrylov", "Ivan Titov"], "authorids": ["s.havrylov@uva.nl", "titov@uva.nl"]}, "tags": [], "invitation": {"tddate": null, "tmdate": 1487342582599, "tcdate": 1487342582599, "id": "ICLR.cc/2017/workshop/-/paper92/public/comment", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2017/workshop/paper92/reviewers"], "reply": {"forum": "SkaxnKEYg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/workshop/reviewers", "ICLR.cc/2017/pcs"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1487342582599}}}, {"tddate": null, "tmdate": 1489399566616, "tcdate": 1489399566616, "number": 1, "id": "ByDGJgEil", "invitation": "ICLR.cc/2017/workshop/-/paper92/official/comment", "forum": "SkaxnKEYg", "replyto": "H19SnLWse", "signatures": ["ICLR.cc/2017/workshop/paper92/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper92/AnonReviewer1"], "content": {"title": "Regarding grounding", "comment": "I would like to thanks the authors for their comments. I have now raised my score to 7 as I had completely misunderstood the part about grounding (as a note, I hope some more information would find their way into the camera ready, even if at the Appendix).\n\nThe way of performing grounding uses a less direct supervision than what I initially thought.  While the overall approach hammers communicative success, I find KL regularization more interesting than just a fully supervised task. Given the authors comments about the discriminativeness of MSCOCO, wouldn't it be possible to induce a language model from a different dataset (say Wikipedia) and see how this compares to the one induced from MSCOCO?\n\nAt the same time, I was puzzled by their authors' comment \"it worth mentioning that Imaginet establishes an upper bound for any communication protocol that looks like a language from the MSCOCO dataset.\" If that is the case, then was there really a need to test the hypothesis of naturalness in the proposed way? Wasn't the proposed grounding \"doomed to fail\" (in terms of not achieving high communicative success)?\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Emergence of Language with Multi-agent Games: Learning to Communicate with Sequences of Symbols", "abstract": "Learning to communicate through interaction, rather than relying on explicit supervision, is often considered a prerequisite for developing a general AI. We study a setting where two agents engage in playing a referential game and, from scratch, develop a communication protocol necessary to succeed in this game. We require that messages they exchange, both at train and test time, are in the form of a language (i.e. sequences of discrete symbols). As the ultimate goal is to ensure that communication is accomplished in natural language, we perform preliminary experiments where we inject prior information about natural language into our model and study properties of the resulting protocol.", "pdf": "/pdf/bb5181d2caf01da2808e9a0a99274b4da70b20d4.pdf", "TL;DR": "We proposed an efficient learning strategy for developing communication protocol(variable-length sequences of discrete symbols) between two agents for playing a referential game.  ", "paperhash": "havrylov|emergence_of_language_with_multiagent_games_learning_to_communicate_with_sequences_of_symbols", "conflicts": ["uva.nl"], "keywords": ["Natural language processing", "Deep learning", "Multi-modal learning", "Games"], "authors": ["Serhii Havrylov", "Ivan Titov"], "authorids": ["s.havrylov@uva.nl", "titov@uva.nl"]}, "tags": [], "invitation": {"tddate": null, "tmdate": 1487342582590, "tcdate": 1487342582590, "id": "ICLR.cc/2017/workshop/-/paper92/official/comment", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "reply": {"forum": "SkaxnKEYg", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper92/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper92/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/workshop/reviewers", "ICLR.cc/2017/pcs"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/workshop/paper92/reviewers", "ICLR.cc/2017/workshop/paper92/areachairs"], "cdate": 1487342582590}}}, {"tddate": null, "nonreaders": null, "tmdate": 1489271160583, "tcdate": 1489182947956, "number": 1, "id": "BJn1-sgje", "invitation": "ICLR.cc/2017/workshop/-/paper92/official/review", "forum": "SkaxnKEYg", "replyto": "SkaxnKEYg", "signatures": ["ICLR.cc/2017/workshop/paper92/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper92/AnonReviewer1"], "content": {"title": "", "rating": "7: Good paper, accept", "review": "This work presents an extension of Lazaridou et al., 2017 (another ICLR submission) to communication between agents with sequence of symbols. Due to the complexity of the problem (generating a sequence of symbols rather than a single symbol), the authors switch from using 1-hot symbols (and thus RL) to using Gumbel-softmax distribution, thus allowing for training the agents in an end-to-end fashion by backprop. Similar to Lazaridou et al., they attempt grounding the communication protocol to natural language (at this point it's not entirely clear to me whether the authors trained the sender on caption generation or the receiver on caption retrieval). Interestingly, when this happens, the induced communication protocol reflects properties of natural language  (as measured by the omission score) while at the same time decreasing the agents' communication performance (from 95% to 52%).\n\nThe fact that a very similar paper has already been accepted in the same venue takes away some of the novelty points. Moreover, the fact that the communicative success with the grounding task decreases so much hints that the proposed way of grounding is not an effective one (also from the text it's not crustal clear how is the grounding achieved as the KL is measured  between the probability of the messages as produced by the sender and some unspecified p_w(m) distribution). Perhaps, the authors need to look into the strength of the regularizer as it seems to be taking over.  The analysis of the appendix is really interesting, as well as the point about hierarchical coding.\n\nOverall, this is a very intriguing line of research and, as the authors point in the conclusions, many open questions remain. That being said, the current work does feel a bit rushed; many parts are not clear (specifically details regarding the grounding part) and the proposed grounding approach doesn't seem to be effective in terms of communication.\n\npros:\n- extending the rather limited setup of Lazaridou et al. to sequences of symbols, resembling more natural language\n- to the best of my knowledge, the use of Gumbel-distribution for text generation is novel\n\ncons:\n- lack of clarity, especially in the section about grounding\n- proposed grounding method is not effective with regards to communicative success\n- rather limited novelty (given the emphasis of the ICLR workshop) as work is direct extension of previous work\n\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Emergence of Language with Multi-agent Games: Learning to Communicate with Sequences of Symbols", "abstract": "Learning to communicate through interaction, rather than relying on explicit supervision, is often considered a prerequisite for developing a general AI. We study a setting where two agents engage in playing a referential game and, from scratch, develop a communication protocol necessary to succeed in this game. We require that messages they exchange, both at train and test time, are in the form of a language (i.e. sequences of discrete symbols). As the ultimate goal is to ensure that communication is accomplished in natural language, we perform preliminary experiments where we inject prior information about natural language into our model and study properties of the resulting protocol.", "pdf": "/pdf/bb5181d2caf01da2808e9a0a99274b4da70b20d4.pdf", "TL;DR": "We proposed an efficient learning strategy for developing communication protocol(variable-length sequences of discrete symbols) between two agents for playing a referential game.  ", "paperhash": "havrylov|emergence_of_language_with_multiagent_games_learning_to_communicate_with_sequences_of_symbols", "conflicts": ["uva.nl"], "keywords": ["Natural language processing", "Deep learning", "Multi-modal learning", "Games"], "authors": ["Serhii Havrylov", "Ivan Titov"], "authorids": ["s.havrylov@uva.nl", "titov@uva.nl"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489269501784, "id": "ICLR.cc/2017/workshop/-/paper92/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper92/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper92/AnonReviewer1", "ICLR.cc/2017/workshop/paper92/AnonReviewer2"], "reply": {"forum": "SkaxnKEYg", "replyto": "SkaxnKEYg", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper92/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper92/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489269501784}}}, {"tddate": null, "tmdate": 1489269500893, "tcdate": 1489269500893, "number": 2, "id": "SyHbXgGig", "invitation": "ICLR.cc/2017/workshop/-/paper92/official/review", "forum": "SkaxnKEYg", "replyto": "SkaxnKEYg", "signatures": ["ICLR.cc/2017/workshop/paper92/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper92/AnonReviewer2"], "content": {"title": "", "rating": "7: Good paper, accept", "review": "Applying the straight-through Gumbel-softmax estimator for end to end differentiable sequence generation is likely an early instance of a trick we will see used more broadly. This has also not been applied to this task before with previous approaches either being continuous or RL. That the process is the same during training and testing is a welcome benefit.\n\nThe analysis into the \"language\" of the agents has fascinating qualities but is also likely representative of encoding the pre-existing knowledge of the VGG-19. With that said, the hierarchical aspect that is captured would not have been carried through the pre-existing knowledge and is a promising insight.\n\nI'd have been interested in seeing one more example of qualitative analysis beyond 5747 simply due to curiousity and whether the \"animal\" aggregation would continue cleanly to other categories.\n\nAs noted by the other author, additional clarification regarding the grounding towards natural language would be welcome. For future work, other aspects of grounding are possible, such as penalizing longer resulting sequences, given that \"redundancy in encodings\" as noted in the paper doesn't seem a favourable trait for natural language.\n\nWhile many questions are left open for future exploration I believe this work is interesting and introduces some new insights into communication between intelligent agents in such a setting.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Emergence of Language with Multi-agent Games: Learning to Communicate with Sequences of Symbols", "abstract": "Learning to communicate through interaction, rather than relying on explicit supervision, is often considered a prerequisite for developing a general AI. We study a setting where two agents engage in playing a referential game and, from scratch, develop a communication protocol necessary to succeed in this game. We require that messages they exchange, both at train and test time, are in the form of a language (i.e. sequences of discrete symbols). As the ultimate goal is to ensure that communication is accomplished in natural language, we perform preliminary experiments where we inject prior information about natural language into our model and study properties of the resulting protocol.", "pdf": "/pdf/bb5181d2caf01da2808e9a0a99274b4da70b20d4.pdf", "TL;DR": "We proposed an efficient learning strategy for developing communication protocol(variable-length sequences of discrete symbols) between two agents for playing a referential game.  ", "paperhash": "havrylov|emergence_of_language_with_multiagent_games_learning_to_communicate_with_sequences_of_symbols", "conflicts": ["uva.nl"], "keywords": ["Natural language processing", "Deep learning", "Multi-modal learning", "Games"], "authors": ["Serhii Havrylov", "Ivan Titov"], "authorids": ["s.havrylov@uva.nl", "titov@uva.nl"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489269501784, "id": "ICLR.cc/2017/workshop/-/paper92/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper92/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper92/AnonReviewer1", "ICLR.cc/2017/workshop/paper92/AnonReviewer2"], "reply": {"forum": "SkaxnKEYg", "replyto": "SkaxnKEYg", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper92/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper92/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489269501784}}}, {"tddate": null, "nonreaders": null, "tmdate": 1489231143218, "tcdate": 1489230913981, "number": 1, "id": "H19SnLWse", "invitation": "ICLR.cc/2017/workshop/-/paper92/public/comment", "forum": "SkaxnKEYg", "replyto": "BJn1-sgje", "signatures": ["~Serhii_Havrylov1"], "readers": ["everyone"], "writers": ["~Serhii_Havrylov1"], "content": {"title": "Re: No Title", "comment": "Thank you very much for your review and feedback.  We would like to comment on a couple of your remarks and clarify points which we think were misunderstood.\n\n>> The fact that a very similar paper has already been accepted in the same venue takes away some of the novelty points. \n\nIndeed, the setup is inspired by Lazaridou et al., 2017.  Though conceptually it seems natural to go from symbols to sequences of symbols, in practice, it is not straightforward to make such an approach scalable and efficient.  In fact, though several neural network approaches have been proposed for inducing protocols consisting of single symbols (including Lazaridou et al; see the paper for reference), we believe we are the first to generalize the set-up to sequences of symbols and also to show that using sequences results in more efficient communication than using single symbols. Also, apart from the setting, our method and that of Lazaridou et al. are very different. \n\n\n>> the authors switch from using 1-hot symbols (and thus RL) to using Gumbel-softmax distribution\n\nWe would like to emphasize that in the proposed method one-hot symbols are still used during training and testing phases. During training and testing, symbols in messages are generated from Categorical distribution (Gumbel-argmax). The fact that we have used neither continuous messages nor RL is another aspect which differentiates us from previous work on multi-agent protocol induction.\n\n>> (at this point it's not entirely clear to me whether the authors trained the sender on caption generation or the receiver on caption retrieval)\n\nIndeed, this kind of grounding is possible, but, in the proposed approach, we neither trained the Sender for the caption generation task nor trained the Receiver for caption retrieval. Our grounding process consists of imposing a prior on the communication protocol q(m|t). Minimizing the Kullback-Leibler divergence from the natural language to the learned protocol KL[q(m|t)||p_NL(m)] favors generated messages to have a high probability according to the distribution p_NL(m) (natural language) but at the same time should have high entropy. \n\nIn other words, though the word \u2018red\u2019 may not refer to \u2018red\u2019 in the protocol, the goal was to ensure that statistical properties of the protocol are similar to these of the natural language, and see what effect it would have on the communicative success.   In hindsight, maybe we should not have referred to it as \u2018grounding\u2019, as we now realized it is potentially misleading.\n\n>> Moreover, the fact that the communicative success with the grounding task decreases so much hints that the proposed way of grounding is not an effective one ... Perhaps, the authors need to look into the strength of the regularizer as it seems to be taking over.\n\nIn our case, we tested a hypothesis whether favoring \u201cnaturalness\u201d of the protocol makes it more efficient. It turns out the answer is no. Also, we tested, whether agents would start using, e.g., nouns as nouns, adjectives as adjectives, without us imposing stricter forms of supervision (e.g. training the Sender for caption generation).\n\nIt worth mentioning that Imaginet establishes an upper bound for any communication protocol that looks like a language from the MSCOCO dataset. Any protocol that will obey the proposed KL constraint (will look like MSCOCO language) will have worse performance than the Imaginet model or equal. Proposed model has comparable performance to the upper bound (Imaginet). \n\nBy decreasing the impact of the KL regularizer, the communication protocol will less resemble natural language, and that contradicts the goal of the grounding process. Also, one should bear in mind that MSCOCO descriptions were not generated for the referential game, that is why they can be not very discriminative.\n\n\n>> ... the KL is measured  between the probability of the messages as produced by the sender and some unspecified p_w(m) distribution)\n\nAs we mentioned in the extended abstract,  we used an estimated language model p_\u03c9(m). We implemented p_\u03c9(m) as an LSTM language model. We used image captions of randomly selected (50%) images from the training set to estimate parameters of the language model. It is worth mentioning that these images were not used for training the Sender and the Receiver. Unfortunately, given the 3-page constraint on the extended abstract, we could not describe all the details of the set-up.\n\n"}, "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Emergence of Language with Multi-agent Games: Learning to Communicate with Sequences of Symbols", "abstract": "Learning to communicate through interaction, rather than relying on explicit supervision, is often considered a prerequisite for developing a general AI. We study a setting where two agents engage in playing a referential game and, from scratch, develop a communication protocol necessary to succeed in this game. We require that messages they exchange, both at train and test time, are in the form of a language (i.e. sequences of discrete symbols). As the ultimate goal is to ensure that communication is accomplished in natural language, we perform preliminary experiments where we inject prior information about natural language into our model and study properties of the resulting protocol.", "pdf": "/pdf/bb5181d2caf01da2808e9a0a99274b4da70b20d4.pdf", "TL;DR": "We proposed an efficient learning strategy for developing communication protocol(variable-length sequences of discrete symbols) between two agents for playing a referential game.  ", "paperhash": "havrylov|emergence_of_language_with_multiagent_games_learning_to_communicate_with_sequences_of_symbols", "conflicts": ["uva.nl"], "keywords": ["Natural language processing", "Deep learning", "Multi-modal learning", "Games"], "authors": ["Serhii Havrylov", "Ivan Titov"], "authorids": ["s.havrylov@uva.nl", "titov@uva.nl"]}, "tags": [], "invitation": {"tddate": null, "tmdate": 1487342582599, "tcdate": 1487342582599, "id": "ICLR.cc/2017/workshop/-/paper92/public/comment", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2017/workshop/paper92/reviewers"], "reply": {"forum": "SkaxnKEYg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/workshop/reviewers", "ICLR.cc/2017/pcs"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1487342582599}}}], "count": 8}