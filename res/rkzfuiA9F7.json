{"notes": [{"id": "rkzfuiA9F7", "original": "HyxE42g5KX", "number": 336, "cdate": 1538087786308, "ddate": null, "tcdate": 1538087786308, "tmdate": 1545355440974, "tddate": null, "forum": "rkzfuiA9F7", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Projective Subspace Networks For Few-Shot Learning", "abstract": "Generalization from limited examples, usually studied under the umbrella of meta-learning, equips learning techniques with the ability to adapt quickly in dynamical environments and proves to be an essential aspect of lifelong learning. In this paper, we introduce the Projective Subspace Networks (PSN), a deep learning paradigm that learns non-linear embeddings from limited supervision. In contrast to previous studies, the embedding in PSN deems samples of a given class to form an affine subspace. We will show that such modeling leads to robust solutions, yielding competitive results on supervised and semi-supervised few-shot classification. Moreover, our PSN approach has the ability of end-to-end learning. In contrast to previous works, our projective subspace can be thought of as a richer representation capturing higher-order information datapoints for modeling new concepts.", "keywords": ["few-shot", "one-shot", "semi-supervised", "meta-learning"], "authorids": ["christian.simon@anu.edu.au", "piotr.koniusz@data61.csiro.au", "mehrtash.harandi@monash.edu"], "authors": ["Christian Simon", "Piotr Koniusz", "Mehrtash Harandi"], "TL;DR": "We proposed Projective Subspace Networks for few-shot and semi-supervised few-shot learning", "pdf": "/pdf/c69937f88b56088a819eb54ea3979f4731bdca75.pdf", "paperhash": "simon|projective_subspace_networks_for_fewshot_learning", "_bibtex": "@misc{\nsimon2019projective,\ntitle={Projective Subspace Networks For Few-Shot Learning},\nauthor={Christian Simon and Piotr Koniusz and Mehrtash Harandi},\nyear={2019},\nurl={https://openreview.net/forum?id=rkzfuiA9F7},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 13, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "SJgmqUmyeN", "original": null, "number": 1, "cdate": 1544660619243, "ddate": null, "tcdate": 1544660619243, "tmdate": 1545354476458, "tddate": null, "forum": "rkzfuiA9F7", "replyto": "rkzfuiA9F7", "invitation": "ICLR.cc/2019/Conference/-/Paper336/Meta_Review", "content": {"metareview": "The reviewers all like the idea, and though the performance is a little better when compared to prototypical networks, the reviewers felt that the contribution over and above prototypical networks was marginal and none of them was willing to champion the paper. There is merit in that there is increased robustness to outliers, and future iterations of the paper should work to strengthen this aspect.\n\nAs a quick nitpick: based on my reading, and on Figure 3, it looks like there might be a typo in the definition of X_k (bottom of page 4). Right now it is defined in terms of the original data space x, when I think it should be defined in terms of the embedding space f(x). Overall this paper is a good contribution to the few-shot learning area.", "confidence": "3: The area chair is somewhat confident", "recommendation": "Reject", "title": "A robust extension of prototypical networks, but needs a clear motivation for this property."}, "signatures": ["ICLR.cc/2019/Conference/Paper336/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper336/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Projective Subspace Networks For Few-Shot Learning", "abstract": "Generalization from limited examples, usually studied under the umbrella of meta-learning, equips learning techniques with the ability to adapt quickly in dynamical environments and proves to be an essential aspect of lifelong learning. In this paper, we introduce the Projective Subspace Networks (PSN), a deep learning paradigm that learns non-linear embeddings from limited supervision. In contrast to previous studies, the embedding in PSN deems samples of a given class to form an affine subspace. We will show that such modeling leads to robust solutions, yielding competitive results on supervised and semi-supervised few-shot classification. Moreover, our PSN approach has the ability of end-to-end learning. In contrast to previous works, our projective subspace can be thought of as a richer representation capturing higher-order information datapoints for modeling new concepts.", "keywords": ["few-shot", "one-shot", "semi-supervised", "meta-learning"], "authorids": ["christian.simon@anu.edu.au", "piotr.koniusz@data61.csiro.au", "mehrtash.harandi@monash.edu"], "authors": ["Christian Simon", "Piotr Koniusz", "Mehrtash Harandi"], "TL;DR": "We proposed Projective Subspace Networks for few-shot and semi-supervised few-shot learning", "pdf": "/pdf/c69937f88b56088a819eb54ea3979f4731bdca75.pdf", "paperhash": "simon|projective_subspace_networks_for_fewshot_learning", "_bibtex": "@misc{\nsimon2019projective,\ntitle={Projective Subspace Networks For Few-Shot Learning},\nauthor={Christian Simon and Piotr Koniusz and Mehrtash Harandi},\nyear={2019},\nurl={https://openreview.net/forum?id=rkzfuiA9F7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper336/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353252094, "tddate": null, "super": null, "final": null, "reply": {"forum": "rkzfuiA9F7", "replyto": "rkzfuiA9F7", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper336/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper336/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper336/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353252094}}}, {"id": "HkgTot5w14", "original": null, "number": 13, "cdate": 1544165796879, "ddate": null, "tcdate": 1544165796879, "tmdate": 1544173296844, "tddate": null, "forum": "rkzfuiA9F7", "replyto": "S1eTvIOrk4", "invitation": "ICLR.cc/2019/Conference/-/Paper336/Official_Comment", "content": {"title": "Comment", "comment": "We want to thank the reviewer for his/her insightful input and suggestions that led to an improved version of our paper. We have taken all remarks on-board when making our revision. It is our pleasure if the reviewer found our revisions/additional experiments interesting and valuable. We hope other readers will also enjoy our work. We are more than happy to take into account any further suggestions the reviewer may have for our work.\n\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper336/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper336/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper336/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Projective Subspace Networks For Few-Shot Learning", "abstract": "Generalization from limited examples, usually studied under the umbrella of meta-learning, equips learning techniques with the ability to adapt quickly in dynamical environments and proves to be an essential aspect of lifelong learning. In this paper, we introduce the Projective Subspace Networks (PSN), a deep learning paradigm that learns non-linear embeddings from limited supervision. In contrast to previous studies, the embedding in PSN deems samples of a given class to form an affine subspace. We will show that such modeling leads to robust solutions, yielding competitive results on supervised and semi-supervised few-shot classification. Moreover, our PSN approach has the ability of end-to-end learning. In contrast to previous works, our projective subspace can be thought of as a richer representation capturing higher-order information datapoints for modeling new concepts.", "keywords": ["few-shot", "one-shot", "semi-supervised", "meta-learning"], "authorids": ["christian.simon@anu.edu.au", "piotr.koniusz@data61.csiro.au", "mehrtash.harandi@monash.edu"], "authors": ["Christian Simon", "Piotr Koniusz", "Mehrtash Harandi"], "TL;DR": "We proposed Projective Subspace Networks for few-shot and semi-supervised few-shot learning", "pdf": "/pdf/c69937f88b56088a819eb54ea3979f4731bdca75.pdf", "paperhash": "simon|projective_subspace_networks_for_fewshot_learning", "_bibtex": "@misc{\nsimon2019projective,\ntitle={Projective Subspace Networks For Few-Shot Learning},\nauthor={Christian Simon and Piotr Koniusz and Mehrtash Harandi},\nyear={2019},\nurl={https://openreview.net/forum?id=rkzfuiA9F7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper336/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621606357, "tddate": null, "super": null, "final": null, "reply": {"forum": "rkzfuiA9F7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper336/Authors", "ICLR.cc/2019/Conference/Paper336/Reviewers", "ICLR.cc/2019/Conference/Paper336/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper336/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper336/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper336/Authors|ICLR.cc/2019/Conference/Paper336/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper336/Reviewers", "ICLR.cc/2019/Conference/Paper336/Authors", "ICLR.cc/2019/Conference/Paper336/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621606357}}}, {"id": "BJx1MS5PJV", "original": null, "number": 12, "cdate": 1544164615449, "ddate": null, "tcdate": 1544164615449, "tmdate": 1544173265799, "tddate": null, "forum": "rkzfuiA9F7", "replyto": "S1eGjtv0RX", "invitation": "ICLR.cc/2019/Conference/-/Paper336/Official_Comment", "content": {"title": "Comment", "comment": "We want to thank the reviewer for his/her insightful suggestions and support given in preparations of our revised manuscript. Of course, we agree with the Remark 1 and we have already rectified this in our off-line draft. We are more than happy to take into account any further suggestions the reviewer may have for our work.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper336/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper336/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper336/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Projective Subspace Networks For Few-Shot Learning", "abstract": "Generalization from limited examples, usually studied under the umbrella of meta-learning, equips learning techniques with the ability to adapt quickly in dynamical environments and proves to be an essential aspect of lifelong learning. In this paper, we introduce the Projective Subspace Networks (PSN), a deep learning paradigm that learns non-linear embeddings from limited supervision. In contrast to previous studies, the embedding in PSN deems samples of a given class to form an affine subspace. We will show that such modeling leads to robust solutions, yielding competitive results on supervised and semi-supervised few-shot classification. Moreover, our PSN approach has the ability of end-to-end learning. In contrast to previous works, our projective subspace can be thought of as a richer representation capturing higher-order information datapoints for modeling new concepts.", "keywords": ["few-shot", "one-shot", "semi-supervised", "meta-learning"], "authorids": ["christian.simon@anu.edu.au", "piotr.koniusz@data61.csiro.au", "mehrtash.harandi@monash.edu"], "authors": ["Christian Simon", "Piotr Koniusz", "Mehrtash Harandi"], "TL;DR": "We proposed Projective Subspace Networks for few-shot and semi-supervised few-shot learning", "pdf": "/pdf/c69937f88b56088a819eb54ea3979f4731bdca75.pdf", "paperhash": "simon|projective_subspace_networks_for_fewshot_learning", "_bibtex": "@misc{\nsimon2019projective,\ntitle={Projective Subspace Networks For Few-Shot Learning},\nauthor={Christian Simon and Piotr Koniusz and Mehrtash Harandi},\nyear={2019},\nurl={https://openreview.net/forum?id=rkzfuiA9F7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper336/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621606357, "tddate": null, "super": null, "final": null, "reply": {"forum": "rkzfuiA9F7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper336/Authors", "ICLR.cc/2019/Conference/Paper336/Reviewers", "ICLR.cc/2019/Conference/Paper336/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper336/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper336/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper336/Authors|ICLR.cc/2019/Conference/Paper336/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper336/Reviewers", "ICLR.cc/2019/Conference/Paper336/Authors", "ICLR.cc/2019/Conference/Paper336/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621606357}}}, {"id": "S1eTvIOrk4", "original": null, "number": 10, "cdate": 1544025700810, "ddate": null, "tcdate": 1544025700810, "tmdate": 1544026030532, "tddate": null, "forum": "rkzfuiA9F7", "replyto": "SkgcVftYRQ", "invitation": "ICLR.cc/2019/Conference/-/Paper336/Official_Comment", "content": {"title": "Thank you for the revision", "comment": "The experiments studying the effect of outliers for the Mini-Imagenet dataset are very interesting and are an effective way to display that PSN is indeed more robust against different types of outliers. Additionally, thank you for clarifying how the dimensionality of subspace impacts results."}, "signatures": ["ICLR.cc/2019/Conference/Paper336/AnonReviewer2"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper336/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper336/AnonReviewer2", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Projective Subspace Networks For Few-Shot Learning", "abstract": "Generalization from limited examples, usually studied under the umbrella of meta-learning, equips learning techniques with the ability to adapt quickly in dynamical environments and proves to be an essential aspect of lifelong learning. In this paper, we introduce the Projective Subspace Networks (PSN), a deep learning paradigm that learns non-linear embeddings from limited supervision. In contrast to previous studies, the embedding in PSN deems samples of a given class to form an affine subspace. We will show that such modeling leads to robust solutions, yielding competitive results on supervised and semi-supervised few-shot classification. Moreover, our PSN approach has the ability of end-to-end learning. In contrast to previous works, our projective subspace can be thought of as a richer representation capturing higher-order information datapoints for modeling new concepts.", "keywords": ["few-shot", "one-shot", "semi-supervised", "meta-learning"], "authorids": ["christian.simon@anu.edu.au", "piotr.koniusz@data61.csiro.au", "mehrtash.harandi@monash.edu"], "authors": ["Christian Simon", "Piotr Koniusz", "Mehrtash Harandi"], "TL;DR": "We proposed Projective Subspace Networks for few-shot and semi-supervised few-shot learning", "pdf": "/pdf/c69937f88b56088a819eb54ea3979f4731bdca75.pdf", "paperhash": "simon|projective_subspace_networks_for_fewshot_learning", "_bibtex": "@misc{\nsimon2019projective,\ntitle={Projective Subspace Networks For Few-Shot Learning},\nauthor={Christian Simon and Piotr Koniusz and Mehrtash Harandi},\nyear={2019},\nurl={https://openreview.net/forum?id=rkzfuiA9F7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper336/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621606357, "tddate": null, "super": null, "final": null, "reply": {"forum": "rkzfuiA9F7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper336/Authors", "ICLR.cc/2019/Conference/Paper336/Reviewers", "ICLR.cc/2019/Conference/Paper336/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper336/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper336/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper336/Authors|ICLR.cc/2019/Conference/Paper336/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper336/Reviewers", "ICLR.cc/2019/Conference/Paper336/Authors", "ICLR.cc/2019/Conference/Paper336/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621606357}}}, {"id": "rJejdLo5hm", "original": null, "number": 2, "cdate": 1541219954928, "ddate": null, "tcdate": 1541219954928, "tmdate": 1543564443898, "tddate": null, "forum": "rkzfuiA9F7", "replyto": "rkzfuiA9F7", "invitation": "ICLR.cc/2019/Conference/-/Paper336/Official_Review", "content": {"title": "Some experiments are missing. ", "review": "This paper proposes a Projective Subspace Network (PSN) for few-shot learning. The PSN represents each support set of classes as a subspace obtained by SVD. Then the method calculates distances between a query and classes by the projection error to the subspace. Instead of using the prototype of the class center, the subspace representation is more robust to outliers. Though the contribution seems to be incremental, it is a reasonable improvement upon Matching Networks and Prototypical Networks.\n\nPros. \n+ The proposed subspace method is simple and reasonable\n+ The performance is better than some related works on few-shot learning. \n\nCos. \n- The authors claimed that subspace representation is more robust to noise within each class samples. However, this is not supported by experiments. The authors evaluated the distractor classes. However, this is not the case when the outlier existed within each class samples. \n\n- For semi-supervised few-shot learning, the authors proposed a fake class with zero means. The effect of this fake class is not evaluated. \n\n- The dimensionality of subspace (n) seems to be not written.\n\n- The sensitivity analysis of the dimensionality of subspace is missing. For subspace methods, it is essential to evaluate the performance w.r.t the dimension. \n\n- Descriptions in the related work section should be improved. It is unclear how the proposed method is related to K-means, K-modes, and K-prototype. Also, the authors wrote that works (Chan et. 2015, Sun et al. 2017) use PCA or SVD to reduce the dimensionality of feature representation in neural networks. However, both methods do not perform dimensional reduction. PCANet (Chan et al . 2015) obtains convolutional filters by applying PCA to input images or feature maps. SVDNet (Sun et al. 2017) applies SVD for obtaining decorrelated weights in a neural network. \n", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper336/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Projective Subspace Networks For Few-Shot Learning", "abstract": "Generalization from limited examples, usually studied under the umbrella of meta-learning, equips learning techniques with the ability to adapt quickly in dynamical environments and proves to be an essential aspect of lifelong learning. In this paper, we introduce the Projective Subspace Networks (PSN), a deep learning paradigm that learns non-linear embeddings from limited supervision. In contrast to previous studies, the embedding in PSN deems samples of a given class to form an affine subspace. We will show that such modeling leads to robust solutions, yielding competitive results on supervised and semi-supervised few-shot classification. Moreover, our PSN approach has the ability of end-to-end learning. In contrast to previous works, our projective subspace can be thought of as a richer representation capturing higher-order information datapoints for modeling new concepts.", "keywords": ["few-shot", "one-shot", "semi-supervised", "meta-learning"], "authorids": ["christian.simon@anu.edu.au", "piotr.koniusz@data61.csiro.au", "mehrtash.harandi@monash.edu"], "authors": ["Christian Simon", "Piotr Koniusz", "Mehrtash Harandi"], "TL;DR": "We proposed Projective Subspace Networks for few-shot and semi-supervised few-shot learning", "pdf": "/pdf/c69937f88b56088a819eb54ea3979f4731bdca75.pdf", "paperhash": "simon|projective_subspace_networks_for_fewshot_learning", "_bibtex": "@misc{\nsimon2019projective,\ntitle={Projective Subspace Networks For Few-Shot Learning},\nauthor={Christian Simon and Piotr Koniusz and Mehrtash Harandi},\nyear={2019},\nurl={https://openreview.net/forum?id=rkzfuiA9F7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper336/Official_Review", "cdate": 1542234484521, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "rkzfuiA9F7", "replyto": "rkzfuiA9F7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper336/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335701534, "tmdate": 1552335701534, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper336/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "S1eGjtv0RX", "original": null, "number": 8, "cdate": 1543563673969, "ddate": null, "tcdate": 1543563673969, "tmdate": 1543564414454, "tddate": null, "forum": "rkzfuiA9F7", "replyto": "ryl_jMYYAQ", "invitation": "ICLR.cc/2019/Conference/-/Paper336/Official_Comment", "content": {"title": "Thank you for the revision.", "comment": "The authors have added the visualization examples of robustness to outliers/noise in class samples of the training set, and experimental results to show its effectiveness. I think this result is fine. \n\nThey also have clarified the dimensionality of subspace and the less sensitivity to the subspace dimensionality. However, the authors only reported the highest and lowest performances. Thus, it is unclear why the subspace $n=K-1$ for training while $n=2$ for testing gives reasonable and robust results.  \n\nFor the fake class, I wanted to see the performance without this fake class in semi-supervised learning. If the authors only borrowed the idea of Ren et al. (2018), it should be clarified in Sec.4.2. \n\nI think the concerns of robustness to noise and subspace dimensionality have been improved in some degrees. So, I would like to change my recommendation to marginal accept. \n\nMinor problem. \nRamark1 in the revised manuscript describes that when a class samples span an infinitesimal region, then affine subspace becomes to a point. However, SVD on data with small variation would produce orthogonal bases U and small singular value matrix \\Sigma. Thus, using U, the affine subspace does not become a point. I agree with the case when we truncate the subspace corresponding to small singular values. In such case, all bases in U will not be used, i.e. it becomes in the case n=0. \n"}, "signatures": ["ICLR.cc/2019/Conference/Paper336/AnonReviewer3"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper336/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper336/AnonReviewer3", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Projective Subspace Networks For Few-Shot Learning", "abstract": "Generalization from limited examples, usually studied under the umbrella of meta-learning, equips learning techniques with the ability to adapt quickly in dynamical environments and proves to be an essential aspect of lifelong learning. In this paper, we introduce the Projective Subspace Networks (PSN), a deep learning paradigm that learns non-linear embeddings from limited supervision. In contrast to previous studies, the embedding in PSN deems samples of a given class to form an affine subspace. We will show that such modeling leads to robust solutions, yielding competitive results on supervised and semi-supervised few-shot classification. Moreover, our PSN approach has the ability of end-to-end learning. In contrast to previous works, our projective subspace can be thought of as a richer representation capturing higher-order information datapoints for modeling new concepts.", "keywords": ["few-shot", "one-shot", "semi-supervised", "meta-learning"], "authorids": ["christian.simon@anu.edu.au", "piotr.koniusz@data61.csiro.au", "mehrtash.harandi@monash.edu"], "authors": ["Christian Simon", "Piotr Koniusz", "Mehrtash Harandi"], "TL;DR": "We proposed Projective Subspace Networks for few-shot and semi-supervised few-shot learning", "pdf": "/pdf/c69937f88b56088a819eb54ea3979f4731bdca75.pdf", "paperhash": "simon|projective_subspace_networks_for_fewshot_learning", "_bibtex": "@misc{\nsimon2019projective,\ntitle={Projective Subspace Networks For Few-Shot Learning},\nauthor={Christian Simon and Piotr Koniusz and Mehrtash Harandi},\nyear={2019},\nurl={https://openreview.net/forum?id=rkzfuiA9F7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper336/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621606357, "tddate": null, "super": null, "final": null, "reply": {"forum": "rkzfuiA9F7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper336/Authors", "ICLR.cc/2019/Conference/Paper336/Reviewers", "ICLR.cc/2019/Conference/Paper336/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper336/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper336/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper336/Authors|ICLR.cc/2019/Conference/Paper336/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper336/Reviewers", "ICLR.cc/2019/Conference/Paper336/Authors", "ICLR.cc/2019/Conference/Paper336/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621606357}}}, {"id": "HJl27WKY0Q", "original": null, "number": 4, "cdate": 1543242019717, "ddate": null, "tcdate": 1543242019717, "tmdate": 1543290083616, "tddate": null, "forum": "rkzfuiA9F7", "replyto": "Hkgla4cno7", "invitation": "ICLR.cc/2019/Conference/-/Paper336/Official_Comment", "content": {"title": "Response to AnonReviewer1 - Part 1", "comment": "\nQ: ``Why is it the case that ... less sensitive to outliers ...''\n\nA: A prototype in the Prototypical Networks (PN) is the average of a set and as such is sensitive to any perturbation of the set, outliers being one. On the other hand, in PSN, a set is represented by a subspace. To have a noticeable change in the orientation of the subspace, one needs to induce drastic changes to the set. Having said this, we performed two extra-experiments to reinforce our conjecture here. \n\nIn the first one, depicted in Fig. 2, we empirically studied the decision boundaries of PSN and PN. The samples (triangle symbol) are drawn from the normal distribution for a two-class problem (column 1 and 2) and a problem with three classes (column 3 and 4). The outliers (square symbol) are spread around initial samples. The facecolors for outliers indicate to which class they have been assigned. In the odd columns, we can see that the prototypes and subspaces discriminate the classes equally well. \nHowever, in the even columns, it is clearly shown that the outliers sway the prototypes and their decision boundaries while the subspace approach handles them more robustly.   \n\nIn the second experiment, placed in appendix A, we provide the 5-way 5-shot and 5-way 10-shot results on the Mini-ImageNet by adding outliers and noise to support examples. There are two setups conducted to examine the robustness of PSN to outliers and additive noise. In the first experiment, the support set contains samples from classes absent in this set. We did not split the dataset into disjoint inlier/outlier sets though, as the outliers were only presented at the test time, leading to more realistic experiments. In the second experiment, perturbations were generated from a Gaussian distribution with random mean and predefined \nvariance. The results shown in Fig. 4 demonstrate that on both tasks, PSN outperforms PN by a significant gap. Note that, we utilized the same CNN architecture (4-convolutional layers) for both PSN and PN. \n\nQ: ``Why define $X_k$ as the support set examples minus the class prototype instead of just the support examples themselves?''\n\nA: Our idea here is to represent a class by an affine subspace which is indeed a generalization of \nthe concept of linear subspace (where the origin is a common point). We indeed started by using linear-subspaces to model each class but empirically found that affine subspaces perform slightly better. To address this comment, we have added a remark to Section 6.\n\nQ: ``How would it then compare to a 1-shot Prototypical Network?''\n\nA: We cannot build an affine subspace with only one example per class, as such we cannot use PSN to address 1-shot learning problems per se. However, simply augmenting support images to obtain two or more samples per class alleviates such an issue straight away. However, this simple issue is beyond the points we make in our paper (it is an orthogonal research direction in one- and few-shot learning). We have reflected this in Section 6 of the revised draft to address the reviewer's comment. \n\nQ: ``Does this mean the dimensionality n of the sub-space is limited by the number of the support examples''\n\nA: Affirmative. Our idea is to construct a subspace representing the set. The reviewer's comment raises an interesting point, whether parts of the orthogonal complement of each subspace can be used to make better decisions. We believe that investigating the effect of orthogonal complements demands a dedicated study and goes beyond our work. Having said this, we reflect this comment in Section 7.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper336/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper336/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper336/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Projective Subspace Networks For Few-Shot Learning", "abstract": "Generalization from limited examples, usually studied under the umbrella of meta-learning, equips learning techniques with the ability to adapt quickly in dynamical environments and proves to be an essential aspect of lifelong learning. In this paper, we introduce the Projective Subspace Networks (PSN), a deep learning paradigm that learns non-linear embeddings from limited supervision. In contrast to previous studies, the embedding in PSN deems samples of a given class to form an affine subspace. We will show that such modeling leads to robust solutions, yielding competitive results on supervised and semi-supervised few-shot classification. Moreover, our PSN approach has the ability of end-to-end learning. In contrast to previous works, our projective subspace can be thought of as a richer representation capturing higher-order information datapoints for modeling new concepts.", "keywords": ["few-shot", "one-shot", "semi-supervised", "meta-learning"], "authorids": ["christian.simon@anu.edu.au", "piotr.koniusz@data61.csiro.au", "mehrtash.harandi@monash.edu"], "authors": ["Christian Simon", "Piotr Koniusz", "Mehrtash Harandi"], "TL;DR": "We proposed Projective Subspace Networks for few-shot and semi-supervised few-shot learning", "pdf": "/pdf/c69937f88b56088a819eb54ea3979f4731bdca75.pdf", "paperhash": "simon|projective_subspace_networks_for_fewshot_learning", "_bibtex": "@misc{\nsimon2019projective,\ntitle={Projective Subspace Networks For Few-Shot Learning},\nauthor={Christian Simon and Piotr Koniusz and Mehrtash Harandi},\nyear={2019},\nurl={https://openreview.net/forum?id=rkzfuiA9F7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper336/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621606357, "tddate": null, "super": null, "final": null, "reply": {"forum": "rkzfuiA9F7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper336/Authors", "ICLR.cc/2019/Conference/Paper336/Reviewers", "ICLR.cc/2019/Conference/Paper336/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper336/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper336/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper336/Authors|ICLR.cc/2019/Conference/Paper336/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper336/Reviewers", "ICLR.cc/2019/Conference/Paper336/Authors", "ICLR.cc/2019/Conference/Paper336/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621606357}}}, {"id": "SkgcVftYRQ", "original": null, "number": 5, "cdate": 1543242289781, "ddate": null, "tcdate": 1543242289781, "tmdate": 1543242473012, "tddate": null, "forum": "rkzfuiA9F7", "replyto": "SkxkiPMn2Q", "invitation": "ICLR.cc/2019/Conference/-/Paper336/Official_Comment", "content": {"title": "Response to AnonReviewer2", "comment": "\nQ: ``The paper seems to be missing what the dimensionality of the subspace is for the experiments?''\n\nA: In our initial submission, we used a rule of thumb to set the dimensionality (n is $K$-1 during training and two at the testing time). \nTo address the reviewer's comment, we studied the effect of subspace dimension in Section 6 and Appendix B. We examined on four different subspace dimensions on 5-way 5-shot and 5-way 20-shot for training and testing stage and found that the performance is not affected badly.  \n\nQ: it seems too strong to say \"...this makes our paper unparalleled to previous studies\"\n\nA: Duly noted. We have rephrased the introduction according to your comment.\n\nQ: ``Is there previous work that has involved back-propagating through SVD?''\n\nA: Yes, in particular the work of  Ionescu et al. [1], Li et al. [2], and Gou et al. [3] used backpropagation through SVD to address semantic segmentation, large scale classification, and visual recognition problems. We have revised section 2 to introduce these works.\n\n[1] Catalin Ionescu, Orestis Vantzos, and Cristian Sminchisescu. Matrix backpropagation for deep networks with structured layers. ICCV, 2015.\n\n[2] Peihua Li, Jiangtao Xie, Qilong Wang, and Wangmeng Zuo. Is second-order information helpful forlarge-scale visual recognition?. ICCV, 2017.\n\n[3] Mengran Gou, Fei Xiong, Octavia Camps, and Mario Sznaier. MoNet: Moments Embedding Network. CVPR, 2018.\n\nQ: ``In Figure 1, ... but not visually shown how PSN is resistant to them?'' \n\nA:   We have added a new figure (Fig. 2) to our paper to address this comment. There, we empirically studied the decision boundaries of PSN and PN. The samples (triangle symbols) are drawn from normal distribution with two (column 1 and 2) and three (column 3 and 4) different classes. The outliers (square symbols) are spread around initial samples. The facecolors of the outliers show to which class the outliers are assigned to. \n\nIn addition, we empirically studied the effect of outliers on the Mini-ImageNet dataset (kindly see Section 5.3 and Appendix A). Therein, we considered two types of perturbations on two few-shot learning problems, namely 5-way 5-shot and 5-way 10-shot. In all the aforementioned experiments, we observed that PSN is more robust to outliers as compared to PN. \n\nQ: In Discussion, paper states, \"Moreover, the Prototypical Network makes use of the class mean and can be easily incorporated in our testbed\": what does this mean exactly?\n\nA: We meant that it was possible to design a hybrid model and benefit from the inference mechanism provided by the prototypes along our PSN. However, since our aim in this paper is to show and contrast the advantage of modelling with subspaces, we did not pursue such a development. Thus, we removed the confusing text from our revised paper.\n\nQ: ``Performance benefit is a bit disappointing''\n\nA: Our experiments show that PSN is superior to Prototypical Networks (PN) in all cases.\nMore importantly, per our new experiments added to the new draft, PSN is more robust to outliers. Furthermore, PSN copes better with the increasing number of classes (ways) and makes a better use of unlabeled samples (as shown in Section 5).  We believe this shows that PSN is a far better model for the problem at hand."}, "signatures": ["ICLR.cc/2019/Conference/Paper336/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper336/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper336/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Projective Subspace Networks For Few-Shot Learning", "abstract": "Generalization from limited examples, usually studied under the umbrella of meta-learning, equips learning techniques with the ability to adapt quickly in dynamical environments and proves to be an essential aspect of lifelong learning. In this paper, we introduce the Projective Subspace Networks (PSN), a deep learning paradigm that learns non-linear embeddings from limited supervision. In contrast to previous studies, the embedding in PSN deems samples of a given class to form an affine subspace. We will show that such modeling leads to robust solutions, yielding competitive results on supervised and semi-supervised few-shot classification. Moreover, our PSN approach has the ability of end-to-end learning. In contrast to previous works, our projective subspace can be thought of as a richer representation capturing higher-order information datapoints for modeling new concepts.", "keywords": ["few-shot", "one-shot", "semi-supervised", "meta-learning"], "authorids": ["christian.simon@anu.edu.au", "piotr.koniusz@data61.csiro.au", "mehrtash.harandi@monash.edu"], "authors": ["Christian Simon", "Piotr Koniusz", "Mehrtash Harandi"], "TL;DR": "We proposed Projective Subspace Networks for few-shot and semi-supervised few-shot learning", "pdf": "/pdf/c69937f88b56088a819eb54ea3979f4731bdca75.pdf", "paperhash": "simon|projective_subspace_networks_for_fewshot_learning", "_bibtex": "@misc{\nsimon2019projective,\ntitle={Projective Subspace Networks For Few-Shot Learning},\nauthor={Christian Simon and Piotr Koniusz and Mehrtash Harandi},\nyear={2019},\nurl={https://openreview.net/forum?id=rkzfuiA9F7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper336/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621606357, "tddate": null, "super": null, "final": null, "reply": {"forum": "rkzfuiA9F7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper336/Authors", "ICLR.cc/2019/Conference/Paper336/Reviewers", "ICLR.cc/2019/Conference/Paper336/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper336/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper336/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper336/Authors|ICLR.cc/2019/Conference/Paper336/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper336/Reviewers", "ICLR.cc/2019/Conference/Paper336/Authors", "ICLR.cc/2019/Conference/Paper336/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621606357}}}, {"id": "ryl_jMYYAQ", "original": null, "number": 6, "cdate": 1543242399946, "ddate": null, "tcdate": 1543242399946, "tmdate": 1543242399946, "tddate": null, "forum": "rkzfuiA9F7", "replyto": "rJejdLo5hm", "invitation": "ICLR.cc/2019/Conference/-/Paper336/Official_Comment", "content": {"title": "Response to AnonReviewer3", "comment": "\nQ:  The authors claimed that subspace representation is more robust to noise within each class samples. However, this is not supported by experiments. \n\nA: To address this comment, we have revised our paper and have added a new figure along new experiments to study the effect of outliers. Kindly see our response to AnonReviewer2 which we repeat below for convenience:\n\nWe have added a new figure (Fig.2) to our paper to address your comment. There, we empirically studied the decision boundaries of PSN and PN. The samples (triangle symbols) are drawn from the normal distribution with two (column 1 and 2) and three( column 3 and 4) different classes. The outliers (square symbols) are spread around initial samples. The facecolors of the outliers show to which class the outliers are assigned to. \n\nIn addition, we empirically studied the effect of outliers on the Mini-ImageNet dataset (see Section 5.3 and Appendix A). Therein, we considered two types of perturbations on two few-shot learning problems, namely  5-way 5-shot and 5-way 10-shot. In all the aforementioned experiments, we observed that PSN is more robust to outliers compared to PN. \n\nQ: The dimensionality of subspace (n) seems to be not written.  The sensitivity analysis of the dimensionality of subspace is missing. \n\nA: We have revised our work (section 6) and added \nan appendix (Appendix B) to our paper and studied the sensitivity of PSN to the dimensionality of subspaces. Based on our experiments, PSN is robust to variations in their subspace dimensionality to a great degree.\n\nQ: For semi-supervised few-shot learning, the authors proposed a fake class with zero means. The effect of this fake class is not evaluated.\n\nA: We actually 'borrow' this idea from Ren et al. (2018) (fake class with zero mean) but we adapt it to our subspace model. However, unlike the variant of PN for semi-supervised learning proposed by Ren et al.(2018), our approach does not need to model any subspaces for the distractor classes. \n\nQ: Descriptions in the related work section should be improved. \n\nA: We have taken this advice on board and revised Section 2 accordingly. Aside from rewording, we have added references to methods that use backpropagation through SVD. \n"}, "signatures": ["ICLR.cc/2019/Conference/Paper336/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper336/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper336/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Projective Subspace Networks For Few-Shot Learning", "abstract": "Generalization from limited examples, usually studied under the umbrella of meta-learning, equips learning techniques with the ability to adapt quickly in dynamical environments and proves to be an essential aspect of lifelong learning. In this paper, we introduce the Projective Subspace Networks (PSN), a deep learning paradigm that learns non-linear embeddings from limited supervision. In contrast to previous studies, the embedding in PSN deems samples of a given class to form an affine subspace. We will show that such modeling leads to robust solutions, yielding competitive results on supervised and semi-supervised few-shot classification. Moreover, our PSN approach has the ability of end-to-end learning. In contrast to previous works, our projective subspace can be thought of as a richer representation capturing higher-order information datapoints for modeling new concepts.", "keywords": ["few-shot", "one-shot", "semi-supervised", "meta-learning"], "authorids": ["christian.simon@anu.edu.au", "piotr.koniusz@data61.csiro.au", "mehrtash.harandi@monash.edu"], "authors": ["Christian Simon", "Piotr Koniusz", "Mehrtash Harandi"], "TL;DR": "We proposed Projective Subspace Networks for few-shot and semi-supervised few-shot learning", "pdf": "/pdf/c69937f88b56088a819eb54ea3979f4731bdca75.pdf", "paperhash": "simon|projective_subspace_networks_for_fewshot_learning", "_bibtex": "@misc{\nsimon2019projective,\ntitle={Projective Subspace Networks For Few-Shot Learning},\nauthor={Christian Simon and Piotr Koniusz and Mehrtash Harandi},\nyear={2019},\nurl={https://openreview.net/forum?id=rkzfuiA9F7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper336/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621606357, "tddate": null, "super": null, "final": null, "reply": {"forum": "rkzfuiA9F7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper336/Authors", "ICLR.cc/2019/Conference/Paper336/Reviewers", "ICLR.cc/2019/Conference/Paper336/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper336/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper336/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper336/Authors|ICLR.cc/2019/Conference/Paper336/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper336/Reviewers", "ICLR.cc/2019/Conference/Paper336/Authors", "ICLR.cc/2019/Conference/Paper336/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621606357}}}, {"id": "Bklp1-tYRQ", "original": null, "number": 3, "cdate": 1543241956536, "ddate": null, "tcdate": 1543241956536, "tmdate": 1543242036502, "tddate": null, "forum": "rkzfuiA9F7", "replyto": "Hkgla4cno7", "invitation": "ICLR.cc/2019/Conference/-/Paper336/Official_Comment", "content": {"title": "Response to AnonReviewer1- Part 2", "comment": "\nQ: ``How to set n (the dimensionality of each subspace) is not obvious.''\n\nA: To address the concern here, we performed an extra experiment on various subspace dimensions ($n$) (kindly see Appendix B for the details). \nIn particular, we considered the problems of 5-way 5-shot and 5-way 20-shot and studied the effect of subspace dimensionality on overall performances. For instance, we varied $n$ from 2 to 5 in both training and testing for the 5-way 5-shot experiment. The experiments show that the algorithm is robust to the dimensionality to a great degree. Our rule of thumb and recommendation in the paper is to set the dimensionality of subspace as $K-1$ for training and $2$ for testing (kindly see Appendix B).\n\nQ: ``if more examples are available it may be less appropriate to create a prototype and more beneficial to create a sub-space?''\n \nA: For this comment, we have provided a thorough analysis by increasing the value of ways and shots in Table 3 of our revised work. In this experiment, we trained \nPSN and PN on 5-way 5-shot setting. Then, at the test time, we provided the trained models with support-sets of various ways and shots. \nThis indeed simulates more realistic tests. Our proposed method can outperform PN with a tangible gap when the number of ways and shots increases.\n\nQ: Can we recover Prototypical Networks as a special case of PSN? If so, how? It would be neat to show under which conditions these are equivalent.\n\nA: If the embedded features have a very minuscule variance, PSN is equivalent to PN. We really appreciate this comment and have reflected it in Section 6.\n\nQ: (a)Are distractor classes sampled from a disjoint pool of classes, or is it that, for example, a class which is a distractor in an episode is a non-distractor in another episode.\n(b)Similarly for labeled / unlabaled at training time. Can the same example appear as labeled in one episode but unlabaled in another? \n\nA: There is a disjoint pool between unlabeled and labeled examples applied to all classes. Unlabeled data in an episode contains only examples from unlabeled pool. Moreover, there are two types of experiments conducted for semi-supervised learning: with and without distractor classes. The distractor class is taken from other classes irrelevant to the current episode and items are sampled from unlabeled pool. Note that a distractor class can appear in another episode as a non-distractor class (this is the protocol).\n\nQ: (a) ``In the introduction, regarding contribution iii. A more appropriate way to describe this is as exploring generalization to different numbers of classes, or \u2018ways\u2019 at test time than what was used at training time.''\n(b) ``In the last line of section 5.3, use N-way instead of K-way''\n\nA: Thank you, we have revised the relevant text based on the above comment."}, "signatures": ["ICLR.cc/2019/Conference/Paper336/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper336/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper336/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Projective Subspace Networks For Few-Shot Learning", "abstract": "Generalization from limited examples, usually studied under the umbrella of meta-learning, equips learning techniques with the ability to adapt quickly in dynamical environments and proves to be an essential aspect of lifelong learning. In this paper, we introduce the Projective Subspace Networks (PSN), a deep learning paradigm that learns non-linear embeddings from limited supervision. In contrast to previous studies, the embedding in PSN deems samples of a given class to form an affine subspace. We will show that such modeling leads to robust solutions, yielding competitive results on supervised and semi-supervised few-shot classification. Moreover, our PSN approach has the ability of end-to-end learning. In contrast to previous works, our projective subspace can be thought of as a richer representation capturing higher-order information datapoints for modeling new concepts.", "keywords": ["few-shot", "one-shot", "semi-supervised", "meta-learning"], "authorids": ["christian.simon@anu.edu.au", "piotr.koniusz@data61.csiro.au", "mehrtash.harandi@monash.edu"], "authors": ["Christian Simon", "Piotr Koniusz", "Mehrtash Harandi"], "TL;DR": "We proposed Projective Subspace Networks for few-shot and semi-supervised few-shot learning", "pdf": "/pdf/c69937f88b56088a819eb54ea3979f4731bdca75.pdf", "paperhash": "simon|projective_subspace_networks_for_fewshot_learning", "_bibtex": "@misc{\nsimon2019projective,\ntitle={Projective Subspace Networks For Few-Shot Learning},\nauthor={Christian Simon and Piotr Koniusz and Mehrtash Harandi},\nyear={2019},\nurl={https://openreview.net/forum?id=rkzfuiA9F7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper336/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621606357, "tddate": null, "super": null, "final": null, "reply": {"forum": "rkzfuiA9F7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper336/Authors", "ICLR.cc/2019/Conference/Paper336/Reviewers", "ICLR.cc/2019/Conference/Paper336/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper336/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper336/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper336/Authors|ICLR.cc/2019/Conference/Paper336/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper336/Reviewers", "ICLR.cc/2019/Conference/Paper336/Authors", "ICLR.cc/2019/Conference/Paper336/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621606357}}}, {"id": "H1xWb1tFCQ", "original": null, "number": 1, "cdate": 1543241464941, "ddate": null, "tcdate": 1543241464941, "tmdate": 1543241464941, "tddate": null, "forum": "rkzfuiA9F7", "replyto": "rkzfuiA9F7", "invitation": "ICLR.cc/2019/Conference/-/Paper336/Official_Comment", "content": {"title": "General Comments", "comment": "We thank the reviewers for their valuable comments and feedback. \nWe have taken onboard all the comments and revised our work accordingly. In particular,\n\n1. We empirically studied the robustness of our algorithm to outliers for the Mini-ImageNet dataset. \nThis is reflected in Section 5 and Appendix A of our revised draft. \nOur study reveals that compared to Prototypical Networks (PN hereafter), PSN is more robust to outliers and additive noise.\n\n2. We  studied the effect of the dimensionality of the subspaces in PSN. This extra experiment is discussed in Section 6 and Appendix B. Our experiments show that PSN behaves robustly with respect to subspace dimensionality in a wide-range. \n\n3. We also studied a form of generalization ability of PSN in Section 5. This is achieved by increasing the number of shot (K) and way (N) for the experiment on the Mini-ImageNet. Again, we observed that PSN outperforms PN by a visible margin.\n\n\nBelow, we provide point-by-point responses to comments by quoting excerpts from the reviews.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper336/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper336/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper336/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Projective Subspace Networks For Few-Shot Learning", "abstract": "Generalization from limited examples, usually studied under the umbrella of meta-learning, equips learning techniques with the ability to adapt quickly in dynamical environments and proves to be an essential aspect of lifelong learning. In this paper, we introduce the Projective Subspace Networks (PSN), a deep learning paradigm that learns non-linear embeddings from limited supervision. In contrast to previous studies, the embedding in PSN deems samples of a given class to form an affine subspace. We will show that such modeling leads to robust solutions, yielding competitive results on supervised and semi-supervised few-shot classification. Moreover, our PSN approach has the ability of end-to-end learning. In contrast to previous works, our projective subspace can be thought of as a richer representation capturing higher-order information datapoints for modeling new concepts.", "keywords": ["few-shot", "one-shot", "semi-supervised", "meta-learning"], "authorids": ["christian.simon@anu.edu.au", "piotr.koniusz@data61.csiro.au", "mehrtash.harandi@monash.edu"], "authors": ["Christian Simon", "Piotr Koniusz", "Mehrtash Harandi"], "TL;DR": "We proposed Projective Subspace Networks for few-shot and semi-supervised few-shot learning", "pdf": "/pdf/c69937f88b56088a819eb54ea3979f4731bdca75.pdf", "paperhash": "simon|projective_subspace_networks_for_fewshot_learning", "_bibtex": "@misc{\nsimon2019projective,\ntitle={Projective Subspace Networks For Few-Shot Learning},\nauthor={Christian Simon and Piotr Koniusz and Mehrtash Harandi},\nyear={2019},\nurl={https://openreview.net/forum?id=rkzfuiA9F7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper336/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621606357, "tddate": null, "super": null, "final": null, "reply": {"forum": "rkzfuiA9F7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper336/Authors", "ICLR.cc/2019/Conference/Paper336/Reviewers", "ICLR.cc/2019/Conference/Paper336/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper336/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper336/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper336/Authors|ICLR.cc/2019/Conference/Paper336/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper336/Reviewers", "ICLR.cc/2019/Conference/Paper336/Authors", "ICLR.cc/2019/Conference/Paper336/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621606357}}}, {"id": "SkxkiPMn2Q", "original": null, "number": 3, "cdate": 1541314455136, "ddate": null, "tcdate": 1541314455136, "tmdate": 1541534081634, "tddate": null, "forum": "rkzfuiA9F7", "replyto": "rkzfuiA9F7", "invitation": "ICLR.cc/2019/Conference/-/Paper336/Official_Review", "content": {"title": "Interesting extension to embedding-based approaches to few-shot learning but results are a bit disappointing", "review": "This paper considers the problem of few-shot learning and proposes a new embedding-based approach. In contrast to previous work (such as Matching Networks and Prototypical Networks) where distance is computed in pure embedding space, this work proposes computing a low-dimensional subspace to represent a class and using the distance from an embedded query point to this subspace. The low-dimensional subspace for a class is computed by running truncated Singular Value Decomposition on the normalized embeddings of all points in the support set for that class and using the top n left singular vectors as the basis for the class's subspace. The authors also propose an extension to their model to the semi-supervised few-shot learning setting by incorporating masked-mean computation and zero-mean cluster for distractor items (both ideas borrowed from Renn 2017 for prototypical networks). Experiments are conducted on Mini-Imagenet in the few-shot learning setting and on Mini-Imagenet and Tiered-ImageNet in the semi-supervised few-shot learning setting.\n\nPros:\n- Proposed idea is novel and proposes an interesting change to existing embedding-based few-shot learning techniques.\n\nCons:\n- Performance benefit is a bit disappointing; Mini-Imagenet few-shot performance improvement relative to Prototypical-Nets is minimal (barely 1% for 5way-5shot and 20way-5shot case). For semi-supervised experiments, there is bigger improvement for Mini-Imagenet (4% for both without distractors and with distractors) but less so for Tiered-ImageNet (close to 0% for without distractors and with distractors).\n\nRemarks:\n- The paper seems to be missing what the dimensionality of the subspace is for the experiments? Was this picked using validation set performance?\n- In first paragraph of page 2, it seems too strong to say \"...this makes our paper unparalleled to previous studies\"; maybe change to \"...this make our proposed model novel relative to previous work\"\n- Is there previous work that has involved back-propagating through SVD? It would be useful to mention these as references.\n- In Figure 1, it is visually shown how outliers can negatively impact Matching-Networks and Prototypical-Networks but not visually shown how PSN is resistant to them?\n- The claim is made that the proposed method is more robust to outliers. Is there more of a justification that can be given for this? Either in terms of some intuition or an experiment that can be run? For example, can it be shown that outliers cause the prototype of a class to move a lot (in terms of distance from original prototype without outliers) whereas the original subspace compared to subspace with outliers is less different by measuring this on Mini-Imagenet?\n- Typo on page 5: \"in what follwos\" => \"in what follows\"\n- In Discussion, paper states, \"Moreover, the Prototypical Network makes use of the class mean and can be easily incorporated in our testbed\": what does this mean exactly?", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper336/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Projective Subspace Networks For Few-Shot Learning", "abstract": "Generalization from limited examples, usually studied under the umbrella of meta-learning, equips learning techniques with the ability to adapt quickly in dynamical environments and proves to be an essential aspect of lifelong learning. In this paper, we introduce the Projective Subspace Networks (PSN), a deep learning paradigm that learns non-linear embeddings from limited supervision. In contrast to previous studies, the embedding in PSN deems samples of a given class to form an affine subspace. We will show that such modeling leads to robust solutions, yielding competitive results on supervised and semi-supervised few-shot classification. Moreover, our PSN approach has the ability of end-to-end learning. In contrast to previous works, our projective subspace can be thought of as a richer representation capturing higher-order information datapoints for modeling new concepts.", "keywords": ["few-shot", "one-shot", "semi-supervised", "meta-learning"], "authorids": ["christian.simon@anu.edu.au", "piotr.koniusz@data61.csiro.au", "mehrtash.harandi@monash.edu"], "authors": ["Christian Simon", "Piotr Koniusz", "Mehrtash Harandi"], "TL;DR": "We proposed Projective Subspace Networks for few-shot and semi-supervised few-shot learning", "pdf": "/pdf/c69937f88b56088a819eb54ea3979f4731bdca75.pdf", "paperhash": "simon|projective_subspace_networks_for_fewshot_learning", "_bibtex": "@misc{\nsimon2019projective,\ntitle={Projective Subspace Networks For Few-Shot Learning},\nauthor={Christian Simon and Piotr Koniusz and Mehrtash Harandi},\nyear={2019},\nurl={https://openreview.net/forum?id=rkzfuiA9F7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper336/Official_Review", "cdate": 1542234484521, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "rkzfuiA9F7", "replyto": "rkzfuiA9F7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper336/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335701534, "tmdate": 1552335701534, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper336/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "Hkgla4cno7", "original": null, "number": 1, "cdate": 1540297911649, "ddate": null, "tcdate": 1540297911649, "tmdate": 1541534081182, "tddate": null, "forum": "rkzfuiA9F7", "replyto": "rkzfuiA9F7", "invitation": "ICLR.cc/2019/Conference/-/Paper336/Official_Review", "content": {"title": "Neat idea but requires clarifying the merits of the approach and differences from previous work", "review": "This paper presents a new method for fully- and semi-supervised few-shot classification that is based on learning a general embedding as usual, and then learning a sub-space of it for each class. A query point is then classified as the class whose sub-space is closest to it.\n\nPros: This is a neat idea and achieves competitive results. Learning a sub-space per class makes intuitive sense to me since it\u2019s plausible that there is a lower-dimensional subspace of the overall embedding space that captures the properties that are common to only examples of a certain class. If this is indeed the case, it seems that indeed classifying query examples into classes based on their distances from the corresponding sub-spaces would lead to good discrimination. \n\nCons: First, an inherent limitation is that this approach is not applicable to one-shot learning, and I have doubts in its merit for very low shot learning (explained below). Second, I\u2019m missing the justification behind a key point used to motivate the approach, which requires clarification (explained below). Third, I feel that certain aspects of the approach were unclear (details to follow). Finally, I feel more analysis is needed to better understand the differences of this method from previous work (concrete suggestions follow). For semi-supervised learning, the novelty regarding how the unlabeled examples are incorporated is limited, as the approach used is previously-introduced in Ren et al, 2018.\n\nOverall, even though I like the idea and the results are good, there are a few points, mentioned in the above section that I feel require additional work before I can strongly recommend acceptance. Most importantly, relating to getting more intuition about why and when this works best, and tying it in better with previous approaches. \n\nA key point requiring clarification.\nThere is a key fact that the authors used to motivate this approach which remains unclear to me: why is it the case that this approach is less sensitive to outliers than previous approaches? In Figure 1, an outlier is pictured in each of subfigures (a) and (b) corresponding to Matching and Prototypical Networks, but not in subfigure (c) which corresponds to PSN. No explanation is provided to justify this conjecture, other than empirical evaluation that is based on the overall accuracy only. In particular, since SVD is used to obtain the sub-spaces, instead of an end-to-end learned projector that directly optimizes the query set accuracy, it\u2019s not clear why if a support point is an outlier it would not affect the sub-space creation. If I\u2019m missing something, please clarify!\n\n(A) Comments on the approach.\n(1) Why define X_k as the support set examples minus the class prototype instead of just the support examples themselves? The latter seems simpler, and should have all the required information for shaping the class\u2019 subspace.\n(2) Note that if X_k is defined as [x_{k,1}, \\dots, x_{k,K}] as proposed in the above point (ie. without subtracting the class mean from each support point) then this method would have been applicable to 1-shot too. How would it then compare to a 1-shot Prototypical Network? Notice that in this case the mean of the class is equal to this one example.\n(3) In general, the truncated SVD decomposition for a class can be written using the matrices U, \\Sigma and V^T with dimensions [D, n], [n, n] and [n, K] respectively, where D is the embedding dimensionality and K is the number of support points belonging to the given class. The middle matrix \\Sigma in the non-truncated version would have dimensions [D, K]. Does this mean that when truncating, n is enforced to be smaller than each of D and K? This would mean that the dimensionality n of the sub-space is limited by the number of the support examples, which in some cases may be very small in few-shot learning. Can you comment on this?\n(4) How to set n (the dimensionality of each subspace) is not obvious. What values were explored? Is there a sweet spot in the trade-off between the observed complexity and the final accuracy?\n\n(B) Comparison with Prototypical Networks.\n(1) In what situations do we expect learning a sub-space per class to do better than learning a  prototype per class? For example, Figure 4 shows the test-time performance as a function of the test \u2018way\u2019. A perhaps more interesting analysis would be to compare the models\u2019 performance as a function of the test *shot*: if more examples are available it may be less appropriate to create a prototype and more beneficial to create a sub-space? \n(2) Can we recover Prototypical Networks as a special case of PSN? If so, how? It would be neat to show under which conditions these are equivalent.\n\n(C) Clarifications regarding the semi-supervised setup.\n(1) Are distractor classes sampled from a disjoint pool of classes, or is it that, for example, a class which is a distractor in an episode is a non-distractor in another episode.\n(2) Similarly for labeled / unlabaled at training time. Can the same example appear as labeled in one episode but unlabaled in another? In Ren et al, 2018, this was prevented by creating an additional labeled/unlabeled split even for the training examples. Therefore they use strictly less overall information at meta-training time than if that split weren\u2019t used. To be comparable with them, it\u2019s important to apply this same setup.\n\n(D) Additional minor comments.\n(1) \u201cTo work at the presence of distractors, we propose to use a fake class with zero mean\u201d. Note that this was already proposed in Ren et al, 2018. They used a zero-mean, high-variance additional cluster whose aim was to \u2018soak up\u2019 the distractor examples to prevent them for polluting legitimate clusters (this was the second model they proposed).\n(2) In the introduction, regarding contribution iii. A more appropriate way to describe this is as exploring generalization to different numbers of classes, or \u2018ways\u2019 at test time than what was used at training time.\n(3) Gidaris and Komodakis (2018) is described in the related work as using a more complicated pipeline. Note however that their pipeline is in place for solving a more challenging problem than standard few-shot classification: they study how a model can maintain the ability to remember training classes while rapidly learning about new \u2018test\u2019 classes.\n(4) In the last line of section 5.3, use N-way instead of K-way since in the rest of the paper K was used to refer to the shot, not the way.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper336/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Projective Subspace Networks For Few-Shot Learning", "abstract": "Generalization from limited examples, usually studied under the umbrella of meta-learning, equips learning techniques with the ability to adapt quickly in dynamical environments and proves to be an essential aspect of lifelong learning. In this paper, we introduce the Projective Subspace Networks (PSN), a deep learning paradigm that learns non-linear embeddings from limited supervision. In contrast to previous studies, the embedding in PSN deems samples of a given class to form an affine subspace. We will show that such modeling leads to robust solutions, yielding competitive results on supervised and semi-supervised few-shot classification. Moreover, our PSN approach has the ability of end-to-end learning. In contrast to previous works, our projective subspace can be thought of as a richer representation capturing higher-order information datapoints for modeling new concepts.", "keywords": ["few-shot", "one-shot", "semi-supervised", "meta-learning"], "authorids": ["christian.simon@anu.edu.au", "piotr.koniusz@data61.csiro.au", "mehrtash.harandi@monash.edu"], "authors": ["Christian Simon", "Piotr Koniusz", "Mehrtash Harandi"], "TL;DR": "We proposed Projective Subspace Networks for few-shot and semi-supervised few-shot learning", "pdf": "/pdf/c69937f88b56088a819eb54ea3979f4731bdca75.pdf", "paperhash": "simon|projective_subspace_networks_for_fewshot_learning", "_bibtex": "@misc{\nsimon2019projective,\ntitle={Projective Subspace Networks For Few-Shot Learning},\nauthor={Christian Simon and Piotr Koniusz and Mehrtash Harandi},\nyear={2019},\nurl={https://openreview.net/forum?id=rkzfuiA9F7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper336/Official_Review", "cdate": 1542234484521, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "rkzfuiA9F7", "replyto": "rkzfuiA9F7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper336/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335701534, "tmdate": 1552335701534, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper336/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 14}