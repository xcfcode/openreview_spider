{"notes": [{"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396558403, "tcdate": 1486396558403, "number": 1, "id": "SJIchMIue", "invitation": "ICLR.cc/2017/conference/-/paper388/acceptance", "forum": "ByC7ww9le", "replyto": "ByC7ww9le", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision", "comment": "Three knowledgable reviewers recommend rejection. While they agree that the paper has interesting aspects, they suggest a more convincing evaluation. The authors did not address some of the reviewer's concerns. The AC strongly encourages the authors to improve their paper and resubmit it to a future conference."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Gaussian Attention Model and Its Application to Knowledge Base Embedding and Question Answering", "abstract": "We propose the Gaussian attention model for content-based neural memory\naccess. With the proposed attention model, a neural network has the\nadditional degree of freedom to control the focus of its attention from\na laser sharp attention to a broad attention. It is applicable whenever\nwe can assume that the distance in the latent space reflects some notion\nof semantics. We use the proposed attention model as a scoring function\nfor the embedding of a knowledge base into a continuous vector space and\nthen train a model that performs question answering about the entities\nin the knowledge base. The proposed attention model can handle both the\npropagation of uncertainty when following a series of relations and also\nthe conjunction of conditions in a natural way. On a dataset of soccer\nplayers who participated in the FIFA World Cup 2014, we demonstrate that\nour model can handle both path queries and conjunctive queries well.", "pdf": "/pdf/529ba3e72398d2711ce7eb0ef6fe03087802afe2.pdf", "TL;DR": "We make (simple) knowledge base queries differentiable using the Gaussian attention model.", "paperhash": "zhang|gaussian_attention_model_and_its_application_to_knowledge_base_embedding_and_question_answering", "keywords": ["Natural language processing", "Supervised Learning", "Deep learning"], "conflicts": ["uchicago.edu", "microsoft.com"], "authors": ["Liwen Zhang", "John Winn", "Ryota Tomioka"], "authorids": ["liwenz@cs.uchicago.edu", "jwinn@microsoft.com", "ryoto@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396558932, "id": "ICLR.cc/2017/conference/-/paper388/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "ByC7ww9le", "replyto": "ByC7ww9le", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396558932}}}, {"tddate": null, "tmdate": 1482309952662, "tcdate": 1482309952662, "number": 3, "id": "H1YrWTvNx", "invitation": "ICLR.cc/2017/conference/-/paper388/official/review", "forum": "ByC7ww9le", "replyto": "ByC7ww9le", "signatures": ["ICLR.cc/2017/conference/paper388/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper388/AnonReviewer2"], "content": {"title": "review", "rating": "5: Marginally below acceptance threshold", "review": "The contribution of this paper can be summarized as:\n\n1, A TransGaussian model (in a similar idea of TransE) which models the subject / object embeddings in a parameterization of Gaussian distribution.  The model can be naturally adapted to path queries like the formulation of (Guu et al, 2015).\n2. Along with the entity / relation representations trained by TransGaussian, an LSTM + attention model is built on natural language questions, aiming at learning a distribution (not normalized though) over relations for question answering.\n3. Experiments on a generated WorldCup2014 dataset, focusing on path queries and conjunctive queries.\n\nOverall, I think the Gaussian parameterization exhibits some nice properties, and could be suitable to KB completion and question answering. However, some details and the main experimental results are not convincing enough to me.  The paper writing also needs to be improved. More comments below:\n\n[Major comments]\n\n- My main concern is that that evaluation results are NOT strong. Either knowledge base completion or KB-based question answering, there are many existing and competitive benchmarks (e.g., FB15k / WebQuestions). Experimenting with such a tiny WordCup2014 dataset is not convincing.  Moreover, the questions are just generated by a few templates, which is far from NL questions. I am not even not sure why we need to apply an LSTM in such scenario. The paper would be much stronger if you can demonstrate its effectiveness on the above benchmarks. \n\n- Conjunctive queries:  the current model assumes that all the detected entities in the question could be aligned to one or more relations and we can take conjunctions in the end. This assumption might be not always correct, so it is more necessary to justify this on real QA datasets.\n\n- The model is named as  \u201cGaussian attention\u201d and I kind of think it is not very closely related to well-known attention mechanism, but more related to KB embedding literature.\n\n[Minor comments]\n- I find Figure 2 a bit confusing. The first row of orange blocks denote KB relations, and the second row of those denote every single word of the NL question. Maybe make it clearer?\n\n- Besides \u201centity recognition\u201d, usually we still need an \u201centity linker\u201d component which links the text mention to the KB entity. \n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Gaussian Attention Model and Its Application to Knowledge Base Embedding and Question Answering", "abstract": "We propose the Gaussian attention model for content-based neural memory\naccess. With the proposed attention model, a neural network has the\nadditional degree of freedom to control the focus of its attention from\na laser sharp attention to a broad attention. It is applicable whenever\nwe can assume that the distance in the latent space reflects some notion\nof semantics. We use the proposed attention model as a scoring function\nfor the embedding of a knowledge base into a continuous vector space and\nthen train a model that performs question answering about the entities\nin the knowledge base. The proposed attention model can handle both the\npropagation of uncertainty when following a series of relations and also\nthe conjunction of conditions in a natural way. On a dataset of soccer\nplayers who participated in the FIFA World Cup 2014, we demonstrate that\nour model can handle both path queries and conjunctive queries well.", "pdf": "/pdf/529ba3e72398d2711ce7eb0ef6fe03087802afe2.pdf", "TL;DR": "We make (simple) knowledge base queries differentiable using the Gaussian attention model.", "paperhash": "zhang|gaussian_attention_model_and_its_application_to_knowledge_base_embedding_and_question_answering", "keywords": ["Natural language processing", "Supervised Learning", "Deep learning"], "conflicts": ["uchicago.edu", "microsoft.com"], "authors": ["Liwen Zhang", "John Winn", "Ryota Tomioka"], "authorids": ["liwenz@cs.uchicago.edu", "jwinn@microsoft.com", "ryoto@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512601059, "id": "ICLR.cc/2017/conference/-/paper388/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper388/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper388/AnonReviewer3", "ICLR.cc/2017/conference/paper388/AnonReviewer1", "ICLR.cc/2017/conference/paper388/AnonReviewer2"], "reply": {"forum": "ByC7ww9le", "replyto": "ByC7ww9le", "writers": {"values-regex": "ICLR.cc/2017/conference/paper388/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper388/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512601059}}}, {"tddate": null, "tmdate": 1482178590708, "tcdate": 1482178590708, "number": 2, "id": "rkDXxTrNe", "invitation": "ICLR.cc/2017/conference/-/paper388/official/review", "forum": "ByC7ww9le", "replyto": "ByC7ww9le", "signatures": ["ICLR.cc/2017/conference/paper388/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper388/AnonReviewer1"], "content": {"title": "Review", "rating": "4: Ok but not good enough - rejection", "review": "This paper presents extensions to previous work using embeddings for modeling Knowledge Bases and performing Q&A on them, centered around the use of multivariate gaussian likelihood instead of inner products to score attention. This is supposed to allow more control on the attention by dealing with its spread.\n\nThis is a dense paper centered around a quite complicated model. With the supplementary material, this makes a 16p paper. It might be clearer to make 2 separate papers: one on KB completion and another one on Q&A.\n\nI like the idea of controlling the spread of the attention. This makes sense. However, I do not feel that this paper is convincing enough to justify its use compared to usual inner products.\n\nFor several reasons:\n- These should be more ablation experiments to separate the different pieces of the model and study their influence separately. The only interesting point in that sense is Table 8 in Appendix B. We need more of this. \n- In particular, a canonical experiments comparing Gaussian interaction vs inner product would be very useful. \n- Experiments on existing benchmarks (for KB completion, or QA) would help. I agree with the authors that it is difficult to find the perfect benchmark, so it is a good idea to propose a new one (WorldCup2014). But this should come in addition to experiments on existing data.\n- Table 11 of Appendix C (page 16) that compares TransE and TransGaussian for the task of link prediction on WordNet can be seen as fixing the two points above (simple setting on existing benchmark). Unfortunately, TransGaussian does not perform well compared to simpler TransE. This, along with the poor results of TransGaussian (SINGLE) of Table 2, indicate that training TransGaussian seems pretty complex, and hence question the actual validity of this architecture.\n\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Gaussian Attention Model and Its Application to Knowledge Base Embedding and Question Answering", "abstract": "We propose the Gaussian attention model for content-based neural memory\naccess. With the proposed attention model, a neural network has the\nadditional degree of freedom to control the focus of its attention from\na laser sharp attention to a broad attention. It is applicable whenever\nwe can assume that the distance in the latent space reflects some notion\nof semantics. We use the proposed attention model as a scoring function\nfor the embedding of a knowledge base into a continuous vector space and\nthen train a model that performs question answering about the entities\nin the knowledge base. The proposed attention model can handle both the\npropagation of uncertainty when following a series of relations and also\nthe conjunction of conditions in a natural way. On a dataset of soccer\nplayers who participated in the FIFA World Cup 2014, we demonstrate that\nour model can handle both path queries and conjunctive queries well.", "pdf": "/pdf/529ba3e72398d2711ce7eb0ef6fe03087802afe2.pdf", "TL;DR": "We make (simple) knowledge base queries differentiable using the Gaussian attention model.", "paperhash": "zhang|gaussian_attention_model_and_its_application_to_knowledge_base_embedding_and_question_answering", "keywords": ["Natural language processing", "Supervised Learning", "Deep learning"], "conflicts": ["uchicago.edu", "microsoft.com"], "authors": ["Liwen Zhang", "John Winn", "Ryota Tomioka"], "authorids": ["liwenz@cs.uchicago.edu", "jwinn@microsoft.com", "ryoto@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512601059, "id": "ICLR.cc/2017/conference/-/paper388/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper388/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper388/AnonReviewer3", "ICLR.cc/2017/conference/paper388/AnonReviewer1", "ICLR.cc/2017/conference/paper388/AnonReviewer2"], "reply": {"forum": "ByC7ww9le", "replyto": "ByC7ww9le", "writers": {"values-regex": "ICLR.cc/2017/conference/paper388/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper388/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512601059}}}, {"tddate": null, "tmdate": 1482165227061, "tcdate": 1482165227061, "number": 3, "id": "BymlhFHVg", "invitation": "ICLR.cc/2017/conference/-/paper388/public/comment", "forum": "ByC7ww9le", "replyto": "SkkQgS-4l", "signatures": ["~Ryota_Tomioka1"], "readers": ["everyone"], "writers": ["~Ryota_Tomioka1"], "content": {"title": "Clarification", "comment": "Thank you for you time reviewing our paper. Let me try to clarify some of your concerns.\n\nFirst, we discuss properties of the learned TransGaussian embedding in Appendix B. In fact, the variance parameters associated with each relation result in an interesting block pattern (Figure 3). This shows that the proposed TransGaussian embedding can disentagle different properties into different subsets of dimensions as shown in Figure 4. This is a feature that clearly distinguishes TransGaussian from the rotationally invariant TransE model.\n\nSecond, it is true that the order of relations is lost in the current formulation. This is the problem we discuss in Section 5 in relation to Neelakantan et al. (2015a). We hope to address this issue using RNNs in the future.\n\nFinally, regarding motivation, our motivation for introducing the Gaussian attention model is that it can compactly represent a set of items. A typical attention model deals with a soft version of an item. Our Gaussian attention model deals with a soft version of a set. As operations on sets, we support composition and conjunction. The nice thing about our Gaussian model is that both of these operations can be carried out in closed form (namely convolution and product). We can think of extending NTM and memory networks so that they can deal with sets instead of items but that is beyond the scope of this paper."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Gaussian Attention Model and Its Application to Knowledge Base Embedding and Question Answering", "abstract": "We propose the Gaussian attention model for content-based neural memory\naccess. With the proposed attention model, a neural network has the\nadditional degree of freedom to control the focus of its attention from\na laser sharp attention to a broad attention. It is applicable whenever\nwe can assume that the distance in the latent space reflects some notion\nof semantics. We use the proposed attention model as a scoring function\nfor the embedding of a knowledge base into a continuous vector space and\nthen train a model that performs question answering about the entities\nin the knowledge base. The proposed attention model can handle both the\npropagation of uncertainty when following a series of relations and also\nthe conjunction of conditions in a natural way. On a dataset of soccer\nplayers who participated in the FIFA World Cup 2014, we demonstrate that\nour model can handle both path queries and conjunctive queries well.", "pdf": "/pdf/529ba3e72398d2711ce7eb0ef6fe03087802afe2.pdf", "TL;DR": "We make (simple) knowledge base queries differentiable using the Gaussian attention model.", "paperhash": "zhang|gaussian_attention_model_and_its_application_to_knowledge_base_embedding_and_question_answering", "keywords": ["Natural language processing", "Supervised Learning", "Deep learning"], "conflicts": ["uchicago.edu", "microsoft.com"], "authors": ["Liwen Zhang", "John Winn", "Ryota Tomioka"], "authorids": ["liwenz@cs.uchicago.edu", "jwinn@microsoft.com", "ryoto@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287595063, "id": "ICLR.cc/2017/conference/-/paper388/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "ByC7ww9le", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper388/reviewers", "ICLR.cc/2017/conference/paper388/areachairs"], "cdate": 1485287595063}}}, {"tddate": null, "tmdate": 1481883787178, "tcdate": 1481883670871, "number": 1, "id": "SkkQgS-4l", "invitation": "ICLR.cc/2017/conference/-/paper388/official/review", "forum": "ByC7ww9le", "replyto": "ByC7ww9le", "signatures": ["ICLR.cc/2017/conference/paper388/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper388/AnonReviewer3"], "content": {"title": "", "rating": "4: Ok but not good enough - rejection", "review": "SUMMARY.\n\nThe paper propose a new scoring function for knowledge base embedding.\nThe scoring function called TransGaussian is an novel take on (or a generalization of) the well-known TransE scoring function.\nThe proposed function is tested on two tasks knowledge-base completion and question answering.\n\n----------\n\nOVERALL JUDGMENT\nWhile I think this proposed work is very interesting and it is an idea worth to explore further, the presentation and the experimental section of the paper have some problems.\nRegarding the presentation, as far as I understand this is not an attention model as intended standardly in the literature.\nPlus, it has hardly anything to share with memory networks/neural Turing machines, the parallel that the authors try to make is not very convincing.\nRegarding the experimental section, for a fair comparison the authors should test their model on standard benchmarks, reporting state-of-the-art models.\nFinally, the paper lack of discussion of results and insights on the behavior of the proposed model.\n\n\n----------\n\nDETAILED COMMENTS\n\n\nIn section 2.2 when the authors calculate \\mu_{context} do not they loose the order of relations? And if it is so, does it make any sense?\n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Gaussian Attention Model and Its Application to Knowledge Base Embedding and Question Answering", "abstract": "We propose the Gaussian attention model for content-based neural memory\naccess. With the proposed attention model, a neural network has the\nadditional degree of freedom to control the focus of its attention from\na laser sharp attention to a broad attention. It is applicable whenever\nwe can assume that the distance in the latent space reflects some notion\nof semantics. We use the proposed attention model as a scoring function\nfor the embedding of a knowledge base into a continuous vector space and\nthen train a model that performs question answering about the entities\nin the knowledge base. The proposed attention model can handle both the\npropagation of uncertainty when following a series of relations and also\nthe conjunction of conditions in a natural way. On a dataset of soccer\nplayers who participated in the FIFA World Cup 2014, we demonstrate that\nour model can handle both path queries and conjunctive queries well.", "pdf": "/pdf/529ba3e72398d2711ce7eb0ef6fe03087802afe2.pdf", "TL;DR": "We make (simple) knowledge base queries differentiable using the Gaussian attention model.", "paperhash": "zhang|gaussian_attention_model_and_its_application_to_knowledge_base_embedding_and_question_answering", "keywords": ["Natural language processing", "Supervised Learning", "Deep learning"], "conflicts": ["uchicago.edu", "microsoft.com"], "authors": ["Liwen Zhang", "John Winn", "Ryota Tomioka"], "authorids": ["liwenz@cs.uchicago.edu", "jwinn@microsoft.com", "ryoto@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512601059, "id": "ICLR.cc/2017/conference/-/paper388/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper388/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper388/AnonReviewer3", "ICLR.cc/2017/conference/paper388/AnonReviewer1", "ICLR.cc/2017/conference/paper388/AnonReviewer2"], "reply": {"forum": "ByC7ww9le", "replyto": "ByC7ww9le", "writers": {"values-regex": "ICLR.cc/2017/conference/paper388/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper388/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512601059}}}, {"tddate": null, "tmdate": 1481378257793, "tcdate": 1481378237326, "number": 2, "id": "BkrTFttQl", "invitation": "ICLR.cc/2017/conference/-/paper388/public/comment", "forum": "ByC7ww9le", "replyto": "HJhyJ2lQe", "signatures": ["~Ryota_Tomioka1"], "readers": ["everyone"], "writers": ["~Ryota_Tomioka1"], "content": {"title": "Responses", "comment": "\"Why not compare TransGaussian on the benchmarks used by this kind of method (like TransE) usually like FB15k?\"\n\nWe definitely should test our model on other datasets as well. Yet, large scale question and answering datasets on knowledge bases are mostly about Simple Q&A where the answer to a question is associated with an atomic fact. e.g. the SimpleQuestion dataset proposed in Bordes et al. (2015) and Serban et al. (2016), \"Generating Factoid Questions With Recurrent Neural Networks-The 30M Factoid Question-Answer Corpus\".\n\nThere are datasets which require a model to map a sentence to a logical form, such as Geo808 and Jobs640. To tackle these datasests, we would need to translate from the set of logical representations typically used for these datasets to the set of smooth Gaussian operations (composition and conjunctions) we have presented in this paper. We are optimistic that we can support many useful operations beyond simple Q&A using our Gaussian attention model.\n\n\n\"(1) Why is there a drop in performance from SINGLE to COMP for TransE? This is not expected.\"\n\nIn Table 2, at least for path queries, TransE (comp) is better than TransE (single).\n\nThe weak performance of TransE (comp) compared to TransE (single) on conjunctive queries mainly comes from not being able to model the two inverse relations (#8 and #9) well. Our hypothesis is that TransE lacks the complexity to take full advantage of the compositional training. We agree that it was also unexpected for us, but please note that no previous work evaluated compositionally trained TransE on conjunctive queries.\n\n\n\"(2) TransGaussian is actually worse than TransE in the SINGLE setting. Is there an explanation?\"\n\nWe hypothesize that since TransGaussian is more flexible than TransE, it needs stronger regularization. That is, compositional training provided the necessary constraint to train TransGaussian efficiently.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Gaussian Attention Model and Its Application to Knowledge Base Embedding and Question Answering", "abstract": "We propose the Gaussian attention model for content-based neural memory\naccess. With the proposed attention model, a neural network has the\nadditional degree of freedom to control the focus of its attention from\na laser sharp attention to a broad attention. It is applicable whenever\nwe can assume that the distance in the latent space reflects some notion\nof semantics. We use the proposed attention model as a scoring function\nfor the embedding of a knowledge base into a continuous vector space and\nthen train a model that performs question answering about the entities\nin the knowledge base. The proposed attention model can handle both the\npropagation of uncertainty when following a series of relations and also\nthe conjunction of conditions in a natural way. On a dataset of soccer\nplayers who participated in the FIFA World Cup 2014, we demonstrate that\nour model can handle both path queries and conjunctive queries well.", "pdf": "/pdf/529ba3e72398d2711ce7eb0ef6fe03087802afe2.pdf", "TL;DR": "We make (simple) knowledge base queries differentiable using the Gaussian attention model.", "paperhash": "zhang|gaussian_attention_model_and_its_application_to_knowledge_base_embedding_and_question_answering", "keywords": ["Natural language processing", "Supervised Learning", "Deep learning"], "conflicts": ["uchicago.edu", "microsoft.com"], "authors": ["Liwen Zhang", "John Winn", "Ryota Tomioka"], "authorids": ["liwenz@cs.uchicago.edu", "jwinn@microsoft.com", "ryoto@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287595063, "id": "ICLR.cc/2017/conference/-/paper388/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "ByC7ww9le", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper388/reviewers", "ICLR.cc/2017/conference/paper388/areachairs"], "cdate": 1485287595063}}}, {"tddate": null, "tmdate": 1480797924126, "tcdate": 1480797924121, "number": 2, "id": "HJhyJ2lQe", "invitation": "ICLR.cc/2017/conference/-/paper388/pre-review/question", "forum": "ByC7ww9le", "replyto": "ByC7ww9le", "signatures": ["ICLR.cc/2017/conference/paper388/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper388/AnonReviewer1"], "content": {"title": "Questions", "question": "- Why not compare TransGaussian on the benchmarks used by this kind of method (like TransE) usually like FB15k?\n- In Table 2: \n(1) Why is there a drop in performance from SINGLE to COMP for TransE? This is not expected.\n(2) TransGaussian is actually worse than TransE in the SINGLE setting. Is there an explanation?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Gaussian Attention Model and Its Application to Knowledge Base Embedding and Question Answering", "abstract": "We propose the Gaussian attention model for content-based neural memory\naccess. With the proposed attention model, a neural network has the\nadditional degree of freedom to control the focus of its attention from\na laser sharp attention to a broad attention. It is applicable whenever\nwe can assume that the distance in the latent space reflects some notion\nof semantics. We use the proposed attention model as a scoring function\nfor the embedding of a knowledge base into a continuous vector space and\nthen train a model that performs question answering about the entities\nin the knowledge base. The proposed attention model can handle both the\npropagation of uncertainty when following a series of relations and also\nthe conjunction of conditions in a natural way. On a dataset of soccer\nplayers who participated in the FIFA World Cup 2014, we demonstrate that\nour model can handle both path queries and conjunctive queries well.", "pdf": "/pdf/529ba3e72398d2711ce7eb0ef6fe03087802afe2.pdf", "TL;DR": "We make (simple) knowledge base queries differentiable using the Gaussian attention model.", "paperhash": "zhang|gaussian_attention_model_and_its_application_to_knowledge_base_embedding_and_question_answering", "keywords": ["Natural language processing", "Supervised Learning", "Deep learning"], "conflicts": ["uchicago.edu", "microsoft.com"], "authors": ["Liwen Zhang", "John Winn", "Ryota Tomioka"], "authorids": ["liwenz@cs.uchicago.edu", "jwinn@microsoft.com", "ryoto@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959306378, "id": "ICLR.cc/2017/conference/-/paper388/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper388/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper388/AnonReviewer3", "ICLR.cc/2017/conference/paper388/AnonReviewer1"], "reply": {"forum": "ByC7ww9le", "replyto": "ByC7ww9le", "writers": {"values-regex": "ICLR.cc/2017/conference/paper388/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper388/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959306378}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1480543479272, "tcdate": 1478289190481, "number": 388, "id": "ByC7ww9le", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "ByC7ww9le", "signatures": ["~Liwen_Zhang1"], "readers": ["everyone"], "content": {"title": "Gaussian Attention Model and Its Application to Knowledge Base Embedding and Question Answering", "abstract": "We propose the Gaussian attention model for content-based neural memory\naccess. With the proposed attention model, a neural network has the\nadditional degree of freedom to control the focus of its attention from\na laser sharp attention to a broad attention. It is applicable whenever\nwe can assume that the distance in the latent space reflects some notion\nof semantics. We use the proposed attention model as a scoring function\nfor the embedding of a knowledge base into a continuous vector space and\nthen train a model that performs question answering about the entities\nin the knowledge base. The proposed attention model can handle both the\npropagation of uncertainty when following a series of relations and also\nthe conjunction of conditions in a natural way. On a dataset of soccer\nplayers who participated in the FIFA World Cup 2014, we demonstrate that\nour model can handle both path queries and conjunctive queries well.", "pdf": "/pdf/529ba3e72398d2711ce7eb0ef6fe03087802afe2.pdf", "TL;DR": "We make (simple) knowledge base queries differentiable using the Gaussian attention model.", "paperhash": "zhang|gaussian_attention_model_and_its_application_to_knowledge_base_embedding_and_question_answering", "keywords": ["Natural language processing", "Supervised Learning", "Deep learning"], "conflicts": ["uchicago.edu", "microsoft.com"], "authors": ["Liwen Zhang", "John Winn", "Ryota Tomioka"], "authorids": ["liwenz@cs.uchicago.edu", "jwinn@microsoft.com", "ryoto@microsoft.com"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "tmdate": 1480505387435, "tcdate": 1480505387431, "number": 1, "id": "S1mEdE3fe", "invitation": "ICLR.cc/2017/conference/-/paper388/public/comment", "forum": "ByC7ww9le", "replyto": "SJZ1i73fl", "signatures": ["~Ryota_Tomioka1"], "readers": ["everyone"], "writers": ["~Ryota_Tomioka1"], "content": {"title": "Response to your questions", "comment": "1) Please take a look at Table 5 in Appendix. The questions were generated using multiple templates per question.\n2) That is right. See bottom of page 7. We wrote \"Note that TransE can be considered as a special case of TransGaussian where the variance matrix is the identity and hence, the scoring formula Eq. (7) is applicable to TransE as well.\"\n3) Sure. Scoring function (1) has two parametes (mean and variance). In general, an attention network computes both the mean and the variance in a context dependent manner. Thus it can control not only the direction of the attention but also the sharpness of the focus. In the proposed knowledge base embedding model the mean and the variance are independent parameters for each relation. You can see in Figure 3 in Appendix that the learned variance is low for coordinates corresponding to the relation and high for other coordinates. This is natural because when you query for a Forward player, the variance should be high in all but the coordinates corresponding to the position of the player.\n4) In Section 2.1, we wrote \"if \\Sigma_r is \ufb01xed to the identity matrix, we are modeling the relation of subject v_s and object v_o as a translation \\delta_r, which is equivalent to the TransE model (Bordes et al., 2013).\" but this is only to relate our model to TransE. We don't fix \\Sigma_r as identity.\n5) This was only because of convenience and we should definitely test our model on other datasets as well."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Gaussian Attention Model and Its Application to Knowledge Base Embedding and Question Answering", "abstract": "We propose the Gaussian attention model for content-based neural memory\naccess. With the proposed attention model, a neural network has the\nadditional degree of freedom to control the focus of its attention from\na laser sharp attention to a broad attention. It is applicable whenever\nwe can assume that the distance in the latent space reflects some notion\nof semantics. We use the proposed attention model as a scoring function\nfor the embedding of a knowledge base into a continuous vector space and\nthen train a model that performs question answering about the entities\nin the knowledge base. The proposed attention model can handle both the\npropagation of uncertainty when following a series of relations and also\nthe conjunction of conditions in a natural way. On a dataset of soccer\nplayers who participated in the FIFA World Cup 2014, we demonstrate that\nour model can handle both path queries and conjunctive queries well.", "pdf": "/pdf/529ba3e72398d2711ce7eb0ef6fe03087802afe2.pdf", "TL;DR": "We make (simple) knowledge base queries differentiable using the Gaussian attention model.", "paperhash": "zhang|gaussian_attention_model_and_its_application_to_knowledge_base_embedding_and_question_answering", "keywords": ["Natural language processing", "Supervised Learning", "Deep learning"], "conflicts": ["uchicago.edu", "microsoft.com"], "authors": ["Liwen Zhang", "John Winn", "Ryota Tomioka"], "authorids": ["liwenz@cs.uchicago.edu", "jwinn@microsoft.com", "ryoto@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287595063, "id": "ICLR.cc/2017/conference/-/paper388/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "ByC7ww9le", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper388/reviewers", "ICLR.cc/2017/conference/paper388/areachairs"], "cdate": 1485287595063}}}, {"tddate": null, "tmdate": 1480501977328, "tcdate": 1480501977323, "number": 1, "id": "SJZ1i73fl", "invitation": "ICLR.cc/2017/conference/-/paper388/pre-review/question", "forum": "ByC7ww9le", "replyto": "ByC7ww9le", "signatures": ["ICLR.cc/2017/conference/paper388/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper388/AnonReviewer3"], "content": {"title": "Some questions", "question": "Hi, I would have some doubts about the paper:\n1) How did you generate natural language questions for the worldcup dataset?\n2) When comparing to Guu et al. in the QA experiments did you just change the TransGaussian scoring function with the TransE (single) and TransE (comp)?\n3) In the abstract you state that with TransGaussian you can control the focus of the attention, it is not clear from the paper how would you do it, can you elaborate on this?\n4) In section 2.1 you stated that \\Sigma_r is fixed to the identity matrix, but after you state that the relation matrices are actually learnt. Could you elaborate on it?\n5) Is there a particular reson why you tested your model on a new dataset and you did not test it on estabilished benchmarks?\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Gaussian Attention Model and Its Application to Knowledge Base Embedding and Question Answering", "abstract": "We propose the Gaussian attention model for content-based neural memory\naccess. With the proposed attention model, a neural network has the\nadditional degree of freedom to control the focus of its attention from\na laser sharp attention to a broad attention. It is applicable whenever\nwe can assume that the distance in the latent space reflects some notion\nof semantics. We use the proposed attention model as a scoring function\nfor the embedding of a knowledge base into a continuous vector space and\nthen train a model that performs question answering about the entities\nin the knowledge base. The proposed attention model can handle both the\npropagation of uncertainty when following a series of relations and also\nthe conjunction of conditions in a natural way. On a dataset of soccer\nplayers who participated in the FIFA World Cup 2014, we demonstrate that\nour model can handle both path queries and conjunctive queries well.", "pdf": "/pdf/529ba3e72398d2711ce7eb0ef6fe03087802afe2.pdf", "TL;DR": "We make (simple) knowledge base queries differentiable using the Gaussian attention model.", "paperhash": "zhang|gaussian_attention_model_and_its_application_to_knowledge_base_embedding_and_question_answering", "keywords": ["Natural language processing", "Supervised Learning", "Deep learning"], "conflicts": ["uchicago.edu", "microsoft.com"], "authors": ["Liwen Zhang", "John Winn", "Ryota Tomioka"], "authorids": ["liwenz@cs.uchicago.edu", "jwinn@microsoft.com", "ryoto@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959306378, "id": "ICLR.cc/2017/conference/-/paper388/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper388/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper388/AnonReviewer3", "ICLR.cc/2017/conference/paper388/AnonReviewer1"], "reply": {"forum": "ByC7ww9le", "replyto": "ByC7ww9le", "writers": {"values-regex": "ICLR.cc/2017/conference/paper388/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper388/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959306378}}}], "count": 10}