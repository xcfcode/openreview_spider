{"notes": [{"id": "ByxLl309Ym", "original": "HkgomVn5tQ", "number": 1077, "cdate": 1538087917817, "ddate": null, "tcdate": 1538087917817, "tmdate": 1545355417099, "tddate": null, "forum": "ByxLl309Ym", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Conditional Inference in Pre-trained Variational Autoencoders via Cross-coding", "abstract": "Variational Autoencoders (VAEs) are a popular generative model, but one in which conditional inference can be challenging. If the decomposition into query and evidence variables is fixed, conditional VAEs provide an attractive solution. To support arbitrary queries, one is generally reduced to Markov Chain Monte Carlo sampling methods that  can suffer from long mixing times.  In this paper, we propose an idea we term cross-coding to approximate the distribution over the latent variables after conditioning on an evidence assignment to some subset of the variables. This allows generating query samples without retraining the full VAE.  We experimentally evaluate three variations of cross-coding showing that (i) can be quickly optimized for different decompositions of evidence and query and (ii) they quantitatively and qualitatively outperform Hamiltonian Monte Carlo.", "keywords": [], "authorids": ["wuga@mie.utoronto.ca", "domke@cs.umass.edu", "ssanner@mie.utoronto.ca"], "authors": ["Ga Wu", "Justin Domke", "Scott Sanner"], "pdf": "/pdf/2acbda5f3f5a9ca4de0b727c8e5892acaea113cd.pdf", "paperhash": "wu|conditional_inference_in_pretrained_variational_autoencoders_via_crosscoding", "_bibtex": "@misc{\nwu2019conditional,\ntitle={Conditional Inference in Pre-trained Variational Autoencoders via Cross-coding},\nauthor={Ga Wu and Justin Domke and Scott Sanner},\nyear={2019},\nurl={https://openreview.net/forum?id=ByxLl309Ym},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 6, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "BJgxwub2yV", "original": null, "number": 1, "cdate": 1544456279549, "ddate": null, "tcdate": 1544456279549, "tmdate": 1545354497800, "tddate": null, "forum": "ByxLl309Ym", "replyto": "ByxLl309Ym", "invitation": "ICLR.cc/2019/Conference/-/Paper1077/Meta_Review", "content": {"metareview": "This paper proposes to approximate arbitrary conditional distribution of a pertained VAE using variational inferences. The paper is technically sound and clearly written. A few variants of the inference network are also compared and evaluated in experiments.\n\nThe main problems of the paper are as follows:\n1. The motivation of training an inference network for a fixed decoder is not well explained.\n2. The application of VI is standard, and offers limited novelty or significance of the proposed method.\n3. The introduction of the new term cross-coding is not necessary and does not bring new insights than a standard VI method.\n\nThe authors argued in the feedback that the central contribution is using augmented VI to do conditioning inference, similar to Rezende at al, but didn't address reviewers' main concerns. I encourage the authors to incorporate the reviewers' comments in a future revision, and explain why this proposed method bring significant contribution to either address a real problem or improve VI methodology.", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Reject", "title": "Not well motivated and lack of novel contribution"}, "signatures": ["ICLR.cc/2019/Conference/Paper1077/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper1077/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Conditional Inference in Pre-trained Variational Autoencoders via Cross-coding", "abstract": "Variational Autoencoders (VAEs) are a popular generative model, but one in which conditional inference can be challenging. If the decomposition into query and evidence variables is fixed, conditional VAEs provide an attractive solution. To support arbitrary queries, one is generally reduced to Markov Chain Monte Carlo sampling methods that  can suffer from long mixing times.  In this paper, we propose an idea we term cross-coding to approximate the distribution over the latent variables after conditioning on an evidence assignment to some subset of the variables. This allows generating query samples without retraining the full VAE.  We experimentally evaluate three variations of cross-coding showing that (i) can be quickly optimized for different decompositions of evidence and query and (ii) they quantitatively and qualitatively outperform Hamiltonian Monte Carlo.", "keywords": [], "authorids": ["wuga@mie.utoronto.ca", "domke@cs.umass.edu", "ssanner@mie.utoronto.ca"], "authors": ["Ga Wu", "Justin Domke", "Scott Sanner"], "pdf": "/pdf/2acbda5f3f5a9ca4de0b727c8e5892acaea113cd.pdf", "paperhash": "wu|conditional_inference_in_pretrained_variational_autoencoders_via_crosscoding", "_bibtex": "@misc{\nwu2019conditional,\ntitle={Conditional Inference in Pre-trained Variational Autoencoders via Cross-coding},\nauthor={Ga Wu and Justin Domke and Scott Sanner},\nyear={2019},\nurl={https://openreview.net/forum?id=ByxLl309Ym},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1077/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545352974089, "tddate": null, "super": null, "final": null, "reply": {"forum": "ByxLl309Ym", "replyto": "ByxLl309Ym", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1077/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper1077/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1077/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545352974089}}}, {"id": "SklMscm3CX", "original": null, "number": 3, "cdate": 1543416473930, "ddate": null, "tcdate": 1543416473930, "tmdate": 1543416473930, "tddate": null, "forum": "ByxLl309Ym", "replyto": "SkxXa4gKRX", "invitation": "ICLR.cc/2019/Conference/-/Paper1077/Official_Comment", "content": {"title": "Response", "comment": "Thanks for your feedback.\n\nMy score of 4 was mainly due to the lack of original contribution. The paper is technically sound, clearly written and interesting to read, but the inference methods discussed are already known and well-understood in the variational-inference community. There isn't anything in the authors' feedback that convinces me I missed something, so I'm afraid my review will remain the same.\n\nI sincerely hope that the authors find positive and constructive feedback in our reviews, and that they appreciate our good intentions and time we put in to help improve the paper. I wish the authors best of luck with their work."}, "signatures": ["ICLR.cc/2019/Conference/Paper1077/AnonReviewer1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1077/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1077/AnonReviewer1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Conditional Inference in Pre-trained Variational Autoencoders via Cross-coding", "abstract": "Variational Autoencoders (VAEs) are a popular generative model, but one in which conditional inference can be challenging. If the decomposition into query and evidence variables is fixed, conditional VAEs provide an attractive solution. To support arbitrary queries, one is generally reduced to Markov Chain Monte Carlo sampling methods that  can suffer from long mixing times.  In this paper, we propose an idea we term cross-coding to approximate the distribution over the latent variables after conditioning on an evidence assignment to some subset of the variables. This allows generating query samples without retraining the full VAE.  We experimentally evaluate three variations of cross-coding showing that (i) can be quickly optimized for different decompositions of evidence and query and (ii) they quantitatively and qualitatively outperform Hamiltonian Monte Carlo.", "keywords": [], "authorids": ["wuga@mie.utoronto.ca", "domke@cs.umass.edu", "ssanner@mie.utoronto.ca"], "authors": ["Ga Wu", "Justin Domke", "Scott Sanner"], "pdf": "/pdf/2acbda5f3f5a9ca4de0b727c8e5892acaea113cd.pdf", "paperhash": "wu|conditional_inference_in_pretrained_variational_autoencoders_via_crosscoding", "_bibtex": "@misc{\nwu2019conditional,\ntitle={Conditional Inference in Pre-trained Variational Autoencoders via Cross-coding},\nauthor={Ga Wu and Justin Domke and Scott Sanner},\nyear={2019},\nurl={https://openreview.net/forum?id=ByxLl309Ym},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1077/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621625347, "tddate": null, "super": null, "final": null, "reply": {"forum": "ByxLl309Ym", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1077/Authors", "ICLR.cc/2019/Conference/Paper1077/Reviewers", "ICLR.cc/2019/Conference/Paper1077/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1077/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1077/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1077/Authors|ICLR.cc/2019/Conference/Paper1077/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1077/Reviewers", "ICLR.cc/2019/Conference/Paper1077/Authors", "ICLR.cc/2019/Conference/Paper1077/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621625347}}}, {"id": "SkxXa4gKRX", "original": null, "number": 1, "cdate": 1543206074914, "ddate": null, "tcdate": 1543206074914, "tmdate": 1543206260421, "tddate": null, "forum": "ByxLl309Ym", "replyto": "ByxLl309Ym", "invitation": "ICLR.cc/2019/Conference/-/Paper1077/Official_Comment", "content": {"title": "Author Feedback", "comment": "Thanks for reviewers for their comments.\n\nThe goal of the paper is to infer p(y|x). The reviewers miss the central point of the paper which is using the framework of Augmented VI to understand why it is justifiable to do inference targeting p(z|x) and the looseness therefore entailed. Note that Rezende at al tackle EXACTLY this problem and design a custom algorithm, which we compared against. If it's all so simple, why would they even do that?\n\nReference:\nDanilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and\napproximate inference in deep generative models. In Proceedings of the 31th International\nConference on Machine Learning (ICML), pp. 1278\u20131286, 2014."}, "signatures": ["ICLR.cc/2019/Conference/Paper1077/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1077/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1077/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Conditional Inference in Pre-trained Variational Autoencoders via Cross-coding", "abstract": "Variational Autoencoders (VAEs) are a popular generative model, but one in which conditional inference can be challenging. If the decomposition into query and evidence variables is fixed, conditional VAEs provide an attractive solution. To support arbitrary queries, one is generally reduced to Markov Chain Monte Carlo sampling methods that  can suffer from long mixing times.  In this paper, we propose an idea we term cross-coding to approximate the distribution over the latent variables after conditioning on an evidence assignment to some subset of the variables. This allows generating query samples without retraining the full VAE.  We experimentally evaluate three variations of cross-coding showing that (i) can be quickly optimized for different decompositions of evidence and query and (ii) they quantitatively and qualitatively outperform Hamiltonian Monte Carlo.", "keywords": [], "authorids": ["wuga@mie.utoronto.ca", "domke@cs.umass.edu", "ssanner@mie.utoronto.ca"], "authors": ["Ga Wu", "Justin Domke", "Scott Sanner"], "pdf": "/pdf/2acbda5f3f5a9ca4de0b727c8e5892acaea113cd.pdf", "paperhash": "wu|conditional_inference_in_pretrained_variational_autoencoders_via_crosscoding", "_bibtex": "@misc{\nwu2019conditional,\ntitle={Conditional Inference in Pre-trained Variational Autoencoders via Cross-coding},\nauthor={Ga Wu and Justin Domke and Scott Sanner},\nyear={2019},\nurl={https://openreview.net/forum?id=ByxLl309Ym},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1077/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621625347, "tddate": null, "super": null, "final": null, "reply": {"forum": "ByxLl309Ym", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1077/Authors", "ICLR.cc/2019/Conference/Paper1077/Reviewers", "ICLR.cc/2019/Conference/Paper1077/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1077/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1077/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1077/Authors|ICLR.cc/2019/Conference/Paper1077/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1077/Reviewers", "ICLR.cc/2019/Conference/Paper1077/Authors", "ICLR.cc/2019/Conference/Paper1077/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621625347}}}, {"id": "S1xb5Tak6Q", "original": null, "number": 3, "cdate": 1541557640896, "ddate": null, "tcdate": 1541557640896, "tmdate": 1541557640896, "tddate": null, "forum": "ByxLl309Ym", "replyto": "ByxLl309Ym", "invitation": "ICLR.cc/2019/Conference/-/Paper1077/Official_Review", "content": {"title": "A paper that needs work in terms of motivation, exposition, and evaluation", "review": "(apologies for this belated review)\n\nSummary\n\nThe authors consider the task of imputing missing data using variational auto-encoders. To do so, they assume a fixed pre-trained generative model, perform variational inference to infer a posterior on latent variables given a partial image, and then use this approximate posterior to predict missing pixels. They compare a variety of parameterizations of the variational distribution to HMC inference, and evaluate on MNIST, Celeb-A and the Anime data. \n\nComments\n\nThere are many things about this paper that I don\u2019t understand. My main concern is that I fail to follow why the authors are interested in this task. In what settings would we be interested in performing non-autoencoding variational inference in order to impute missing data? Moreover, in cases where are interested in performing such imputations, what would we like to use the results for? This paper seems like a nice demo, but I\u2019m not entirely convinced I see a compelling application. \n\nMy second concern is about the baselines that are considered. If I were interested in carrying out this inference task, my inclination would not be to run an HMC chain to convergence, but instead to do something like annealed importance sampling (AIS), where at each step I run an iteration of HMC on a large batch of samples on a sequence of target densities that interpolate between the prior and full joint p(x, Z). If computational cost is a concern, I imagine this would not be more expensive than training a density estimator. Moreover, whereas HMC is generally not known to be a good method for estimating marginal likelihoods, AIS methods generally perform much better.\n\nFinally I find the language used in this paper confusing. Cross-coding seems a misnomer for the technique that the authors propose. Isn\u2019t this simply a form of variational inference in which q\u03c8(Z) approximates p\u03b8(\u0396 | x)? The term \u201c-coding\u201d suggests that we somehow define an encoder that accepts the query as input. Moreover, isn\u2019t the XCoder network just a neural density estimator? \n\nFinally, Lemma 1 seems like a really roundabout way of deriving a lower bound. The authors could instead just write:\n\n\tlog p(x)\n\t>=\n\tE_q(Z,Y)[log p(x, Y, Z) - log q(Z, \u03a5)]\n\t=\n\tE_q(Z,Y)[log p(x | \u0396) + log p(Y | Z) + log p(Z) - log q(Z) - log p(\u03a5 | Z)]\n\t=\n\tE_q(Z,Y)[log p(x | \u0396) + log p(Z) - log q(Z)]\n\t=\n\tE_q(Z)[log p(x, \u0396) - log q(Z)]\n\nThis avoids confusing terminology such as cross-coding, and shows that what the authors are doing is in fact just variational inference. Am I missing something here?\n\nI am also confused about how the comparison to HMC is set up. If you\u2019re training q\u03c8(Z), then you presumably need generate a certain number samples at training time. Shouldn\u2019t you add this number of samples number of samples you generate in HMC, in order to get a more apples to apples comparison in terms of the amount of computation performed? As it stands, it is hard to evaluate whether these methods are given a similar number of samples. \n\nFinally, I am not quite sure what to make of the experimental evaluation. We see some scatter plots on MNIST with a 2D latent space, and some faces of celebrities in which there is arguably some sample diversity, although most of this diversity arises in blurry looking hairstyles. However, since the authors condition on the eyes, rather than, say, the nose or mouth, it is hard to know how good a job the network is doing at generalizing to multiple plausible faces. \n\nOverall, I find it difficult to judge the merit of this paper. Is this task in fact hard? Is it useful? Are the results good? Maybe the authors can give us some additional guidance on these questions.\n\nQuestions\n\n- I\u2019m a bit worried that not all the samples that we see in Figure 6 may have equally high probability under the posterior. Could the authors compute and report importance weights?\n\n\tW = p(x, Z) / q(Z)\n\t\n- Could the authors say something about the effective sample size that we obtain when using the learned distribution q(Z) as a proposal? \n\t\n\tESS = (\u03a3_k w^k)^2 / (\u03a3_k (w^k)^2)\n\n- Should it be the case that the ESS is low, and the weights are high variance, could the authors generate a sufficient number of samples to ensure the the ESS = 25 (i.e. the number of images in the figure) and then show the 25 highest-weight samples (or resample 25 images with probability proportional to their weight)?\n\n\t\nMinor \n\n\n- Equation (3): There\u2019s an extra p_\u03b8 in the first integral\n\n- In the proof in Appendix 6.1 \n\n\tKL[ q\u03c8(Z) \u2016 p\u03b8(Z | x) ] + KL[ q\u03c8(Y | Z) \u2016 p\u03b8(Y | Z, x)]\n\nit would be clearer to explicitly denote the expectation over q\u03c8(Z)\n\n\tKL[ q\u03c8(Z) \u2016 p\u03b8(Z | x) ] + E_q\u03c8(Z)[ KL[ q\u03c8(Y | Z) \u2016 p\u03b8(Y | Z, x)] ]\n\t\n(I had to google lecture notes to find out that this expectation is sometimes implicit, which \nas far as I know is not very standard). \n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1077/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Conditional Inference in Pre-trained Variational Autoencoders via Cross-coding", "abstract": "Variational Autoencoders (VAEs) are a popular generative model, but one in which conditional inference can be challenging. If the decomposition into query and evidence variables is fixed, conditional VAEs provide an attractive solution. To support arbitrary queries, one is generally reduced to Markov Chain Monte Carlo sampling methods that  can suffer from long mixing times.  In this paper, we propose an idea we term cross-coding to approximate the distribution over the latent variables after conditioning on an evidence assignment to some subset of the variables. This allows generating query samples without retraining the full VAE.  We experimentally evaluate three variations of cross-coding showing that (i) can be quickly optimized for different decompositions of evidence and query and (ii) they quantitatively and qualitatively outperform Hamiltonian Monte Carlo.", "keywords": [], "authorids": ["wuga@mie.utoronto.ca", "domke@cs.umass.edu", "ssanner@mie.utoronto.ca"], "authors": ["Ga Wu", "Justin Domke", "Scott Sanner"], "pdf": "/pdf/2acbda5f3f5a9ca4de0b727c8e5892acaea113cd.pdf", "paperhash": "wu|conditional_inference_in_pretrained_variational_autoencoders_via_crosscoding", "_bibtex": "@misc{\nwu2019conditional,\ntitle={Conditional Inference in Pre-trained Variational Autoencoders via Cross-coding},\nauthor={Ga Wu and Justin Domke and Scott Sanner},\nyear={2019},\nurl={https://openreview.net/forum?id=ByxLl309Ym},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1077/Official_Review", "cdate": 1542234311304, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "ByxLl309Ym", "replyto": "ByxLl309Ym", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1077/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335867775, "tmdate": 1552335867775, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1077/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "S1xh9yst37", "original": null, "number": 2, "cdate": 1541152660210, "ddate": null, "tcdate": 1541152660210, "tmdate": 1541533442575, "tddate": null, "forum": "ByxLl309Ym", "replyto": "ByxLl309Ym", "invitation": "ICLR.cc/2019/Conference/-/Paper1077/Official_Review", "content": {"title": "I don\u2019t quite see what is new about this paper", "review": "This paper proposes the use of unamortized Black Box Variational Inference for data imputation (given a fixed VAE with a factorized decoder), where the choice of variational distribution is a standard flow model. \n\nThe exploitation of the decoder factorization and the choice to set q(y | z) = p(y | z) was explored in the Bottleneck Conditional Density Estimation paper.\n\nTo my understanding, this paper fails to contextualize their work with the existing literature and is simply an exercise in the rote application of existing inference procedures to a well-established inference problem (data imputation). \n\nUnless the authors can convince me of the novelty of their approach or what I have overlooked in their proposal, I do not recommend this paper for acceptance.\n\nReferences:\nRanganath, et al. Black Box Variational Inference. AISTATS 2014.\nShu, et al. Bottleneck Conditional Density Estimation. ICML 2017.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1077/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Conditional Inference in Pre-trained Variational Autoencoders via Cross-coding", "abstract": "Variational Autoencoders (VAEs) are a popular generative model, but one in which conditional inference can be challenging. If the decomposition into query and evidence variables is fixed, conditional VAEs provide an attractive solution. To support arbitrary queries, one is generally reduced to Markov Chain Monte Carlo sampling methods that  can suffer from long mixing times.  In this paper, we propose an idea we term cross-coding to approximate the distribution over the latent variables after conditioning on an evidence assignment to some subset of the variables. This allows generating query samples without retraining the full VAE.  We experimentally evaluate three variations of cross-coding showing that (i) can be quickly optimized for different decompositions of evidence and query and (ii) they quantitatively and qualitatively outperform Hamiltonian Monte Carlo.", "keywords": [], "authorids": ["wuga@mie.utoronto.ca", "domke@cs.umass.edu", "ssanner@mie.utoronto.ca"], "authors": ["Ga Wu", "Justin Domke", "Scott Sanner"], "pdf": "/pdf/2acbda5f3f5a9ca4de0b727c8e5892acaea113cd.pdf", "paperhash": "wu|conditional_inference_in_pretrained_variational_autoencoders_via_crosscoding", "_bibtex": "@misc{\nwu2019conditional,\ntitle={Conditional Inference in Pre-trained Variational Autoencoders via Cross-coding},\nauthor={Ga Wu and Justin Domke and Scott Sanner},\nyear={2019},\nurl={https://openreview.net/forum?id=ByxLl309Ym},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1077/Official_Review", "cdate": 1542234311304, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "ByxLl309Ym", "replyto": "ByxLl309Ym", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1077/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335867775, "tmdate": 1552335867775, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1077/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "rkl3pcrM2X", "original": null, "number": 1, "cdate": 1540672195888, "ddate": null, "tcdate": 1540672195888, "tmdate": 1541533442368, "tddate": null, "forum": "ByxLl309Ym", "replyto": "ByxLl309Ym", "invitation": "ICLR.cc/2019/Conference/-/Paper1077/Official_Review", "content": {"title": "Interesting read, but little original contribution", "review": "Paper summary:\n\nGiven a pre-trained VAE (e.g. over images), this paper is about inferring the distribution over missing variables (e.g. given half the pixels, what is a plausible completion?). The paper describes an approach based on variational inference with normalizing flows: given observed variables, the posterior over the VAE's latents is inferred (variationally) and plausible completions for missing variables are sampled from the VAE decoder.\n\nTechnical quality:\n\nThe presented method is technically correct. The evaluation carefully compares different types of normalizing flow and HMC, and seems to follow good practices.\n\nI have a suggestion for improving the GVI method. The way it's described in the paper, GVI requires computing the determinant of a DxD matrix, which costs O(D^3), and there is no guarantee that the matrix is invertible. However, this approach over-parameterizes the covariance matrix of the modelled Gaussian. Without losing any flexibility, you can use a lower triangular matrix with strictly positive diagonal elements (e.g. the diagonal elements can be parameterized as the exp of unconstrained variables). That way, the determinant costs O(D) (it's just the product of diagonal elements) and you ensure that the matrix is invertible (because the determinant is strictly positive), without hurting expressivity. You can think of this as parameterizing the Cholesky decomposition of the covariance matrix.\n\nAlso, there are more flexible normalizing flows, such as Inverse Autoregressive Flow, that can be used instead of the planar flow used in the paper.\n\nClarity:\n\nThe paper is written clearly and in full detail, and the mathematical exposition is clear and precise.\n\nSome typos and minor suggestions for improvement:\n- It'd be good to move Alg. 1 and Fig. 1 near where they are first referenced.\n- Page 2: over to \\theta --> over \\theta\n- Eq. 3: p_\\theta appears twice in the middle.\n- one can use MCMC to attempt sampling --> one can use MCMC to sample\n- Eq. 5: should be q_\\psi as subscript of E.\n- Fig. 7, caption: should be GVI vs. NF.\n- In references, should be properly capitalized: Hamiltonian, Langevin, Monte Carlo, Bayes, BFGS\n- Lemma 1: joint divergence is equivalent to --> joint divergence is equal to\n- Lemma 1: in the chain rule for KL, the second KL term should be averaged w.r.t. its free variables.\n\nOriginality:\n\nIn my opinion, there is little original contribution in this paper. The inference method presented (variational inference with normalizing flows) is well-known and already in use. The paper applies this method to VAEs, which is a straightforward application of a well-known inference method to a relatively simple graphical model (z -> {x, y}, with x, y independent given z).\n\nI don't see the need for introducing a new term (cross-coder). According to the paper, a cross-coder is precisely a normalizing flow (i.e. an invertible smooth transformation of a simple density). I think new terms for already existing ideas add cognitive load to the community, and are better avoided.\n\nSignificance:\n\nIn my opinion, constructing generative models that can handle arbitrary patterns of missing data is an important research direction. However, this is not exactly what the paper is about: the paper is about inference in a given generative model. Given that there is (in my opinion) no new methodology in the paper, I wouldn't consider this paper a significant contribution.\n\nI would also suggest that in a future version of the paper there is more motivation (e.g. in the introduction) of why the problem the paper is concerned with (i.e. missing data in generative models) is significant. Is it just for image completion / data imputation, or are there other practical problems? Is it important as part of another method / solution to another problem?\n\nReview summary:\n\nPros:\n- Technically correct, gives full detail.\n- Well and clearly written, precise with maths.\n- Evaluation section interesting to read.\n\nCons:\n- No original contribution.\n- Could do a better job motivating the importance of the problem.\n\nMinor points:\n- I don't completely agree with the way VAEs are described in sec. 2.1. As written, it follows that VAEs must have a Gaussian prior and a conditionally independent decoder. Although these are common choices in practice, they are not necessary: for example, one could take the prior to be a Masked Autoregressive Flow and the decoder a PixelCNN.\n- Same for observation 1. This is not an observation, but an assumption; that is, the paper assumes that the decoder is conditionally independent. This is of course an assumption that we can satisfy by design, but it's a design choice that restricts the decoder in a specific way.", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper1077/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Conditional Inference in Pre-trained Variational Autoencoders via Cross-coding", "abstract": "Variational Autoencoders (VAEs) are a popular generative model, but one in which conditional inference can be challenging. If the decomposition into query and evidence variables is fixed, conditional VAEs provide an attractive solution. To support arbitrary queries, one is generally reduced to Markov Chain Monte Carlo sampling methods that  can suffer from long mixing times.  In this paper, we propose an idea we term cross-coding to approximate the distribution over the latent variables after conditioning on an evidence assignment to some subset of the variables. This allows generating query samples without retraining the full VAE.  We experimentally evaluate three variations of cross-coding showing that (i) can be quickly optimized for different decompositions of evidence and query and (ii) they quantitatively and qualitatively outperform Hamiltonian Monte Carlo.", "keywords": [], "authorids": ["wuga@mie.utoronto.ca", "domke@cs.umass.edu", "ssanner@mie.utoronto.ca"], "authors": ["Ga Wu", "Justin Domke", "Scott Sanner"], "pdf": "/pdf/2acbda5f3f5a9ca4de0b727c8e5892acaea113cd.pdf", "paperhash": "wu|conditional_inference_in_pretrained_variational_autoencoders_via_crosscoding", "_bibtex": "@misc{\nwu2019conditional,\ntitle={Conditional Inference in Pre-trained Variational Autoencoders via Cross-coding},\nauthor={Ga Wu and Justin Domke and Scott Sanner},\nyear={2019},\nurl={https://openreview.net/forum?id=ByxLl309Ym},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1077/Official_Review", "cdate": 1542234311304, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "ByxLl309Ym", "replyto": "ByxLl309Ym", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1077/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335867775, "tmdate": 1552335867775, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1077/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 7}