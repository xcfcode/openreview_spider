{"notes": [{"id": "Sygn20VtwH", "original": "B1eBGjYOwr", "number": 1370, "cdate": 1569439411530, "ddate": null, "tcdate": 1569439411530, "tmdate": 1577168280572, "tddate": null, "forum": "Sygn20VtwH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Metagross: Meta Gated Recursive Controller Units for Sequence Modeling", "authors": ["Yi Tay", "Yikang Shen", "Alvin Chan", "Yew Soon Ong"], "authorids": ["ytay017@e.ntu.edu.sg", "yikang.shn@gmail.com", "guoweial001@e.ntu.edu.sg", "asysong@ntu.edu.sg"], "keywords": ["Deep Learning", "Natural Language Processing", "Recurrent Neural Networks"], "TL;DR": "Recursive Parameterization of Recurrent Models improve performance ", "abstract": "This paper proposes Metagross (Meta Gated Recursive Controller), a new neural sequence modeling unit. Our proposed unit is characterized by recursive parameterization of its gating functions, i.e., gating mechanisms of Metagross are controlled by instances of itself, which are repeatedly called in a recursive fashion. This can be interpreted as a form of meta-gating and recursively parameterizing a recurrent model. We postulate that our proposed inductive bias provides modeling benefits pertaining to learning with inherently hierarchically-structured sequence data (e.g., language, logical or music tasks). To this end, we conduct extensive experiments on recursive logic tasks (sorting, tree traversal, logical inference), sequential pixel-by-pixel classification, semantic parsing, code generation, machine translation and polyphonic music modeling, demonstrating the widespread utility of the proposed approach, i.e., achieving state-of-the-art (or close) performance on all tasks.", "pdf": "/pdf/ae6983aa8105f045ffe6a93a12ef0fa3292a6b44.pdf", "paperhash": "tay|metagross_meta_gated_recursive_controller_units_for_sequence_modeling", "original_pdf": "/attachment/ae6983aa8105f045ffe6a93a12ef0fa3292a6b44.pdf", "_bibtex": "@misc{\ntay2020metagross,\ntitle={Metagross: Meta Gated Recursive Controller Units for Sequence Modeling},\nauthor={Yi Tay and Yikang Shen and Alvin Chan and Yew Soon Ong},\nyear={2020},\nurl={https://openreview.net/forum?id=Sygn20VtwH}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "Mc4YmrEgS0", "original": null, "number": 1, "cdate": 1576798721707, "ddate": null, "tcdate": 1576798721707, "tmdate": 1576800914880, "tddate": null, "forum": "Sygn20VtwH", "replyto": "Sygn20VtwH", "invitation": "ICLR.cc/2020/Conference/Paper1370/-/Decision", "content": {"decision": "Reject", "comment": "This paper proposes a recurrent architecture based on a recursive gating mechanism. The reviewers leaned towards rejection on the basis of questions regarding novelty, analysis, and the experimental setting. Surprisingly, the authors chose not to engage in discussion, as all reviewers seems pretty open to having their minds changed. If none of the reviewers will champion the paper, and the authors cannot be bothered to champion their own work, I see no reason to recommend acceptance.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Metagross: Meta Gated Recursive Controller Units for Sequence Modeling", "authors": ["Yi Tay", "Yikang Shen", "Alvin Chan", "Yew Soon Ong"], "authorids": ["ytay017@e.ntu.edu.sg", "yikang.shn@gmail.com", "guoweial001@e.ntu.edu.sg", "asysong@ntu.edu.sg"], "keywords": ["Deep Learning", "Natural Language Processing", "Recurrent Neural Networks"], "TL;DR": "Recursive Parameterization of Recurrent Models improve performance ", "abstract": "This paper proposes Metagross (Meta Gated Recursive Controller), a new neural sequence modeling unit. Our proposed unit is characterized by recursive parameterization of its gating functions, i.e., gating mechanisms of Metagross are controlled by instances of itself, which are repeatedly called in a recursive fashion. This can be interpreted as a form of meta-gating and recursively parameterizing a recurrent model. We postulate that our proposed inductive bias provides modeling benefits pertaining to learning with inherently hierarchically-structured sequence data (e.g., language, logical or music tasks). To this end, we conduct extensive experiments on recursive logic tasks (sorting, tree traversal, logical inference), sequential pixel-by-pixel classification, semantic parsing, code generation, machine translation and polyphonic music modeling, demonstrating the widespread utility of the proposed approach, i.e., achieving state-of-the-art (or close) performance on all tasks.", "pdf": "/pdf/ae6983aa8105f045ffe6a93a12ef0fa3292a6b44.pdf", "paperhash": "tay|metagross_meta_gated_recursive_controller_units_for_sequence_modeling", "original_pdf": "/attachment/ae6983aa8105f045ffe6a93a12ef0fa3292a6b44.pdf", "_bibtex": "@misc{\ntay2020metagross,\ntitle={Metagross: Meta Gated Recursive Controller Units for Sequence Modeling},\nauthor={Yi Tay and Yikang Shen and Alvin Chan and Yew Soon Ong},\nyear={2020},\nurl={https://openreview.net/forum?id=Sygn20VtwH}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "Sygn20VtwH", "replyto": "Sygn20VtwH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795727052, "tmdate": 1576800279261, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1370/-/Decision"}}}, {"id": "BJlXlGyJ9H", "original": null, "number": 3, "cdate": 1571906026697, "ddate": null, "tcdate": 1571906026697, "tmdate": 1573902896203, "tddate": null, "forum": "Sygn20VtwH", "replyto": "Sygn20VtwH", "invitation": "ICLR.cc/2020/Conference/Paper1370/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #2", "review": "Update: As no rebuttal has been posted I stand by my assessment.\n\nSummary\nThis papers proposes a recursive parameterization of gates in a recurrent model. Instead of directly conditioning gates on the input and previous hidden representation, the proposed model recursively calls itself to parameterize the gate. The recursion depth is dynamically determined (up to a predefined maximum recursion depth). The approach shows slight improvements over baselines on a range of tasks.\n\nStrengths\nSlight improvements on a wide range of downstream tasks\nQualitative analysis of dynamic recursion highlighting the adaptability of the method to different task properties\n\nWeaknesses\nFrom the equations in Section 2, I understand that a gate is conditioned not only on the input and previous hidden representation, but also the input and hidden representations from repeated application of the RNN cell for the given time step. As far as I understand, this is very closely related to ACT by Graves, Alex. \"Adaptive computation time for recurrent neural networks.\" 2016. They also have a dynamic way of determining how many recursive RNN cell applications per step should be performed.\nI am missing a clear description of differences of the proposed approach to the baselines tested in Section 3. At some point it is mentioned that a different optimizer was used (R-Adam). Are there any other confounding factors? I believe it is important to get clarity on these given that the difference between the model and baselines are very small.\nResults in Table 1 are highlighted in a misleading way. For example, stacked BiLSTM do as well for tree traversal (EM, n=5 and EM, n=10). For logical inference, there is a more recent paper investigating the limits of RNNs and I believe comparisons on that dataset could strengthen the paper: Evans, Richard, et al. \"Can Neural Networks Understand Logical Entailment?.\" ICLR 2018.\n\nMinor Comments\np1: \"the ability to reason deeply\" \u2013 I understand you mean literally \"deep\", but also \"reason\" is a loaded term.\np1: \"bears a totally different meaning\" should be made concrete\np3: I am a bit confused by the fact that Metagross is used to extend Transformers here given that you called it a recurrent (and recursive) model on the previous slide.\np4: What is the number of layers used in he stacked BiLSTM and is this the same as the maximum depth used in Metagross? I believe for a fair comparison they should be the same. I am also missing stacked LSTM in experiments on logical inference (Table 2).\nQuestions to Authors", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper1370/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1370/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Metagross: Meta Gated Recursive Controller Units for Sequence Modeling", "authors": ["Yi Tay", "Yikang Shen", "Alvin Chan", "Yew Soon Ong"], "authorids": ["ytay017@e.ntu.edu.sg", "yikang.shn@gmail.com", "guoweial001@e.ntu.edu.sg", "asysong@ntu.edu.sg"], "keywords": ["Deep Learning", "Natural Language Processing", "Recurrent Neural Networks"], "TL;DR": "Recursive Parameterization of Recurrent Models improve performance ", "abstract": "This paper proposes Metagross (Meta Gated Recursive Controller), a new neural sequence modeling unit. Our proposed unit is characterized by recursive parameterization of its gating functions, i.e., gating mechanisms of Metagross are controlled by instances of itself, which are repeatedly called in a recursive fashion. This can be interpreted as a form of meta-gating and recursively parameterizing a recurrent model. We postulate that our proposed inductive bias provides modeling benefits pertaining to learning with inherently hierarchically-structured sequence data (e.g., language, logical or music tasks). To this end, we conduct extensive experiments on recursive logic tasks (sorting, tree traversal, logical inference), sequential pixel-by-pixel classification, semantic parsing, code generation, machine translation and polyphonic music modeling, demonstrating the widespread utility of the proposed approach, i.e., achieving state-of-the-art (or close) performance on all tasks.", "pdf": "/pdf/ae6983aa8105f045ffe6a93a12ef0fa3292a6b44.pdf", "paperhash": "tay|metagross_meta_gated_recursive_controller_units_for_sequence_modeling", "original_pdf": "/attachment/ae6983aa8105f045ffe6a93a12ef0fa3292a6b44.pdf", "_bibtex": "@misc{\ntay2020metagross,\ntitle={Metagross: Meta Gated Recursive Controller Units for Sequence Modeling},\nauthor={Yi Tay and Yikang Shen and Alvin Chan and Yew Soon Ong},\nyear={2020},\nurl={https://openreview.net/forum?id=Sygn20VtwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Sygn20VtwH", "replyto": "Sygn20VtwH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1370/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1370/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575650847329, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1370/Reviewers"], "noninvitees": [], "tcdate": 1570237738368, "tmdate": 1575650847345, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1370/-/Official_Review"}}}, {"id": "S1gofkQqYS", "original": null, "number": 1, "cdate": 1571594003156, "ddate": null, "tcdate": 1571594003156, "tmdate": 1572972477535, "tddate": null, "forum": "Sygn20VtwH", "replyto": "Sygn20VtwH", "invitation": "ICLR.cc/2020/Conference/Paper1370/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "The authors propose a variant of gating functions for recurrent neural networks and feed-forward layers of Transformer and apply it to variety of tasks including toy tasks such as sorting, tree traversal and more realistic tasks such as machine translation. The gating function is applied recursively for N number of steps and depth of recursion is learned softly in data-driven function. Authors show similar or slightly better performance of their approach when applied to LSTM and Transformer compared to vanilla LSTM and Transformer.  \n\nI have several comments regarding this work:\n\n1) I believe there is a typo in equation in section 2 describing parametrization of o^{n}_{t} where h_{t-1} has un-necessary upper-script {n}\n\n2) I would like to see the equations showing differences/similarities between MetaGross gating function and GRU/LSTM in Section 2 to better understand how MetaGross relates to previous work.\n\n3) Calling parallel version of MetaGross that computes gating values given entire sequence *non-autoregressive* is really correct since decoding for machine translation is still done in autoregressive fashion one token at a time. I would suggest the authors to remove word non-autoregressive and just stick with word parallel.\n\n4) The obvious connection with this work and work of Alex Graves on Adaptive Computation for Recurrent Neural Nets and many of its followups including Universal Transformers by Dehghani et al is missing from experimental comparison and is not even mentioned in related section. I believe that not mentioning these papers and not comparing to them empirically  this is a major drawback of this paper. \n\n5) I don't think that Task 3 (Logical Inference) contains a language vocabulary of six words because it is a natural english language unless I am misunderstanding something. \n\n6) What does EM stand for in Table 1. Would be great if you could include description of what EM, P(perplexity), n(depth of recursion) stands for in caption of Table 1\n\nOverall it is a good and well written paper, although I believe that the variants of recursively gated functions have been proposed and applied before (see comment 4). Would be open to discussions and raising scores if authors convince me otherwise. "}, "signatures": ["ICLR.cc/2020/Conference/Paper1370/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1370/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Metagross: Meta Gated Recursive Controller Units for Sequence Modeling", "authors": ["Yi Tay", "Yikang Shen", "Alvin Chan", "Yew Soon Ong"], "authorids": ["ytay017@e.ntu.edu.sg", "yikang.shn@gmail.com", "guoweial001@e.ntu.edu.sg", "asysong@ntu.edu.sg"], "keywords": ["Deep Learning", "Natural Language Processing", "Recurrent Neural Networks"], "TL;DR": "Recursive Parameterization of Recurrent Models improve performance ", "abstract": "This paper proposes Metagross (Meta Gated Recursive Controller), a new neural sequence modeling unit. Our proposed unit is characterized by recursive parameterization of its gating functions, i.e., gating mechanisms of Metagross are controlled by instances of itself, which are repeatedly called in a recursive fashion. This can be interpreted as a form of meta-gating and recursively parameterizing a recurrent model. We postulate that our proposed inductive bias provides modeling benefits pertaining to learning with inherently hierarchically-structured sequence data (e.g., language, logical or music tasks). To this end, we conduct extensive experiments on recursive logic tasks (sorting, tree traversal, logical inference), sequential pixel-by-pixel classification, semantic parsing, code generation, machine translation and polyphonic music modeling, demonstrating the widespread utility of the proposed approach, i.e., achieving state-of-the-art (or close) performance on all tasks.", "pdf": "/pdf/ae6983aa8105f045ffe6a93a12ef0fa3292a6b44.pdf", "paperhash": "tay|metagross_meta_gated_recursive_controller_units_for_sequence_modeling", "original_pdf": "/attachment/ae6983aa8105f045ffe6a93a12ef0fa3292a6b44.pdf", "_bibtex": "@misc{\ntay2020metagross,\ntitle={Metagross: Meta Gated Recursive Controller Units for Sequence Modeling},\nauthor={Yi Tay and Yikang Shen and Alvin Chan and Yew Soon Ong},\nyear={2020},\nurl={https://openreview.net/forum?id=Sygn20VtwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Sygn20VtwH", "replyto": "Sygn20VtwH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1370/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1370/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575650847329, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1370/Reviewers"], "noninvitees": [], "tcdate": 1570237738368, "tmdate": 1575650847345, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1370/-/Official_Review"}}}, {"id": "S1xtQSg0Fr", "original": null, "number": 2, "cdate": 1571845409295, "ddate": null, "tcdate": 1571845409295, "tmdate": 1572972477501, "tddate": null, "forum": "Sygn20VtwH", "replyto": "Sygn20VtwH", "invitation": "ICLR.cc/2020/Conference/Paper1370/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a neural sequence modelling unit called METAGROSS.  In principle, the aim of this unit is to introduce recursive parametrization of  gating functions, building on the gated RNN paradigm.  The authors motivate this work by arguing that while gated-RNNs tackle vanishing gradient problems and facilitate learning long-range dependencies in sequences, improvements can be made with respect to learning on hierarchically-structured data.  The authors propose a method to do so by also learning the depth of the parametrization, and claim that the inductive bias that emerges from this configuration is beneficial to learning such tasks.\n\nI think the idea behind this work is sensible: introducing a meta-controller with recursive parametrization of gating functions for hierarchical tasks is sensible.  Another also strong point of this paper is that several experiments under different settings are presented, along with ablation studies and some model exploration.  The authors also show that integrating a non-autoregressive variant of the proposed meta-controller into the Transformer architecture can also be beneficial.  Results in general do show improvement over compared architectures.\n\nOn the negative side and besides empirical/experimental evidence, the paper would be much more convinving with some more insight into the model itself and some more qualitative evidence.  Figure 1 shows the architecture of the proposed method with a max depth of 3 indicating soft recursion with grayscale levels.  However, this figure is not referenced in the text and not explained further.  The non-autoregressive version (sec 2.3) simply does away with the dependence on hidden states and applies the proposed architecture directly on the input.  I think it would be very beneficial to see a simple toy example where the benefits of utilizing this meta-architecture can be qualitatively explained.  Finally, one can argue that this work is incremental, in the sense that it is a (relatively straightfwd) combination of meta-controllers with recursive architectures.  Differences and variations with respect to other methods in literature should be more clearly explained.\n\nsome more questions\n- although most experiments show some resilience with respect to the varying max depth parameter, have the authors noticed any problems and limitations arising from setting the max-depth to be very high?  for example, in Table 1 it seems that accuracy may drop when increasing max length.  I think that more discussions and ellaboration on this could be useful, as the authors also propose a way of learning the depth parameters and also state that this is task dependent.  \n- Figures 7-8-9 show variations of dynamic recursion on three databases.  Although these figures do show variation in learning (ranging from high activation fluctuations to static), it would be interesting to examine why this fluctuations occur in CIFAR  \n- which brings me to the second question:  although the results do not show this, could this task-dependent nature of the controller lead to more chances of overfitting on a given training set that may be noisy?\n- Although the authors perform ablation tests with multiple units and evaluate for max depth, we don't see many experiments with depth of more than 2 or 3 (besides table 1 - in many experiments the max depth is not mentioned).  Are the conclusions the same with all experiments wrt depth?\n- It would be useful to have a comment on model complexity"}, "signatures": ["ICLR.cc/2020/Conference/Paper1370/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1370/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Metagross: Meta Gated Recursive Controller Units for Sequence Modeling", "authors": ["Yi Tay", "Yikang Shen", "Alvin Chan", "Yew Soon Ong"], "authorids": ["ytay017@e.ntu.edu.sg", "yikang.shn@gmail.com", "guoweial001@e.ntu.edu.sg", "asysong@ntu.edu.sg"], "keywords": ["Deep Learning", "Natural Language Processing", "Recurrent Neural Networks"], "TL;DR": "Recursive Parameterization of Recurrent Models improve performance ", "abstract": "This paper proposes Metagross (Meta Gated Recursive Controller), a new neural sequence modeling unit. Our proposed unit is characterized by recursive parameterization of its gating functions, i.e., gating mechanisms of Metagross are controlled by instances of itself, which are repeatedly called in a recursive fashion. This can be interpreted as a form of meta-gating and recursively parameterizing a recurrent model. We postulate that our proposed inductive bias provides modeling benefits pertaining to learning with inherently hierarchically-structured sequence data (e.g., language, logical or music tasks). To this end, we conduct extensive experiments on recursive logic tasks (sorting, tree traversal, logical inference), sequential pixel-by-pixel classification, semantic parsing, code generation, machine translation and polyphonic music modeling, demonstrating the widespread utility of the proposed approach, i.e., achieving state-of-the-art (or close) performance on all tasks.", "pdf": "/pdf/ae6983aa8105f045ffe6a93a12ef0fa3292a6b44.pdf", "paperhash": "tay|metagross_meta_gated_recursive_controller_units_for_sequence_modeling", "original_pdf": "/attachment/ae6983aa8105f045ffe6a93a12ef0fa3292a6b44.pdf", "_bibtex": "@misc{\ntay2020metagross,\ntitle={Metagross: Meta Gated Recursive Controller Units for Sequence Modeling},\nauthor={Yi Tay and Yikang Shen and Alvin Chan and Yew Soon Ong},\nyear={2020},\nurl={https://openreview.net/forum?id=Sygn20VtwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Sygn20VtwH", "replyto": "Sygn20VtwH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1370/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1370/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575650847329, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1370/Reviewers"], "noninvitees": [], "tcdate": 1570237738368, "tmdate": 1575650847345, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1370/-/Official_Review"}}}], "count": 5}