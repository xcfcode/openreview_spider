{"notes": [{"id": "mj7WsaHYxj", "original": "jQKKSh2__gZ", "number": 142, "cdate": 1601308024496, "ddate": null, "tcdate": 1601308024496, "tmdate": 1614985620039, "tddate": null, "forum": "mj7WsaHYxj", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "FLAG: Adversarial Data Augmentation for Graph Neural Networks", "authorids": ["~Kezhi_Kong1", "~Guohao_Li1", "~Mucong_Ding1", "~Zuxuan_Wu1", "~Chen_Zhu2", "~Bernard_Ghanem1", "~Gavin_Taylor1", "~Tom_Goldstein1"], "authors": ["Kezhi Kong", "Guohao Li", "Mucong Ding", "Zuxuan Wu", "Chen Zhu", "Bernard Ghanem", "Gavin Taylor", "Tom Goldstein"], "keywords": ["Graph Neural Networks", "Data Augmentation", "Adversarial Training"], "abstract": "Data augmentation helps neural networks generalize better, but it remains an open question how to effectively augment graph data to enhance the performance of GNNs (Graph Neural Networks). While most existing graph regularizers focus on augmenting graph topological structures by adding/removing edges, we offer a novel direction to augment in the input node feature space for better performance. We propose a simple but effective solution, FLAG (Free Large-scale Adversarial Augmentation on Graphs), which iteratively augments node features with gradient-based adversarial perturbations during training, and boosts performance at test time. Empirically, FLAG can be easily implemented with a dozen lines of code and is flexible enough to function with any GNN backbone, on a wide variety of large-scale datasets, and in both transductive and inductive settings. Without modifying a model's architecture or training setup, FLAG yields a consistent and salient performance boost across both node and graph classification tasks. Using FLAG, we reach state-of-the-art performance on the large-scale ogbg-molpcba, ogbg-ppa, and ogbg-code datasets.  ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kong|flag_adversarial_data_augmentation_for_graph_neural_networks", "one-sentence_summary": "We show that adversarial data augmentation generalizes Graph Neural Networks on large-scale datasets.", "pdf": "/pdf/f154c19f743894f4c4fdaf90a2439109a3374ada.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=Z7dgKUsF6f", "_bibtex": "@misc{\nkong2021flag,\ntitle={{\\{}FLAG{\\}}: Adversarial Data Augmentation for Graph Neural Networks},\nauthor={Kezhi Kong and Guohao Li and Mucong Ding and Zuxuan Wu and Chen Zhu and Bernard Ghanem and Gavin Taylor and Tom Goldstein},\nyear={2021},\nurl={https://openreview.net/forum?id=mj7WsaHYxj}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 11, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "7nRgl5xilO7", "original": null, "number": 1, "cdate": 1610040539180, "ddate": null, "tcdate": 1610040539180, "tmdate": 1610474149512, "tddate": null, "forum": "mj7WsaHYxj", "replyto": "mj7WsaHYxj", "invitation": "ICLR.cc/2021/Conference/Paper142/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "This paper studies the problem of adversarial training for graph neural networks. The proposed method is build on the free training approach, and more specifically FreeLB, with some additional tricks including bias perturbation (for node-classification) and unbounded attacks.  While these additions are potentially useful, there are only limited investigation into their effect.  Putting aside the technical distinctions of the method with prior work, this paper can also be viewed as an empirical study of adversarial training techniques on graph data with various GNN architectures.  It is worth noting that overall the conclusions on \"adversarial training\" are positive, we do see consistent improvement over a variety of architectures and tasks.   The issues however, is that it is unclear whether these improvements can be similarly achieved using prior technique like FreeLB (oblation is only done on one single task, where biased perturbation is shown to lead to minor improvement).   The paper also provides some results showing the effect of depth of the network as well as different training strategies such as batch norm, dropout with general adversarial training.  These results are interesting to see but do seem to be limited in both scope and depth.  It appears that the authors have two goals in mind, one is to propose FLAG and demonstrate its usefulness, and the other is to provide a better understanding of how adversarial training works for GNNs in general.  Given the limited novelty of FLAG compared to prior methods, the main contribution actually comes from the later part, which unfortunately is somewhat underdeveloped.  \n"}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "FLAG: Adversarial Data Augmentation for Graph Neural Networks", "authorids": ["~Kezhi_Kong1", "~Guohao_Li1", "~Mucong_Ding1", "~Zuxuan_Wu1", "~Chen_Zhu2", "~Bernard_Ghanem1", "~Gavin_Taylor1", "~Tom_Goldstein1"], "authors": ["Kezhi Kong", "Guohao Li", "Mucong Ding", "Zuxuan Wu", "Chen Zhu", "Bernard Ghanem", "Gavin Taylor", "Tom Goldstein"], "keywords": ["Graph Neural Networks", "Data Augmentation", "Adversarial Training"], "abstract": "Data augmentation helps neural networks generalize better, but it remains an open question how to effectively augment graph data to enhance the performance of GNNs (Graph Neural Networks). While most existing graph regularizers focus on augmenting graph topological structures by adding/removing edges, we offer a novel direction to augment in the input node feature space for better performance. We propose a simple but effective solution, FLAG (Free Large-scale Adversarial Augmentation on Graphs), which iteratively augments node features with gradient-based adversarial perturbations during training, and boosts performance at test time. Empirically, FLAG can be easily implemented with a dozen lines of code and is flexible enough to function with any GNN backbone, on a wide variety of large-scale datasets, and in both transductive and inductive settings. Without modifying a model's architecture or training setup, FLAG yields a consistent and salient performance boost across both node and graph classification tasks. Using FLAG, we reach state-of-the-art performance on the large-scale ogbg-molpcba, ogbg-ppa, and ogbg-code datasets.  ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kong|flag_adversarial_data_augmentation_for_graph_neural_networks", "one-sentence_summary": "We show that adversarial data augmentation generalizes Graph Neural Networks on large-scale datasets.", "pdf": "/pdf/f154c19f743894f4c4fdaf90a2439109a3374ada.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=Z7dgKUsF6f", "_bibtex": "@misc{\nkong2021flag,\ntitle={{\\{}FLAG{\\}}: Adversarial Data Augmentation for Graph Neural Networks},\nauthor={Kezhi Kong and Guohao Li and Mucong Ding and Zuxuan Wu and Chen Zhu and Bernard Ghanem and Gavin Taylor and Tom Goldstein},\nyear={2021},\nurl={https://openreview.net/forum?id=mj7WsaHYxj}\n}"}, "tags": [], "invitation": {"reply": {"forum": "mj7WsaHYxj", "replyto": "mj7WsaHYxj", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040539167, "tmdate": 1610474149495, "id": "ICLR.cc/2021/Conference/Paper142/-/Decision"}}}, {"id": "9690QDAknnr", "original": null, "number": 1, "cdate": 1603561737760, "ddate": null, "tcdate": 1603561737760, "tmdate": 1607123308675, "tddate": null, "forum": "mj7WsaHYxj", "replyto": "mj7WsaHYxj", "invitation": "ICLR.cc/2021/Conference/Paper142/-/Official_Review", "content": {"title": "Good method and results, but it is not compared with other data augmentation methods", "review": "---- Summary\n\nThis paper proposes FLAG (Free Large-scale Adversarial Augmentation on Graphs), an adversarial data augmentation technique that can be applied to different GNN models in order to improve their generalization. The proposed technique consists on adding adversarial perturbations to the nodes\u2019 features solving the standard min-max problem for adversarial training. In this setup, a noise vector is added to the input features which tries to maximize the loss by performing gradient ascent, while the classifier is trained to minimize the loss despite the added adversarial noise.\nThe authors show that their method can easily be added to standard GNN models like GCN, GAT, GraphSAGE and DeeperGCN with minimal changes, and that it improves these models on different tasks.\n\n---- Pros\n\n* The authors evaluate their method on the Open Graph Benchmark (OGB), which is a standard benchmark that facilitates comparison with other methods.\n* FLAG is a generic and model agnostic method that can be applied to already established models like GCN, GAT and GraphSAGE.\n* Clear explanation of the proposed method with a concise pytorch implementation as an example.\n* Good empirical results with different models on different OGB tasks.\n\n---- Cons\n\n* My main concern with this paper is that the proposed augmentation technique is not compared against any other graph augmentation techniques. What is the justification for using this one over other techniques? Can FLAG work with other data augmentation techniques? Showing experiments of FLAG in combination or compared to other techniques would strengthen the experiments section.\n\n---- Questions to the authors\n\n* As I said in my concerns, if this kind of feature data augmentation is orthogonal to structure-based data augmentation that adds or removes edges (or nodes), it would be very interesting to do an experiment showing to which degree it can be used together with other graph augmentation techniques.\n*The motivation for the biased perturbation explained in the \u201cBiased perturbation for node classification\u201d paragraph in Section 3 is not very clear. The authors claim that nodes that are far away from a target (labelled) node will have lower effect on the prediction for that node, thus they increase the adversarial step size $\\alpha_u$ for the unlabelled nodes. That will increase the amount of noise added to the unlabelled nodes, but why is that beneficial? Also, what is the relation between unlabelled nodes and their distance to the target nodes? Why does it have anything to do with promoting invariance for further-away nodes? \n\n---- Minor comments\n\n- Although the approach is a bit different, the Adversarial AutoAugment paper [1] seems to be quite related, the authors might want to reference it in their paper. Besides the task domain (images vs graphs) the main difference is that instead of adversarially learning perturbations, in [1] data augmentation policies are adversarially learnt to apply perturbations to the input. \n- Also, there are a couple of contemporary papers on graph data augmentation [2, 3] that the authors could reference too.\n\n---- Justification for score\n\nThe paper is well written and the method interesting and well explained, but it lacks comparisons with other graph data augmentation approaches.\n\n------ Post-Rebuttal Update ------\n\nI have read the author's response, thank you for adding more experiments with other graph data augmentation techniques. However, the explanation and justification for the \"biased perturbations\" still seems a bit weak and under explored in the experiments section, so I've decided to keep my initial score.\n\n---- References\n\n[1] Zhang, Xinyu, et al. \"Adversarial autoaugment.\" arXiv preprint arXiv:1912.11188 (2019).\n\n[2] Jiajun Zhou, Jie Shen, and Qi Xuan. 2020. Data Augmentation for Graph Classification. In Proceedings of the 29th ACM International Conference on Information & Knowledge Management (CIKM '20). Association for Computing Machinery, New York, NY, USA, 2341\u20132344. DOI:https://doi.org/10.1145/3340531.3412086\n\n[3] Zhao, T., Liu, Y., Neves, L., Woodford, O., Jiang, M., & Shah, N. (2020). Data Augmentation for Graph Neural Networks. arXiv preprint arXiv:2006.06830.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper142/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper142/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "FLAG: Adversarial Data Augmentation for Graph Neural Networks", "authorids": ["~Kezhi_Kong1", "~Guohao_Li1", "~Mucong_Ding1", "~Zuxuan_Wu1", "~Chen_Zhu2", "~Bernard_Ghanem1", "~Gavin_Taylor1", "~Tom_Goldstein1"], "authors": ["Kezhi Kong", "Guohao Li", "Mucong Ding", "Zuxuan Wu", "Chen Zhu", "Bernard Ghanem", "Gavin Taylor", "Tom Goldstein"], "keywords": ["Graph Neural Networks", "Data Augmentation", "Adversarial Training"], "abstract": "Data augmentation helps neural networks generalize better, but it remains an open question how to effectively augment graph data to enhance the performance of GNNs (Graph Neural Networks). While most existing graph regularizers focus on augmenting graph topological structures by adding/removing edges, we offer a novel direction to augment in the input node feature space for better performance. We propose a simple but effective solution, FLAG (Free Large-scale Adversarial Augmentation on Graphs), which iteratively augments node features with gradient-based adversarial perturbations during training, and boosts performance at test time. Empirically, FLAG can be easily implemented with a dozen lines of code and is flexible enough to function with any GNN backbone, on a wide variety of large-scale datasets, and in both transductive and inductive settings. Without modifying a model's architecture or training setup, FLAG yields a consistent and salient performance boost across both node and graph classification tasks. Using FLAG, we reach state-of-the-art performance on the large-scale ogbg-molpcba, ogbg-ppa, and ogbg-code datasets.  ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kong|flag_adversarial_data_augmentation_for_graph_neural_networks", "one-sentence_summary": "We show that adversarial data augmentation generalizes Graph Neural Networks on large-scale datasets.", "pdf": "/pdf/f154c19f743894f4c4fdaf90a2439109a3374ada.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=Z7dgKUsF6f", "_bibtex": "@misc{\nkong2021flag,\ntitle={{\\{}FLAG{\\}}: Adversarial Data Augmentation for Graph Neural Networks},\nauthor={Kezhi Kong and Guohao Li and Mucong Ding and Zuxuan Wu and Chen Zhu and Bernard Ghanem and Gavin Taylor and Tom Goldstein},\nyear={2021},\nurl={https://openreview.net/forum?id=mj7WsaHYxj}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "mj7WsaHYxj", "replyto": "mj7WsaHYxj", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper142/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538149475, "tmdate": 1606915810918, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper142/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper142/-/Official_Review"}}}, {"id": "CJsOFXTopZ1", "original": null, "number": 2, "cdate": 1603756090097, "ddate": null, "tcdate": 1603756090097, "tmdate": 1606766236113, "tddate": null, "forum": "mj7WsaHYxj", "replyto": "mj7WsaHYxj", "invitation": "ICLR.cc/2021/Conference/Paper142/-/Official_Review", "content": {"title": "A good empirical study with several new insights.", "review": "This paper investigates adversarial feature augmentation for improving the generalizability of graph neural networks. The authors adopt an existing augmentation algorithm and apply on the nodes of each training graph, and use the perturbed graphs for training. The focus of the paper is extensive experimentation in various tasks and settings to illustrate the effectiveness of adversarial augmentation in graph-based tasks. The experiments provide new, non-trivial insights, such as the effect of the number of network layers on the effectiveness of augmentation. The paper is well-written and easy to read.\n\nNevertheless, there are a few drawbacks:\n\n1. Even though the method is completely adopted from prior work and there are no novelties, the authors claim they propose a new solution for graph data augmentation. It is true that adversarial feature augmentation has not been studied for graph neural nets, but it is a straightforward idea to apply an existing feature augmentation method on graph nodes, which are represented using feature vectors just like other types of data. \n\n2. Even though the performance improvement is consistent across tasks, it is not significant. Hence, my takeaway from the paper (which is in fact a valuable takeaway) is that adversarial augmentation is not considerably effective for graph neural nets, no matter what dataset and network architecture is used. A truly \"free\" augmentation is even less effective, as shown in table 2. To be clear, this is not a weakness of this paper. However, the authors are encouraged to acknowledge it and use a more neutral and objective language when describing the performance.\n\n3. Some conclusions are not reliable enough. For instance, the impact of biased perturbation is statistically insignificant in table 2, and those limited numbers are not sufficient to make such a deduction. Moreover, section 6 is not convincing. Just because augmentation hurts the performance of MLP on images but improves on graphs doesn't mean data distribution is the primary determinant of the efficacy of augmentation. It is similarly insufficient that augmentation improves the results on a discrete graph but doesn't improve when noise is added. Moreover, the authors do not elaborate \"how\" the data distribution affects the augmentation. Just proving that data distribution has an effect is a trivial and unhelpful fact. Furthermore, the authors do not support the claim that model architecture does not affect the efficacy of augmentation. In fact, they prove otherwise by figure 1 (left), where the network depth has a direct effect on the performance gap.\n\n4. Table 5 is confusing, as it does not demonstrate any relevant information. The two bottom rows of both tables are a repetition of results from table 1, and only the first row is new, which only shows that GAT performs worse without BatchNorm or Dropout. The table does not show how FLAG performs without BatchNorm and Dropout, and hence there is no new takeaway from this table that is relevant to the purpose of this paper.\n\nIn sum, although the paper provides new and valuable findings, the experiment analysis and conclusions that are made are not strong enough for a purely empirical study. Hence, I recommend improving and resubmitting the paper, either by adding methodological novelty, or by bolstering the analysis sections to make more reliable and useful conclusions. I also encourage the authors to clarify any part that I may have misunderstood, as I am keen to adjust my rating accordingly.\n\n######## Post-Rebuttal Updates:\n\nI appreciate the authors' response, but my main concerns were not addressed. Particularly, I still believe the novelty of this method is extremely limited, as it directly applies an existing method on graph node embeddings. The main novelties are biased perturbation and unbounded attack, which are both very simple modifications, and were not adequately studied in experiments. Although the authors show extensive comparison on various datasets, their comparison does not particularly show the contribution of biased perturbation and unbounded attack. They only show the effect of biased perturbation using a single number in Table 2, which is not convincing. \n\n\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper142/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper142/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "FLAG: Adversarial Data Augmentation for Graph Neural Networks", "authorids": ["~Kezhi_Kong1", "~Guohao_Li1", "~Mucong_Ding1", "~Zuxuan_Wu1", "~Chen_Zhu2", "~Bernard_Ghanem1", "~Gavin_Taylor1", "~Tom_Goldstein1"], "authors": ["Kezhi Kong", "Guohao Li", "Mucong Ding", "Zuxuan Wu", "Chen Zhu", "Bernard Ghanem", "Gavin Taylor", "Tom Goldstein"], "keywords": ["Graph Neural Networks", "Data Augmentation", "Adversarial Training"], "abstract": "Data augmentation helps neural networks generalize better, but it remains an open question how to effectively augment graph data to enhance the performance of GNNs (Graph Neural Networks). While most existing graph regularizers focus on augmenting graph topological structures by adding/removing edges, we offer a novel direction to augment in the input node feature space for better performance. We propose a simple but effective solution, FLAG (Free Large-scale Adversarial Augmentation on Graphs), which iteratively augments node features with gradient-based adversarial perturbations during training, and boosts performance at test time. Empirically, FLAG can be easily implemented with a dozen lines of code and is flexible enough to function with any GNN backbone, on a wide variety of large-scale datasets, and in both transductive and inductive settings. Without modifying a model's architecture or training setup, FLAG yields a consistent and salient performance boost across both node and graph classification tasks. Using FLAG, we reach state-of-the-art performance on the large-scale ogbg-molpcba, ogbg-ppa, and ogbg-code datasets.  ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kong|flag_adversarial_data_augmentation_for_graph_neural_networks", "one-sentence_summary": "We show that adversarial data augmentation generalizes Graph Neural Networks on large-scale datasets.", "pdf": "/pdf/f154c19f743894f4c4fdaf90a2439109a3374ada.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=Z7dgKUsF6f", "_bibtex": "@misc{\nkong2021flag,\ntitle={{\\{}FLAG{\\}}: Adversarial Data Augmentation for Graph Neural Networks},\nauthor={Kezhi Kong and Guohao Li and Mucong Ding and Zuxuan Wu and Chen Zhu and Bernard Ghanem and Gavin Taylor and Tom Goldstein},\nyear={2021},\nurl={https://openreview.net/forum?id=mj7WsaHYxj}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "mj7WsaHYxj", "replyto": "mj7WsaHYxj", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper142/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538149475, "tmdate": 1606915810918, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper142/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper142/-/Official_Review"}}}, {"id": "ZvhvbV7eBrb", "original": null, "number": 3, "cdate": 1605179506415, "ddate": null, "tcdate": 1605179506415, "tmdate": 1605601237493, "tddate": null, "forum": "mj7WsaHYxj", "replyto": "CJsOFXTopZ1", "invitation": "ICLR.cc/2021/Conference/Paper142/-/Official_Comment", "content": {"title": "Thanks for your review", "comment": "We thank the reviewer for the valuable feedbacks and for noting that our paper carries out extensive experiments and provides non-trivial insights. \n\n1: \u201cThe method is completely adopted from prior work.\u201d\n\nWe respectfully disagree. As stated in the Section 3, FLAG adopts \u201cFree\u201d method and gradient accumulation for adversary crafting and model weights updating. These two methods are both common practices of adversarial training. Besides, the proposed unbounded attack and biased perturbation are simple but important techniques that work solely for GNNs to simplify hyperparameter search and to promote reasonable extent of invariance with respect to nodes. Therefore we claim that FLAG is a new adversarial augmentation method on graphs.\n\n2: \"Even though the performance improvement is consistent across tasks, it is not significant.\u201d\n\nWe clarify that, the primary motivation of our work is to show that adversarial augmentations are able to generalize GNNs across varied GNN baselines, prediction tasks (node and graph), graph datasets, etc. We acknowledge the existence of several marginal performance lifts (even drops), but note that such a highly general method can also yield significant improvements like what GAT gets on ogbn-products (2.31%) and what GIN-Virtual obtains on ogbg-ppa (2.08%). Also we agree to use a more neutral tone and we will refine our writing in the newer version of the paper. \n\n3.1: \u201csection 6 is not convincing\u201d\n\nTo clarify, in Section 6 we aim to provide a new perspective to analyze and explain the effect of adversarial augmentation. To our knowledge, our work is the first to discuss and try to explain the different behaviors of adversarial augmentation on different tasks, and our analysis is endorsed by Reviewer1. Intuitively the discrepancy can most possibly come from the following two aspects: model differences (message-passing GNNs vs. feed-forward CNNs (MLPs)), data distribution differences (discrete distribution of graphs vs. continuous distribution of images).\n\nWe conjecture that the main cause comes from the latter with the following evidence:\n(a). Clean performance of CNNs is harmed by adversarial augmentations on CIFAR-10, CIFAR-100 and ImageNet, but is boosted on MNIST [3], whose pixel values are much closer to discrete distribution.\n(b). NLP tasks share the same phenomenon, where word embeddings are represented as discrete one-hot vectors.\n(c). Using FLAG to augment MLPs (a kind of feed-forward neural net like CNNs, which adversarial augmentation also has adverse effects on in computer vision), as reported in Section 6, on ogbn-arxiv the test performance goes from 55.50\u00b10.23% to **56.02\u00b10.19%**. To further support the argument, we add another experiment using FLAG to lift MLPs from 61.06\u00b10.08% to **62.41\u00b10.16%** on ogbn-products.\n(d). As shown in the simple example, when node features are drawn from continuous normal distribution, FGSM starts to harm the performance of GCN. \n\nAll of the four observations supports our conjecture. We hope our analysis and findings can provide insights and catalyze future works on this important topic.\n\n3.2: \"the impact of biased perturbation is statistically insignificant in table 2\u201d\n\nWe disagree. Note that the node size of ogbn-products is more than **2.4M**, from which 90% of the nodes are test nodes (2.2M). The lift from biased perturbation is 0.47%, which means **10K+** more nodes are successfully classified and the improvement is non-trivial. It is even more than the amount of the test set of CIFAR-10. We also respectfully point out that this ablation study is endorsed by Reviewer1.\n\n3.3: \u201cthe authors do not support the claim that model architecture does not affect the efficacy of augmentation.\u201d\n\nWe apologize for the confusion. To clarify, we discuss about the discrepancy between message-passing GNNs vs. feed-forward CNNs (MLPs), rather than specific model architectures (types, layers, widths, etc.). Also we focus on the positive vs. negative effect difference, rather than extent. We will refine our writing in the newer version.\n\n4: \"Table 5 is confusing\u201d\n\nWe apologize for the confusion. We clarify that it is not intuitive that batchnorm and dropout are supposed to work with adversarial augmentations directly. [1,2] showed that special batch norm and dropout techniques were important to cope with adversarial augmentation. Here in table 5 we show that FLAG can directly work with both techniques to improve performance further without extra designs, which adds to the simplicity and compatibility of our method. The reusing of existing numbers are to make comparisons clearer. We will reorganize the writing in newer version of our paper. \n\nPlease let us know if you have further questions or comments!\n\n[1] Xie, et al. \"Adversarial examples improve image recognition. CVPR 2020\n\n[2] Zhu, et al. \"Freelb: Enhanced adversarial training for natural language understanding.\" ICLR 2019\n\n[3] Tsipras, et al. \"Robustness May Be at Odds with Accuracy.\" ICLR 2018"}, "signatures": ["ICLR.cc/2021/Conference/Paper142/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper142/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "FLAG: Adversarial Data Augmentation for Graph Neural Networks", "authorids": ["~Kezhi_Kong1", "~Guohao_Li1", "~Mucong_Ding1", "~Zuxuan_Wu1", "~Chen_Zhu2", "~Bernard_Ghanem1", "~Gavin_Taylor1", "~Tom_Goldstein1"], "authors": ["Kezhi Kong", "Guohao Li", "Mucong Ding", "Zuxuan Wu", "Chen Zhu", "Bernard Ghanem", "Gavin Taylor", "Tom Goldstein"], "keywords": ["Graph Neural Networks", "Data Augmentation", "Adversarial Training"], "abstract": "Data augmentation helps neural networks generalize better, but it remains an open question how to effectively augment graph data to enhance the performance of GNNs (Graph Neural Networks). While most existing graph regularizers focus on augmenting graph topological structures by adding/removing edges, we offer a novel direction to augment in the input node feature space for better performance. We propose a simple but effective solution, FLAG (Free Large-scale Adversarial Augmentation on Graphs), which iteratively augments node features with gradient-based adversarial perturbations during training, and boosts performance at test time. Empirically, FLAG can be easily implemented with a dozen lines of code and is flexible enough to function with any GNN backbone, on a wide variety of large-scale datasets, and in both transductive and inductive settings. Without modifying a model's architecture or training setup, FLAG yields a consistent and salient performance boost across both node and graph classification tasks. Using FLAG, we reach state-of-the-art performance on the large-scale ogbg-molpcba, ogbg-ppa, and ogbg-code datasets.  ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kong|flag_adversarial_data_augmentation_for_graph_neural_networks", "one-sentence_summary": "We show that adversarial data augmentation generalizes Graph Neural Networks on large-scale datasets.", "pdf": "/pdf/f154c19f743894f4c4fdaf90a2439109a3374ada.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=Z7dgKUsF6f", "_bibtex": "@misc{\nkong2021flag,\ntitle={{\\{}FLAG{\\}}: Adversarial Data Augmentation for Graph Neural Networks},\nauthor={Kezhi Kong and Guohao Li and Mucong Ding and Zuxuan Wu and Chen Zhu and Bernard Ghanem and Gavin Taylor and Tom Goldstein},\nyear={2021},\nurl={https://openreview.net/forum?id=mj7WsaHYxj}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "mj7WsaHYxj", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper142/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper142/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper142/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper142/Authors|ICLR.cc/2021/Conference/Paper142/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper142/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923874160, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper142/-/Official_Comment"}}}, {"id": "rg0sscLspJC", "original": null, "number": 4, "cdate": 1605180810300, "ddate": null, "tcdate": 1605180810300, "tmdate": 1605599012903, "tddate": null, "forum": "mj7WsaHYxj", "replyto": "T9ubYQwEiKE", "invitation": "ICLR.cc/2021/Conference/Paper142/-/Official_Comment", "content": {"title": "Thanks for your review", "comment": "We thank the reviewer for helpful feedbacks and for noting that FLAG is a general method for GNNs.\n\n\n1.\u201cThis work applies FreeLB adversarial training [1] to graph neural networks.\u201d\n\n\nWe wholeheartedly disagree with the reviewer on this viewpoint. \u201cFree\u201d batch-replay and gradient accumulation are both common techniques on adversarial training. To most exploit the power of adversarial augmentation on graphs, biased perturbation and unbounded attack are also adopted. The effect of biased perturbation is shown in the ablation studies, and unbounded attack helps simplify hyperparameter search greatly. Also as shown in the right part of Table 4, on ogbn-products with GAT as backbone, FLAG wins over PGD, Free, and FreeLB by a decent margin.   \n\n\n2.\u201cThe method improves results by a very small percentage even for cases with low baseline accuracy.\u201d\n\n\nTo clarify, we refer the reviewer to our responses of the Question 2 and Question 3.2 to Reviewer3.\n\n\n3.\"Adversarial training with small perturbations may be considered as a best practice rather than a novel contribution.\u201d\n\n\nWe disagree. Adversarial augmentation is indeed a best practice to improve the robustness of models against attacks out of security purpose, but there are limited amount of works promoting the generalizing effect of adversarial augmentations which is at odds with the wide belief that it harms model clean accuracy. We hope our work can arouse the interest of graph community in this straightforward and scalable approach to generalizing GNNs.\n\n\n4.\u201cDeeperGCN+FLAG and GAT+FLAG are currently ranked 5th and 6th on the OGB leaderboard for node property prediction. DeeperGCN+FLAG is currently ranked 2nd on the OGB leaderboard for graph property prediction.\u201d\n\n\nAlthough the motivation of our work is to reveal the generalizing ability of adversarial augmentations and propose a method that is widely effective, FLAG still reaches high on the leaderboard.\nTo rectify, upon submitting to the OpenReview, on the graph classification task of the Open Graph Benchmark Leaderboard, FLAG reaches\n\n| ogbg-molpcba | 1st |\n \n| ogbg-ppa | 1st | \n\n| ogbg-code | 1st | \n\n| ogbg-molhiv | 2nd | \n\nUpon submitting to the OpenReview, on the node classification task of the Open Graph Benchmark Leaderboard, FLAG reaches\n\n| ogbn-products | 2nd |\n\n| ogbn-proteins | 2nd |\n\n| ogbn-arxiv | 2nd |\n\n| ogbn-mag (heterogeneous) | 3rd |\n\nPlease let us know if you have further questions or comments!"}, "signatures": ["ICLR.cc/2021/Conference/Paper142/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper142/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "FLAG: Adversarial Data Augmentation for Graph Neural Networks", "authorids": ["~Kezhi_Kong1", "~Guohao_Li1", "~Mucong_Ding1", "~Zuxuan_Wu1", "~Chen_Zhu2", "~Bernard_Ghanem1", "~Gavin_Taylor1", "~Tom_Goldstein1"], "authors": ["Kezhi Kong", "Guohao Li", "Mucong Ding", "Zuxuan Wu", "Chen Zhu", "Bernard Ghanem", "Gavin Taylor", "Tom Goldstein"], "keywords": ["Graph Neural Networks", "Data Augmentation", "Adversarial Training"], "abstract": "Data augmentation helps neural networks generalize better, but it remains an open question how to effectively augment graph data to enhance the performance of GNNs (Graph Neural Networks). While most existing graph regularizers focus on augmenting graph topological structures by adding/removing edges, we offer a novel direction to augment in the input node feature space for better performance. We propose a simple but effective solution, FLAG (Free Large-scale Adversarial Augmentation on Graphs), which iteratively augments node features with gradient-based adversarial perturbations during training, and boosts performance at test time. Empirically, FLAG can be easily implemented with a dozen lines of code and is flexible enough to function with any GNN backbone, on a wide variety of large-scale datasets, and in both transductive and inductive settings. Without modifying a model's architecture or training setup, FLAG yields a consistent and salient performance boost across both node and graph classification tasks. Using FLAG, we reach state-of-the-art performance on the large-scale ogbg-molpcba, ogbg-ppa, and ogbg-code datasets.  ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kong|flag_adversarial_data_augmentation_for_graph_neural_networks", "one-sentence_summary": "We show that adversarial data augmentation generalizes Graph Neural Networks on large-scale datasets.", "pdf": "/pdf/f154c19f743894f4c4fdaf90a2439109a3374ada.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=Z7dgKUsF6f", "_bibtex": "@misc{\nkong2021flag,\ntitle={{\\{}FLAG{\\}}: Adversarial Data Augmentation for Graph Neural Networks},\nauthor={Kezhi Kong and Guohao Li and Mucong Ding and Zuxuan Wu and Chen Zhu and Bernard Ghanem and Gavin Taylor and Tom Goldstein},\nyear={2021},\nurl={https://openreview.net/forum?id=mj7WsaHYxj}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "mj7WsaHYxj", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper142/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper142/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper142/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper142/Authors|ICLR.cc/2021/Conference/Paper142/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper142/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923874160, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper142/-/Official_Comment"}}}, {"id": "T9ubYQwEiKE", "original": null, "number": 4, "cdate": 1603908843031, "ddate": null, "tcdate": 1603908843031, "tmdate": 1605529358896, "tddate": null, "forum": "mj7WsaHYxj", "replyto": "mj7WsaHYxj", "invitation": "ICLR.cc/2021/Conference/Paper142/-/Official_Review", "content": {"title": "Applying FreeLB adversarial training to GNNs", "review": "This work applies FreeLB adversarial training [1] to graph neural networks.\n\nStrengths:\nThere are various best practices which are commonly applied in common task framework competitions\nsuch as data augmentation, adversarial training, and ensembles that improve results by fractions \nor a small number of percentage points. This work falls under such methods, and proposes to \nperform FreeLB adversarial training by computing the loss with respect to perturbations.\nThe method is general and may be applied to any GNN for a very small improvement.\n\nWeaknesses:\nThe algorithm (1)[1] is not novel and its application to GNNs is an incremental contribution.\nThe method improves results by a very small percentage even for cases with low baseline accuracy.\nAdversarial training with small perturbations may be considered as a best practice rather than a novel contribution.\nDeeperGCN+FLAG and GAT+FLAG are currently ranked 5th and 6th on the OGB leaderboard for node property prediction.\nDeeperGCN+FLAG is currently ranked 2nd on the OGB leaderboard for graph property prediction.\n\n[1] FreeLB: Enhanced Adversarial Training for Natural Language Understanding, Zhu et al, 2019.\n\nDue to the many relative improvements of FLAG,\nrather than absolute rankings, on the OGB leaderboards, \nI think that the overall contribution is above the acceptance threshold.\n\n1. Node property prediction\na. ogbn-products: +2: DeeperGCN+FLAG, +5: GAT+FLAG, +6: GraphSAGE+FLAG\t\nb. ogbn-proteins: +1: DeeperGCN+FLAG\t\nc. ogbn-arxiv: +1: GAT+FLAG, +8: GraphSAGE+FLAG, +5: DeeperGCN+FLAG, +4: GCN+FLAG, +1: MLP+FLAG\t\nd. ogbn-mag: +1 R-GCN+FLAG\t\n\n2. Graph property prediction\na. ogbg-molhiv: +3: DeeperGCN+FLAG, +1: GIN+virtual node+FLAG, +2: GCN+FLAG, +3: GIN+FLAG\t\nb. ogbg-molpcba: +2: DeeperGCN+VN+FLAG, +2: GIN+virtual node+FLAG, +1: GCN+VN+FLAG, +1: GIN+FLAG, +2: GCN+FLAG\t\nc. ogbg-ppa: +1: DeeperGCN+FLAG, +1: GIN+virtual node+FLAG, +3: GCN+virtual node+FLAG, +1: GIN+FLAG\t\nd. ogbg-code: +2: GCN+virtual node+FLAG, +4: GIN+virtual node+FLAG, +4: GIN+FLAG, +2: GCN+FLAG", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper142/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper142/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "FLAG: Adversarial Data Augmentation for Graph Neural Networks", "authorids": ["~Kezhi_Kong1", "~Guohao_Li1", "~Mucong_Ding1", "~Zuxuan_Wu1", "~Chen_Zhu2", "~Bernard_Ghanem1", "~Gavin_Taylor1", "~Tom_Goldstein1"], "authors": ["Kezhi Kong", "Guohao Li", "Mucong Ding", "Zuxuan Wu", "Chen Zhu", "Bernard Ghanem", "Gavin Taylor", "Tom Goldstein"], "keywords": ["Graph Neural Networks", "Data Augmentation", "Adversarial Training"], "abstract": "Data augmentation helps neural networks generalize better, but it remains an open question how to effectively augment graph data to enhance the performance of GNNs (Graph Neural Networks). While most existing graph regularizers focus on augmenting graph topological structures by adding/removing edges, we offer a novel direction to augment in the input node feature space for better performance. We propose a simple but effective solution, FLAG (Free Large-scale Adversarial Augmentation on Graphs), which iteratively augments node features with gradient-based adversarial perturbations during training, and boosts performance at test time. Empirically, FLAG can be easily implemented with a dozen lines of code and is flexible enough to function with any GNN backbone, on a wide variety of large-scale datasets, and in both transductive and inductive settings. Without modifying a model's architecture or training setup, FLAG yields a consistent and salient performance boost across both node and graph classification tasks. Using FLAG, we reach state-of-the-art performance on the large-scale ogbg-molpcba, ogbg-ppa, and ogbg-code datasets.  ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kong|flag_adversarial_data_augmentation_for_graph_neural_networks", "one-sentence_summary": "We show that adversarial data augmentation generalizes Graph Neural Networks on large-scale datasets.", "pdf": "/pdf/f154c19f743894f4c4fdaf90a2439109a3374ada.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=Z7dgKUsF6f", "_bibtex": "@misc{\nkong2021flag,\ntitle={{\\{}FLAG{\\}}: Adversarial Data Augmentation for Graph Neural Networks},\nauthor={Kezhi Kong and Guohao Li and Mucong Ding and Zuxuan Wu and Chen Zhu and Bernard Ghanem and Gavin Taylor and Tom Goldstein},\nyear={2021},\nurl={https://openreview.net/forum?id=mj7WsaHYxj}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "mj7WsaHYxj", "replyto": "mj7WsaHYxj", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper142/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538149475, "tmdate": 1606915810918, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper142/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper142/-/Official_Review"}}}, {"id": "ove0ZF7rBii", "original": null, "number": 7, "cdate": 1605301711751, "ddate": null, "tcdate": 1605301711751, "tmdate": 1605301711751, "tddate": null, "forum": "mj7WsaHYxj", "replyto": "2wH0E5h_173", "invitation": "ICLR.cc/2021/Conference/Paper142/-/Official_Comment", "content": {"title": "Thanks for your interest in our paper", "comment": "Hello Yuning,\n\nThanks for your interest in our paper. Advanced graph augmentation methods will very promisingly boost the performance of Graph Contrastive Learning. We are glad to see future works that incorporate adversarial data augmentation into the Graph Contrastive Learning pipeline. "}, "signatures": ["ICLR.cc/2021/Conference/Paper142/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper142/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "FLAG: Adversarial Data Augmentation for Graph Neural Networks", "authorids": ["~Kezhi_Kong1", "~Guohao_Li1", "~Mucong_Ding1", "~Zuxuan_Wu1", "~Chen_Zhu2", "~Bernard_Ghanem1", "~Gavin_Taylor1", "~Tom_Goldstein1"], "authors": ["Kezhi Kong", "Guohao Li", "Mucong Ding", "Zuxuan Wu", "Chen Zhu", "Bernard Ghanem", "Gavin Taylor", "Tom Goldstein"], "keywords": ["Graph Neural Networks", "Data Augmentation", "Adversarial Training"], "abstract": "Data augmentation helps neural networks generalize better, but it remains an open question how to effectively augment graph data to enhance the performance of GNNs (Graph Neural Networks). While most existing graph regularizers focus on augmenting graph topological structures by adding/removing edges, we offer a novel direction to augment in the input node feature space for better performance. We propose a simple but effective solution, FLAG (Free Large-scale Adversarial Augmentation on Graphs), which iteratively augments node features with gradient-based adversarial perturbations during training, and boosts performance at test time. Empirically, FLAG can be easily implemented with a dozen lines of code and is flexible enough to function with any GNN backbone, on a wide variety of large-scale datasets, and in both transductive and inductive settings. Without modifying a model's architecture or training setup, FLAG yields a consistent and salient performance boost across both node and graph classification tasks. Using FLAG, we reach state-of-the-art performance on the large-scale ogbg-molpcba, ogbg-ppa, and ogbg-code datasets.  ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kong|flag_adversarial_data_augmentation_for_graph_neural_networks", "one-sentence_summary": "We show that adversarial data augmentation generalizes Graph Neural Networks on large-scale datasets.", "pdf": "/pdf/f154c19f743894f4c4fdaf90a2439109a3374ada.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=Z7dgKUsF6f", "_bibtex": "@misc{\nkong2021flag,\ntitle={{\\{}FLAG{\\}}: Adversarial Data Augmentation for Graph Neural Networks},\nauthor={Kezhi Kong and Guohao Li and Mucong Ding and Zuxuan Wu and Chen Zhu and Bernard Ghanem and Gavin Taylor and Tom Goldstein},\nyear={2021},\nurl={https://openreview.net/forum?id=mj7WsaHYxj}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "mj7WsaHYxj", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper142/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper142/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper142/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper142/Authors|ICLR.cc/2021/Conference/Paper142/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper142/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923874160, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper142/-/Official_Comment"}}}, {"id": "riS8rLyt077", "original": null, "number": 6, "cdate": 1605300601726, "ddate": null, "tcdate": 1605300601726, "tmdate": 1605301319609, "tddate": null, "forum": "mj7WsaHYxj", "replyto": "9690QDAknnr", "invitation": "ICLR.cc/2021/Conference/Paper142/-/Official_Comment", "content": {"title": "Thanks for your review", "comment": "We thank the reviewer for constructive feedbacks and valuable comments, and for acknowledging the merits of our approach. \n\n1.\u201cfeature data augmentation is orthogonal to structure-based data augmentation\u201d\n\nWe agree, and it adds to the generality of our method. Below we show that our feature-based augmentation is compatible with two widely-used structure-based methods.\n\nNeighbor sampling proposed by [1] is one kind of structure-based data augmentation. During training time, each node randomly samples k neighbors to aggregate information from. The other neighbor nodes and connecting edges are dropped. On the ogbn-products dataset with GraphSAGE as baseline, the experimental results are as below:\n\n| method                                                             |   test acc |\n\n| GraphSAGE                                                      | 78.50 \u00b1 0.14 |\n\n| GraphSAGE + Neighbor Sampling               |  78.70 \u00b1 0.36 |\n\n| GraphSAGE + Neighbor Sampling + FLAG  | 79.36 \u00b1 0.57 |\n\nVirtual Node method [3] augments the graph by adding one synthetic node who connects with all the original nodes in the graph. From Table 3, almost all the numbers support the claim that FLAG works with Virtual Node well to further improve accuracy, the only exception is from GCN on ogbg-molhiv, but note that FLAG fails to improve GCN+virtual node because the existence of virtual node already hurts the performance of the vanilla GCN on this dataset. We highlight one representative group of experiments on ogbg-ppa with GIN as baseline here.\n\n|  method                            |  test acc |\n\n| GIN                                    |  68.92 \u00b11.00 |\n\n| GIN+virutal node            | 70.37 \u00b11.07 |\n\n| GIN+virutal node+FLAG | 72.45 \u00b11.14 |\n\nWhat\u2019s more, DropEdge [2] is the simple approach to randomly dropping edges in the graph to promote generalization. However our experimental results show that DropEdge fails to further generalize GAT on ogbn-products in the first place, so we exclude it in the discussion here. We thank the reviewer for this valuable comment. We will cover this aspect in our newer version of the paper.\n\n2.\"That will increase the amount of noise added to the unlabelled nodes, but why is that beneficial?\u201d\n\nIt is natural to think that we don\u2019t want the prediction of a node to be highly impacted by another which is far away from it. So when classifying one node, we want the output of the GNN classifier to have higher invariance with the input features of  the nodes that are further away. To realize this idea, our approach is to inject larger noise on the features of unlabeled nodes to make the loss landscape smoother.   \n\n3.\"Also, what is the relation between unlabelled nodes and their distance to the target nodes? Why does it have anything to do with promoting invariance for further-away nodes?\"\n\nWhen training GNNs for node classification, the most common practice is to compute final objective only with respective to all the labeled nodes. In other words, only the final-layer representations of labeled nodes are fed into the objective function (e.g., cross entropy loss). Our practice of having two different step-sizes is a simplification for our high-level idea (promoting higher invariance for further-away nodes). Labeled nodes are the ones to classify, and the distance between it with itself is 0, so we set smaller step sizes for them; meanwhile the distance between unlabeled node with labeled node is at least 1. For simplicity during implementation we set a universal larger step size for all the unlabeled nodes. We will refine our writing to make it clearer.\n\n4.Reference\n\nWe thank the reviewer for mentioning related literature and we will consider referencing them.\n\nPlease let us know if you have further questions or comments!\n\n[1] Hamilton, Will, Zhitao Ying, and Jure Leskovec. \"Inductive representation learning on large graphs.\" Advances in neural information processing systems. 2017.\n\n[2] Rong, Yu, et al. \"DropEdge: Towards Deep Graph Convolutional Networks on Node Classification.\" International Conference on Learning Representations. 2019.\n\n[3] Gilmer, Justin, et al. \"Neural Message Passing for Quantum Chemistry.\" ICML. 2017."}, "signatures": ["ICLR.cc/2021/Conference/Paper142/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper142/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "FLAG: Adversarial Data Augmentation for Graph Neural Networks", "authorids": ["~Kezhi_Kong1", "~Guohao_Li1", "~Mucong_Ding1", "~Zuxuan_Wu1", "~Chen_Zhu2", "~Bernard_Ghanem1", "~Gavin_Taylor1", "~Tom_Goldstein1"], "authors": ["Kezhi Kong", "Guohao Li", "Mucong Ding", "Zuxuan Wu", "Chen Zhu", "Bernard Ghanem", "Gavin Taylor", "Tom Goldstein"], "keywords": ["Graph Neural Networks", "Data Augmentation", "Adversarial Training"], "abstract": "Data augmentation helps neural networks generalize better, but it remains an open question how to effectively augment graph data to enhance the performance of GNNs (Graph Neural Networks). While most existing graph regularizers focus on augmenting graph topological structures by adding/removing edges, we offer a novel direction to augment in the input node feature space for better performance. We propose a simple but effective solution, FLAG (Free Large-scale Adversarial Augmentation on Graphs), which iteratively augments node features with gradient-based adversarial perturbations during training, and boosts performance at test time. Empirically, FLAG can be easily implemented with a dozen lines of code and is flexible enough to function with any GNN backbone, on a wide variety of large-scale datasets, and in both transductive and inductive settings. Without modifying a model's architecture or training setup, FLAG yields a consistent and salient performance boost across both node and graph classification tasks. Using FLAG, we reach state-of-the-art performance on the large-scale ogbg-molpcba, ogbg-ppa, and ogbg-code datasets.  ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kong|flag_adversarial_data_augmentation_for_graph_neural_networks", "one-sentence_summary": "We show that adversarial data augmentation generalizes Graph Neural Networks on large-scale datasets.", "pdf": "/pdf/f154c19f743894f4c4fdaf90a2439109a3374ada.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=Z7dgKUsF6f", "_bibtex": "@misc{\nkong2021flag,\ntitle={{\\{}FLAG{\\}}: Adversarial Data Augmentation for Graph Neural Networks},\nauthor={Kezhi Kong and Guohao Li and Mucong Ding and Zuxuan Wu and Chen Zhu and Bernard Ghanem and Gavin Taylor and Tom Goldstein},\nyear={2021},\nurl={https://openreview.net/forum?id=mj7WsaHYxj}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "mj7WsaHYxj", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper142/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper142/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper142/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper142/Authors|ICLR.cc/2021/Conference/Paper142/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper142/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923874160, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper142/-/Official_Comment"}}}, {"id": "MOKuHtxhXz-", "original": null, "number": 5, "cdate": 1605219212715, "ddate": null, "tcdate": 1605219212715, "tmdate": 1605219212715, "tddate": null, "forum": "mj7WsaHYxj", "replyto": "AnBRf2GFUqv", "invitation": "ICLR.cc/2021/Conference/Paper142/-/Official_Comment", "content": {"title": "Thanks for your review", "comment": "We thank the reviewer for insightful feedbacks and for endorsing our analysis and ablation studies. \n\n1.\u201cFor Figure 1, could you provide more analysis on the distribution shift besides the accuracy? \u201c\n\nYes. Common graph datasets come with discrete node features (i.e., bag-of-words embeddings) and discrete edge connections (symmetric binary adjacency matrix). In our example, when node features are originally discrete, FGSM (a simple one-step adversarial perturbation crafting method) successfully boosts the performance, however when gaussian noises are introduced, the effect goes from positive to negative. For data drawn from discrete distribution (i.e., categorical distribution), it is hard to define its smoothness with respect to the final loss. Our conjecture is that the sparsity of the dataset matters, where graphical data is intrinsically sparse due to the discrete property.  [1] showed that when only a few of training samples are available to train the model, where the sparsity of data is obvious, adversarial data augmentation is helpful in terms of clean accuracy. We believe the question on how and why distribution shift affects the effects of adversarial data augmentation is interesting and worth more attention. \n\n2.\u201cmore insights on how these approaches used in FLAG.\u201d\n\nFLAG leverages the batch-replay technique proposed by \u201cfree\" to promote efficient adversarial training. Besides that while computing the gradient ascent step for the perturbation, gradients are also accumulated for the model weights. What\u2019 s more biased perturbation and unbounded attack  proposed by us which are targeted at GNNs also help FLAG perform better. While in NLP, the perturbation happens in the embedding space, FLAG injects noise in the input node feature space on the node classification task. All the word embeddings are perturbed with the universal step size, while in FLAG target and training nodes are treated differently. What\u2019 more for the V+L task, the authors added KL loss into the final objective function, which would consume at least twice the GPU memory needed for training, making scalable training more challenging. In FLAG our final objective function always follows the usage of the original baseline, which won\u2019t introduce any extra space overhead and guarantee FLAG\u2019s scalability to larger graphs and deeper nets. Therefore FLAG is a novel adversarial training method for GNNs.  \n\nPlease let us know if you have further questions or comments!\n\n\n\n[1] Tsipras, Dimitris, et al. \"Robustness May Be at Odds with Accuracy.\" International Conference on Learning Representations. 2018."}, "signatures": ["ICLR.cc/2021/Conference/Paper142/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper142/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "FLAG: Adversarial Data Augmentation for Graph Neural Networks", "authorids": ["~Kezhi_Kong1", "~Guohao_Li1", "~Mucong_Ding1", "~Zuxuan_Wu1", "~Chen_Zhu2", "~Bernard_Ghanem1", "~Gavin_Taylor1", "~Tom_Goldstein1"], "authors": ["Kezhi Kong", "Guohao Li", "Mucong Ding", "Zuxuan Wu", "Chen Zhu", "Bernard Ghanem", "Gavin Taylor", "Tom Goldstein"], "keywords": ["Graph Neural Networks", "Data Augmentation", "Adversarial Training"], "abstract": "Data augmentation helps neural networks generalize better, but it remains an open question how to effectively augment graph data to enhance the performance of GNNs (Graph Neural Networks). While most existing graph regularizers focus on augmenting graph topological structures by adding/removing edges, we offer a novel direction to augment in the input node feature space for better performance. We propose a simple but effective solution, FLAG (Free Large-scale Adversarial Augmentation on Graphs), which iteratively augments node features with gradient-based adversarial perturbations during training, and boosts performance at test time. Empirically, FLAG can be easily implemented with a dozen lines of code and is flexible enough to function with any GNN backbone, on a wide variety of large-scale datasets, and in both transductive and inductive settings. Without modifying a model's architecture or training setup, FLAG yields a consistent and salient performance boost across both node and graph classification tasks. Using FLAG, we reach state-of-the-art performance on the large-scale ogbg-molpcba, ogbg-ppa, and ogbg-code datasets.  ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kong|flag_adversarial_data_augmentation_for_graph_neural_networks", "one-sentence_summary": "We show that adversarial data augmentation generalizes Graph Neural Networks on large-scale datasets.", "pdf": "/pdf/f154c19f743894f4c4fdaf90a2439109a3374ada.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=Z7dgKUsF6f", "_bibtex": "@misc{\nkong2021flag,\ntitle={{\\{}FLAG{\\}}: Adversarial Data Augmentation for Graph Neural Networks},\nauthor={Kezhi Kong and Guohao Li and Mucong Ding and Zuxuan Wu and Chen Zhu and Bernard Ghanem and Gavin Taylor and Tom Goldstein},\nyear={2021},\nurl={https://openreview.net/forum?id=mj7WsaHYxj}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "mj7WsaHYxj", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper142/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper142/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper142/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper142/Authors|ICLR.cc/2021/Conference/Paper142/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper142/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923874160, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper142/-/Official_Comment"}}}, {"id": "2wH0E5h_173", "original": null, "number": 1, "cdate": 1605035639272, "ddate": null, "tcdate": 1605035639272, "tmdate": 1605036289926, "tddate": null, "forum": "mj7WsaHYxj", "replyto": "mj7WsaHYxj", "invitation": "ICLR.cc/2021/Conference/Paper142/-/Public_Comment", "content": {"title": "Interesting perspective and related work", "comment": "We would like to draw your attention that we have related work showing augmentation in graphs can boost robustness while it is incorporated in the contrastive learning framework. We hope to have further discussions and references with you.\n\nGraph Contrastive Learning with Augmentations, NeurIPS 2020.\nhttps://arxiv.org/abs/2010.13902"}, "signatures": ["~Yuning_You1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "~Yuning_You1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "FLAG: Adversarial Data Augmentation for Graph Neural Networks", "authorids": ["~Kezhi_Kong1", "~Guohao_Li1", "~Mucong_Ding1", "~Zuxuan_Wu1", "~Chen_Zhu2", "~Bernard_Ghanem1", "~Gavin_Taylor1", "~Tom_Goldstein1"], "authors": ["Kezhi Kong", "Guohao Li", "Mucong Ding", "Zuxuan Wu", "Chen Zhu", "Bernard Ghanem", "Gavin Taylor", "Tom Goldstein"], "keywords": ["Graph Neural Networks", "Data Augmentation", "Adversarial Training"], "abstract": "Data augmentation helps neural networks generalize better, but it remains an open question how to effectively augment graph data to enhance the performance of GNNs (Graph Neural Networks). While most existing graph regularizers focus on augmenting graph topological structures by adding/removing edges, we offer a novel direction to augment in the input node feature space for better performance. We propose a simple but effective solution, FLAG (Free Large-scale Adversarial Augmentation on Graphs), which iteratively augments node features with gradient-based adversarial perturbations during training, and boosts performance at test time. Empirically, FLAG can be easily implemented with a dozen lines of code and is flexible enough to function with any GNN backbone, on a wide variety of large-scale datasets, and in both transductive and inductive settings. Without modifying a model's architecture or training setup, FLAG yields a consistent and salient performance boost across both node and graph classification tasks. Using FLAG, we reach state-of-the-art performance on the large-scale ogbg-molpcba, ogbg-ppa, and ogbg-code datasets.  ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kong|flag_adversarial_data_augmentation_for_graph_neural_networks", "one-sentence_summary": "We show that adversarial data augmentation generalizes Graph Neural Networks on large-scale datasets.", "pdf": "/pdf/f154c19f743894f4c4fdaf90a2439109a3374ada.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=Z7dgKUsF6f", "_bibtex": "@misc{\nkong2021flag,\ntitle={{\\{}FLAG{\\}}: Adversarial Data Augmentation for Graph Neural Networks},\nauthor={Kezhi Kong and Guohao Li and Mucong Ding and Zuxuan Wu and Chen Zhu and Bernard Ghanem and Gavin Taylor and Tom Goldstein},\nyear={2021},\nurl={https://openreview.net/forum?id=mj7WsaHYxj}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "mj7WsaHYxj", "readers": {"description": "User groups that will be able to read this comment.", "values": ["everyone"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed."}}, "expdate": 1605630600000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper142/Authors", "ICLR.cc/2021/Conference/Paper142/Reviewers", "ICLR.cc/2021/Conference/Paper142/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1605024985235, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper142/-/Public_Comment"}}}, {"id": "AnBRf2GFUqv", "original": null, "number": 3, "cdate": 1603864051835, "ddate": null, "tcdate": 1603864051835, "tmdate": 1605024754462, "tddate": null, "forum": "mj7WsaHYxj", "replyto": "mj7WsaHYxj", "invitation": "ICLR.cc/2021/Conference/Paper142/-/Official_Review", "content": {"title": "Official Review of Paper142", "review": "This paper presented an adversarial augmentation technique for graph neural networks. Particularly, it proposed to inject perturbations to the embedding space of the feature space with the \"free\" strategy. The results on three datasets show the effect of the proposed method. \n\nThis is the first known work on applying this technique to graph NN. I really like the analysis of why this method works on graph data, which shows that the data distribution shift is important. The ablation studies are also strong, especially on the bias of perturbations, and going deep, etc. For Figure 1, could you provide more analysis on the distribution shift besides the accuracy? \n\nThere are a few concerns about this paper. Similar techniques have been applied to other domains, such as NLP and V+L [1,2]. The \"free\" strategy is also not new [3]. it is better for the author to provide more insights on how these approaches used in FLAG.\n\n1. FreeLB: Enhanced Adversarial Training for Language Understanding, ICLR 2020\n2. Large-Scale Adversarial Training for Vision-and-Language Representation Learning, Neurips 2020\n3. Adversarial Training for Free, Neurips 2019", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper142/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper142/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "FLAG: Adversarial Data Augmentation for Graph Neural Networks", "authorids": ["~Kezhi_Kong1", "~Guohao_Li1", "~Mucong_Ding1", "~Zuxuan_Wu1", "~Chen_Zhu2", "~Bernard_Ghanem1", "~Gavin_Taylor1", "~Tom_Goldstein1"], "authors": ["Kezhi Kong", "Guohao Li", "Mucong Ding", "Zuxuan Wu", "Chen Zhu", "Bernard Ghanem", "Gavin Taylor", "Tom Goldstein"], "keywords": ["Graph Neural Networks", "Data Augmentation", "Adversarial Training"], "abstract": "Data augmentation helps neural networks generalize better, but it remains an open question how to effectively augment graph data to enhance the performance of GNNs (Graph Neural Networks). While most existing graph regularizers focus on augmenting graph topological structures by adding/removing edges, we offer a novel direction to augment in the input node feature space for better performance. We propose a simple but effective solution, FLAG (Free Large-scale Adversarial Augmentation on Graphs), which iteratively augments node features with gradient-based adversarial perturbations during training, and boosts performance at test time. Empirically, FLAG can be easily implemented with a dozen lines of code and is flexible enough to function with any GNN backbone, on a wide variety of large-scale datasets, and in both transductive and inductive settings. Without modifying a model's architecture or training setup, FLAG yields a consistent and salient performance boost across both node and graph classification tasks. Using FLAG, we reach state-of-the-art performance on the large-scale ogbg-molpcba, ogbg-ppa, and ogbg-code datasets.  ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kong|flag_adversarial_data_augmentation_for_graph_neural_networks", "one-sentence_summary": "We show that adversarial data augmentation generalizes Graph Neural Networks on large-scale datasets.", "pdf": "/pdf/f154c19f743894f4c4fdaf90a2439109a3374ada.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=Z7dgKUsF6f", "_bibtex": "@misc{\nkong2021flag,\ntitle={{\\{}FLAG{\\}}: Adversarial Data Augmentation for Graph Neural Networks},\nauthor={Kezhi Kong and Guohao Li and Mucong Ding and Zuxuan Wu and Chen Zhu and Bernard Ghanem and Gavin Taylor and Tom Goldstein},\nyear={2021},\nurl={https://openreview.net/forum?id=mj7WsaHYxj}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "mj7WsaHYxj", "replyto": "mj7WsaHYxj", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper142/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538149475, "tmdate": 1606915810918, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper142/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper142/-/Official_Review"}}}], "count": 12}