{"notes": [{"id": "rJeMUYhzDV", "original": null, "number": 23, "cdate": 1552234826064, "ddate": null, "tcdate": 1552234826064, "tmdate": 1552235057458, "tddate": null, "forum": "H1MzKs05F7", "replyto": "SJg52-sfDN", "invitation": "ICLR.cc/2019/Conference/-/Paper426/Official_Comment", "content": {"title": "Dimensional scaling is accounted for", "comment": "Thank you for your comment. Perturbation scaling has been taking into account in our analysis. See paragraph \"Calibrating the threshold \\epsilon to the attack-norm.\" in Sec.2 and see Appendix D.\n\nThe idea is essentially equivalent to what you explain: keep the signal to noise ratio (i.e. perturbation norm over image norm) constant across dimensions. In l-infinity norm, this leads to a constant perturbation threshold \\epsilon_\\infty; in l2-norm, this leads to a threshold epsilon_2 proportional to \\sqrt d; and in lp-norm proportional to d^{1/p}. That is why the x-axis of Figs 1a-c are scaled by d^{1/p}.\n\nNote that our main theorem shows that adversarial damage increases like sqrt d, but only after adjusting the perturbation threshold. \nMore precisely, it shows that |dL(x)|_q scales like d^{1/q - 1/2}. So, with perturbation threshold epsilon_p of lp-attacks scaling like d^{1/p}, we get\n\n\tadv. dam. := max_{|\\delta|_p \\leq \\epsilon_p} L(x+\\delta) - L(x)\n\t\t  \\approx \\epsilon_p |dL(x)|_q\n\t\t  = \\sqrt d,\n\nwhere 1/p + 1/q = 1. \n\n(Note that using p=infty and q=1 has the \"advantage\" to yield a gradient norm |dL|_1 that is proportional to adversarial damage.)"}, "signatures": ["ICLR.cc/2019/Conference/Paper426/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper426/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper426/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial Vulnerability of Neural Networks Increases with Input Dimension", "abstract": "Over the past four years, neural networks have been proven vulnerable to adversarial images: targeted but imperceptible image perturbations lead to drastically different predictions. We show that adversarial vulnerability increases with the gradients of the training objective when viewed as a function of the inputs. For most current network architectures, we prove that the L1-norm of these gradients grows as the square root of the input size. These nets therefore become increasingly vulnerable with growing image size. Our proofs rely on the network\u2019s weight distribution at initialization, but extensive experiments confirm that our conclusions still hold after usual training.", "keywords": ["adversarial vulnerability", "neural networks", "gradients", "FGSM", "adversarial data-augmentation", "gradient regularization", "robust optimization"], "authorids": ["cjsimon@tuebingen.mpg.de", "yol@fb.com", "leon@bottou.org", "bs@tuebingen.mpg.de", "dlp@fb.com"], "authors": ["Carl-Johann Simon-Gabriel", "Yann Ollivier", "L\u00e9on Bottou", "Bernhard Sch\u00f6lkopf", "David Lopez-Paz"], "TL;DR": "Neural nets have large gradients by design; that makes them adversarially vulnerable.", "pdf": "/pdf/ee82a3413f64426b3c9095ba85a6738aeeecfe6c.pdf", "paperhash": "simongabriel|adversarial_vulnerability_of_neural_networks_increases_with_input_dimension", "_bibtex": "@misc{\nsimon-gabriel2019adversarial,\ntitle={Adversarial Vulnerability of Neural Networks Increases with Input Dimension},\nauthor={Carl-Johann Simon-Gabriel and Yann Ollivier and L\u00e9on Bottou and Bernhard Sch\u00f6lkopf and David Lopez-Paz},\nyear={2019},\nurl={https://openreview.net/forum?id=H1MzKs05F7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper426/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621607730, "tddate": null, "super": null, "final": null, "reply": {"forum": "H1MzKs05F7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper426/Authors", "ICLR.cc/2019/Conference/Paper426/Reviewers", "ICLR.cc/2019/Conference/Paper426/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper426/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper426/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper426/Authors|ICLR.cc/2019/Conference/Paper426/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper426/Reviewers", "ICLR.cc/2019/Conference/Paper426/Authors", "ICLR.cc/2019/Conference/Paper426/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621607730}}}, {"id": "SJg52-sfDN", "original": null, "number": 1, "cdate": 1552228786039, "ddate": null, "tcdate": 1552228786039, "tmdate": 1552228786039, "tddate": null, "forum": "H1MzKs05F7", "replyto": "H1MzKs05F7", "invitation": "ICLR.cc/2019/Conference/-/Paper426/Public_Comment", "content": {"comment": "When comparing images at different resolution, it is important to scale the units of measurement.  This is common practice in mathematics and physical science, but is often not done in computer science.\n\nFor example, consider a square of side length 1/2 centered in a 1 by 1 box.  The area of the square is clearly 1/4.  At resolution 32x32, it makes sense to multiply lengths by h = 1/32 and volumes by h^2.   Then if we consider resolution 128X128, we keep the same units.   \n\nDimensional scaling will affect the conclusions of this article about the size of perturbations.  For example, consider a small physical perturbation on a patch of size \\delta x \\delta with average size \\epsilon.    In scaled units, the norm of the vector is nearly same, at all resolutions.   On the other hand, in unscaled units, it appears to be a larger vector at higher resolution.  But the size of the original image vector is also larger, by the same proportion.\n\nSpecifically, the image had 4 times as many 1s at higher resolution, and the perturbation has 4 times as many components of size \\epsilon.\nSo the relative size of the perturbation didn\u2019t change:\n\nMain point: since the norm of the images as well as the norm of the perturbations grows with the dimensions, when we measure the size of the perturbation in non-dimensional units, it does not change.  \n\nSee https://en.wikipedia.org/wiki/Dimensional_analysis for more information.", "title": "About dimensional scaling"}, "signatures": ["~Adam_M_Oberman1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Adam_M_Oberman1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial Vulnerability of Neural Networks Increases with Input Dimension", "abstract": "Over the past four years, neural networks have been proven vulnerable to adversarial images: targeted but imperceptible image perturbations lead to drastically different predictions. We show that adversarial vulnerability increases with the gradients of the training objective when viewed as a function of the inputs. For most current network architectures, we prove that the L1-norm of these gradients grows as the square root of the input size. These nets therefore become increasingly vulnerable with growing image size. Our proofs rely on the network\u2019s weight distribution at initialization, but extensive experiments confirm that our conclusions still hold after usual training.", "keywords": ["adversarial vulnerability", "neural networks", "gradients", "FGSM", "adversarial data-augmentation", "gradient regularization", "robust optimization"], "authorids": ["cjsimon@tuebingen.mpg.de", "yol@fb.com", "leon@bottou.org", "bs@tuebingen.mpg.de", "dlp@fb.com"], "authors": ["Carl-Johann Simon-Gabriel", "Yann Ollivier", "L\u00e9on Bottou", "Bernhard Sch\u00f6lkopf", "David Lopez-Paz"], "TL;DR": "Neural nets have large gradients by design; that makes them adversarially vulnerable.", "pdf": "/pdf/ee82a3413f64426b3c9095ba85a6738aeeecfe6c.pdf", "paperhash": "simongabriel|adversarial_vulnerability_of_neural_networks_increases_with_input_dimension", "_bibtex": "@misc{\nsimon-gabriel2019adversarial,\ntitle={Adversarial Vulnerability of Neural Networks Increases with Input Dimension},\nauthor={Carl-Johann Simon-Gabriel and Yann Ollivier and L\u00e9on Bottou and Bernhard Sch\u00f6lkopf and David Lopez-Paz},\nyear={2019},\nurl={https://openreview.net/forum?id=H1MzKs05F7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper426/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311843008, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "H1MzKs05F7", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper426/Authors", "ICLR.cc/2019/Conference/Paper426/Reviewers", "ICLR.cc/2019/Conference/Paper426/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper426/Authors", "ICLR.cc/2019/Conference/Paper426/Reviewers", "ICLR.cc/2019/Conference/Paper426/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311843008}}}, {"id": "H1MzKs05F7", "original": "rJglN1r5t7", "number": 426, "cdate": 1538087802179, "ddate": null, "tcdate": 1538087802179, "tmdate": 1545355439186, "tddate": null, "forum": "H1MzKs05F7", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Adversarial Vulnerability of Neural Networks Increases with Input Dimension", "abstract": "Over the past four years, neural networks have been proven vulnerable to adversarial images: targeted but imperceptible image perturbations lead to drastically different predictions. We show that adversarial vulnerability increases with the gradients of the training objective when viewed as a function of the inputs. For most current network architectures, we prove that the L1-norm of these gradients grows as the square root of the input size. These nets therefore become increasingly vulnerable with growing image size. Our proofs rely on the network\u2019s weight distribution at initialization, but extensive experiments confirm that our conclusions still hold after usual training.", "keywords": ["adversarial vulnerability", "neural networks", "gradients", "FGSM", "adversarial data-augmentation", "gradient regularization", "robust optimization"], "authorids": ["cjsimon@tuebingen.mpg.de", "yol@fb.com", "leon@bottou.org", "bs@tuebingen.mpg.de", "dlp@fb.com"], "authors": ["Carl-Johann Simon-Gabriel", "Yann Ollivier", "L\u00e9on Bottou", "Bernhard Sch\u00f6lkopf", "David Lopez-Paz"], "TL;DR": "Neural nets have large gradients by design; that makes them adversarially vulnerable.", "pdf": "/pdf/ee82a3413f64426b3c9095ba85a6738aeeecfe6c.pdf", "paperhash": "simongabriel|adversarial_vulnerability_of_neural_networks_increases_with_input_dimension", "_bibtex": "@misc{\nsimon-gabriel2019adversarial,\ntitle={Adversarial Vulnerability of Neural Networks Increases with Input Dimension},\nauthor={Carl-Johann Simon-Gabriel and Yann Ollivier and L\u00e9on Bottou and Bernhard Sch\u00f6lkopf and David Lopez-Paz},\nyear={2019},\nurl={https://openreview.net/forum?id=H1MzKs05F7},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 27, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "HkgzA3qze4", "original": null, "number": 1, "cdate": 1544887497611, "ddate": null, "tcdate": 1544887497611, "tmdate": 1545354478120, "tddate": null, "forum": "H1MzKs05F7", "replyto": "H1MzKs05F7", "invitation": "ICLR.cc/2019/Conference/-/Paper426/Meta_Review", "content": {"metareview": "This paper suggests that adversarial vulnerability scales with the dimension of the input of neural networks, and support this hypothesis theoretically and experimentally. \n\nThe work is well-written, and all of the reviewers appreciated the easy-to-read and clear nature of the theoretical results, including the assumptions and limitations. (The AC did not consider the criticisms raised by Reviewer 3 justified. The norm-bound perturbations considered here are a sufficiently interesting unsolved problem in the community and a clear prerequisite to solving the broader network robustness problem.) \n\nHowever, many of the reviewers also agreed that the theoretical assumptions - and, in particular, the random initialization of the weights - greatly oversimplify the problem. Reviewers point out that the lack of data dependence and only considering the norm of the gradient considerably limit the significance of the corresponding theoretical results, and also does not properly address the issue of gradient masking. ", "confidence": "5: The area chair is absolutely certain", "recommendation": "Reject", "title": "The paper has intriguing ideas but requires more work"}, "signatures": ["ICLR.cc/2019/Conference/Paper426/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper426/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial Vulnerability of Neural Networks Increases with Input Dimension", "abstract": "Over the past four years, neural networks have been proven vulnerable to adversarial images: targeted but imperceptible image perturbations lead to drastically different predictions. We show that adversarial vulnerability increases with the gradients of the training objective when viewed as a function of the inputs. For most current network architectures, we prove that the L1-norm of these gradients grows as the square root of the input size. These nets therefore become increasingly vulnerable with growing image size. Our proofs rely on the network\u2019s weight distribution at initialization, but extensive experiments confirm that our conclusions still hold after usual training.", "keywords": ["adversarial vulnerability", "neural networks", "gradients", "FGSM", "adversarial data-augmentation", "gradient regularization", "robust optimization"], "authorids": ["cjsimon@tuebingen.mpg.de", "yol@fb.com", "leon@bottou.org", "bs@tuebingen.mpg.de", "dlp@fb.com"], "authors": ["Carl-Johann Simon-Gabriel", "Yann Ollivier", "L\u00e9on Bottou", "Bernhard Sch\u00f6lkopf", "David Lopez-Paz"], "TL;DR": "Neural nets have large gradients by design; that makes them adversarially vulnerable.", "pdf": "/pdf/ee82a3413f64426b3c9095ba85a6738aeeecfe6c.pdf", "paperhash": "simongabriel|adversarial_vulnerability_of_neural_networks_increases_with_input_dimension", "_bibtex": "@misc{\nsimon-gabriel2019adversarial,\ntitle={Adversarial Vulnerability of Neural Networks Increases with Input Dimension},\nauthor={Carl-Johann Simon-Gabriel and Yann Ollivier and L\u00e9on Bottou and Bernhard Sch\u00f6lkopf and David Lopez-Paz},\nyear={2019},\nurl={https://openreview.net/forum?id=H1MzKs05F7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper426/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353221804, "tddate": null, "super": null, "final": null, "reply": {"forum": "H1MzKs05F7", "replyto": "H1MzKs05F7", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper426/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper426/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper426/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353221804}}}, {"id": "SJg9Wd9j1N", "original": null, "number": 22, "cdate": 1544427521625, "ddate": null, "tcdate": 1544427521625, "tmdate": 1544427521625, "tddate": null, "forum": "H1MzKs05F7", "replyto": "H1MzKs05F7", "invitation": "ICLR.cc/2019/Conference/-/Paper426/Official_Comment", "content": {"title": "Overall summary and clarification of our paper's current primary goal", "comment": "We thank all reviewers for their comments and implication in the review process and seize this opportunity for a final recap on our paper and its contributions.\n\nFirst, our work re-emphasises the strong link between small adversarial perturbations and gradient norms. Although this link was known, many still believe that simple gradients cannot explain f. ex. the vulnerability to iterative methods. Our experiment on Fig.2 with PGD trained networks clearly suggests otherwise.\n\nSecond, and more importantly, we are first to point out the prior vulnerability of classical networks, its exact scaling with the input dimension, and its relative independence of the architecture and dataset. We are also first to precisely measure and confirm this dimension-dependence on trained networks, using usual training and real-world, classical datasets. \n\nOur results however do not precisely study *how* prior-vulnerability acts on post usual (robust) training vulnerability. Nor do they prove that no robust training method will ever be able to robustify our current networks. It is not their current ambition. We will clarify this in the final version and rephrase possible over-statements adequately. However, the current results do *suggest* that architectures with less vulnerable priors might be easier to robustify; so much so, that we are now mainly being reproached not to have dug further in these directions. \n\nWe agree that these are thrilling questions. But since the paper is already long, and its afore-mentioned contributions already novel and non-trivial, we see the present controversy about its implications rather as a pro than a cons for publication. The debate attests that our results do indeed yield non-trivial new questions to be answered.\n\nWe therefore call upon all reviewers and the AC, for the final decision, to focus less on what hasn't been done, and more on our actual contributions and the new perspectives that they open for future research."}, "signatures": ["ICLR.cc/2019/Conference/Paper426/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper426/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper426/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial Vulnerability of Neural Networks Increases with Input Dimension", "abstract": "Over the past four years, neural networks have been proven vulnerable to adversarial images: targeted but imperceptible image perturbations lead to drastically different predictions. We show that adversarial vulnerability increases with the gradients of the training objective when viewed as a function of the inputs. For most current network architectures, we prove that the L1-norm of these gradients grows as the square root of the input size. These nets therefore become increasingly vulnerable with growing image size. Our proofs rely on the network\u2019s weight distribution at initialization, but extensive experiments confirm that our conclusions still hold after usual training.", "keywords": ["adversarial vulnerability", "neural networks", "gradients", "FGSM", "adversarial data-augmentation", "gradient regularization", "robust optimization"], "authorids": ["cjsimon@tuebingen.mpg.de", "yol@fb.com", "leon@bottou.org", "bs@tuebingen.mpg.de", "dlp@fb.com"], "authors": ["Carl-Johann Simon-Gabriel", "Yann Ollivier", "L\u00e9on Bottou", "Bernhard Sch\u00f6lkopf", "David Lopez-Paz"], "TL;DR": "Neural nets have large gradients by design; that makes them adversarially vulnerable.", "pdf": "/pdf/ee82a3413f64426b3c9095ba85a6738aeeecfe6c.pdf", "paperhash": "simongabriel|adversarial_vulnerability_of_neural_networks_increases_with_input_dimension", "_bibtex": "@misc{\nsimon-gabriel2019adversarial,\ntitle={Adversarial Vulnerability of Neural Networks Increases with Input Dimension},\nauthor={Carl-Johann Simon-Gabriel and Yann Ollivier and L\u00e9on Bottou and Bernhard Sch\u00f6lkopf and David Lopez-Paz},\nyear={2019},\nurl={https://openreview.net/forum?id=H1MzKs05F7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper426/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621607730, "tddate": null, "super": null, "final": null, "reply": {"forum": "H1MzKs05F7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper426/Authors", "ICLR.cc/2019/Conference/Paper426/Reviewers", "ICLR.cc/2019/Conference/Paper426/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper426/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper426/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper426/Authors|ICLR.cc/2019/Conference/Paper426/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper426/Reviewers", "ICLR.cc/2019/Conference/Paper426/Authors", "ICLR.cc/2019/Conference/Paper426/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621607730}}}, {"id": "rkxEtwcsyN", "original": null, "number": 21, "cdate": 1544427387934, "ddate": null, "tcdate": 1544427387934, "tmdate": 1544427387934, "tddate": null, "forum": "H1MzKs05F7", "replyto": "Hyl2CLpMyV", "invitation": "ICLR.cc/2019/Conference/-/Paper426/Official_Comment", "content": {"title": "Thanks for your comment + clarification of our primary goal", "comment": "We thank your for your comment and accurate analysis, how some formulations may have mislead the interpretation of our paper. We will reformulate any possible over-statements adequately for the final version.\n\nWe agree that the new experiments suggest that the prior- and post-training vulnerability are related. However, let us stress that proving this relation was not the ambition of this paper. Our primary goal is simply to point out that a/ the prior vulnerability exists, scales like \\sqrt(d) and is independent of the net-architecture and the dataset; and b/ that the same dimension dependence can be observed after usual training. Our title does not lie on these observations: our empirical experiments do support that vulnerability increases with usual training, which *is* the standard/common/default training method. But, if it helps, we are willing to change our title to \"*Prior* adv. vuln. of n.n. increases with input dim.\".\n\nMore generally, since the paper is already long, and a/&b/ already novel and non-trivial results/observations, we see the present controversy about their implications rather as pro than a cons for publication. The debate attests that our results indeed yield non-trivial new questions for the community to answer.\n\nWe will be happy to include this clarification in our final version."}, "signatures": ["ICLR.cc/2019/Conference/Paper426/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper426/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper426/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial Vulnerability of Neural Networks Increases with Input Dimension", "abstract": "Over the past four years, neural networks have been proven vulnerable to adversarial images: targeted but imperceptible image perturbations lead to drastically different predictions. We show that adversarial vulnerability increases with the gradients of the training objective when viewed as a function of the inputs. For most current network architectures, we prove that the L1-norm of these gradients grows as the square root of the input size. These nets therefore become increasingly vulnerable with growing image size. Our proofs rely on the network\u2019s weight distribution at initialization, but extensive experiments confirm that our conclusions still hold after usual training.", "keywords": ["adversarial vulnerability", "neural networks", "gradients", "FGSM", "adversarial data-augmentation", "gradient regularization", "robust optimization"], "authorids": ["cjsimon@tuebingen.mpg.de", "yol@fb.com", "leon@bottou.org", "bs@tuebingen.mpg.de", "dlp@fb.com"], "authors": ["Carl-Johann Simon-Gabriel", "Yann Ollivier", "L\u00e9on Bottou", "Bernhard Sch\u00f6lkopf", "David Lopez-Paz"], "TL;DR": "Neural nets have large gradients by design; that makes them adversarially vulnerable.", "pdf": "/pdf/ee82a3413f64426b3c9095ba85a6738aeeecfe6c.pdf", "paperhash": "simongabriel|adversarial_vulnerability_of_neural_networks_increases_with_input_dimension", "_bibtex": "@misc{\nsimon-gabriel2019adversarial,\ntitle={Adversarial Vulnerability of Neural Networks Increases with Input Dimension},\nauthor={Carl-Johann Simon-Gabriel and Yann Ollivier and L\u00e9on Bottou and Bernhard Sch\u00f6lkopf and David Lopez-Paz},\nyear={2019},\nurl={https://openreview.net/forum?id=H1MzKs05F7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper426/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621607730, "tddate": null, "super": null, "final": null, "reply": {"forum": "H1MzKs05F7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper426/Authors", "ICLR.cc/2019/Conference/Paper426/Reviewers", "ICLR.cc/2019/Conference/Paper426/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper426/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper426/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper426/Authors|ICLR.cc/2019/Conference/Paper426/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper426/Reviewers", "ICLR.cc/2019/Conference/Paper426/Authors", "ICLR.cc/2019/Conference/Paper426/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621607730}}}, {"id": "r1lEwQ1XJV", "original": null, "number": 20, "cdate": 1543856987977, "ddate": null, "tcdate": 1543856987977, "tmdate": 1543856987977, "tddate": null, "forum": "H1MzKs05F7", "replyto": "Hyl2CLpMyV", "invitation": "ICLR.cc/2019/Conference/-/Paper426/Official_Comment", "content": {"title": "Further discussion", "comment": "I second that the dependence on dimension is over-emphasized and the current title is misleading. Based on the experiments in other work, it still seems like the effect of dimension vanishes for adversarially trained models. \n\n\"In fact I think that plotting gradient norms over training is _crucial_ to support the argument that the situation at initialization is predictive of the situation at convergence.\" \n\nWhile I find Figure 4 interesting, I'm still not convinced of the argument. To really tie initialization to final performance the authors should show what happens as they change the variance of the weight initialization, if you purposely initialize poorly/better do you get even worse/better sensitivity at test time? Finally, showing the interaction with adversarial training or Gaussian data augmentation is crucial if the authors wish to claim that better initialization strategies can improve upon adversarial training. "}, "signatures": ["ICLR.cc/2019/Conference/Paper426/AnonReviewer3"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper426/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper426/AnonReviewer3", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial Vulnerability of Neural Networks Increases with Input Dimension", "abstract": "Over the past four years, neural networks have been proven vulnerable to adversarial images: targeted but imperceptible image perturbations lead to drastically different predictions. We show that adversarial vulnerability increases with the gradients of the training objective when viewed as a function of the inputs. For most current network architectures, we prove that the L1-norm of these gradients grows as the square root of the input size. These nets therefore become increasingly vulnerable with growing image size. Our proofs rely on the network\u2019s weight distribution at initialization, but extensive experiments confirm that our conclusions still hold after usual training.", "keywords": ["adversarial vulnerability", "neural networks", "gradients", "FGSM", "adversarial data-augmentation", "gradient regularization", "robust optimization"], "authorids": ["cjsimon@tuebingen.mpg.de", "yol@fb.com", "leon@bottou.org", "bs@tuebingen.mpg.de", "dlp@fb.com"], "authors": ["Carl-Johann Simon-Gabriel", "Yann Ollivier", "L\u00e9on Bottou", "Bernhard Sch\u00f6lkopf", "David Lopez-Paz"], "TL;DR": "Neural nets have large gradients by design; that makes them adversarially vulnerable.", "pdf": "/pdf/ee82a3413f64426b3c9095ba85a6738aeeecfe6c.pdf", "paperhash": "simongabriel|adversarial_vulnerability_of_neural_networks_increases_with_input_dimension", "_bibtex": "@misc{\nsimon-gabriel2019adversarial,\ntitle={Adversarial Vulnerability of Neural Networks Increases with Input Dimension},\nauthor={Carl-Johann Simon-Gabriel and Yann Ollivier and L\u00e9on Bottou and Bernhard Sch\u00f6lkopf and David Lopez-Paz},\nyear={2019},\nurl={https://openreview.net/forum?id=H1MzKs05F7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper426/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621607730, "tddate": null, "super": null, "final": null, "reply": {"forum": "H1MzKs05F7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper426/Authors", "ICLR.cc/2019/Conference/Paper426/Reviewers", "ICLR.cc/2019/Conference/Paper426/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper426/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper426/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper426/Authors|ICLR.cc/2019/Conference/Paper426/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper426/Reviewers", "ICLR.cc/2019/Conference/Paper426/Authors", "ICLR.cc/2019/Conference/Paper426/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621607730}}}, {"id": "Hyl2CLpMyV", "original": null, "number": 19, "cdate": 1543849683895, "ddate": null, "tcdate": 1543849683895, "tmdate": 1543849683895, "tddate": null, "forum": "H1MzKs05F7", "replyto": "S1eK78EXR7", "invitation": "ICLR.cc/2019/Conference/-/Paper426/Official_Comment", "content": {"title": "Reviewer response", "comment": "I appreciate the authors engaging in the discussion and performing additional experiments. I do think that the additional experiments strengthen the paper. In fact I think that plotting gradient norms over training is _crucial_ to support the argument that the situation at initialization is predictive of the situation at convergence.\n\nMoreover, I think that during the discussion here, the authors raised interesting points that are more nuanced than the discussion presented in the paper. I would encourage them to continue working on the paper incorporating the new experiments and discussion (this is independent of the final ICLR decision).\n\nFinally, I personally found the emphasis on connecting vulnerability and dimensionality over-emphasized, in the title, the abstract, and the main narrative. I believe that the most interesting contribution of the paper is understanding the impact of initialization on the vulnerability of the resulting classifier. In my opinion, a title \"The impact of initialization on adversarial vulnerability\" (and a narrative focusing more on this aspect) would have resulted in a very different discussion here, as well as a different perception of the paper. But again, this is my personal opinion and has nothing to do with whether I think the paper should be accepted or not."}, "signatures": ["ICLR.cc/2019/Conference/Paper426/AnonReviewer4"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper426/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper426/AnonReviewer4", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial Vulnerability of Neural Networks Increases with Input Dimension", "abstract": "Over the past four years, neural networks have been proven vulnerable to adversarial images: targeted but imperceptible image perturbations lead to drastically different predictions. We show that adversarial vulnerability increases with the gradients of the training objective when viewed as a function of the inputs. For most current network architectures, we prove that the L1-norm of these gradients grows as the square root of the input size. These nets therefore become increasingly vulnerable with growing image size. Our proofs rely on the network\u2019s weight distribution at initialization, but extensive experiments confirm that our conclusions still hold after usual training.", "keywords": ["adversarial vulnerability", "neural networks", "gradients", "FGSM", "adversarial data-augmentation", "gradient regularization", "robust optimization"], "authorids": ["cjsimon@tuebingen.mpg.de", "yol@fb.com", "leon@bottou.org", "bs@tuebingen.mpg.de", "dlp@fb.com"], "authors": ["Carl-Johann Simon-Gabriel", "Yann Ollivier", "L\u00e9on Bottou", "Bernhard Sch\u00f6lkopf", "David Lopez-Paz"], "TL;DR": "Neural nets have large gradients by design; that makes them adversarially vulnerable.", "pdf": "/pdf/ee82a3413f64426b3c9095ba85a6738aeeecfe6c.pdf", "paperhash": "simongabriel|adversarial_vulnerability_of_neural_networks_increases_with_input_dimension", "_bibtex": "@misc{\nsimon-gabriel2019adversarial,\ntitle={Adversarial Vulnerability of Neural Networks Increases with Input Dimension},\nauthor={Carl-Johann Simon-Gabriel and Yann Ollivier and L\u00e9on Bottou and Bernhard Sch\u00f6lkopf and David Lopez-Paz},\nyear={2019},\nurl={https://openreview.net/forum?id=H1MzKs05F7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper426/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621607730, "tddate": null, "super": null, "final": null, "reply": {"forum": "H1MzKs05F7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper426/Authors", "ICLR.cc/2019/Conference/Paper426/Reviewers", "ICLR.cc/2019/Conference/Paper426/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper426/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper426/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper426/Authors|ICLR.cc/2019/Conference/Paper426/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper426/Reviewers", "ICLR.cc/2019/Conference/Paper426/Authors", "ICLR.cc/2019/Conference/Paper426/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621607730}}}, {"id": "HklcFfmhR7", "original": null, "number": 17, "cdate": 1543414402238, "ddate": null, "tcdate": 1543414402238, "tmdate": 1543414402238, "tddate": null, "forum": "H1MzKs05F7", "replyto": "H1MzKs05F7", "invitation": "ICLR.cc/2019/Conference/-/Paper426/Official_Comment", "content": {"title": "Summary of paper updates", "comment": "We thank the reviewers for their stimulating comments that motivated the changes below, which strengthen the paper. We hope that this new version fully addresses the reviewers' concerns by highlighting more clearly the value and limitations of our results on prior vulnerability.\n\n(1st update)\n\nOn Fig. 2, we added PGD training with random starts. The new curves corroborate the validity of our first-order analysis. Overall conclusions remain unchanged.\n\n(Latest update)\n\nWe added a paragraph (Sec 5.1) discussing the implications of our results on prior vulnerability. It relies on a new figure (Fig. 4) that reveals an unmistakable discrepancy between the gradient-norm evolution on the training and test sets respectively. This suggests that nets tend to recover their prior gradient properties outside the training sample, which in turn calls for new architectures with smaller prior gradients and strengthens the importance of our analysis of current priors.\n\nThis new figure required to re-do the experiments on the dimension-dependence of vulnerability and gradients (Sec. 4.2), which therefore got updated as well (Fig. 3). To accelerate training, we used upsampled CIFAR-10 datasets, and pushed the previous results on downsampled ImageNet subsets to Appendix E. Conclusions are unchanged.\n\nThe related literature section (\u00a7 On network vulnerability) now mentions reference [1] (pointed out by AnonReviewer3), which links small worst-case perturbations to larger average perturbations.\n\nFinally, we rephrased our conclusion to reflect more accurately the implications and limitations of our results.\n\n[1] Robustness of classifiers: From adversarial to random noise, Fawzi et al., NIPS, 2016"}, "signatures": ["ICLR.cc/2019/Conference/Paper426/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper426/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper426/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial Vulnerability of Neural Networks Increases with Input Dimension", "abstract": "Over the past four years, neural networks have been proven vulnerable to adversarial images: targeted but imperceptible image perturbations lead to drastically different predictions. We show that adversarial vulnerability increases with the gradients of the training objective when viewed as a function of the inputs. For most current network architectures, we prove that the L1-norm of these gradients grows as the square root of the input size. These nets therefore become increasingly vulnerable with growing image size. Our proofs rely on the network\u2019s weight distribution at initialization, but extensive experiments confirm that our conclusions still hold after usual training.", "keywords": ["adversarial vulnerability", "neural networks", "gradients", "FGSM", "adversarial data-augmentation", "gradient regularization", "robust optimization"], "authorids": ["cjsimon@tuebingen.mpg.de", "yol@fb.com", "leon@bottou.org", "bs@tuebingen.mpg.de", "dlp@fb.com"], "authors": ["Carl-Johann Simon-Gabriel", "Yann Ollivier", "L\u00e9on Bottou", "Bernhard Sch\u00f6lkopf", "David Lopez-Paz"], "TL;DR": "Neural nets have large gradients by design; that makes them adversarially vulnerable.", "pdf": "/pdf/ee82a3413f64426b3c9095ba85a6738aeeecfe6c.pdf", "paperhash": "simongabriel|adversarial_vulnerability_of_neural_networks_increases_with_input_dimension", "_bibtex": "@misc{\nsimon-gabriel2019adversarial,\ntitle={Adversarial Vulnerability of Neural Networks Increases with Input Dimension},\nauthor={Carl-Johann Simon-Gabriel and Yann Ollivier and L\u00e9on Bottou and Bernhard Sch\u00f6lkopf and David Lopez-Paz},\nyear={2019},\nurl={https://openreview.net/forum?id=H1MzKs05F7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper426/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621607730, "tddate": null, "super": null, "final": null, "reply": {"forum": "H1MzKs05F7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper426/Authors", "ICLR.cc/2019/Conference/Paper426/Reviewers", "ICLR.cc/2019/Conference/Paper426/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper426/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper426/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper426/Authors|ICLR.cc/2019/Conference/Paper426/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper426/Reviewers", "ICLR.cc/2019/Conference/Paper426/Authors", "ICLR.cc/2019/Conference/Paper426/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621607730}}}, {"id": "BkeQknMiRQ", "original": null, "number": 15, "cdate": 1543347163006, "ddate": null, "tcdate": 1543347163006, "tmdate": 1543387698505, "tddate": null, "forum": "H1MzKs05F7", "replyto": "SkgnHhtlAQ", "invitation": "ICLR.cc/2019/Conference/-/Paper426/Official_Comment", "content": {"title": "Thanks for the paper update, I still have concerns regarding the discussion of Schmidt et. al.", "comment": "In order to improve upon the bound of Schmidt et. al. one needs a data dependent prior. Simply enforcing additional smoothness by gradient penalization is something that would apply to any dataset, so I don't see how this can explain the remaining sensitivity to test points for the adversarially trained model. Simply observing that these test points have bad gradients adds no information, of course test points which remain sensitive should be expected to have larger gradient, you're saying the model isn't robust because it isn't robust. We can't force perfect smoothness because the constant function will not fit the data, and there are settings where we may be approaching the limit of what smoothness assumptions can buy. For MNIST there are points of different classes within l2 distance of 3 of each other, and adversarially trained models have been shown to have robustness comparable to that distance. \n\nMore experiments are needed to show that data independent gradient regularization can improve upon adversarial training."}, "signatures": ["ICLR.cc/2019/Conference/Paper426/AnonReviewer3"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper426/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper426/AnonReviewer3", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial Vulnerability of Neural Networks Increases with Input Dimension", "abstract": "Over the past four years, neural networks have been proven vulnerable to adversarial images: targeted but imperceptible image perturbations lead to drastically different predictions. We show that adversarial vulnerability increases with the gradients of the training objective when viewed as a function of the inputs. For most current network architectures, we prove that the L1-norm of these gradients grows as the square root of the input size. These nets therefore become increasingly vulnerable with growing image size. Our proofs rely on the network\u2019s weight distribution at initialization, but extensive experiments confirm that our conclusions still hold after usual training.", "keywords": ["adversarial vulnerability", "neural networks", "gradients", "FGSM", "adversarial data-augmentation", "gradient regularization", "robust optimization"], "authorids": ["cjsimon@tuebingen.mpg.de", "yol@fb.com", "leon@bottou.org", "bs@tuebingen.mpg.de", "dlp@fb.com"], "authors": ["Carl-Johann Simon-Gabriel", "Yann Ollivier", "L\u00e9on Bottou", "Bernhard Sch\u00f6lkopf", "David Lopez-Paz"], "TL;DR": "Neural nets have large gradients by design; that makes them adversarially vulnerable.", "pdf": "/pdf/ee82a3413f64426b3c9095ba85a6738aeeecfe6c.pdf", "paperhash": "simongabriel|adversarial_vulnerability_of_neural_networks_increases_with_input_dimension", "_bibtex": "@misc{\nsimon-gabriel2019adversarial,\ntitle={Adversarial Vulnerability of Neural Networks Increases with Input Dimension},\nauthor={Carl-Johann Simon-Gabriel and Yann Ollivier and L\u00e9on Bottou and Bernhard Sch\u00f6lkopf and David Lopez-Paz},\nyear={2019},\nurl={https://openreview.net/forum?id=H1MzKs05F7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper426/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621607730, "tddate": null, "super": null, "final": null, "reply": {"forum": "H1MzKs05F7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper426/Authors", "ICLR.cc/2019/Conference/Paper426/Reviewers", "ICLR.cc/2019/Conference/Paper426/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper426/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper426/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper426/Authors|ICLR.cc/2019/Conference/Paper426/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper426/Reviewers", "ICLR.cc/2019/Conference/Paper426/Authors", "ICLR.cc/2019/Conference/Paper426/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621607730}}}, {"id": "SylH2yQsCX", "original": null, "number": 16, "cdate": 1543348140810, "ddate": null, "tcdate": 1543348140810, "tmdate": 1543387022080, "tddate": null, "forum": "H1MzKs05F7", "replyto": "H1xnecPrC7", "invitation": "ICLR.cc/2019/Conference/-/Paper426/Official_Comment", "content": {"title": "Thanks for the response", "comment": "I appreciate the authors engaging in this important discussion. I think it's important for the introduction of papers to properly discuss the motivation of their work. Your introduction alludes to adversaries constructing imperceptible perturbations to fool statistical classifiers, as if this is a realistic concern to be dealt with. I see limited practical impact for \"defending\" against imperceptible perturbations. If an adversary wants to fool a statistical classifier, in most settings they can do far worse than tiny changes to the input.\n\nIf your motivation is instead methods for learning smoother functions because you believe it will improve model generalization or robustness to distributional shift, write that and evaluate accordingly. \n\nAs I wrote in my review, I'm willing to evaluate the technical details of your work and indeed I see model smoothness as a useful prior for improving more general notions of robustness. While I think your work would be strengthened had you directly considered more realistic forms of robustness, if you are suggesting that my score is purely because your definition of model robustness has limited practical motivation, that is not true."}, "signatures": ["ICLR.cc/2019/Conference/Paper426/AnonReviewer3"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper426/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper426/AnonReviewer3", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial Vulnerability of Neural Networks Increases with Input Dimension", "abstract": "Over the past four years, neural networks have been proven vulnerable to adversarial images: targeted but imperceptible image perturbations lead to drastically different predictions. We show that adversarial vulnerability increases with the gradients of the training objective when viewed as a function of the inputs. For most current network architectures, we prove that the L1-norm of these gradients grows as the square root of the input size. These nets therefore become increasingly vulnerable with growing image size. Our proofs rely on the network\u2019s weight distribution at initialization, but extensive experiments confirm that our conclusions still hold after usual training.", "keywords": ["adversarial vulnerability", "neural networks", "gradients", "FGSM", "adversarial data-augmentation", "gradient regularization", "robust optimization"], "authorids": ["cjsimon@tuebingen.mpg.de", "yol@fb.com", "leon@bottou.org", "bs@tuebingen.mpg.de", "dlp@fb.com"], "authors": ["Carl-Johann Simon-Gabriel", "Yann Ollivier", "L\u00e9on Bottou", "Bernhard Sch\u00f6lkopf", "David Lopez-Paz"], "TL;DR": "Neural nets have large gradients by design; that makes them adversarially vulnerable.", "pdf": "/pdf/ee82a3413f64426b3c9095ba85a6738aeeecfe6c.pdf", "paperhash": "simongabriel|adversarial_vulnerability_of_neural_networks_increases_with_input_dimension", "_bibtex": "@misc{\nsimon-gabriel2019adversarial,\ntitle={Adversarial Vulnerability of Neural Networks Increases with Input Dimension},\nauthor={Carl-Johann Simon-Gabriel and Yann Ollivier and L\u00e9on Bottou and Bernhard Sch\u00f6lkopf and David Lopez-Paz},\nyear={2019},\nurl={https://openreview.net/forum?id=H1MzKs05F7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper426/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621607730, "tddate": null, "super": null, "final": null, "reply": {"forum": "H1MzKs05F7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper426/Authors", "ICLR.cc/2019/Conference/Paper426/Reviewers", "ICLR.cc/2019/Conference/Paper426/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper426/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper426/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper426/Authors|ICLR.cc/2019/Conference/Paper426/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper426/Reviewers", "ICLR.cc/2019/Conference/Paper426/Authors", "ICLR.cc/2019/Conference/Paper426/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621607730}}}, {"id": "H1xnecPrC7", "original": null, "number": 13, "cdate": 1542973940367, "ddate": null, "tcdate": 1542973940367, "tmdate": 1542973940367, "tddate": null, "forum": "H1MzKs05F7", "replyto": "rklddGv7A7", "invitation": "ICLR.cc/2019/Conference/-/Paper426/Official_Comment", "content": {"title": "Answer regarding adversarial terminology", "comment": "Thank you for your quick reactions.\n\nWe understand that you have different intuitions and preferences than us about this problem. But your own reference [1] suggests that both views (vulnerability to small worst-case perturbations as opposed to larger average perturbations) are essentially equivalent. We will be happy to mention that.\n\nMore generally, do you believe that you should oppose the publication of our paper because you would have written a different one?  In our opinion, a diversity of viewpoints is a good thing."}, "signatures": ["ICLR.cc/2019/Conference/Paper426/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper426/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper426/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial Vulnerability of Neural Networks Increases with Input Dimension", "abstract": "Over the past four years, neural networks have been proven vulnerable to adversarial images: targeted but imperceptible image perturbations lead to drastically different predictions. We show that adversarial vulnerability increases with the gradients of the training objective when viewed as a function of the inputs. For most current network architectures, we prove that the L1-norm of these gradients grows as the square root of the input size. These nets therefore become increasingly vulnerable with growing image size. Our proofs rely on the network\u2019s weight distribution at initialization, but extensive experiments confirm that our conclusions still hold after usual training.", "keywords": ["adversarial vulnerability", "neural networks", "gradients", "FGSM", "adversarial data-augmentation", "gradient regularization", "robust optimization"], "authorids": ["cjsimon@tuebingen.mpg.de", "yol@fb.com", "leon@bottou.org", "bs@tuebingen.mpg.de", "dlp@fb.com"], "authors": ["Carl-Johann Simon-Gabriel", "Yann Ollivier", "L\u00e9on Bottou", "Bernhard Sch\u00f6lkopf", "David Lopez-Paz"], "TL;DR": "Neural nets have large gradients by design; that makes them adversarially vulnerable.", "pdf": "/pdf/ee82a3413f64426b3c9095ba85a6738aeeecfe6c.pdf", "paperhash": "simongabriel|adversarial_vulnerability_of_neural_networks_increases_with_input_dimension", "_bibtex": "@misc{\nsimon-gabriel2019adversarial,\ntitle={Adversarial Vulnerability of Neural Networks Increases with Input Dimension},\nauthor={Carl-Johann Simon-Gabriel and Yann Ollivier and L\u00e9on Bottou and Bernhard Sch\u00f6lkopf and David Lopez-Paz},\nyear={2019},\nurl={https://openreview.net/forum?id=H1MzKs05F7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper426/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621607730, "tddate": null, "super": null, "final": null, "reply": {"forum": "H1MzKs05F7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper426/Authors", "ICLR.cc/2019/Conference/Paper426/Reviewers", "ICLR.cc/2019/Conference/Paper426/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper426/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper426/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper426/Authors|ICLR.cc/2019/Conference/Paper426/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper426/Reviewers", "ICLR.cc/2019/Conference/Paper426/Authors", "ICLR.cc/2019/Conference/Paper426/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621607730}}}, {"id": "rklddGv7A7", "original": null, "number": 12, "cdate": 1542840944164, "ddate": null, "tcdate": 1542840944164, "tmdate": 1542840944164, "tddate": null, "forum": "H1MzKs05F7", "replyto": "rkgD7oKgCm", "invitation": "ICLR.cc/2019/Conference/-/Paper426/Official_Comment", "content": {"title": "Regarding the adversarial terminology", "comment": "The criticism on the study of lp adversarial examples is nuanced and I wanted to clarify what I meant. Security motivations aside, it's been shown that the phenomenon of small worst case perturbations is very closely linked to the phenomenon that vision models are sensitive to large average case perturbations [1]. Indeed the same linearity assumptions made in this work directly link the two phenomena --- for a linear model if I tell you the distance to the decision boundary I have also told you the error rate in the presence of large additive Gaussian noise. Obtaining models which are robust to average case image corruptions is of course desirable (they will actually occur in deployment). Perhaps the reason adversarial researchers don't consider robustness to average case image corruptions is they find it less \"intriguing\", but this is ultimately a subjective opinion. The authors mentioned that we can't hope to understand robustness to large corruptions if we don't first understand robustness to small corruptions. To flip this around it's also true that we certainly won't be robust in the worst case if we aren't first robust in the average case.\n\nAll I'm recommending researchers consider measuring is how their theory and defenses are making progress towards generalization outside the natural data distribution, because that ultimately is what this phenomenon is. Enforcing smoothness priors seems like an interesting and potentially useful approach towards this goal, why not demonstrate the usefulness of this prior by showing improved generalization (as measured by test error in corrupted image distributions) rather than *only* measuring the smoothness of our learned functions? ML has been around for a while, and smoothness has long been proposed as an important model prior, but I don't understand why we suddenly see smoothness itself as the goal rather than a means towards better and more robust models. \n\n1. https://arxiv.org/abs/1608.08967"}, "signatures": ["ICLR.cc/2019/Conference/Paper426/AnonReviewer3"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper426/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper426/AnonReviewer3", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial Vulnerability of Neural Networks Increases with Input Dimension", "abstract": "Over the past four years, neural networks have been proven vulnerable to adversarial images: targeted but imperceptible image perturbations lead to drastically different predictions. We show that adversarial vulnerability increases with the gradients of the training objective when viewed as a function of the inputs. For most current network architectures, we prove that the L1-norm of these gradients grows as the square root of the input size. These nets therefore become increasingly vulnerable with growing image size. Our proofs rely on the network\u2019s weight distribution at initialization, but extensive experiments confirm that our conclusions still hold after usual training.", "keywords": ["adversarial vulnerability", "neural networks", "gradients", "FGSM", "adversarial data-augmentation", "gradient regularization", "robust optimization"], "authorids": ["cjsimon@tuebingen.mpg.de", "yol@fb.com", "leon@bottou.org", "bs@tuebingen.mpg.de", "dlp@fb.com"], "authors": ["Carl-Johann Simon-Gabriel", "Yann Ollivier", "L\u00e9on Bottou", "Bernhard Sch\u00f6lkopf", "David Lopez-Paz"], "TL;DR": "Neural nets have large gradients by design; that makes them adversarially vulnerable.", "pdf": "/pdf/ee82a3413f64426b3c9095ba85a6738aeeecfe6c.pdf", "paperhash": "simongabriel|adversarial_vulnerability_of_neural_networks_increases_with_input_dimension", "_bibtex": "@misc{\nsimon-gabriel2019adversarial,\ntitle={Adversarial Vulnerability of Neural Networks Increases with Input Dimension},\nauthor={Carl-Johann Simon-Gabriel and Yann Ollivier and L\u00e9on Bottou and Bernhard Sch\u00f6lkopf and David Lopez-Paz},\nyear={2019},\nurl={https://openreview.net/forum?id=H1MzKs05F7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper426/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621607730, "tddate": null, "super": null, "final": null, "reply": {"forum": "H1MzKs05F7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper426/Authors", "ICLR.cc/2019/Conference/Paper426/Reviewers", "ICLR.cc/2019/Conference/Paper426/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper426/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper426/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper426/Authors|ICLR.cc/2019/Conference/Paper426/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper426/Reviewers", "ICLR.cc/2019/Conference/Paper426/Authors", "ICLR.cc/2019/Conference/Paper426/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621607730}}}, {"id": "S1eK78EXR7", "original": null, "number": 11, "cdate": 1542829601445, "ddate": null, "tcdate": 1542829601445, "tmdate": 1542831871420, "tddate": null, "forum": "H1MzKs05F7", "replyto": "B1lARktMRm", "invitation": "ICLR.cc/2019/Conference/-/Paper426/Official_Comment", "content": {"title": "Thank you for your comments & new preliminary experiments to strengthen our claim", "comment": "Thank you for your quick acknowledgment of our changes and your new comments.\n\nConcerning:\n\"The authors raise an additional point in their reply, namely that robust models do not generalize to the test set because the gradients are not small for test examples (despite being small for training examples). While I find this possibility intriguing, I don't see it being supported experimentally yet.\"\n\nMany thanks for this question. During the last days, we have run some experiments to assay exactly this point.\n\nWe essentially re-ran the CIFAR-10 experiments (Figs 1&2) with pgd training and tracked the evolution (over training epochs) of the average gradient-norm on training and test sets respectively. The (preliminary) results are unmistakable: there is a clear discrepancy between the gradient-norms on train and test sets. On the training set, they first increase a bit, but then neatly decrease during training; on the test set however, they constantly increase.\nWe also ran these experiments with up-sampled CIFAR-10 images (by copying pixels). The higher the resolution, the bigger the discrepancy between training and test set norms.\nThese observations perfectly fit those of [2] (SOTA robust training reduces adversarial vulnerability on test but not on training set) and the idea that a significant amount of adversarial vulnerability can be explained by large gradients.\n\nWe will add those experiments as soon as they are finished and double-checked (i.e. probably only in the final version, after the decision, since we have no material time to do this well in the coming days). We appreciate the discussion and believe it will substantially strengthen the paper. The discussion (both in content and level) illustrates that this topic is of major interest to the community, and we are confident that the final paper will satisfy your (and our) standards of mature science.\n\n[2] Adversarially Robust Generalization Requires More Data, Schmidt et al., 2018"}, "signatures": ["ICLR.cc/2019/Conference/Paper426/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper426/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper426/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial Vulnerability of Neural Networks Increases with Input Dimension", "abstract": "Over the past four years, neural networks have been proven vulnerable to adversarial images: targeted but imperceptible image perturbations lead to drastically different predictions. We show that adversarial vulnerability increases with the gradients of the training objective when viewed as a function of the inputs. For most current network architectures, we prove that the L1-norm of these gradients grows as the square root of the input size. These nets therefore become increasingly vulnerable with growing image size. Our proofs rely on the network\u2019s weight distribution at initialization, but extensive experiments confirm that our conclusions still hold after usual training.", "keywords": ["adversarial vulnerability", "neural networks", "gradients", "FGSM", "adversarial data-augmentation", "gradient regularization", "robust optimization"], "authorids": ["cjsimon@tuebingen.mpg.de", "yol@fb.com", "leon@bottou.org", "bs@tuebingen.mpg.de", "dlp@fb.com"], "authors": ["Carl-Johann Simon-Gabriel", "Yann Ollivier", "L\u00e9on Bottou", "Bernhard Sch\u00f6lkopf", "David Lopez-Paz"], "TL;DR": "Neural nets have large gradients by design; that makes them adversarially vulnerable.", "pdf": "/pdf/ee82a3413f64426b3c9095ba85a6738aeeecfe6c.pdf", "paperhash": "simongabriel|adversarial_vulnerability_of_neural_networks_increases_with_input_dimension", "_bibtex": "@misc{\nsimon-gabriel2019adversarial,\ntitle={Adversarial Vulnerability of Neural Networks Increases with Input Dimension},\nauthor={Carl-Johann Simon-Gabriel and Yann Ollivier and L\u00e9on Bottou and Bernhard Sch\u00f6lkopf and David Lopez-Paz},\nyear={2019},\nurl={https://openreview.net/forum?id=H1MzKs05F7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper426/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621607730, "tddate": null, "super": null, "final": null, "reply": {"forum": "H1MzKs05F7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper426/Authors", "ICLR.cc/2019/Conference/Paper426/Reviewers", "ICLR.cc/2019/Conference/Paper426/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper426/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper426/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper426/Authors|ICLR.cc/2019/Conference/Paper426/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper426/Reviewers", "ICLR.cc/2019/Conference/Paper426/Authors", "ICLR.cc/2019/Conference/Paper426/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621607730}}}, {"id": "rklD0Zcn67", "original": null, "number": 4, "cdate": 1542394319341, "ddate": null, "tcdate": 1542394319341, "tmdate": 1542782971554, "tddate": null, "forum": "H1MzKs05F7", "replyto": "H1MzKs05F7", "invitation": "ICLR.cc/2019/Conference/-/Paper426/Official_Review", "content": {"title": "An interesting approach", "review": "The paper studies how the vulnerability of a neural network model depends on its input dimension. The authors prove that for an *untrained* model, randomly initialized with Xavier initialization, the gradient of the loss wrt the input is essentially independent of the architecture and task. This implies that the major factor affecting the norm of that gradient is the input dimension. They then support their argument by experiments measuring the relation between adversarial vulnerability and gradient norm using various *trained* models (including adversarially regularized ones).\n\nI find the main theoretical result interesting. While this is a known fact for the simple case of linear classifiers, extending it to arbitrarily deep networks is a valuable contribution. The proof crucially relies on properties of the specific initialization scheme to show that the gradient does not change too much during backproparagation through the layers. The most significant limitation of the result (which the authors kindly acknowledge) is that this result only holds at initialization. Hence it cannot distinguish between different training methods or between how different architectures evolve during training. Since the situation in adversarial robustness is much more nuanced, I am skeptical about the significance of such statements.\n\nOn the experimental side, the finding that gradient regularization improves adversarial robustness to small epsilon values has been made multiple times in the past (as the authors cite in the related work section). It is worth noting that the epsilon considered is 0.005 in L_inf (1.275/255) which is pretty small. This value corresponds to the \"small-epsilon regime\" where the behavior of the model is fairly linear around the original inputs and thus defenses such as FGSM-training and gradient regularization are effective.\n\nThe authors also perform an interesting experiment where they train models on downsampled ImageNet datasets and find that indeed larger input dimension leads to more vulnerable models.\n\nWhile I find the results interesting, I do not see clear implications. The fact that the vulnerability of a classifier depends on the L1 norm of the input gradient is already known for any locally linear classifier (i.e. deep models too), and it is fairly clear that the L1 norm will have a dimension dependence. The fact that it does not depend on architecture or task at initialization is interesting but of limited significance in my opinion. Given that the experimental results are also not particularly novel, I recommend rejection.\n\n[UPDATE]: Given the overall discussion and paper updates, I consider the current version of the paper (marginally) crossing the ICLR bar. I update my score from a 5 to a 6.\n\nMinor comments to the authors:\n-- I think || x ||_* is more clear than |||x||| for the dual norm.\n-- Consider using lambda for the regularization, epsilon is confusing since it is overloaded.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper426/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Adversarial Vulnerability of Neural Networks Increases with Input Dimension", "abstract": "Over the past four years, neural networks have been proven vulnerable to adversarial images: targeted but imperceptible image perturbations lead to drastically different predictions. We show that adversarial vulnerability increases with the gradients of the training objective when viewed as a function of the inputs. For most current network architectures, we prove that the L1-norm of these gradients grows as the square root of the input size. These nets therefore become increasingly vulnerable with growing image size. Our proofs rely on the network\u2019s weight distribution at initialization, but extensive experiments confirm that our conclusions still hold after usual training.", "keywords": ["adversarial vulnerability", "neural networks", "gradients", "FGSM", "adversarial data-augmentation", "gradient regularization", "robust optimization"], "authorids": ["cjsimon@tuebingen.mpg.de", "yol@fb.com", "leon@bottou.org", "bs@tuebingen.mpg.de", "dlp@fb.com"], "authors": ["Carl-Johann Simon-Gabriel", "Yann Ollivier", "L\u00e9on Bottou", "Bernhard Sch\u00f6lkopf", "David Lopez-Paz"], "TL;DR": "Neural nets have large gradients by design; that makes them adversarially vulnerable.", "pdf": "/pdf/ee82a3413f64426b3c9095ba85a6738aeeecfe6c.pdf", "paperhash": "simongabriel|adversarial_vulnerability_of_neural_networks_increases_with_input_dimension", "_bibtex": "@misc{\nsimon-gabriel2019adversarial,\ntitle={Adversarial Vulnerability of Neural Networks Increases with Input Dimension},\nauthor={Carl-Johann Simon-Gabriel and Yann Ollivier and L\u00e9on Bottou and Bernhard Sch\u00f6lkopf and David Lopez-Paz},\nyear={2019},\nurl={https://openreview.net/forum?id=H1MzKs05F7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper426/Official_Review", "cdate": 1542234464328, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "H1MzKs05F7", "replyto": "H1MzKs05F7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper426/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335721423, "tmdate": 1552335721423, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper426/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "B1lARktMRm", "original": null, "number": 10, "cdate": 1542782933950, "ddate": null, "tcdate": 1542782933950, "tmdate": 1542782933950, "tddate": null, "forum": "H1MzKs05F7", "replyto": "HJemastx07", "invitation": "ICLR.cc/2019/Conference/-/Paper426/Official_Comment", "content": {"title": "Reviewer response", "comment": "I appreciate the author's response to my review and their overall reply.\n\nI find the fact that the same linear relation holds for adversarially robust models interesting. The authors raise an additional point in their reply, namely that robust models do not generalize to the test set because the gradients are not small for test examples (despite being small for training examples). While I find this possibility intriguing, I don't see it being supported experimentally yet. If this was true, we would be able to differentiate between test examples that are vulnerable and test examples that are not based on their gradient norm, right?\n\nThe authors argue that their results open a new direction towards training robust models by using different initialization priors. I agree that this is an interesting conjecture. However, I do not find this argument supported by the current findings of the paper. As such, I cannot consider this a contribution of this paper. It is the author's responsibility to provide arguments supporting the potential of this research direction.\n\nGiven the overall discussion and paper edits, I consider the current version of the paper (marginally) crossing the ICLR bar and have updated my score to reflect that. Nevertheless, I would encourage the authors to update the discussion in their paper since many of the points raised here are more nuanced than those made in the paper."}, "signatures": ["ICLR.cc/2019/Conference/Paper426/AnonReviewer4"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper426/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper426/AnonReviewer4", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial Vulnerability of Neural Networks Increases with Input Dimension", "abstract": "Over the past four years, neural networks have been proven vulnerable to adversarial images: targeted but imperceptible image perturbations lead to drastically different predictions. We show that adversarial vulnerability increases with the gradients of the training objective when viewed as a function of the inputs. For most current network architectures, we prove that the L1-norm of these gradients grows as the square root of the input size. These nets therefore become increasingly vulnerable with growing image size. Our proofs rely on the network\u2019s weight distribution at initialization, but extensive experiments confirm that our conclusions still hold after usual training.", "keywords": ["adversarial vulnerability", "neural networks", "gradients", "FGSM", "adversarial data-augmentation", "gradient regularization", "robust optimization"], "authorids": ["cjsimon@tuebingen.mpg.de", "yol@fb.com", "leon@bottou.org", "bs@tuebingen.mpg.de", "dlp@fb.com"], "authors": ["Carl-Johann Simon-Gabriel", "Yann Ollivier", "L\u00e9on Bottou", "Bernhard Sch\u00f6lkopf", "David Lopez-Paz"], "TL;DR": "Neural nets have large gradients by design; that makes them adversarially vulnerable.", "pdf": "/pdf/ee82a3413f64426b3c9095ba85a6738aeeecfe6c.pdf", "paperhash": "simongabriel|adversarial_vulnerability_of_neural_networks_increases_with_input_dimension", "_bibtex": "@misc{\nsimon-gabriel2019adversarial,\ntitle={Adversarial Vulnerability of Neural Networks Increases with Input Dimension},\nauthor={Carl-Johann Simon-Gabriel and Yann Ollivier and L\u00e9on Bottou and Bernhard Sch\u00f6lkopf and David Lopez-Paz},\nyear={2019},\nurl={https://openreview.net/forum?id=H1MzKs05F7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper426/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621607730, "tddate": null, "super": null, "final": null, "reply": {"forum": "H1MzKs05F7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper426/Authors", "ICLR.cc/2019/Conference/Paper426/Reviewers", "ICLR.cc/2019/Conference/Paper426/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper426/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper426/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper426/Authors|ICLR.cc/2019/Conference/Paper426/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper426/Reviewers", "ICLR.cc/2019/Conference/Paper426/Authors", "ICLR.cc/2019/Conference/Paper426/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621607730}}}, {"id": "S1lSrptgAm", "original": null, "number": 9, "cdate": 1542655292857, "ddate": null, "tcdate": 1542655292857, "tmdate": 1542695048499, "tddate": null, "forum": "H1MzKs05F7", "replyto": "BJgK8oPUh7", "invitation": "ICLR.cc/2019/Conference/-/Paper426/Official_Comment", "content": {"title": "Specific answers to your comments & concerns", "comment": "We thank you for your time, review, comments and concerns, which we hope to address in full.\n\n- Concerning: \"This analysis only seems to work for 'well-behaved' models. For models ... apply\"\n\nIndeed, we only analyse differentiable models. First, note that our results already cover many usual networks (not just a small subset). Second, we think that understanding such well-behaved models is a first step towards understanding non-differentiable ones. (For example, some non-differentiable functions can be considered differentiable at a rougher scale. But this opens a whole new research direction, while the text is long enough...)\n\n- Concerning: \"the first order Taylor expansion argument on top of randomly initialized weights is oversimplifying the complicated problem.\"\n\nNot all adversarial vulnerability might be first-order, but first-order vulnerability *is* an aspect of vulnerability (not an oversimplification of a problem). If there is first-order vulnerability, then there is vulnerability. Moreover, our results actually suggest that first-order vulnerability and its relation to gradients explains an *essential* part of vulnerability (see Fig 2d, and paragraph \"Validity of first order expansion\").\n\n- Concerning: \"the analysis probably cannot cover the  adversarially PGD trained models [MMS+17] and the certifiably robust ones\" & \"It is especially worrisome to me that the paper does not cover the adversarially-augmented training based iterative attacks, e.g. PGD TRAINED models\"\n\nWe added experiments with PGD training on CIFAR-10 (see Fig 2 & 6). Our conclusions stay unchanged. The new experiments support our claim that first-order vulnerability plays an essential role.\n\n- Concerning: \"What matters the most is 'why the strongest model is still not robust?' not 'why some weak models are not robust?'\"\n\nWe think that understanding the vulnerability of \"weak\" models (i.e. at initialization or with usual training) may help understanding the vulnerability of SOTA-robustly trained nets. See our post on \"why prior vulnerability matters\".\n\n- Concerning our last sentence:\n\nWe can reformulate it to:\n\"Nevertheless, they show that at least this type of first-order vulnerability is present, common, and firmly rooted *in the priors* of our current network architectures. In future, we may hence want to complement our robust regularisation techniques by new architectures (or architectural building blocks) with less vulnerable priors.\"\n(Anything in that direction would do. We are open to propositions.)"}, "signatures": ["ICLR.cc/2019/Conference/Paper426/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper426/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper426/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial Vulnerability of Neural Networks Increases with Input Dimension", "abstract": "Over the past four years, neural networks have been proven vulnerable to adversarial images: targeted but imperceptible image perturbations lead to drastically different predictions. We show that adversarial vulnerability increases with the gradients of the training objective when viewed as a function of the inputs. For most current network architectures, we prove that the L1-norm of these gradients grows as the square root of the input size. These nets therefore become increasingly vulnerable with growing image size. Our proofs rely on the network\u2019s weight distribution at initialization, but extensive experiments confirm that our conclusions still hold after usual training.", "keywords": ["adversarial vulnerability", "neural networks", "gradients", "FGSM", "adversarial data-augmentation", "gradient regularization", "robust optimization"], "authorids": ["cjsimon@tuebingen.mpg.de", "yol@fb.com", "leon@bottou.org", "bs@tuebingen.mpg.de", "dlp@fb.com"], "authors": ["Carl-Johann Simon-Gabriel", "Yann Ollivier", "L\u00e9on Bottou", "Bernhard Sch\u00f6lkopf", "David Lopez-Paz"], "TL;DR": "Neural nets have large gradients by design; that makes them adversarially vulnerable.", "pdf": "/pdf/ee82a3413f64426b3c9095ba85a6738aeeecfe6c.pdf", "paperhash": "simongabriel|adversarial_vulnerability_of_neural_networks_increases_with_input_dimension", "_bibtex": "@misc{\nsimon-gabriel2019adversarial,\ntitle={Adversarial Vulnerability of Neural Networks Increases with Input Dimension},\nauthor={Carl-Johann Simon-Gabriel and Yann Ollivier and L\u00e9on Bottou and Bernhard Sch\u00f6lkopf and David Lopez-Paz},\nyear={2019},\nurl={https://openreview.net/forum?id=H1MzKs05F7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper426/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621607730, "tddate": null, "super": null, "final": null, "reply": {"forum": "H1MzKs05F7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper426/Authors", "ICLR.cc/2019/Conference/Paper426/Reviewers", "ICLR.cc/2019/Conference/Paper426/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper426/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper426/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper426/Authors|ICLR.cc/2019/Conference/Paper426/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper426/Reviewers", "ICLR.cc/2019/Conference/Paper426/Authors", "ICLR.cc/2019/Conference/Paper426/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621607730}}}, {"id": "SkgnHhtlAQ", "original": null, "number": 7, "cdate": 1542655044292, "ddate": null, "tcdate": 1542655044292, "tmdate": 1542694900929, "tddate": null, "forum": "H1MzKs05F7", "replyto": "H1gaUtVATX", "invitation": "ICLR.cc/2019/Conference/-/Paper426/Official_Comment", "content": {"title": "Specific answers to your comments & concerns", "comment": "We thank you for your expanded reviews, comments, questions and references, which we hope to address in full.\n\n-Concerning: \"The experiments for adversarially trained models in [1] directly contradict the title of this paper\"\n\nSee point 2/ of our overall reply.\n\n- Concerning: \"for some settings of the weights you can show a bound such as is discussed in the paper, but there are other settings (perhaps even initializations) of the weights for which the conclusion will not hold.\"\n\nThe current initialization-methods are used to avoid exploding/vanishing activations at init. Any other initialization would need to solve that issue. See point 3/ of our overall reply.\n\n- Concerning: \"fixing the initialization seems unlikely to buy us much more than what adversarial training achieves, and the experiments in [1] suggest to me the conclusion of this work is limited in scope\"\n\nPlease refer to our overall thread on \"why prior vulnerability matters\" and how it might help understanding and harnessing the vulnerability of (robustly) trained networks.\n\n- Concerning: \"we hit a limit as we increase the epsilon considered for the perturbations\"\n\nEven for small epsilons, our networks are surprisingly vulnerable. If we don't understand the small epsilon vulnerability, then we won't understand big epsilons.\n\n- Concerning: \"it\u2019s not clear to me what actionable insights we can conclude from this work, and how this can be used to improve upon the current SOTA.\"\n\nAgain, please see our thread on \"why prior vulnerability matters\"  and how it may help understanding and harnessing the vulnerability of (robustly) trained networks.\n\n- Concerning: \"it was found that adversarial training eventually gives robustness to the training set, but this robustness does not generalize to the test set [2]... the data distribution.\"\n\nSee 4/ in our overall reply.\n\n- Concerning: \"in [2] it was shown that there is no learning algorithm [that] can become robust to small perturbations, unless that model is trained on significantly more data.\"\n\nPlease refer to our thread on \"why prior vulnerability matters\". As we explain there, to get better generalisation you can either increase your amount of training data, or decrease the complexity of your model, i.e. choose better (non-vulnerable!) priors.\n\n- Concerning assumption H1 and reference [3]:\n\nthe mean field approach of [3] relies on very strong independence approximations, namely, neglecting individual effects and replacing them with overall averaged effects with similar statistics. This amounts to disregarding most correlations. We do believe a mean-field treatment of our approach is possible, but in the end, the mean field approximations are much stronger than our assumption H1, though similar in spirit.\n\n[1] Are adversarial examples inevitable?, 2018\n[3] Deep Information Propagation, Schoenholz et al., 2017"}, "signatures": ["ICLR.cc/2019/Conference/Paper426/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper426/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper426/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial Vulnerability of Neural Networks Increases with Input Dimension", "abstract": "Over the past four years, neural networks have been proven vulnerable to adversarial images: targeted but imperceptible image perturbations lead to drastically different predictions. We show that adversarial vulnerability increases with the gradients of the training objective when viewed as a function of the inputs. For most current network architectures, we prove that the L1-norm of these gradients grows as the square root of the input size. These nets therefore become increasingly vulnerable with growing image size. Our proofs rely on the network\u2019s weight distribution at initialization, but extensive experiments confirm that our conclusions still hold after usual training.", "keywords": ["adversarial vulnerability", "neural networks", "gradients", "FGSM", "adversarial data-augmentation", "gradient regularization", "robust optimization"], "authorids": ["cjsimon@tuebingen.mpg.de", "yol@fb.com", "leon@bottou.org", "bs@tuebingen.mpg.de", "dlp@fb.com"], "authors": ["Carl-Johann Simon-Gabriel", "Yann Ollivier", "L\u00e9on Bottou", "Bernhard Sch\u00f6lkopf", "David Lopez-Paz"], "TL;DR": "Neural nets have large gradients by design; that makes them adversarially vulnerable.", "pdf": "/pdf/ee82a3413f64426b3c9095ba85a6738aeeecfe6c.pdf", "paperhash": "simongabriel|adversarial_vulnerability_of_neural_networks_increases_with_input_dimension", "_bibtex": "@misc{\nsimon-gabriel2019adversarial,\ntitle={Adversarial Vulnerability of Neural Networks Increases with Input Dimension},\nauthor={Carl-Johann Simon-Gabriel and Yann Ollivier and L\u00e9on Bottou and Bernhard Sch\u00f6lkopf and David Lopez-Paz},\nyear={2019},\nurl={https://openreview.net/forum?id=H1MzKs05F7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper426/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621607730, "tddate": null, "super": null, "final": null, "reply": {"forum": "H1MzKs05F7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper426/Authors", "ICLR.cc/2019/Conference/Paper426/Reviewers", "ICLR.cc/2019/Conference/Paper426/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper426/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper426/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper426/Authors|ICLR.cc/2019/Conference/Paper426/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper426/Reviewers", "ICLR.cc/2019/Conference/Paper426/Authors", "ICLR.cc/2019/Conference/Paper426/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621607730}}}, {"id": "SJxgihtxRm", "original": null, "number": 8, "cdate": 1542655127678, "ddate": null, "tcdate": 1542655127678, "tmdate": 1542655127678, "tddate": null, "forum": "H1MzKs05F7", "replyto": "H1eawIzc3X", "invitation": "ICLR.cc/2019/Conference/-/Paper426/Official_Comment", "content": {"title": "Specific answers to your few questions", "comment": "We thank you for your review and your very positive evaluation.\n\nConcerning Fig 4 in Appendix A:\nAppendix A is preliminary work, whose goal is essentially to illustrate how our insights on the prior-vulnerability of neural networks can help us design more robust networks; in this case, by preferring average-poolings over other pooling-operations. But we agree that this section only contains preliminary results, which is why it is in appendix, not main text.\n\nConcerning: \"could the authors comment on possible defences to adversarial attack by altering this weight distribution? (By, for example, imposing that the average value must grow like 1/d)?\"\nPlease refer to point 3/ of our overall reply, which explains what problems arise if we just change the overall weight-size at init.\n\nWe hope that this addresses your small concerns/questions and thank you, once again, for your evaluation."}, "signatures": ["ICLR.cc/2019/Conference/Paper426/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper426/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper426/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial Vulnerability of Neural Networks Increases with Input Dimension", "abstract": "Over the past four years, neural networks have been proven vulnerable to adversarial images: targeted but imperceptible image perturbations lead to drastically different predictions. We show that adversarial vulnerability increases with the gradients of the training objective when viewed as a function of the inputs. For most current network architectures, we prove that the L1-norm of these gradients grows as the square root of the input size. These nets therefore become increasingly vulnerable with growing image size. Our proofs rely on the network\u2019s weight distribution at initialization, but extensive experiments confirm that our conclusions still hold after usual training.", "keywords": ["adversarial vulnerability", "neural networks", "gradients", "FGSM", "adversarial data-augmentation", "gradient regularization", "robust optimization"], "authorids": ["cjsimon@tuebingen.mpg.de", "yol@fb.com", "leon@bottou.org", "bs@tuebingen.mpg.de", "dlp@fb.com"], "authors": ["Carl-Johann Simon-Gabriel", "Yann Ollivier", "L\u00e9on Bottou", "Bernhard Sch\u00f6lkopf", "David Lopez-Paz"], "TL;DR": "Neural nets have large gradients by design; that makes them adversarially vulnerable.", "pdf": "/pdf/ee82a3413f64426b3c9095ba85a6738aeeecfe6c.pdf", "paperhash": "simongabriel|adversarial_vulnerability_of_neural_networks_increases_with_input_dimension", "_bibtex": "@misc{\nsimon-gabriel2019adversarial,\ntitle={Adversarial Vulnerability of Neural Networks Increases with Input Dimension},\nauthor={Carl-Johann Simon-Gabriel and Yann Ollivier and L\u00e9on Bottou and Bernhard Sch\u00f6lkopf and David Lopez-Paz},\nyear={2019},\nurl={https://openreview.net/forum?id=H1MzKs05F7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper426/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621607730, "tddate": null, "super": null, "final": null, "reply": {"forum": "H1MzKs05F7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper426/Authors", "ICLR.cc/2019/Conference/Paper426/Reviewers", "ICLR.cc/2019/Conference/Paper426/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper426/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper426/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper426/Authors|ICLR.cc/2019/Conference/Paper426/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper426/Reviewers", "ICLR.cc/2019/Conference/Paper426/Authors", "ICLR.cc/2019/Conference/Paper426/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621607730}}}, {"id": "HJemastx07", "original": null, "number": 6, "cdate": 1542654907010, "ddate": null, "tcdate": 1542654907010, "tmdate": 1542654907010, "tddate": null, "forum": "H1MzKs05F7", "replyto": "rklD0Zcn67", "invitation": "ICLR.cc/2019/Conference/-/Paper426/Official_Comment", "content": {"title": "Some specific answers to your comments", "comment": "We thank you for your careful review, and for pointing out that many people do indeed care about small worst-case l_p-perturbations, and why.\n\n- Concerning: \"I am skeptical about the significance of such statements [at initialization].\" & \"While I find the results interesting, I do not see clear implications.\"\n\nPlease refer to our overall thread on, why understanding the vulnerability of priors helps understanding post (robust) training vulnerability. See also point 4/ in our overall reply.\n\n- Concerning: \"While this is a known fact for the simple case of linear classifiers...\"\n\nEven with only linear classifiers, previous published work has predicted a linear increase of vulnerability with input-dimension rather than sqrt(d), because they did not take the dimension-dependance of the weights into account.\n\n- Concerning: \"it is fairly clear that the L1 norm will have a dimension dependence\"\n\nMaybe, but it is all about getting the numbers right. Our predictions correspond to the *exact* increase-rate measured in practice.\n\n- Concerning: \"The fact that it does not depend on architecture or task at initialization is interesting but of limited significance in my opinion.\"\n\nThis independence on architecture at initialization shows that, if we want to get non-vulnerable priors, we need to re-think our initialization scheme and/or introduce a new architectural building block. As to why we would want non-vulnerable priors, again, please see our overall thread on the subject.\n\nOnce again, we thank you for your review and hope that our answers may help you to re-evaluate the significance of our results."}, "signatures": ["ICLR.cc/2019/Conference/Paper426/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper426/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper426/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial Vulnerability of Neural Networks Increases with Input Dimension", "abstract": "Over the past four years, neural networks have been proven vulnerable to adversarial images: targeted but imperceptible image perturbations lead to drastically different predictions. We show that adversarial vulnerability increases with the gradients of the training objective when viewed as a function of the inputs. For most current network architectures, we prove that the L1-norm of these gradients grows as the square root of the input size. These nets therefore become increasingly vulnerable with growing image size. Our proofs rely on the network\u2019s weight distribution at initialization, but extensive experiments confirm that our conclusions still hold after usual training.", "keywords": ["adversarial vulnerability", "neural networks", "gradients", "FGSM", "adversarial data-augmentation", "gradient regularization", "robust optimization"], "authorids": ["cjsimon@tuebingen.mpg.de", "yol@fb.com", "leon@bottou.org", "bs@tuebingen.mpg.de", "dlp@fb.com"], "authors": ["Carl-Johann Simon-Gabriel", "Yann Ollivier", "L\u00e9on Bottou", "Bernhard Sch\u00f6lkopf", "David Lopez-Paz"], "TL;DR": "Neural nets have large gradients by design; that makes them adversarially vulnerable.", "pdf": "/pdf/ee82a3413f64426b3c9095ba85a6738aeeecfe6c.pdf", "paperhash": "simongabriel|adversarial_vulnerability_of_neural_networks_increases_with_input_dimension", "_bibtex": "@misc{\nsimon-gabriel2019adversarial,\ntitle={Adversarial Vulnerability of Neural Networks Increases with Input Dimension},\nauthor={Carl-Johann Simon-Gabriel and Yann Ollivier and L\u00e9on Bottou and Bernhard Sch\u00f6lkopf and David Lopez-Paz},\nyear={2019},\nurl={https://openreview.net/forum?id=H1MzKs05F7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper426/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621607730, "tddate": null, "super": null, "final": null, "reply": {"forum": "H1MzKs05F7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper426/Authors", "ICLR.cc/2019/Conference/Paper426/Reviewers", "ICLR.cc/2019/Conference/Paper426/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper426/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper426/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper426/Authors|ICLR.cc/2019/Conference/Paper426/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper426/Reviewers", "ICLR.cc/2019/Conference/Paper426/Authors", "ICLR.cc/2019/Conference/Paper426/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621607730}}}, {"id": "rkgD7oKgCm", "original": null, "number": 5, "cdate": 1542654751143, "ddate": null, "tcdate": 1542654751143, "tmdate": 1542654751143, "tddate": null, "forum": "H1MzKs05F7", "replyto": "rkxRoqKxRm", "invitation": "ICLR.cc/2019/Conference/-/Paper426/Official_Comment", "content": {"title": "Why prior vulnerability matters (compendium to the overall reply)", "comment": "The reviewers acknowledge that our results --in particular our theorems and their empirical verification in Fig 3-- are novel and interesting. But some of them wonder how understanding the vulnerability of networks at initialization or after usual training helps understanding their vulnerability after (SOTA-) robust training. We try to answer these concerns with the following two points:\n\na/ Why understanding prior-vulnerability of neural networks may partly explain their vulnerability after robust training.\n\nWithout the right priors, even the best learning algorithms fail.\nFor example, if instead of using carefully designed CNNs, we used fully connected (FC) nets for image classification, we'd get much worse accuracies; especially if our FC net had the same amount of neurons than the CNN (although, theoretically, it could learn the same classifier than the CNN). The situation for adversarial vulnerability is the same: of course, smart training algorithms can help to improve robustness. And the fact that the classifiers get more robust shows that they do help up to some extent. But very ill-behaved priors (i.e. naturally vulnerable priors) will certainly harden their task.\n\nThe example pointed out by AnonReviewer3, about recent work [2] showing that adversarial training is efficient on the training set but usually fails to generalise to the test set, may actually be evidence in our favour. Think about FC nets again: they typically get 100% accuracy on the training set, but completely fail to generalise to the test set. The fact that this (essentially) does not happen to CNNs --despite using the same training algorithm!-- shows the importance of choosing well-behaved priors on our architectures/classifiers. Of course, one can argue, as in [2], that with more data, this would not happen, and hence, that adversarial vulnerability is essentially a sample-size problem. But an alternative way to get robust classifiers with the same amount of data might be to use better priors, i.e. more carefully designed architectures. Which brings us to our second point. \n\nb/ Our paper essentially shows that our priors are already adversarially vulnerable and usual training does not escape these priors. But how can we use those insights? What \"actionable\" insight does it give us?\n\nFor one thing, as argued in a/, pointing out the vulnerability of our priors gives the community a clear reason to search for better priors, so as to complement our robustification algorithms. But they give even more: our theorems, with their clear assumptions and proofs, may actually guide this search. We may for example ask: how can I design an architecture/block that escapes our assumptions? Or: what essential parts of the proofs actually \"generate vulnerability\"? Appendix B goes in that direction: noticing that overly large weights are an essential reason for vulnerability, it analyses what happens if we introduce average-pooling layers (that have weights of size 1/d rather than 1/sqrt(d)). This section is still preliminary (which is why it is in appendix), but it illustrates how our results, despite being only on priors, can yield concrete, actionable results to improve adversarial robustness.\n\n[2] Adversarially Robust Generalization Requires More Data, Schmidt et al., 2018\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper426/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper426/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper426/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial Vulnerability of Neural Networks Increases with Input Dimension", "abstract": "Over the past four years, neural networks have been proven vulnerable to adversarial images: targeted but imperceptible image perturbations lead to drastically different predictions. We show that adversarial vulnerability increases with the gradients of the training objective when viewed as a function of the inputs. For most current network architectures, we prove that the L1-norm of these gradients grows as the square root of the input size. These nets therefore become increasingly vulnerable with growing image size. Our proofs rely on the network\u2019s weight distribution at initialization, but extensive experiments confirm that our conclusions still hold after usual training.", "keywords": ["adversarial vulnerability", "neural networks", "gradients", "FGSM", "adversarial data-augmentation", "gradient regularization", "robust optimization"], "authorids": ["cjsimon@tuebingen.mpg.de", "yol@fb.com", "leon@bottou.org", "bs@tuebingen.mpg.de", "dlp@fb.com"], "authors": ["Carl-Johann Simon-Gabriel", "Yann Ollivier", "L\u00e9on Bottou", "Bernhard Sch\u00f6lkopf", "David Lopez-Paz"], "TL;DR": "Neural nets have large gradients by design; that makes them adversarially vulnerable.", "pdf": "/pdf/ee82a3413f64426b3c9095ba85a6738aeeecfe6c.pdf", "paperhash": "simongabriel|adversarial_vulnerability_of_neural_networks_increases_with_input_dimension", "_bibtex": "@misc{\nsimon-gabriel2019adversarial,\ntitle={Adversarial Vulnerability of Neural Networks Increases with Input Dimension},\nauthor={Carl-Johann Simon-Gabriel and Yann Ollivier and L\u00e9on Bottou and Bernhard Sch\u00f6lkopf and David Lopez-Paz},\nyear={2019},\nurl={https://openreview.net/forum?id=H1MzKs05F7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper426/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621607730, "tddate": null, "super": null, "final": null, "reply": {"forum": "H1MzKs05F7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper426/Authors", "ICLR.cc/2019/Conference/Paper426/Reviewers", "ICLR.cc/2019/Conference/Paper426/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper426/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper426/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper426/Authors|ICLR.cc/2019/Conference/Paper426/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper426/Reviewers", "ICLR.cc/2019/Conference/Paper426/Authors", "ICLR.cc/2019/Conference/Paper426/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621607730}}}, {"id": "rkxRoqKxRm", "original": null, "number": 4, "cdate": 1542654630030, "ddate": null, "tcdate": 1542654630030, "tmdate": 1542654630030, "tddate": null, "forum": "H1MzKs05F7", "replyto": "H1MzKs05F7", "invitation": "ICLR.cc/2019/Conference/-/Paper426/Official_Comment", "content": {"title": "Overall answer to reviews", "comment": "First, we would like to thank all the reviewers and contributors to the discussions. We are happy to see that our study raises so many questions.  We are in fact a bit surprised to be caught in such a storm, especially since nobody seems to disagree about the facts at a technical level...\n\nWe do believe our paper offers the most precise study of the effects of first-order adversarial perturbations; especially, proving independence from network structure, and re-emphasizing the direct relationship with input dimension. The limitations are clearly stated in the text. We do maintain there is a connection with adversarial attacks: if there is a vulnerability at first order, then there is a vulnerability. And we thank AnonReviewer4 for pointing out that many people do care about first-order worst-case perturbations. However, we agree that not all vulnerability comes from first-order phenomena.  \n\nThere seems to be an ongoing debate about the best terminology for the objects studied in our paper. We are happy to term them first-order inputs perturbations, first-order worst-case corruption, first-order adversarial perturbations, or anything similar that makes sense. We will be happy to receive suggestions and edit the text accordingly.  Would that be acceptable?\n\nAs to why our insights on prior vulnerability is relevant to understand the vulnerability of SOTA robust models: as mentioned by AnonReviewer3 citing [2], SOTA models still don't manage to reduce gradients on out-of-training examples (see point 5/ below). This is why we think our study is still highly relevant even after strong adversarial training. (Also see our separate thread on why prior vulnerability matters.)\n\nAs for technical remarks:\n\n1/ More defences closer to SOTA, especially, iterative ones: the version updated today covers PGD iterative defences (see Figs. 2 & 6). The conclusions are unchanged.\n\n2/ Concerning Fig 4b of the later work [1]: Fig 4b of [1] is *after* robust adversarial training; our claims refer to before adversarial training. So [1] does not refute our claims: rather, it confirms that robust adversarial training operates via reducing gradient issues.\nMore precisely, to be comparable across dimensions, the x-axis of Fig 4 of [1] should be rescaled to epsilon / sqrt(d), because their epsilon is measured in l_2-norm rather than l_infty (see our Eq. (3), or their text).  After proper rescaling, Fig 4a shows that naturally trained nets are more vulnerable with growing input-dimension. This confirms our own theorems and experiments (our Fig. 3). After rescaling, the curves of Fig 4b seem to overlap: indeed this suggests that our predicted dimension-dependence vanishes *after* robust adversarial training (rather than usual training). This does not contradict our results, it comforts and completes them: usual training does not escape the prior's vulnerability, and in some situations, ill-behaved priors can be escaped by robust training.\n\n3/ Initialization: most people use He-like initializations for a very good reason: it is essentially the only way to obtain bounded activations at init and be symmetric over the inputs of each unit. Our work thus underlines a conflict between keeping reasonably-valued activations, and having reasonably-sized input gradients. This certainly suggests future studies of other initializations. Such initializations will somehow have to break symmetry between units, and thus introduce implicit priors. Some possibilities would be to favor low frequencies in the image, or to prioritize the learning of some units wrt others by initializing them to larger weights (thus implicitly telling the network to try the large-activation units first, which in effect is a prior on the number of units). We have started to play with this, but this is a whole new research direction, while the present text is long enough...\n\n4/ Concerning \"adversarial training eventually gives robustness to the training set, but this robustness does not generalize to the test set [2]. For these models, the gradients are well behaved local to the training points (...) but the gradients aren\u2019t well behaved for new iid samples from the data distribution.\"\nThis does not contradict our theory/experiments. On the contrary: it appears that outside the training points where gradients have been decreased, the network might keep the gradient properties from its prior, namely, naturally large gradients. This example hence rather shows that our findings on priors and the resulting gradients should be kept in mind.\n\nOnce more, we would like to thank the reviewers for the rich debate. We hope that the technical points have been addressed. We do believe our results are non-trivial and relevant. It seems that the \"adversarial\" terminology raises issues: we are open to consensus suggestions on this point.\n\n[1] Are adversarial examples inevitable, anonymous, under review for ICLR 2019"}, "signatures": ["ICLR.cc/2019/Conference/Paper426/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper426/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper426/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial Vulnerability of Neural Networks Increases with Input Dimension", "abstract": "Over the past four years, neural networks have been proven vulnerable to adversarial images: targeted but imperceptible image perturbations lead to drastically different predictions. We show that adversarial vulnerability increases with the gradients of the training objective when viewed as a function of the inputs. For most current network architectures, we prove that the L1-norm of these gradients grows as the square root of the input size. These nets therefore become increasingly vulnerable with growing image size. Our proofs rely on the network\u2019s weight distribution at initialization, but extensive experiments confirm that our conclusions still hold after usual training.", "keywords": ["adversarial vulnerability", "neural networks", "gradients", "FGSM", "adversarial data-augmentation", "gradient regularization", "robust optimization"], "authorids": ["cjsimon@tuebingen.mpg.de", "yol@fb.com", "leon@bottou.org", "bs@tuebingen.mpg.de", "dlp@fb.com"], "authors": ["Carl-Johann Simon-Gabriel", "Yann Ollivier", "L\u00e9on Bottou", "Bernhard Sch\u00f6lkopf", "David Lopez-Paz"], "TL;DR": "Neural nets have large gradients by design; that makes them adversarially vulnerable.", "pdf": "/pdf/ee82a3413f64426b3c9095ba85a6738aeeecfe6c.pdf", "paperhash": "simongabriel|adversarial_vulnerability_of_neural_networks_increases_with_input_dimension", "_bibtex": "@misc{\nsimon-gabriel2019adversarial,\ntitle={Adversarial Vulnerability of Neural Networks Increases with Input Dimension},\nauthor={Carl-Johann Simon-Gabriel and Yann Ollivier and L\u00e9on Bottou and Bernhard Sch\u00f6lkopf and David Lopez-Paz},\nyear={2019},\nurl={https://openreview.net/forum?id=H1MzKs05F7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper426/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621607730, "tddate": null, "super": null, "final": null, "reply": {"forum": "H1MzKs05F7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper426/Authors", "ICLR.cc/2019/Conference/Paper426/Reviewers", "ICLR.cc/2019/Conference/Paper426/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper426/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper426/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper426/Authors|ICLR.cc/2019/Conference/Paper426/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper426/Reviewers", "ICLR.cc/2019/Conference/Paper426/Authors", "ICLR.cc/2019/Conference/Paper426/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621607730}}}, {"id": "H1gaUtVATX", "original": null, "number": 3, "cdate": 1542502741240, "ddate": null, "tcdate": 1542502741240, "tmdate": 1542573078362, "tddate": null, "forum": "H1MzKs05F7", "replyto": "HkxbjTl6TQ", "invitation": "ICLR.cc/2019/Conference/-/Paper426/Official_Comment", "content": {"title": "My score holds regardless of the motivation question, here are my technical questions/concerns.", "comment": "The experiments for adversarially trained models in [1] directly contradict the title of this paper. The models trained on the higher dimensional space are more robust (see Figure 4 b.). Can the authors comment on this? My understanding is that for some settings of the weights you can show a bound such as is discussed in the paper, but there are other settings (perhaps even initializations) of the weights for which the conclusion will not hold. To me this suggests that a more appropriate title would be \u201cimproper initialization of neural networks can cause sensitivity to small perturbations\". However, fixing the initialization seems unlikely to buy us much more than what adversarial training achieves, and the experiments in [1] suggest to me the conclusion of this work is limited in scope. Indeed, even adversarially trained models are still sensitive to \"small\" perturbations, only the epsilon at which they are sensitive to increases slightly.\n\nAs you mention, many prior works have explored gradient penalties as a way to increase robustness, and some have perhaps helped a little bit as an adversarial defense, but we hit a limit as we increase the epsilon considered for the perturbations, and its not clear whether or not this can improve upon adversarially trained models. Because of this, it\u2019s not clear to me what actionable insights we can conclude from this work, and how this can be used to improve upon the current SOTA. In fact, it was found that adversarial training eventually gives robustness to the training set, but this robustness does not generalize to the test set [2]. For these models, the gradients are well behaved local to the training points (and thus any gradient based loss function will be minimized for the training points) but the gradients aren\u2019t well behaved for new iid samples from the data distribution. \n\nFurthermore, in [2] it was shown that there is no learning algorithm can become robust to small perturbations, unless that model is trained on significantly more data. So at least for the synthetic data distribution they consider there is no  data independent initialization scheme that can achieve robustness.\n\nMinor nit: You might be able to remove the unrealistic assumption H1 from Theorem 4 by considering the theory from [3].\n\n1. https://arxiv.org/abs/1809.02104\n2. https://arxiv.org/abs/1804.11285\n3. https://arxiv.org/abs/1611.01232\n\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper426/AnonReviewer3"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper426/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper426/AnonReviewer3", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial Vulnerability of Neural Networks Increases with Input Dimension", "abstract": "Over the past four years, neural networks have been proven vulnerable to adversarial images: targeted but imperceptible image perturbations lead to drastically different predictions. We show that adversarial vulnerability increases with the gradients of the training objective when viewed as a function of the inputs. For most current network architectures, we prove that the L1-norm of these gradients grows as the square root of the input size. These nets therefore become increasingly vulnerable with growing image size. Our proofs rely on the network\u2019s weight distribution at initialization, but extensive experiments confirm that our conclusions still hold after usual training.", "keywords": ["adversarial vulnerability", "neural networks", "gradients", "FGSM", "adversarial data-augmentation", "gradient regularization", "robust optimization"], "authorids": ["cjsimon@tuebingen.mpg.de", "yol@fb.com", "leon@bottou.org", "bs@tuebingen.mpg.de", "dlp@fb.com"], "authors": ["Carl-Johann Simon-Gabriel", "Yann Ollivier", "L\u00e9on Bottou", "Bernhard Sch\u00f6lkopf", "David Lopez-Paz"], "TL;DR": "Neural nets have large gradients by design; that makes them adversarially vulnerable.", "pdf": "/pdf/ee82a3413f64426b3c9095ba85a6738aeeecfe6c.pdf", "paperhash": "simongabriel|adversarial_vulnerability_of_neural_networks_increases_with_input_dimension", "_bibtex": "@misc{\nsimon-gabriel2019adversarial,\ntitle={Adversarial Vulnerability of Neural Networks Increases with Input Dimension},\nauthor={Carl-Johann Simon-Gabriel and Yann Ollivier and L\u00e9on Bottou and Bernhard Sch\u00f6lkopf and David Lopez-Paz},\nyear={2019},\nurl={https://openreview.net/forum?id=H1MzKs05F7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper426/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621607730, "tddate": null, "super": null, "final": null, "reply": {"forum": "H1MzKs05F7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper426/Authors", "ICLR.cc/2019/Conference/Paper426/Reviewers", "ICLR.cc/2019/Conference/Paper426/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper426/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper426/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper426/Authors|ICLR.cc/2019/Conference/Paper426/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper426/Reviewers", "ICLR.cc/2019/Conference/Paper426/Authors", "ICLR.cc/2019/Conference/Paper426/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621607730}}}, {"id": "B1e9VFERpQ", "original": null, "number": 2, "cdate": 1542502706077, "ddate": null, "tcdate": 1542502706077, "tmdate": 1542534180480, "tddate": null, "forum": "H1MzKs05F7", "replyto": "HkxbjTl6TQ", "invitation": "ICLR.cc/2019/Conference/-/Paper426/Official_Comment", "content": {"title": "Separate thread to discuss motivation", "comment": "I'm creating a separate thread to discuss motivation, I'm willing to evaluate this paper on the technical aspects but motivation is important and the authors of this work could have asked a very related and better motivated question and written essentially the same paper.\n\nWhy focus on small lp perturbations instead of studying the broader phenomenon that convolutional neural networks significantly underperform humans when classifying distorted images: https://arxiv.org/abs/1705.02498, https://openreview.net/forum?id=HJz6tiCqYm . The fact that models are not perfect in the presence of random image corruptions is concerning as it implies that:\n\na. They are not even average case invariant to moderate simple perturbations.\nb. They depend on their input in ways that we would not expect/want them to.\nc. There are security vulnerabilities that will affect classifiers in many realistic deployed settings, say a street sign classifier misclassifying an input because it is a rainy day. \nd. We will never be robust in worst-case settings until we are first robust in the average case.\n\nMoreover it has been known for some time that the sensitivity of models to small l2 perturbations is the same phenomenon as the sensitivity of models in the presence of large average case corruptions, see this 2016 NIPS paper: https://papers.nips.cc/paper/6331-robustness-of-classifiers-from-adversarial-to-random-noise. So your paper is essentially studying the same phenomenon that I am recommending you study, you have just chosen an odd, difficult to motivate, and difficult to evaluate metric for measuring the robustness of image classifiers. If you find small worst case perturbations surprising and interesting, but moderate average case perturbations not surprising or interesting, then I can help clarify the connection. \n\nOverall, my position is not so much that lp adversarial examples are uninteresting, it's just every adversarial example paper that only focuses on lp perturbations rather than studying model generalization in non-iid settings is artificially limiting the impact and scope of their work."}, "signatures": ["ICLR.cc/2019/Conference/Paper426/AnonReviewer3"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper426/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper426/AnonReviewer3", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial Vulnerability of Neural Networks Increases with Input Dimension", "abstract": "Over the past four years, neural networks have been proven vulnerable to adversarial images: targeted but imperceptible image perturbations lead to drastically different predictions. We show that adversarial vulnerability increases with the gradients of the training objective when viewed as a function of the inputs. For most current network architectures, we prove that the L1-norm of these gradients grows as the square root of the input size. These nets therefore become increasingly vulnerable with growing image size. Our proofs rely on the network\u2019s weight distribution at initialization, but extensive experiments confirm that our conclusions still hold after usual training.", "keywords": ["adversarial vulnerability", "neural networks", "gradients", "FGSM", "adversarial data-augmentation", "gradient regularization", "robust optimization"], "authorids": ["cjsimon@tuebingen.mpg.de", "yol@fb.com", "leon@bottou.org", "bs@tuebingen.mpg.de", "dlp@fb.com"], "authors": ["Carl-Johann Simon-Gabriel", "Yann Ollivier", "L\u00e9on Bottou", "Bernhard Sch\u00f6lkopf", "David Lopez-Paz"], "TL;DR": "Neural nets have large gradients by design; that makes them adversarially vulnerable.", "pdf": "/pdf/ee82a3413f64426b3c9095ba85a6738aeeecfe6c.pdf", "paperhash": "simongabriel|adversarial_vulnerability_of_neural_networks_increases_with_input_dimension", "_bibtex": "@misc{\nsimon-gabriel2019adversarial,\ntitle={Adversarial Vulnerability of Neural Networks Increases with Input Dimension},\nauthor={Carl-Johann Simon-Gabriel and Yann Ollivier and L\u00e9on Bottou and Bernhard Sch\u00f6lkopf and David Lopez-Paz},\nyear={2019},\nurl={https://openreview.net/forum?id=H1MzKs05F7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper426/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621607730, "tddate": null, "super": null, "final": null, "reply": {"forum": "H1MzKs05F7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper426/Authors", "ICLR.cc/2019/Conference/Paper426/Reviewers", "ICLR.cc/2019/Conference/Paper426/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper426/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper426/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper426/Authors|ICLR.cc/2019/Conference/Paper426/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper426/Reviewers", "ICLR.cc/2019/Conference/Paper426/Authors", "ICLR.cc/2019/Conference/Paper426/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621607730}}}, {"id": "HkxbjTl6TQ", "original": null, "number": 1, "cdate": 1542421913386, "ddate": null, "tcdate": 1542421913386, "tmdate": 1542421913386, "tddate": null, "forum": "H1MzKs05F7", "replyto": "rylBpNERnm", "invitation": "ICLR.cc/2019/Conference/-/Paper426/Official_Comment", "content": {"title": "The criticism of studying L_p perturbations is unjustified.", "comment": "I understand and agree with the argument that robustness to small L_p perturbations is by no means a meaningful security guarantee. Whether this area of research is relevant to ML security is a topic of active debate that is beyond the scope of this review. However, understanding the robustness of models to small L_p perturbations is important for a variety of reasons. I will try and outline some of them below.\n\nClearly, the vulnerability of modern ML classifiers to imperceptible perturbations is concerning as it implies that:\na. They are not worst-case invariant to small, simple perturbations.\nb. They depend on their input in ways that we would not expect/want them to.\n(c. There could be security vulnerabilities that would evade human supervision.)\n\nSo the natural question to ask is: \"Are L_p adversarial examples inevitable? If so, for which models/datasets?\"\nWe  still do not know the answers to these questions. \n-- If this vulnerability is indeed inherent, then what are meaningful notions of worst-case invariances that our classifiers should satisfy? Even then, does our partial progress towards L_p robustness help us develop tools for different problems?\n-- If this is simply a limitation of our current models and methods, then we will eventually be able to create L_p-robust classifiers for large datasets. This would help us better understand how to enforce invariances to our model. We could then start working towards broader families of perturbations that we want to be robust to. But before any of this happens we need to understand if we can at least solve the (conceptually) very simple problem of \"Can we be robust to small Lp norms?\". Moreover, if we construct models that are Lp robust, are these models more useful for standard tasks in some way? \n\nWe could have a very lengthy discussion about the topic, but this is clearly not the right place for that. Arguably, this topic of research is of interest to a sizeable part of the ICLR community. I would thus encourage the reviewer to focus on the technical content of the paper and let the AC decide on whether L_p robustness is a topic of interest for ICLR.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper426/AnonReviewer4"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper426/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper426/AnonReviewer4", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial Vulnerability of Neural Networks Increases with Input Dimension", "abstract": "Over the past four years, neural networks have been proven vulnerable to adversarial images: targeted but imperceptible image perturbations lead to drastically different predictions. We show that adversarial vulnerability increases with the gradients of the training objective when viewed as a function of the inputs. For most current network architectures, we prove that the L1-norm of these gradients grows as the square root of the input size. These nets therefore become increasingly vulnerable with growing image size. Our proofs rely on the network\u2019s weight distribution at initialization, but extensive experiments confirm that our conclusions still hold after usual training.", "keywords": ["adversarial vulnerability", "neural networks", "gradients", "FGSM", "adversarial data-augmentation", "gradient regularization", "robust optimization"], "authorids": ["cjsimon@tuebingen.mpg.de", "yol@fb.com", "leon@bottou.org", "bs@tuebingen.mpg.de", "dlp@fb.com"], "authors": ["Carl-Johann Simon-Gabriel", "Yann Ollivier", "L\u00e9on Bottou", "Bernhard Sch\u00f6lkopf", "David Lopez-Paz"], "TL;DR": "Neural nets have large gradients by design; that makes them adversarially vulnerable.", "pdf": "/pdf/ee82a3413f64426b3c9095ba85a6738aeeecfe6c.pdf", "paperhash": "simongabriel|adversarial_vulnerability_of_neural_networks_increases_with_input_dimension", "_bibtex": "@misc{\nsimon-gabriel2019adversarial,\ntitle={Adversarial Vulnerability of Neural Networks Increases with Input Dimension},\nauthor={Carl-Johann Simon-Gabriel and Yann Ollivier and L\u00e9on Bottou and Bernhard Sch\u00f6lkopf and David Lopez-Paz},\nyear={2019},\nurl={https://openreview.net/forum?id=H1MzKs05F7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper426/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621607730, "tddate": null, "super": null, "final": null, "reply": {"forum": "H1MzKs05F7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper426/Authors", "ICLR.cc/2019/Conference/Paper426/Reviewers", "ICLR.cc/2019/Conference/Paper426/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper426/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper426/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper426/Authors|ICLR.cc/2019/Conference/Paper426/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper426/Reviewers", "ICLR.cc/2019/Conference/Paper426/Authors", "ICLR.cc/2019/Conference/Paper426/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621607730}}}, {"id": "rylBpNERnm", "original": null, "number": 3, "cdate": 1541452989295, "ddate": null, "tcdate": 1541452989295, "tmdate": 1541534005631, "tddate": null, "forum": "H1MzKs05F7", "replyto": "H1MzKs05F7", "invitation": "ICLR.cc/2019/Conference/-/Paper426/Official_Review", "content": {"title": "Considered question seems poorly motivated, significance of analysis and conclusions yet to be demonstrated", "review": "This paper argues that adversarial vulnerability of neural networks increases with input dimension. Theoretical and empirical evidence are given which connect the l_p norm of the gradient of the training objective with the existence of small-worst case l_q perturbations. This connection is made by assuming that the learned function is well approximated by a linear function local to the sampled input x. By making assumptions on the initialization scheme for some simple architectures, the authors show that the l_p norm of the gradient for randomly initialized network will be large, and provide empirical evidence that these assumptions hold after training. These assumptions imply bounds on the typical magnitude of the gradient of the loss with respect to a single input coordinate, this then implies that the overall gradient norm will depend on the input dimension.\n\nI found this paper well written. The mathematical assumptions are presented in a clear, easy to understand manner. Also high level intuition is given around their main theorems which help the reader understand the main ideas. However, I have a  number of concerns about this work.\n\nThe first is, I do not buy the motivation for studying the \"phenomenon\" of small worst-case l_p perturbations. I realize this statement applies to a large body of literature, but since the publication of  [1] we are still lacking concrete motivating scenarios for the l_p action space. I would encourage the authors instead to ask the closely related but more general question of how we can improve model generalization outside the natural distribution of images, such as generalization in the presence of commonly occurring image corruptions [2]. It's possible that the analysis in this work could better our understanding model generalization in the presence of different image corruptions, indeed by making similar linearity assumptions as considered in this work, test error in additive Gaussian noise can be linked with distance to the decision boundary [3,4]. However, this particular question was not explored in this work.\n\nSecond, the work is one of many to relate the norm of the gradient with adversarial robustness (for example, this has been proposed as a defense mechanism in [5,6]). I also suspect that the main theorem relating gradient norm to initialization should easily follow for more general settings using the mean field theory developed by [7,8] (this would be particularly useful for removing assumption H1, which assumes the ReLU activation is a random variable independent of the weights). Overall, I don't see how gradient norms explain why statistical classifiers make mistakes, particularly for more realistic attacker action spaces [9]. Even for \"small\" l_p adversarial examples there seem to be limitations as to how much gradient norms can explain the phenomenon --- for example even max margin classifiers such as SVM's have \"adversarial examples\". Furthermore, adversarial training has been shown to reach a point where the model is \"robust\" locally to training points but this robustness does not generalize to the points in the test set [10]. In fact, for the synthetic data distributions considered in [10], it's proven that no learning algorithm can achieve robustness given insufficient training data.\n\nFinally, the main conclusion of this work \"adversarial vulnerability of neural networks increases with input dimension\" is an overly general statement which needs a much more nuanced view. While experiments shown in [11] support this conclusion for naturally trained networks, it is shown that when adversarial training is applied the model is more robust when the input dimension is higher (see Figure 4 a. and b.). Perhaps the assumptions for Theorem 4 are violated for these adversarially trained models. \n\n1. https://arxiv.org/abs/1807.06732\n2. https://arxiv.org/abs/1807.01697\n3. https://arxiv.org/abs/1608.08967\n4. https://openreview.net/forum?id=S1xoy3CcYX&noteId=BklKxJBF57.\n5. https://arxiv.org/abs/1704.08847\n6. https://arxiv.org/abs/1608.07690\n7. https://arxiv.org/abs/1611.01232\n8. https://arxiv.org/abs/1806.05393\n9. https://arxiv.org/abs/1712.09665\n10. https://arxiv.org/abs/1804.11285\n11. https://arxiv.org/pdf/1809.02104.pdf", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper426/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial Vulnerability of Neural Networks Increases with Input Dimension", "abstract": "Over the past four years, neural networks have been proven vulnerable to adversarial images: targeted but imperceptible image perturbations lead to drastically different predictions. We show that adversarial vulnerability increases with the gradients of the training objective when viewed as a function of the inputs. For most current network architectures, we prove that the L1-norm of these gradients grows as the square root of the input size. These nets therefore become increasingly vulnerable with growing image size. Our proofs rely on the network\u2019s weight distribution at initialization, but extensive experiments confirm that our conclusions still hold after usual training.", "keywords": ["adversarial vulnerability", "neural networks", "gradients", "FGSM", "adversarial data-augmentation", "gradient regularization", "robust optimization"], "authorids": ["cjsimon@tuebingen.mpg.de", "yol@fb.com", "leon@bottou.org", "bs@tuebingen.mpg.de", "dlp@fb.com"], "authors": ["Carl-Johann Simon-Gabriel", "Yann Ollivier", "L\u00e9on Bottou", "Bernhard Sch\u00f6lkopf", "David Lopez-Paz"], "TL;DR": "Neural nets have large gradients by design; that makes them adversarially vulnerable.", "pdf": "/pdf/ee82a3413f64426b3c9095ba85a6738aeeecfe6c.pdf", "paperhash": "simongabriel|adversarial_vulnerability_of_neural_networks_increases_with_input_dimension", "_bibtex": "@misc{\nsimon-gabriel2019adversarial,\ntitle={Adversarial Vulnerability of Neural Networks Increases with Input Dimension},\nauthor={Carl-Johann Simon-Gabriel and Yann Ollivier and L\u00e9on Bottou and Bernhard Sch\u00f6lkopf and David Lopez-Paz},\nyear={2019},\nurl={https://openreview.net/forum?id=H1MzKs05F7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper426/Official_Review", "cdate": 1542234464328, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "H1MzKs05F7", "replyto": "H1MzKs05F7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper426/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335721423, "tmdate": 1552335721423, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper426/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "H1eawIzc3X", "original": null, "number": 2, "cdate": 1541183076622, "ddate": null, "tcdate": 1541183076622, "tmdate": 1541534005383, "tddate": null, "forum": "H1MzKs05F7", "replyto": "H1MzKs05F7", "invitation": "ICLR.cc/2019/Conference/-/Paper426/Official_Review", "content": {"title": "A solid contribution to the study of adversarial examples.", "review": "The authors provide a compelling theoretical explanation for a large class of adversarial examples.  While this explanation (rooted in the norm of gradients of neural networks being the culprit for the existence of adversarial examples) is not new, they unify several old perspectives, and convincingly argue for genuinely new scaling relationships (i.e. \\sqrt(d) versus linear in d scaling of sensitivity to adversarial perturbations versus input size). They prove a number of theorems relating these scaling relationships to a broad swathe of relevant model architectures, and provide thorough empirical evidence of their work.\n\nI can honestly find very little to complain about in this work--the prose is clear, and the proofs are correct as far as I can tell (though I found Figure 4 in the appendix (left panel) to not be hugely compelling.  More data here would be great!)\n\nAs much of the analysis hinges on the particularities of the weight distribution at initialization, could the authors comment on possible defenses to adversarial attack by altering this weight distribution? (By, for example, imposing that the average value must grow like 1/d)?", "rating": "9: Top 15% of accepted papers, strong accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper426/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial Vulnerability of Neural Networks Increases with Input Dimension", "abstract": "Over the past four years, neural networks have been proven vulnerable to adversarial images: targeted but imperceptible image perturbations lead to drastically different predictions. We show that adversarial vulnerability increases with the gradients of the training objective when viewed as a function of the inputs. For most current network architectures, we prove that the L1-norm of these gradients grows as the square root of the input size. These nets therefore become increasingly vulnerable with growing image size. Our proofs rely on the network\u2019s weight distribution at initialization, but extensive experiments confirm that our conclusions still hold after usual training.", "keywords": ["adversarial vulnerability", "neural networks", "gradients", "FGSM", "adversarial data-augmentation", "gradient regularization", "robust optimization"], "authorids": ["cjsimon@tuebingen.mpg.de", "yol@fb.com", "leon@bottou.org", "bs@tuebingen.mpg.de", "dlp@fb.com"], "authors": ["Carl-Johann Simon-Gabriel", "Yann Ollivier", "L\u00e9on Bottou", "Bernhard Sch\u00f6lkopf", "David Lopez-Paz"], "TL;DR": "Neural nets have large gradients by design; that makes them adversarially vulnerable.", "pdf": "/pdf/ee82a3413f64426b3c9095ba85a6738aeeecfe6c.pdf", "paperhash": "simongabriel|adversarial_vulnerability_of_neural_networks_increases_with_input_dimension", "_bibtex": "@misc{\nsimon-gabriel2019adversarial,\ntitle={Adversarial Vulnerability of Neural Networks Increases with Input Dimension},\nauthor={Carl-Johann Simon-Gabriel and Yann Ollivier and L\u00e9on Bottou and Bernhard Sch\u00f6lkopf and David Lopez-Paz},\nyear={2019},\nurl={https://openreview.net/forum?id=H1MzKs05F7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper426/Official_Review", "cdate": 1542234464328, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "H1MzKs05F7", "replyto": "H1MzKs05F7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper426/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335721423, "tmdate": 1552335721423, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper426/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "BJgK8oPUh7", "original": null, "number": 1, "cdate": 1540942673390, "ddate": null, "tcdate": 1540942673390, "tmdate": 1541534005174, "tddate": null, "forum": "H1MzKs05F7", "replyto": "H1MzKs05F7", "invitation": "ICLR.cc/2019/Conference/-/Paper426/Official_Review", "content": {"title": "interesting work but with limited applicability and significance demonstrated", "review": "This paper analyzes the relationship between \"adversarial vulnerability\" with input dimensionality of neural network. The paper proves that, under certain assumptions, as the input dimensionality increases, neural networks exhibit increasingly large gradients thus are more adversarially vulnerable. Experiments were done on neural networks trained by penalizing input gradients and FGSM-adversarial training. Similar trends on vulnerability vs dimensionality are found.\n\nThe paper is clearly written and easy to follow. I appreciate that the authors also clearly stated the limitation of the theoretical analysis.\n\nThe theoretical analyses on vulnerability and dimensionality is novel and provide some insights. But it is unlikely such analysis is significant There are a few reasons:\n- This analysis only seems to work for \"well-behaved\" models. For models with gradient masking, obfuscated gradients or even non-differentiable models, it is not clear that how this will apply. (and I appreciate that the authors also acknowledge this in the paper.) It is unclear how this specific gradient based analysis can help the understanding of the adversarial perturbation phenomena. After all, the first order Taylor expansion argument on top of randomly initialized weights is oversimplifying the complicated problem.\n- One very important special case of the point above: the analysis probably cannot cover the  adversarially PGD trained models [MMS+17] and the certifiably robust ones. Such models may have small gradients inside the box constraint, but can have large gradients between different classes.\n\n\nOn the empirical results, the authors made a few interesting observations, for example the close correspondence between \"Adv Train\" and \"Grad Regu\" models. \nMy concern is that the experiments were done on a narrow range of models, which only have \"weak\" adversarial training / defenses.\nAdversarial robustness is hard to achieve. What matters the most is \"why the strongest model is still not robust?\" not \"why some weak models are not robust?\" \nIt is especially worrisome to me that the paper does not cover the adversarially-augmented training based iterative attacks, e.g. PGD TRAINED models [MMS+17] which is the SOTA on MNIST/CIFAR10 L_\\infty robustness benchmark.\nWithout comprehensive analyses on SOTA robust models, it is hard to justify the validity of the theoretical analysis in this paper, and the conclusions made by the paper.\nFor example, re: the last sentence in the conclusion: \"They hence suggest to tackle adversarial vulnerability by designing new architectures (or new architectural building blocks) rather than by new regularization techniques.\" The reasoning is not obvious to me given the current evidence shown in the paper.\n\n[MMS+17] Madry A, Makelov A, Schmidt L, Tsipras D, Vladu A. Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper426/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial Vulnerability of Neural Networks Increases with Input Dimension", "abstract": "Over the past four years, neural networks have been proven vulnerable to adversarial images: targeted but imperceptible image perturbations lead to drastically different predictions. We show that adversarial vulnerability increases with the gradients of the training objective when viewed as a function of the inputs. For most current network architectures, we prove that the L1-norm of these gradients grows as the square root of the input size. These nets therefore become increasingly vulnerable with growing image size. Our proofs rely on the network\u2019s weight distribution at initialization, but extensive experiments confirm that our conclusions still hold after usual training.", "keywords": ["adversarial vulnerability", "neural networks", "gradients", "FGSM", "adversarial data-augmentation", "gradient regularization", "robust optimization"], "authorids": ["cjsimon@tuebingen.mpg.de", "yol@fb.com", "leon@bottou.org", "bs@tuebingen.mpg.de", "dlp@fb.com"], "authors": ["Carl-Johann Simon-Gabriel", "Yann Ollivier", "L\u00e9on Bottou", "Bernhard Sch\u00f6lkopf", "David Lopez-Paz"], "TL;DR": "Neural nets have large gradients by design; that makes them adversarially vulnerable.", "pdf": "/pdf/ee82a3413f64426b3c9095ba85a6738aeeecfe6c.pdf", "paperhash": "simongabriel|adversarial_vulnerability_of_neural_networks_increases_with_input_dimension", "_bibtex": "@misc{\nsimon-gabriel2019adversarial,\ntitle={Adversarial Vulnerability of Neural Networks Increases with Input Dimension},\nauthor={Carl-Johann Simon-Gabriel and Yann Ollivier and L\u00e9on Bottou and Bernhard Sch\u00f6lkopf and David Lopez-Paz},\nyear={2019},\nurl={https://openreview.net/forum?id=H1MzKs05F7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper426/Official_Review", "cdate": 1542234464328, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "H1MzKs05F7", "replyto": "H1MzKs05F7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper426/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335721423, "tmdate": 1552335721423, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper426/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 28}