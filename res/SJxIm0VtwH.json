{"notes": [{"id": "SJxIm0VtwH", "original": "BkxvjKrOvr", "number": 1038, "cdate": 1569439261720, "ddate": null, "tcdate": 1569439261720, "tmdate": 1608862910013, "tddate": null, "forum": "SJxIm0VtwH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Towards Better Understanding of Adaptive Gradient Algorithms in Generative Adversarial Nets", "authors": ["Mingrui Liu", "Youssef Mroueh", "Jerret Ross", "Wei Zhang", "Xiaodong Cui", "Payel Das", "Tianbao Yang"], "authorids": ["mingrui-liu@uiowa.edu", "mroueh@us.ibm.com", "rossja@us.ibm.com", "weiz@us.ibm.com", "cuix@us.ibm.com", "daspa@us.ibm.com", "tianbao-yang@uiowa.edu"], "keywords": ["Generative Adversarial Nets", "Adaptive Gradient Algorithms"], "TL;DR": "This paper provides novel analysis of adaptive gradient algorithms for solving non-convex non-concave min-max problems as GANs, and explains the reason why adaptive gradient methods outperform its non-adaptive counterparts by empirical studies.", "abstract": "Adaptive gradient algorithms perform gradient-based updates using the history of gradients and are ubiquitous in training deep neural networks. While adaptive gradient methods theory is well understood for minimization problems, the underlying factors driving their empirical success in min-max problems such as GANs remain unclear. In this paper, we aim at bridging  this gap from both theoretical and empirical perspectives. First, we analyze a variant of Optimistic Stochastic Gradient (OSG) proposed in~\\citep{daskalakis2017training} for solving a class of non-convex non-concave min-max problem and establish $O(\\epsilon^{-4})$ complexity for finding $\\epsilon$-first-order stationary point, in which the algorithm only requires invoking one stochastic first-order oracle while enjoying state-of-the-art iteration complexity achieved by stochastic extragradient method by~\\citep{iusem2017extragradient}. Then we propose an adaptive variant of OSG named Optimistic Adagrad (OAdagrad) and reveal an \\emph{improved} adaptive complexity $O\\left(\\epsilon^{-\\frac{2}{1-\\alpha}}\\right)$, where $\\alpha$ characterizes the growth rate of the cumulative stochastic gradient and $0\\leq \\alpha\\leq 1/2$. To the best of our knowledge, this is the first work for establishing adaptive complexity in non-convex non-concave min-max optimization. Empirically, our experiments show that indeed adaptive gradient algorithms outperform their non-adaptive counterparts in GAN training. Moreover, this observation can be explained by the slow growth rate of the cumulative stochastic gradient, as observed empirically.", "pdf": "/pdf/1dda48e5d305320db2272f8621cff728afde476b.pdf", "paperhash": "liu|towards_better_understanding_of_adaptive_gradient_algorithms_in_generative_adversarial_nets", "_bibtex": "@inproceedings{\nLiu2020Towards,\ntitle={Towards Better Understanding of Adaptive Gradient Algorithms in Generative Adversarial Nets},\nauthor={Mingrui Liu and Youssef Mroueh and Jerret Ross and Wei Zhang and Xiaodong Cui and Payel Das and Tianbao Yang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJxIm0VtwH}\n}", "original_pdf": "/attachment/37fc9fc009ff2d927a743d72c2a80e5b91c5c0d5.pdf"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 11, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "PyfLO-MWu", "original": null, "number": 1, "cdate": 1576798712905, "ddate": null, "tcdate": 1576798712905, "tmdate": 1576800923535, "tddate": null, "forum": "SJxIm0VtwH", "replyto": "SJxIm0VtwH", "invitation": "ICLR.cc/2020/Conference/Paper1038/-/Decision", "content": {"decision": "Accept (Poster)", "comment": "This work proposes a new adaptive method for solving certain min-max problems.\n\nThe reviewers all appreciated the work and most of their concerns were addressed in the rebuttal. Given the current interest in both adaptive methods and min-max problems, this work is suited for publication at ICLR.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards Better Understanding of Adaptive Gradient Algorithms in Generative Adversarial Nets", "authors": ["Mingrui Liu", "Youssef Mroueh", "Jerret Ross", "Wei Zhang", "Xiaodong Cui", "Payel Das", "Tianbao Yang"], "authorids": ["mingrui-liu@uiowa.edu", "mroueh@us.ibm.com", "rossja@us.ibm.com", "weiz@us.ibm.com", "cuix@us.ibm.com", "daspa@us.ibm.com", "tianbao-yang@uiowa.edu"], "keywords": ["Generative Adversarial Nets", "Adaptive Gradient Algorithms"], "TL;DR": "This paper provides novel analysis of adaptive gradient algorithms for solving non-convex non-concave min-max problems as GANs, and explains the reason why adaptive gradient methods outperform its non-adaptive counterparts by empirical studies.", "abstract": "Adaptive gradient algorithms perform gradient-based updates using the history of gradients and are ubiquitous in training deep neural networks. While adaptive gradient methods theory is well understood for minimization problems, the underlying factors driving their empirical success in min-max problems such as GANs remain unclear. In this paper, we aim at bridging  this gap from both theoretical and empirical perspectives. First, we analyze a variant of Optimistic Stochastic Gradient (OSG) proposed in~\\citep{daskalakis2017training} for solving a class of non-convex non-concave min-max problem and establish $O(\\epsilon^{-4})$ complexity for finding $\\epsilon$-first-order stationary point, in which the algorithm only requires invoking one stochastic first-order oracle while enjoying state-of-the-art iteration complexity achieved by stochastic extragradient method by~\\citep{iusem2017extragradient}. Then we propose an adaptive variant of OSG named Optimistic Adagrad (OAdagrad) and reveal an \\emph{improved} adaptive complexity $O\\left(\\epsilon^{-\\frac{2}{1-\\alpha}}\\right)$, where $\\alpha$ characterizes the growth rate of the cumulative stochastic gradient and $0\\leq \\alpha\\leq 1/2$. To the best of our knowledge, this is the first work for establishing adaptive complexity in non-convex non-concave min-max optimization. Empirically, our experiments show that indeed adaptive gradient algorithms outperform their non-adaptive counterparts in GAN training. Moreover, this observation can be explained by the slow growth rate of the cumulative stochastic gradient, as observed empirically.", "pdf": "/pdf/1dda48e5d305320db2272f8621cff728afde476b.pdf", "paperhash": "liu|towards_better_understanding_of_adaptive_gradient_algorithms_in_generative_adversarial_nets", "_bibtex": "@inproceedings{\nLiu2020Towards,\ntitle={Towards Better Understanding of Adaptive Gradient Algorithms in Generative Adversarial Nets},\nauthor={Mingrui Liu and Youssef Mroueh and Jerret Ross and Wei Zhang and Xiaodong Cui and Payel Das and Tianbao Yang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJxIm0VtwH}\n}", "original_pdf": "/attachment/37fc9fc009ff2d927a743d72c2a80e5b91c5c0d5.pdf"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "SJxIm0VtwH", "replyto": "SJxIm0VtwH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795713936, "tmdate": 1576800263666, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1038/-/Decision"}}}, {"id": "BJenu0PTFB", "original": null, "number": 2, "cdate": 1571810932136, "ddate": null, "tcdate": 1571810932136, "tmdate": 1574490998032, "tddate": null, "forum": "SJxIm0VtwH", "replyto": "SJxIm0VtwH", "invitation": "ICLR.cc/2020/Conference/Paper1038/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #2", "review": "This paper proposed a new algorithm (Optimistic Stochastic Gradient) for solving a class of non-convex non-concave min-max problem. The convergence theory is established for finding first order stationary point. The authors also proposed an adaptive variant of the proposed algorithm, called OAdagrad and showed an improved adaptive complexity.\n\n- It is not immediately clear to me why the update of Algorithm 1 becomes the algorithm in (Daskalaskis et al. 2017) when there is no constraint. Can the authors further explain this clearly? The two variables in (Daskalaskis et al. 2017) are updated with different signs, but here  z=(u,v)  (I assume, the authors should clearly define z too), it seems that u and v are updated with the same sign?\n\n- In terms of the theoretical contributions, aside from the presenting theorems, can the authors also comment on what is the key point to achieve the derived results? For example, in Algorithm 1, the only difference from stochastic extragradient method is T(z_{k-1}) instead of T(x_{k-1}), and the theorem achieves the same iteration complexity with weaker assumption.  Is this because the replacement term, or it is the proving technique improvement that can also be applied to stochastic extragradient method?\n\n- For Algorithm 2, why there is no projection operator like in Algorithm 1? Also, the algorithm design is a bit different from traditional AdaGrad as their H matrix is the historical average of all past gradient square (element-wise). Here the s_i is L_2 norm of historical gradient, there is no averaging (divided by k) operator. Can the authors elaborate on this algorithm design?\n\n- For experiments part, it is better for the authors to compare with more baseline methods such as Optimistic Adam. For Figure 1, can the authors put different batches in different plot so that we can directly compare the performances of different algorithms? It does not make sense that alternating Adam failed in training SA-GAN here but success in the original paper. Can the authors figure it out and provide the comparison on this?\n\nDetailed comments: \n\n- it is better to define m_t in or before Algorithm 1 to make it clear for the readers.\n- In Page 7, comparison with \u2026 paragraph, should be \\beta_1 = 0, \\beta_2 -> 1? If so, it is still a bit different with OAdagrad?\n\n\n======================\nafter the rebuttal\n\nI thank the authors for their response and it addressed most of my concerns.  ", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper1038/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1038/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards Better Understanding of Adaptive Gradient Algorithms in Generative Adversarial Nets", "authors": ["Mingrui Liu", "Youssef Mroueh", "Jerret Ross", "Wei Zhang", "Xiaodong Cui", "Payel Das", "Tianbao Yang"], "authorids": ["mingrui-liu@uiowa.edu", "mroueh@us.ibm.com", "rossja@us.ibm.com", "weiz@us.ibm.com", "cuix@us.ibm.com", "daspa@us.ibm.com", "tianbao-yang@uiowa.edu"], "keywords": ["Generative Adversarial Nets", "Adaptive Gradient Algorithms"], "TL;DR": "This paper provides novel analysis of adaptive gradient algorithms for solving non-convex non-concave min-max problems as GANs, and explains the reason why adaptive gradient methods outperform its non-adaptive counterparts by empirical studies.", "abstract": "Adaptive gradient algorithms perform gradient-based updates using the history of gradients and are ubiquitous in training deep neural networks. While adaptive gradient methods theory is well understood for minimization problems, the underlying factors driving their empirical success in min-max problems such as GANs remain unclear. In this paper, we aim at bridging  this gap from both theoretical and empirical perspectives. First, we analyze a variant of Optimistic Stochastic Gradient (OSG) proposed in~\\citep{daskalakis2017training} for solving a class of non-convex non-concave min-max problem and establish $O(\\epsilon^{-4})$ complexity for finding $\\epsilon$-first-order stationary point, in which the algorithm only requires invoking one stochastic first-order oracle while enjoying state-of-the-art iteration complexity achieved by stochastic extragradient method by~\\citep{iusem2017extragradient}. Then we propose an adaptive variant of OSG named Optimistic Adagrad (OAdagrad) and reveal an \\emph{improved} adaptive complexity $O\\left(\\epsilon^{-\\frac{2}{1-\\alpha}}\\right)$, where $\\alpha$ characterizes the growth rate of the cumulative stochastic gradient and $0\\leq \\alpha\\leq 1/2$. To the best of our knowledge, this is the first work for establishing adaptive complexity in non-convex non-concave min-max optimization. Empirically, our experiments show that indeed adaptive gradient algorithms outperform their non-adaptive counterparts in GAN training. Moreover, this observation can be explained by the slow growth rate of the cumulative stochastic gradient, as observed empirically.", "pdf": "/pdf/1dda48e5d305320db2272f8621cff728afde476b.pdf", "paperhash": "liu|towards_better_understanding_of_adaptive_gradient_algorithms_in_generative_adversarial_nets", "_bibtex": "@inproceedings{\nLiu2020Towards,\ntitle={Towards Better Understanding of Adaptive Gradient Algorithms in Generative Adversarial Nets},\nauthor={Mingrui Liu and Youssef Mroueh and Jerret Ross and Wei Zhang and Xiaodong Cui and Payel Das and Tianbao Yang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJxIm0VtwH}\n}", "original_pdf": "/attachment/37fc9fc009ff2d927a743d72c2a80e5b91c5c0d5.pdf"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SJxIm0VtwH", "replyto": "SJxIm0VtwH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1038/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1038/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575695084737, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1038/Reviewers"], "noninvitees": [], "tcdate": 1570237743307, "tmdate": 1575695084748, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1038/-/Official_Review"}}}, {"id": "r1xHK7B9KB", "original": null, "number": 1, "cdate": 1571603325189, "ddate": null, "tcdate": 1571603325189, "tmdate": 1574132803176, "tddate": null, "forum": "SJxIm0VtwH", "replyto": "SJxIm0VtwH", "invitation": "ICLR.cc/2020/Conference/Paper1038/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #3", "review": "--------------------------\nAfter the revisions made by the authors, my main concerns about the paper have been removed. Therefore, I am raising my score to 6 (weak accept).\n--------------------------\n\nSummary\n\nThe present work is concerned with the development of algorithms for the solution of variational inequalities in the stochastic setting, that is when the gradient computations are corrupted by noise. In this setting, the authors propose a variation of the extragradient method which they call Optimistic Stochastic Gradient (OSG), and show that by using a suitable method of variance reduction, the convergence rate of the algorithm matches the state of the art rate of convergence while relaxing the assumptions on the VI from pseudomonotone to assuming that the associated minty variational inequality has a solution. The authors furthermore introduce an adagrad-version of the same algorithm and show that improved convergence rates can be obtained depending on the growth rate of the cumulative stocastic gradients. An extensive suite of experiments studies the empirical performance of the proposed algorithms and compares them to the commonly used Adam optimizer.\n\nDecision\n\nIn its present form, while some of the contributions of the paper seem to be relevant contributions (theoretical analysis of adagrad version, state of the art performance of optimistic mirror descent), other contributions (some of which are advertised strongly in the abstract) are incremental or implicit in earlier work (optimistic stochastic gradient descent, relaxation from pseudomonotone to minty VI). Furthermore, while the experimental part is detailed, the connection to the theory could arguably be strengthened more. I believe that by more concisely focusing the paper on its innovative aspects while relating it more directly to existing prior work, it could make for a much more valuable contribution to the literature, which is why I vote for rejecting the paper in its present form.\n\nAdditional Detail on decision\n\nNovelty of OSG\nThe authors themselves note that the difference between OSG and OMD of Deskalakis et al is the inclusion of a projection step and the variance reduction by averaging multiple gradient evaluation at each iteration (which corresponds to choosing a different batch size for different iterations). I don't think these modifications are major enough to warrant \"introduction of a new algorithm\". The fact that all experiments seem to be conducted without constraints and constant batch size further strengthens this impression.\n\nRelaxation to Minty VI\nAnother claimed improvement of the paper is relaxing the assumption of pseudomonotonicity to the mere assumption of existence of a variational inequality. However, if I'm not mistaken (please correct me if this assesment is incorrect), the only part where the pseudomonoonicity assumption enters the proof in Iusem et al is on page 36, to prove the last inequality of Equation (105). Here, however, the assumption of a Minty VI could equally be used. Thus, the weakened monotonicity assumption is not related to the use of SGO as opposed to extragradient, which is not apparent from the paper.\n\nSuggestions for revision\n\nI do think that the material can make for a solid paper, but I think it would strengthen rather than weaken the contribution to point out in more detail the close precursors of some of the results in the paper. Notably, rather than inventing a new algorithm (OSG), the paper proposes a way to combine variance reduction with Optimistic mirror descent of Deskalakis et al such as to achieve state of the art convergence rates which so far were only known for the variance reduced extragradient method. Furthermore, it points out that these convergence results (just as in the case of variance reduced extragradient) hold as soon as the associated Minty variational inequality holds true. This latter point would be much stronger, if examples were given that illustrate why this is a meaningful extension. Finally, the paper derives improved rates of an OAdagrad. In my opinion this point should be made more prominent by deemphasizing the part on OSG.\n\nThere is a typo in the definition of pseudomonotonicity.\n\nQuestions for authors\n\n(1) Does figure 1 show iterations or epochs on the x axis? In order to support the theoretical claims, wouldn't it need to show the epochs on the x-axis? Otherwise, a larger batch-size simply corresponds to having access to more calls of the stochastic gradient oracle.\n\n(2) Is there a new technical difficulty to overcome when replacing the extragradient method with the OSG compared to the proof of Iusem et al? If yes, can you give a concise description of it?\n\n(3)It is not \"evident from the definition\" to me, why pseudo-monotonicity implies the existence of a solution of the Minty variational inequality, even though I believe that this is true. Could the authors explain this to me?\n\n(4) The main improvement of OSG over extragradient lies in it needing to compute half the number of gradients per iteration. Is it possible to  explicitly compare the constants in the bounds on the convergence rates of the two algorithms? It seems that this would be required to truly make the case of a reduced complexity of OSG.\n\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper1038/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1038/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards Better Understanding of Adaptive Gradient Algorithms in Generative Adversarial Nets", "authors": ["Mingrui Liu", "Youssef Mroueh", "Jerret Ross", "Wei Zhang", "Xiaodong Cui", "Payel Das", "Tianbao Yang"], "authorids": ["mingrui-liu@uiowa.edu", "mroueh@us.ibm.com", "rossja@us.ibm.com", "weiz@us.ibm.com", "cuix@us.ibm.com", "daspa@us.ibm.com", "tianbao-yang@uiowa.edu"], "keywords": ["Generative Adversarial Nets", "Adaptive Gradient Algorithms"], "TL;DR": "This paper provides novel analysis of adaptive gradient algorithms for solving non-convex non-concave min-max problems as GANs, and explains the reason why adaptive gradient methods outperform its non-adaptive counterparts by empirical studies.", "abstract": "Adaptive gradient algorithms perform gradient-based updates using the history of gradients and are ubiquitous in training deep neural networks. While adaptive gradient methods theory is well understood for minimization problems, the underlying factors driving their empirical success in min-max problems such as GANs remain unclear. In this paper, we aim at bridging  this gap from both theoretical and empirical perspectives. First, we analyze a variant of Optimistic Stochastic Gradient (OSG) proposed in~\\citep{daskalakis2017training} for solving a class of non-convex non-concave min-max problem and establish $O(\\epsilon^{-4})$ complexity for finding $\\epsilon$-first-order stationary point, in which the algorithm only requires invoking one stochastic first-order oracle while enjoying state-of-the-art iteration complexity achieved by stochastic extragradient method by~\\citep{iusem2017extragradient}. Then we propose an adaptive variant of OSG named Optimistic Adagrad (OAdagrad) and reveal an \\emph{improved} adaptive complexity $O\\left(\\epsilon^{-\\frac{2}{1-\\alpha}}\\right)$, where $\\alpha$ characterizes the growth rate of the cumulative stochastic gradient and $0\\leq \\alpha\\leq 1/2$. To the best of our knowledge, this is the first work for establishing adaptive complexity in non-convex non-concave min-max optimization. Empirically, our experiments show that indeed adaptive gradient algorithms outperform their non-adaptive counterparts in GAN training. Moreover, this observation can be explained by the slow growth rate of the cumulative stochastic gradient, as observed empirically.", "pdf": "/pdf/1dda48e5d305320db2272f8621cff728afde476b.pdf", "paperhash": "liu|towards_better_understanding_of_adaptive_gradient_algorithms_in_generative_adversarial_nets", "_bibtex": "@inproceedings{\nLiu2020Towards,\ntitle={Towards Better Understanding of Adaptive Gradient Algorithms in Generative Adversarial Nets},\nauthor={Mingrui Liu and Youssef Mroueh and Jerret Ross and Wei Zhang and Xiaodong Cui and Payel Das and Tianbao Yang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJxIm0VtwH}\n}", "original_pdf": "/attachment/37fc9fc009ff2d927a743d72c2a80e5b91c5c0d5.pdf"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SJxIm0VtwH", "replyto": "SJxIm0VtwH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1038/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1038/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575695084737, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1038/Reviewers"], "noninvitees": [], "tcdate": 1570237743307, "tmdate": 1575695084748, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1038/-/Official_Review"}}}, {"id": "S1gnHVNijr", "original": null, "number": 7, "cdate": 1573762115518, "ddate": null, "tcdate": 1573762115518, "tmdate": 1573762115518, "tddate": null, "forum": "SJxIm0VtwH", "replyto": "H1xBbhmjoB", "invitation": "ICLR.cc/2020/Conference/Paper1038/-/Official_Comment", "content": {"title": "Thank you for your suggestions. ", "comment": "Thank you for pointing it out. We agree with it and have dropped this remark."}, "signatures": ["ICLR.cc/2020/Conference/Paper1038/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1038/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards Better Understanding of Adaptive Gradient Algorithms in Generative Adversarial Nets", "authors": ["Mingrui Liu", "Youssef Mroueh", "Jerret Ross", "Wei Zhang", "Xiaodong Cui", "Payel Das", "Tianbao Yang"], "authorids": ["mingrui-liu@uiowa.edu", "mroueh@us.ibm.com", "rossja@us.ibm.com", "weiz@us.ibm.com", "cuix@us.ibm.com", "daspa@us.ibm.com", "tianbao-yang@uiowa.edu"], "keywords": ["Generative Adversarial Nets", "Adaptive Gradient Algorithms"], "TL;DR": "This paper provides novel analysis of adaptive gradient algorithms for solving non-convex non-concave min-max problems as GANs, and explains the reason why adaptive gradient methods outperform its non-adaptive counterparts by empirical studies.", "abstract": "Adaptive gradient algorithms perform gradient-based updates using the history of gradients and are ubiquitous in training deep neural networks. While adaptive gradient methods theory is well understood for minimization problems, the underlying factors driving their empirical success in min-max problems such as GANs remain unclear. In this paper, we aim at bridging  this gap from both theoretical and empirical perspectives. First, we analyze a variant of Optimistic Stochastic Gradient (OSG) proposed in~\\citep{daskalakis2017training} for solving a class of non-convex non-concave min-max problem and establish $O(\\epsilon^{-4})$ complexity for finding $\\epsilon$-first-order stationary point, in which the algorithm only requires invoking one stochastic first-order oracle while enjoying state-of-the-art iteration complexity achieved by stochastic extragradient method by~\\citep{iusem2017extragradient}. Then we propose an adaptive variant of OSG named Optimistic Adagrad (OAdagrad) and reveal an \\emph{improved} adaptive complexity $O\\left(\\epsilon^{-\\frac{2}{1-\\alpha}}\\right)$, where $\\alpha$ characterizes the growth rate of the cumulative stochastic gradient and $0\\leq \\alpha\\leq 1/2$. To the best of our knowledge, this is the first work for establishing adaptive complexity in non-convex non-concave min-max optimization. Empirically, our experiments show that indeed adaptive gradient algorithms outperform their non-adaptive counterparts in GAN training. Moreover, this observation can be explained by the slow growth rate of the cumulative stochastic gradient, as observed empirically.", "pdf": "/pdf/1dda48e5d305320db2272f8621cff728afde476b.pdf", "paperhash": "liu|towards_better_understanding_of_adaptive_gradient_algorithms_in_generative_adversarial_nets", "_bibtex": "@inproceedings{\nLiu2020Towards,\ntitle={Towards Better Understanding of Adaptive Gradient Algorithms in Generative Adversarial Nets},\nauthor={Mingrui Liu and Youssef Mroueh and Jerret Ross and Wei Zhang and Xiaodong Cui and Payel Das and Tianbao Yang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJxIm0VtwH}\n}", "original_pdf": "/attachment/37fc9fc009ff2d927a743d72c2a80e5b91c5c0d5.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJxIm0VtwH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1038/Authors", "ICLR.cc/2020/Conference/Paper1038/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1038/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1038/Reviewers", "ICLR.cc/2020/Conference/Paper1038/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1038/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1038/Authors|ICLR.cc/2020/Conference/Paper1038/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504162221, "tmdate": 1576860549691, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1038/Authors", "ICLR.cc/2020/Conference/Paper1038/Reviewers", "ICLR.cc/2020/Conference/Paper1038/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1038/-/Official_Comment"}}}, {"id": "H1xBbhmjoB", "original": null, "number": 6, "cdate": 1573759996969, "ddate": null, "tcdate": 1573759996969, "tmdate": 1573759996969, "tddate": null, "forum": "SJxIm0VtwH", "replyto": "H1x0vZUPsB", "invitation": "ICLR.cc/2020/Conference/Paper1038/-/Official_Comment", "content": {"title": "Thank you for the revisions", "comment": "Thank you making the effort to answer my questions and suggestions for revision.\n\nA minor comment at this point: On page 8 of the new version you write \n\"Figure 1 suggests\nthat OAdagrad performs better than OSG and Alternating Adam, and OAdagrad results in higher IS.\nIn addition, for OSG and OAdagrad, large batch size helps the training, which is consistent with our\ntheory.\"\n\nI am not convinced how closely the fact that you described is related to you theory. Even without any theory it seems very natural that more accurate gradient estimates will improve convergence speed and I don't see how the more subtle results of your theory are confirmed by those experiments.\nPlease let me know if you disagree but otherwise I would maybe advise to drop this remark. "}, "signatures": ["ICLR.cc/2020/Conference/Paper1038/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1038/AnonReviewer3", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards Better Understanding of Adaptive Gradient Algorithms in Generative Adversarial Nets", "authors": ["Mingrui Liu", "Youssef Mroueh", "Jerret Ross", "Wei Zhang", "Xiaodong Cui", "Payel Das", "Tianbao Yang"], "authorids": ["mingrui-liu@uiowa.edu", "mroueh@us.ibm.com", "rossja@us.ibm.com", "weiz@us.ibm.com", "cuix@us.ibm.com", "daspa@us.ibm.com", "tianbao-yang@uiowa.edu"], "keywords": ["Generative Adversarial Nets", "Adaptive Gradient Algorithms"], "TL;DR": "This paper provides novel analysis of adaptive gradient algorithms for solving non-convex non-concave min-max problems as GANs, and explains the reason why adaptive gradient methods outperform its non-adaptive counterparts by empirical studies.", "abstract": "Adaptive gradient algorithms perform gradient-based updates using the history of gradients and are ubiquitous in training deep neural networks. While adaptive gradient methods theory is well understood for minimization problems, the underlying factors driving their empirical success in min-max problems such as GANs remain unclear. In this paper, we aim at bridging  this gap from both theoretical and empirical perspectives. First, we analyze a variant of Optimistic Stochastic Gradient (OSG) proposed in~\\citep{daskalakis2017training} for solving a class of non-convex non-concave min-max problem and establish $O(\\epsilon^{-4})$ complexity for finding $\\epsilon$-first-order stationary point, in which the algorithm only requires invoking one stochastic first-order oracle while enjoying state-of-the-art iteration complexity achieved by stochastic extragradient method by~\\citep{iusem2017extragradient}. Then we propose an adaptive variant of OSG named Optimistic Adagrad (OAdagrad) and reveal an \\emph{improved} adaptive complexity $O\\left(\\epsilon^{-\\frac{2}{1-\\alpha}}\\right)$, where $\\alpha$ characterizes the growth rate of the cumulative stochastic gradient and $0\\leq \\alpha\\leq 1/2$. To the best of our knowledge, this is the first work for establishing adaptive complexity in non-convex non-concave min-max optimization. Empirically, our experiments show that indeed adaptive gradient algorithms outperform their non-adaptive counterparts in GAN training. Moreover, this observation can be explained by the slow growth rate of the cumulative stochastic gradient, as observed empirically.", "pdf": "/pdf/1dda48e5d305320db2272f8621cff728afde476b.pdf", "paperhash": "liu|towards_better_understanding_of_adaptive_gradient_algorithms_in_generative_adversarial_nets", "_bibtex": "@inproceedings{\nLiu2020Towards,\ntitle={Towards Better Understanding of Adaptive Gradient Algorithms in Generative Adversarial Nets},\nauthor={Mingrui Liu and Youssef Mroueh and Jerret Ross and Wei Zhang and Xiaodong Cui and Payel Das and Tianbao Yang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJxIm0VtwH}\n}", "original_pdf": "/attachment/37fc9fc009ff2d927a743d72c2a80e5b91c5c0d5.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJxIm0VtwH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1038/Authors", "ICLR.cc/2020/Conference/Paper1038/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1038/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1038/Reviewers", "ICLR.cc/2020/Conference/Paper1038/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1038/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1038/Authors|ICLR.cc/2020/Conference/Paper1038/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504162221, "tmdate": 1576860549691, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1038/Authors", "ICLR.cc/2020/Conference/Paper1038/Reviewers", "ICLR.cc/2020/Conference/Paper1038/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1038/-/Official_Comment"}}}, {"id": "ByePmZ8Psr", "original": null, "number": 4, "cdate": 1573507358523, "ddate": null, "tcdate": 1573507358523, "tmdate": 1573526285099, "tddate": null, "forum": "SJxIm0VtwH", "replyto": "BJenu0PTFB", "invitation": "ICLR.cc/2020/Conference/Paper1038/-/Official_Comment", "content": {"title": "1/2 Thank you for your feedback, we included updates and clarifications.  ", "comment": "Thanks for your valuable comments. We have made revisions according to your suggestions in red text. \n\nQ1: Why the update of Algorithm 1 becomes the algorithm in (Daskalaskis et al. 2017) when there is no constraint? It seems that u and v are updated with the same sign?\n\nA: $u$ and $v$ are updated with different signs due to the definition of operator $T$, which is defined as $T(x)=[\\nabla_u F(u,v), -\\nabla_v F(u,v)]^\\top$ (please note that there is a negative sign before $\\nabla_v F(u,v)$). Let's clarify the equivalence of the update rule between Algorithm 1 and the algorithm in (Daskalakis et al. 2017) when there is no constraint. Define $\\hat{g}_k = \\frac{1}{m_k}\\sum_{i=1}^{m_k}T(z_k;\\xi_k^i)$, then the update rule of Algorithm 1 becomes $z_k=x_{k-1}-\\eta \\hat{g}_{k-1}$ (i) and $x_k=x_{k-1}-\\eta \\hat{g}_k$ (ii). These two together imply that $z_{k+1}=x_k-\\eta \\hat{g}_k=x_{k-1}-2\\eta \\hat{g}_k=z_k+\\eta \\hat{g}_{k-1}-2\\eta \\hat{g}_k$ (iii), where the first equality comes from (i) by replacing $k$ to $k+1$, the second equality holds by (ii), and the third equality holds by using (i) again. (iii) is the algorithm in (Daskalakis et al. 2017). \n\nWe have added the details in the supplement in the revision for your information (Section F in the appendix) . \n\n\nQ2: In terms of theoretical contributions, what is the key point to achieve the derived results? For example, why the theorem for Algorithm 1 achieves the same iteration complexity with weaker assumption?\n\nA: For Algorithm 1, the main difference from extra-gradient (Iusem et al 2017) is that Algorithm 1 only requires computing stochastic gradient once instead of twice in each iteration. For theoretical analysis, the key point is that we use improved analysis over that of Iusem et al 2017, which can tolerate the error term induced by the replacement without hurting the iteration complexity. Indeed, we have realized that Iusem et al\u2019 analysis can hold under the same MVI assumption. For Algorithm 2, the novelty is to use an adaptive step size and its proof is much more involved in order to dealing with the complex adaptive step size. We not only show the improved adaptive complexity of Algorithm 2 but also eliminate the large minibatch size requirement as needed in Algorithm 1. The key point to derive Theorem 2 is to carefully design a variable-metric which adapts to the geometry of data during the update to get potentially faster convergence (Lemma 5) while controlling the error term not to blow up (Lemma 3, Lemma 4, Lemma 6) simultaneously. \n\n\nQ3: For Algorithm 2, why there is no projection operator like in Algorithm 1?\n\nA: In this paper, we only analyze Algorithm 2 for the unconstrained case, i.e. $X = R^d$. We have made it more clear in the revised version. Thanks for pointing it out. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1038/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1038/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards Better Understanding of Adaptive Gradient Algorithms in Generative Adversarial Nets", "authors": ["Mingrui Liu", "Youssef Mroueh", "Jerret Ross", "Wei Zhang", "Xiaodong Cui", "Payel Das", "Tianbao Yang"], "authorids": ["mingrui-liu@uiowa.edu", "mroueh@us.ibm.com", "rossja@us.ibm.com", "weiz@us.ibm.com", "cuix@us.ibm.com", "daspa@us.ibm.com", "tianbao-yang@uiowa.edu"], "keywords": ["Generative Adversarial Nets", "Adaptive Gradient Algorithms"], "TL;DR": "This paper provides novel analysis of adaptive gradient algorithms for solving non-convex non-concave min-max problems as GANs, and explains the reason why adaptive gradient methods outperform its non-adaptive counterparts by empirical studies.", "abstract": "Adaptive gradient algorithms perform gradient-based updates using the history of gradients and are ubiquitous in training deep neural networks. While adaptive gradient methods theory is well understood for minimization problems, the underlying factors driving their empirical success in min-max problems such as GANs remain unclear. In this paper, we aim at bridging  this gap from both theoretical and empirical perspectives. First, we analyze a variant of Optimistic Stochastic Gradient (OSG) proposed in~\\citep{daskalakis2017training} for solving a class of non-convex non-concave min-max problem and establish $O(\\epsilon^{-4})$ complexity for finding $\\epsilon$-first-order stationary point, in which the algorithm only requires invoking one stochastic first-order oracle while enjoying state-of-the-art iteration complexity achieved by stochastic extragradient method by~\\citep{iusem2017extragradient}. Then we propose an adaptive variant of OSG named Optimistic Adagrad (OAdagrad) and reveal an \\emph{improved} adaptive complexity $O\\left(\\epsilon^{-\\frac{2}{1-\\alpha}}\\right)$, where $\\alpha$ characterizes the growth rate of the cumulative stochastic gradient and $0\\leq \\alpha\\leq 1/2$. To the best of our knowledge, this is the first work for establishing adaptive complexity in non-convex non-concave min-max optimization. Empirically, our experiments show that indeed adaptive gradient algorithms outperform their non-adaptive counterparts in GAN training. Moreover, this observation can be explained by the slow growth rate of the cumulative stochastic gradient, as observed empirically.", "pdf": "/pdf/1dda48e5d305320db2272f8621cff728afde476b.pdf", "paperhash": "liu|towards_better_understanding_of_adaptive_gradient_algorithms_in_generative_adversarial_nets", "_bibtex": "@inproceedings{\nLiu2020Towards,\ntitle={Towards Better Understanding of Adaptive Gradient Algorithms in Generative Adversarial Nets},\nauthor={Mingrui Liu and Youssef Mroueh and Jerret Ross and Wei Zhang and Xiaodong Cui and Payel Das and Tianbao Yang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJxIm0VtwH}\n}", "original_pdf": "/attachment/37fc9fc009ff2d927a743d72c2a80e5b91c5c0d5.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJxIm0VtwH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1038/Authors", "ICLR.cc/2020/Conference/Paper1038/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1038/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1038/Reviewers", "ICLR.cc/2020/Conference/Paper1038/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1038/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1038/Authors|ICLR.cc/2020/Conference/Paper1038/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504162221, "tmdate": 1576860549691, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1038/Authors", "ICLR.cc/2020/Conference/Paper1038/Reviewers", "ICLR.cc/2020/Conference/Paper1038/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1038/-/Official_Comment"}}}, {"id": "H1x0vZUPsB", "original": null, "number": 5, "cdate": 1573507429966, "ddate": null, "tcdate": 1573507429966, "tmdate": 1573509348467, "tddate": null, "forum": "SJxIm0VtwH", "replyto": "r1xHK7B9KB", "invitation": "ICLR.cc/2020/Conference/Paper1038/-/Official_Comment", "content": {"title": "Thank you for your feedback. We updated the paper accordingly.", "comment": "Thanks for your valuable comments. We have revised our paper according to your suggestions: 1) For OSG, we do not emphasize it is a new algorithm but rather emphasize our contribution on its analysis for general min-max problems. In the revision, we explicitly mention that OSG is not a new algorithm, which has been analyzed by Daskalakis et al. 2017 for the bilinear case. We emphasize that our work is the first to analyze OSG for a general min-max problem under the MVI assumption, and achieves the state-of-the-art results as stochastic extra-gradient method by Iusem et al. 2017. 2) We emphasize that the main difference between OSG and that of Iusem et al. 2017 is the number of stochastic gradient calculations per-iteration, and pointing out that the pseudomonotone assumption used by Iusem et al. 2017 can be replaced by our MVI assumption. 3) In page 4 remark point (a), we have clarified the relationship between the MVI assumption and the pseudomonotone assumption and also given an example that satisfies the MVI assumption but not the pseudomonotone assumption. 4) We emphasize that the main goal of this paper is to study the benefits of using adaptive gradient algorithms in training generative adversarial nets, and introducing its non-adaptive version (OSG) first is for the sake of comparison purpose, in order to highlight the benefits of adaptive methods in this context. The revisions have been highlighted in red. \n\n\nQ1: Does figure 1 show iterations or epochs on x-axis?\n\nA: The x-axis represents the number of iterations. We have included experiments with x-axis being the number of epochs at Figure 7 in the Appendix E (page 22) in the revised version.\n\n\nQ2: Is there a new technical difficulty to overcome when replacing the extragradient method with the OSG compared to the proof of Iusem et al? If yes, can you give a concise description of it?\n\nA: There are some subtle differences that requires deep investigation of OSG. One is the proof of Lemma 1 in Appendix C.2. Because of the replacement, the inequality (10) becomes different due to the different update, then we need to expand the term $\\|z_{k-1}-z_k\\|^2$ in a different way. The second one is in the proof of Theorem 1. Iusem et al. 2017 proved the convergence in terms of $x_k$, but we have to prove the convergence in terms of sequence $z_k$.\n\n\nQ3: Why pseudo-monotonicity implies the existence of a solution of the Minty variational inequality?\n\nA: This implication was not well explained in the manuscript, we corrected it. We assume that SVI has a nonempty solution set and that the operator T is pseudomonotone. Under those two assumptions there exists a solution for Minty VI (MVI).  We have made it more clear in the revised version (cf. Remark point (a) in page 4).\n\n\nQ4: Is it possible to explicitly compare the constants in the bounds on the convergence rate of the two algorithms?\n\nA: We did not make efforts to optimize the constants in the complexity of OSG. Currently, it is comparable to that but a little worse than (Iusem et al. 2017). In particular, our bound has a constant factor 4 (in our inequality (15)) while Iusem et al. 2017 has constant factor 2 (in their inequality (107)) when trying to bound the proximal gradient. However, our main goal is this paper is to demonstrate OAdaGrad could have a lower order of complexity in the presence of slow growth rate of cumulative stochastic gradient.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1038/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1038/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards Better Understanding of Adaptive Gradient Algorithms in Generative Adversarial Nets", "authors": ["Mingrui Liu", "Youssef Mroueh", "Jerret Ross", "Wei Zhang", "Xiaodong Cui", "Payel Das", "Tianbao Yang"], "authorids": ["mingrui-liu@uiowa.edu", "mroueh@us.ibm.com", "rossja@us.ibm.com", "weiz@us.ibm.com", "cuix@us.ibm.com", "daspa@us.ibm.com", "tianbao-yang@uiowa.edu"], "keywords": ["Generative Adversarial Nets", "Adaptive Gradient Algorithms"], "TL;DR": "This paper provides novel analysis of adaptive gradient algorithms for solving non-convex non-concave min-max problems as GANs, and explains the reason why adaptive gradient methods outperform its non-adaptive counterparts by empirical studies.", "abstract": "Adaptive gradient algorithms perform gradient-based updates using the history of gradients and are ubiquitous in training deep neural networks. While adaptive gradient methods theory is well understood for minimization problems, the underlying factors driving their empirical success in min-max problems such as GANs remain unclear. In this paper, we aim at bridging  this gap from both theoretical and empirical perspectives. First, we analyze a variant of Optimistic Stochastic Gradient (OSG) proposed in~\\citep{daskalakis2017training} for solving a class of non-convex non-concave min-max problem and establish $O(\\epsilon^{-4})$ complexity for finding $\\epsilon$-first-order stationary point, in which the algorithm only requires invoking one stochastic first-order oracle while enjoying state-of-the-art iteration complexity achieved by stochastic extragradient method by~\\citep{iusem2017extragradient}. Then we propose an adaptive variant of OSG named Optimistic Adagrad (OAdagrad) and reveal an \\emph{improved} adaptive complexity $O\\left(\\epsilon^{-\\frac{2}{1-\\alpha}}\\right)$, where $\\alpha$ characterizes the growth rate of the cumulative stochastic gradient and $0\\leq \\alpha\\leq 1/2$. To the best of our knowledge, this is the first work for establishing adaptive complexity in non-convex non-concave min-max optimization. Empirically, our experiments show that indeed adaptive gradient algorithms outperform their non-adaptive counterparts in GAN training. Moreover, this observation can be explained by the slow growth rate of the cumulative stochastic gradient, as observed empirically.", "pdf": "/pdf/1dda48e5d305320db2272f8621cff728afde476b.pdf", "paperhash": "liu|towards_better_understanding_of_adaptive_gradient_algorithms_in_generative_adversarial_nets", "_bibtex": "@inproceedings{\nLiu2020Towards,\ntitle={Towards Better Understanding of Adaptive Gradient Algorithms in Generative Adversarial Nets},\nauthor={Mingrui Liu and Youssef Mroueh and Jerret Ross and Wei Zhang and Xiaodong Cui and Payel Das and Tianbao Yang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJxIm0VtwH}\n}", "original_pdf": "/attachment/37fc9fc009ff2d927a743d72c2a80e5b91c5c0d5.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJxIm0VtwH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1038/Authors", "ICLR.cc/2020/Conference/Paper1038/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1038/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1038/Reviewers", "ICLR.cc/2020/Conference/Paper1038/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1038/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1038/Authors|ICLR.cc/2020/Conference/Paper1038/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504162221, "tmdate": 1576860549691, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1038/Authors", "ICLR.cc/2020/Conference/Paper1038/Reviewers", "ICLR.cc/2020/Conference/Paper1038/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1038/-/Official_Comment"}}}, {"id": "B1eReZUvoS", "original": null, "number": 3, "cdate": 1573507318094, "ddate": null, "tcdate": 1573507318094, "tmdate": 1573508826436, "tddate": null, "forum": "SJxIm0VtwH", "replyto": "BJenu0PTFB", "invitation": "ICLR.cc/2020/Conference/Paper1038/-/Official_Comment", "content": {"title": "2/2 Thank you for your feedback, we included updates and clarifications.", "comment": "Q4: For Algorithm 2, the algorithm design is a bit different from traditional Adagrad as their H matrix is the historical average of all past gradient square (element-wise). Here the s_i is L_2 norm of historical gradient, there is no averaging (divided by k) operator. Can the authors elaborate on this algorithm design?\n\nA: It is indeed the same as traditional Adagrad. Please note that in the original Adagrad paper (Duchi et al, 2011), there are two variants (in their Figure 1): the first one is using primal-dual subgradient update, and the second one is using composite mirror descent update. The first one utilizes the averaging operator while the second one does not. Our algorithm design is inspired by the second variant.\n\nQ5: Experiments.\n\nA: \n(1) The reason why we didn't compare with Optimistic Adam was that our algorithm is a special case of Optimistic Adam with $\\beta_1=0$ and $\\beta_2$ approaches to $1$. We mentioned this in the paragraph above section 5 at Page 7. The authors of the original Adam paper also make the similar claim.\n\n(2) We have added another Figure 7 in Appendix E (page 22) in the revision to put different batches in different plots to compare algorithms. Thanks for the suggestion.\n\n(3) The reason why alternating Adam failed in our SA-GAN experiment was that we used different batch size (128) compared with the original paper (256). Due to limited computational resources, we only used 2 machines with minibatch 64 for each machine, while they used 4 machines in SA-GAN paper (with 64 minibatch per machine). It appears that using the specific range of learning rates we tried didn't work out very well. We noticed that Alternating Adam was very sensitive to the choice of learning rates and we did not manage to get Alternating Adam to work for our particular batch size and for the ranges of learning rates we tried. This does not mean that alternating Adam fails, it just needs more tuning to find the correct range of learning rates for the particular batch size we have. This is in line with the conclusions of the paper \u201care all GANs created equal? A large scale study \u201d https://arxiv.org/abs/1711.10337 , that shows that the computational budget dedicated to tuning the hyperparameters in the GAN context is crucial to their performance. We clarified this in the paper (In experiment Section page 9). \n\nNote that we presented results of Alternating Adam on CIFAR 10 where the training succeed. \n\n\nQ6: Detailed comments.\n\nA: Thank you for carefully reading our paper! We have defined $m_t$ and make it more clear in the revision. In terms of the paragraph in page 7, you are absolutely right, it should be $\\beta_1=0, \\beta_2\\rightarrow1$. As we mentioned in Q5(1) and also in the paragraph, in this case it is equivalent to OAdagrad. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1038/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1038/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards Better Understanding of Adaptive Gradient Algorithms in Generative Adversarial Nets", "authors": ["Mingrui Liu", "Youssef Mroueh", "Jerret Ross", "Wei Zhang", "Xiaodong Cui", "Payel Das", "Tianbao Yang"], "authorids": ["mingrui-liu@uiowa.edu", "mroueh@us.ibm.com", "rossja@us.ibm.com", "weiz@us.ibm.com", "cuix@us.ibm.com", "daspa@us.ibm.com", "tianbao-yang@uiowa.edu"], "keywords": ["Generative Adversarial Nets", "Adaptive Gradient Algorithms"], "TL;DR": "This paper provides novel analysis of adaptive gradient algorithms for solving non-convex non-concave min-max problems as GANs, and explains the reason why adaptive gradient methods outperform its non-adaptive counterparts by empirical studies.", "abstract": "Adaptive gradient algorithms perform gradient-based updates using the history of gradients and are ubiquitous in training deep neural networks. While adaptive gradient methods theory is well understood for minimization problems, the underlying factors driving their empirical success in min-max problems such as GANs remain unclear. In this paper, we aim at bridging  this gap from both theoretical and empirical perspectives. First, we analyze a variant of Optimistic Stochastic Gradient (OSG) proposed in~\\citep{daskalakis2017training} for solving a class of non-convex non-concave min-max problem and establish $O(\\epsilon^{-4})$ complexity for finding $\\epsilon$-first-order stationary point, in which the algorithm only requires invoking one stochastic first-order oracle while enjoying state-of-the-art iteration complexity achieved by stochastic extragradient method by~\\citep{iusem2017extragradient}. Then we propose an adaptive variant of OSG named Optimistic Adagrad (OAdagrad) and reveal an \\emph{improved} adaptive complexity $O\\left(\\epsilon^{-\\frac{2}{1-\\alpha}}\\right)$, where $\\alpha$ characterizes the growth rate of the cumulative stochastic gradient and $0\\leq \\alpha\\leq 1/2$. To the best of our knowledge, this is the first work for establishing adaptive complexity in non-convex non-concave min-max optimization. Empirically, our experiments show that indeed adaptive gradient algorithms outperform their non-adaptive counterparts in GAN training. Moreover, this observation can be explained by the slow growth rate of the cumulative stochastic gradient, as observed empirically.", "pdf": "/pdf/1dda48e5d305320db2272f8621cff728afde476b.pdf", "paperhash": "liu|towards_better_understanding_of_adaptive_gradient_algorithms_in_generative_adversarial_nets", "_bibtex": "@inproceedings{\nLiu2020Towards,\ntitle={Towards Better Understanding of Adaptive Gradient Algorithms in Generative Adversarial Nets},\nauthor={Mingrui Liu and Youssef Mroueh and Jerret Ross and Wei Zhang and Xiaodong Cui and Payel Das and Tianbao Yang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJxIm0VtwH}\n}", "original_pdf": "/attachment/37fc9fc009ff2d927a743d72c2a80e5b91c5c0d5.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJxIm0VtwH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1038/Authors", "ICLR.cc/2020/Conference/Paper1038/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1038/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1038/Reviewers", "ICLR.cc/2020/Conference/Paper1038/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1038/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1038/Authors|ICLR.cc/2020/Conference/Paper1038/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504162221, "tmdate": 1576860549691, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1038/Authors", "ICLR.cc/2020/Conference/Paper1038/Reviewers", "ICLR.cc/2020/Conference/Paper1038/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1038/-/Official_Comment"}}}, {"id": "BJejdlUvsH", "original": null, "number": 2, "cdate": 1573507187389, "ddate": null, "tcdate": 1573507187389, "tmdate": 1573507994897, "tddate": null, "forum": "SJxIm0VtwH", "replyto": "rklsNMOaYS", "invitation": "ICLR.cc/2020/Conference/Paper1038/-/Official_Comment", "content": {"title": "Thank you for your review.", "comment": "Thanks for your encouraging comments and your constructive feedback.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1038/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1038/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards Better Understanding of Adaptive Gradient Algorithms in Generative Adversarial Nets", "authors": ["Mingrui Liu", "Youssef Mroueh", "Jerret Ross", "Wei Zhang", "Xiaodong Cui", "Payel Das", "Tianbao Yang"], "authorids": ["mingrui-liu@uiowa.edu", "mroueh@us.ibm.com", "rossja@us.ibm.com", "weiz@us.ibm.com", "cuix@us.ibm.com", "daspa@us.ibm.com", "tianbao-yang@uiowa.edu"], "keywords": ["Generative Adversarial Nets", "Adaptive Gradient Algorithms"], "TL;DR": "This paper provides novel analysis of adaptive gradient algorithms for solving non-convex non-concave min-max problems as GANs, and explains the reason why adaptive gradient methods outperform its non-adaptive counterparts by empirical studies.", "abstract": "Adaptive gradient algorithms perform gradient-based updates using the history of gradients and are ubiquitous in training deep neural networks. While adaptive gradient methods theory is well understood for minimization problems, the underlying factors driving their empirical success in min-max problems such as GANs remain unclear. In this paper, we aim at bridging  this gap from both theoretical and empirical perspectives. First, we analyze a variant of Optimistic Stochastic Gradient (OSG) proposed in~\\citep{daskalakis2017training} for solving a class of non-convex non-concave min-max problem and establish $O(\\epsilon^{-4})$ complexity for finding $\\epsilon$-first-order stationary point, in which the algorithm only requires invoking one stochastic first-order oracle while enjoying state-of-the-art iteration complexity achieved by stochastic extragradient method by~\\citep{iusem2017extragradient}. Then we propose an adaptive variant of OSG named Optimistic Adagrad (OAdagrad) and reveal an \\emph{improved} adaptive complexity $O\\left(\\epsilon^{-\\frac{2}{1-\\alpha}}\\right)$, where $\\alpha$ characterizes the growth rate of the cumulative stochastic gradient and $0\\leq \\alpha\\leq 1/2$. To the best of our knowledge, this is the first work for establishing adaptive complexity in non-convex non-concave min-max optimization. Empirically, our experiments show that indeed adaptive gradient algorithms outperform their non-adaptive counterparts in GAN training. Moreover, this observation can be explained by the slow growth rate of the cumulative stochastic gradient, as observed empirically.", "pdf": "/pdf/1dda48e5d305320db2272f8621cff728afde476b.pdf", "paperhash": "liu|towards_better_understanding_of_adaptive_gradient_algorithms_in_generative_adversarial_nets", "_bibtex": "@inproceedings{\nLiu2020Towards,\ntitle={Towards Better Understanding of Adaptive Gradient Algorithms in Generative Adversarial Nets},\nauthor={Mingrui Liu and Youssef Mroueh and Jerret Ross and Wei Zhang and Xiaodong Cui and Payel Das and Tianbao Yang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJxIm0VtwH}\n}", "original_pdf": "/attachment/37fc9fc009ff2d927a743d72c2a80e5b91c5c0d5.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJxIm0VtwH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1038/Authors", "ICLR.cc/2020/Conference/Paper1038/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1038/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1038/Reviewers", "ICLR.cc/2020/Conference/Paper1038/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1038/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1038/Authors|ICLR.cc/2020/Conference/Paper1038/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504162221, "tmdate": 1576860549691, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1038/Authors", "ICLR.cc/2020/Conference/Paper1038/Reviewers", "ICLR.cc/2020/Conference/Paper1038/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1038/-/Official_Comment"}}}, {"id": "Hyl_8gIDjS", "original": null, "number": 1, "cdate": 1573507152129, "ddate": null, "tcdate": 1573507152129, "tmdate": 1573507152129, "tddate": null, "forum": "SJxIm0VtwH", "replyto": "SJxIm0VtwH", "invitation": "ICLR.cc/2020/Conference/Paper1038/-/Official_Comment", "content": {"title": "General Comments", "comment": "Thanks for all the comments. We have updated our manuscript per reviewers\u2019 suggestions. All updates are marked in red. The main summary of the updates are:\n\n1. We change the statement in terms of OSG in both abstract and the claim of main contribution, as suggested by R3. We put our work in the context of previous works when introducing OSG and draw connections to prior works (e.g. Daskalakis et al. 2017, Iusem et al. 2017). \n\n2. Per R3\u2019s suggestion, we mention that our MVI assumption is also sufficient to derive results in (Iusem et al. 2017) in Table 1. The MVI assumption and pseudo-monotonicity are clarified, and an example that satisfies MVI assumption but not pseudo-monotonicity is also provided (cf. Remark point (a) in page 4).\n\n3. We explain the equivalence between OSG and the algorithm in (Daskalakis et al. 2017) in Appendix F, as suggested by R2.\n\n4. We have added another Figure 7 in Appendix E (page 22) in the revision to put different batches in different plots to compare algorithms. The figures show the inception score versus the number of epochs, as suggested by R2 and R3.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1038/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1038/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards Better Understanding of Adaptive Gradient Algorithms in Generative Adversarial Nets", "authors": ["Mingrui Liu", "Youssef Mroueh", "Jerret Ross", "Wei Zhang", "Xiaodong Cui", "Payel Das", "Tianbao Yang"], "authorids": ["mingrui-liu@uiowa.edu", "mroueh@us.ibm.com", "rossja@us.ibm.com", "weiz@us.ibm.com", "cuix@us.ibm.com", "daspa@us.ibm.com", "tianbao-yang@uiowa.edu"], "keywords": ["Generative Adversarial Nets", "Adaptive Gradient Algorithms"], "TL;DR": "This paper provides novel analysis of adaptive gradient algorithms for solving non-convex non-concave min-max problems as GANs, and explains the reason why adaptive gradient methods outperform its non-adaptive counterparts by empirical studies.", "abstract": "Adaptive gradient algorithms perform gradient-based updates using the history of gradients and are ubiquitous in training deep neural networks. While adaptive gradient methods theory is well understood for minimization problems, the underlying factors driving their empirical success in min-max problems such as GANs remain unclear. In this paper, we aim at bridging  this gap from both theoretical and empirical perspectives. First, we analyze a variant of Optimistic Stochastic Gradient (OSG) proposed in~\\citep{daskalakis2017training} for solving a class of non-convex non-concave min-max problem and establish $O(\\epsilon^{-4})$ complexity for finding $\\epsilon$-first-order stationary point, in which the algorithm only requires invoking one stochastic first-order oracle while enjoying state-of-the-art iteration complexity achieved by stochastic extragradient method by~\\citep{iusem2017extragradient}. Then we propose an adaptive variant of OSG named Optimistic Adagrad (OAdagrad) and reveal an \\emph{improved} adaptive complexity $O\\left(\\epsilon^{-\\frac{2}{1-\\alpha}}\\right)$, where $\\alpha$ characterizes the growth rate of the cumulative stochastic gradient and $0\\leq \\alpha\\leq 1/2$. To the best of our knowledge, this is the first work for establishing adaptive complexity in non-convex non-concave min-max optimization. Empirically, our experiments show that indeed adaptive gradient algorithms outperform their non-adaptive counterparts in GAN training. Moreover, this observation can be explained by the slow growth rate of the cumulative stochastic gradient, as observed empirically.", "pdf": "/pdf/1dda48e5d305320db2272f8621cff728afde476b.pdf", "paperhash": "liu|towards_better_understanding_of_adaptive_gradient_algorithms_in_generative_adversarial_nets", "_bibtex": "@inproceedings{\nLiu2020Towards,\ntitle={Towards Better Understanding of Adaptive Gradient Algorithms in Generative Adversarial Nets},\nauthor={Mingrui Liu and Youssef Mroueh and Jerret Ross and Wei Zhang and Xiaodong Cui and Payel Das and Tianbao Yang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJxIm0VtwH}\n}", "original_pdf": "/attachment/37fc9fc009ff2d927a743d72c2a80e5b91c5c0d5.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJxIm0VtwH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1038/Authors", "ICLR.cc/2020/Conference/Paper1038/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1038/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1038/Reviewers", "ICLR.cc/2020/Conference/Paper1038/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1038/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1038/Authors|ICLR.cc/2020/Conference/Paper1038/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504162221, "tmdate": 1576860549691, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1038/Authors", "ICLR.cc/2020/Conference/Paper1038/Reviewers", "ICLR.cc/2020/Conference/Paper1038/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1038/-/Official_Comment"}}}, {"id": "rklsNMOaYS", "original": null, "number": 3, "cdate": 1571811891459, "ddate": null, "tcdate": 1571811891459, "tmdate": 1572972520283, "tddate": null, "forum": "SJxIm0VtwH", "replyto": "SJxIm0VtwH", "invitation": "ICLR.cc/2020/Conference/Paper1038/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "This paper proposes two methods named OSG and OAdagrad for solving stochastic non-convex non-concave min-max problems. In theoretical analyses, a convergence rate of $O(\\epsilon^{-4})$ and a much better rate are provided for OSG and OAdagrad, respectively, to find the $\\epsilon$-accurate first-order stationary point. Finally, the superior performance of the proposed method is empirically verified on training generative adversarial networks (GANs).\n\nClarity:\nThe paper is well organized and easy to read.\n\nQuality:\nThe work is of good quality and is technically sound. However, I did not verify the proof in detail.\n\nSignificance:\nA stochastic non-convex non-concave minimax problem studied in this paper is recently considered as an important class of the optimization problems because important machine learning problems such as GANs fall into this class and most past papers studied convex-concave min-max problems instead. A few studies [Iusem+(2017), Lin+(2018)] proposed optimization algorithms for this problem and derived convergence rates $O(\\epsilon^{-4})$ and $O(\\epsilon^{-6})$, respectively. On the other hand, proposed methods have several preferable properties compared to these methods. For instance, OSG exhibits a comparable convergence rate to [Iusem+(2017)] with a fewer per-iteration complexity and OAdagrad exhibits a much faster convergence rate $O(\\epsilon^{-2/(1-\\alpha)})$ depending on the parameter $\\alpha$ that is an order of the growth of the cumulative stochastic gradients norm. In addition, the order of $\\alpha$ is shown to be slow in general and certainly faster convergence rates are also confirmed in experiments on training GANs. Thus, experimental results seem consistent with the theory. \nSince a derived convergence rate of OAdagrad is potentially much faster than those of existing methods, I think the OAdagrad is one of the promising methods for training GANs."}, "signatures": ["ICLR.cc/2020/Conference/Paper1038/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1038/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards Better Understanding of Adaptive Gradient Algorithms in Generative Adversarial Nets", "authors": ["Mingrui Liu", "Youssef Mroueh", "Jerret Ross", "Wei Zhang", "Xiaodong Cui", "Payel Das", "Tianbao Yang"], "authorids": ["mingrui-liu@uiowa.edu", "mroueh@us.ibm.com", "rossja@us.ibm.com", "weiz@us.ibm.com", "cuix@us.ibm.com", "daspa@us.ibm.com", "tianbao-yang@uiowa.edu"], "keywords": ["Generative Adversarial Nets", "Adaptive Gradient Algorithms"], "TL;DR": "This paper provides novel analysis of adaptive gradient algorithms for solving non-convex non-concave min-max problems as GANs, and explains the reason why adaptive gradient methods outperform its non-adaptive counterparts by empirical studies.", "abstract": "Adaptive gradient algorithms perform gradient-based updates using the history of gradients and are ubiquitous in training deep neural networks. While adaptive gradient methods theory is well understood for minimization problems, the underlying factors driving their empirical success in min-max problems such as GANs remain unclear. In this paper, we aim at bridging  this gap from both theoretical and empirical perspectives. First, we analyze a variant of Optimistic Stochastic Gradient (OSG) proposed in~\\citep{daskalakis2017training} for solving a class of non-convex non-concave min-max problem and establish $O(\\epsilon^{-4})$ complexity for finding $\\epsilon$-first-order stationary point, in which the algorithm only requires invoking one stochastic first-order oracle while enjoying state-of-the-art iteration complexity achieved by stochastic extragradient method by~\\citep{iusem2017extragradient}. Then we propose an adaptive variant of OSG named Optimistic Adagrad (OAdagrad) and reveal an \\emph{improved} adaptive complexity $O\\left(\\epsilon^{-\\frac{2}{1-\\alpha}}\\right)$, where $\\alpha$ characterizes the growth rate of the cumulative stochastic gradient and $0\\leq \\alpha\\leq 1/2$. To the best of our knowledge, this is the first work for establishing adaptive complexity in non-convex non-concave min-max optimization. Empirically, our experiments show that indeed adaptive gradient algorithms outperform their non-adaptive counterparts in GAN training. Moreover, this observation can be explained by the slow growth rate of the cumulative stochastic gradient, as observed empirically.", "pdf": "/pdf/1dda48e5d305320db2272f8621cff728afde476b.pdf", "paperhash": "liu|towards_better_understanding_of_adaptive_gradient_algorithms_in_generative_adversarial_nets", "_bibtex": "@inproceedings{\nLiu2020Towards,\ntitle={Towards Better Understanding of Adaptive Gradient Algorithms in Generative Adversarial Nets},\nauthor={Mingrui Liu and Youssef Mroueh and Jerret Ross and Wei Zhang and Xiaodong Cui and Payel Das and Tianbao Yang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJxIm0VtwH}\n}", "original_pdf": "/attachment/37fc9fc009ff2d927a743d72c2a80e5b91c5c0d5.pdf"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SJxIm0VtwH", "replyto": "SJxIm0VtwH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1038/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1038/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575695084737, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1038/Reviewers"], "noninvitees": [], "tcdate": 1570237743307, "tmdate": 1575695084748, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1038/-/Official_Review"}}}], "count": 12}