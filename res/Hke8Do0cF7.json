{"notes": [{"id": "Hke8Do0cF7", "original": "ryeSygd5tm", "number": 268, "cdate": 1538087774358, "ddate": null, "tcdate": 1538087774358, "tmdate": 1545355384648, "tddate": null, "forum": "Hke8Do0cF7", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Deep processing of structured data", "abstract": "We construct a general unified framework for learning representation of structured\ndata, i.e. data which cannot be represented as the fixed-length vectors (e.g. sets,\ngraphs, texts or images of varying sizes). The key factor is played by an intermediate\nnetwork called SAN (Set Aggregating Network), which maps a structured\nobject to a fixed length vector in a high dimensional latent space. Our main theoretical\nresult shows that for sufficiently large dimension of the latent space, SAN is\ncapable of learning a unique representation for every input example. Experiments\ndemonstrate that replacing pooling operation by SAN in convolutional networks\nleads to better results in classifying images with different sizes. Moreover, its direct\napplication to text and graph data allows to obtain results close to SOTA, by\nsimpler networks with smaller number of parameters than competitive models.", "keywords": ["structured data", "representation learning", "deep neural networks"], "authorids": ["l.maziarka@gmail.com", "marek.smieja@uj.edu.pl", "aknoow@gmail.com", "jacek.tabor@uj.edu.pl", "lukasz.struski@uj.edu.pl", "przemyslaw.spurek@uj.edu.pl"], "authors": ["\u0141ukasz Maziarka", "Marek \u015amieja", "Aleksandra Nowak", "Jacek Tabor", "\u0141ukasz Struski", "Przemys\u0142aw Spurek"], "TL;DR": "General framework of learning representation of structured inputs.", "pdf": "/pdf/d35f16cffb07c6c0a82faee9f91dc1e905481a36.pdf", "paperhash": "maziarka|deep_processing_of_structured_data", "_bibtex": "@misc{\nmaziarka2019deep,\ntitle={Deep processing of structured data},\nauthor={\u0141ukasz Maziarka and Marek \u015amieja and Aleksandra Nowak and Jacek Tabor and \u0141ukasz Struski and Przemys\u0142aw Spurek},\nyear={2019},\nurl={https://openreview.net/forum?id=Hke8Do0cF7},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 5, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "Bkg3Wh4-xN", "original": null, "number": 1, "cdate": 1544797187740, "ddate": null, "tcdate": 1544797187740, "tmdate": 1545354525450, "tddate": null, "forum": "Hke8Do0cF7", "replyto": "Hke8Do0cF7", "invitation": "ICLR.cc/2019/Conference/-/Paper268/Meta_Review", "content": {"metareview": "The reviewers agree this paper is not good enough for ICLR.", "confidence": "5: The area chair is absolutely certain", "recommendation": "Reject", "title": "Not acceptable in current form"}, "signatures": ["ICLR.cc/2019/Conference/Paper268/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper268/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep processing of structured data", "abstract": "We construct a general unified framework for learning representation of structured\ndata, i.e. data which cannot be represented as the fixed-length vectors (e.g. sets,\ngraphs, texts or images of varying sizes). The key factor is played by an intermediate\nnetwork called SAN (Set Aggregating Network), which maps a structured\nobject to a fixed length vector in a high dimensional latent space. Our main theoretical\nresult shows that for sufficiently large dimension of the latent space, SAN is\ncapable of learning a unique representation for every input example. Experiments\ndemonstrate that replacing pooling operation by SAN in convolutional networks\nleads to better results in classifying images with different sizes. Moreover, its direct\napplication to text and graph data allows to obtain results close to SOTA, by\nsimpler networks with smaller number of parameters than competitive models.", "keywords": ["structured data", "representation learning", "deep neural networks"], "authorids": ["l.maziarka@gmail.com", "marek.smieja@uj.edu.pl", "aknoow@gmail.com", "jacek.tabor@uj.edu.pl", "lukasz.struski@uj.edu.pl", "przemyslaw.spurek@uj.edu.pl"], "authors": ["\u0141ukasz Maziarka", "Marek \u015amieja", "Aleksandra Nowak", "Jacek Tabor", "\u0141ukasz Struski", "Przemys\u0142aw Spurek"], "TL;DR": "General framework of learning representation of structured inputs.", "pdf": "/pdf/d35f16cffb07c6c0a82faee9f91dc1e905481a36.pdf", "paperhash": "maziarka|deep_processing_of_structured_data", "_bibtex": "@misc{\nmaziarka2019deep,\ntitle={Deep processing of structured data},\nauthor={\u0141ukasz Maziarka and Marek \u015amieja and Aleksandra Nowak and Jacek Tabor and \u0141ukasz Struski and Przemys\u0142aw Spurek},\nyear={2019},\nurl={https://openreview.net/forum?id=Hke8Do0cF7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper268/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353275078, "tddate": null, "super": null, "final": null, "reply": {"forum": "Hke8Do0cF7", "replyto": "Hke8Do0cF7", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper268/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper268/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper268/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353275078}}}, {"id": "HklXqCVrkN", "original": null, "number": 4, "cdate": 1544011403207, "ddate": null, "tcdate": 1544011403207, "tmdate": 1544011403207, "tddate": null, "forum": "Hke8Do0cF7", "replyto": "ryeYlC9H07", "invitation": "ICLR.cc/2019/Conference/-/Paper268/Official_Comment", "content": {"title": "?", "comment": "The only comment from the authors that I see is that they agree that the paper isn't ready. So, we all agree. What a wonderful, harmonic world ;)\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper268/AnonReviewer3"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper268/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper268/AnonReviewer3", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep processing of structured data", "abstract": "We construct a general unified framework for learning representation of structured\ndata, i.e. data which cannot be represented as the fixed-length vectors (e.g. sets,\ngraphs, texts or images of varying sizes). The key factor is played by an intermediate\nnetwork called SAN (Set Aggregating Network), which maps a structured\nobject to a fixed length vector in a high dimensional latent space. Our main theoretical\nresult shows that for sufficiently large dimension of the latent space, SAN is\ncapable of learning a unique representation for every input example. Experiments\ndemonstrate that replacing pooling operation by SAN in convolutional networks\nleads to better results in classifying images with different sizes. Moreover, its direct\napplication to text and graph data allows to obtain results close to SOTA, by\nsimpler networks with smaller number of parameters than competitive models.", "keywords": ["structured data", "representation learning", "deep neural networks"], "authorids": ["l.maziarka@gmail.com", "marek.smieja@uj.edu.pl", "aknoow@gmail.com", "jacek.tabor@uj.edu.pl", "lukasz.struski@uj.edu.pl", "przemyslaw.spurek@uj.edu.pl"], "authors": ["\u0141ukasz Maziarka", "Marek \u015amieja", "Aleksandra Nowak", "Jacek Tabor", "\u0141ukasz Struski", "Przemys\u0142aw Spurek"], "TL;DR": "General framework of learning representation of structured inputs.", "pdf": "/pdf/d35f16cffb07c6c0a82faee9f91dc1e905481a36.pdf", "paperhash": "maziarka|deep_processing_of_structured_data", "_bibtex": "@misc{\nmaziarka2019deep,\ntitle={Deep processing of structured data},\nauthor={\u0141ukasz Maziarka and Marek \u015amieja and Aleksandra Nowak and Jacek Tabor and \u0141ukasz Struski and Przemys\u0142aw Spurek},\nyear={2019},\nurl={https://openreview.net/forum?id=Hke8Do0cF7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper268/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621622712, "tddate": null, "super": null, "final": null, "reply": {"forum": "Hke8Do0cF7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper268/Authors", "ICLR.cc/2019/Conference/Paper268/Reviewers", "ICLR.cc/2019/Conference/Paper268/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper268/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper268/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper268/Authors|ICLR.cc/2019/Conference/Paper268/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper268/Reviewers", "ICLR.cc/2019/Conference/Paper268/Authors", "ICLR.cc/2019/Conference/Paper268/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621622712}}}, {"id": "B1lI6n7_a7", "original": null, "number": 3, "cdate": 1542106302445, "ddate": null, "tcdate": 1542106302445, "tmdate": 1542106302445, "tddate": null, "forum": "Hke8Do0cF7", "replyto": "Hke8Do0cF7", "invitation": "ICLR.cc/2019/Conference/-/Paper268/Official_Review", "content": {"title": "review", "review": "Summary:\n\nThe paper argues that plain (fully connected) neural networks cannot represent structured data, e.g. sequences, graphs, etc. Specialized architectures have instead been invented for each such case, e.g. recurrent neural networks, graph networks etc. The paper then proposes to treat these structured data types as sets and propose a neural network method for converting a set into a fixed size vector, which can then be classified using plain neural networks. The method works by projecting each member of the set into M dimensions where they are passed through a ReLU and summed. The paper proves that given high enough dimensionality M, no information will be lost during this process. The paper performs experiments on graph data, text data and image data. The paper concludes \"We proposed a general framework for processing structured data by neural network.\"\n\nQuality:\n\nThe paper seems rushed. I think the paper has some language problems which unfortunately detract from the overall impression. \"through its all one-dimensional projections\". Should that be \"through all its one-dimensional projections\"? \"Analogically\". \"can lead to the >lose< of meaningful information\". \"where (the?) Radon transform\". \n\nClarity:\n\nThe aim and purpose of the paper is fairly clear. I think the method explanation is overly complicated. If I understand correctly, the proposed method is simply to map each set element to an M dimensional space using a linear projection followed by a ReLU and then summing the set elements. The section on how to implement it in practice further complicates the paper unnecessarily in my opinion. It's not clear why or whether the ReLU is important.\n\nOriginality:\n\nTransforming sets into fixed size vectors is not new. See e.g. [1] and [2], which the paper does not reference or compare to.\nTo the papers defence I think the main idea is to map any structured data into a set, and then use a (relatively) simple method on it.\n\nSignificance:\n\nThe paper acknowledges that methods do indeed exist for the various structured data types, but claim that they are complex, and that the proposed method is a simple general alternative. As such the significance of the paper hinges on whether the proposed method is indeed simpler, and how it compares when it comes to performance. The proposed method is simple and general, but it does require that the information lost when converting the structured data to the set is encoded as features in the set elements, e.g. the sequence index is added. For images, the normalized position must be added. For graphs, the edges should be added, which interestingly is not done in the single graph experiment (I'd be curious how the edges could be added). This detracts somewhat from the claim to generality, since each structured data type must still be handled differently.\n\nIt's clear to me that the structural priors built into the specialized networks, e.g. recurrent, graph, etc. should help for these data structures. For this reason I think the proposed method will have a hard time comparing head to head. That is OK, it becomes a tradeoff between generality versus performance. Unfortunately I'm not convinced of the performance by the experiments. Specifically the proposed method is not compared head to head against the methods it proposes to replace. \n * On the graph data it is compared against various classifiers that use a fixed size, handcrafted representation. This is disingenuous in my opinion. For graph data, the natural method it proposed to replace is a graph neural network. \n * On the sequence data it is compared against a 1D convolutional neural network. The canonical sequence model is a recurrent neural network. Also for the larger IMDB dataset the performance drop against a CNN is considerable.\n * The image experiments are probably the strongest case. Here the authors make a compelling case that the SAN pooling operator is better than max-pooling. Unfortunately the authors use a dataset which have already been size normalized during pre-processing, and then un-normalize it. It'd be more convincing if the authors showed superior performance on a dataset of images of varying sizes using SAN, compared to normalizing them during pre-processing (with comparable runtime and parameter counts).\n\nPros:\n - Ambitious aim\n - Simple method\n\nCons: \n - Proposing to replace xyz methods, then not fairly comparing to them in their respective domains.\n - Not referencing relevant prior work on neural networks for sets.\n - Paper seems rushed/unfinished\n - Implicitly assumes classification/regression for entire structured data. In many cases, e.g. Named Entity Recognition, and many graph problems, the problem is to classify/regress on the individual elements, as part of the whole.\n\nI could see the paper accepted, if sold less as the end-all solution for structured data and more as simply an improved pooling operator, with SOTA experiments to back up the claims. This would also allow the authors to use SOTA methods for pre-processing the data, e.g. RNNs, graph neural networks, etc., as they don't need to compete against every method. \n\n(btw. instead of zero padding the batches, the authors can probably maintain a list of indices of set elements and use https://www.tensorflow.org/api_docs/python/tf/math/segment_sum)\n\n[1] - Vinyals, Oriol, Samy Bengio, and Manjunath Kudlur. \"Order matters: Sequence to sequence for sets.\" arXiv preprint arXiv:1511.06391 (2015).\n[2] - Zaheer, Manzil, et al. \"Deep sets.\" Advances in Neural Information Processing Systems. 2017.", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper268/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep processing of structured data", "abstract": "We construct a general unified framework for learning representation of structured\ndata, i.e. data which cannot be represented as the fixed-length vectors (e.g. sets,\ngraphs, texts or images of varying sizes). The key factor is played by an intermediate\nnetwork called SAN (Set Aggregating Network), which maps a structured\nobject to a fixed length vector in a high dimensional latent space. Our main theoretical\nresult shows that for sufficiently large dimension of the latent space, SAN is\ncapable of learning a unique representation for every input example. Experiments\ndemonstrate that replacing pooling operation by SAN in convolutional networks\nleads to better results in classifying images with different sizes. Moreover, its direct\napplication to text and graph data allows to obtain results close to SOTA, by\nsimpler networks with smaller number of parameters than competitive models.", "keywords": ["structured data", "representation learning", "deep neural networks"], "authorids": ["l.maziarka@gmail.com", "marek.smieja@uj.edu.pl", "aknoow@gmail.com", "jacek.tabor@uj.edu.pl", "lukasz.struski@uj.edu.pl", "przemyslaw.spurek@uj.edu.pl"], "authors": ["\u0141ukasz Maziarka", "Marek \u015amieja", "Aleksandra Nowak", "Jacek Tabor", "\u0141ukasz Struski", "Przemys\u0142aw Spurek"], "TL;DR": "General framework of learning representation of structured inputs.", "pdf": "/pdf/d35f16cffb07c6c0a82faee9f91dc1e905481a36.pdf", "paperhash": "maziarka|deep_processing_of_structured_data", "_bibtex": "@misc{\nmaziarka2019deep,\ntitle={Deep processing of structured data},\nauthor={\u0141ukasz Maziarka and Marek \u015amieja and Aleksandra Nowak and Jacek Tabor and \u0141ukasz Struski and Przemys\u0142aw Spurek},\nyear={2019},\nurl={https://openreview.net/forum?id=Hke8Do0cF7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper268/Official_Review", "cdate": 1542234500526, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "Hke8Do0cF7", "replyto": "Hke8Do0cF7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper268/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335686314, "tmdate": 1552335686314, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper268/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "Byg3dPot37", "original": null, "number": 2, "cdate": 1541154676210, "ddate": null, "tcdate": 1541154676210, "tmdate": 1541534142838, "tddate": null, "forum": "Hke8Do0cF7", "replyto": "Hke8Do0cF7", "invitation": "ICLR.cc/2019/Conference/-/Paper268/Official_Review", "content": {"title": "The paper \"DEEP PROCESSING OF STRUCTURED DATA\" is about transforming an arbitrary number of fixed-length input vectors to a fixed-length output vector for a neural network.", "review": "+ Results for SAN are on par with hand-crafted feature sets.\n+ An easy to implement strategy to get from a set of an arbitrary number of fixed-length input vectors to one fixed-length output vector representing the set.\n\n\n- The work is quite straight-forward and incremental as it basically replaces max-pooling with another function (RELU).\n- Adding zero tuples (x,0) as input for the SAN is basically zero-padding. Therefore, the differences/advantages to zero padding and other pooling strategies do not become clear.\n- The obvious baseline of using doc2vec for text classification is missing. This baseline would be beneficial to set the results into context. \n- Reference to Schlichtkrull et al. [1] is missing who are actually doing something quite similar for graph data.\n\n\nSince the work is quite straight-forward and the comparisons to related work are not sufficiently covered, I am currently leaning towards rejecting this paper.\n\n\n\n[1] Schlichtkrull, M., Kipf, T. N., Bloem, P., van den Berg, R., Titov, I., & Welling, M. (2018, June). Modeling relational data with graph convolutional networks. In European Semantic Web Conference (pp. 593-607). Springer, Cham.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper268/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep processing of structured data", "abstract": "We construct a general unified framework for learning representation of structured\ndata, i.e. data which cannot be represented as the fixed-length vectors (e.g. sets,\ngraphs, texts or images of varying sizes). The key factor is played by an intermediate\nnetwork called SAN (Set Aggregating Network), which maps a structured\nobject to a fixed length vector in a high dimensional latent space. Our main theoretical\nresult shows that for sufficiently large dimension of the latent space, SAN is\ncapable of learning a unique representation for every input example. Experiments\ndemonstrate that replacing pooling operation by SAN in convolutional networks\nleads to better results in classifying images with different sizes. Moreover, its direct\napplication to text and graph data allows to obtain results close to SOTA, by\nsimpler networks with smaller number of parameters than competitive models.", "keywords": ["structured data", "representation learning", "deep neural networks"], "authorids": ["l.maziarka@gmail.com", "marek.smieja@uj.edu.pl", "aknoow@gmail.com", "jacek.tabor@uj.edu.pl", "lukasz.struski@uj.edu.pl", "przemyslaw.spurek@uj.edu.pl"], "authors": ["\u0141ukasz Maziarka", "Marek \u015amieja", "Aleksandra Nowak", "Jacek Tabor", "\u0141ukasz Struski", "Przemys\u0142aw Spurek"], "TL;DR": "General framework of learning representation of structured inputs.", "pdf": "/pdf/d35f16cffb07c6c0a82faee9f91dc1e905481a36.pdf", "paperhash": "maziarka|deep_processing_of_structured_data", "_bibtex": "@misc{\nmaziarka2019deep,\ntitle={Deep processing of structured data},\nauthor={\u0141ukasz Maziarka and Marek \u015amieja and Aleksandra Nowak and Jacek Tabor and \u0141ukasz Struski and Przemys\u0142aw Spurek},\nyear={2019},\nurl={https://openreview.net/forum?id=Hke8Do0cF7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper268/Official_Review", "cdate": 1542234500526, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "Hke8Do0cF7", "replyto": "Hke8Do0cF7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper268/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335686314, "tmdate": 1552335686314, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper268/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "Hylth5Zwhm", "original": null, "number": 1, "cdate": 1540983473030, "ddate": null, "tcdate": 1540983473030, "tmdate": 1541534142631, "tddate": null, "forum": "Hke8Do0cF7", "replyto": "Hke8Do0cF7", "invitation": "ICLR.cc/2019/Conference/-/Paper268/Official_Review", "content": {"title": "Review", "review": "The paper at hand describes an approach to enable neural networks to take arbitrary structured data (basically any data that is not easily represented as a fixed-dimensional vector) as input. The paper describes how such data can be represented as a set (e.g. a sequence is represented as a set of index + data) and then an auxiliary network called the set aggregating network (SAN)  is used to represent the data in a high dimensional latent space. In addition to the idea, the paper provides a theoretical analysis of the approach which shows that with a sufficiently high dimensional representation the network is able to learn a unique representation for each input example. \n\nGood in this paper: \n - nicely on topic - this is truly about learning representations\n - interesting idea with some (albeit not-surprising) theoretical analysis\n\nNot yet great in this paper: \n - the paper feels a bit premature in multiple ways, to me most importantly the experiments appear to be really rushed. \n  Looking at table 1 - it is really unclear how to read this. The table is hardly explained and it would be good to actually compare the method to the state of the art. I understand that the authors care more about generality here - but it's much easier to build something generic that is very far from the state of the art than to build something that is closer to the state of the art. Also - I feel it would have been interesting to allow for a more direct comparison of the SAN results with the other methods. Similarly, in Table 2 - how far away is this from the state of the art.\n- the variable size image-recognition task seems a bit artificial - I believe that scaling images to a fixed size is a reasonable idea and is well understood and also something that humans can work with. Dealing with variable size images for the purpose of not having a fixed size vector seems artificial and unnecessary - in this case here specifically the images are same size to begin with. By using SAN you loose a lot of the understanding of computer vision research of the last decade (e.g. it's clear that CNNs are a good idea - with SAN - you cannot really do that anymore) - so, I feel this experiment here doesn't add anything. \n\nI feel these comments can probably be addressed by rethinking the experimental evaluation. At this point, I think the paper provides a nice idea with a theoretical analysis - but it doesn't provide enough experimental evidence that this works. \n\nMinor comments: \n - p1: Analogically -> just drop the word\n- p1: citation of Brin & Page -> this seems a bit out of place - yes, it is a method working on graphs, but it is not relevant in the context of the paper - to the best of my knowledge there are no neural networks in this. \n- p2: where Radon - >where the Radon\n- p2: \"One can easily see \" -> I cannot. Please ellaborate", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper268/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep processing of structured data", "abstract": "We construct a general unified framework for learning representation of structured\ndata, i.e. data which cannot be represented as the fixed-length vectors (e.g. sets,\ngraphs, texts or images of varying sizes). The key factor is played by an intermediate\nnetwork called SAN (Set Aggregating Network), which maps a structured\nobject to a fixed length vector in a high dimensional latent space. Our main theoretical\nresult shows that for sufficiently large dimension of the latent space, SAN is\ncapable of learning a unique representation for every input example. Experiments\ndemonstrate that replacing pooling operation by SAN in convolutional networks\nleads to better results in classifying images with different sizes. Moreover, its direct\napplication to text and graph data allows to obtain results close to SOTA, by\nsimpler networks with smaller number of parameters than competitive models.", "keywords": ["structured data", "representation learning", "deep neural networks"], "authorids": ["l.maziarka@gmail.com", "marek.smieja@uj.edu.pl", "aknoow@gmail.com", "jacek.tabor@uj.edu.pl", "lukasz.struski@uj.edu.pl", "przemyslaw.spurek@uj.edu.pl"], "authors": ["\u0141ukasz Maziarka", "Marek \u015amieja", "Aleksandra Nowak", "Jacek Tabor", "\u0141ukasz Struski", "Przemys\u0142aw Spurek"], "TL;DR": "General framework of learning representation of structured inputs.", "pdf": "/pdf/d35f16cffb07c6c0a82faee9f91dc1e905481a36.pdf", "paperhash": "maziarka|deep_processing_of_structured_data", "_bibtex": "@misc{\nmaziarka2019deep,\ntitle={Deep processing of structured data},\nauthor={\u0141ukasz Maziarka and Marek \u015amieja and Aleksandra Nowak and Jacek Tabor and \u0141ukasz Struski and Przemys\u0142aw Spurek},\nyear={2019},\nurl={https://openreview.net/forum?id=Hke8Do0cF7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper268/Official_Review", "cdate": 1542234500526, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "Hke8Do0cF7", "replyto": "Hke8Do0cF7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper268/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335686314, "tmdate": 1552335686314, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper268/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 6}