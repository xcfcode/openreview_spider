{"notes": [{"id": "CYHMIhbuLFl", "original": "gs9a3E0rC6_", "number": 1881, "cdate": 1601308207332, "ddate": null, "tcdate": 1601308207332, "tmdate": 1614985738286, "tddate": null, "forum": "CYHMIhbuLFl", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Contextual HyperNetworks  for Novel Feature Adaptation", "authorids": ["~Angus_Lamb1", "e.s.saveliev@gmail.com", "~Yingzhen_Li1", "~Sebastian_Tschiatschek1", "camilla.longden@microsoft.com", "simon.woodhead@eedi.co.uk", "~Jos\u00e9_Miguel_Hern\u00e1ndez-Lobato1", "~Richard_E_Turner1", "~Pashmina_Cameron1", "~Cheng_Zhang1"], "authors": ["Angus Lamb", "Evgeny Saveliev", "Yingzhen Li", "Sebastian Tschiatschek", "Camilla Longden", "Simon Woodhead", "Jos\u00e9 Miguel Hern\u00e1ndez-Lobato", "Richard E Turner", "Pashmina Cameron", "Cheng Zhang"], "keywords": ["Meta learning", "few-shot learning", "continual learning", "recommender systems", "deep learning"], "abstract": "While deep learning has obtained state-of-the-art results in many applications, the adaptation of neural network architectures to incorporate new features remains a research challenge. This issue is particularly severe in online learning settings, where new features are added continually with few or no associated observations. As such, methods for adapting neural networks to novel features which are both time and data-efficient are desired. To address this, we propose the Contextual HyperNetwork (CHN), which predicts the network weights associated with new features by incorporating information from both existing data as well as the few observations for the new feature and any associated feature metadata. At prediction time, the CHN requires only a single forward pass through a small neural network, yielding a significant speed-up when compared to re-training and fine-tuning approaches. In order to showcase the performance of CHNs, in this work we use a CHN to augment a partial variational autoencoder (P-VAE), a flexible deep generative model which can impute the values of missing features in sparsely-observed data. We show that this system obtains significantly improved performance for novel feature adaptation over existing imputation and meta-learning baselines across recommender systems, e-learning, and healthcare tasks.", "one-sentence_summary": "We introduce an auxiliary neural network to extend existing neural networks to make accurate predictions for new features in the few-shot learning regime, given a small number of observations and/or metadata for the new feature.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lamb|contextual_hypernetworks_for_novel_feature_adaptation", "supplementary_material": "", "pdf": "/pdf/dad11a3cea27e7d0d62ec1154262f381e27fe808.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=TFFjU4Nq8F", "_bibtex": "@misc{\nlamb2021contextual,\ntitle={Contextual HyperNetworks  for Novel Feature Adaptation},\nauthor={Angus Lamb and Evgeny Saveliev and Yingzhen Li and Sebastian Tschiatschek and Camilla Longden and Simon Woodhead and Jos{\\'e} Miguel Hern{\\'a}ndez-Lobato and Richard E Turner and Pashmina Cameron and Cheng Zhang},\nyear={2021},\nurl={https://openreview.net/forum?id=CYHMIhbuLFl}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 11, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "2j53W2-QTd", "original": null, "number": 1, "cdate": 1610040400057, "ddate": null, "tcdate": 1610040400057, "tmdate": 1610473995875, "tddate": null, "forum": "CYHMIhbuLFl", "replyto": "CYHMIhbuLFl", "invitation": "ICLR.cc/2021/Conference/Paper1881/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "This work presents a straightforward and easy to understand method for using hypernetworks to adapt existing models to be able to increase their output space. The method itself is also interesting and is detailed enough for reproducibility. However, the experiments and results should be improved by expanding the demonstration of CHNs beyond the narrow P-VAE application and comparing against relevant baselines in the recommendation system literature.\n\nPros\n- Clear writing.\n- Detailed hyperparameters to aid reproducibility.\n- Straightforward model.\n\nCons\n- Lack of sufficient comparison to related work, especially to existing recommendation systems that handle the cold-start issue and to Vartak, 2017.\n- Limited results that only demonstrate application to P-VAE meaning it's still unknown if CHNs work well with other models. The result on the synthetic dataset is also less persuasive.\n- Lack of sufficient ablations, i.e. training a SVM/linear regression model until convergence."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Contextual HyperNetworks  for Novel Feature Adaptation", "authorids": ["~Angus_Lamb1", "e.s.saveliev@gmail.com", "~Yingzhen_Li1", "~Sebastian_Tschiatschek1", "camilla.longden@microsoft.com", "simon.woodhead@eedi.co.uk", "~Jos\u00e9_Miguel_Hern\u00e1ndez-Lobato1", "~Richard_E_Turner1", "~Pashmina_Cameron1", "~Cheng_Zhang1"], "authors": ["Angus Lamb", "Evgeny Saveliev", "Yingzhen Li", "Sebastian Tschiatschek", "Camilla Longden", "Simon Woodhead", "Jos\u00e9 Miguel Hern\u00e1ndez-Lobato", "Richard E Turner", "Pashmina Cameron", "Cheng Zhang"], "keywords": ["Meta learning", "few-shot learning", "continual learning", "recommender systems", "deep learning"], "abstract": "While deep learning has obtained state-of-the-art results in many applications, the adaptation of neural network architectures to incorporate new features remains a research challenge. This issue is particularly severe in online learning settings, where new features are added continually with few or no associated observations. As such, methods for adapting neural networks to novel features which are both time and data-efficient are desired. To address this, we propose the Contextual HyperNetwork (CHN), which predicts the network weights associated with new features by incorporating information from both existing data as well as the few observations for the new feature and any associated feature metadata. At prediction time, the CHN requires only a single forward pass through a small neural network, yielding a significant speed-up when compared to re-training and fine-tuning approaches. In order to showcase the performance of CHNs, in this work we use a CHN to augment a partial variational autoencoder (P-VAE), a flexible deep generative model which can impute the values of missing features in sparsely-observed data. We show that this system obtains significantly improved performance for novel feature adaptation over existing imputation and meta-learning baselines across recommender systems, e-learning, and healthcare tasks.", "one-sentence_summary": "We introduce an auxiliary neural network to extend existing neural networks to make accurate predictions for new features in the few-shot learning regime, given a small number of observations and/or metadata for the new feature.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lamb|contextual_hypernetworks_for_novel_feature_adaptation", "supplementary_material": "", "pdf": "/pdf/dad11a3cea27e7d0d62ec1154262f381e27fe808.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=TFFjU4Nq8F", "_bibtex": "@misc{\nlamb2021contextual,\ntitle={Contextual HyperNetworks  for Novel Feature Adaptation},\nauthor={Angus Lamb and Evgeny Saveliev and Yingzhen Li and Sebastian Tschiatschek and Camilla Longden and Simon Woodhead and Jos{\\'e} Miguel Hern{\\'a}ndez-Lobato and Richard E Turner and Pashmina Cameron and Cheng Zhang},\nyear={2021},\nurl={https://openreview.net/forum?id=CYHMIhbuLFl}\n}"}, "tags": [], "invitation": {"reply": {"forum": "CYHMIhbuLFl", "replyto": "CYHMIhbuLFl", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040400042, "tmdate": 1610473995857, "id": "ICLR.cc/2021/Conference/Paper1881/-/Decision"}}}, {"id": "s7A5HdQjHmU", "original": null, "number": 2, "cdate": 1603500193040, "ddate": null, "tcdate": 1603500193040, "tmdate": 1607014948690, "tddate": null, "forum": "CYHMIhbuLFl", "replyto": "CYHMIhbuLFl", "invitation": "ICLR.cc/2021/Conference/Paper1881/-/Official_Review", "content": {"title": "Review (update)", "review": "The paper proposes a few-shot meta-learning method for recommender system that uses a new feature's meta-information and observed samples for the features to predict the network weights for predicting the feature value from other features. The paper focuses on the cold-start problem where few samples with a new feature observed is available. The method outperforms a wide range of baselines on MovieLens-1M, a medical synthetic dataset, and a e-learning dataset.\n\nDisclaimer: I am not familiar with recommender systems.\n\nPros:\n- Methods straightforward and easy to understand, and does not have much ad-hoc design choices that are hard to validate.\n- The experiments are relatively thorough. A lot of baselines.\n\nCons\n- Novelty is limited -- a mixture of meta-learning and content-based method. There are also existing few-shot continual learning papers available (e.g. https://openaccess.thecvf.com/content_CVPR_2020/papers/Tao_Few-Shot_Class-Incremental_Learning_CVPR_2020_paper.pdf), so the motivation needs to consider differences from them.\n- Experiments does not do external comparisons with other recommender systems that deal with cold starting other than its own baselines.\n- Despite the large amount of baselines, ablation still lacks: (1) Train from random, but instead of training for a fixed number of epochs, just train an SVM or linear regression until convergence. This is what people would do as a baseline. (2) not important but it would be nice to ablate the CHN by taking out metadata and $C_n$ from input.\n- Not clear if the set of datasets is persuasive. One is a synthetic dataset. One recommendation system and one grading dataset.\n- There is no Appendix despite referencing it.\n\nMinor issues:\n- Not clear how the \"adapting to new features\" is different from meta-learning's adapting to new tasks, since they also use old tasks to inform new tasks. The abstract seems to suggest the method can take new features as input incrementally, which is a little confusing.\n- Overclaim at the end of Section 2.2: it is not O(1) if you have to do distributed computing. If you allow distributed computing, NP=P.\n- It's hard to tell why MAML is similar to CHN in one dataset but underperforms drastically in the e-learning dataset.\n\n\nPost-rebuttal:\nI appreciate the additional ablation study, but unfortunately the results did not strengthen the paper's distinction from related work. The explanation of the motives and related work comparisons only clarified differences between this paper and prior work that are either inherent to the task being addressed, or contribution unsupported by experiments. Unless the AC agrees with the authors that the paper is acceptable even without external comparisons (despite being a merge of two lines of work), I will not be changing my score.", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1881/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1881/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Contextual HyperNetworks  for Novel Feature Adaptation", "authorids": ["~Angus_Lamb1", "e.s.saveliev@gmail.com", "~Yingzhen_Li1", "~Sebastian_Tschiatschek1", "camilla.longden@microsoft.com", "simon.woodhead@eedi.co.uk", "~Jos\u00e9_Miguel_Hern\u00e1ndez-Lobato1", "~Richard_E_Turner1", "~Pashmina_Cameron1", "~Cheng_Zhang1"], "authors": ["Angus Lamb", "Evgeny Saveliev", "Yingzhen Li", "Sebastian Tschiatschek", "Camilla Longden", "Simon Woodhead", "Jos\u00e9 Miguel Hern\u00e1ndez-Lobato", "Richard E Turner", "Pashmina Cameron", "Cheng Zhang"], "keywords": ["Meta learning", "few-shot learning", "continual learning", "recommender systems", "deep learning"], "abstract": "While deep learning has obtained state-of-the-art results in many applications, the adaptation of neural network architectures to incorporate new features remains a research challenge. This issue is particularly severe in online learning settings, where new features are added continually with few or no associated observations. As such, methods for adapting neural networks to novel features which are both time and data-efficient are desired. To address this, we propose the Contextual HyperNetwork (CHN), which predicts the network weights associated with new features by incorporating information from both existing data as well as the few observations for the new feature and any associated feature metadata. At prediction time, the CHN requires only a single forward pass through a small neural network, yielding a significant speed-up when compared to re-training and fine-tuning approaches. In order to showcase the performance of CHNs, in this work we use a CHN to augment a partial variational autoencoder (P-VAE), a flexible deep generative model which can impute the values of missing features in sparsely-observed data. We show that this system obtains significantly improved performance for novel feature adaptation over existing imputation and meta-learning baselines across recommender systems, e-learning, and healthcare tasks.", "one-sentence_summary": "We introduce an auxiliary neural network to extend existing neural networks to make accurate predictions for new features in the few-shot learning regime, given a small number of observations and/or metadata for the new feature.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lamb|contextual_hypernetworks_for_novel_feature_adaptation", "supplementary_material": "", "pdf": "/pdf/dad11a3cea27e7d0d62ec1154262f381e27fe808.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=TFFjU4Nq8F", "_bibtex": "@misc{\nlamb2021contextual,\ntitle={Contextual HyperNetworks  for Novel Feature Adaptation},\nauthor={Angus Lamb and Evgeny Saveliev and Yingzhen Li and Sebastian Tschiatschek and Camilla Longden and Simon Woodhead and Jos{\\'e} Miguel Hern{\\'a}ndez-Lobato and Richard E Turner and Pashmina Cameron and Cheng Zhang},\nyear={2021},\nurl={https://openreview.net/forum?id=CYHMIhbuLFl}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "CYHMIhbuLFl", "replyto": "CYHMIhbuLFl", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1881/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538108693, "tmdate": 1606915772624, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1881/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1881/-/Official_Review"}}}, {"id": "QkN9bzywC3E", "original": null, "number": 8, "cdate": 1606283774879, "ddate": null, "tcdate": 1606283774879, "tmdate": 1606283774879, "tddate": null, "forum": "CYHMIhbuLFl", "replyto": "8m4CtbYpr99", "invitation": "ICLR.cc/2021/Conference/Paper1881/-/Official_Comment", "content": {"title": "Areas of disagreement with the rebuttal", "comment": "> Our approach differs from existing meta-learning... For instance, if we are given an image of a red car and a new class to be learned is \u201cfour wheeled vehicles\u201d, most meta-learning approaches would not make explicit use of the fact that this data point may have been labelled as \u201ccar\u201d or \u201cred\u201d in previous tasks, instead relying on intermediate feature representations from a classifier or adapting parameters.\n\nI see the difference, but I see no problem relying on intermediate feature representations -- for example, in prototypical networks for few-shot learning, the embedding can be learned in such a way that the average representation of a class can be used as its classifier. Images for \"four wheeled vehicles\" and images for \"car\" and \"red\" would have similarities in their features, and the prototypical networks can use past task information as well. The only difference from these methods is then the introduction of explicit meta information, which I was asking for ablation studies. Unfortunately it does not show a significant difference (\"...ablation for the use of metadata on MovieLens-1M in Appendix C.5. This shows a very small effect...\"). Therefore, a reader would like to know what is different in practice about this method vs other few-shot learning papers, which the experiments do not provide. \n\nAlso regarding using other tasks' info: it is especially hard to NOT directly use other tasks in a recommender system, seeing that prior tasks are literally used as features as the input, so using meta-learning / few-shot learning on recommender systems almost guarantees directly using info from other tasks, so I am skeptical about claiming this as a separate difference than just applying those methods in recommender systems.\n\n> In our experiments we aimed to assess the efficacy of CHNs versus a range of baselines when applied to a consistent model, in order to ensure a fair comparison between the methods...\n\nIf the paper only wants to prove their additions to be effective, I see much less impact because inferring classifiers for few-shot learning is proven to work in prior prototypical few-shot learning work, and adding meta-info to improve performance sounds straightforward.\n\n> Appendix\n\nTerribly sorry about that! I was not aware that openreview allows supplemental material.\n\n> Despite the large amount of baselines, ablation still lacks: (1) Train from random...\n\nThis critique is missed in the rebuttal."}, "signatures": ["ICLR.cc/2021/Conference/Paper1881/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1881/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Contextual HyperNetworks  for Novel Feature Adaptation", "authorids": ["~Angus_Lamb1", "e.s.saveliev@gmail.com", "~Yingzhen_Li1", "~Sebastian_Tschiatschek1", "camilla.longden@microsoft.com", "simon.woodhead@eedi.co.uk", "~Jos\u00e9_Miguel_Hern\u00e1ndez-Lobato1", "~Richard_E_Turner1", "~Pashmina_Cameron1", "~Cheng_Zhang1"], "authors": ["Angus Lamb", "Evgeny Saveliev", "Yingzhen Li", "Sebastian Tschiatschek", "Camilla Longden", "Simon Woodhead", "Jos\u00e9 Miguel Hern\u00e1ndez-Lobato", "Richard E Turner", "Pashmina Cameron", "Cheng Zhang"], "keywords": ["Meta learning", "few-shot learning", "continual learning", "recommender systems", "deep learning"], "abstract": "While deep learning has obtained state-of-the-art results in many applications, the adaptation of neural network architectures to incorporate new features remains a research challenge. This issue is particularly severe in online learning settings, where new features are added continually with few or no associated observations. As such, methods for adapting neural networks to novel features which are both time and data-efficient are desired. To address this, we propose the Contextual HyperNetwork (CHN), which predicts the network weights associated with new features by incorporating information from both existing data as well as the few observations for the new feature and any associated feature metadata. At prediction time, the CHN requires only a single forward pass through a small neural network, yielding a significant speed-up when compared to re-training and fine-tuning approaches. In order to showcase the performance of CHNs, in this work we use a CHN to augment a partial variational autoencoder (P-VAE), a flexible deep generative model which can impute the values of missing features in sparsely-observed data. We show that this system obtains significantly improved performance for novel feature adaptation over existing imputation and meta-learning baselines across recommender systems, e-learning, and healthcare tasks.", "one-sentence_summary": "We introduce an auxiliary neural network to extend existing neural networks to make accurate predictions for new features in the few-shot learning regime, given a small number of observations and/or metadata for the new feature.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lamb|contextual_hypernetworks_for_novel_feature_adaptation", "supplementary_material": "", "pdf": "/pdf/dad11a3cea27e7d0d62ec1154262f381e27fe808.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=TFFjU4Nq8F", "_bibtex": "@misc{\nlamb2021contextual,\ntitle={Contextual HyperNetworks  for Novel Feature Adaptation},\nauthor={Angus Lamb and Evgeny Saveliev and Yingzhen Li and Sebastian Tschiatschek and Camilla Longden and Simon Woodhead and Jos{\\'e} Miguel Hern{\\'a}ndez-Lobato and Richard E Turner and Pashmina Cameron and Cheng Zhang},\nyear={2021},\nurl={https://openreview.net/forum?id=CYHMIhbuLFl}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "CYHMIhbuLFl", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1881/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1881/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1881/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1881/Authors|ICLR.cc/2021/Conference/Paper1881/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1881/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923854714, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1881/-/Official_Comment"}}}, {"id": "8m4CtbYpr99", "original": null, "number": 5, "cdate": 1606231437481, "ddate": null, "tcdate": 1606231437481, "tmdate": 1606236288848, "tddate": null, "forum": "CYHMIhbuLFl", "replyto": "s7A5HdQjHmU", "invitation": "ICLR.cc/2021/Conference/Paper1881/-/Official_Comment", "content": {"title": "Response", "comment": "Thank you for your review, please find our responses below:\n\n\u2022\t.. existing few-shot continual learning papers available...\n\nOur approach differs from existing meta-learning approaches in that CHN explicitly utilizes a data point\u2019s task labels for previous tasks (i.e. existing features in the model) when adapting to a new task. On the other hand, existing meta-learning methods for image classification applications typically treat each task independently and do NOT use previous task labels in an explicit manner. For instance, if we are given an image of a red car and a new class to be learned is \u201cfour wheeled vehicles\u201d, most meta-learning approaches would not make explicit use of the fact that this data point may have been labelled as \u201ccar\u201d or \u201cred\u201d in previous tasks, instead relying on intermediate feature representations from a classifier or adapting parameters. We instead sought a method for multi-task learning that could make explicit use of all such past labels for all data points in the context set, as these labels effective constitute the entirety of the data point in collaborative filtering-based approaches to recommender systems.\n\n\u2022\texternal comparisons with other recommender systems \n\nIn our experiments we aimed to assess the efficacy of CHNs versus a range of baselines when applied to a consistent model, in order to ensure a fair comparison between the methods: as such, we were more interested in assessing the relative ordering of performance between the methods in order to highlight the efficacy of CHNs, than the absolute performance of the overall system. To be clear, our contribution is intended to be the CHN rather than the P-VAE + CHN system, and the P-VAE is provided as an exemplar model. We do not contest that there are likely models in the recommender system literature which will yield better absolute performance, and hope that applying CHNs to such models may be a promising direction for future work in recommender systems.\n\n\u2022\tIt would be nice to ablate the CHN by taking out metadata and Cn from input.\n\nAn ablation of the performance of the CHN when $\\mathcal{C}_n$ is removed is effectively provided by the performance plots in the paper (e.g. Figure 5) by reading the performance at $k=0$, as this corresponds to an empty context set. We have additionally included an ablation for the use of metadata on MovieLens-1M in Appendix C.5. This shows a very small effect, which we believe is due to the feature metadata for MovieLens-1M being uninformative relative to the data points themselves. We will try to provide the additional baselines described if time permits.\n\n\u2022\tNot clear if the set of datasets is persuasive. \n\nWe tried to cover three realistic application scenarios for CHNs: both the recommender system and E-learning datasets are large (>1 million observations), with the education dataset one of the largest available. While a simulated dataset, the medical dataset was designed to preserve the properties of a corresponding real-world dataset while side-stepping the associated privacy concerns, and so we believe it represents a real world setting for medical applications.\n\n\u2022\tThere is no Appendix despite referencing it.\n\nThank you for bringing this to our attention: the appendices were included as supplementary material, these have been added to the main paper in the revised version.\n\n\u2022\tNot clear how the \"adapting to new features\" is different from meta-learning's adapting to new tasks, since they also use old tasks to inform new tasks. The abstract seems to suggest the method can take new features as input incrementally, which is a little confusing.\n\nSee response to first point. We want to learn a method that can explicitly reason about each data point in the context set\u2019s labels on other tasks, rather than treating each task as independent. . For example, the ratings of different movies from the same user are not independent tasks, but classifying between MNIST digits 1 and 2 and between digits 3 and 4 are independent tasks. \n\n\u2022\tOverclaim at the end of Section 2.2: it is not O(1) if you have to do distributed computing. If you allow distributed computing, NP=P.\n\nThank you for bringing this to our attention, we have updated the manuscript accordingly.\n\n\u2022\tIt's hard to tell why MAML is similar to CHN in one dataset but underperforms drastically in the e-learning dataset.\n\nAppendix B includes a detailed description of the protocol used for MAML, and it also includes figures for performance across a range of fine-tuning epoch numbers. Since MAML fine-tunes from a single shared head initialisation, it may be that some datasets require a greater degree of task-specific adaptation of the head parameters than others.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1881/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1881/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Contextual HyperNetworks  for Novel Feature Adaptation", "authorids": ["~Angus_Lamb1", "e.s.saveliev@gmail.com", "~Yingzhen_Li1", "~Sebastian_Tschiatschek1", "camilla.longden@microsoft.com", "simon.woodhead@eedi.co.uk", "~Jos\u00e9_Miguel_Hern\u00e1ndez-Lobato1", "~Richard_E_Turner1", "~Pashmina_Cameron1", "~Cheng_Zhang1"], "authors": ["Angus Lamb", "Evgeny Saveliev", "Yingzhen Li", "Sebastian Tschiatschek", "Camilla Longden", "Simon Woodhead", "Jos\u00e9 Miguel Hern\u00e1ndez-Lobato", "Richard E Turner", "Pashmina Cameron", "Cheng Zhang"], "keywords": ["Meta learning", "few-shot learning", "continual learning", "recommender systems", "deep learning"], "abstract": "While deep learning has obtained state-of-the-art results in many applications, the adaptation of neural network architectures to incorporate new features remains a research challenge. This issue is particularly severe in online learning settings, where new features are added continually with few or no associated observations. As such, methods for adapting neural networks to novel features which are both time and data-efficient are desired. To address this, we propose the Contextual HyperNetwork (CHN), which predicts the network weights associated with new features by incorporating information from both existing data as well as the few observations for the new feature and any associated feature metadata. At prediction time, the CHN requires only a single forward pass through a small neural network, yielding a significant speed-up when compared to re-training and fine-tuning approaches. In order to showcase the performance of CHNs, in this work we use a CHN to augment a partial variational autoencoder (P-VAE), a flexible deep generative model which can impute the values of missing features in sparsely-observed data. We show that this system obtains significantly improved performance for novel feature adaptation over existing imputation and meta-learning baselines across recommender systems, e-learning, and healthcare tasks.", "one-sentence_summary": "We introduce an auxiliary neural network to extend existing neural networks to make accurate predictions for new features in the few-shot learning regime, given a small number of observations and/or metadata for the new feature.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lamb|contextual_hypernetworks_for_novel_feature_adaptation", "supplementary_material": "", "pdf": "/pdf/dad11a3cea27e7d0d62ec1154262f381e27fe808.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=TFFjU4Nq8F", "_bibtex": "@misc{\nlamb2021contextual,\ntitle={Contextual HyperNetworks  for Novel Feature Adaptation},\nauthor={Angus Lamb and Evgeny Saveliev and Yingzhen Li and Sebastian Tschiatschek and Camilla Longden and Simon Woodhead and Jos{\\'e} Miguel Hern{\\'a}ndez-Lobato and Richard E Turner and Pashmina Cameron and Cheng Zhang},\nyear={2021},\nurl={https://openreview.net/forum?id=CYHMIhbuLFl}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "CYHMIhbuLFl", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1881/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1881/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1881/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1881/Authors|ICLR.cc/2021/Conference/Paper1881/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1881/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923854714, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1881/-/Official_Comment"}}}, {"id": "dQRCMHUGO3P", "original": null, "number": 6, "cdate": 1606231819782, "ddate": null, "tcdate": 1606231819782, "tmdate": 1606235666837, "tddate": null, "forum": "CYHMIhbuLFl", "replyto": "v5c8OMZj2PF", "invitation": "ICLR.cc/2021/Conference/Paper1881/-/Official_Comment", "content": {"title": "Response", "comment": "Thank you for your review, please find our responses below:\n\n\u2022\twhat does it mean to have a fixed point for each k on the x-axis ? \n\nThe purpose of Figure 5 is to demonstrate the performance of CHN and baseline methods on test inputs with varied number of observed features. Therefore in Figure 5, the context set size $k_n$ is fixed to a value $k$ for each point on the x axis. That is to say, we use a context set of $k_n=k$ observations as input to each method for each new feature. This plot clearly shows the benefits of CHN in low observation regime and the trend of performance by varying the number of observations. During training, $k_n$ is sampled uniformly from Uniform [0,32] to ensure that the CHN is able to generate good parameters for a range of values of k. \n\n\u2022\tHow is the new feature incorporated in the model for future use i.e., in a continual learning type of setup. \n\nIt would certainly be possible to use the CHN to predict e.g. the embedding weights associated with a new feature in the PVAE encoder, so that if this feature is included in future data points the model is able to encode it in a useful manner without requiring retraining of the model. However, we did not consider encoding additional features in our experiments.\n\n\u2022\tIt does seem that the focus is on an unsupervised setting (as P-VAE) was used. How will the CHN be used for classification or other types of supervised learning tasks.\n\nThe CHN is intended to be used for supervised tasks and relies on a supervised training loss \u2013 the P-VAE model is used as a flexible example model that can make predictions for missing features, where the identity of the missing features varies between each data point as in e.g. recommender system applications. In the original paper, the P-VAE is applied to supervised learning settings when the missing values of the input data are considered targets. For example, the medical dataset can be viewed as a binary classification task, where each of the unobserved pathophysiologies is considered a prediction target. This can be extended simply to multi-class classification through use of a categorical likelihood term for the corresponding target dimensions.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1881/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1881/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Contextual HyperNetworks  for Novel Feature Adaptation", "authorids": ["~Angus_Lamb1", "e.s.saveliev@gmail.com", "~Yingzhen_Li1", "~Sebastian_Tschiatschek1", "camilla.longden@microsoft.com", "simon.woodhead@eedi.co.uk", "~Jos\u00e9_Miguel_Hern\u00e1ndez-Lobato1", "~Richard_E_Turner1", "~Pashmina_Cameron1", "~Cheng_Zhang1"], "authors": ["Angus Lamb", "Evgeny Saveliev", "Yingzhen Li", "Sebastian Tschiatschek", "Camilla Longden", "Simon Woodhead", "Jos\u00e9 Miguel Hern\u00e1ndez-Lobato", "Richard E Turner", "Pashmina Cameron", "Cheng Zhang"], "keywords": ["Meta learning", "few-shot learning", "continual learning", "recommender systems", "deep learning"], "abstract": "While deep learning has obtained state-of-the-art results in many applications, the adaptation of neural network architectures to incorporate new features remains a research challenge. This issue is particularly severe in online learning settings, where new features are added continually with few or no associated observations. As such, methods for adapting neural networks to novel features which are both time and data-efficient are desired. To address this, we propose the Contextual HyperNetwork (CHN), which predicts the network weights associated with new features by incorporating information from both existing data as well as the few observations for the new feature and any associated feature metadata. At prediction time, the CHN requires only a single forward pass through a small neural network, yielding a significant speed-up when compared to re-training and fine-tuning approaches. In order to showcase the performance of CHNs, in this work we use a CHN to augment a partial variational autoencoder (P-VAE), a flexible deep generative model which can impute the values of missing features in sparsely-observed data. We show that this system obtains significantly improved performance for novel feature adaptation over existing imputation and meta-learning baselines across recommender systems, e-learning, and healthcare tasks.", "one-sentence_summary": "We introduce an auxiliary neural network to extend existing neural networks to make accurate predictions for new features in the few-shot learning regime, given a small number of observations and/or metadata for the new feature.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lamb|contextual_hypernetworks_for_novel_feature_adaptation", "supplementary_material": "", "pdf": "/pdf/dad11a3cea27e7d0d62ec1154262f381e27fe808.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=TFFjU4Nq8F", "_bibtex": "@misc{\nlamb2021contextual,\ntitle={Contextual HyperNetworks  for Novel Feature Adaptation},\nauthor={Angus Lamb and Evgeny Saveliev and Yingzhen Li and Sebastian Tschiatschek and Camilla Longden and Simon Woodhead and Jos{\\'e} Miguel Hern{\\'a}ndez-Lobato and Richard E Turner and Pashmina Cameron and Cheng Zhang},\nyear={2021},\nurl={https://openreview.net/forum?id=CYHMIhbuLFl}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "CYHMIhbuLFl", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1881/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1881/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1881/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1881/Authors|ICLR.cc/2021/Conference/Paper1881/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1881/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923854714, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1881/-/Official_Comment"}}}, {"id": "GeafY9TrQvX", "original": null, "number": 4, "cdate": 1606231281692, "ddate": null, "tcdate": 1606231281692, "tmdate": 1606231281692, "tddate": null, "forum": "CYHMIhbuLFl", "replyto": "eknH11FVgl3", "invitation": "ICLR.cc/2021/Conference/Paper1881/-/Official_Comment", "content": {"title": "Response", "comment": "Thank you for your review, please find our responses below:\n\n\u2022\t...illustrate how to apply CHNs to other pretty common techniques in recommender system field..\n\nThe current formulation is applicable to any deep learning-based model that can be extended to additional features, by taking some intermediate representation of the input features and using these to represent the context set associated with predicting a new feature.  Firstly, P-VAE represents generic auto encoder models in recommender systems. For example, any autoencoder-based method taking user histories as input, such as (Liang et al, 2018) or (Sedhain et al., 2015) permit a near-identical application of P-VAEs. Thus, applying our method is very straightforward. Secondly, other methods such as deep matrix factorization (Xue et.al 2017) can benefit from our methods more as both users and items are treated as fixed sized vector in the original setting. We can apply our method to both the user embedding learning part of the network as well as item embedding part of network in the same way as how we used in combination with P-VAEs. \n\n\u2022\t. the training latency...\nThank you for highlighting this: description of training times for CHNs was included in an appendix as part of the supplementary material for the original submission and may have been missed. To clarify this, we have moved this information to a new Timing Experiments subsection (4.5). \n\n\u2022\tLack  baselines and studies....\nWe aimed to provide a wide range of baselines that could be applied in this setting to solve the cold start problem, including item-based approaches such as nearest neighbours, content-based approaches such as averaging the parameters matching the new feature\u2019s metadata, and meta-learning approaches such as MAML. We aimed to augment existing meta-learning approaches to be able to explicitly utilize information about previous tasks (i.e. existing features in the model) when adapting to a new task, rather than treating each task independently, and felt that many existing meta-learning methods would not apply in this setting. If you have any suggestions for specific baselines that you feel should be given a comparison, we would be happy to consider them. \n\n\u2022\t...no meta information is incorporated in the baselines...\nWe believe our experimental setting is fair to baselines: the mean head parameters baseline used for MovieLens-1M and the E-learning dataset also utilises metadata. This is done by taking the mean of the trained head parameters for training set features only over features whose metadata categories are a subset of those of the new feature under consideration. We have included a comparison of performance with/without metadata for both CHNs and mean head parameters in Appendix C.5, Figure 10. \n\n\u2022\tLimited evaluation metics. Each task only either uses RMSE or AUC. \n\nFor the experiments on the Neuropathic Pain and E-learning datasets, we have additionally included the RMSE values in Appendix C.4, Figure 9 in order to provide another evaluation metric for these tasks, and we see that the CHN remains competitive when assessed by this metric. AUROC was chosen in place of accuracy for the main text in order to better accommodate imbalanced classes in the two classification tasks. The fact that CHN performs better/competitive in both RMSE and AUROC shows our approach is indeed better than baseline approaches across a range of datasets for different applications. \n\nReferences:\nLiang, Dawen, et al. \"Variational autoencoders for collaborative filtering.\" Proceedings of the 2018 World Wide Web Conference. 2018. \nSedhain, Suvash, et al. \"Autorec: Autoencoders meet collaborative filtering.\" Proceedings of the 24th international conference on World Wide Web. 2015.\nMLA, Xue, Hong-Jian, et al. \"Deep Matrix Factorization Models for Recommender Systems.\" IJCAI. Vol. 17. 2017.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1881/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1881/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Contextual HyperNetworks  for Novel Feature Adaptation", "authorids": ["~Angus_Lamb1", "e.s.saveliev@gmail.com", "~Yingzhen_Li1", "~Sebastian_Tschiatschek1", "camilla.longden@microsoft.com", "simon.woodhead@eedi.co.uk", "~Jos\u00e9_Miguel_Hern\u00e1ndez-Lobato1", "~Richard_E_Turner1", "~Pashmina_Cameron1", "~Cheng_Zhang1"], "authors": ["Angus Lamb", "Evgeny Saveliev", "Yingzhen Li", "Sebastian Tschiatschek", "Camilla Longden", "Simon Woodhead", "Jos\u00e9 Miguel Hern\u00e1ndez-Lobato", "Richard E Turner", "Pashmina Cameron", "Cheng Zhang"], "keywords": ["Meta learning", "few-shot learning", "continual learning", "recommender systems", "deep learning"], "abstract": "While deep learning has obtained state-of-the-art results in many applications, the adaptation of neural network architectures to incorporate new features remains a research challenge. This issue is particularly severe in online learning settings, where new features are added continually with few or no associated observations. As such, methods for adapting neural networks to novel features which are both time and data-efficient are desired. To address this, we propose the Contextual HyperNetwork (CHN), which predicts the network weights associated with new features by incorporating information from both existing data as well as the few observations for the new feature and any associated feature metadata. At prediction time, the CHN requires only a single forward pass through a small neural network, yielding a significant speed-up when compared to re-training and fine-tuning approaches. In order to showcase the performance of CHNs, in this work we use a CHN to augment a partial variational autoencoder (P-VAE), a flexible deep generative model which can impute the values of missing features in sparsely-observed data. We show that this system obtains significantly improved performance for novel feature adaptation over existing imputation and meta-learning baselines across recommender systems, e-learning, and healthcare tasks.", "one-sentence_summary": "We introduce an auxiliary neural network to extend existing neural networks to make accurate predictions for new features in the few-shot learning regime, given a small number of observations and/or metadata for the new feature.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lamb|contextual_hypernetworks_for_novel_feature_adaptation", "supplementary_material": "", "pdf": "/pdf/dad11a3cea27e7d0d62ec1154262f381e27fe808.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=TFFjU4Nq8F", "_bibtex": "@misc{\nlamb2021contextual,\ntitle={Contextual HyperNetworks  for Novel Feature Adaptation},\nauthor={Angus Lamb and Evgeny Saveliev and Yingzhen Li and Sebastian Tschiatschek and Camilla Longden and Simon Woodhead and Jos{\\'e} Miguel Hern{\\'a}ndez-Lobato and Richard E Turner and Pashmina Cameron and Cheng Zhang},\nyear={2021},\nurl={https://openreview.net/forum?id=CYHMIhbuLFl}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "CYHMIhbuLFl", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1881/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1881/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1881/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1881/Authors|ICLR.cc/2021/Conference/Paper1881/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1881/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923854714, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1881/-/Official_Comment"}}}, {"id": "xJC6ZQDVSm", "original": null, "number": 3, "cdate": 1606231078061, "ddate": null, "tcdate": 1606231078061, "tmdate": 1606231078061, "tddate": null, "forum": "CYHMIhbuLFl", "replyto": "sCpwKREHi_-", "invitation": "ICLR.cc/2021/Conference/Paper1881/-/Official_Comment", "content": {"title": "Response", "comment": "Thank for your review, please find our responses below:\n\n\u2022\t...models that take the graph topology..\n\nThank you for pointing out the GNN work which can address a similar issue. We have added a paragraph in the end of related work discussing these related works. In general, it is indeed true that GNN based methods naturally can encode new nodes. However, there is a large range of applications where GNN not be suitable, which can either be due to its high computational or memory requirement, or the problem setting may not be easily formulated as graphs. Our work addresses more generic deep learning models beyond GNN, for example vector based neural networks as we used in the experiments. \n\n\u2022\t... Vartak 2017. Vartak 2017 views the cold-start problem in recommender systems as a meta-learning ...\n\nThanks for highlighting the relationship between our work and (Vartak et al., 2017) , we have extended our discussion of this work in the revised submission. We believe that our work is substantially different from (Vartak et al., 2017). Whereas this method learns either the weights of a linear classifier, or solely the biases of a non-linear classifier, whereas we learn both. Their method can only be applied to classification problems as it relies on a class representative embedding R^c_j, whereas our method is applicable to both classification and regression problems. The architectures are very different in general, with the \u201cmain model\u201d in (Vartak et al., 2017) taking an item history embedding and then passing it through a network with weights generated by the user history, whereas we input the user history and pass it through a network whose final layer has weights generated by the item history. We view our main advantage over this method as our ability to augment a much larger primary model with a small number of new adapted parameters, as opposed to generating the whole network as they do, allowing us to keep the computational costs of adaptation small without sacrificing model capacity by using a smaller model or a factorization of the weights.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1881/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1881/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Contextual HyperNetworks  for Novel Feature Adaptation", "authorids": ["~Angus_Lamb1", "e.s.saveliev@gmail.com", "~Yingzhen_Li1", "~Sebastian_Tschiatschek1", "camilla.longden@microsoft.com", "simon.woodhead@eedi.co.uk", "~Jos\u00e9_Miguel_Hern\u00e1ndez-Lobato1", "~Richard_E_Turner1", "~Pashmina_Cameron1", "~Cheng_Zhang1"], "authors": ["Angus Lamb", "Evgeny Saveliev", "Yingzhen Li", "Sebastian Tschiatschek", "Camilla Longden", "Simon Woodhead", "Jos\u00e9 Miguel Hern\u00e1ndez-Lobato", "Richard E Turner", "Pashmina Cameron", "Cheng Zhang"], "keywords": ["Meta learning", "few-shot learning", "continual learning", "recommender systems", "deep learning"], "abstract": "While deep learning has obtained state-of-the-art results in many applications, the adaptation of neural network architectures to incorporate new features remains a research challenge. This issue is particularly severe in online learning settings, where new features are added continually with few or no associated observations. As such, methods for adapting neural networks to novel features which are both time and data-efficient are desired. To address this, we propose the Contextual HyperNetwork (CHN), which predicts the network weights associated with new features by incorporating information from both existing data as well as the few observations for the new feature and any associated feature metadata. At prediction time, the CHN requires only a single forward pass through a small neural network, yielding a significant speed-up when compared to re-training and fine-tuning approaches. In order to showcase the performance of CHNs, in this work we use a CHN to augment a partial variational autoencoder (P-VAE), a flexible deep generative model which can impute the values of missing features in sparsely-observed data. We show that this system obtains significantly improved performance for novel feature adaptation over existing imputation and meta-learning baselines across recommender systems, e-learning, and healthcare tasks.", "one-sentence_summary": "We introduce an auxiliary neural network to extend existing neural networks to make accurate predictions for new features in the few-shot learning regime, given a small number of observations and/or metadata for the new feature.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lamb|contextual_hypernetworks_for_novel_feature_adaptation", "supplementary_material": "", "pdf": "/pdf/dad11a3cea27e7d0d62ec1154262f381e27fe808.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=TFFjU4Nq8F", "_bibtex": "@misc{\nlamb2021contextual,\ntitle={Contextual HyperNetworks  for Novel Feature Adaptation},\nauthor={Angus Lamb and Evgeny Saveliev and Yingzhen Li and Sebastian Tschiatschek and Camilla Longden and Simon Woodhead and Jos{\\'e} Miguel Hern{\\'a}ndez-Lobato and Richard E Turner and Pashmina Cameron and Cheng Zhang},\nyear={2021},\nurl={https://openreview.net/forum?id=CYHMIhbuLFl}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "CYHMIhbuLFl", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1881/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1881/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1881/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1881/Authors|ICLR.cc/2021/Conference/Paper1881/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1881/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923854714, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1881/-/Official_Comment"}}}, {"id": "Sf83LqNlCYo", "original": null, "number": 2, "cdate": 1606230915486, "ddate": null, "tcdate": 1606230915486, "tmdate": 1606230915486, "tddate": null, "forum": "CYHMIhbuLFl", "replyto": "CYHMIhbuLFl", "invitation": "ICLR.cc/2021/Conference/Paper1881/-/Official_Comment", "content": {"title": "Revised version", "comment": "We thank the reviewers for their reviews and helpful comments. We have uploaded a revised version of the submission. Major updates to the main text are highlighted in red. The major edits include:\n1.\tInclusion of appendices in main submission rather than as separate supplementary material\n2.\tExpanded related work to include discussions on suggested papers.\n3.\tMoved the CHN training time information (originally in the appendix) to a new \u201cTiming Experiments\u201d subsection (4.5) in the main text.\n4.\tUpdated appendix to include new results:\na.\tAn ablation on fine-tuning the new heads on the context set of meta test inputs, with the new heads initialised either randomly or using CHN (Appendix C.3). The results show that CHN initialisation is significantly better than training from a random initialisation for a range of context set sizes.\nb.\tThe RMSE performance of CHN & baseline methods on the Neuropathic Pain and E-learning datasets (Appendix C.4). Results show that both RMSE and AUC metrics consistently rate CHN performance as highly competitive.\nc.\tAn ablation study on the usage of meta-data in MovieLens-1M experiments (Appendix C.5).\n\nWe would also like to briefly clarify the main contribution of the paper. Our goal is to enable fast adaptation of a machine learning model when new output features are added to augment the originally observed data. This is different from typical meta-learning/few-shot learning setting where the new task is typically treated independently from previous tasks. Instead, the proposed CHN explicitly utilizes previous information for each datapoint to predict the newly added features in an amortised manner. We envision that CHN can be applied to generic machine learning models, and in the paper, we used the P-VAE as an exemplar model to carry out our empirical evaluations. Our experiments aim to cover a wide range of applications (recommender systems, E-learning, medical diagnosis) that this new feature adaptation task is relevant, and our results showed that CHN performs better than/competitively with all considered baselines on these applications.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1881/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1881/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Contextual HyperNetworks  for Novel Feature Adaptation", "authorids": ["~Angus_Lamb1", "e.s.saveliev@gmail.com", "~Yingzhen_Li1", "~Sebastian_Tschiatschek1", "camilla.longden@microsoft.com", "simon.woodhead@eedi.co.uk", "~Jos\u00e9_Miguel_Hern\u00e1ndez-Lobato1", "~Richard_E_Turner1", "~Pashmina_Cameron1", "~Cheng_Zhang1"], "authors": ["Angus Lamb", "Evgeny Saveliev", "Yingzhen Li", "Sebastian Tschiatschek", "Camilla Longden", "Simon Woodhead", "Jos\u00e9 Miguel Hern\u00e1ndez-Lobato", "Richard E Turner", "Pashmina Cameron", "Cheng Zhang"], "keywords": ["Meta learning", "few-shot learning", "continual learning", "recommender systems", "deep learning"], "abstract": "While deep learning has obtained state-of-the-art results in many applications, the adaptation of neural network architectures to incorporate new features remains a research challenge. This issue is particularly severe in online learning settings, where new features are added continually with few or no associated observations. As such, methods for adapting neural networks to novel features which are both time and data-efficient are desired. To address this, we propose the Contextual HyperNetwork (CHN), which predicts the network weights associated with new features by incorporating information from both existing data as well as the few observations for the new feature and any associated feature metadata. At prediction time, the CHN requires only a single forward pass through a small neural network, yielding a significant speed-up when compared to re-training and fine-tuning approaches. In order to showcase the performance of CHNs, in this work we use a CHN to augment a partial variational autoencoder (P-VAE), a flexible deep generative model which can impute the values of missing features in sparsely-observed data. We show that this system obtains significantly improved performance for novel feature adaptation over existing imputation and meta-learning baselines across recommender systems, e-learning, and healthcare tasks.", "one-sentence_summary": "We introduce an auxiliary neural network to extend existing neural networks to make accurate predictions for new features in the few-shot learning regime, given a small number of observations and/or metadata for the new feature.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lamb|contextual_hypernetworks_for_novel_feature_adaptation", "supplementary_material": "", "pdf": "/pdf/dad11a3cea27e7d0d62ec1154262f381e27fe808.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=TFFjU4Nq8F", "_bibtex": "@misc{\nlamb2021contextual,\ntitle={Contextual HyperNetworks  for Novel Feature Adaptation},\nauthor={Angus Lamb and Evgeny Saveliev and Yingzhen Li and Sebastian Tschiatschek and Camilla Longden and Simon Woodhead and Jos{\\'e} Miguel Hern{\\'a}ndez-Lobato and Richard E Turner and Pashmina Cameron and Cheng Zhang},\nyear={2021},\nurl={https://openreview.net/forum?id=CYHMIhbuLFl}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "CYHMIhbuLFl", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1881/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1881/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1881/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1881/Authors|ICLR.cc/2021/Conference/Paper1881/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1881/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923854714, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1881/-/Official_Comment"}}}, {"id": "v5c8OMZj2PF", "original": null, "number": 1, "cdate": 1603496245952, "ddate": null, "tcdate": 1603496245952, "tmdate": 1605024336903, "tddate": null, "forum": "CYHMIhbuLFl", "replyto": "CYHMIhbuLFl", "invitation": "ICLR.cc/2021/Conference/Paper1881/-/Official_Review", "content": {"title": "Interesting work", "review": "This work proposes CHN, a framework to extend an existing model to incorporate new features as they become available. The benefits of CHN are demonstrated by utilizing it to extend a P-VAE.\n\nThis work proposes a structured procedure to incorporate new features in an incoming data for use with a trained model. The proposed CHN uses a combination of a context vector and a meta vector (generated through a neural network from the new features's meta data) to produce parameters $\\hat{\\theta}_n$ (for the new feature $x_n$). This $\\hat{\\theta}_n$ can then be used with the base model's representations (encoded vector for the P-VAE setup) for downstream tasks.\n\nThe work does seem to have merit as existence of missing raw features / new information about data in the new samples is not uncommon for practical scenarios. Therefore, an approach that does not require extensive resource usage to produce a new model that can utilize the new information is very useful. \n\nThis work empirically demonstrates the benefit of CHN over several tasks along with demonstrating its efficiency. There are, however, some points that need to be addressed.\n\n\nIn Figure 5, the x-axis is the context size which I am assuming is $k$. However, in section 2.3 it is mentioned that $k_n$ is sampled from Uniform[0,32]. In such a setup, what does it mean to have a fixed point for each k on the x-axis ? Does it mean that $k_n$ in this case was not sampled from a uniform distribution ? Perhaps Figure 5 was misunderstood by me and a clarification would be useful.\n\n \nHow is the new feature incorporated in the model for future use i.e., in a continual learning type of setup. The new information might be useful for future feature revelations. Therefore, incorporating it in the base model or the model $\\psi$ will be useful as otherwise any new information received after the training of the base model (say the base P-VAE) will be lost to future instance of new feature introductions. This seems to be a very practical requirement. Is there a way to achieve this without having a big mitigating impact on speed and efficiency?\n\nIt does seem that the focus is on an unsupervised setting (as P-VAE) was used. How will the CHN be used for classification or other types of supervised learning tasks. \n\n", "rating": "6: Marginally above acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2021/Conference/Paper1881/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1881/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Contextual HyperNetworks  for Novel Feature Adaptation", "authorids": ["~Angus_Lamb1", "e.s.saveliev@gmail.com", "~Yingzhen_Li1", "~Sebastian_Tschiatschek1", "camilla.longden@microsoft.com", "simon.woodhead@eedi.co.uk", "~Jos\u00e9_Miguel_Hern\u00e1ndez-Lobato1", "~Richard_E_Turner1", "~Pashmina_Cameron1", "~Cheng_Zhang1"], "authors": ["Angus Lamb", "Evgeny Saveliev", "Yingzhen Li", "Sebastian Tschiatschek", "Camilla Longden", "Simon Woodhead", "Jos\u00e9 Miguel Hern\u00e1ndez-Lobato", "Richard E Turner", "Pashmina Cameron", "Cheng Zhang"], "keywords": ["Meta learning", "few-shot learning", "continual learning", "recommender systems", "deep learning"], "abstract": "While deep learning has obtained state-of-the-art results in many applications, the adaptation of neural network architectures to incorporate new features remains a research challenge. This issue is particularly severe in online learning settings, where new features are added continually with few or no associated observations. As such, methods for adapting neural networks to novel features which are both time and data-efficient are desired. To address this, we propose the Contextual HyperNetwork (CHN), which predicts the network weights associated with new features by incorporating information from both existing data as well as the few observations for the new feature and any associated feature metadata. At prediction time, the CHN requires only a single forward pass through a small neural network, yielding a significant speed-up when compared to re-training and fine-tuning approaches. In order to showcase the performance of CHNs, in this work we use a CHN to augment a partial variational autoencoder (P-VAE), a flexible deep generative model which can impute the values of missing features in sparsely-observed data. We show that this system obtains significantly improved performance for novel feature adaptation over existing imputation and meta-learning baselines across recommender systems, e-learning, and healthcare tasks.", "one-sentence_summary": "We introduce an auxiliary neural network to extend existing neural networks to make accurate predictions for new features in the few-shot learning regime, given a small number of observations and/or metadata for the new feature.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lamb|contextual_hypernetworks_for_novel_feature_adaptation", "supplementary_material": "", "pdf": "/pdf/dad11a3cea27e7d0d62ec1154262f381e27fe808.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=TFFjU4Nq8F", "_bibtex": "@misc{\nlamb2021contextual,\ntitle={Contextual HyperNetworks  for Novel Feature Adaptation},\nauthor={Angus Lamb and Evgeny Saveliev and Yingzhen Li and Sebastian Tschiatschek and Camilla Longden and Simon Woodhead and Jos{\\'e} Miguel Hern{\\'a}ndez-Lobato and Richard E Turner and Pashmina Cameron and Cheng Zhang},\nyear={2021},\nurl={https://openreview.net/forum?id=CYHMIhbuLFl}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "CYHMIhbuLFl", "replyto": "CYHMIhbuLFl", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1881/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538108693, "tmdate": 1606915772624, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1881/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1881/-/Official_Review"}}}, {"id": "eknH11FVgl3", "original": null, "number": 3, "cdate": 1603899992670, "ddate": null, "tcdate": 1603899992670, "tmdate": 1605024336780, "tddate": null, "forum": "CYHMIhbuLFl", "replyto": "CYHMIhbuLFl", "invitation": "ICLR.cc/2021/Conference/Paper1881/-/Official_Review", "content": {"title": "Experimental setup leaves something to be desired", "review": "The paper proposes Contextual HyperNetworks (CHNs) as an auxiliary model to generate parameters from existing data, and observations and other metadata associated with new feature to address cold start problem of new feature. Besides, it doesn\u2019t need either re-train or fine-tune at prediction time. The CHN is applied to P-VAE and some experimental results are provided to demonstrate its effectiveness in some application, i.e., recommender system, e-learning and healthcare tasks.\n\nThe motivation is clear and reasonable, and the proposed method with hyper-network is pretty interesting and attractive. However, experimental section has large improvement room.\n\n Pros:\n\n* The paper proposes an interesting direction to use hyper-network to solve cold-start problems in some critical tasks, such as recommender systems and etc.\n* The logic of paper is clear and easy to follow.\n\nCons:\n\n* The paper only illustrates how to apply CHNs to P-VAE, however, it\u2019s better also to illustrate how to apply CHNs to other pretty common techniques in recommender system field (or e-learning or others), such as deep learning-based collaborative filtering methods. If there is no more one application, it is difficult to demonstrate the proposed method is generalizable enough to other models, including how easy it could be extended and how effectiveness it could have after extension.\n* The paper only shows the advantage of prediction time, but it doesn\u2019t discuss a lot on the training latency. It\u2019s better to also discuss this in the paper and compare with other methods.\n* The experiment setup is not strong enough to demonstrate the effectiveness of the proposed methods, for example,\n    * Lack of major important baselines and studies. In the paper, only several extension on how to handle new features on top of P-VAE is given in the comparison. However, it is unclear how it performs with other methods which target at solving cold-start problem. In other others, current experiments cannot provide evidence to show it outperforms other methods which could be used to address cold start problem.\n    * Fairness regarding to no meta information is incorporated in the baselines. CHNs is leverage additional meta information, however, other baselines don\u2019t use this kind of information. \n    * Limited evaluation metics. Each task only either uses RMSE or AUC. So it lacks evidence to show whether the proposed method could outperform others in different validation metrics instead of bias.\n\n", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1881/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1881/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Contextual HyperNetworks  for Novel Feature Adaptation", "authorids": ["~Angus_Lamb1", "e.s.saveliev@gmail.com", "~Yingzhen_Li1", "~Sebastian_Tschiatschek1", "camilla.longden@microsoft.com", "simon.woodhead@eedi.co.uk", "~Jos\u00e9_Miguel_Hern\u00e1ndez-Lobato1", "~Richard_E_Turner1", "~Pashmina_Cameron1", "~Cheng_Zhang1"], "authors": ["Angus Lamb", "Evgeny Saveliev", "Yingzhen Li", "Sebastian Tschiatschek", "Camilla Longden", "Simon Woodhead", "Jos\u00e9 Miguel Hern\u00e1ndez-Lobato", "Richard E Turner", "Pashmina Cameron", "Cheng Zhang"], "keywords": ["Meta learning", "few-shot learning", "continual learning", "recommender systems", "deep learning"], "abstract": "While deep learning has obtained state-of-the-art results in many applications, the adaptation of neural network architectures to incorporate new features remains a research challenge. This issue is particularly severe in online learning settings, where new features are added continually with few or no associated observations. As such, methods for adapting neural networks to novel features which are both time and data-efficient are desired. To address this, we propose the Contextual HyperNetwork (CHN), which predicts the network weights associated with new features by incorporating information from both existing data as well as the few observations for the new feature and any associated feature metadata. At prediction time, the CHN requires only a single forward pass through a small neural network, yielding a significant speed-up when compared to re-training and fine-tuning approaches. In order to showcase the performance of CHNs, in this work we use a CHN to augment a partial variational autoencoder (P-VAE), a flexible deep generative model which can impute the values of missing features in sparsely-observed data. We show that this system obtains significantly improved performance for novel feature adaptation over existing imputation and meta-learning baselines across recommender systems, e-learning, and healthcare tasks.", "one-sentence_summary": "We introduce an auxiliary neural network to extend existing neural networks to make accurate predictions for new features in the few-shot learning regime, given a small number of observations and/or metadata for the new feature.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lamb|contextual_hypernetworks_for_novel_feature_adaptation", "supplementary_material": "", "pdf": "/pdf/dad11a3cea27e7d0d62ec1154262f381e27fe808.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=TFFjU4Nq8F", "_bibtex": "@misc{\nlamb2021contextual,\ntitle={Contextual HyperNetworks  for Novel Feature Adaptation},\nauthor={Angus Lamb and Evgeny Saveliev and Yingzhen Li and Sebastian Tschiatschek and Camilla Longden and Simon Woodhead and Jos{\\'e} Miguel Hern{\\'a}ndez-Lobato and Richard E Turner and Pashmina Cameron and Cheng Zhang},\nyear={2021},\nurl={https://openreview.net/forum?id=CYHMIhbuLFl}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "CYHMIhbuLFl", "replyto": "CYHMIhbuLFl", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1881/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538108693, "tmdate": 1606915772624, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1881/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1881/-/Official_Review"}}}, {"id": "sCpwKREHi_-", "original": null, "number": 4, "cdate": 1603906661701, "ddate": null, "tcdate": 1603906661701, "tmdate": 1605024336721, "tddate": null, "forum": "CYHMIhbuLFl", "replyto": "CYHMIhbuLFl", "invitation": "ICLR.cc/2021/Conference/Paper1881/-/Official_Review", "content": {"title": "An okay submission but with limited novelty.", "review": "This submission focuses on the cold start problem of new entities (new items in a recommender system, new treatments in a medical application, etc.). It combines the strengths of the *relations* between a new entity and the existing entities, and the *content* features of the new entity, by fusing the two kinds of information into a neural network that outputs the estimated representation of the new entity. The proposed method outperforms several intuitive na\u00efve strategies as well as MAML.\n\nPros:\n- The writing is clear.\n- Good reproducibility. Details, including hyperparameters, are listed in the appendix.\n\nCons:\n1. Similar models -- models that take the graph topology around a new node as well as the node\u2019s feature as input and produces the node\u2019s embedding without any extra training/finetuning step -- have long existed in the literature. For example, one can google \u201cinductive learning + graph embedding\u201d, \u201cout-of-sample extension + graph embedding\u201d, etc., and find plenty of related works. Not to mention that the nowadays (over-)popular graph neural networks naturally support this.\n2. It is unclear how this work improves upon Vartak 2017. Vartak 2017 views the cold-start problem in recommender systems as a meta-learning problem as well. It also combines the user rating data and the item\u2019s content features. What\u2019s new when compared to Vartak 2017, and how well the proposed method outperforms Vartak 2017 empirically?", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1881/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1881/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Contextual HyperNetworks  for Novel Feature Adaptation", "authorids": ["~Angus_Lamb1", "e.s.saveliev@gmail.com", "~Yingzhen_Li1", "~Sebastian_Tschiatschek1", "camilla.longden@microsoft.com", "simon.woodhead@eedi.co.uk", "~Jos\u00e9_Miguel_Hern\u00e1ndez-Lobato1", "~Richard_E_Turner1", "~Pashmina_Cameron1", "~Cheng_Zhang1"], "authors": ["Angus Lamb", "Evgeny Saveliev", "Yingzhen Li", "Sebastian Tschiatschek", "Camilla Longden", "Simon Woodhead", "Jos\u00e9 Miguel Hern\u00e1ndez-Lobato", "Richard E Turner", "Pashmina Cameron", "Cheng Zhang"], "keywords": ["Meta learning", "few-shot learning", "continual learning", "recommender systems", "deep learning"], "abstract": "While deep learning has obtained state-of-the-art results in many applications, the adaptation of neural network architectures to incorporate new features remains a research challenge. This issue is particularly severe in online learning settings, where new features are added continually with few or no associated observations. As such, methods for adapting neural networks to novel features which are both time and data-efficient are desired. To address this, we propose the Contextual HyperNetwork (CHN), which predicts the network weights associated with new features by incorporating information from both existing data as well as the few observations for the new feature and any associated feature metadata. At prediction time, the CHN requires only a single forward pass through a small neural network, yielding a significant speed-up when compared to re-training and fine-tuning approaches. In order to showcase the performance of CHNs, in this work we use a CHN to augment a partial variational autoencoder (P-VAE), a flexible deep generative model which can impute the values of missing features in sparsely-observed data. We show that this system obtains significantly improved performance for novel feature adaptation over existing imputation and meta-learning baselines across recommender systems, e-learning, and healthcare tasks.", "one-sentence_summary": "We introduce an auxiliary neural network to extend existing neural networks to make accurate predictions for new features in the few-shot learning regime, given a small number of observations and/or metadata for the new feature.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lamb|contextual_hypernetworks_for_novel_feature_adaptation", "supplementary_material": "", "pdf": "/pdf/dad11a3cea27e7d0d62ec1154262f381e27fe808.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=TFFjU4Nq8F", "_bibtex": "@misc{\nlamb2021contextual,\ntitle={Contextual HyperNetworks  for Novel Feature Adaptation},\nauthor={Angus Lamb and Evgeny Saveliev and Yingzhen Li and Sebastian Tschiatschek and Camilla Longden and Simon Woodhead and Jos{\\'e} Miguel Hern{\\'a}ndez-Lobato and Richard E Turner and Pashmina Cameron and Cheng Zhang},\nyear={2021},\nurl={https://openreview.net/forum?id=CYHMIhbuLFl}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "CYHMIhbuLFl", "replyto": "CYHMIhbuLFl", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1881/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538108693, "tmdate": 1606915772624, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1881/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1881/-/Official_Review"}}}], "count": 12}