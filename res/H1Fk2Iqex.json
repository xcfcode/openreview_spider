{"notes": [{"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396505964, "tcdate": 1486396505964, "number": 1, "id": "H1GvnGIul", "invitation": "ICLR.cc/2017/conference/-/paper323/acceptance", "forum": "H1Fk2Iqex", "replyto": "H1Fk2Iqex", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"title": "ICLR committee final decision", "comment": "This paper studies efficient signal representations to perform bioacoustic classification based on CNNs. Contrary to image classification, where most useful information can be extracted with spatially localized kernels, bioacoustic signatures are more localized in the frequency domain, requiring to rethink the design of convolutional architectures. The authors propose to enforce the lower layers of the architecture with chirplet transforms, which are localized in the time-frequency plane as wavelets, but with time-varying central frequency. They present preliminary numerical experiments showing promising improvements over existing baselines. \n \n The reviewers found interest in the method, but raised concerns on the relatively narrow scope of the method, as well as the clarity and rigor of the presentation. Whereas the first concern is up to debate, I agree that the paper currently suffers from poor english which affects its clarity. \n \n Despite these concerns, the AC finds the contribution useful in the broader context of inductive bias and injecting priors in neural networks. This is an example where the inductive priors that work well on images (localized convolutions rather than generic fully connected layers) are not sufficient unless given massive amounts of data. The AC thus recommends rejection, but invites the contribution to the workshop track.", "decision": "Invite to Workshop Track"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fast Chirplet Transform to Enhance CNN Machine Listening - Validation on Animal calls and Speech", "abstract": "The scattering framework offers an optimal hierarchical convolutional decomposition according to its kernels.  Convolutional Neural Net (CNN) can be seen asan optimal kernel decomposition, nevertheless it requires large amount of trainingdata to learn its kernels.  We propose a trade-off between these two approaches: a Chirplet kernel as an efficient Q constant bioacoustic representation to pretrainCNN. First we motivate Chirplet bioinspired auditory representation. Second we give the first algorithm (and code) of a Fast Chirplet Transform (FCT). Third, we demonstrate the computation efficiency of FCT on large environmental data base: months of Orca recordings, and 1000 Birds species from the LifeClef challenge. Fourth, we validate FCT on the vowels subset of the Speech TIMIT dataset. The results show that FCT accelerates CNN when it pretrains low level layers: it reduces training duration by -28% for birds classification, and by -26% for vowels classification. Scores are also enhanced by FCT pretraining, with a relative gain of +7.8% of Mean Average Precision on birds,  and +2.3% of vowel accuracy against raw audio CNN. We conclude on perspectives on tonotopic FCT deep machine listening, and inter-species bioacoustic transfer learning to generalise the representation of animal communication systems.", "pdf": "/pdf/19cd6593b3706f4ae2f90b8d46e0f7338f30bc6e.pdf", "TL;DR": "Proposing a chirplet transform in order to regulate the input of deep-CNN and possible extension to chirplet learning for deep learning bioacoustics", "paperhash": "glotin|fast_chirplet_transform_to_enhance_cnn_machine_listening_validation_on_animal_calls_and_speech", "keywords": ["Applications", "Supervised Learning", "Deep learning", "Speech"], "conflicts": ["rice.edu", "univ-tln.fr"], "authors": ["Herve Glotin", "Julien Ricard", "Randall Balestriero"], "authorids": ["glotin@univ-tln.fr", "julien.ricard@gmail.com", "randallbalestriero@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396507866, "id": "ICLR.cc/2017/conference/-/paper323/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "H1Fk2Iqex", "replyto": "H1Fk2Iqex", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396507866}}}, {"tddate": null, "tmdate": 1484948818226, "tcdate": 1484948818226, "number": 9, "id": "BJqISbgDx", "invitation": "ICLR.cc/2017/conference/-/paper323/public/comment", "forum": "H1Fk2Iqex", "replyto": "SkC18djIx", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "compact support / Algo in Appendix / TIMIT exp. details", "comment": "Thanks for your comments : \n\na) \n' \"A wavelet is an atom with compact support in time and frequency domain which integrates to 0\".\nFor example, is a Morlet wavelet with compact support in time and frequency?\n'\n\nIndeed the analytical support of the wavelets (even continuous wavelets) is not compact since by definition of the exponential exp(w)>0 but they are very well localized. It can be considered compact in the applied case where roundoff error lead to a value of 0 quickly after moving around the center frequency. We precised it in the revised paper V3.\n\nb) \n\"And what are the benefits of the \"Algo 1\" and \"Algo 2\"? Maybe you could put them into an appendix.\"\n\nRight, we moved Algo 1 & 2 in appendix.\n\nc) \nWe precised TIMIT data set definitions (it was only in appendix before). Note that we run at T= 310 ms as recommended in Palaz & Collobert 2013. Note that we experiment on vowel only, due to lack of time, so we are not able to compare to global Phone error rate. However we focus in this paper on the gain using FCT pretraining.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fast Chirplet Transform to Enhance CNN Machine Listening - Validation on Animal calls and Speech", "abstract": "The scattering framework offers an optimal hierarchical convolutional decomposition according to its kernels.  Convolutional Neural Net (CNN) can be seen asan optimal kernel decomposition, nevertheless it requires large amount of trainingdata to learn its kernels.  We propose a trade-off between these two approaches: a Chirplet kernel as an efficient Q constant bioacoustic representation to pretrainCNN. First we motivate Chirplet bioinspired auditory representation. Second we give the first algorithm (and code) of a Fast Chirplet Transform (FCT). Third, we demonstrate the computation efficiency of FCT on large environmental data base: months of Orca recordings, and 1000 Birds species from the LifeClef challenge. Fourth, we validate FCT on the vowels subset of the Speech TIMIT dataset. The results show that FCT accelerates CNN when it pretrains low level layers: it reduces training duration by -28% for birds classification, and by -26% for vowels classification. Scores are also enhanced by FCT pretraining, with a relative gain of +7.8% of Mean Average Precision on birds,  and +2.3% of vowel accuracy against raw audio CNN. We conclude on perspectives on tonotopic FCT deep machine listening, and inter-species bioacoustic transfer learning to generalise the representation of animal communication systems.", "pdf": "/pdf/19cd6593b3706f4ae2f90b8d46e0f7338f30bc6e.pdf", "TL;DR": "Proposing a chirplet transform in order to regulate the input of deep-CNN and possible extension to chirplet learning for deep learning bioacoustics", "paperhash": "glotin|fast_chirplet_transform_to_enhance_cnn_machine_listening_validation_on_animal_calls_and_speech", "keywords": ["Applications", "Supervised Learning", "Deep learning", "Speech"], "conflicts": ["rice.edu", "univ-tln.fr"], "authors": ["Herve Glotin", "Julien Ricard", "Randall Balestriero"], "authorids": ["glotin@univ-tln.fr", "julien.ricard@gmail.com", "randallbalestriero@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287621462, "id": "ICLR.cc/2017/conference/-/paper323/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "H1Fk2Iqex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper323/reviewers", "ICLR.cc/2017/conference/paper323/areachairs"], "cdate": 1485287621462}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1484948544817, "tcdate": 1478286305268, "number": 323, "id": "H1Fk2Iqex", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "H1Fk2Iqex", "signatures": ["~Randall_Balestriero1"], "readers": ["everyone"], "content": {"title": "Fast Chirplet Transform to Enhance CNN Machine Listening - Validation on Animal calls and Speech", "abstract": "The scattering framework offers an optimal hierarchical convolutional decomposition according to its kernels.  Convolutional Neural Net (CNN) can be seen asan optimal kernel decomposition, nevertheless it requires large amount of trainingdata to learn its kernels.  We propose a trade-off between these two approaches: a Chirplet kernel as an efficient Q constant bioacoustic representation to pretrainCNN. First we motivate Chirplet bioinspired auditory representation. Second we give the first algorithm (and code) of a Fast Chirplet Transform (FCT). Third, we demonstrate the computation efficiency of FCT on large environmental data base: months of Orca recordings, and 1000 Birds species from the LifeClef challenge. Fourth, we validate FCT on the vowels subset of the Speech TIMIT dataset. The results show that FCT accelerates CNN when it pretrains low level layers: it reduces training duration by -28% for birds classification, and by -26% for vowels classification. Scores are also enhanced by FCT pretraining, with a relative gain of +7.8% of Mean Average Precision on birds,  and +2.3% of vowel accuracy against raw audio CNN. We conclude on perspectives on tonotopic FCT deep machine listening, and inter-species bioacoustic transfer learning to generalise the representation of animal communication systems.", "pdf": "/pdf/19cd6593b3706f4ae2f90b8d46e0f7338f30bc6e.pdf", "TL;DR": "Proposing a chirplet transform in order to regulate the input of deep-CNN and possible extension to chirplet learning for deep learning bioacoustics", "paperhash": "glotin|fast_chirplet_transform_to_enhance_cnn_machine_listening_validation_on_animal_calls_and_speech", "keywords": ["Applications", "Supervised Learning", "Deep learning", "Speech"], "conflicts": ["rice.edu", "univ-tln.fr"], "authors": ["Herve Glotin", "Julien Ricard", "Randall Balestriero"], "authorids": ["glotin@univ-tln.fr", "julien.ricard@gmail.com", "randallbalestriero@gmail.com"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 16, "writable": false, "overwriting": ["HJWjTANFx"], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "tmdate": 1484649957996, "tcdate": 1484649957996, "number": 3, "id": "SkC18djIx", "invitation": "ICLR.cc/2017/conference/-/paper323/official/comment", "forum": "H1Fk2Iqex", "replyto": "BkXSUSB8g", "signatures": ["ICLR.cc/2017/conference/paper323/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper323/AnonReviewer1"], "content": {"title": "Not everything is adressed", "comment": "I still see claims such that:\n\"A wavelet is an atom with compact support in time and frequency domain which integrates to 0\".\nFor example, is a Morlet wavelet with compact support in time and frequency?\n\nAnd what are the benefits of the \"Algo 1\" and \"Algo 2\"? Maybe you could put them into an appendix.\n\nIn the conclusion \" First we\u2019ll work\".\n\nWrite explicitly the setting for the TIMIT dataset (like T=300ms) and potentially add some references, since I thought you were using T=32ms where the accuracy is quite higher(like ~85% as far as I remember). Please add as well more numerical results from other papers in Table 2."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fast Chirplet Transform to Enhance CNN Machine Listening - Validation on Animal calls and Speech", "abstract": "The scattering framework offers an optimal hierarchical convolutional decomposition according to its kernels.  Convolutional Neural Net (CNN) can be seen asan optimal kernel decomposition, nevertheless it requires large amount of trainingdata to learn its kernels.  We propose a trade-off between these two approaches: a Chirplet kernel as an efficient Q constant bioacoustic representation to pretrainCNN. First we motivate Chirplet bioinspired auditory representation. Second we give the first algorithm (and code) of a Fast Chirplet Transform (FCT). Third, we demonstrate the computation efficiency of FCT on large environmental data base: months of Orca recordings, and 1000 Birds species from the LifeClef challenge. Fourth, we validate FCT on the vowels subset of the Speech TIMIT dataset. The results show that FCT accelerates CNN when it pretrains low level layers: it reduces training duration by -28% for birds classification, and by -26% for vowels classification. Scores are also enhanced by FCT pretraining, with a relative gain of +7.8% of Mean Average Precision on birds,  and +2.3% of vowel accuracy against raw audio CNN. We conclude on perspectives on tonotopic FCT deep machine listening, and inter-species bioacoustic transfer learning to generalise the representation of animal communication systems.", "pdf": "/pdf/19cd6593b3706f4ae2f90b8d46e0f7338f30bc6e.pdf", "TL;DR": "Proposing a chirplet transform in order to regulate the input of deep-CNN and possible extension to chirplet learning for deep learning bioacoustics", "paperhash": "glotin|fast_chirplet_transform_to_enhance_cnn_machine_listening_validation_on_animal_calls_and_speech", "keywords": ["Applications", "Supervised Learning", "Deep learning", "Speech"], "conflicts": ["rice.edu", "univ-tln.fr"], "authors": ["Herve Glotin", "Julien Ricard", "Randall Balestriero"], "authorids": ["glotin@univ-tln.fr", "julien.ricard@gmail.com", "randallbalestriero@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287621312, "id": "ICLR.cc/2017/conference/-/paper323/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "H1Fk2Iqex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper323/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper323/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper323/reviewers", "ICLR.cc/2017/conference/paper323/areachairs"], "cdate": 1485287621312}}}, {"tddate": null, "tmdate": 1484499501613, "tcdate": 1484499501613, "number": 8, "id": "H18EcXFUl", "invitation": "ICLR.cc/2017/conference/-/paper323/public/comment", "forum": "H1Fk2Iqex", "replyto": "Bk_tqfJ4l", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "Tests on TIMIT are added in the available revised paper", "comment": "We clarified terms and ran the model on SPEECH TIMIT, we show an improvement of +2.3% of vowel accuracy against raw audio CNN, while the training is divided by a factor 2.\nWe added the code of the FCT. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fast Chirplet Transform to Enhance CNN Machine Listening - Validation on Animal calls and Speech", "abstract": "The scattering framework offers an optimal hierarchical convolutional decomposition according to its kernels.  Convolutional Neural Net (CNN) can be seen asan optimal kernel decomposition, nevertheless it requires large amount of trainingdata to learn its kernels.  We propose a trade-off between these two approaches: a Chirplet kernel as an efficient Q constant bioacoustic representation to pretrainCNN. First we motivate Chirplet bioinspired auditory representation. Second we give the first algorithm (and code) of a Fast Chirplet Transform (FCT). Third, we demonstrate the computation efficiency of FCT on large environmental data base: months of Orca recordings, and 1000 Birds species from the LifeClef challenge. Fourth, we validate FCT on the vowels subset of the Speech TIMIT dataset. The results show that FCT accelerates CNN when it pretrains low level layers: it reduces training duration by -28% for birds classification, and by -26% for vowels classification. Scores are also enhanced by FCT pretraining, with a relative gain of +7.8% of Mean Average Precision on birds,  and +2.3% of vowel accuracy against raw audio CNN. We conclude on perspectives on tonotopic FCT deep machine listening, and inter-species bioacoustic transfer learning to generalise the representation of animal communication systems.", "pdf": "/pdf/19cd6593b3706f4ae2f90b8d46e0f7338f30bc6e.pdf", "TL;DR": "Proposing a chirplet transform in order to regulate the input of deep-CNN and possible extension to chirplet learning for deep learning bioacoustics", "paperhash": "glotin|fast_chirplet_transform_to_enhance_cnn_machine_listening_validation_on_animal_calls_and_speech", "keywords": ["Applications", "Supervised Learning", "Deep learning", "Speech"], "conflicts": ["rice.edu", "univ-tln.fr"], "authors": ["Herve Glotin", "Julien Ricard", "Randall Balestriero"], "authorids": ["glotin@univ-tln.fr", "julien.ricard@gmail.com", "randallbalestriero@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287621462, "id": "ICLR.cc/2017/conference/-/paper323/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "H1Fk2Iqex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper323/reviewers", "ICLR.cc/2017/conference/paper323/areachairs"], "cdate": 1485287621462}}}, {"tddate": null, "tmdate": 1484499329307, "tcdate": 1484499329307, "number": 7, "id": "SyFKYmtLl", "invitation": "ICLR.cc/2017/conference/-/paper323/public/comment", "forum": "H1Fk2Iqex", "replyto": "HJVMLcbVe", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "Added source code (annexe and Github)", "comment": "The current available revised version contains the code and link to GitHub, the writing is improved, and we added SPEECH TIMIT experimentation showing improvement on vowel recognition using proposed FCT."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fast Chirplet Transform to Enhance CNN Machine Listening - Validation on Animal calls and Speech", "abstract": "The scattering framework offers an optimal hierarchical convolutional decomposition according to its kernels.  Convolutional Neural Net (CNN) can be seen asan optimal kernel decomposition, nevertheless it requires large amount of trainingdata to learn its kernels.  We propose a trade-off between these two approaches: a Chirplet kernel as an efficient Q constant bioacoustic representation to pretrainCNN. First we motivate Chirplet bioinspired auditory representation. Second we give the first algorithm (and code) of a Fast Chirplet Transform (FCT). Third, we demonstrate the computation efficiency of FCT on large environmental data base: months of Orca recordings, and 1000 Birds species from the LifeClef challenge. Fourth, we validate FCT on the vowels subset of the Speech TIMIT dataset. The results show that FCT accelerates CNN when it pretrains low level layers: it reduces training duration by -28% for birds classification, and by -26% for vowels classification. Scores are also enhanced by FCT pretraining, with a relative gain of +7.8% of Mean Average Precision on birds,  and +2.3% of vowel accuracy against raw audio CNN. We conclude on perspectives on tonotopic FCT deep machine listening, and inter-species bioacoustic transfer learning to generalise the representation of animal communication systems.", "pdf": "/pdf/19cd6593b3706f4ae2f90b8d46e0f7338f30bc6e.pdf", "TL;DR": "Proposing a chirplet transform in order to regulate the input of deep-CNN and possible extension to chirplet learning for deep learning bioacoustics", "paperhash": "glotin|fast_chirplet_transform_to_enhance_cnn_machine_listening_validation_on_animal_calls_and_speech", "keywords": ["Applications", "Supervised Learning", "Deep learning", "Speech"], "conflicts": ["rice.edu", "univ-tln.fr"], "authors": ["Herve Glotin", "Julien Ricard", "Randall Balestriero"], "authorids": ["glotin@univ-tln.fr", "julien.ricard@gmail.com", "randallbalestriero@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287621462, "id": "ICLR.cc/2017/conference/-/paper323/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "H1Fk2Iqex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper323/reviewers", "ICLR.cc/2017/conference/paper323/areachairs"], "cdate": 1485287621462}}}, {"tddate": null, "tmdate": 1484499177001, "tcdate": 1484499177001, "number": 6, "id": "S1bgY7Y8x", "invitation": "ICLR.cc/2017/conference/-/paper323/public/comment", "forum": "H1Fk2Iqex", "replyto": "ryiPhhUNx", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "Tests on TIMIT are added in the available revised paper", "comment": "We ran the model on TIMIT, we show an improvement of +2.3% of vowel accuracy against raw audio CNN."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fast Chirplet Transform to Enhance CNN Machine Listening - Validation on Animal calls and Speech", "abstract": "The scattering framework offers an optimal hierarchical convolutional decomposition according to its kernels.  Convolutional Neural Net (CNN) can be seen asan optimal kernel decomposition, nevertheless it requires large amount of trainingdata to learn its kernels.  We propose a trade-off between these two approaches: a Chirplet kernel as an efficient Q constant bioacoustic representation to pretrainCNN. First we motivate Chirplet bioinspired auditory representation. Second we give the first algorithm (and code) of a Fast Chirplet Transform (FCT). Third, we demonstrate the computation efficiency of FCT on large environmental data base: months of Orca recordings, and 1000 Birds species from the LifeClef challenge. Fourth, we validate FCT on the vowels subset of the Speech TIMIT dataset. The results show that FCT accelerates CNN when it pretrains low level layers: it reduces training duration by -28% for birds classification, and by -26% for vowels classification. Scores are also enhanced by FCT pretraining, with a relative gain of +7.8% of Mean Average Precision on birds,  and +2.3% of vowel accuracy against raw audio CNN. We conclude on perspectives on tonotopic FCT deep machine listening, and inter-species bioacoustic transfer learning to generalise the representation of animal communication systems.", "pdf": "/pdf/19cd6593b3706f4ae2f90b8d46e0f7338f30bc6e.pdf", "TL;DR": "Proposing a chirplet transform in order to regulate the input of deep-CNN and possible extension to chirplet learning for deep learning bioacoustics", "paperhash": "glotin|fast_chirplet_transform_to_enhance_cnn_machine_listening_validation_on_animal_calls_and_speech", "keywords": ["Applications", "Supervised Learning", "Deep learning", "Speech"], "conflicts": ["rice.edu", "univ-tln.fr"], "authors": ["Herve Glotin", "Julien Ricard", "Randall Balestriero"], "authorids": ["glotin@univ-tln.fr", "julien.ricard@gmail.com", "randallbalestriero@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287621462, "id": "ICLR.cc/2017/conference/-/paper323/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "H1Fk2Iqex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper323/reviewers", "ICLR.cc/2017/conference/paper323/areachairs"], "cdate": 1485287621462}}}, {"tddate": null, "tmdate": 1484245088177, "tcdate": 1484244538884, "number": 5, "id": "BkXSUSB8g", "invitation": "ICLR.cc/2017/conference/-/paper323/public/comment", "forum": "H1Fk2Iqex", "replyto": "HJVMLcbVe", "signatures": ["~Randall_Balestriero1"], "readers": ["everyone"], "writers": ["~Randall_Balestriero1"], "content": {"title": "Answer to Reviewer1", "comment": "Thank you for the comments and reviewing process.\nWe now have new performances results on the TIMIT (speech) dataset. This plus a reworking of the paper had been done.\nAll the changes are present in the updated paper version.\n\nRegards"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fast Chirplet Transform to Enhance CNN Machine Listening - Validation on Animal calls and Speech", "abstract": "The scattering framework offers an optimal hierarchical convolutional decomposition according to its kernels.  Convolutional Neural Net (CNN) can be seen asan optimal kernel decomposition, nevertheless it requires large amount of trainingdata to learn its kernels.  We propose a trade-off between these two approaches: a Chirplet kernel as an efficient Q constant bioacoustic representation to pretrainCNN. First we motivate Chirplet bioinspired auditory representation. Second we give the first algorithm (and code) of a Fast Chirplet Transform (FCT). Third, we demonstrate the computation efficiency of FCT on large environmental data base: months of Orca recordings, and 1000 Birds species from the LifeClef challenge. Fourth, we validate FCT on the vowels subset of the Speech TIMIT dataset. The results show that FCT accelerates CNN when it pretrains low level layers: it reduces training duration by -28% for birds classification, and by -26% for vowels classification. Scores are also enhanced by FCT pretraining, with a relative gain of +7.8% of Mean Average Precision on birds,  and +2.3% of vowel accuracy against raw audio CNN. We conclude on perspectives on tonotopic FCT deep machine listening, and inter-species bioacoustic transfer learning to generalise the representation of animal communication systems.", "pdf": "/pdf/19cd6593b3706f4ae2f90b8d46e0f7338f30bc6e.pdf", "TL;DR": "Proposing a chirplet transform in order to regulate the input of deep-CNN and possible extension to chirplet learning for deep learning bioacoustics", "paperhash": "glotin|fast_chirplet_transform_to_enhance_cnn_machine_listening_validation_on_animal_calls_and_speech", "keywords": ["Applications", "Supervised Learning", "Deep learning", "Speech"], "conflicts": ["rice.edu", "univ-tln.fr"], "authors": ["Herve Glotin", "Julien Ricard", "Randall Balestriero"], "authorids": ["glotin@univ-tln.fr", "julien.ricard@gmail.com", "randallbalestriero@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287621462, "id": "ICLR.cc/2017/conference/-/paper323/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "H1Fk2Iqex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper323/reviewers", "ICLR.cc/2017/conference/paper323/areachairs"], "cdate": 1485287621462}}}, {"tddate": null, "tmdate": 1483367503577, "tcdate": 1483367503577, "number": 4, "id": "rJdUEk_Bx", "invitation": "ICLR.cc/2017/conference/-/paper323/public/comment", "forum": "H1Fk2Iqex", "replyto": "Bk_tqfJ4l", "signatures": ["~Randall_Balestriero1"], "readers": ["everyone"], "writers": ["~Randall_Balestriero1"], "content": {"title": "Answers to AnonReviewer3", "comment": "Thank you for your questions.\nThis idea of pre-training is not similar to what is proposed by Mallat, in fact in the scattering transform, everything is deterministic and not just \"initialized\" the only learning is done by the final classifier. Only recently they changed this to pre-training due to the success of neural networks.\nThe used bird call classification challenge provides the largest sound corpus for this kind of bioacoustic classification tasks. The gain is not just concerning the gain in classification accuracy but also the speed of convergence of the neural network.\nWe are currently working on the TIMIT (speech) dataset to present more results on the chirpnet performances, hopefully before the review deadline.\n\nFeel free to ask anymore points.\n\nRegards."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fast Chirplet Transform to Enhance CNN Machine Listening - Validation on Animal calls and Speech", "abstract": "The scattering framework offers an optimal hierarchical convolutional decomposition according to its kernels.  Convolutional Neural Net (CNN) can be seen asan optimal kernel decomposition, nevertheless it requires large amount of trainingdata to learn its kernels.  We propose a trade-off between these two approaches: a Chirplet kernel as an efficient Q constant bioacoustic representation to pretrainCNN. First we motivate Chirplet bioinspired auditory representation. Second we give the first algorithm (and code) of a Fast Chirplet Transform (FCT). Third, we demonstrate the computation efficiency of FCT on large environmental data base: months of Orca recordings, and 1000 Birds species from the LifeClef challenge. Fourth, we validate FCT on the vowels subset of the Speech TIMIT dataset. The results show that FCT accelerates CNN when it pretrains low level layers: it reduces training duration by -28% for birds classification, and by -26% for vowels classification. Scores are also enhanced by FCT pretraining, with a relative gain of +7.8% of Mean Average Precision on birds,  and +2.3% of vowel accuracy against raw audio CNN. We conclude on perspectives on tonotopic FCT deep machine listening, and inter-species bioacoustic transfer learning to generalise the representation of animal communication systems.", "pdf": "/pdf/19cd6593b3706f4ae2f90b8d46e0f7338f30bc6e.pdf", "TL;DR": "Proposing a chirplet transform in order to regulate the input of deep-CNN and possible extension to chirplet learning for deep learning bioacoustics", "paperhash": "glotin|fast_chirplet_transform_to_enhance_cnn_machine_listening_validation_on_animal_calls_and_speech", "keywords": ["Applications", "Supervised Learning", "Deep learning", "Speech"], "conflicts": ["rice.edu", "univ-tln.fr"], "authors": ["Herve Glotin", "Julien Ricard", "Randall Balestriero"], "authorids": ["glotin@univ-tln.fr", "julien.ricard@gmail.com", "randallbalestriero@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287621462, "id": "ICLR.cc/2017/conference/-/paper323/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "H1Fk2Iqex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper323/reviewers", "ICLR.cc/2017/conference/paper323/areachairs"], "cdate": 1485287621462}}}, {"tddate": null, "tmdate": 1482375427189, "tcdate": 1482375427189, "number": 3, "id": "HyiZW6d4g", "invitation": "ICLR.cc/2017/conference/-/paper323/public/comment", "forum": "H1Fk2Iqex", "replyto": "ryiPhhUNx", "signatures": ["~Randall_Balestriero1"], "readers": ["everyone"], "writers": ["~Randall_Balestriero1"], "content": {"title": "Answer to AnonReviewer2", "comment": "This is indeed the direction that we discuss in this paper. We trained a CNN from raw audio (see page12) to then we show faster training and better MAP with our approach.\nA full feature learning should be taken for large scale problems being tackled with deep learning approaches. \nHowever, in order to use fully learned models, the number of observations must be important (at least as many as the number of free parameters in the model), and from clean mono species recordings, and important regularization should be applied precisely. \n\nNowadays, bioacoustic research (see http://sabiod.org to get links to the largest available challenge on bird classification, including some we organized at NIPS and ICML workshops), the volume of available mono-species clean recordings to learn the underlying distribution per species is limited. \n\nThe experiment we run in this paper is based on the largest Amazon avian dataset (cf LifeClef 2015, 16, 17).  The selected species we train are represented by the real available files for each species. It is not so much as you can see.\n\nAn alternative is thus to use our prior knowledge from advanced Q constant acoustic representation, to try to bias the network towards this direction which in our case corresponds to applying a chirplet transform to the raw waveform, and then retrain the system to adapt the chirpnet to the training data. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fast Chirplet Transform to Enhance CNN Machine Listening - Validation on Animal calls and Speech", "abstract": "The scattering framework offers an optimal hierarchical convolutional decomposition according to its kernels.  Convolutional Neural Net (CNN) can be seen asan optimal kernel decomposition, nevertheless it requires large amount of trainingdata to learn its kernels.  We propose a trade-off between these two approaches: a Chirplet kernel as an efficient Q constant bioacoustic representation to pretrainCNN. First we motivate Chirplet bioinspired auditory representation. Second we give the first algorithm (and code) of a Fast Chirplet Transform (FCT). Third, we demonstrate the computation efficiency of FCT on large environmental data base: months of Orca recordings, and 1000 Birds species from the LifeClef challenge. Fourth, we validate FCT on the vowels subset of the Speech TIMIT dataset. The results show that FCT accelerates CNN when it pretrains low level layers: it reduces training duration by -28% for birds classification, and by -26% for vowels classification. Scores are also enhanced by FCT pretraining, with a relative gain of +7.8% of Mean Average Precision on birds,  and +2.3% of vowel accuracy against raw audio CNN. We conclude on perspectives on tonotopic FCT deep machine listening, and inter-species bioacoustic transfer learning to generalise the representation of animal communication systems.", "pdf": "/pdf/19cd6593b3706f4ae2f90b8d46e0f7338f30bc6e.pdf", "TL;DR": "Proposing a chirplet transform in order to regulate the input of deep-CNN and possible extension to chirplet learning for deep learning bioacoustics", "paperhash": "glotin|fast_chirplet_transform_to_enhance_cnn_machine_listening_validation_on_animal_calls_and_speech", "keywords": ["Applications", "Supervised Learning", "Deep learning", "Speech"], "conflicts": ["rice.edu", "univ-tln.fr"], "authors": ["Herve Glotin", "Julien Ricard", "Randall Balestriero"], "authorids": ["glotin@univ-tln.fr", "julien.ricard@gmail.com", "randallbalestriero@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287621462, "id": "ICLR.cc/2017/conference/-/paper323/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "H1Fk2Iqex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper323/reviewers", "ICLR.cc/2017/conference/paper323/areachairs"], "cdate": 1485287621462}}}, {"tddate": null, "tmdate": 1482243171047, "tcdate": 1482243171047, "number": 3, "id": "ryiPhhUNx", "invitation": "ICLR.cc/2017/conference/-/paper323/official/review", "forum": "H1Fk2Iqex", "replyto": "H1Fk2Iqex", "signatures": ["ICLR.cc/2017/conference/paper323/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper323/AnonReviewer2"], "content": {"title": "Chirplet transforms for small data tasks", "rating": "6: Marginally above acceptance threshold", "review": "While I understand the difficulty of collecting audio data from animals, I think this type of feature engineering does not go in the right direction. I would rather see a model than learns the feature representation from data.  I would think it should be possible to collect a more substantial corpus in zoos / nature etc, and then train a generative model. The underlying learned feature representation could be then used to feed a classifier. I'm not familiar with the particularities of this task, it's hard to judge the improvements by using chirplets.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fast Chirplet Transform to Enhance CNN Machine Listening - Validation on Animal calls and Speech", "abstract": "The scattering framework offers an optimal hierarchical convolutional decomposition according to its kernels.  Convolutional Neural Net (CNN) can be seen asan optimal kernel decomposition, nevertheless it requires large amount of trainingdata to learn its kernels.  We propose a trade-off between these two approaches: a Chirplet kernel as an efficient Q constant bioacoustic representation to pretrainCNN. First we motivate Chirplet bioinspired auditory representation. Second we give the first algorithm (and code) of a Fast Chirplet Transform (FCT). Third, we demonstrate the computation efficiency of FCT on large environmental data base: months of Orca recordings, and 1000 Birds species from the LifeClef challenge. Fourth, we validate FCT on the vowels subset of the Speech TIMIT dataset. The results show that FCT accelerates CNN when it pretrains low level layers: it reduces training duration by -28% for birds classification, and by -26% for vowels classification. Scores are also enhanced by FCT pretraining, with a relative gain of +7.8% of Mean Average Precision on birds,  and +2.3% of vowel accuracy against raw audio CNN. We conclude on perspectives on tonotopic FCT deep machine listening, and inter-species bioacoustic transfer learning to generalise the representation of animal communication systems.", "pdf": "/pdf/19cd6593b3706f4ae2f90b8d46e0f7338f30bc6e.pdf", "TL;DR": "Proposing a chirplet transform in order to regulate the input of deep-CNN and possible extension to chirplet learning for deep learning bioacoustics", "paperhash": "glotin|fast_chirplet_transform_to_enhance_cnn_machine_listening_validation_on_animal_calls_and_speech", "keywords": ["Applications", "Supervised Learning", "Deep learning", "Speech"], "conflicts": ["rice.edu", "univ-tln.fr"], "authors": ["Herve Glotin", "Julien Ricard", "Randall Balestriero"], "authorids": ["glotin@univ-tln.fr", "julien.ricard@gmail.com", "randallbalestriero@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512623666, "id": "ICLR.cc/2017/conference/-/paper323/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper323/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper323/AnonReviewer3", "ICLR.cc/2017/conference/paper323/AnonReviewer1", "ICLR.cc/2017/conference/paper323/AnonReviewer2"], "reply": {"forum": "H1Fk2Iqex", "replyto": "H1Fk2Iqex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper323/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper323/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512623666}}}, {"tddate": null, "tmdate": 1481929500218, "tcdate": 1481929500218, "number": 2, "id": "rkV77xzNl", "invitation": "ICLR.cc/2017/conference/-/paper323/public/comment", "forum": "H1Fk2Iqex", "replyto": "HJ-hDVlNe", "signatures": ["~Randall_Balestriero1"], "readers": ["everyone"], "writers": ["~Randall_Balestriero1"], "content": {"title": "Answer", "comment": "\"This holds for continuous wavelets such as Morlet, DOG. For the case of discrete wavelets, this is indeed inappropriate.\"\n    Theorem 2.7, page 45, A wavelet tour of signal processing, Mallat, 2009\n\n->So indeed in practice the support is on the whole domain since by definition they are exponentials. However, in our case, we redefine the support of the wavelet as the set on which it is greater than a specific given threshold \\epsilon and in this case, the Morlet and DOG for example have indeed a compact support around their center frequency and given the scale.\n\n \n\n\"We use the local scattering since the averaging is not global. This is to reduce some transient artifacts and high frequency noise. For birds which has quite \"long\" songs, the low-pass will not affect them a priori.\nIt is indeed first order scattering and the higher order coefficients are not computed explicitly since they should be learned through the next CNN layer.\"\nNo, there is absolutely no reason second order coefficients of scattering should be learned yet I would be glad if you could indicate such works. Besides, this argument is wrong in the sens that the low-pass filter could be learned as well and might avoid a loss of information.\nWith this argument, one could claim that the first layer should be chirplets, since it will be learned by the CNN... However, structuring the first layer seems to be a good idea.\"\n\n->I don't understand why you claim that second order scattering coefficients shouldn't be learned. For example, learning the \\lambda_2 wavelets could be interesting since in the joint-scattering, it could lead to using chirplets instead of standard 2D Gabor wavelets for example. That being said, I agree on learning the low-pass filter. \n\n\"Naturally the representation is more sparse after averaging due to the disregard of high frequency information.\"\nThis is incorrect. A wavelet transform is sparse on many natural signals. When you average it, you reduce the sparsity: the modulus+averaing avoids this kind of phenomenon. That might be checked numerically and it really depends on the nature of your data.\n\n->Indeed, this was incorrect. The representation before averaging and after averaging were sparse but indeed, after averaging you loose sparsity since the nonlinearity accumulates energy around w=0. However, the SNR was increased after averaging.\n\n\"We did not applied logarithm compression, as the optimal compression shall be learned in the next CNN stage. Similarly, thresholding functions would be learnt for any denoising.\"\nThis is a strong claim and I do not understand why a classifier should necessarily compress the information/explictely denoise. Do you mean the loss is invariant to noise, thus the representation should be as well?\n\n->The classifier is trained with clean data up to now, and the objective function yields to adapted / optimal non linear transformations of the input features.\n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fast Chirplet Transform to Enhance CNN Machine Listening - Validation on Animal calls and Speech", "abstract": "The scattering framework offers an optimal hierarchical convolutional decomposition according to its kernels.  Convolutional Neural Net (CNN) can be seen asan optimal kernel decomposition, nevertheless it requires large amount of trainingdata to learn its kernels.  We propose a trade-off between these two approaches: a Chirplet kernel as an efficient Q constant bioacoustic representation to pretrainCNN. First we motivate Chirplet bioinspired auditory representation. Second we give the first algorithm (and code) of a Fast Chirplet Transform (FCT). Third, we demonstrate the computation efficiency of FCT on large environmental data base: months of Orca recordings, and 1000 Birds species from the LifeClef challenge. Fourth, we validate FCT on the vowels subset of the Speech TIMIT dataset. The results show that FCT accelerates CNN when it pretrains low level layers: it reduces training duration by -28% for birds classification, and by -26% for vowels classification. Scores are also enhanced by FCT pretraining, with a relative gain of +7.8% of Mean Average Precision on birds,  and +2.3% of vowel accuracy against raw audio CNN. We conclude on perspectives on tonotopic FCT deep machine listening, and inter-species bioacoustic transfer learning to generalise the representation of animal communication systems.", "pdf": "/pdf/19cd6593b3706f4ae2f90b8d46e0f7338f30bc6e.pdf", "TL;DR": "Proposing a chirplet transform in order to regulate the input of deep-CNN and possible extension to chirplet learning for deep learning bioacoustics", "paperhash": "glotin|fast_chirplet_transform_to_enhance_cnn_machine_listening_validation_on_animal_calls_and_speech", "keywords": ["Applications", "Supervised Learning", "Deep learning", "Speech"], "conflicts": ["rice.edu", "univ-tln.fr"], "authors": ["Herve Glotin", "Julien Ricard", "Randall Balestriero"], "authorids": ["glotin@univ-tln.fr", "julien.ricard@gmail.com", "randallbalestriero@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287621462, "id": "ICLR.cc/2017/conference/-/paper323/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "H1Fk2Iqex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper323/reviewers", "ICLR.cc/2017/conference/paper323/areachairs"], "cdate": 1485287621462}}}, {"tddate": null, "tmdate": 1481905675595, "tcdate": 1481905675595, "number": 2, "id": "HJVMLcbVe", "invitation": "ICLR.cc/2017/conference/-/paper323/official/review", "forum": "H1Fk2Iqex", "replyto": "H1Fk2Iqex", "signatures": ["ICLR.cc/2017/conference/paper323/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper323/AnonReviewer1"], "content": {"title": "A work to try to structure a deep network", "rating": "6: Marginally above acceptance threshold", "review": "Pros: \n- Introduction of a nice filter banks and its implementation\n- Good numerical results\n- Refinement of the representation via back propagation, and a demonstration that it speeds up learning\n\nCons:\n- The algorithms (section 3.1) are not necessary, and they even affect the presentation of the paper. However, a source code would be great!\n- The link with a scattering transform is not clear\n- Sometimes (as mentionned in some of my comments), the writing could be improved.\n\nFrom a personal point of view, I also believe the negative points I mention can be easily removed.", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fast Chirplet Transform to Enhance CNN Machine Listening - Validation on Animal calls and Speech", "abstract": "The scattering framework offers an optimal hierarchical convolutional decomposition according to its kernels.  Convolutional Neural Net (CNN) can be seen asan optimal kernel decomposition, nevertheless it requires large amount of trainingdata to learn its kernels.  We propose a trade-off between these two approaches: a Chirplet kernel as an efficient Q constant bioacoustic representation to pretrainCNN. First we motivate Chirplet bioinspired auditory representation. Second we give the first algorithm (and code) of a Fast Chirplet Transform (FCT). Third, we demonstrate the computation efficiency of FCT on large environmental data base: months of Orca recordings, and 1000 Birds species from the LifeClef challenge. Fourth, we validate FCT on the vowels subset of the Speech TIMIT dataset. The results show that FCT accelerates CNN when it pretrains low level layers: it reduces training duration by -28% for birds classification, and by -26% for vowels classification. Scores are also enhanced by FCT pretraining, with a relative gain of +7.8% of Mean Average Precision on birds,  and +2.3% of vowel accuracy against raw audio CNN. We conclude on perspectives on tonotopic FCT deep machine listening, and inter-species bioacoustic transfer learning to generalise the representation of animal communication systems.", "pdf": "/pdf/19cd6593b3706f4ae2f90b8d46e0f7338f30bc6e.pdf", "TL;DR": "Proposing a chirplet transform in order to regulate the input of deep-CNN and possible extension to chirplet learning for deep learning bioacoustics", "paperhash": "glotin|fast_chirplet_transform_to_enhance_cnn_machine_listening_validation_on_animal_calls_and_speech", "keywords": ["Applications", "Supervised Learning", "Deep learning", "Speech"], "conflicts": ["rice.edu", "univ-tln.fr"], "authors": ["Herve Glotin", "Julien Ricard", "Randall Balestriero"], "authorids": ["glotin@univ-tln.fr", "julien.ricard@gmail.com", "randallbalestriero@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512623666, "id": "ICLR.cc/2017/conference/-/paper323/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper323/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper323/AnonReviewer3", "ICLR.cc/2017/conference/paper323/AnonReviewer1", "ICLR.cc/2017/conference/paper323/AnonReviewer2"], "reply": {"forum": "H1Fk2Iqex", "replyto": "H1Fk2Iqex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper323/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper323/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512623666}}}, {"tddate": null, "tmdate": 1481815977577, "tcdate": 1481815977449, "number": 2, "id": "HJ-hDVlNe", "invitation": "ICLR.cc/2017/conference/-/paper323/official/comment", "forum": "H1Fk2Iqex", "replyto": "SynnawVmx", "signatures": ["ICLR.cc/2017/conference/paper323/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper323/AnonReviewer1"], "content": {"title": "Answer", "comment": "\"This holds for continuous wavelets such as Morlet, DOG. For the case of discrete wavelets, this is indeed inappropriate.\"\nTheorem 2.7, page 45, A wavelet tour of signal processing, Mallat, 2009\n\n\"Indeed a downsampling should be done on each of the Ux(lambda,t) but this would lead to non constant size across lambda which would not be suited for a CNN input.\"\nIt depends on the way you implement it, I just meant there is a redundancy of the coefficients.\n\n\"We use the local scattering since the averaging is not global. This is to reduce some transient artifacts and high frequency noise. For birds which has quite \"long\" songs, the low-pass will not affect them a priori.\nIt is indeed first order scattering and the higher order coefficients are not computed explicitly since they should be learned through the next CNN layer.\"\nNo, there is absolutely no reason second order coefficients of scattering should be learned yet I would be glad if you could indicate such works. Besides, this argument is wrong in the sens that the low-pass filter could be learned as well and might avoid a loss of information. \nWith this argument, one could claim that the first layer should be chirplets, since it will be learned by the CNN... However, structuring the first layer seems to be a good idea.\n\n\"The joint scattering or even the more recent spiral scattering would indeed be more suited for analysis. Yet since the aim is to feed the representation into a 2D CNN these representation would have too much parameters.\"\nOk.\n\n\"Naturally the representation is more sparse after averaging due to the disregard of high frequency information.\"\nThis is incorrect. A wavelet transform is sparse on many natural signals. When you average it, you reduce the sparsity: the modulus+averaing avoids this kind of phenomenon. That might be checked numerically and it really depends on the nature of your data.\n\n\"Littlewood Paley usually refers to the renormalization of the filters in the Fourier domain as referred in Mallat paper. This ensures that the scattering coefficients converge to 0 layers after layers. Is this what you are referring too ? If yes, not littlewood paley was applied on the filter-bank.\"\nI will clarify this comment which is indeed not clear. I was thinking to \\sum |\\hat \\psi_\\lambda|^2 for instance.\n\n\"We did not applied logarithm compression, as the optimal compression shall be learned in the next CNN stage. Similarly, thresholding functions would be learnt for any denoising.\"\nThis is a strong claim and I do not understand why a classifier should necessarily compress the information/explictely denoise. Do you mean the loss is invariant to noise, thus the representation should be as well?\n\n\"It is standard for bird signals but would need some fine tuning for different dataset. No cross-validation as part of the CNN pipeline was done due to the time needed for training one model.\"\nIt is very promising for future research!"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fast Chirplet Transform to Enhance CNN Machine Listening - Validation on Animal calls and Speech", "abstract": "The scattering framework offers an optimal hierarchical convolutional decomposition according to its kernels.  Convolutional Neural Net (CNN) can be seen asan optimal kernel decomposition, nevertheless it requires large amount of trainingdata to learn its kernels.  We propose a trade-off between these two approaches: a Chirplet kernel as an efficient Q constant bioacoustic representation to pretrainCNN. First we motivate Chirplet bioinspired auditory representation. Second we give the first algorithm (and code) of a Fast Chirplet Transform (FCT). Third, we demonstrate the computation efficiency of FCT on large environmental data base: months of Orca recordings, and 1000 Birds species from the LifeClef challenge. Fourth, we validate FCT on the vowels subset of the Speech TIMIT dataset. The results show that FCT accelerates CNN when it pretrains low level layers: it reduces training duration by -28% for birds classification, and by -26% for vowels classification. Scores are also enhanced by FCT pretraining, with a relative gain of +7.8% of Mean Average Precision on birds,  and +2.3% of vowel accuracy against raw audio CNN. We conclude on perspectives on tonotopic FCT deep machine listening, and inter-species bioacoustic transfer learning to generalise the representation of animal communication systems.", "pdf": "/pdf/19cd6593b3706f4ae2f90b8d46e0f7338f30bc6e.pdf", "TL;DR": "Proposing a chirplet transform in order to regulate the input of deep-CNN and possible extension to chirplet learning for deep learning bioacoustics", "paperhash": "glotin|fast_chirplet_transform_to_enhance_cnn_machine_listening_validation_on_animal_calls_and_speech", "keywords": ["Applications", "Supervised Learning", "Deep learning", "Speech"], "conflicts": ["rice.edu", "univ-tln.fr"], "authors": ["Herve Glotin", "Julien Ricard", "Randall Balestriero"], "authorids": ["glotin@univ-tln.fr", "julien.ricard@gmail.com", "randallbalestriero@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287621312, "id": "ICLR.cc/2017/conference/-/paper323/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "H1Fk2Iqex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper323/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper323/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper323/reviewers", "ICLR.cc/2017/conference/paper323/areachairs"], "cdate": 1485287621312}}}, {"tddate": null, "tmdate": 1481742976540, "tcdate": 1481742976533, "number": 1, "id": "Bk_tqfJ4l", "invitation": "ICLR.cc/2017/conference/-/paper323/official/review", "forum": "H1Fk2Iqex", "replyto": "H1Fk2Iqex", "signatures": ["ICLR.cc/2017/conference/paper323/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper323/AnonReviewer3"], "content": {"title": "", "rating": "4: Ok but not good enough - rejection", "review": "The authors advocate use of chirplets as a basis for modeling audio signals.  They introduce a fast chiplet transform for efficient computation. Also introduced is the idea of initializing (pre-training) CNN layers to mimic chirplet transform of audio signal (similar to ideas proposed by Mallet et al. on scattering transforms).  The paper is fairly easy to follow but in a few places contains undefined terms (e.g. AM-FM, MAP).\n\nWhile the idea of using chirplet transform is interesting, my main concern is that the empirical evidence provided is in a rather narrow domain of bird call classification.  Furthermore, the accuracy gains shown in that domain are relatively small (61% MAP for log-Mel features vs 61.5% for chirplet transforms).  I would recommend that authors provide evidence for how this generalizes to other audio (including speech) tasks.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fast Chirplet Transform to Enhance CNN Machine Listening - Validation on Animal calls and Speech", "abstract": "The scattering framework offers an optimal hierarchical convolutional decomposition according to its kernels.  Convolutional Neural Net (CNN) can be seen asan optimal kernel decomposition, nevertheless it requires large amount of trainingdata to learn its kernels.  We propose a trade-off between these two approaches: a Chirplet kernel as an efficient Q constant bioacoustic representation to pretrainCNN. First we motivate Chirplet bioinspired auditory representation. Second we give the first algorithm (and code) of a Fast Chirplet Transform (FCT). Third, we demonstrate the computation efficiency of FCT on large environmental data base: months of Orca recordings, and 1000 Birds species from the LifeClef challenge. Fourth, we validate FCT on the vowels subset of the Speech TIMIT dataset. The results show that FCT accelerates CNN when it pretrains low level layers: it reduces training duration by -28% for birds classification, and by -26% for vowels classification. Scores are also enhanced by FCT pretraining, with a relative gain of +7.8% of Mean Average Precision on birds,  and +2.3% of vowel accuracy against raw audio CNN. We conclude on perspectives on tonotopic FCT deep machine listening, and inter-species bioacoustic transfer learning to generalise the representation of animal communication systems.", "pdf": "/pdf/19cd6593b3706f4ae2f90b8d46e0f7338f30bc6e.pdf", "TL;DR": "Proposing a chirplet transform in order to regulate the input of deep-CNN and possible extension to chirplet learning for deep learning bioacoustics", "paperhash": "glotin|fast_chirplet_transform_to_enhance_cnn_machine_listening_validation_on_animal_calls_and_speech", "keywords": ["Applications", "Supervised Learning", "Deep learning", "Speech"], "conflicts": ["rice.edu", "univ-tln.fr"], "authors": ["Herve Glotin", "Julien Ricard", "Randall Balestriero"], "authorids": ["glotin@univ-tln.fr", "julien.ricard@gmail.com", "randallbalestriero@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512623666, "id": "ICLR.cc/2017/conference/-/paper323/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper323/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper323/AnonReviewer3", "ICLR.cc/2017/conference/paper323/AnonReviewer1", "ICLR.cc/2017/conference/paper323/AnonReviewer2"], "reply": {"forum": "H1Fk2Iqex", "replyto": "H1Fk2Iqex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper323/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper323/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512623666}}}, {"tddate": null, "tmdate": 1481301034240, "tcdate": 1481043380485, "number": 1, "id": "SynnawVmx", "invitation": "ICLR.cc/2017/conference/-/paper323/public/comment", "forum": "H1Fk2Iqex", "replyto": "r1vcK5pze", "signatures": ["~Randall_Balestriero1"], "readers": ["everyone"], "writers": ["~Randall_Balestriero1"], "content": {"title": "Answers", "comment": "\" A wavelet is an atom with compact support in time and frequency domain which integrates to 0\"\n        You mean \"often localized\". (Haar wavelets are not and the compact support in fourier and spatial domain is mathematically not possible)\n\n->This holds for continuous wavelets such as Morlet, DOG. For the case of discrete wavelets, this is indeed inappropriate.\n\nOtherwise, few remarks: The wavelet should be normalized. There should be a downsampling on Ux(lambda,t). The phase is not an information here, it\u2019s a variability since it corresponds to a local translation.\n\n->Indeed a downsampling should be done on each of the Ux(lambda,t) but this would lead to non constant size across lambda which would not be suited for a CNN input.\n\n\nIt seems that you are using first order coefficients of scattering. Why so? The representation could be potentially more discriminative with higher order coefficients. Do you have some suggestions of operators that could be used to get second order coefficients? Did you try to remove the low-pass filtering? Indeed, your representation might lose discriminative information because of this averaging. \n\n\n->We use the local scattering since the averaging is not global. This is to reduce some transient artifacts and high frequency noise. For birds which has quite \"long\" songs, the low-pass will not affect them a priori.\nIt is indeed first order scattering and the higher order coefficients are not computed explicitly since they should be learned through the next CNN layer.\n\n\nHave you done a comparison of your pipeline with a translation scattering transform or a joined scattering transform( https://arxiv.org/pdf/1512.02125.pdf )? It is claimed that in the case of chirp(and especially birds), the representation is more sparse.\n\n->The joint scattering or even the more recent spiral scattering would indeed be more suited for analysis. Yet since the aim is to feed the representation into a 2D CNN these representation would have too much parameters.\n\n\nWhat is the Littlewood-paley of your representation? Before and after averaging, is the representation sparse?\n\n->Naturally the representation is more sparse after averaging due to the disregard of high frequency information.\nLittlewood Paley usually refers to the renormalization of the filters in the Fourier domain as referred in Mallat paper. This ensures that the scattering coefficients converge to 0 layers after layers. Is this what you are referring too ? If yes, not littlewood paley was applied on the filter-bank.\n\n\nDid you apply a logarithm on the Chirplet coefficients, with the CNN+Chirplet? This dataset seems to have a lot of noise, have you tried to use a threshold?\n\n->We did not applied logarithm compression, as the optimal compression shall be learned in the next CNN stage. Similarly, thresholding functions would be learnt for any denoising.\n\n\nHow did you chose the parameters to create your chirplet filters? Are they standard or cross-validated? Do you think you will have to adapt them when using a new dataset?(e.g. are they generic?)\n\n->It is standard for bird signals but would need some fine tuning for different dataset. No cross-validation as part of the CNN pipeline was done due to the time needed for training one model.\n\n\nWhat is the final size of the representation of a signal of size T? I think there is 110 channels, but it is not clear to me!\n->Each example has shape 80 (frequency bins) x 110 (time bins).\n \nHow did you design the Audio2Chirp CNN ? \n->The hyperparameters of the audio2chirp network were chosen to satisfy the following constraints:\n- long filters to capture modulations on long time scale (for the lower chirplet coefficients)\n- not too much  downsampling between layers (1 layer only would imply 100x downsampling to go from 11025 time bins to 110)\n\nThank you for your consideration and questions !\nRegards"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fast Chirplet Transform to Enhance CNN Machine Listening - Validation on Animal calls and Speech", "abstract": "The scattering framework offers an optimal hierarchical convolutional decomposition according to its kernels.  Convolutional Neural Net (CNN) can be seen asan optimal kernel decomposition, nevertheless it requires large amount of trainingdata to learn its kernels.  We propose a trade-off between these two approaches: a Chirplet kernel as an efficient Q constant bioacoustic representation to pretrainCNN. First we motivate Chirplet bioinspired auditory representation. Second we give the first algorithm (and code) of a Fast Chirplet Transform (FCT). Third, we demonstrate the computation efficiency of FCT on large environmental data base: months of Orca recordings, and 1000 Birds species from the LifeClef challenge. Fourth, we validate FCT on the vowels subset of the Speech TIMIT dataset. The results show that FCT accelerates CNN when it pretrains low level layers: it reduces training duration by -28% for birds classification, and by -26% for vowels classification. Scores are also enhanced by FCT pretraining, with a relative gain of +7.8% of Mean Average Precision on birds,  and +2.3% of vowel accuracy against raw audio CNN. We conclude on perspectives on tonotopic FCT deep machine listening, and inter-species bioacoustic transfer learning to generalise the representation of animal communication systems.", "pdf": "/pdf/19cd6593b3706f4ae2f90b8d46e0f7338f30bc6e.pdf", "TL;DR": "Proposing a chirplet transform in order to regulate the input of deep-CNN and possible extension to chirplet learning for deep learning bioacoustics", "paperhash": "glotin|fast_chirplet_transform_to_enhance_cnn_machine_listening_validation_on_animal_calls_and_speech", "keywords": ["Applications", "Supervised Learning", "Deep learning", "Speech"], "conflicts": ["rice.edu", "univ-tln.fr"], "authors": ["Herve Glotin", "Julien Ricard", "Randall Balestriero"], "authorids": ["glotin@univ-tln.fr", "julien.ricard@gmail.com", "randallbalestriero@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287621462, "id": "ICLR.cc/2017/conference/-/paper323/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "H1Fk2Iqex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper323/reviewers", "ICLR.cc/2017/conference/paper323/areachairs"], "cdate": 1485287621462}}}, {"tddate": null, "tmdate": 1480595854820, "tcdate": 1480595854815, "number": 1, "id": "r1vcK5pze", "invitation": "ICLR.cc/2017/conference/-/paper323/pre-review/question", "forum": "H1Fk2Iqex", "replyto": "H1Fk2Iqex", "signatures": ["ICLR.cc/2017/conference/paper323/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper323/AnonReviewer1"], "content": {"title": "Design of the Chirplets", "question": "First, few typos:\n2.2: \" A wavelet is an atom with compact support in time and frequency domain which integrates to 0\"\nYou mean \"often localized\". (Haar wavelets are not and the compact support in fourier and spatial domain is mathematically not possible)\n\n2.2: Otherwise, few remarks: The wavelet should be normalized. There should be a downsampling on Ux(lambda,t). The phase is not an information here, it\u2019s a variability since it corresponds to a local translation.\n\n\nI would have a few questions:\n\nIt seems that you are using first order coefficients of scattering. Why so? The representation could be potentially more discriminative with higher order coefficients. Do you have some suggestions of operators that could be used to get second order coefficients? Did you try to remove the low-pass filtering? Indeed, your representation might lose discriminative information because of this averaging. \n\nHave you done a comparison of your pipeline with a translation scattering transform or a joined scattering transform( https://arxiv.org/pdf/1512.02125.pdf )? It is claimed that in the case of chirp(and especially birds), the representation is more sparse.\n\nWhat is the Littlewood-paley of your representation? Before and after averaging, is the representation sparse?\n\nDid you apply a logarithm on the Chirplet coefficients, with the CNN+Chirplet? This dataset seems to have a lot of noise, have you tried to use a threshold?\n\nHow did you chose the parameters to create your chirplet filters? Are they standard or cross-validated? Do you think you will have to adapt them when using a new dataset?(e.g. are they generic?)\n\nWhat is the final size of the representation of a signal of size T? I think there is 110 channels, but it is not clear to me!\n\nHow did you design the Audio2Chirp CNN, described in Appendix B? Could you initialize the filters with the Chirplet transform?\n\nThank you!\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fast Chirplet Transform to Enhance CNN Machine Listening - Validation on Animal calls and Speech", "abstract": "The scattering framework offers an optimal hierarchical convolutional decomposition according to its kernels.  Convolutional Neural Net (CNN) can be seen asan optimal kernel decomposition, nevertheless it requires large amount of trainingdata to learn its kernels.  We propose a trade-off between these two approaches: a Chirplet kernel as an efficient Q constant bioacoustic representation to pretrainCNN. First we motivate Chirplet bioinspired auditory representation. Second we give the first algorithm (and code) of a Fast Chirplet Transform (FCT). Third, we demonstrate the computation efficiency of FCT on large environmental data base: months of Orca recordings, and 1000 Birds species from the LifeClef challenge. Fourth, we validate FCT on the vowels subset of the Speech TIMIT dataset. The results show that FCT accelerates CNN when it pretrains low level layers: it reduces training duration by -28% for birds classification, and by -26% for vowels classification. Scores are also enhanced by FCT pretraining, with a relative gain of +7.8% of Mean Average Precision on birds,  and +2.3% of vowel accuracy against raw audio CNN. We conclude on perspectives on tonotopic FCT deep machine listening, and inter-species bioacoustic transfer learning to generalise the representation of animal communication systems.", "pdf": "/pdf/19cd6593b3706f4ae2f90b8d46e0f7338f30bc6e.pdf", "TL;DR": "Proposing a chirplet transform in order to regulate the input of deep-CNN and possible extension to chirplet learning for deep learning bioacoustics", "paperhash": "glotin|fast_chirplet_transform_to_enhance_cnn_machine_listening_validation_on_animal_calls_and_speech", "keywords": ["Applications", "Supervised Learning", "Deep learning", "Speech"], "conflicts": ["rice.edu", "univ-tln.fr"], "authors": ["Herve Glotin", "Julien Ricard", "Randall Balestriero"], "authorids": ["glotin@univ-tln.fr", "julien.ricard@gmail.com", "randallbalestriero@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959340447, "id": "ICLR.cc/2017/conference/-/paper323/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper323/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper323/AnonReviewer1"], "reply": {"forum": "H1Fk2Iqex", "replyto": "H1Fk2Iqex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper323/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper323/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959340447}}}], "count": 17}