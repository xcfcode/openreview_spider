{"notes": [{"id": "vsWa_oGpMr", "original": null, "number": 10, "cdate": 1584434038101, "ddate": null, "tcdate": 1584434038101, "tmdate": 1584434056981, "tddate": null, "forum": "HkgsPhNYPS", "replyto": "75Tf1cUN6f", "invitation": "ICLR.cc/2020/Conference/Paper16/-/Official_Comment", "content": {"title": "Pending approval", "comment": "Hello, Pengfei,\n\nThank you very much for your interest in our work!\nWe have started the approval procedure for the release of the code. \nHowever, we are not sure if it will be successful soon.\nWe will keep you informed as soon as things change.\n\nBest,\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper16/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper16/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ductam.nguyen08@gmail.com", "chaithanyakumar.mummadi@de.bosch.com", "thiphuongnhung.ngo@de.bosch.com", "hoai.phuong.nguyen198@gmail.com", "laura.beggel@de.bosch.com", "brox@cs.uni-freiburg.de"], "title": "SELF: Learning to Filter Noisy Labels with Self-Ensembling", "authors": ["Duc Tam Nguyen", "Chaithanya Kumar Mummadi", "Thi Phuong Nhung Ngo", "Thi Hoai Phuong Nguyen", "Laura Beggel", "Thomas Brox"], "pdf": "/pdf/3bafa3876cfccbe25d2fc5c4db5cd495aae2e6c7.pdf", "TL;DR": "We propose a self-ensemble framework to train more robust deep learning models under noisy labeled datasets.", "abstract": "Deep neural networks (DNNs) have been shown to over-fit a dataset when being trained with noisy labels for a long enough time. To overcome this problem, we present a simple and effective method self-ensemble label filtering (SELF) to progressively filter out the wrong labels during training. Our method improves the task performance by gradually allowing supervision only from the potentially non-noisy (clean) labels and stops learning on the filtered noisy labels. For the filtering, we form running averages of predictions over the entire training dataset using the network output at different training epochs. We show that these ensemble estimates yield more accurate identification of inconsistent predictions throughout training than the single estimates of the network at the most recent training epoch. While filtered samples are removed entirely from the supervised training loss, we dynamically leverage them via semi-supervised learning in the unsupervised loss. We demonstrate the positive effect of such an approach on various image classification tasks under both symmetric and asymmetric label noise and at different noise ratios. It substantially outperforms all previous works on noise-aware learning across different datasets and can be applied to a broad set of network architectures.", "keywords": ["Ensemble Learning", "Robust Learning", "Noisy Labels", "Labels Filtering"], "paperhash": "nguyen|self_learning_to_filter_noisy_labels_with_selfensembling", "_bibtex": "@inproceedings{\nNguyen2020SELF:,\ntitle={SELF: Learning to Filter Noisy Labels with Self-Ensembling},\nauthor={Duc Tam Nguyen and Chaithanya Kumar Mummadi and Thi Phuong Nhung Ngo and Thi Hoai Phuong Nguyen and Laura Beggel and Thomas Brox},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HkgsPhNYPS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/f53c1264354a72df6a3fdcfe8a3344abd4fde7f2.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HkgsPhNYPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper16/Authors", "ICLR.cc/2020/Conference/Paper16/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper16/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper16/Reviewers", "ICLR.cc/2020/Conference/Paper16/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper16/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper16/Authors|ICLR.cc/2020/Conference/Paper16/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504177650, "tmdate": 1576860528453, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper16/Authors", "ICLR.cc/2020/Conference/Paper16/Reviewers", "ICLR.cc/2020/Conference/Paper16/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper16/-/Official_Comment"}}}, {"id": "75Tf1cUN6f", "original": null, "number": 2, "cdate": 1584375194055, "ddate": null, "tcdate": 1584375194055, "tmdate": 1584375194055, "tddate": null, "forum": "HkgsPhNYPS", "replyto": "HkgsPhNYPS", "invitation": "ICLR.cc/2020/Conference/Paper16/-/Public_Comment", "content": {"title": "Request for Code Release", "comment": "Thank you for your excellent work. I would like to reproduce the results. Would you consider releasing the code of your method SELF for easy reproduce?"}, "signatures": ["~Pengfei_Chen1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Pengfei_Chen1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ductam.nguyen08@gmail.com", "chaithanyakumar.mummadi@de.bosch.com", "thiphuongnhung.ngo@de.bosch.com", "hoai.phuong.nguyen198@gmail.com", "laura.beggel@de.bosch.com", "brox@cs.uni-freiburg.de"], "title": "SELF: Learning to Filter Noisy Labels with Self-Ensembling", "authors": ["Duc Tam Nguyen", "Chaithanya Kumar Mummadi", "Thi Phuong Nhung Ngo", "Thi Hoai Phuong Nguyen", "Laura Beggel", "Thomas Brox"], "pdf": "/pdf/3bafa3876cfccbe25d2fc5c4db5cd495aae2e6c7.pdf", "TL;DR": "We propose a self-ensemble framework to train more robust deep learning models under noisy labeled datasets.", "abstract": "Deep neural networks (DNNs) have been shown to over-fit a dataset when being trained with noisy labels for a long enough time. To overcome this problem, we present a simple and effective method self-ensemble label filtering (SELF) to progressively filter out the wrong labels during training. Our method improves the task performance by gradually allowing supervision only from the potentially non-noisy (clean) labels and stops learning on the filtered noisy labels. For the filtering, we form running averages of predictions over the entire training dataset using the network output at different training epochs. We show that these ensemble estimates yield more accurate identification of inconsistent predictions throughout training than the single estimates of the network at the most recent training epoch. While filtered samples are removed entirely from the supervised training loss, we dynamically leverage them via semi-supervised learning in the unsupervised loss. We demonstrate the positive effect of such an approach on various image classification tasks under both symmetric and asymmetric label noise and at different noise ratios. It substantially outperforms all previous works on noise-aware learning across different datasets and can be applied to a broad set of network architectures.", "keywords": ["Ensemble Learning", "Robust Learning", "Noisy Labels", "Labels Filtering"], "paperhash": "nguyen|self_learning_to_filter_noisy_labels_with_selfensembling", "_bibtex": "@inproceedings{\nNguyen2020SELF:,\ntitle={SELF: Learning to Filter Noisy Labels with Self-Ensembling},\nauthor={Duc Tam Nguyen and Chaithanya Kumar Mummadi and Thi Phuong Nhung Ngo and Thi Hoai Phuong Nguyen and Laura Beggel and Thomas Brox},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HkgsPhNYPS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/f53c1264354a72df6a3fdcfe8a3344abd4fde7f2.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HkgsPhNYPS", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504215132, "tmdate": 1576860562323, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper16/Authors", "ICLR.cc/2020/Conference/Paper16/Reviewers", "ICLR.cc/2020/Conference/Paper16/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper16/-/Public_Comment"}}}, {"id": "HkgsPhNYPS", "original": "BJe3rTSqrr", "number": 16, "cdate": 1569438818815, "ddate": null, "tcdate": 1569438818815, "tmdate": 1583912047675, "tddate": null, "forum": "HkgsPhNYPS", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["ductam.nguyen08@gmail.com", "chaithanyakumar.mummadi@de.bosch.com", "thiphuongnhung.ngo@de.bosch.com", "hoai.phuong.nguyen198@gmail.com", "laura.beggel@de.bosch.com", "brox@cs.uni-freiburg.de"], "title": "SELF: Learning to Filter Noisy Labels with Self-Ensembling", "authors": ["Duc Tam Nguyen", "Chaithanya Kumar Mummadi", "Thi Phuong Nhung Ngo", "Thi Hoai Phuong Nguyen", "Laura Beggel", "Thomas Brox"], "pdf": "/pdf/3bafa3876cfccbe25d2fc5c4db5cd495aae2e6c7.pdf", "TL;DR": "We propose a self-ensemble framework to train more robust deep learning models under noisy labeled datasets.", "abstract": "Deep neural networks (DNNs) have been shown to over-fit a dataset when being trained with noisy labels for a long enough time. To overcome this problem, we present a simple and effective method self-ensemble label filtering (SELF) to progressively filter out the wrong labels during training. Our method improves the task performance by gradually allowing supervision only from the potentially non-noisy (clean) labels and stops learning on the filtered noisy labels. For the filtering, we form running averages of predictions over the entire training dataset using the network output at different training epochs. We show that these ensemble estimates yield more accurate identification of inconsistent predictions throughout training than the single estimates of the network at the most recent training epoch. While filtered samples are removed entirely from the supervised training loss, we dynamically leverage them via semi-supervised learning in the unsupervised loss. We demonstrate the positive effect of such an approach on various image classification tasks under both symmetric and asymmetric label noise and at different noise ratios. It substantially outperforms all previous works on noise-aware learning across different datasets and can be applied to a broad set of network architectures.", "keywords": ["Ensemble Learning", "Robust Learning", "Noisy Labels", "Labels Filtering"], "paperhash": "nguyen|self_learning_to_filter_noisy_labels_with_selfensembling", "_bibtex": "@inproceedings{\nNguyen2020SELF:,\ntitle={SELF: Learning to Filter Noisy Labels with Self-Ensembling},\nauthor={Duc Tam Nguyen and Chaithanya Kumar Mummadi and Thi Phuong Nhung Ngo and Thi Hoai Phuong Nguyen and Laura Beggel and Thomas Brox},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HkgsPhNYPS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/f53c1264354a72df6a3fdcfe8a3344abd4fde7f2.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 13, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "ryZbbeOxeH", "original": null, "number": 1, "cdate": 1576798685021, "ddate": null, "tcdate": 1576798685021, "tmdate": 1576800949859, "tddate": null, "forum": "HkgsPhNYPS", "replyto": "HkgsPhNYPS", "invitation": "ICLR.cc/2020/Conference/Paper16/-/Decision", "content": {"decision": "Accept (Poster)", "comment": "The authors addressed the issues raised by the reviewers; I suggest to accept this paper.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ductam.nguyen08@gmail.com", "chaithanyakumar.mummadi@de.bosch.com", "thiphuongnhung.ngo@de.bosch.com", "hoai.phuong.nguyen198@gmail.com", "laura.beggel@de.bosch.com", "brox@cs.uni-freiburg.de"], "title": "SELF: Learning to Filter Noisy Labels with Self-Ensembling", "authors": ["Duc Tam Nguyen", "Chaithanya Kumar Mummadi", "Thi Phuong Nhung Ngo", "Thi Hoai Phuong Nguyen", "Laura Beggel", "Thomas Brox"], "pdf": "/pdf/3bafa3876cfccbe25d2fc5c4db5cd495aae2e6c7.pdf", "TL;DR": "We propose a self-ensemble framework to train more robust deep learning models under noisy labeled datasets.", "abstract": "Deep neural networks (DNNs) have been shown to over-fit a dataset when being trained with noisy labels for a long enough time. To overcome this problem, we present a simple and effective method self-ensemble label filtering (SELF) to progressively filter out the wrong labels during training. Our method improves the task performance by gradually allowing supervision only from the potentially non-noisy (clean) labels and stops learning on the filtered noisy labels. For the filtering, we form running averages of predictions over the entire training dataset using the network output at different training epochs. We show that these ensemble estimates yield more accurate identification of inconsistent predictions throughout training than the single estimates of the network at the most recent training epoch. While filtered samples are removed entirely from the supervised training loss, we dynamically leverage them via semi-supervised learning in the unsupervised loss. We demonstrate the positive effect of such an approach on various image classification tasks under both symmetric and asymmetric label noise and at different noise ratios. It substantially outperforms all previous works on noise-aware learning across different datasets and can be applied to a broad set of network architectures.", "keywords": ["Ensemble Learning", "Robust Learning", "Noisy Labels", "Labels Filtering"], "paperhash": "nguyen|self_learning_to_filter_noisy_labels_with_selfensembling", "_bibtex": "@inproceedings{\nNguyen2020SELF:,\ntitle={SELF: Learning to Filter Noisy Labels with Self-Ensembling},\nauthor={Duc Tam Nguyen and Chaithanya Kumar Mummadi and Thi Phuong Nhung Ngo and Thi Hoai Phuong Nguyen and Laura Beggel and Thomas Brox},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HkgsPhNYPS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/f53c1264354a72df6a3fdcfe8a3344abd4fde7f2.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "HkgsPhNYPS", "replyto": "HkgsPhNYPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795708154, "tmdate": 1576800256504, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper16/-/Decision"}}}, {"id": "HklMHtHhjB", "original": null, "number": 7, "cdate": 1573833017763, "ddate": null, "tcdate": 1573833017763, "tmdate": 1573833017763, "tddate": null, "forum": "HkgsPhNYPS", "replyto": "BkxwUHrnsS", "invitation": "ICLR.cc/2020/Conference/Paper16/-/Official_Comment", "content": {"title": "Clarification", "comment": "Thank you for asking!\n\nWe implement JointOpt (Tanaka et al., 2018) based on the official implementation.\nFor SL (Wang et al., 2019) , D2L (Ma et al., 2018), we adopt the performance from the respective publication.\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper16/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper16/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ductam.nguyen08@gmail.com", "chaithanyakumar.mummadi@de.bosch.com", "thiphuongnhung.ngo@de.bosch.com", "hoai.phuong.nguyen198@gmail.com", "laura.beggel@de.bosch.com", "brox@cs.uni-freiburg.de"], "title": "SELF: Learning to Filter Noisy Labels with Self-Ensembling", "authors": ["Duc Tam Nguyen", "Chaithanya Kumar Mummadi", "Thi Phuong Nhung Ngo", "Thi Hoai Phuong Nguyen", "Laura Beggel", "Thomas Brox"], "pdf": "/pdf/3bafa3876cfccbe25d2fc5c4db5cd495aae2e6c7.pdf", "TL;DR": "We propose a self-ensemble framework to train more robust deep learning models under noisy labeled datasets.", "abstract": "Deep neural networks (DNNs) have been shown to over-fit a dataset when being trained with noisy labels for a long enough time. To overcome this problem, we present a simple and effective method self-ensemble label filtering (SELF) to progressively filter out the wrong labels during training. Our method improves the task performance by gradually allowing supervision only from the potentially non-noisy (clean) labels and stops learning on the filtered noisy labels. For the filtering, we form running averages of predictions over the entire training dataset using the network output at different training epochs. We show that these ensemble estimates yield more accurate identification of inconsistent predictions throughout training than the single estimates of the network at the most recent training epoch. While filtered samples are removed entirely from the supervised training loss, we dynamically leverage them via semi-supervised learning in the unsupervised loss. We demonstrate the positive effect of such an approach on various image classification tasks under both symmetric and asymmetric label noise and at different noise ratios. It substantially outperforms all previous works on noise-aware learning across different datasets and can be applied to a broad set of network architectures.", "keywords": ["Ensemble Learning", "Robust Learning", "Noisy Labels", "Labels Filtering"], "paperhash": "nguyen|self_learning_to_filter_noisy_labels_with_selfensembling", "_bibtex": "@inproceedings{\nNguyen2020SELF:,\ntitle={SELF: Learning to Filter Noisy Labels with Self-Ensembling},\nauthor={Duc Tam Nguyen and Chaithanya Kumar Mummadi and Thi Phuong Nhung Ngo and Thi Hoai Phuong Nguyen and Laura Beggel and Thomas Brox},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HkgsPhNYPS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/f53c1264354a72df6a3fdcfe8a3344abd4fde7f2.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HkgsPhNYPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper16/Authors", "ICLR.cc/2020/Conference/Paper16/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper16/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper16/Reviewers", "ICLR.cc/2020/Conference/Paper16/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper16/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper16/Authors|ICLR.cc/2020/Conference/Paper16/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504177650, "tmdate": 1576860528453, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper16/Authors", "ICLR.cc/2020/Conference/Paper16/Reviewers", "ICLR.cc/2020/Conference/Paper16/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper16/-/Official_Comment"}}}, {"id": "BkxwUHrnsS", "original": null, "number": 6, "cdate": 1573832015300, "ddate": null, "tcdate": 1573832015300, "tmdate": 1573832015300, "tddate": null, "forum": "HkgsPhNYPS", "replyto": "rkeySME3iB", "invitation": "ICLR.cc/2020/Conference/Paper16/-/Official_Comment", "content": {"title": "About the results in Table 1 & 2...", "comment": "Are the results in Tables 1 & 2 in the manner copy and paste for baselines? "}, "signatures": ["ICLR.cc/2020/Conference/Paper16/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper16/AnonReviewer3", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ductam.nguyen08@gmail.com", "chaithanyakumar.mummadi@de.bosch.com", "thiphuongnhung.ngo@de.bosch.com", "hoai.phuong.nguyen198@gmail.com", "laura.beggel@de.bosch.com", "brox@cs.uni-freiburg.de"], "title": "SELF: Learning to Filter Noisy Labels with Self-Ensembling", "authors": ["Duc Tam Nguyen", "Chaithanya Kumar Mummadi", "Thi Phuong Nhung Ngo", "Thi Hoai Phuong Nguyen", "Laura Beggel", "Thomas Brox"], "pdf": "/pdf/3bafa3876cfccbe25d2fc5c4db5cd495aae2e6c7.pdf", "TL;DR": "We propose a self-ensemble framework to train more robust deep learning models under noisy labeled datasets.", "abstract": "Deep neural networks (DNNs) have been shown to over-fit a dataset when being trained with noisy labels for a long enough time. To overcome this problem, we present a simple and effective method self-ensemble label filtering (SELF) to progressively filter out the wrong labels during training. Our method improves the task performance by gradually allowing supervision only from the potentially non-noisy (clean) labels and stops learning on the filtered noisy labels. For the filtering, we form running averages of predictions over the entire training dataset using the network output at different training epochs. We show that these ensemble estimates yield more accurate identification of inconsistent predictions throughout training than the single estimates of the network at the most recent training epoch. While filtered samples are removed entirely from the supervised training loss, we dynamically leverage them via semi-supervised learning in the unsupervised loss. We demonstrate the positive effect of such an approach on various image classification tasks under both symmetric and asymmetric label noise and at different noise ratios. It substantially outperforms all previous works on noise-aware learning across different datasets and can be applied to a broad set of network architectures.", "keywords": ["Ensemble Learning", "Robust Learning", "Noisy Labels", "Labels Filtering"], "paperhash": "nguyen|self_learning_to_filter_noisy_labels_with_selfensembling", "_bibtex": "@inproceedings{\nNguyen2020SELF:,\ntitle={SELF: Learning to Filter Noisy Labels with Self-Ensembling},\nauthor={Duc Tam Nguyen and Chaithanya Kumar Mummadi and Thi Phuong Nhung Ngo and Thi Hoai Phuong Nguyen and Laura Beggel and Thomas Brox},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HkgsPhNYPS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/f53c1264354a72df6a3fdcfe8a3344abd4fde7f2.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HkgsPhNYPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper16/Authors", "ICLR.cc/2020/Conference/Paper16/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper16/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper16/Reviewers", "ICLR.cc/2020/Conference/Paper16/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper16/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper16/Authors|ICLR.cc/2020/Conference/Paper16/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504177650, "tmdate": 1576860528453, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper16/Authors", "ICLR.cc/2020/Conference/Paper16/Reviewers", "ICLR.cc/2020/Conference/Paper16/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper16/-/Official_Comment"}}}, {"id": "Sked0MN3sS", "original": null, "number": 4, "cdate": 1573827279969, "ddate": null, "tcdate": 1573827279969, "tmdate": 1573830060993, "tddate": null, "forum": "HkgsPhNYPS", "replyto": "SyeJgpOAtS", "invitation": "ICLR.cc/2020/Conference/Paper16/-/Official_Comment", "content": {"title": "Thank you for providing fascinating perspective on the proposed framework SELF!", "comment": "--- Summary ---\nThank you for providing interesting perspectives and initiates precious discussions on the proposed method SELF. The updated supplementary materials include the question of required assumptions, failure cases, the model convergence. Additionally, the contributions are made more explicit in the Introduction. \n\n--- Major comments ---\n1.1. More explicit contributions: \nThank you for this hint! We completely agree and have modified the description of contributions.\n\n1.2. Is the sample filtering technique novel:\nCorrect, we are the first to employ the disagreement with the model predictions to filter samples progressively. \n\n1.3. Is the self-ensembling the primary contribution: \nYes, besides the progressive refinement of the set of correct samples. \n\n2.1. Required assumptions: Exciting question! \n\n- Natural robustness assumption of deep networks (Rolnick et al. 2017): The networks attempt to learn the easiest way to explain most of the data. SELF uses this assumption to kickstart the process.\n\n- Correct samples dominate over wrongly labeled samples: At 80% noise on CIFAR-10, the correctly labeled cats (20% out of all cat images) still dominates over samples wrongly labeled as cat (8.888% for each class). \n\n- Independence results in less overfitting: SELF performs best if the noises on the validation set and training set are independent and identically distributed. SELF uses the validation data for early stopping. Hence, a high correlation of label noise between train and valid increases the chance of model overfitting.\n\n- Sufficient label noise randomness assumption: see below (2.2)\n\n2.2. What does sufficiently random label noise mean:\n\n- Sufficient label noise randomness assumption: Each class consists of multiple clusters. If one distinct cluster is entirely covered by label noise, SELF will struggle to expand the set of correct samples to cover this cluster.\n\n- SELF performs progressive expansion of the correct labels sets. At larger noise ratios, not all clusters are covered by the identified correct samples. Therefore on task containing many classes, e.g., CIFAR-100, the model performance decreases faster than on CIFAR-10.\n\n2.3. Failure cases: \nWhen the assumptions from 2.1. are strongly violated. Each assumption should have its own \"critical\" threshold for violation. A future in-depth analysis to challenge the assumptions is fascinating.\n\n2.4. Labels noise only in a particular region or depend on the features:\n\n- The asymmetric noise on CIFAR-10 closely resembles this scenario. Concretely, the class plane might be randomly flipped to the class ship. Most samples should share similar features within the class plane. \n\n- Adding label noise to more fine-grained samples regions is interesting. However, coherent regions in a learned feature space might depend strongly on the network architectures used to learn on the clean datasets. Hence, the simulation or acquisition of these noisy types is not trivial.\n\n2.5. Only tested on label-dependent noise: \nUniform (symmetric) label noise is label-independent. Sample from each class is randomly flipped uniformly to all classes. \n\n3.1. Convergence of SELF: We have not observed any model divergence in the experiments.\n\n- The natural robustness assumption (2.1) needs to hold to kickstart the process. In SELF, the model learns the easiest way to explain most of the data. \n\n- As the progressive filtering proceeds, the noise ratio is slowly reduced. So the learning task becomes significantly easier. The always-on unsupervised objective compensates for the small sample size induced by filtering on all samples.\n\n3.2. Guarantee for oscillation of wrong labels: \nWe argue that no guarantee exists for the oscillation of all wrong samples. Some samples might resemble the wrong class more than their actual class. Depending on the perspective, the tiger might look more like a cat than other tigers.\n\n3.3. Is the new objective function well-specified: \nThe learning objective in SELF adds to the standard loss two aspects: the model remains close to its self-ensemble and progressive filtering of noisy labels. The self-ensemble learning is well-defined. The progressive filtering is also robust and do not add any instabilities to the training process. \n\n--- Minor comments ---\n- Fixed or will be fixed in the final version.\n- Front-load the justification for using self-ensembling: Great hint! Updated the Introduction accordingly.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper16/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper16/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ductam.nguyen08@gmail.com", "chaithanyakumar.mummadi@de.bosch.com", "thiphuongnhung.ngo@de.bosch.com", "hoai.phuong.nguyen198@gmail.com", "laura.beggel@de.bosch.com", "brox@cs.uni-freiburg.de"], "title": "SELF: Learning to Filter Noisy Labels with Self-Ensembling", "authors": ["Duc Tam Nguyen", "Chaithanya Kumar Mummadi", "Thi Phuong Nhung Ngo", "Thi Hoai Phuong Nguyen", "Laura Beggel", "Thomas Brox"], "pdf": "/pdf/3bafa3876cfccbe25d2fc5c4db5cd495aae2e6c7.pdf", "TL;DR": "We propose a self-ensemble framework to train more robust deep learning models under noisy labeled datasets.", "abstract": "Deep neural networks (DNNs) have been shown to over-fit a dataset when being trained with noisy labels for a long enough time. To overcome this problem, we present a simple and effective method self-ensemble label filtering (SELF) to progressively filter out the wrong labels during training. Our method improves the task performance by gradually allowing supervision only from the potentially non-noisy (clean) labels and stops learning on the filtered noisy labels. For the filtering, we form running averages of predictions over the entire training dataset using the network output at different training epochs. We show that these ensemble estimates yield more accurate identification of inconsistent predictions throughout training than the single estimates of the network at the most recent training epoch. While filtered samples are removed entirely from the supervised training loss, we dynamically leverage them via semi-supervised learning in the unsupervised loss. We demonstrate the positive effect of such an approach on various image classification tasks under both symmetric and asymmetric label noise and at different noise ratios. It substantially outperforms all previous works on noise-aware learning across different datasets and can be applied to a broad set of network architectures.", "keywords": ["Ensemble Learning", "Robust Learning", "Noisy Labels", "Labels Filtering"], "paperhash": "nguyen|self_learning_to_filter_noisy_labels_with_selfensembling", "_bibtex": "@inproceedings{\nNguyen2020SELF:,\ntitle={SELF: Learning to Filter Noisy Labels with Self-Ensembling},\nauthor={Duc Tam Nguyen and Chaithanya Kumar Mummadi and Thi Phuong Nhung Ngo and Thi Hoai Phuong Nguyen and Laura Beggel and Thomas Brox},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HkgsPhNYPS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/f53c1264354a72df6a3fdcfe8a3344abd4fde7f2.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HkgsPhNYPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper16/Authors", "ICLR.cc/2020/Conference/Paper16/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper16/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper16/Reviewers", "ICLR.cc/2020/Conference/Paper16/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper16/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper16/Authors|ICLR.cc/2020/Conference/Paper16/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504177650, "tmdate": 1576860528453, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper16/Authors", "ICLR.cc/2020/Conference/Paper16/Reviewers", "ICLR.cc/2020/Conference/Paper16/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper16/-/Official_Comment"}}}, {"id": "BJejYQ4hiH", "original": null, "number": 5, "cdate": 1573827458691, "ddate": null, "tcdate": 1573827458691, "tmdate": 1573829930775, "tddate": null, "forum": "HkgsPhNYPS", "replyto": "SylY2HN4tr", "invitation": "ICLR.cc/2020/Conference/Paper16/-/Official_Comment", "content": {"title": "Thank you for the precious feedback!", "comment": "--- Summary-----\nThank you for providing invaluable feedback. The introduction is updated to highlight the contributions more explicit. The updated Tab. 8 (Appendix) includes semi-supervised learning experiments when combined with recent works. Your questions about other semi-supervised techniques and the validation procedure have led to precious insights: see below (2 and 3). \n\n--- Detailed -----\n\n1. Contributions: \n    \n- As you have acknowledged - the primary contribution is progressive filtering, i.e., form a consensus of predictions (self-ensemble predictions) of the network over different training iterations. The analysis shows that such consensus yield better estimates to progressively filter out the noisy labels. \n    \n- The key idea of SELF relies on the self-ensemble predictions of a model. Therefore, it is essential to incorporate a model with stable training behavior to obtain better estimates from the consensus. Concretely, we employ a semi-supervised technique as a backbone to SELF, which provides a stable ground for SELF to retain better consensus throughout learning. Note that this is different from just a mere combination of semi-supervised techniques with a sample selection method. \n    \n- The updated Tab. 8 (Appendix) includes additional comparisons in the submission to demonstrate the significance of the SELF-framework by comparing the results against previous works + semi-supervised techniques. The semi-supervised learning provides a robust model consensus to filter out the noisy labels. SELF benefits from this semi-supervised learning more than other techniques (see below).\n    \n2.1. Add semi-supervised learning to baselines:\n\nThanks for suggesting this comparison! These results have provided additional insights into the SELF-framework. In detail, the updated Tab. 8 (Appendix) shows the in-deep analysis of semi-supervised learning strategies combined with recent works. Our progressive filtering approach is shown to be effective and performs well regardless of the choice of the semi-supervised technique backbone. Overall, the proposed framework SELF outperforms all these combinations.\n\n2.2. Other semi-supervised regularization can improve supervised learning:\n\nAgreed! However, the focus of this work is to propose self-ensembling as a principled approach to counter label noise. Note that the semi-supervised technique (like Mean-Teacher) is a backbone for our principle. The combination with better semi-supervised techniques is indeed an excellent future direction. \n\n3. Backward-loss correction is a unique way to estimate classification risk: \n\n- We observe that the risk estimation in SELF is far from optimal. Results from Tab. 1 shows that SELF relying on noisy validation set performs much worse than SELF*  (up to 10% drop in performance at 80% label noise on CIFAR-10). Here, SELF* uses a clean validation set.\n\n- This current work follows the common practice of using such noisy validation data under the literature of learning from noisy labels. A more in-depth analysis of this aspect is worth further investigation."}, "signatures": ["ICLR.cc/2020/Conference/Paper16/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper16/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ductam.nguyen08@gmail.com", "chaithanyakumar.mummadi@de.bosch.com", "thiphuongnhung.ngo@de.bosch.com", "hoai.phuong.nguyen198@gmail.com", "laura.beggel@de.bosch.com", "brox@cs.uni-freiburg.de"], "title": "SELF: Learning to Filter Noisy Labels with Self-Ensembling", "authors": ["Duc Tam Nguyen", "Chaithanya Kumar Mummadi", "Thi Phuong Nhung Ngo", "Thi Hoai Phuong Nguyen", "Laura Beggel", "Thomas Brox"], "pdf": "/pdf/3bafa3876cfccbe25d2fc5c4db5cd495aae2e6c7.pdf", "TL;DR": "We propose a self-ensemble framework to train more robust deep learning models under noisy labeled datasets.", "abstract": "Deep neural networks (DNNs) have been shown to over-fit a dataset when being trained with noisy labels for a long enough time. To overcome this problem, we present a simple and effective method self-ensemble label filtering (SELF) to progressively filter out the wrong labels during training. Our method improves the task performance by gradually allowing supervision only from the potentially non-noisy (clean) labels and stops learning on the filtered noisy labels. For the filtering, we form running averages of predictions over the entire training dataset using the network output at different training epochs. We show that these ensemble estimates yield more accurate identification of inconsistent predictions throughout training than the single estimates of the network at the most recent training epoch. While filtered samples are removed entirely from the supervised training loss, we dynamically leverage them via semi-supervised learning in the unsupervised loss. We demonstrate the positive effect of such an approach on various image classification tasks under both symmetric and asymmetric label noise and at different noise ratios. It substantially outperforms all previous works on noise-aware learning across different datasets and can be applied to a broad set of network architectures.", "keywords": ["Ensemble Learning", "Robust Learning", "Noisy Labels", "Labels Filtering"], "paperhash": "nguyen|self_learning_to_filter_noisy_labels_with_selfensembling", "_bibtex": "@inproceedings{\nNguyen2020SELF:,\ntitle={SELF: Learning to Filter Noisy Labels with Self-Ensembling},\nauthor={Duc Tam Nguyen and Chaithanya Kumar Mummadi and Thi Phuong Nhung Ngo and Thi Hoai Phuong Nguyen and Laura Beggel and Thomas Brox},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HkgsPhNYPS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/f53c1264354a72df6a3fdcfe8a3344abd4fde7f2.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HkgsPhNYPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper16/Authors", "ICLR.cc/2020/Conference/Paper16/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper16/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper16/Reviewers", "ICLR.cc/2020/Conference/Paper16/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper16/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper16/Authors|ICLR.cc/2020/Conference/Paper16/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504177650, "tmdate": 1576860528453, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper16/Authors", "ICLR.cc/2020/Conference/Paper16/Reviewers", "ICLR.cc/2020/Conference/Paper16/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper16/-/Official_Comment"}}}, {"id": "rkeySME3iB", "original": null, "number": 3, "cdate": 1573827126718, "ddate": null, "tcdate": 1573827126718, "tmdate": 1573829231275, "tddate": null, "forum": "HkgsPhNYPS", "replyto": "rkek1NdX5r", "invitation": "ICLR.cc/2020/Conference/Paper16/-/Official_Comment", "content": {"title": "Thank you for your suggestions! The additional results provide valuable insights into our method SELF.", "comment": "Thank you for your great feedback. The contributions are now more explicit. The updated Tab. 8 (Appendix) contains semi-supervised learning experiments in combination with recent works. Thank you for your suggestions! The additional results provide valuable insights into our method SELF.\n\n1. Clarified contributions:\n\n- The method SELF is a simple but effective mechanism to filter out noisy labels during training and exclude wrong labels from supervision. Prior works on semi-supervised learning (Laine and Aila, 2016; Luo et al., 2018) suffer from the influence of label noise on the supervised loss and thus influence the task performance.\n\n- To the best of our knowledge, we are the first to identify and propose self-ensembling as a principled technique to learn robustly. Other self-ensembling techniques, such as multiple checkpoints or model restarts, are also applicable. Self-ensembling works because the predictions of wrongly labeled samples often oscillate over training iterations.\n\n- As you have correctly acknowledged - the primary contribution is progressive filtering, i.e., form a consensus of predictions (self-ensemble predictions) of the network over different training iterations. The analysis shows that such consensus yield better estimates to progressively filter out the noisy labels. \n\n- The key idea of SELF relies on the self-ensemble predictions of a model. Therefore, it is essential to incorporate a model with stable training behavior to obtain better estimates from the consensus. Concretely, we employ a semi-supervised technique as a backbone to SELF, which provides a stable ground for SELF to retain better consensus throughout learning. The updated in-depth analysis of semi-supervised learning confirms this effect. More follows in 4. Note that this is different from just a mere combination of semi-supervised techniques with a sample selection method. \n\n2. Add ideal accuracy on the clean test set: \nGreat suggestion! The updated Tab. 3 include the baseline accuracy of networks that are trained in a fully supervised setting at 0% label noise. All performances are reported on the clean test set.\n\n3. Reorganize tables and figures: \nThe tables and figures are rearranged to improve readability.\n\n4. Add semi-supervised learning to baselines: \nThanks for suggesting this comparison! These results have provided additional insights into the SELF-framework. In detail, the updated Tab. 8 (Appendix) shows the in-deep analysis of semi-supervised learning strategies combined with recent works. Our progressive filtering approach is shown to be effective and performs well regardless of the choice of the semi-supervised technique backbone. Overall, the proposed framework SELF outperforms all these combinations. \n\n5. More baselines: \nThanks for making us aware of these highly relevant previous works: SL (Wang et al., 2019), JointOpt (Tanaka et al., 2018), D2L (Ma et al., 2018). The updated Tab. 1 and Tab. 2 include these baselines. The  SELF-framework consistently outperforms these methods. "}, "signatures": ["ICLR.cc/2020/Conference/Paper16/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper16/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ductam.nguyen08@gmail.com", "chaithanyakumar.mummadi@de.bosch.com", "thiphuongnhung.ngo@de.bosch.com", "hoai.phuong.nguyen198@gmail.com", "laura.beggel@de.bosch.com", "brox@cs.uni-freiburg.de"], "title": "SELF: Learning to Filter Noisy Labels with Self-Ensembling", "authors": ["Duc Tam Nguyen", "Chaithanya Kumar Mummadi", "Thi Phuong Nhung Ngo", "Thi Hoai Phuong Nguyen", "Laura Beggel", "Thomas Brox"], "pdf": "/pdf/3bafa3876cfccbe25d2fc5c4db5cd495aae2e6c7.pdf", "TL;DR": "We propose a self-ensemble framework to train more robust deep learning models under noisy labeled datasets.", "abstract": "Deep neural networks (DNNs) have been shown to over-fit a dataset when being trained with noisy labels for a long enough time. To overcome this problem, we present a simple and effective method self-ensemble label filtering (SELF) to progressively filter out the wrong labels during training. Our method improves the task performance by gradually allowing supervision only from the potentially non-noisy (clean) labels and stops learning on the filtered noisy labels. For the filtering, we form running averages of predictions over the entire training dataset using the network output at different training epochs. We show that these ensemble estimates yield more accurate identification of inconsistent predictions throughout training than the single estimates of the network at the most recent training epoch. While filtered samples are removed entirely from the supervised training loss, we dynamically leverage them via semi-supervised learning in the unsupervised loss. We demonstrate the positive effect of such an approach on various image classification tasks under both symmetric and asymmetric label noise and at different noise ratios. It substantially outperforms all previous works on noise-aware learning across different datasets and can be applied to a broad set of network architectures.", "keywords": ["Ensemble Learning", "Robust Learning", "Noisy Labels", "Labels Filtering"], "paperhash": "nguyen|self_learning_to_filter_noisy_labels_with_selfensembling", "_bibtex": "@inproceedings{\nNguyen2020SELF:,\ntitle={SELF: Learning to Filter Noisy Labels with Self-Ensembling},\nauthor={Duc Tam Nguyen and Chaithanya Kumar Mummadi and Thi Phuong Nhung Ngo and Thi Hoai Phuong Nguyen and Laura Beggel and Thomas Brox},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HkgsPhNYPS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/f53c1264354a72df6a3fdcfe8a3344abd4fde7f2.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HkgsPhNYPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper16/Authors", "ICLR.cc/2020/Conference/Paper16/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper16/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper16/Reviewers", "ICLR.cc/2020/Conference/Paper16/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper16/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper16/Authors|ICLR.cc/2020/Conference/Paper16/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504177650, "tmdate": 1576860528453, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper16/Authors", "ICLR.cc/2020/Conference/Paper16/Reviewers", "ICLR.cc/2020/Conference/Paper16/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper16/-/Official_Comment"}}}, {"id": "S1lUabE3oH", "original": null, "number": 2, "cdate": 1573827006398, "ddate": null, "tcdate": 1573827006398, "tmdate": 1573828563127, "tddate": null, "forum": "HkgsPhNYPS", "replyto": "BJguQ66MsH", "invitation": "ICLR.cc/2020/Conference/Paper16/-/Official_Comment", "content": {"title": "We observe that our model always converges, even at extreme noise ratios.", "comment": "- The most critical step is the first training step on the initial label set. \n\n- Due to the natural robustness of deep networks [Rolnick et al. (2017)], the model can still be improved by training and hence do not diverge. The intuition is that deep networks attempt to learn the \"easiest\" patterns. For instance, at 80% noise on CIFAR-10, the correctly labeled cats (20% out of all cat images) still dominates over samples wrongly labeled as cat ($8.\\bar{8}$ $\\%$ for each class). In this cat example, the model learns the features to explain the largest numbers of samples. Hence, the model learns important cat features to explain 20% of the samples.\n\n- As the progressive filtering proceeds, the noise ratio is slowly reduced.  The learning on the refined label set is easier for the model. The unsupervised objective compensates for the reduction of the data set size.  Consequently, the progressive filtering help model convergence.\n\n- Empirically, we observe that our model always converges, even at extreme noise ratios.\n \n- Note that 90% noise on CIFAR-10 corresponds to completely random labels. There exists no model which is better than random. Consequently, the critical limit on CIFAR-100 is at a 99% noise ratio (uniform noise)."}, "signatures": ["ICLR.cc/2020/Conference/Paper16/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper16/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ductam.nguyen08@gmail.com", "chaithanyakumar.mummadi@de.bosch.com", "thiphuongnhung.ngo@de.bosch.com", "hoai.phuong.nguyen198@gmail.com", "laura.beggel@de.bosch.com", "brox@cs.uni-freiburg.de"], "title": "SELF: Learning to Filter Noisy Labels with Self-Ensembling", "authors": ["Duc Tam Nguyen", "Chaithanya Kumar Mummadi", "Thi Phuong Nhung Ngo", "Thi Hoai Phuong Nguyen", "Laura Beggel", "Thomas Brox"], "pdf": "/pdf/3bafa3876cfccbe25d2fc5c4db5cd495aae2e6c7.pdf", "TL;DR": "We propose a self-ensemble framework to train more robust deep learning models under noisy labeled datasets.", "abstract": "Deep neural networks (DNNs) have been shown to over-fit a dataset when being trained with noisy labels for a long enough time. To overcome this problem, we present a simple and effective method self-ensemble label filtering (SELF) to progressively filter out the wrong labels during training. Our method improves the task performance by gradually allowing supervision only from the potentially non-noisy (clean) labels and stops learning on the filtered noisy labels. For the filtering, we form running averages of predictions over the entire training dataset using the network output at different training epochs. We show that these ensemble estimates yield more accurate identification of inconsistent predictions throughout training than the single estimates of the network at the most recent training epoch. While filtered samples are removed entirely from the supervised training loss, we dynamically leverage them via semi-supervised learning in the unsupervised loss. We demonstrate the positive effect of such an approach on various image classification tasks under both symmetric and asymmetric label noise and at different noise ratios. It substantially outperforms all previous works on noise-aware learning across different datasets and can be applied to a broad set of network architectures.", "keywords": ["Ensemble Learning", "Robust Learning", "Noisy Labels", "Labels Filtering"], "paperhash": "nguyen|self_learning_to_filter_noisy_labels_with_selfensembling", "_bibtex": "@inproceedings{\nNguyen2020SELF:,\ntitle={SELF: Learning to Filter Noisy Labels with Self-Ensembling},\nauthor={Duc Tam Nguyen and Chaithanya Kumar Mummadi and Thi Phuong Nhung Ngo and Thi Hoai Phuong Nguyen and Laura Beggel and Thomas Brox},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HkgsPhNYPS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/f53c1264354a72df6a3fdcfe8a3344abd4fde7f2.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HkgsPhNYPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper16/Authors", "ICLR.cc/2020/Conference/Paper16/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper16/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper16/Reviewers", "ICLR.cc/2020/Conference/Paper16/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper16/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper16/Authors|ICLR.cc/2020/Conference/Paper16/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504177650, "tmdate": 1576860528453, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper16/Authors", "ICLR.cc/2020/Conference/Paper16/Reviewers", "ICLR.cc/2020/Conference/Paper16/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper16/-/Official_Comment"}}}, {"id": "BJguQ66MsH", "original": null, "number": 1, "cdate": 1573211424044, "ddate": null, "tcdate": 1573211424044, "tmdate": 1573211424044, "tddate": null, "forum": "HkgsPhNYPS", "replyto": "HkgsPhNYPS", "invitation": "ICLR.cc/2020/Conference/Paper16/-/Public_Comment", "content": {"title": "Convergence issues", "comment": "The ensemble predictions given by the teacher model can also be wrong, thus the selected images can be noisy. At extreme noise levels, say 90%, the model is not guaranteed to converge. It may diverge since most of the predictions are inconsistent and the selected images are few and noisy. What do you think about it?"}, "signatures": ["~ning_mou1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~ning_mou1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ductam.nguyen08@gmail.com", "chaithanyakumar.mummadi@de.bosch.com", "thiphuongnhung.ngo@de.bosch.com", "hoai.phuong.nguyen198@gmail.com", "laura.beggel@de.bosch.com", "brox@cs.uni-freiburg.de"], "title": "SELF: Learning to Filter Noisy Labels with Self-Ensembling", "authors": ["Duc Tam Nguyen", "Chaithanya Kumar Mummadi", "Thi Phuong Nhung Ngo", "Thi Hoai Phuong Nguyen", "Laura Beggel", "Thomas Brox"], "pdf": "/pdf/3bafa3876cfccbe25d2fc5c4db5cd495aae2e6c7.pdf", "TL;DR": "We propose a self-ensemble framework to train more robust deep learning models under noisy labeled datasets.", "abstract": "Deep neural networks (DNNs) have been shown to over-fit a dataset when being trained with noisy labels for a long enough time. To overcome this problem, we present a simple and effective method self-ensemble label filtering (SELF) to progressively filter out the wrong labels during training. Our method improves the task performance by gradually allowing supervision only from the potentially non-noisy (clean) labels and stops learning on the filtered noisy labels. For the filtering, we form running averages of predictions over the entire training dataset using the network output at different training epochs. We show that these ensemble estimates yield more accurate identification of inconsistent predictions throughout training than the single estimates of the network at the most recent training epoch. While filtered samples are removed entirely from the supervised training loss, we dynamically leverage them via semi-supervised learning in the unsupervised loss. We demonstrate the positive effect of such an approach on various image classification tasks under both symmetric and asymmetric label noise and at different noise ratios. It substantially outperforms all previous works on noise-aware learning across different datasets and can be applied to a broad set of network architectures.", "keywords": ["Ensemble Learning", "Robust Learning", "Noisy Labels", "Labels Filtering"], "paperhash": "nguyen|self_learning_to_filter_noisy_labels_with_selfensembling", "_bibtex": "@inproceedings{\nNguyen2020SELF:,\ntitle={SELF: Learning to Filter Noisy Labels with Self-Ensembling},\nauthor={Duc Tam Nguyen and Chaithanya Kumar Mummadi and Thi Phuong Nhung Ngo and Thi Hoai Phuong Nguyen and Laura Beggel and Thomas Brox},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HkgsPhNYPS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/f53c1264354a72df6a3fdcfe8a3344abd4fde7f2.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HkgsPhNYPS", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504215132, "tmdate": 1576860562323, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper16/Authors", "ICLR.cc/2020/Conference/Paper16/Reviewers", "ICLR.cc/2020/Conference/Paper16/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper16/-/Public_Comment"}}}, {"id": "SylY2HN4tr", "original": null, "number": 1, "cdate": 1571206577198, "ddate": null, "tcdate": 1571206577198, "tmdate": 1572972649273, "tddate": null, "forum": "HkgsPhNYPS", "replyto": "HkgsPhNYPS", "invitation": "ICLR.cc/2020/Conference/Paper16/-/Official_Review", "content": {"rating": "6: Weak Accept", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper proposed \"self-ensemble label filtering\" for learning with noisy labels where the label noise is instance-independent (in fact, the noise model is the class-conditional noise). Among the existing directions in this area, it falls into the sample selection direction, but it also takes semi-supervised learning based on the likely noisy data into account.\n\nNovelty: borderline. As other sample selection methods, the proposed one would like to identify the training data with correct labels. What's new is that the authors \"form running averages of predictions over the entire training dataset using the network output at different training epochs\" and show that \"these ensemble estimates yield more accurate identification of inconsistent predictions throughout training than the single estimates of the network at the most recent training epoch\". This is the major contribution of the paper. Furthermore, the data likely to have incorrect labels are not thrown away but used in a semi-supervised manner. This is a minor contribution, because semi-supervised learning is orthogonal to label-noise learning and everybody in this area knows the combination of them can work better in practice. Note that this is an academic/scientific paper, not an industrial product, so you don't need to combine all things that might work.\n\nSignificance: high. The proposed method significantly outperformed all baseline methods. However, it's not completely fair to compare a label-noise + semi-supervised method with other label-noise only methods... As a matter of fact, you don't need to apply perturbation consistency (or other semi-supervised) regularization after identifying the training data with incorrect labels. Semi-supervised regularization such as virtual adversarial training can even improve supervised learning.\n\nIssues: It's known under class-conditional noise model, the backward loss correction is the unique way to estimate the classification risk (or equivalently, the classification accuracy) given noisy validation data. So how can the validation (i.e., hyperparameter tuning) be performed for the proposed and baseline methods in Table 1 given noisy validation data? "}, "signatures": ["ICLR.cc/2020/Conference/Paper16/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper16/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ductam.nguyen08@gmail.com", "chaithanyakumar.mummadi@de.bosch.com", "thiphuongnhung.ngo@de.bosch.com", "hoai.phuong.nguyen198@gmail.com", "laura.beggel@de.bosch.com", "brox@cs.uni-freiburg.de"], "title": "SELF: Learning to Filter Noisy Labels with Self-Ensembling", "authors": ["Duc Tam Nguyen", "Chaithanya Kumar Mummadi", "Thi Phuong Nhung Ngo", "Thi Hoai Phuong Nguyen", "Laura Beggel", "Thomas Brox"], "pdf": "/pdf/3bafa3876cfccbe25d2fc5c4db5cd495aae2e6c7.pdf", "TL;DR": "We propose a self-ensemble framework to train more robust deep learning models under noisy labeled datasets.", "abstract": "Deep neural networks (DNNs) have been shown to over-fit a dataset when being trained with noisy labels for a long enough time. To overcome this problem, we present a simple and effective method self-ensemble label filtering (SELF) to progressively filter out the wrong labels during training. Our method improves the task performance by gradually allowing supervision only from the potentially non-noisy (clean) labels and stops learning on the filtered noisy labels. For the filtering, we form running averages of predictions over the entire training dataset using the network output at different training epochs. We show that these ensemble estimates yield more accurate identification of inconsistent predictions throughout training than the single estimates of the network at the most recent training epoch. While filtered samples are removed entirely from the supervised training loss, we dynamically leverage them via semi-supervised learning in the unsupervised loss. We demonstrate the positive effect of such an approach on various image classification tasks under both symmetric and asymmetric label noise and at different noise ratios. It substantially outperforms all previous works on noise-aware learning across different datasets and can be applied to a broad set of network architectures.", "keywords": ["Ensemble Learning", "Robust Learning", "Noisy Labels", "Labels Filtering"], "paperhash": "nguyen|self_learning_to_filter_noisy_labels_with_selfensembling", "_bibtex": "@inproceedings{\nNguyen2020SELF:,\ntitle={SELF: Learning to Filter Noisy Labels with Self-Ensembling},\nauthor={Duc Tam Nguyen and Chaithanya Kumar Mummadi and Thi Phuong Nhung Ngo and Thi Hoai Phuong Nguyen and Laura Beggel and Thomas Brox},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HkgsPhNYPS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/f53c1264354a72df6a3fdcfe8a3344abd4fde7f2.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HkgsPhNYPS", "replyto": "HkgsPhNYPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper16/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper16/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574752096427, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper16/Reviewers"], "noninvitees": [], "tcdate": 1570237758356, "tmdate": 1574752096439, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper16/-/Official_Review"}}}, {"id": "SyeJgpOAtS", "original": null, "number": 2, "cdate": 1571880167310, "ddate": null, "tcdate": 1571880167310, "tmdate": 1572972649238, "tddate": null, "forum": "HkgsPhNYPS", "replyto": "HkgsPhNYPS", "invitation": "ICLR.cc/2020/Conference/Paper16/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "--- Overall ---\n\nThis paper proposes an algorithm for learning from data with noisy labels which alternates between updating the model and removing samples that look like they have noisy labels, thereby allowing the training procedure to focus on clean samples. Overall, I found the paper very well-written, the proposed approach reasonable, and the experiments convincing. I have some questions about what assumptions are required for such a procedure to work, but in general, I think this is a strong paper.\n\n--- Major comments ---\n\n1. I found it somewhat unclear how large the methodological contribution was. In particular, has the approach of filtering out samples based on disagreement with predictions from the model been tried before (i.e. the primary contribution is self-ensembling)? When the proposed method includes multiple pieces (Mean teacher + iteratively creating a filtered dataset + self-ensembling), I recommend being *very* explicit about which parts are new contributions. With that said, I greatly appreciated the ablation experiments which really highlight the importance of each piece.\n\n2. I don't feel like I have a good sense for what assumptions need to be satisfied for the proposed method to work. For example, in section 2.2 the authors say \"If the noise is sufficiently random, the set of correct labels will be representative to achieve high model performance\". What is meant by \"sufficiently random\" here? Is there a formal version of this assumption? Do any independence or positivity assumptions need to be satisfied? Most importantly, what happens when the label noise does not look like what you expect? I would love to see some experiments examining the failure modes of the algorithm. For example, what happens when label errors are concentrated in a particular region of the feature space (or just generally depend on the features)? In this case, even if the filtering procedure work perfectly, the filtered dataset will have a different feature distribution than the data distribution leading to potential covariate shift problems. If I understood the experiments correctly, the method was only tested on label-depended noise models.\n\n3. Along the same lines: what are the necessary conditions to guarantee that this procedure converges? While the authors suggest that self-ensembling prevents samples from oscillating in and out of training set, is this a guarantee or an empirical observation? More broadly, it is not totally clear what the filtering does to the objective function or whether this procedure is even formally optimizing a well specified objective function (potentially some temperature limit of a soft-weighted objective?). \n\n--- Minor comments ---\n\n1. Figures 1 and 4 are not readable in black and grey-scale.\n\n2. I would front-load the justification for using self-ensembling. In particular, I think the two sentences starting with \"When learning under label noise,...\" on page 2 could be moved much earlier. \n\n3. I'll be interested to see what the other reviewers say, but I found Figure 2 hard to follow.\n\n4. The formatting of Section 4.2.4 makes it a bit hard to figure out where the text starts (as opposed to the table captions)."}, "signatures": ["ICLR.cc/2020/Conference/Paper16/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper16/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ductam.nguyen08@gmail.com", "chaithanyakumar.mummadi@de.bosch.com", "thiphuongnhung.ngo@de.bosch.com", "hoai.phuong.nguyen198@gmail.com", "laura.beggel@de.bosch.com", "brox@cs.uni-freiburg.de"], "title": "SELF: Learning to Filter Noisy Labels with Self-Ensembling", "authors": ["Duc Tam Nguyen", "Chaithanya Kumar Mummadi", "Thi Phuong Nhung Ngo", "Thi Hoai Phuong Nguyen", "Laura Beggel", "Thomas Brox"], "pdf": "/pdf/3bafa3876cfccbe25d2fc5c4db5cd495aae2e6c7.pdf", "TL;DR": "We propose a self-ensemble framework to train more robust deep learning models under noisy labeled datasets.", "abstract": "Deep neural networks (DNNs) have been shown to over-fit a dataset when being trained with noisy labels for a long enough time. To overcome this problem, we present a simple and effective method self-ensemble label filtering (SELF) to progressively filter out the wrong labels during training. Our method improves the task performance by gradually allowing supervision only from the potentially non-noisy (clean) labels and stops learning on the filtered noisy labels. For the filtering, we form running averages of predictions over the entire training dataset using the network output at different training epochs. We show that these ensemble estimates yield more accurate identification of inconsistent predictions throughout training than the single estimates of the network at the most recent training epoch. While filtered samples are removed entirely from the supervised training loss, we dynamically leverage them via semi-supervised learning in the unsupervised loss. We demonstrate the positive effect of such an approach on various image classification tasks under both symmetric and asymmetric label noise and at different noise ratios. It substantially outperforms all previous works on noise-aware learning across different datasets and can be applied to a broad set of network architectures.", "keywords": ["Ensemble Learning", "Robust Learning", "Noisy Labels", "Labels Filtering"], "paperhash": "nguyen|self_learning_to_filter_noisy_labels_with_selfensembling", "_bibtex": "@inproceedings{\nNguyen2020SELF:,\ntitle={SELF: Learning to Filter Noisy Labels with Self-Ensembling},\nauthor={Duc Tam Nguyen and Chaithanya Kumar Mummadi and Thi Phuong Nhung Ngo and Thi Hoai Phuong Nguyen and Laura Beggel and Thomas Brox},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HkgsPhNYPS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/f53c1264354a72df6a3fdcfe8a3344abd4fde7f2.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HkgsPhNYPS", "replyto": "HkgsPhNYPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper16/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper16/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574752096427, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper16/Reviewers"], "noninvitees": [], "tcdate": 1570237758356, "tmdate": 1574752096439, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper16/-/Official_Review"}}}, {"id": "rkek1NdX5r", "original": null, "number": 3, "cdate": 1572205526812, "ddate": null, "tcdate": 1572205526812, "tmdate": 1572972649195, "tddate": null, "forum": "HkgsPhNYPS", "replyto": "HkgsPhNYPS", "invitation": "ICLR.cc/2020/Conference/Paper16/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "Summary:\nThe paper proposed a self-ensemble label filtering (SELF) method to deal with the noisy label learning problem. They progressively filter out the wrong labels during training, i.e.,  filtered samples are removed entirely from the supervised\ntraining loss, and are leveraged via semi-supervised learning in the unsupervised loss. The filtering is based on identification of inconsistent predictions throughout training. \n\nStrengths:\n1. The motivation of the paper is very clear. \n2. Experiments are conducted on various dataset CIFAR10, CIFAR-100 and ImageNet. \n\nWeakness:\n1. The contribution of SELF is not clear. Just a combining of several previously proposed components?   \n2. For the experimental comparisons, the authors at least should report the acc on clean test set, which is useful for understanding the ideal case performance. \n3. The organization of the tables and figures are somehow hard to read. \n3. The comparisons are not fair. SELF incorporate semi-supervised techniques while baselines are not. \n4. The author missed some important baselines here. \n     1) Symmetric cross entropy for robust learning with noisy labels, ICCV2019 \n     2) Joint Optimization Framework for Learning with Noisy Labels, CVPR2018 \n     3) Dimensionality-driven learning with noisy labels, ICML2018"}, "signatures": ["ICLR.cc/2020/Conference/Paper16/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper16/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ductam.nguyen08@gmail.com", "chaithanyakumar.mummadi@de.bosch.com", "thiphuongnhung.ngo@de.bosch.com", "hoai.phuong.nguyen198@gmail.com", "laura.beggel@de.bosch.com", "brox@cs.uni-freiburg.de"], "title": "SELF: Learning to Filter Noisy Labels with Self-Ensembling", "authors": ["Duc Tam Nguyen", "Chaithanya Kumar Mummadi", "Thi Phuong Nhung Ngo", "Thi Hoai Phuong Nguyen", "Laura Beggel", "Thomas Brox"], "pdf": "/pdf/3bafa3876cfccbe25d2fc5c4db5cd495aae2e6c7.pdf", "TL;DR": "We propose a self-ensemble framework to train more robust deep learning models under noisy labeled datasets.", "abstract": "Deep neural networks (DNNs) have been shown to over-fit a dataset when being trained with noisy labels for a long enough time. To overcome this problem, we present a simple and effective method self-ensemble label filtering (SELF) to progressively filter out the wrong labels during training. Our method improves the task performance by gradually allowing supervision only from the potentially non-noisy (clean) labels and stops learning on the filtered noisy labels. For the filtering, we form running averages of predictions over the entire training dataset using the network output at different training epochs. We show that these ensemble estimates yield more accurate identification of inconsistent predictions throughout training than the single estimates of the network at the most recent training epoch. While filtered samples are removed entirely from the supervised training loss, we dynamically leverage them via semi-supervised learning in the unsupervised loss. We demonstrate the positive effect of such an approach on various image classification tasks under both symmetric and asymmetric label noise and at different noise ratios. It substantially outperforms all previous works on noise-aware learning across different datasets and can be applied to a broad set of network architectures.", "keywords": ["Ensemble Learning", "Robust Learning", "Noisy Labels", "Labels Filtering"], "paperhash": "nguyen|self_learning_to_filter_noisy_labels_with_selfensembling", "_bibtex": "@inproceedings{\nNguyen2020SELF:,\ntitle={SELF: Learning to Filter Noisy Labels with Self-Ensembling},\nauthor={Duc Tam Nguyen and Chaithanya Kumar Mummadi and Thi Phuong Nhung Ngo and Thi Hoai Phuong Nguyen and Laura Beggel and Thomas Brox},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HkgsPhNYPS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/f53c1264354a72df6a3fdcfe8a3344abd4fde7f2.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HkgsPhNYPS", "replyto": "HkgsPhNYPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper16/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper16/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574752096427, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper16/Reviewers"], "noninvitees": [], "tcdate": 1570237758356, "tmdate": 1574752096439, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper16/-/Official_Review"}}}], "count": 14}