{"notes": [{"id": "Bkg8jjC9KQ", "original": "rklZBDq9FX", "number": 626, "cdate": 1538087838226, "ddate": null, "tcdate": 1538087838226, "tmdate": 1560261305693, "tddate": null, "forum": "Bkg8jjC9KQ", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Optimistic mirror descent in saddle-point problems: Going the extra (gradient) mile", "abstract": "Owing to their connection with generative adversarial networks (GANs), saddle-point problems have recently attracted considerable interest in machine learning and beyond. By necessity, most theoretical guarantees revolve around convex-concave (or even linear) problems; however, making theoretical inroads towards efficient GAN training depends crucially on moving beyond this classic framework. To make piecemeal progress along these lines, we analyze the behavior of mirror descent (MD) in a class of non-monotone problems whose solutions coincide with those of a naturally associated variational inequality \u2013 a property which we call coherence. We first show that ordinary, \u201cvanilla\u201d MD converges under a strict version of this condition, but not otherwise; in particular, it may fail to converge even in bilinear models with a unique solution. We then show that this deficiency is mitigated by optimism: by taking an \u201cextra-gradient\u201d step, optimistic mirror descent (OMD) converges in all coherent problems. Our analysis generalizes and extends the results of Daskalakis et al. [2018] for optimistic gradient descent (OGD) in bilinear problems, and makes concrete headway for provable convergence beyond convex-concave games. We also provide stochastic analogues of these results, and we validate our analysis by numerical experiments in a wide array of GAN models (including Gaussian mixture models, and the CelebA and CIFAR-10 datasets).", "keywords": ["Mirror descent", "extra-gradient", "generative adversarial networks", "saddle-point problems"], "authorids": ["panayotis.mertikopoulos@imag.fr", "bruno_lecouat@i2r.a-star.edu.sg", "houssam_zenati@i2r.a-star.edu.sg", "foocs@i2r.a-star.edu.sg", "vijay@i2r.a-star.edu.sg", "georgios@sutd.edu.sg"], "authors": ["Panayotis Mertikopoulos", "Bruno Lecouat", "Houssam Zenati", "Chuan-Sheng Foo", "Vijay Chandrasekhar", "Georgios Piliouras"], "TL;DR": "We show how the inclusion of an extra-gradient step in first-order GAN training methods can improve stability and lead to improved convergence results.", "pdf": "/pdf/e68620ae4c38287559cc1c2f04ff57bf59833d07.pdf", "paperhash": "mertikopoulos|optimistic_mirror_descent_in_saddlepoint_problems_going_the_extra_gradient_mile", "_bibtex": "@inproceedings{\nmertikopoulos2018optimistic,\ntitle={Optimistic mirror descent in saddle-point problems: Going the extra(-gradient) mile},\nauthor={Panayotis Mertikopoulos and Bruno Lecouat and Houssam Zenati and Chuan-Sheng Foo and Vijay Chandrasekhar and Georgios Piliouras},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Bkg8jjC9KQ},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 11, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "BkgeT0Eu4E", "original": null, "number": 10, "cdate": 1549450935861, "ddate": null, "tcdate": 1549450935861, "tmdate": 1549450935861, "tddate": null, "forum": "Bkg8jjC9KQ", "replyto": "BJlCswEOEN", "invitation": "ICLR.cc/2019/Conference/-/Paper626/Official_Comment", "content": {"title": "Convergence to some solution, yes, not necessarily the x* in the proof, no", "comment": "Hey,\n\nThanks for your comments and positive feedback!\n\nYes, in the proof of Theorem 4.1, x* is a \"special\" solution point which satisfies the variational inequality formulation (VI) of the saddle-point problem globally. This point is used to establish convergence to the solution set of the problem, but it is not necessarily the end state of the algorithm - i.e., it is not the \"solution point x*\" alluded to in the statement of the theorem.\n\nThanks for catching this ambiguity, typo correction on its way!"}, "signatures": ["ICLR.cc/2019/Conference/Paper626/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper626/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper626/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Optimistic mirror descent in saddle-point problems: Going the extra (gradient) mile", "abstract": "Owing to their connection with generative adversarial networks (GANs), saddle-point problems have recently attracted considerable interest in machine learning and beyond. By necessity, most theoretical guarantees revolve around convex-concave (or even linear) problems; however, making theoretical inroads towards efficient GAN training depends crucially on moving beyond this classic framework. To make piecemeal progress along these lines, we analyze the behavior of mirror descent (MD) in a class of non-monotone problems whose solutions coincide with those of a naturally associated variational inequality \u2013 a property which we call coherence. We first show that ordinary, \u201cvanilla\u201d MD converges under a strict version of this condition, but not otherwise; in particular, it may fail to converge even in bilinear models with a unique solution. We then show that this deficiency is mitigated by optimism: by taking an \u201cextra-gradient\u201d step, optimistic mirror descent (OMD) converges in all coherent problems. Our analysis generalizes and extends the results of Daskalakis et al. [2018] for optimistic gradient descent (OGD) in bilinear problems, and makes concrete headway for provable convergence beyond convex-concave games. We also provide stochastic analogues of these results, and we validate our analysis by numerical experiments in a wide array of GAN models (including Gaussian mixture models, and the CelebA and CIFAR-10 datasets).", "keywords": ["Mirror descent", "extra-gradient", "generative adversarial networks", "saddle-point problems"], "authorids": ["panayotis.mertikopoulos@imag.fr", "bruno_lecouat@i2r.a-star.edu.sg", "houssam_zenati@i2r.a-star.edu.sg", "foocs@i2r.a-star.edu.sg", "vijay@i2r.a-star.edu.sg", "georgios@sutd.edu.sg"], "authors": ["Panayotis Mertikopoulos", "Bruno Lecouat", "Houssam Zenati", "Chuan-Sheng Foo", "Vijay Chandrasekhar", "Georgios Piliouras"], "TL;DR": "We show how the inclusion of an extra-gradient step in first-order GAN training methods can improve stability and lead to improved convergence results.", "pdf": "/pdf/e68620ae4c38287559cc1c2f04ff57bf59833d07.pdf", "paperhash": "mertikopoulos|optimistic_mirror_descent_in_saddlepoint_problems_going_the_extra_gradient_mile", "_bibtex": "@inproceedings{\nmertikopoulos2018optimistic,\ntitle={Optimistic mirror descent in saddle-point problems: Going the extra(-gradient) mile},\nauthor={Panayotis Mertikopoulos and Bruno Lecouat and Houssam Zenati and Chuan-Sheng Foo and Vijay Chandrasekhar and Georgios Piliouras},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Bkg8jjC9KQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper626/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621618135, "tddate": null, "super": null, "final": null, "reply": {"forum": "Bkg8jjC9KQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper626/Authors", "ICLR.cc/2019/Conference/Paper626/Reviewers", "ICLR.cc/2019/Conference/Paper626/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper626/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper626/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper626/Authors|ICLR.cc/2019/Conference/Paper626/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper626/Reviewers", "ICLR.cc/2019/Conference/Paper626/Authors", "ICLR.cc/2019/Conference/Paper626/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621618135}}}, {"id": "BJlCswEOEN", "original": null, "number": 2, "cdate": 1549449125763, "ddate": null, "tcdate": 1549449125763, "tmdate": 1549449125763, "tddate": null, "forum": "Bkg8jjC9KQ", "replyto": "Bkg8jjC9KQ", "invitation": "ICLR.cc/2019/Conference/-/Paper626/Public_Comment", "content": {"comment": "Hi, very nice paper! In the proof of Theorem 4.1 (in appendix D, end of page 19), where you only have coherence (not necessarily strict), it seems like you only establish convergence of OMD to some point but not necessarily to x^*. Am I missing something?", "title": "Proof of Theorem 4.1"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Optimistic mirror descent in saddle-point problems: Going the extra (gradient) mile", "abstract": "Owing to their connection with generative adversarial networks (GANs), saddle-point problems have recently attracted considerable interest in machine learning and beyond. By necessity, most theoretical guarantees revolve around convex-concave (or even linear) problems; however, making theoretical inroads towards efficient GAN training depends crucially on moving beyond this classic framework. To make piecemeal progress along these lines, we analyze the behavior of mirror descent (MD) in a class of non-monotone problems whose solutions coincide with those of a naturally associated variational inequality \u2013 a property which we call coherence. We first show that ordinary, \u201cvanilla\u201d MD converges under a strict version of this condition, but not otherwise; in particular, it may fail to converge even in bilinear models with a unique solution. We then show that this deficiency is mitigated by optimism: by taking an \u201cextra-gradient\u201d step, optimistic mirror descent (OMD) converges in all coherent problems. Our analysis generalizes and extends the results of Daskalakis et al. [2018] for optimistic gradient descent (OGD) in bilinear problems, and makes concrete headway for provable convergence beyond convex-concave games. We also provide stochastic analogues of these results, and we validate our analysis by numerical experiments in a wide array of GAN models (including Gaussian mixture models, and the CelebA and CIFAR-10 datasets).", "keywords": ["Mirror descent", "extra-gradient", "generative adversarial networks", "saddle-point problems"], "authorids": ["panayotis.mertikopoulos@imag.fr", "bruno_lecouat@i2r.a-star.edu.sg", "houssam_zenati@i2r.a-star.edu.sg", "foocs@i2r.a-star.edu.sg", "vijay@i2r.a-star.edu.sg", "georgios@sutd.edu.sg"], "authors": ["Panayotis Mertikopoulos", "Bruno Lecouat", "Houssam Zenati", "Chuan-Sheng Foo", "Vijay Chandrasekhar", "Georgios Piliouras"], "TL;DR": "We show how the inclusion of an extra-gradient step in first-order GAN training methods can improve stability and lead to improved convergence results.", "pdf": "/pdf/e68620ae4c38287559cc1c2f04ff57bf59833d07.pdf", "paperhash": "mertikopoulos|optimistic_mirror_descent_in_saddlepoint_problems_going_the_extra_gradient_mile", "_bibtex": "@inproceedings{\nmertikopoulos2018optimistic,\ntitle={Optimistic mirror descent in saddle-point problems: Going the extra(-gradient) mile},\nauthor={Panayotis Mertikopoulos and Bruno Lecouat and Houssam Zenati and Chuan-Sheng Foo and Vijay Chandrasekhar and Georgios Piliouras},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Bkg8jjC9KQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper626/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311791199, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "Bkg8jjC9KQ", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper626/Authors", "ICLR.cc/2019/Conference/Paper626/Reviewers", "ICLR.cc/2019/Conference/Paper626/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper626/Authors", "ICLR.cc/2019/Conference/Paper626/Reviewers", "ICLR.cc/2019/Conference/Paper626/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311791199}}}, {"id": "r1gcb0RBgV", "original": null, "number": 1, "cdate": 1545100801749, "ddate": null, "tcdate": 1545100801749, "tmdate": 1545354479212, "tddate": null, "forum": "Bkg8jjC9KQ", "replyto": "Bkg8jjC9KQ", "invitation": "ICLR.cc/2019/Conference/-/Paper626/Meta_Review", "content": {"metareview": "This paper investigates the usage of the extragradient step for solving saddle-point problems with non-monotone stochastic variational inequalities, motivated by GANs. The authors propose an assumption weaker/diffrerent than the pseudo-monotonicity of the variational inequality for their convergence analysis (that they call \"coherence\"). Interestingly, they are able to show the (asympotic) last iterate convergence for the extragradient algorithm in this case (in contrast to standard results which normally requires averaging of the iterates for the stochastic *and* mototone variational inequality such as the cited work by Gidel et al.). The authors also describe an interesting difference between the gradient method without the extragradient step (mirror descent) vs. with (that they called optimistic mirror descent).\n\nR2 thought the coherence condition was too related to the notion of pseudo-monoticity for which one could easily extend previous known convergence results for stochastic variational inequality. The AC thinks that this point was well answered by the authors rebuttal and in their revision: the conditions are sufficiently different, and while there is still much to do to analyze non variational inequalities or having realistic assumptions, this paper makes some non-trivial and interesting steps in this direction. The AC thus sides with expert reviewer R1 and recommends acceptance.", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Accept (Poster)", "title": "Some progress for analysis of non-monotone variational inequalities"}, "signatures": ["ICLR.cc/2019/Conference/Paper626/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper626/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Optimistic mirror descent in saddle-point problems: Going the extra (gradient) mile", "abstract": "Owing to their connection with generative adversarial networks (GANs), saddle-point problems have recently attracted considerable interest in machine learning and beyond. By necessity, most theoretical guarantees revolve around convex-concave (or even linear) problems; however, making theoretical inroads towards efficient GAN training depends crucially on moving beyond this classic framework. To make piecemeal progress along these lines, we analyze the behavior of mirror descent (MD) in a class of non-monotone problems whose solutions coincide with those of a naturally associated variational inequality \u2013 a property which we call coherence. We first show that ordinary, \u201cvanilla\u201d MD converges under a strict version of this condition, but not otherwise; in particular, it may fail to converge even in bilinear models with a unique solution. We then show that this deficiency is mitigated by optimism: by taking an \u201cextra-gradient\u201d step, optimistic mirror descent (OMD) converges in all coherent problems. Our analysis generalizes and extends the results of Daskalakis et al. [2018] for optimistic gradient descent (OGD) in bilinear problems, and makes concrete headway for provable convergence beyond convex-concave games. We also provide stochastic analogues of these results, and we validate our analysis by numerical experiments in a wide array of GAN models (including Gaussian mixture models, and the CelebA and CIFAR-10 datasets).", "keywords": ["Mirror descent", "extra-gradient", "generative adversarial networks", "saddle-point problems"], "authorids": ["panayotis.mertikopoulos@imag.fr", "bruno_lecouat@i2r.a-star.edu.sg", "houssam_zenati@i2r.a-star.edu.sg", "foocs@i2r.a-star.edu.sg", "vijay@i2r.a-star.edu.sg", "georgios@sutd.edu.sg"], "authors": ["Panayotis Mertikopoulos", "Bruno Lecouat", "Houssam Zenati", "Chuan-Sheng Foo", "Vijay Chandrasekhar", "Georgios Piliouras"], "TL;DR": "We show how the inclusion of an extra-gradient step in first-order GAN training methods can improve stability and lead to improved convergence results.", "pdf": "/pdf/e68620ae4c38287559cc1c2f04ff57bf59833d07.pdf", "paperhash": "mertikopoulos|optimistic_mirror_descent_in_saddlepoint_problems_going_the_extra_gradient_mile", "_bibtex": "@inproceedings{\nmertikopoulos2018optimistic,\ntitle={Optimistic mirror descent in saddle-point problems: Going the extra(-gradient) mile},\nauthor={Panayotis Mertikopoulos and Bruno Lecouat and Houssam Zenati and Chuan-Sheng Foo and Vijay Chandrasekhar and Georgios Piliouras},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Bkg8jjC9KQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper626/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353146865, "tddate": null, "super": null, "final": null, "reply": {"forum": "Bkg8jjC9KQ", "replyto": "Bkg8jjC9KQ", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper626/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper626/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper626/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353146865}}}, {"id": "r1x7Pw_4yE", "original": null, "number": 9, "cdate": 1543960410626, "ddate": null, "tcdate": 1543960410626, "tmdate": 1543960498662, "tddate": null, "forum": "Bkg8jjC9KQ", "replyto": "HJxro3I4kV", "invitation": "ICLR.cc/2019/Conference/-/Paper626/Official_Comment", "content": {"title": "Thanks for the extra round of feedback!", "comment": "Many thanks for the extra round of feedback and the encouraging remarks! We reply to the points you raised below:\n\n1. Regarding the example of a coherent problem with a general convex solution set.\n\nAgain, for simplicity, focus on the optimization case, i.e., the minimization of a function f:X->R (X convex). In this case, letting X* = argmin f, and writing g(x) for the (sub)gradient of f, the (strict) coherence requirement takes the form:\n  \u2028- <g(x),x-x*>\u22650 for all x in X and all x* in X*.\n  - Equality holds above if and only if x lies in X*.\n\nNow, fix some convex subset C of X, and let f(x) = dist(x,C)^2 (where dist denotes the standard Euclidean setwise distance). By construction, f is convex (though not strictly so) and X*=C. Convexity guarantees the first requirement of coherence. For the second, note that g(x) is a multiple of x - proj_C(x) so, for any x* in X*, the product <g(x),x-x*> vanishes only if x lies itself in C (since C=X*).\n\nOf course, the above function is convex, but if we perturb f away from C = X* in an appropriate way, non-convex examples can also be constructed (though there are diminishing returns regarding the simplicity of the resulting example).\n\n[NB: just to avoid any misunderstanding, the above concerns the definition of coherence as presented in the *original* version of the paper; the current version includes examples with non-convex solution sets like x^2 y^2 as we outlined in our first reply.]\n\n\n2. Thanks for the pointer to Chen and Rockafellar, it looks very promising for future study! The reviewer's suggestion seems very plausible but the devil is often in the details, so we would need more time in order to provide a more definitive reply.\n\n\nWe cannot revise the paper at this time, but we'd of course be happy to do so along the lines above if accepted."}, "signatures": ["ICLR.cc/2019/Conference/Paper626/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper626/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper626/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Optimistic mirror descent in saddle-point problems: Going the extra (gradient) mile", "abstract": "Owing to their connection with generative adversarial networks (GANs), saddle-point problems have recently attracted considerable interest in machine learning and beyond. By necessity, most theoretical guarantees revolve around convex-concave (or even linear) problems; however, making theoretical inroads towards efficient GAN training depends crucially on moving beyond this classic framework. To make piecemeal progress along these lines, we analyze the behavior of mirror descent (MD) in a class of non-monotone problems whose solutions coincide with those of a naturally associated variational inequality \u2013 a property which we call coherence. We first show that ordinary, \u201cvanilla\u201d MD converges under a strict version of this condition, but not otherwise; in particular, it may fail to converge even in bilinear models with a unique solution. We then show that this deficiency is mitigated by optimism: by taking an \u201cextra-gradient\u201d step, optimistic mirror descent (OMD) converges in all coherent problems. Our analysis generalizes and extends the results of Daskalakis et al. [2018] for optimistic gradient descent (OGD) in bilinear problems, and makes concrete headway for provable convergence beyond convex-concave games. We also provide stochastic analogues of these results, and we validate our analysis by numerical experiments in a wide array of GAN models (including Gaussian mixture models, and the CelebA and CIFAR-10 datasets).", "keywords": ["Mirror descent", "extra-gradient", "generative adversarial networks", "saddle-point problems"], "authorids": ["panayotis.mertikopoulos@imag.fr", "bruno_lecouat@i2r.a-star.edu.sg", "houssam_zenati@i2r.a-star.edu.sg", "foocs@i2r.a-star.edu.sg", "vijay@i2r.a-star.edu.sg", "georgios@sutd.edu.sg"], "authors": ["Panayotis Mertikopoulos", "Bruno Lecouat", "Houssam Zenati", "Chuan-Sheng Foo", "Vijay Chandrasekhar", "Georgios Piliouras"], "TL;DR": "We show how the inclusion of an extra-gradient step in first-order GAN training methods can improve stability and lead to improved convergence results.", "pdf": "/pdf/e68620ae4c38287559cc1c2f04ff57bf59833d07.pdf", "paperhash": "mertikopoulos|optimistic_mirror_descent_in_saddlepoint_problems_going_the_extra_gradient_mile", "_bibtex": "@inproceedings{\nmertikopoulos2018optimistic,\ntitle={Optimistic mirror descent in saddle-point problems: Going the extra(-gradient) mile},\nauthor={Panayotis Mertikopoulos and Bruno Lecouat and Houssam Zenati and Chuan-Sheng Foo and Vijay Chandrasekhar and Georgios Piliouras},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Bkg8jjC9KQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper626/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621618135, "tddate": null, "super": null, "final": null, "reply": {"forum": "Bkg8jjC9KQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper626/Authors", "ICLR.cc/2019/Conference/Paper626/Reviewers", "ICLR.cc/2019/Conference/Paper626/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper626/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper626/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper626/Authors|ICLR.cc/2019/Conference/Paper626/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper626/Reviewers", "ICLR.cc/2019/Conference/Paper626/Authors", "ICLR.cc/2019/Conference/Paper626/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621618135}}}, {"id": "HJxro3I4kV", "original": null, "number": 8, "cdate": 1543953565318, "ddate": null, "tcdate": 1543953565318, "tmdate": 1543953565318, "tddate": null, "forum": "Bkg8jjC9KQ", "replyto": "r1lVGZvtTm", "invitation": "ICLR.cc/2019/Conference/-/Paper626/Official_Comment", "content": {"title": "Thank you for you detailed answer", "comment": "Thank you for you detailed answer.\n\n\"[We can provide a concrete example if the referee finds this useful]\" would love to.\n\nRegarding 3. I would like to say that the strict coherence assumption is an extension of the strict monotonicity assumption with which you can also prove last iterate converge. Nemirovski, Nesterov, Juditski focus on general monotonicity (the equivalent of you general coherence with which you do not prove any last iterate convergence result)\nAn interesting point I would like to make is that Last iterate convergence have been proven in the literature under the *strong* monotonicity assumption see for instance [Chen et al. 1997] (the Forward-backward algorithm is a generalization of the MD algorithm). Maybe you could have convergence rate under a *strong* coherence assumption (but also raising the question to what extend *strong* coherence assumption is realistic)\n\n\nChen, George HG, and R. Tyrrell Rockafellar. \"Convergence rates in forward--backward splitting.\" SIAM Journal on Optimization 7.2 (1997): 421-444.\n\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper626/AnonReviewer1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper626/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper626/AnonReviewer1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Optimistic mirror descent in saddle-point problems: Going the extra (gradient) mile", "abstract": "Owing to their connection with generative adversarial networks (GANs), saddle-point problems have recently attracted considerable interest in machine learning and beyond. By necessity, most theoretical guarantees revolve around convex-concave (or even linear) problems; however, making theoretical inroads towards efficient GAN training depends crucially on moving beyond this classic framework. To make piecemeal progress along these lines, we analyze the behavior of mirror descent (MD) in a class of non-monotone problems whose solutions coincide with those of a naturally associated variational inequality \u2013 a property which we call coherence. We first show that ordinary, \u201cvanilla\u201d MD converges under a strict version of this condition, but not otherwise; in particular, it may fail to converge even in bilinear models with a unique solution. We then show that this deficiency is mitigated by optimism: by taking an \u201cextra-gradient\u201d step, optimistic mirror descent (OMD) converges in all coherent problems. Our analysis generalizes and extends the results of Daskalakis et al. [2018] for optimistic gradient descent (OGD) in bilinear problems, and makes concrete headway for provable convergence beyond convex-concave games. We also provide stochastic analogues of these results, and we validate our analysis by numerical experiments in a wide array of GAN models (including Gaussian mixture models, and the CelebA and CIFAR-10 datasets).", "keywords": ["Mirror descent", "extra-gradient", "generative adversarial networks", "saddle-point problems"], "authorids": ["panayotis.mertikopoulos@imag.fr", "bruno_lecouat@i2r.a-star.edu.sg", "houssam_zenati@i2r.a-star.edu.sg", "foocs@i2r.a-star.edu.sg", "vijay@i2r.a-star.edu.sg", "georgios@sutd.edu.sg"], "authors": ["Panayotis Mertikopoulos", "Bruno Lecouat", "Houssam Zenati", "Chuan-Sheng Foo", "Vijay Chandrasekhar", "Georgios Piliouras"], "TL;DR": "We show how the inclusion of an extra-gradient step in first-order GAN training methods can improve stability and lead to improved convergence results.", "pdf": "/pdf/e68620ae4c38287559cc1c2f04ff57bf59833d07.pdf", "paperhash": "mertikopoulos|optimistic_mirror_descent_in_saddlepoint_problems_going_the_extra_gradient_mile", "_bibtex": "@inproceedings{\nmertikopoulos2018optimistic,\ntitle={Optimistic mirror descent in saddle-point problems: Going the extra(-gradient) mile},\nauthor={Panayotis Mertikopoulos and Bruno Lecouat and Houssam Zenati and Chuan-Sheng Foo and Vijay Chandrasekhar and Georgios Piliouras},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Bkg8jjC9KQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper626/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621618135, "tddate": null, "super": null, "final": null, "reply": {"forum": "Bkg8jjC9KQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper626/Authors", "ICLR.cc/2019/Conference/Paper626/Reviewers", "ICLR.cc/2019/Conference/Paper626/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper626/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper626/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper626/Authors|ICLR.cc/2019/Conference/Paper626/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper626/Reviewers", "ICLR.cc/2019/Conference/Paper626/Authors", "ICLR.cc/2019/Conference/Paper626/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621618135}}}, {"id": "rkxGKGDY6m", "original": null, "number": 4, "cdate": 1542185593922, "ddate": null, "tcdate": 1542185593922, "tmdate": 1542204643374, "tddate": null, "forum": "Bkg8jjC9KQ", "replyto": "SyeTm7oIhm", "invitation": "ICLR.cc/2019/Conference/-/Paper626/Official_Comment", "content": {"title": "Thanks for the feedback! (see below why pseudo-monotonicity is quite different)", "comment": "We thank the reviewer for their constructive remarks! We reply point-by-point below:\n\n1.\tTo be sure, coherence does not cover all GAN problems: GANs can be so complex that we feel that any endeavor to account for all problems would be chimeric (at least, given our current level of understanding of the GAN landscape). Being fully aware of this, our goal in this paper was simply to provide concrete theoretical evidence that the inclusion of an extra-gradient step can help resolve many of the problems that arise in practice (and, in particular, cycling and oscillatory mode collapses). In this regard, our paper tackles a significantly wider framework than the 2018 ICLR paper of Daskalakis et al. which only addressed bilinear models.\n\nFurthermore, we would like to point out that Corollaries 3.2 and 3.3 are only *sufficient* conditions for coherence. To make an analogy with convex analysis, in practice, when trying to determine whether a given function is convex, one of the standard techniques is to show that its Hessian matrix is diagonally dominant - and, hence, positive-semidefinite. Obviously, this is just a sufficient condition, but it is still useful in practice. We view Corollaries 3.2 and 3.2 in a similar light: they show that our results cover a wide array of cases of practical (and theoretical) interest, without attempting to be exhaustive.\n\n\n2.\tRegarding the relation with pseudo-monotonicity: despite any formal similarities, we would like to point out that coherence and pseudo-monotonicity can be quite different. As an example, take the objective function (2.2) in our paper: for x_1 = 1/2, we get f(1/2,x_2) = (x_2^2 - 2)^2 (4 + 5x_2^2) / 16, which has *two* well-separated maximizers, i.e. it is not even quasi-concave - implying in turn that (2.2) is not pseudo-monotone (it is, in fact, multi-modal in x_2).\n\nMoreover, as we pointed in our reply to Reviewer 2, the version of coherence that we presented was the simplest possible one (and we did so for reasons of clarity and ease of presentation). Our definition can be weakened substantially by considering the following definition of \"weak coherence\":\n\nDefinition: We say that f is weakly coherent if:\n(i) There exists a solution p of (SP) that satisfies (VI).\n(ii) Every solution x* of (SP) satisfies (VI) locally, i.e., g(x) (x - x*) \u2265 0 for all x sufficiently close to x*.\n\nAs we pointed out in our reply to Reviewer 2, under this *weaker* definition of coherence, the solution set of (SP) need no longer be convex, thus making the difference with pseudo-monotone problems even more pronounced. As a very simple example, consider the case where Player 1 controls x,y in [-1,1], and the objective function is f(x,y) = x^2 y^2, i.e., Player 2 has no impact in the game (just for simplicity). In this case, the solution set of the problem is the cross-shaped set X* = {(x,y) : x=0 or y=0}, which is non-convex - in stark contrast to the convex structure of the solution set of pseudo-monotone problems.\n\nWe will update our manuscript accordingly as soon as possible to make this change!\n\nWe will also include a detailed discussion of the paper by Noor et al. - we were not aware of it, and we thank the reviewer for bringing it to our attention.\n\n\n3.\tRegarding the integration of Adam in our proof technique: we agree with the reviewer that this is a worthwhile extension, but not one that can be properly undertaken without completely changing the structure of the paper and its focus. Adam has a very specific update structure and requires the introduction of significant machinery to handle theoretically, so we do not see how it can be done without greatly shifting the scope and balance of our treatment and analysis."}, "signatures": ["ICLR.cc/2019/Conference/Paper626/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper626/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper626/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Optimistic mirror descent in saddle-point problems: Going the extra (gradient) mile", "abstract": "Owing to their connection with generative adversarial networks (GANs), saddle-point problems have recently attracted considerable interest in machine learning and beyond. By necessity, most theoretical guarantees revolve around convex-concave (or even linear) problems; however, making theoretical inroads towards efficient GAN training depends crucially on moving beyond this classic framework. To make piecemeal progress along these lines, we analyze the behavior of mirror descent (MD) in a class of non-monotone problems whose solutions coincide with those of a naturally associated variational inequality \u2013 a property which we call coherence. We first show that ordinary, \u201cvanilla\u201d MD converges under a strict version of this condition, but not otherwise; in particular, it may fail to converge even in bilinear models with a unique solution. We then show that this deficiency is mitigated by optimism: by taking an \u201cextra-gradient\u201d step, optimistic mirror descent (OMD) converges in all coherent problems. Our analysis generalizes and extends the results of Daskalakis et al. [2018] for optimistic gradient descent (OGD) in bilinear problems, and makes concrete headway for provable convergence beyond convex-concave games. We also provide stochastic analogues of these results, and we validate our analysis by numerical experiments in a wide array of GAN models (including Gaussian mixture models, and the CelebA and CIFAR-10 datasets).", "keywords": ["Mirror descent", "extra-gradient", "generative adversarial networks", "saddle-point problems"], "authorids": ["panayotis.mertikopoulos@imag.fr", "bruno_lecouat@i2r.a-star.edu.sg", "houssam_zenati@i2r.a-star.edu.sg", "foocs@i2r.a-star.edu.sg", "vijay@i2r.a-star.edu.sg", "georgios@sutd.edu.sg"], "authors": ["Panayotis Mertikopoulos", "Bruno Lecouat", "Houssam Zenati", "Chuan-Sheng Foo", "Vijay Chandrasekhar", "Georgios Piliouras"], "TL;DR": "We show how the inclusion of an extra-gradient step in first-order GAN training methods can improve stability and lead to improved convergence results.", "pdf": "/pdf/e68620ae4c38287559cc1c2f04ff57bf59833d07.pdf", "paperhash": "mertikopoulos|optimistic_mirror_descent_in_saddlepoint_problems_going_the_extra_gradient_mile", "_bibtex": "@inproceedings{\nmertikopoulos2018optimistic,\ntitle={Optimistic mirror descent in saddle-point problems: Going the extra(-gradient) mile},\nauthor={Panayotis Mertikopoulos and Bruno Lecouat and Houssam Zenati and Chuan-Sheng Foo and Vijay Chandrasekhar and Georgios Piliouras},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Bkg8jjC9KQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper626/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621618135, "tddate": null, "super": null, "final": null, "reply": {"forum": "Bkg8jjC9KQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper626/Authors", "ICLR.cc/2019/Conference/Paper626/Reviewers", "ICLR.cc/2019/Conference/Paper626/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper626/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper626/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper626/Authors|ICLR.cc/2019/Conference/Paper626/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper626/Reviewers", "ICLR.cc/2019/Conference/Paper626/Authors", "ICLR.cc/2019/Conference/Paper626/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621618135}}}, {"id": "r1lVGZvtTm", "original": null, "number": 3, "cdate": 1542185228025, "ddate": null, "tcdate": 1542185228025, "tmdate": 1542185228025, "tddate": null, "forum": "Bkg8jjC9KQ", "replyto": "H1xUfVr9hX", "invitation": "ICLR.cc/2019/Conference/-/Paper626/Official_Comment", "content": {"title": "Thanks for the feedback!", "comment": "We thank the reviewer for their in-depth remarks and positive evaluation! We reply point-by-point below:\n\n\n1.\tRegarding the structure of the solution set of a coherent problem: we agree that this structural question can be investigated further but, given space constraints, we are concerned that this might potentially dilute the focus of the paper. Nevertheless, we would like to take advantage of the openreview format to answer in detail the referee's questions regarding the solution set of a coherent problem:\n- As the referee already points out, uniqueness can be easily taken care of by considering the constant function: the solution set of this problem is the entire feasible region, though the problem is null coherent [and vacuously strictly coherent if we interpret Definition 2.1 to hold for the empty set in the case of strict coherence.] More interesting examples with a zeroed-out direction also exist: for instance, the problem f(x_1,x_2) = x_1^2 is strictly coherent, but its solution set is an affine space.\n- Whether the solution set is an affine space intersected with the set of constraints: in the current formulation, it can be shown that the solution set of a coherent problem is a convex space, though not necessarily one obtained as the intersection of an affine set with the feasible region. [We can provide a concrete example if the referee finds this useful]\n- However, as we state in the paper, the definition of coherence can be weakened substantially, and our results still go through. Specifically, consider the following definition of \"weak coherence\":\n\nDefinition: We say that f is weakly coherent if:\n(i) There exists a solution p of (SP) that satisfies (VI).\n(ii) Every solution x* of (SP) satisfies (VI) locally, i.e., g(x) (x - x*) \u2265 0 for all x sufficiently close to x*.\n\nUnder this *weaker* definition of coherence, the solution set of (SP) need no longer be convex! To see this, consider a very simple optimization example where Player 1 controls x,y in [-1,1], and the objective function is f(x,y) = x^2 y^2 (i.e., Player 2 has no impact in the game, just for simplicity). In this case, the solution set of the problem is the cross-shaped set X* = {(x,y) : x=0 or y=0}, which is non-convex!\n\nWe chose to focus on the case where the solutions of (SP) and (VI) coincide for simplicity and clarity of presentation; however, we will update our manuscript accordingly as soon as possible to make this change!\n\n\n2.\tIndeed, the results are only asymptotic - but, as the reviewer states, we know of virtually no other results at this level of generality, and the analysis has to start somewhere. We agree that getting rates is an important problem, but we believe that all this cannot be addressed within a single paper.\n\n\n3.\tRegarding the similarity of proof techniques with MD/OMD: we would like to point out that conventional MD/OMD proof techniques are typically quite different as they focus on the convergence of the so-called \"ergodic average\" of the sequence of iterates (see e.g., the cited literature by Nemirovski, Nesterov, Juditski et al., and many others). Averaging techniques rely crucially on the problem being convex-concave and cannot be used in a non-monotone setting; as a result, we took a completely different approach relying on a quasi-Fej\u00e9r analysis inspired by recent work on Bregman proximal methods in operator theory.\n\n\n4.\tWe concur that our results can be extended to non-zero-sum games, this is a great observation! Again, we did not make this link explicit in our paper for simplicity, but we will definitely update our manuscript accordingly.\n\n\n5.\tRegarding the name \"optimistic mirror descent\". In the original NIPS 2013 paper of Rakhlin and Sridharan, the authors present two variants of OMD: one is essentially the mirror-prox algorithm of Nemirovski (2004), and the other is a \"momentum\"-like variant which was further studied by Daskalakis et al. in their recent 2018 ICLR paper. Regrettably, there is a fair bit of confusion in the literature regarding what \"optimistic\" descent is: personally, we have a strong preference for the original \"mirror-prox\" terminology of Nemirovski (after all, in saddle-point problems, the method is *not* a descent method). However, we used the OMD terminology of Rakhlin and Sridharan because it seems to be more easily recognizable in the GAN community.\n\n\n6. Minor comments: We will take care of those, thanks!"}, "signatures": ["ICLR.cc/2019/Conference/Paper626/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper626/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper626/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Optimistic mirror descent in saddle-point problems: Going the extra (gradient) mile", "abstract": "Owing to their connection with generative adversarial networks (GANs), saddle-point problems have recently attracted considerable interest in machine learning and beyond. By necessity, most theoretical guarantees revolve around convex-concave (or even linear) problems; however, making theoretical inroads towards efficient GAN training depends crucially on moving beyond this classic framework. To make piecemeal progress along these lines, we analyze the behavior of mirror descent (MD) in a class of non-monotone problems whose solutions coincide with those of a naturally associated variational inequality \u2013 a property which we call coherence. We first show that ordinary, \u201cvanilla\u201d MD converges under a strict version of this condition, but not otherwise; in particular, it may fail to converge even in bilinear models with a unique solution. We then show that this deficiency is mitigated by optimism: by taking an \u201cextra-gradient\u201d step, optimistic mirror descent (OMD) converges in all coherent problems. Our analysis generalizes and extends the results of Daskalakis et al. [2018] for optimistic gradient descent (OGD) in bilinear problems, and makes concrete headway for provable convergence beyond convex-concave games. We also provide stochastic analogues of these results, and we validate our analysis by numerical experiments in a wide array of GAN models (including Gaussian mixture models, and the CelebA and CIFAR-10 datasets).", "keywords": ["Mirror descent", "extra-gradient", "generative adversarial networks", "saddle-point problems"], "authorids": ["panayotis.mertikopoulos@imag.fr", "bruno_lecouat@i2r.a-star.edu.sg", "houssam_zenati@i2r.a-star.edu.sg", "foocs@i2r.a-star.edu.sg", "vijay@i2r.a-star.edu.sg", "georgios@sutd.edu.sg"], "authors": ["Panayotis Mertikopoulos", "Bruno Lecouat", "Houssam Zenati", "Chuan-Sheng Foo", "Vijay Chandrasekhar", "Georgios Piliouras"], "TL;DR": "We show how the inclusion of an extra-gradient step in first-order GAN training methods can improve stability and lead to improved convergence results.", "pdf": "/pdf/e68620ae4c38287559cc1c2f04ff57bf59833d07.pdf", "paperhash": "mertikopoulos|optimistic_mirror_descent_in_saddlepoint_problems_going_the_extra_gradient_mile", "_bibtex": "@inproceedings{\nmertikopoulos2018optimistic,\ntitle={Optimistic mirror descent in saddle-point problems: Going the extra(-gradient) mile},\nauthor={Panayotis Mertikopoulos and Bruno Lecouat and Houssam Zenati and Chuan-Sheng Foo and Vijay Chandrasekhar and Georgios Piliouras},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Bkg8jjC9KQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper626/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621618135, "tddate": null, "super": null, "final": null, "reply": {"forum": "Bkg8jjC9KQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper626/Authors", "ICLR.cc/2019/Conference/Paper626/Reviewers", "ICLR.cc/2019/Conference/Paper626/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper626/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper626/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper626/Authors|ICLR.cc/2019/Conference/Paper626/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper626/Reviewers", "ICLR.cc/2019/Conference/Paper626/Authors", "ICLR.cc/2019/Conference/Paper626/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621618135}}}, {"id": "SkxF31vKpm", "original": null, "number": 2, "cdate": 1542184881190, "ddate": null, "tcdate": 1542184881190, "tmdate": 1542184881190, "tddate": null, "forum": "Bkg8jjC9KQ", "replyto": "HkxnI7tn3Q", "invitation": "ICLR.cc/2019/Conference/-/Paper626/Official_Comment", "content": {"title": "Thanks for the feedback!", "comment": "We thank the reviewer for their positive and encouraging feedback! We also feel that the inclusion of an extra-gradient step can greatly enhance the stability of GAN training methods, and can provide further key insights."}, "signatures": ["ICLR.cc/2019/Conference/Paper626/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper626/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper626/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Optimistic mirror descent in saddle-point problems: Going the extra (gradient) mile", "abstract": "Owing to their connection with generative adversarial networks (GANs), saddle-point problems have recently attracted considerable interest in machine learning and beyond. By necessity, most theoretical guarantees revolve around convex-concave (or even linear) problems; however, making theoretical inroads towards efficient GAN training depends crucially on moving beyond this classic framework. To make piecemeal progress along these lines, we analyze the behavior of mirror descent (MD) in a class of non-monotone problems whose solutions coincide with those of a naturally associated variational inequality \u2013 a property which we call coherence. We first show that ordinary, \u201cvanilla\u201d MD converges under a strict version of this condition, but not otherwise; in particular, it may fail to converge even in bilinear models with a unique solution. We then show that this deficiency is mitigated by optimism: by taking an \u201cextra-gradient\u201d step, optimistic mirror descent (OMD) converges in all coherent problems. Our analysis generalizes and extends the results of Daskalakis et al. [2018] for optimistic gradient descent (OGD) in bilinear problems, and makes concrete headway for provable convergence beyond convex-concave games. We also provide stochastic analogues of these results, and we validate our analysis by numerical experiments in a wide array of GAN models (including Gaussian mixture models, and the CelebA and CIFAR-10 datasets).", "keywords": ["Mirror descent", "extra-gradient", "generative adversarial networks", "saddle-point problems"], "authorids": ["panayotis.mertikopoulos@imag.fr", "bruno_lecouat@i2r.a-star.edu.sg", "houssam_zenati@i2r.a-star.edu.sg", "foocs@i2r.a-star.edu.sg", "vijay@i2r.a-star.edu.sg", "georgios@sutd.edu.sg"], "authors": ["Panayotis Mertikopoulos", "Bruno Lecouat", "Houssam Zenati", "Chuan-Sheng Foo", "Vijay Chandrasekhar", "Georgios Piliouras"], "TL;DR": "We show how the inclusion of an extra-gradient step in first-order GAN training methods can improve stability and lead to improved convergence results.", "pdf": "/pdf/e68620ae4c38287559cc1c2f04ff57bf59833d07.pdf", "paperhash": "mertikopoulos|optimistic_mirror_descent_in_saddlepoint_problems_going_the_extra_gradient_mile", "_bibtex": "@inproceedings{\nmertikopoulos2018optimistic,\ntitle={Optimistic mirror descent in saddle-point problems: Going the extra(-gradient) mile},\nauthor={Panayotis Mertikopoulos and Bruno Lecouat and Houssam Zenati and Chuan-Sheng Foo and Vijay Chandrasekhar and Georgios Piliouras},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Bkg8jjC9KQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper626/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621618135, "tddate": null, "super": null, "final": null, "reply": {"forum": "Bkg8jjC9KQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper626/Authors", "ICLR.cc/2019/Conference/Paper626/Reviewers", "ICLR.cc/2019/Conference/Paper626/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper626/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper626/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper626/Authors|ICLR.cc/2019/Conference/Paper626/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper626/Reviewers", "ICLR.cc/2019/Conference/Paper626/Authors", "ICLR.cc/2019/Conference/Paper626/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621618135}}}, {"id": "HkxnI7tn3Q", "original": null, "number": 3, "cdate": 1541342036214, "ddate": null, "tcdate": 1541342036214, "tmdate": 1541533828290, "tddate": null, "forum": "Bkg8jjC9KQ", "replyto": "Bkg8jjC9KQ", "invitation": "ICLR.cc/2019/Conference/-/Paper626/Official_Review", "content": {"title": "A good paper", "review": "This paper is trying to find a saddle-point of a Lagrangian using mirror descent. Mirror descent based methods use Bregman divergence to encode the convexity and smoothness of objective function beyond the euclidean structure. The main contribution of this paper is adding an extra gradient step to the standard MD, i.e., step 5 in Algorithm 2 as well as stochastic versions. Numerical experiments support their results.", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper626/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Optimistic mirror descent in saddle-point problems: Going the extra (gradient) mile", "abstract": "Owing to their connection with generative adversarial networks (GANs), saddle-point problems have recently attracted considerable interest in machine learning and beyond. By necessity, most theoretical guarantees revolve around convex-concave (or even linear) problems; however, making theoretical inroads towards efficient GAN training depends crucially on moving beyond this classic framework. To make piecemeal progress along these lines, we analyze the behavior of mirror descent (MD) in a class of non-monotone problems whose solutions coincide with those of a naturally associated variational inequality \u2013 a property which we call coherence. We first show that ordinary, \u201cvanilla\u201d MD converges under a strict version of this condition, but not otherwise; in particular, it may fail to converge even in bilinear models with a unique solution. We then show that this deficiency is mitigated by optimism: by taking an \u201cextra-gradient\u201d step, optimistic mirror descent (OMD) converges in all coherent problems. Our analysis generalizes and extends the results of Daskalakis et al. [2018] for optimistic gradient descent (OGD) in bilinear problems, and makes concrete headway for provable convergence beyond convex-concave games. We also provide stochastic analogues of these results, and we validate our analysis by numerical experiments in a wide array of GAN models (including Gaussian mixture models, and the CelebA and CIFAR-10 datasets).", "keywords": ["Mirror descent", "extra-gradient", "generative adversarial networks", "saddle-point problems"], "authorids": ["panayotis.mertikopoulos@imag.fr", "bruno_lecouat@i2r.a-star.edu.sg", "houssam_zenati@i2r.a-star.edu.sg", "foocs@i2r.a-star.edu.sg", "vijay@i2r.a-star.edu.sg", "georgios@sutd.edu.sg"], "authors": ["Panayotis Mertikopoulos", "Bruno Lecouat", "Houssam Zenati", "Chuan-Sheng Foo", "Vijay Chandrasekhar", "Georgios Piliouras"], "TL;DR": "We show how the inclusion of an extra-gradient step in first-order GAN training methods can improve stability and lead to improved convergence results.", "pdf": "/pdf/e68620ae4c38287559cc1c2f04ff57bf59833d07.pdf", "paperhash": "mertikopoulos|optimistic_mirror_descent_in_saddlepoint_problems_going_the_extra_gradient_mile", "_bibtex": "@inproceedings{\nmertikopoulos2018optimistic,\ntitle={Optimistic mirror descent in saddle-point problems: Going the extra(-gradient) mile},\nauthor={Panayotis Mertikopoulos and Bruno Lecouat and Houssam Zenati and Chuan-Sheng Foo and Vijay Chandrasekhar and Georgios Piliouras},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Bkg8jjC9KQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper626/Official_Review", "cdate": 1542234416703, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "Bkg8jjC9KQ", "replyto": "Bkg8jjC9KQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper626/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335766794, "tmdate": 1552335766794, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper626/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "H1xUfVr9hX", "original": null, "number": 2, "cdate": 1541194765793, "ddate": null, "tcdate": 1541194765793, "tmdate": 1541533828091, "tddate": null, "forum": "Bkg8jjC9KQ", "replyto": "Bkg8jjC9KQ", "invitation": "ICLR.cc/2019/Conference/-/Paper626/Official_Review", "content": {"title": "A first step to handle non-convexity in saddle point optimization", "review": "This work provides the converge proof of the last iterates of two stochastic methods (almost surely) that the author called  mirror descent and optimistic mirror descent under an assumption weaker than monotonicity called coherence. \nRoughly, the definition of coherence is the equivalence between being a  saddle point and the solution of the Minty variational inequality. \n\nOverall, I think that this paper try to tackle an interesting problem which is to prove convergence of saddle point algorithms under weaker assumption than monotonicity of the operator.\n\nHowever, I have some concerns: \n\n- I think that the properties of coherent saddle point could be more investigated. For instance is the set of coherent saddle point connected ? It would be very relevant for GANs. You claim that \"neither strict, nor null coherence imply a unique solution to (SP),\" but I do not see any proof of that statement (both provided examples have a unique SP). I agree that you can set $g$ to $0$ in some directions to get an affine space a of saddle points but is there examples where the set of solution is not an affine space (intersected with the constraints) ? \n- First of all the results are only asymptotic. (I agree that it can be mitigated saying that there is (almost) no results on non-monotone VI and it is a first step to try to handle non-convexity of the objective functions.)\n- One big pro of this work might have been new proof techniques to handle non-monotonicity in variational inequalities but the coherence assumption looks like to be the weakest condition to use the standard proof technique of convergence of the (MD) and (OMD). Nevertheless, this work is still interesting since it handles in a subtle way stochasticity (I did not have time to check Theorem 2.18 [Hall & Heyde 1980], I would be good to repeat it in the appendix for self-completeness)\n- This work could be easily extended to non zero-sum games which is crucial in practice since most of the state of the art GANs (such as WGAN with gradient penalty or non saturating GAN) are non zero-sum games. \n- Are you sure of the use of the denomination Optimistic mirror descent ? What you are presenting is the extragradient method. These two methods are slightly different, If you look at (5) in (Daskalaki et al., 2018) you'll notice that the updates are slightly different from you (OMD), particularly (OMD) require two gradient computations per iteration whereas (5) in (Daskalaki et al., 2018) requires only one. (it just requires to memorize the previous gradient)\n\nMinor comment: \n- For saddle point (and more generally variational inequalities) Mirror descent is no longer a descent algorithm. The name used by the literature is mirror-prox method (see Juditsky's paper) \n- in (C.1) U_n is not defined anywhere but I guess it is $\\hat g_n - g(X_n)$.\n- Some cited paper are published paper but cited as arXiv paper. \n- Lemma D.1 could be extended to the case (\\sigma \\neq 0) but the additional noise term might be hard to handle to get a result similar as Thm 4.1\nfor $\\sigma \\neq 0$.", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper626/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Optimistic mirror descent in saddle-point problems: Going the extra (gradient) mile", "abstract": "Owing to their connection with generative adversarial networks (GANs), saddle-point problems have recently attracted considerable interest in machine learning and beyond. By necessity, most theoretical guarantees revolve around convex-concave (or even linear) problems; however, making theoretical inroads towards efficient GAN training depends crucially on moving beyond this classic framework. To make piecemeal progress along these lines, we analyze the behavior of mirror descent (MD) in a class of non-monotone problems whose solutions coincide with those of a naturally associated variational inequality \u2013 a property which we call coherence. We first show that ordinary, \u201cvanilla\u201d MD converges under a strict version of this condition, but not otherwise; in particular, it may fail to converge even in bilinear models with a unique solution. We then show that this deficiency is mitigated by optimism: by taking an \u201cextra-gradient\u201d step, optimistic mirror descent (OMD) converges in all coherent problems. Our analysis generalizes and extends the results of Daskalakis et al. [2018] for optimistic gradient descent (OGD) in bilinear problems, and makes concrete headway for provable convergence beyond convex-concave games. We also provide stochastic analogues of these results, and we validate our analysis by numerical experiments in a wide array of GAN models (including Gaussian mixture models, and the CelebA and CIFAR-10 datasets).", "keywords": ["Mirror descent", "extra-gradient", "generative adversarial networks", "saddle-point problems"], "authorids": ["panayotis.mertikopoulos@imag.fr", "bruno_lecouat@i2r.a-star.edu.sg", "houssam_zenati@i2r.a-star.edu.sg", "foocs@i2r.a-star.edu.sg", "vijay@i2r.a-star.edu.sg", "georgios@sutd.edu.sg"], "authors": ["Panayotis Mertikopoulos", "Bruno Lecouat", "Houssam Zenati", "Chuan-Sheng Foo", "Vijay Chandrasekhar", "Georgios Piliouras"], "TL;DR": "We show how the inclusion of an extra-gradient step in first-order GAN training methods can improve stability and lead to improved convergence results.", "pdf": "/pdf/e68620ae4c38287559cc1c2f04ff57bf59833d07.pdf", "paperhash": "mertikopoulos|optimistic_mirror_descent_in_saddlepoint_problems_going_the_extra_gradient_mile", "_bibtex": "@inproceedings{\nmertikopoulos2018optimistic,\ntitle={Optimistic mirror descent in saddle-point problems: Going the extra(-gradient) mile},\nauthor={Panayotis Mertikopoulos and Bruno Lecouat and Houssam Zenati and Chuan-Sheng Foo and Vijay Chandrasekhar and Georgios Piliouras},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Bkg8jjC9KQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper626/Official_Review", "cdate": 1542234416703, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "Bkg8jjC9KQ", "replyto": "Bkg8jjC9KQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper626/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335766794, "tmdate": 1552335766794, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper626/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "SyeTm7oIhm", "original": null, "number": 1, "cdate": 1540956965438, "ddate": null, "tcdate": 1540956965438, "tmdate": 1541533827850, "tddate": null, "forum": "Bkg8jjC9KQ", "replyto": "Bkg8jjC9KQ", "invitation": "ICLR.cc/2019/Conference/-/Paper626/Official_Review", "content": {"title": "Coherent condition is highly related to the pseudo-monotone property in operator theory. ", "review": "Prons: \nThis paper provides an optimistic mirror descent algorithm to solving minmax optimization problem. Its global convergence is guaranteed under the coherence property. The experimental results are promising.\n\nCons: \n1.\tThe coherence property is still a strong assumption. The sufficient conditions provided in Corollary 3.2 and 3.3 to guarantee coherence property are too specific to cover existing GAN models.         \n\n2.\tThe current theoretical contribution seems incrementally. From the perspective of operator theory, the coherence property is highly related to the pseudo-monotone property. Extragradient method to solve the pseudo-monotone VIP has already existed in the literature [1]. The proposed OMD can be simply regarded a stochastic extension of [1] and simultaneously generalize the European distance in [1] to Bregman distance. \n\n3.\tThe integrating of Adam and OMD in the experiments is very interesting. To match the experiments, we highly recommend the authors to show the convergence of OMD + Adam with or without coherence condition, rather than requiring a diminishing learning rate.\n\n[1] Noor, Muhammad Aslam, et al. \"Extragradient methods for solving nonconvex variational inequalities.\" Journal of Computational and Applied Mathematics 235.9 (2011): 3104-3108.\n", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper626/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Optimistic mirror descent in saddle-point problems: Going the extra (gradient) mile", "abstract": "Owing to their connection with generative adversarial networks (GANs), saddle-point problems have recently attracted considerable interest in machine learning and beyond. By necessity, most theoretical guarantees revolve around convex-concave (or even linear) problems; however, making theoretical inroads towards efficient GAN training depends crucially on moving beyond this classic framework. To make piecemeal progress along these lines, we analyze the behavior of mirror descent (MD) in a class of non-monotone problems whose solutions coincide with those of a naturally associated variational inequality \u2013 a property which we call coherence. We first show that ordinary, \u201cvanilla\u201d MD converges under a strict version of this condition, but not otherwise; in particular, it may fail to converge even in bilinear models with a unique solution. We then show that this deficiency is mitigated by optimism: by taking an \u201cextra-gradient\u201d step, optimistic mirror descent (OMD) converges in all coherent problems. Our analysis generalizes and extends the results of Daskalakis et al. [2018] for optimistic gradient descent (OGD) in bilinear problems, and makes concrete headway for provable convergence beyond convex-concave games. We also provide stochastic analogues of these results, and we validate our analysis by numerical experiments in a wide array of GAN models (including Gaussian mixture models, and the CelebA and CIFAR-10 datasets).", "keywords": ["Mirror descent", "extra-gradient", "generative adversarial networks", "saddle-point problems"], "authorids": ["panayotis.mertikopoulos@imag.fr", "bruno_lecouat@i2r.a-star.edu.sg", "houssam_zenati@i2r.a-star.edu.sg", "foocs@i2r.a-star.edu.sg", "vijay@i2r.a-star.edu.sg", "georgios@sutd.edu.sg"], "authors": ["Panayotis Mertikopoulos", "Bruno Lecouat", "Houssam Zenati", "Chuan-Sheng Foo", "Vijay Chandrasekhar", "Georgios Piliouras"], "TL;DR": "We show how the inclusion of an extra-gradient step in first-order GAN training methods can improve stability and lead to improved convergence results.", "pdf": "/pdf/e68620ae4c38287559cc1c2f04ff57bf59833d07.pdf", "paperhash": "mertikopoulos|optimistic_mirror_descent_in_saddlepoint_problems_going_the_extra_gradient_mile", "_bibtex": "@inproceedings{\nmertikopoulos2018optimistic,\ntitle={Optimistic mirror descent in saddle-point problems: Going the extra(-gradient) mile},\nauthor={Panayotis Mertikopoulos and Bruno Lecouat and Houssam Zenati and Chuan-Sheng Foo and Vijay Chandrasekhar and Georgios Piliouras},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Bkg8jjC9KQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper626/Official_Review", "cdate": 1542234416703, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "Bkg8jjC9KQ", "replyto": "Bkg8jjC9KQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper626/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335766794, "tmdate": 1552335766794, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper626/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 12}