{"notes": [{"id": "rJedV3R5tm", "original": "B1xaOP8FtQ", "number": 1462, "cdate": 1538087983522, "ddate": null, "tcdate": 1538087983522, "tmdate": 1558024137911, "tddate": null, "forum": "rJedV3R5tm", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "RelGAN: Relational Generative Adversarial Networks for Text Generation", "abstract": "Generative adversarial networks (GANs) have achieved great success at generating realistic images. However, the text generation still remains a challenging task for modern GAN architectures. In this work, we propose RelGAN, a new GAN architecture for text generation, consisting of three main components: a relational memory based generator for the long-distance dependency modeling, the Gumbel-Softmax relaxation for training GANs on discrete data, and multiple embedded representations in the discriminator to provide a more informative signal for the generator updates. Our experiments show that RelGAN outperforms current state-of-the-art models in terms of sample quality and diversity, and we also reveal via ablation studies that each component of RelGAN contributes critically to its performance improvements. Moreover, a key advantage of our method, that distinguishes it from other GANs, is the ability to control the trade-off between sample quality and diversity via the use of a single adjustable parameter. Finally, RelGAN is the first architecture that makes GANs with Gumbel-Softmax relaxation succeed in generating realistic text.", "keywords": ["RelGAN", "text generation", "relational memory", "Gumbel-Softmax relaxation", "multiple embedded representations"], "authorids": ["wn8@rice.edu", "nnarodytska@vmware.com", "abp4@rice.edu"], "authors": ["Weili Nie", "Nina Narodytska", "Ankit Patel"], "pdf": "/pdf/28ff6712d62fef4d4846fca5be5df06a8ffd41d2.pdf", "paperhash": "nie|relgan_relational_generative_adversarial_networks_for_text_generation", "_bibtex": "@inproceedings{\nnie2018relgan,\ntitle={Rel{GAN}: Relational Generative Adversarial Networks for Text Generation},\nauthor={Weili Nie and Nina Narodytska and Ankit Patel},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJedV3R5tm},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "H1xW5Lt1pm", "original": null, "number": 1, "cdate": 1541539465103, "ddate": null, "tcdate": 1541539465103, "tmdate": 1545354502932, "tddate": null, "forum": "rJedV3R5tm", "replyto": "rJedV3R5tm", "invitation": "ICLR.cc/2019/Conference/-/Paper1462/Meta_Review", "content": {"metareview": "\npros:\n- well-written and clear\n- good evaluation with convincing ablations\n- moderately novel\n\ncons:\n- Reviewers 1 and 3 feel the paper is somewhat incremental over previous work, combining previously proposed ideas.\n\n(Reviewer 2 originally had concerns about the testing methodology but feels that the paper has improved in revision)\n(Reviewer 3 suggests an additional comparison to related work which was addressed in revision)\n\nI appreciate the authors' revisions and engagement during the discussion period.  Overall the paper is good and I'm recommending acceptance.", "confidence": "3: The area chair is somewhat confident", "recommendation": "Accept (Poster)", "title": "Good paper; a little incremental"}, "signatures": ["ICLR.cc/2019/Conference/Paper1462/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper1462/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "RelGAN: Relational Generative Adversarial Networks for Text Generation", "abstract": "Generative adversarial networks (GANs) have achieved great success at generating realistic images. However, the text generation still remains a challenging task for modern GAN architectures. In this work, we propose RelGAN, a new GAN architecture for text generation, consisting of three main components: a relational memory based generator for the long-distance dependency modeling, the Gumbel-Softmax relaxation for training GANs on discrete data, and multiple embedded representations in the discriminator to provide a more informative signal for the generator updates. Our experiments show that RelGAN outperforms current state-of-the-art models in terms of sample quality and diversity, and we also reveal via ablation studies that each component of RelGAN contributes critically to its performance improvements. Moreover, a key advantage of our method, that distinguishes it from other GANs, is the ability to control the trade-off between sample quality and diversity via the use of a single adjustable parameter. Finally, RelGAN is the first architecture that makes GANs with Gumbel-Softmax relaxation succeed in generating realistic text.", "keywords": ["RelGAN", "text generation", "relational memory", "Gumbel-Softmax relaxation", "multiple embedded representations"], "authorids": ["wn8@rice.edu", "nnarodytska@vmware.com", "abp4@rice.edu"], "authors": ["Weili Nie", "Nina Narodytska", "Ankit Patel"], "pdf": "/pdf/28ff6712d62fef4d4846fca5be5df06a8ffd41d2.pdf", "paperhash": "nie|relgan_relational_generative_adversarial_networks_for_text_generation", "_bibtex": "@inproceedings{\nnie2018relgan,\ntitle={Rel{GAN}: Relational Generative Adversarial Networks for Text Generation},\nauthor={Weili Nie and Nina Narodytska and Ankit Patel},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJedV3R5tm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1462/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545352830894, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJedV3R5tm", "replyto": "rJedV3R5tm", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1462/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper1462/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1462/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545352830894}}}, {"id": "HJxbxO4cn7", "original": null, "number": 1, "cdate": 1541191657107, "ddate": null, "tcdate": 1541191657107, "tmdate": 1544993933143, "tddate": null, "forum": "rJedV3R5tm", "replyto": "rJedV3R5tm", "invitation": "ICLR.cc/2019/Conference/-/Paper1462/Official_Review", "content": {"title": "interesting idea and experiments well-executed", "review": "==========================\nI have read the authors' response and other reviewers' comments. Thanks the authors for taking great effort in answering my questions. Generally, I feel satisfied with the repsonse, and prefer an acceptance recommendation. \n==========================\nContributions:\n\nThe main contribution of this paper is the proposed RelGAN. First, instead of using a standard LSTM as generator, the authors propose using a relational memory based generator. Second, instead of using a single CNN as discriminator, the authors use multiple embedded representations. Third, Gumbel-softmax relaxation is also used for training GANs on discrete textual data. The authors also claim the proposed model has the ability to control the trade-off between sample quality and diversity via a single adjustable parameter. \n\nDetailed comments:\n\n(1) Novelty: This paper is not a breakthrough paper, mainly following previous work and propose new designs to improve the performance. However, it still contains some novelty inside, for example, the model choice of the generator and discriminator. I think the observation that the temperature control used in the Gumbel-softmax can reflect the trade-off between quality and diversity is interesting. \n\nHowever, I feel the claim in the last sentence of the abstract and introduction is a little bit strong. Though this paper seems to be the first to really use Gumbel-softmax for text generation, similar techniques like using annealed softmax to approximate argmax has already been used in previous work (Zhang et al., 2017). Since this is similar to Gumbel-softmax, I think this may need additional one or two sentences to clarify this for more careful discussion.  \n\nFurther, I would also recommend the authors discuss the following paper [a] to make this work more comprehensive as to the discussion of related work. [a] also uses annealed softmax approximation, and also divide the GAN approaches as RL-based and RL-free, similar in spirit as the discuss in this paper. \n\n[a] Adversarial Text Generation via Feature-Mover's Distance, NIPS 2018.\n\n(2) Presentation: This paper is carefully written and easy to follow. I enjoyed reading the paper. \n\n(3) Evaluation: Experiments are generally well-executed, with ablation study also provided. However, human evaluation is lacked, which I think is essential for this line of work. I have a few questions listed below. \n\nQuestions:\n\n(1) In section 2.4, it mentions that the generator needs pre-training. So, my question is: does the discriminator also need pre-training? If so, how the discriminator is pre-trained?\n\n(2) In Table 1 & 2 & 3, how does your model compare with MaskGAN? If this can be provided, it would be better. \n\n(3) Instead of using NLL_{gen}, a natural question is: what are the self-BLEU score results since it was used in previous work?\n\n(4) The \\beta_max value used in the synthetic and real datasets is quite different. For example, \\beta_max = 1 or 2 in synthetic data, while \\beta_max = 100 or 1000 is used in real data. What is the observation here? Can the authors provide some insights into this?\n\n(5) I feel Figure 3 is interesting. As the authors noted, NLL_gen measures diversity, NLL_oracle measures quality. Looking at Figure 3, does this mean GAN model produces higher quality samples than MLE pretrained models, while GAN models also produces less diverse samples than MLE models? This is due to NLL_gen increases after pretraining, while NLL_oracle further decreases after pretraining. However, this conclusion also seems strange. Can the authors provide some discussion on this? \n\n(6) Can human evaluation be performed since automatic metrics are not reliable enough?\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1462/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "RelGAN: Relational Generative Adversarial Networks for Text Generation", "abstract": "Generative adversarial networks (GANs) have achieved great success at generating realistic images. However, the text generation still remains a challenging task for modern GAN architectures. In this work, we propose RelGAN, a new GAN architecture for text generation, consisting of three main components: a relational memory based generator for the long-distance dependency modeling, the Gumbel-Softmax relaxation for training GANs on discrete data, and multiple embedded representations in the discriminator to provide a more informative signal for the generator updates. Our experiments show that RelGAN outperforms current state-of-the-art models in terms of sample quality and diversity, and we also reveal via ablation studies that each component of RelGAN contributes critically to its performance improvements. Moreover, a key advantage of our method, that distinguishes it from other GANs, is the ability to control the trade-off between sample quality and diversity via the use of a single adjustable parameter. Finally, RelGAN is the first architecture that makes GANs with Gumbel-Softmax relaxation succeed in generating realistic text.", "keywords": ["RelGAN", "text generation", "relational memory", "Gumbel-Softmax relaxation", "multiple embedded representations"], "authorids": ["wn8@rice.edu", "nnarodytska@vmware.com", "abp4@rice.edu"], "authors": ["Weili Nie", "Nina Narodytska", "Ankit Patel"], "pdf": "/pdf/28ff6712d62fef4d4846fca5be5df06a8ffd41d2.pdf", "paperhash": "nie|relgan_relational_generative_adversarial_networks_for_text_generation", "_bibtex": "@inproceedings{\nnie2018relgan,\ntitle={Rel{GAN}: Relational Generative Adversarial Networks for Text Generation},\nauthor={Weili Nie and Nina Narodytska and Ankit Patel},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJedV3R5tm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1462/Official_Review", "cdate": 1542234224623, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "rJedV3R5tm", "replyto": "rJedV3R5tm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1462/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335951793, "tmdate": 1552335951793, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1462/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "HkexOxGYa7", "original": null, "number": 6, "cdate": 1542164583949, "ddate": null, "tcdate": 1542164583949, "tmdate": 1542164583949, "tddate": null, "forum": "rJedV3R5tm", "replyto": "S1li5OZtpQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1462/Official_Comment", "content": {"title": "Good Improvement", "comment": "The changes addressed my concerns and the score is now improved from 6 to 8.\n\nFor \"teacher-forcing\": I misread MLE as the same as MaliGAN, given the name similarities (\"maximum likelihood\"). Thanks for the clarification."}, "signatures": ["ICLR.cc/2019/Conference/Paper1462/AnonReviewer2"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1462/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1462/AnonReviewer2", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "RelGAN: Relational Generative Adversarial Networks for Text Generation", "abstract": "Generative adversarial networks (GANs) have achieved great success at generating realistic images. However, the text generation still remains a challenging task for modern GAN architectures. In this work, we propose RelGAN, a new GAN architecture for text generation, consisting of three main components: a relational memory based generator for the long-distance dependency modeling, the Gumbel-Softmax relaxation for training GANs on discrete data, and multiple embedded representations in the discriminator to provide a more informative signal for the generator updates. Our experiments show that RelGAN outperforms current state-of-the-art models in terms of sample quality and diversity, and we also reveal via ablation studies that each component of RelGAN contributes critically to its performance improvements. Moreover, a key advantage of our method, that distinguishes it from other GANs, is the ability to control the trade-off between sample quality and diversity via the use of a single adjustable parameter. Finally, RelGAN is the first architecture that makes GANs with Gumbel-Softmax relaxation succeed in generating realistic text.", "keywords": ["RelGAN", "text generation", "relational memory", "Gumbel-Softmax relaxation", "multiple embedded representations"], "authorids": ["wn8@rice.edu", "nnarodytska@vmware.com", "abp4@rice.edu"], "authors": ["Weili Nie", "Nina Narodytska", "Ankit Patel"], "pdf": "/pdf/28ff6712d62fef4d4846fca5be5df06a8ffd41d2.pdf", "paperhash": "nie|relgan_relational_generative_adversarial_networks_for_text_generation", "_bibtex": "@inproceedings{\nnie2018relgan,\ntitle={Rel{GAN}: Relational Generative Adversarial Networks for Text Generation},\nauthor={Weili Nie and Nina Narodytska and Ankit Patel},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJedV3R5tm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1462/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621620527, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJedV3R5tm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1462/Authors", "ICLR.cc/2019/Conference/Paper1462/Reviewers", "ICLR.cc/2019/Conference/Paper1462/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1462/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1462/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1462/Authors|ICLR.cc/2019/Conference/Paper1462/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1462/Reviewers", "ICLR.cc/2019/Conference/Paper1462/Authors", "ICLR.cc/2019/Conference/Paper1462/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621620527}}}, {"id": "B1gqWjV5hX", "original": null, "number": 2, "cdate": 1541192450098, "ddate": null, "tcdate": 1541192450098, "tmdate": 1542164517237, "tddate": null, "forum": "rJedV3R5tm", "replyto": "rJedV3R5tm", "invitation": "ICLR.cc/2019/Conference/-/Paper1462/Official_Review", "content": {"title": "Good Paper", "review": "Update: the authors' response and changes to the paper properly addressed the concerns below. Therefore the score was improved from 6 to 8.\n\n----\n\n\nThe paper makes several contributions: 1. it extends GAN to text via Gumbel-softmax relaxation, which seems more effective than the other approaches using REINFORCE or maximum likelihood principle. 2. It shows that using relational memory for LSTM gives better results. 3. Ablation study on the necessity of the relational memory, the relaxation parameter and multi-embedding in the discriminator is performed.\n\nThe paper's ideas are novel and good in general, and would make a good contribution to ICLR 2019. However, there are a few things in need of improvement before it is suite for publication. I am willing to improve the scores if the following comments are properly addressed.\n\nFirst of all, the paper does not compare with recurrent networks trained using only the \"teacher-forcing\" algorithm without using GAN. This means that at a high level, the paper is insufficient to show that GAN is necessary for text generation at all. That said, since almost every other text GAN paper also failed to do this, and the paper's contribution on using Gumbel-softmax relaxation and the relational memory is novel, I did not get too harsh on the scoring because of this.\n\nSecondly, whether using BLEU on the entire testing dataset is a good idea for benchmarking is controversial. If the testing data is too large, it could be easily saturated. On the other hand, if the testing data is small, it may not be sufficient to capture the quality well. I did not hold the authors responsible on this either, because it was used in previously published results. However, the paper did propose to use an oracle, and it might be a good idea to use a \"teacher-forcing\" trained RNN anyways since it is necessary to show whether GAN is a good idea for text generation to begin with (see the previous comment).\n\nA third comment is that I had wished the paper did more exploration on the relaxation parameter \\beta. Ideally, if \\beta is too large, the output would be too skewed towards a one-hot vector such that instability in the gradients occurs. On the other hand, if \\beta is too small, the output might not be close enough to one-hot vectors to make the discriminator focus on textual differences rather than numerical differences (i.e., between a continuous and a one-hot vector). It would make sense for the paper to show both ends of these failing cases, which is not apparent with only 2 hyper-parameter choices.\n\nFinally, the first paragraph in section 2.2.2 suggests that the gap between discrete and continuous outputs is the reason for mode collapsing. This is false. For image generation, when all the outputs are continuous, there is still mode collapsing happening with GANs. The authors could say that the discrete-continuous gap contributes to mode-collapsing, but this is not too good either because it will require the paper to conduct experiments beyond text generation to show this. Authors should make changes here.", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1462/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "RelGAN: Relational Generative Adversarial Networks for Text Generation", "abstract": "Generative adversarial networks (GANs) have achieved great success at generating realistic images. However, the text generation still remains a challenging task for modern GAN architectures. In this work, we propose RelGAN, a new GAN architecture for text generation, consisting of three main components: a relational memory based generator for the long-distance dependency modeling, the Gumbel-Softmax relaxation for training GANs on discrete data, and multiple embedded representations in the discriminator to provide a more informative signal for the generator updates. Our experiments show that RelGAN outperforms current state-of-the-art models in terms of sample quality and diversity, and we also reveal via ablation studies that each component of RelGAN contributes critically to its performance improvements. Moreover, a key advantage of our method, that distinguishes it from other GANs, is the ability to control the trade-off between sample quality and diversity via the use of a single adjustable parameter. Finally, RelGAN is the first architecture that makes GANs with Gumbel-Softmax relaxation succeed in generating realistic text.", "keywords": ["RelGAN", "text generation", "relational memory", "Gumbel-Softmax relaxation", "multiple embedded representations"], "authorids": ["wn8@rice.edu", "nnarodytska@vmware.com", "abp4@rice.edu"], "authors": ["Weili Nie", "Nina Narodytska", "Ankit Patel"], "pdf": "/pdf/28ff6712d62fef4d4846fca5be5df06a8ffd41d2.pdf", "paperhash": "nie|relgan_relational_generative_adversarial_networks_for_text_generation", "_bibtex": "@inproceedings{\nnie2018relgan,\ntitle={Rel{GAN}: Relational Generative Adversarial Networks for Text Generation},\nauthor={Weili Nie and Nina Narodytska and Ankit Patel},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJedV3R5tm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1462/Official_Review", "cdate": 1542234224623, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "rJedV3R5tm", "replyto": "rJedV3R5tm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1462/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335951793, "tmdate": 1552335951793, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1462/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "HkeOU5ZYa7", "original": null, "number": 5, "cdate": 1542163024474, "ddate": null, "tcdate": 1542163024474, "tmdate": 1542163024474, "tddate": null, "forum": "rJedV3R5tm", "replyto": "ryxH-q-FpQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1462/Official_Comment", "content": {"title": "Reply to Reviewer3 (Part 2) ", "comment": "5. \u201cDiscuss the phenomenon -- NLL_gen increases after pre-training, while NLL_oracle decreases after pretraining in Figure 3?\u201d\n\nThis phenomenon is not unique in RelGAN (with a small \\beta_max in particular). In general, GANs can generate text with better quality but are also more likely to suffer from mode collapse than the MLE trained counterparts on text generation, as has been shown in many previous works, such as [2,3,4]. This is why we have always seen a decrease in the NLL_oracle score (better sample quality) together with an increase of the NLL_gen score (worse sample diversity) while training current GANs (not only RelGAN with a small \\beta_max) on the synthetic data. What makes RelGAN different from other GANs is that we can control the trade-off between sample quality and diversity with a single tunable hyperparameter. Tables 2 & 3 showed that if \\beta_max is tuned properly, we can make the sample diversity of RelGAN to be very close to (or even better than) that of the MLE pre-trained LSTMs, while at the same time achieving much better sample quality.  \n\n6. \u201cCan human evaluation be performed since automatic metrics are not reliable enough?\u201d\n\nThanks for the suggestion! We have done human evaluation for the EMNLP2017 WMT News dataset on Amazon Mechanical Turk, where the evaluation criteria details are provided in Table 5 (Appendix B.2 in the revised version of the paper) and the results are shown in Table 4 (Section 3.3 in the revised version of the paper). We can see the human scores in Table 4 also prefer RelGAN to other GANs and the MLE baseline models.\n\nPlease let us know if we have addressed your concerns and if you have further comments. \n\n\n[1] Lu et al., \u201cNeural Text Generation: Past, Present and Beyond.\u201d arXiv preprint arXiv:1803.07133.\n[2] Fedus et al., \u201cMaskGAN: Better Text Generation via Filling in the _.\u201d  in ICLR 2018.\n[3] Zhu et al., \u201cTexygen: A Benchmarking Platform for Text Generation Models.\u201d in SIGIR 2018. \n[4] Semeniuta et al., \u201cOn accurate evaluation of gans for language generation.\u201d arXiv preprint arXiv:1806.04936, 2018.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1462/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1462/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1462/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "RelGAN: Relational Generative Adversarial Networks for Text Generation", "abstract": "Generative adversarial networks (GANs) have achieved great success at generating realistic images. However, the text generation still remains a challenging task for modern GAN architectures. In this work, we propose RelGAN, a new GAN architecture for text generation, consisting of three main components: a relational memory based generator for the long-distance dependency modeling, the Gumbel-Softmax relaxation for training GANs on discrete data, and multiple embedded representations in the discriminator to provide a more informative signal for the generator updates. Our experiments show that RelGAN outperforms current state-of-the-art models in terms of sample quality and diversity, and we also reveal via ablation studies that each component of RelGAN contributes critically to its performance improvements. Moreover, a key advantage of our method, that distinguishes it from other GANs, is the ability to control the trade-off between sample quality and diversity via the use of a single adjustable parameter. Finally, RelGAN is the first architecture that makes GANs with Gumbel-Softmax relaxation succeed in generating realistic text.", "keywords": ["RelGAN", "text generation", "relational memory", "Gumbel-Softmax relaxation", "multiple embedded representations"], "authorids": ["wn8@rice.edu", "nnarodytska@vmware.com", "abp4@rice.edu"], "authors": ["Weili Nie", "Nina Narodytska", "Ankit Patel"], "pdf": "/pdf/28ff6712d62fef4d4846fca5be5df06a8ffd41d2.pdf", "paperhash": "nie|relgan_relational_generative_adversarial_networks_for_text_generation", "_bibtex": "@inproceedings{\nnie2018relgan,\ntitle={Rel{GAN}: Relational Generative Adversarial Networks for Text Generation},\nauthor={Weili Nie and Nina Narodytska and Ankit Patel},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJedV3R5tm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1462/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621620527, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJedV3R5tm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1462/Authors", "ICLR.cc/2019/Conference/Paper1462/Reviewers", "ICLR.cc/2019/Conference/Paper1462/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1462/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1462/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1462/Authors|ICLR.cc/2019/Conference/Paper1462/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1462/Reviewers", "ICLR.cc/2019/Conference/Paper1462/Authors", "ICLR.cc/2019/Conference/Paper1462/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621620527}}}, {"id": "ryxH-q-FpQ", "original": null, "number": 4, "cdate": 1542162940763, "ddate": null, "tcdate": 1542162940763, "tmdate": 1542162981196, "tddate": null, "forum": "rJedV3R5tm", "replyto": "HJxbxO4cn7", "invitation": "ICLR.cc/2019/Conference/-/Paper1462/Official_Comment", "content": {"title": "Reply to Reviewer3 (Part 1)", "comment": "Thank you very much for your review and helpful comments. We address your specific questions and comments below:\n\n*Related Work*\n1. TextGAN: Thanks for the reviewer\u2019s suggestion! We have provided a more detailed discussion in the \u201cRelated Work\u201d Section to clarify the difference between RelGAN and TextGAN (Zhang et al., 2017) in dealing with the non-differentiability issue.\n\n2. FM-GAN: Thanks for pointing out this recent paper. We have also added a discussion of this paper in the \u201cRelated Work\u201d Section.\n\n*Questions*\n1. \u201cDoes discriminator need pre-training?\u201d\n\n No, the discriminator in RelGAN does not need pre-training.\n\n2. \u201cHow does RelGAN compared to MaskGAN?\u201d\n\n[1] has recently showed that MaskGAN has much lower BLEU scores compared to our baseline models (MLE, SeqGAN, RankGAN, LeakGAN), where the evaluation settings in [1] are the same with our work. Thus, MaskGAN also performs worse than RelGAN in terms of BLEU scores. \n\n3. \u201cWhat are the self-BLEU score results since it was used in previous work?\u201d\n\nA short answer: We did evaluate all methods using our implemented self-BLEU scores. However, we found that they may not be suitable for evaluating sample diversity. Also, we found an issue in the implementation of self-BLEU on the open-source Texygen platform that was used in prior work. We are currently in contact with authors of Texygen regarding the issue. Before we reach an agreement, we think it would be better not to use self-BLEU scores.\n\nMore details:\nAs far as we know, self-BLEU scores are first proposed by the authors of the Texygen benchmarking platform to evaluate diversity, where the basic idea is to calculate the BLEU scores by choosing each sentence in the set of generated sentences as hypothesis and the others as reference, and then take an average of BLEU scores over all the generated sentences. However, when looking into the implementation of self-BLEU scores: https://github.com/geek-ai/Texygen/blob/master/utils/metrics/SelfBleu.py, we found a severe issue inside for evaluating self-BLEU over training: Only in the first time of evaluation that the reference and hypothesis come from the same \u201ctest data\u201d (i.e. the whole set of generated sentences). After that, the hypothesis keeps updated but the reference remains unchanged (due to \u201cself.is_first=False\u201d), which means hypothesis and reference are not from the same \u201ctest data\u201d any more, and thus the scores obtained under this implementation is not self-BLEU scores! \n\nTo this end, we modified their implementation to make sure that the hypothesis and reference are always from the same \u201ctest data\u201d and found that the self-BLEU (2-5) scores are always 1 when evaluating all the models (MLE, SeqGAN, RankGAN, LeakGAN and RelGAN). Also as inspired by the Reviewer2\u2019s comments, we tried to reduce the number of the \u201ctest data\u201d by applying a small portion of the whole generated data as reference and still get 1 for the self-BLEU scores (even for the portion 5%). Therefore, if we understand correctly, self-BLEU scores may not be suitable for evaluating sample diversity. \n\n4.  \u201cWhy is the \\beta_max value used in the synthetic and real datasets quite different?\u201d\n\nAs there is a trade-off between sample diversity and quality in RelGAN, controlled by the tunable parameter \\beta_max in both synthetic and real data, we can adjust \\beta_max depending on different evaluation goals. In the synthetic data experiments, the goal is to show that RelGAN could outperform other models in terms of NLL_oracle (i.e. sample quality) regardless of the sample diversity. Thus, we chose very small \\beta_max (1 or 2). In the real data experiments, however, the goal was to show that RelGAN could generate real text with both high quality and better diversity. Thus, we chose some intermediate values of \\beta-max (100 or 1000) to find a good trade-off where both BLEU and NLL_gen scores can outperform other models. If instead we had chosen \\beta_max=1, we would get significantly higher BLEU scores (as shown in Table 7 of Appendix H in the revised version of the paper), but at the cost of worse NLL_gen scores than other models (which would be in conflict with the main goal of the real data experiments).\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1462/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1462/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1462/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "RelGAN: Relational Generative Adversarial Networks for Text Generation", "abstract": "Generative adversarial networks (GANs) have achieved great success at generating realistic images. However, the text generation still remains a challenging task for modern GAN architectures. In this work, we propose RelGAN, a new GAN architecture for text generation, consisting of three main components: a relational memory based generator for the long-distance dependency modeling, the Gumbel-Softmax relaxation for training GANs on discrete data, and multiple embedded representations in the discriminator to provide a more informative signal for the generator updates. Our experiments show that RelGAN outperforms current state-of-the-art models in terms of sample quality and diversity, and we also reveal via ablation studies that each component of RelGAN contributes critically to its performance improvements. Moreover, a key advantage of our method, that distinguishes it from other GANs, is the ability to control the trade-off between sample quality and diversity via the use of a single adjustable parameter. Finally, RelGAN is the first architecture that makes GANs with Gumbel-Softmax relaxation succeed in generating realistic text.", "keywords": ["RelGAN", "text generation", "relational memory", "Gumbel-Softmax relaxation", "multiple embedded representations"], "authorids": ["wn8@rice.edu", "nnarodytska@vmware.com", "abp4@rice.edu"], "authors": ["Weili Nie", "Nina Narodytska", "Ankit Patel"], "pdf": "/pdf/28ff6712d62fef4d4846fca5be5df06a8ffd41d2.pdf", "paperhash": "nie|relgan_relational_generative_adversarial_networks_for_text_generation", "_bibtex": "@inproceedings{\nnie2018relgan,\ntitle={Rel{GAN}: Relational Generative Adversarial Networks for Text Generation},\nauthor={Weili Nie and Nina Narodytska and Ankit Patel},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJedV3R5tm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1462/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621620527, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJedV3R5tm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1462/Authors", "ICLR.cc/2019/Conference/Paper1462/Reviewers", "ICLR.cc/2019/Conference/Paper1462/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1462/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1462/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1462/Authors|ICLR.cc/2019/Conference/Paper1462/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1462/Reviewers", "ICLR.cc/2019/Conference/Paper1462/Authors", "ICLR.cc/2019/Conference/Paper1462/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621620527}}}, {"id": "HyeNJtWtaQ", "original": null, "number": 3, "cdate": 1542162651605, "ddate": null, "tcdate": 1542162651605, "tmdate": 1542162651605, "tddate": null, "forum": "rJedV3R5tm", "replyto": "S1li5OZtpQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1462/Official_Comment", "content": {"title": "Reply to Reviewer2 (Part 2)", "comment": "3. \u201cIt would make sense for the paper to show both ends of these failing cases with the exploration on more values of \\beta_max.\u201d \n\nThank you for the suggestion. We have added Appendix H to explore the impact of different inverse temperature \\beta_max in RelGAN, especially where the failing cases at the two extremes are discussed. \n\nFirst, our results confirm that, similar to the synthetic data experiments, there also exists a consistent trade-off between sample quality and diversity in the real data, controlled by the tunable hyperparameter \\beta_max. It also reveals failing cases at the two extremes: On the one hand, if \\beta_max is too small, RelGAN suffers from severe mode collapse (large NLL_gen scores) and training instability (high variance of scores). On the other hand, if \\beta_max is too large, the sample quality improvement of RelGAN becomes marginal (low BLEU scores). Therefore, in the main text we have chosen the two intermediate values, \\beta_max = 100 & 1000, to show the advantages of RelGAN over other models, while still demonstrating its ability to control the trade-off between sample quality and diversity.\n\n4. \u201cThe first paragraph in section 2.2.2 in terms of describing mode collapse is misleading.\u201d\n\nWe agree that the last sentence of the first paragraph in Section 2.2.2 is a little bit misleading. We appreciate the reviewer\u2019s suggestion and have changed it to \u201cIntuitively, this might be one factor that contributes to mode collapse in RelGAN on text generation.\u201d, which we believe avoids making an argument that applies to mode collapse in general GANs on image generation.\n\nPlease let us know if we have addressed your concerns and if you have further comments. \n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1462/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1462/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1462/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "RelGAN: Relational Generative Adversarial Networks for Text Generation", "abstract": "Generative adversarial networks (GANs) have achieved great success at generating realistic images. However, the text generation still remains a challenging task for modern GAN architectures. In this work, we propose RelGAN, a new GAN architecture for text generation, consisting of three main components: a relational memory based generator for the long-distance dependency modeling, the Gumbel-Softmax relaxation for training GANs on discrete data, and multiple embedded representations in the discriminator to provide a more informative signal for the generator updates. Our experiments show that RelGAN outperforms current state-of-the-art models in terms of sample quality and diversity, and we also reveal via ablation studies that each component of RelGAN contributes critically to its performance improvements. Moreover, a key advantage of our method, that distinguishes it from other GANs, is the ability to control the trade-off between sample quality and diversity via the use of a single adjustable parameter. Finally, RelGAN is the first architecture that makes GANs with Gumbel-Softmax relaxation succeed in generating realistic text.", "keywords": ["RelGAN", "text generation", "relational memory", "Gumbel-Softmax relaxation", "multiple embedded representations"], "authorids": ["wn8@rice.edu", "nnarodytska@vmware.com", "abp4@rice.edu"], "authors": ["Weili Nie", "Nina Narodytska", "Ankit Patel"], "pdf": "/pdf/28ff6712d62fef4d4846fca5be5df06a8ffd41d2.pdf", "paperhash": "nie|relgan_relational_generative_adversarial_networks_for_text_generation", "_bibtex": "@inproceedings{\nnie2018relgan,\ntitle={Rel{GAN}: Relational Generative Adversarial Networks for Text Generation},\nauthor={Weili Nie and Nina Narodytska and Ankit Patel},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJedV3R5tm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1462/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621620527, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJedV3R5tm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1462/Authors", "ICLR.cc/2019/Conference/Paper1462/Reviewers", "ICLR.cc/2019/Conference/Paper1462/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1462/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1462/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1462/Authors|ICLR.cc/2019/Conference/Paper1462/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1462/Reviewers", "ICLR.cc/2019/Conference/Paper1462/Authors", "ICLR.cc/2019/Conference/Paper1462/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621620527}}}, {"id": "S1li5OZtpQ", "original": null, "number": 2, "cdate": 1542162579285, "ddate": null, "tcdate": 1542162579285, "tmdate": 1542162579285, "tddate": null, "forum": "rJedV3R5tm", "replyto": "B1gqWjV5hX", "invitation": "ICLR.cc/2019/Conference/-/Paper1462/Official_Comment", "content": {"title": "Reply to Reviewer2 (Part 1)", "comment": "Thank you very much for your review and helpful comments. We address your specific questions and comments below:\n\n1. \u201cThe paper does not compare with RNNs trained using only the \"teacher-forcing\" algorithm without using GAN.\u201d\n\nWe think there are some misunderstandings here that we would like to clarify. One of our baseline algorithms is  \u201cMLE\u201d,  where recurrent networks are trained by using the teacher-forcing algorithm (see Tables 1, 2 & 3 for comparison with RelGAN). RelGAN consistently outperforms the baseline model \u201cMLE\u201d in terms of both BLEU scores (sample quality) and NLL_gen (sample diversity). For clarity, we have added a sentence in Section 3.1 to explicitly explain \u201cMLE\u201d in the revised version of the paper. \n\n2. \u201cWhether using BLEU on the entire testing dataset is a good idea for benchmarking is controversial.\u201d\n\nFirst, we agree that absolute BLEU scores depend on the size of test dataset. As such, we have used different subsets (25%, 50%, 75%, 100%) of the original test data for COCO Image Captions to evaluate the generated text from both RelGAN and MLE. We evaluate each subset of the test data 6 times and record the average BLEU scores. We find that for both RelGAN and MLE, the (average) BLEU scores consistently increase with the fraction of test data used. Therefore, whenever we show BLEU scores, the size of the test data MUST be provided as well for fair comparison. This is similar to the Frechet inception distance (FID) score used for evaluating generated images, where the number of generated and real samples used to calculate the FID score also influences the value of the score. \n\nSecond, the following empirical observation shows that the reviewer\u2019s concern on the \u201ccontroversiality of BLEU scores\u201d may be not that severe in this paper: The relative BLEU score differences between RelGAN and MLE remain approximately invariant for different portions of the test data. For instance, BLEU-2 with portion 25% is 0.750 for RelGAN vs. 0.649 for MLE (with the difference 0.101) and BLEU-2 with portion 75% is 0.811 for RelGAN vs. 0.718 for MLE (with the difference 0.093). Thus, if we focus on the relative comparison between different models, the size of the test dataset may not matter much, as long as it is not too small or too large. \n\nFinally, if we correctly understand the reviewer's suggestion of using a \u201cteacher-forcing\u201d trained RNN for evaluation, the reviewer refers to the \u201cvalidation perplexity\u201d metric. [1] has previously used validation perplexity to evaluate the sample quality of MaskGAN, where they showed that in some cases, the perplexity increases steadily while sample quality still remained relatively consistent. More broadly, [2] has also pointed out that validation perplexity does not necessarily correlate with sample quality in the evaluation of generative models, and so validation perplexity may not be a good replacement for BLEU scores in terms of evaluating sample quality. Instead, as suggested by Reviewer3, we have added the human Turing test on Amazon Mechanical Turk (where results are shown in Table 4 in the revised version of the paper), which we think could be a good complementary to BLEU scores.\n\n\n[1] Fedus et al., \u201cMaskGAN: Better Text Generation via Filling in the _.\u201d  in ICLR 2018.\n[2] Theis et al., \u201cA note on the evaluation of generative models.\u201d in ICLR 2016."}, "signatures": ["ICLR.cc/2019/Conference/Paper1462/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1462/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1462/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "RelGAN: Relational Generative Adversarial Networks for Text Generation", "abstract": "Generative adversarial networks (GANs) have achieved great success at generating realistic images. However, the text generation still remains a challenging task for modern GAN architectures. In this work, we propose RelGAN, a new GAN architecture for text generation, consisting of three main components: a relational memory based generator for the long-distance dependency modeling, the Gumbel-Softmax relaxation for training GANs on discrete data, and multiple embedded representations in the discriminator to provide a more informative signal for the generator updates. Our experiments show that RelGAN outperforms current state-of-the-art models in terms of sample quality and diversity, and we also reveal via ablation studies that each component of RelGAN contributes critically to its performance improvements. Moreover, a key advantage of our method, that distinguishes it from other GANs, is the ability to control the trade-off between sample quality and diversity via the use of a single adjustable parameter. Finally, RelGAN is the first architecture that makes GANs with Gumbel-Softmax relaxation succeed in generating realistic text.", "keywords": ["RelGAN", "text generation", "relational memory", "Gumbel-Softmax relaxation", "multiple embedded representations"], "authorids": ["wn8@rice.edu", "nnarodytska@vmware.com", "abp4@rice.edu"], "authors": ["Weili Nie", "Nina Narodytska", "Ankit Patel"], "pdf": "/pdf/28ff6712d62fef4d4846fca5be5df06a8ffd41d2.pdf", "paperhash": "nie|relgan_relational_generative_adversarial_networks_for_text_generation", "_bibtex": "@inproceedings{\nnie2018relgan,\ntitle={Rel{GAN}: Relational Generative Adversarial Networks for Text Generation},\nauthor={Weili Nie and Nina Narodytska and Ankit Patel},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJedV3R5tm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1462/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621620527, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJedV3R5tm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1462/Authors", "ICLR.cc/2019/Conference/Paper1462/Reviewers", "ICLR.cc/2019/Conference/Paper1462/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1462/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1462/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1462/Authors|ICLR.cc/2019/Conference/Paper1462/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1462/Reviewers", "ICLR.cc/2019/Conference/Paper1462/Authors", "ICLR.cc/2019/Conference/Paper1462/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621620527}}}, {"id": "HJxjoDZtTX", "original": null, "number": 1, "cdate": 1542162339481, "ddate": null, "tcdate": 1542162339481, "tmdate": 1542162339481, "tddate": null, "forum": "rJedV3R5tm", "replyto": "rygzGj-p2X", "invitation": "ICLR.cc/2019/Conference/-/Paper1462/Official_Comment", "content": {"title": "Reply to Reviewer1", "comment": "Thank you very much for your review and helpful comments. We address your specific questions and comments below:\n\n1. \u201cNon-differentiability is different to denote the gradient as 0.\u201d\n\nWe agree that in general non-differentiability does not directly imply a vanishing gradient as in (4). To draw this conclusion, we first consider that the sampling operations in (3) are not differentiable, i.e., the output of the generator is discrete, taking values from a finite set. This implies a step function (with multiple steps in general) at the end of the generator, which is not differentiable only at a finite set of points (with measure zero). Since the derivative of a step function is 0 almost everywhere, the gradient of the generator\u2019s output w.r.t. its parameters will also be zero almost everywhere. For clarity, we have added this reasoning to the revised version of the paper.\n\n2. \u201cAre the multiple representations in discriminator simply multiple \u201cEmbedding\u201d matrices?\u201d\n\nYes, we apply multiple different embedding matrices, each of which linearly transforms one input sentence into a separate embedded representation. In our proposed discriminator framework, each embedded representation is independently passed through the later layers of the discriminator neural network (denoted as \u201cCNN-based classifier\u201d in the paper) and the loss function to obtain an individual score (e.g. \u201creal\u201d or \u201cfake\u201d). Finally, the ultimate score to be propagated back to the generator is the average of these individual scores. Our ablation study experiments showed the advantages of this simple improvement in the discriminator.\n\n3. \u201cWhy curves in RelGAN eventually fall after around 1000 iterations?\u201d\n\nGood point! We have added Appendix F in the revised version of the paper to discuss this phenomenon. As shown in Figure 10, there is a diversity-quality transition during training of RelGAN: Early on in training, it learns to aggressively improve sample quality while sacrificing diversity. Later on, it turns instead to maximizing sample diversity while gradually decreasing sample quality. Intuitively, we think it may be much easier for the generator to produce realistic samples -- regardless of their diversity -- in order to fool the discriminator in the early stages of training. As the discriminator becomes better at distinguishing samples with less diversity over iterations, the generator has to focus more on producing more diverse samples to fool the discriminator.\n\n4. \u201cDo you try training from scratch without pre-training?\u201d\n\nYes, we have tested the performance of RelGAN without pre-training by using different GAN losses (including WGAN-GP as the reviewer has mentioned). The results and analysis are provided in Appendix G. We find that without pre-training, there is still a significant improvement for RelGAN compared with the random generation, in particular for the standard GAN loss. In contrast, without pre-training, previous RL-based GANs for text generation, such as SeqGAN and RankGAN, always get stuck around their initialization points and are not able to improve performance at all. This demonstrates that RelGAN may be a more promising GAN architecture to explore in order to reduce dependence on pre-training for text generation. In future work, we plan to do an extensive hyperparameter search to further improve the performance of RelGAN without pre-training.\n\nRelated Work: Thanks for pointing out this paper. We have added a discussion of this paper in the \u201cRelated Work\u201d Section.\n\nPlease let us know if we have addressed your concerns and if you have further comments. "}, "signatures": ["ICLR.cc/2019/Conference/Paper1462/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1462/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1462/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "RelGAN: Relational Generative Adversarial Networks for Text Generation", "abstract": "Generative adversarial networks (GANs) have achieved great success at generating realistic images. However, the text generation still remains a challenging task for modern GAN architectures. In this work, we propose RelGAN, a new GAN architecture for text generation, consisting of three main components: a relational memory based generator for the long-distance dependency modeling, the Gumbel-Softmax relaxation for training GANs on discrete data, and multiple embedded representations in the discriminator to provide a more informative signal for the generator updates. Our experiments show that RelGAN outperforms current state-of-the-art models in terms of sample quality and diversity, and we also reveal via ablation studies that each component of RelGAN contributes critically to its performance improvements. Moreover, a key advantage of our method, that distinguishes it from other GANs, is the ability to control the trade-off between sample quality and diversity via the use of a single adjustable parameter. Finally, RelGAN is the first architecture that makes GANs with Gumbel-Softmax relaxation succeed in generating realistic text.", "keywords": ["RelGAN", "text generation", "relational memory", "Gumbel-Softmax relaxation", "multiple embedded representations"], "authorids": ["wn8@rice.edu", "nnarodytska@vmware.com", "abp4@rice.edu"], "authors": ["Weili Nie", "Nina Narodytska", "Ankit Patel"], "pdf": "/pdf/28ff6712d62fef4d4846fca5be5df06a8ffd41d2.pdf", "paperhash": "nie|relgan_relational_generative_adversarial_networks_for_text_generation", "_bibtex": "@inproceedings{\nnie2018relgan,\ntitle={Rel{GAN}: Relational Generative Adversarial Networks for Text Generation},\nauthor={Weili Nie and Nina Narodytska and Ankit Patel},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJedV3R5tm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1462/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621620527, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJedV3R5tm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1462/Authors", "ICLR.cc/2019/Conference/Paper1462/Reviewers", "ICLR.cc/2019/Conference/Paper1462/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1462/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1462/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1462/Authors|ICLR.cc/2019/Conference/Paper1462/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1462/Reviewers", "ICLR.cc/2019/Conference/Paper1462/Authors", "ICLR.cc/2019/Conference/Paper1462/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621620527}}}, {"id": "rygzGj-p2X", "original": null, "number": 3, "cdate": 1541376777691, "ddate": null, "tcdate": 1541376777691, "tmdate": 1541533114438, "tddate": null, "forum": "rJedV3R5tm", "replyto": "rJedV3R5tm", "invitation": "ICLR.cc/2019/Conference/-/Paper1462/Official_Review", "content": {"title": "Interesting work which makes Gumbel-softmax relaxation work in GAN-based text generation using a relational memory", "review": "Overall:\nThis paper proposes RelGAN, a new GAN architecture for text generation, consisting of three main components: a relational memory based generator for the long-distance dependency modeling, the Gumbel-Softmax relaxation for training GANs on discrete data, and multiple embedded representations in the discriminator to provide a more informative signal\nfor the generator updates.\n\nQuality and Clarity:\nThe paper is well-written and easy to read. \n\nOriginality :\nAlthough each of the components (relational memory, Gumbel-softmax) was already proposed by previous works, it is interesting to combine these into a new GAN-based text generator. \nHowever, the basic setup is not novel enough. The model still requires pre-training the generator using MLE. The major difference are the architectures (relational memory, multi-embedding discriminator) and training directly through Gumbel-softmax trick which has been investigated in (Kusner and Hernandez-Lobato, 2016). \n\nSignificance:\nThe experiments in both synthetic and real data are in detail, and the results are good and significant.\n\n-------------------\nComments:\n-- In (4), sampling is known as non-differentiable which means that we cannot get a valid definition of gradients. It is different to denote the gradient as 0.\n-- Are the multiple representations in discriminator simply multiple \u201cEmbedding\u201d matrices?\n-- Curves using Gumbel-softmax trick + RM will eventually fall after around 1000 iterations in all the figures. Why this would happen?\n-- Do you try training from scratch without pre-training? For instance, using WGAN as the discriminator\n\n\nRelated work:\n-- Maybe also consider to the following paper which used Gumbel-softmax relaxation for improving the generation quality in neural machine translation related?\nGu, Jiatao, Daniel Jiwoong Im, and Victor OK Li. \"Neural machine translation with gumbel-greedy decoding.\" arXiv preprint arXiv:1706.07518 (2017).\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1462/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "RelGAN: Relational Generative Adversarial Networks for Text Generation", "abstract": "Generative adversarial networks (GANs) have achieved great success at generating realistic images. However, the text generation still remains a challenging task for modern GAN architectures. In this work, we propose RelGAN, a new GAN architecture for text generation, consisting of three main components: a relational memory based generator for the long-distance dependency modeling, the Gumbel-Softmax relaxation for training GANs on discrete data, and multiple embedded representations in the discriminator to provide a more informative signal for the generator updates. Our experiments show that RelGAN outperforms current state-of-the-art models in terms of sample quality and diversity, and we also reveal via ablation studies that each component of RelGAN contributes critically to its performance improvements. Moreover, a key advantage of our method, that distinguishes it from other GANs, is the ability to control the trade-off between sample quality and diversity via the use of a single adjustable parameter. Finally, RelGAN is the first architecture that makes GANs with Gumbel-Softmax relaxation succeed in generating realistic text.", "keywords": ["RelGAN", "text generation", "relational memory", "Gumbel-Softmax relaxation", "multiple embedded representations"], "authorids": ["wn8@rice.edu", "nnarodytska@vmware.com", "abp4@rice.edu"], "authors": ["Weili Nie", "Nina Narodytska", "Ankit Patel"], "pdf": "/pdf/28ff6712d62fef4d4846fca5be5df06a8ffd41d2.pdf", "paperhash": "nie|relgan_relational_generative_adversarial_networks_for_text_generation", "_bibtex": "@inproceedings{\nnie2018relgan,\ntitle={Rel{GAN}: Relational Generative Adversarial Networks for Text Generation},\nauthor={Weili Nie and Nina Narodytska and Ankit Patel},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJedV3R5tm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1462/Official_Review", "cdate": 1542234224623, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "rJedV3R5tm", "replyto": "rJedV3R5tm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1462/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335951793, "tmdate": 1552335951793, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1462/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 11}