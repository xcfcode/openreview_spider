{"notes": [{"tddate": null, "ddate": null, "cdate": null, "original": null, "tmdate": 1490028607659, "tcdate": 1490028607659, "number": 1, "id": "S1druF6il", "invitation": "ICLR.cc/2017/workshop/-/paper113/acceptance", "forum": "rJR7aaEYx", "replyto": "rJR7aaEYx", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Image Captioning with Sparse LTSM", "abstract": "Long Short-Term Memory (LSTM) is widely used to solve sequence modeling problems, for example, image captioning. We found the LSTM cells are heavily redundant. We adopt network pruning to reduce the redundancy of LSTM and introduce sparsity as new regularization to reduce overfitting. We can achieve better performance than the dense baseline while reducing the total number of parameters in LSTM by more than 80%, from 2.1 million to only 0.4 million. Sparse LSTM can improve the BLUE-4 score by 1.3 points on Flickr8k dataset and CIDER score by 1.7 points on MSCOCO dataset. We explore four types of pruning policies on LSTM, visualize the sparsity pattern, weight distribution of sparse LSTM and analyze the pros and cons of each policy.", "pdf": "/pdf/a555d0a7849882765f8cced6448a10ad51c4deee.pdf", "TL;DR": "We achieve better performance with 80% less parameters by introducing sparsity to LSTM", "paperhash": "lin|image_captioning_with_sparse_ltsm", "keywords": ["Deep learning"], "conflicts": ["nvidia.com", "stanford.edu", "tsinghua.edu.cn"], "authors": ["Yujun Lin", "Song Han", "Yu Wang", "William J. Dally"], "authorids": ["linyy14@mails.tsinghua.edu.cn", "songhan@stanford.edu", "yu-wang@tsinghua.edu.cn", "dally@stanford.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1490028608222, "id": "ICLR.cc/2017/workshop/-/paper113/acceptance", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "rJR7aaEYx", "replyto": "rJR7aaEYx", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept", "Reject"]}}}, "nonreaders": [], "cdate": 1490028608222}}}, {"tddate": null, "tmdate": 1489443652246, "tcdate": 1489443652246, "number": 2, "id": "B1hBo94jx", "invitation": "ICLR.cc/2017/workshop/-/paper113/official/review", "forum": "rJR7aaEYx", "replyto": "rJR7aaEYx", "signatures": ["ICLR.cc/2017/workshop/paper113/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper113/AnonReviewer2"], "content": {"title": "", "rating": "4: Ok but not good enough - rejection", "review": "This paper uses the sparsity-inducing techniques of Han et al 2016 and Narang et al 2017 for the LSTM used in image captioning. This amounts to four techniques Type 1 ... Type 4, with 1,1,3,4 hyperparameters respectively. It is not clear from the paper how these hyperparameters are set or how they were tuned, except for the mentioned 80% sparsity. The experiments consist of running the 4 types and comparing them to a baseline, which uses the full LSTM. One has to squint quite hard to see which sparsity approach works best, and the result tables don't offer very consistent takeaways.\n\nOverall, it is not clear if these results should be a paper by itself, or merely one section in either Han et al 2016 or Narang et al 2017 papers, both of which report experiments with RNNs already. Consistent with the results already presented in these much more thorough papers, the added sparsity appears to have a small regularizing effect. Therefore, it is not clear what value is added with this work.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Image Captioning with Sparse LTSM", "abstract": "Long Short-Term Memory (LSTM) is widely used to solve sequence modeling problems, for example, image captioning. We found the LSTM cells are heavily redundant. We adopt network pruning to reduce the redundancy of LSTM and introduce sparsity as new regularization to reduce overfitting. We can achieve better performance than the dense baseline while reducing the total number of parameters in LSTM by more than 80%, from 2.1 million to only 0.4 million. Sparse LSTM can improve the BLUE-4 score by 1.3 points on Flickr8k dataset and CIDER score by 1.7 points on MSCOCO dataset. We explore four types of pruning policies on LSTM, visualize the sparsity pattern, weight distribution of sparse LSTM and analyze the pros and cons of each policy.", "pdf": "/pdf/a555d0a7849882765f8cced6448a10ad51c4deee.pdf", "TL;DR": "We achieve better performance with 80% less parameters by introducing sparsity to LSTM", "paperhash": "lin|image_captioning_with_sparse_ltsm", "keywords": ["Deep learning"], "conflicts": ["nvidia.com", "stanford.edu", "tsinghua.edu.cn"], "authors": ["Yujun Lin", "Song Han", "Yu Wang", "William J. Dally"], "authorids": ["linyy14@mails.tsinghua.edu.cn", "songhan@stanford.edu", "yu-wang@tsinghua.edu.cn", "dally@stanford.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489443653089, "id": "ICLR.cc/2017/workshop/-/paper113/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper113/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper113/AnonReviewer1", "ICLR.cc/2017/workshop/paper113/AnonReviewer2"], "reply": {"forum": "rJR7aaEYx", "replyto": "rJR7aaEYx", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper113/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper113/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489443653089}}}, {"tddate": null, "nonreaders": null, "tmdate": 1489200085875, "tcdate": 1489200000303, "number": 1, "id": "BkuK7kZog", "invitation": "ICLR.cc/2017/workshop/-/paper113/official/review", "forum": "rJR7aaEYx", "replyto": "rJR7aaEYx", "signatures": ["ICLR.cc/2017/workshop/paper113/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper113/AnonReviewer1"], "content": {"title": "", "rating": "5: Marginally below acceptance threshold", "review": "This paper explores regularization of LSTM-style recurrent networks using sparsity, showing improved image captioning performance from hybrid CNN-LSTM models and comparing different approaches to applying sparsification.\n\n\nPros:\n\nExplores sparsity a regularization technique for LSTMs, a widely used recurrent network architecture.\n\nAnalyzes several sparsification protocols, varying the rate and timing of sparsification during training.\n\nExperiments show that the simplest sparsification approach (Type I) where sparsification is applied only once after a single iteration of training gets consistently good performance, better than the baseline and mostly better than the more complex protocols (Types II-IV).\n\n\nCons:\n\nNot much novelty -- the aim of the paper is to compare two existing RNN sparsification methods (Han et al 2016b & Narang et al. 2017) for image captioning with LSTMs.  It\u2019s not clear that the experimental results would consistently generalize to other problems or non-RNN architectures. (Han et al. 2016b, the source of the sparsification method, includes experiments in other problem settings and with pure convnets, in addition to image captioning RNNs.)\n\nThe largest performance improvements from sparsification aren\u2019t very big, and the runtime improvement isn\u2019t quantified (I would guess it\u2019s relatively small in this case, with the convnet being the bulk of the execution time).\n\nOnly the sparsification schedule is varied in the experiments.  The paper could explore the impact of other hyperparameters, such as the sparsity proportion (all experiments use 80% sparsity).\n\nMinor: the \u201crough ranking\u201d in Sec 3.1 claims Type III is generally the worst performing approach -- this is true for the MSCOCO results, but it seems to actually perform the best for Flickr8k (except with BLEU-4).\n\nMinor: Figure 4 sparsity patterns -- if the only takeaway from these is that different LSTM gates have different degrees of sparsity, these visualizations could be replaced with a simple table or plot of that summary statistic for each gate.  The actual patterns (besides the overall \u201cdarkness\u201d indicating the degree of sparsity) seem like noise.\n\n\nOverall, the paper doesn\u2019t propose anything new, and though the evaluation may be a useful reference for practitioners, its scope is too limited given that only the sparsification schedule is explored.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Image Captioning with Sparse LTSM", "abstract": "Long Short-Term Memory (LSTM) is widely used to solve sequence modeling problems, for example, image captioning. We found the LSTM cells are heavily redundant. We adopt network pruning to reduce the redundancy of LSTM and introduce sparsity as new regularization to reduce overfitting. We can achieve better performance than the dense baseline while reducing the total number of parameters in LSTM by more than 80%, from 2.1 million to only 0.4 million. Sparse LSTM can improve the BLUE-4 score by 1.3 points on Flickr8k dataset and CIDER score by 1.7 points on MSCOCO dataset. We explore four types of pruning policies on LSTM, visualize the sparsity pattern, weight distribution of sparse LSTM and analyze the pros and cons of each policy.", "pdf": "/pdf/a555d0a7849882765f8cced6448a10ad51c4deee.pdf", "TL;DR": "We achieve better performance with 80% less parameters by introducing sparsity to LSTM", "paperhash": "lin|image_captioning_with_sparse_ltsm", "keywords": ["Deep learning"], "conflicts": ["nvidia.com", "stanford.edu", "tsinghua.edu.cn"], "authors": ["Yujun Lin", "Song Han", "Yu Wang", "William J. Dally"], "authorids": ["linyy14@mails.tsinghua.edu.cn", "songhan@stanford.edu", "yu-wang@tsinghua.edu.cn", "dally@stanford.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489443653089, "id": "ICLR.cc/2017/workshop/-/paper113/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper113/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper113/AnonReviewer1", "ICLR.cc/2017/workshop/paper113/AnonReviewer2"], "reply": {"forum": "rJR7aaEYx", "replyto": "rJR7aaEYx", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper113/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper113/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489443653089}}}, {"tddate": null, "replyto": null, "nonreaders": null, "ddate": null, "tmdate": 1487361231347, "tcdate": 1487359270551, "number": 113, "id": "rJR7aaEYx", "invitation": "ICLR.cc/2017/workshop/-/submission", "forum": "rJR7aaEYx", "signatures": ["~Yujun_Lin1"], "readers": ["everyone"], "content": {"title": "Image Captioning with Sparse LTSM", "abstract": "Long Short-Term Memory (LSTM) is widely used to solve sequence modeling problems, for example, image captioning. We found the LSTM cells are heavily redundant. We adopt network pruning to reduce the redundancy of LSTM and introduce sparsity as new regularization to reduce overfitting. We can achieve better performance than the dense baseline while reducing the total number of parameters in LSTM by more than 80%, from 2.1 million to only 0.4 million. Sparse LSTM can improve the BLUE-4 score by 1.3 points on Flickr8k dataset and CIDER score by 1.7 points on MSCOCO dataset. We explore four types of pruning policies on LSTM, visualize the sparsity pattern, weight distribution of sparse LSTM and analyze the pros and cons of each policy.", "pdf": "/pdf/a555d0a7849882765f8cced6448a10ad51c4deee.pdf", "TL;DR": "We achieve better performance with 80% less parameters by introducing sparsity to LSTM", "paperhash": "lin|image_captioning_with_sparse_ltsm", "keywords": ["Deep learning"], "conflicts": ["nvidia.com", "stanford.edu", "tsinghua.edu.cn"], "authors": ["Yujun Lin", "Song Han", "Yu Wang", "William J. Dally"], "authorids": ["linyy14@mails.tsinghua.edu.cn", "songhan@stanford.edu", "yu-wang@tsinghua.edu.cn", "dally@stanford.edu"]}, "writers": [], "details": {"replyCount": 3, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1487690420000, "tmdate": 1484242559574, "id": "ICLR.cc/2017/workshop/-/submission", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1495466420000, "cdate": 1484242559574}}}], "count": 4}