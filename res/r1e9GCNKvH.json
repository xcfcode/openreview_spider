{"notes": [{"id": "r1e9GCNKvH", "original": "HJglvlrdPB", "number": 1011, "cdate": 1569439250185, "ddate": null, "tcdate": 1569439250185, "tmdate": 1583912051480, "tddate": null, "forum": "r1e9GCNKvH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "One-Shot Pruning of Recurrent Neural Networks by Jacobian Spectrum Evaluation", "authors": ["Shunshi Zhang", "Bradly C. Stadie"], "authorids": ["matthew.zhang@mail.utoronto.ca", "bstadie@berkeley.edu"], "keywords": ["Pruning", "RNNs", "Sparsity"], "TL;DR": "New Objective for One-Shot Pruning Recurrent Neural Networks", "abstract": "  Recent advances in the sparse neural network literature have made it possible to prune many large feed forward and convolutional networks with only a small quantity of data. Yet, these same techniques often falter when applied to the problem of recovering sparse recurrent networks. These failures are quantitative: when pruned with recent techniques, RNNs typically obtain worse performance than they do under a simple random pruning scheme. The failures are also qualitative: the distribution of active weights in a pruned LSTM or GRU network tend to be concentrated in specific neurons and gates, and not well dispersed across the entire architecture. We seek to rectify both the quantitative and qualitative issues with recurrent network pruning by introducing a new recurrent pruning objective derived from the spectrum of the recurrent Jacobian. Our objective is data efficient (requiring only 64 data points to prune the network), easy to implement, and produces 95 % sparse GRUs that significantly improve on existing baselines. We evaluate on sequential MNIST, Billion Words, and Wikitext. ", "pdf": "/pdf/ec8730f5965a0e9fab955521dc143850be6a34f2.pdf", "paperhash": "zhang|oneshot_pruning_of_recurrent_neural_networks_by_jacobian_spectrum_evaluation", "_bibtex": "@inproceedings{\nZhang2020One-Shot,\ntitle={One-Shot Pruning of Recurrent Neural Networks by Jacobian Spectrum Evaluation},\nauthor={Shunshi Zhang and Bradly C. Stadie},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=r1e9GCNKvH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/b1cc4a8f9cd7ea0bfd1e181add92ce79e552b63d.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "7_VANJ06hg", "original": null, "number": 1, "cdate": 1576798712212, "ddate": null, "tcdate": 1576798712212, "tmdate": 1576800924186, "tddate": null, "forum": "r1e9GCNKvH", "replyto": "r1e9GCNKvH", "invitation": "ICLR.cc/2020/Conference/Paper1011/-/Decision", "content": {"decision": "Accept (Poster)", "comment": "Based on current unanimous reviews, the paper is accepted.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "One-Shot Pruning of Recurrent Neural Networks by Jacobian Spectrum Evaluation", "authors": ["Shunshi Zhang", "Bradly C. Stadie"], "authorids": ["matthew.zhang@mail.utoronto.ca", "bstadie@berkeley.edu"], "keywords": ["Pruning", "RNNs", "Sparsity"], "TL;DR": "New Objective for One-Shot Pruning Recurrent Neural Networks", "abstract": "  Recent advances in the sparse neural network literature have made it possible to prune many large feed forward and convolutional networks with only a small quantity of data. Yet, these same techniques often falter when applied to the problem of recovering sparse recurrent networks. These failures are quantitative: when pruned with recent techniques, RNNs typically obtain worse performance than they do under a simple random pruning scheme. The failures are also qualitative: the distribution of active weights in a pruned LSTM or GRU network tend to be concentrated in specific neurons and gates, and not well dispersed across the entire architecture. We seek to rectify both the quantitative and qualitative issues with recurrent network pruning by introducing a new recurrent pruning objective derived from the spectrum of the recurrent Jacobian. Our objective is data efficient (requiring only 64 data points to prune the network), easy to implement, and produces 95 % sparse GRUs that significantly improve on existing baselines. We evaluate on sequential MNIST, Billion Words, and Wikitext. ", "pdf": "/pdf/ec8730f5965a0e9fab955521dc143850be6a34f2.pdf", "paperhash": "zhang|oneshot_pruning_of_recurrent_neural_networks_by_jacobian_spectrum_evaluation", "_bibtex": "@inproceedings{\nZhang2020One-Shot,\ntitle={One-Shot Pruning of Recurrent Neural Networks by Jacobian Spectrum Evaluation},\nauthor={Shunshi Zhang and Bradly C. Stadie},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=r1e9GCNKvH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/b1cc4a8f9cd7ea0bfd1e181add92ce79e552b63d.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "r1e9GCNKvH", "replyto": "r1e9GCNKvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795723785, "tmdate": 1576800275320, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1011/-/Decision"}}}, {"id": "S1lr4INojr", "original": null, "number": 6, "cdate": 1573762604638, "ddate": null, "tcdate": 1573762604638, "tmdate": 1573762604638, "tddate": null, "forum": "r1e9GCNKvH", "replyto": "HJg30sq9jS", "invitation": "ICLR.cc/2020/Conference/Paper1011/-/Official_Comment", "content": {"title": "Re:Response to rebuttal ", "comment": "Thank you for your thoughtful comments and taking the time to review our rebuttal. We really appreciate it, and we will continue to refine our results in the remaining time."}, "signatures": ["ICLR.cc/2020/Conference/Paper1011/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1011/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "One-Shot Pruning of Recurrent Neural Networks by Jacobian Spectrum Evaluation", "authors": ["Shunshi Zhang", "Bradly C. Stadie"], "authorids": ["matthew.zhang@mail.utoronto.ca", "bstadie@berkeley.edu"], "keywords": ["Pruning", "RNNs", "Sparsity"], "TL;DR": "New Objective for One-Shot Pruning Recurrent Neural Networks", "abstract": "  Recent advances in the sparse neural network literature have made it possible to prune many large feed forward and convolutional networks with only a small quantity of data. Yet, these same techniques often falter when applied to the problem of recovering sparse recurrent networks. These failures are quantitative: when pruned with recent techniques, RNNs typically obtain worse performance than they do under a simple random pruning scheme. The failures are also qualitative: the distribution of active weights in a pruned LSTM or GRU network tend to be concentrated in specific neurons and gates, and not well dispersed across the entire architecture. We seek to rectify both the quantitative and qualitative issues with recurrent network pruning by introducing a new recurrent pruning objective derived from the spectrum of the recurrent Jacobian. Our objective is data efficient (requiring only 64 data points to prune the network), easy to implement, and produces 95 % sparse GRUs that significantly improve on existing baselines. We evaluate on sequential MNIST, Billion Words, and Wikitext. ", "pdf": "/pdf/ec8730f5965a0e9fab955521dc143850be6a34f2.pdf", "paperhash": "zhang|oneshot_pruning_of_recurrent_neural_networks_by_jacobian_spectrum_evaluation", "_bibtex": "@inproceedings{\nZhang2020One-Shot,\ntitle={One-Shot Pruning of Recurrent Neural Networks by Jacobian Spectrum Evaluation},\nauthor={Shunshi Zhang and Bradly C. Stadie},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=r1e9GCNKvH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/b1cc4a8f9cd7ea0bfd1e181add92ce79e552b63d.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1e9GCNKvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1011/Authors", "ICLR.cc/2020/Conference/Paper1011/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1011/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1011/Reviewers", "ICLR.cc/2020/Conference/Paper1011/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1011/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1011/Authors|ICLR.cc/2020/Conference/Paper1011/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504162655, "tmdate": 1576860557246, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1011/Authors", "ICLR.cc/2020/Conference/Paper1011/Reviewers", "ICLR.cc/2020/Conference/Paper1011/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1011/-/Official_Comment"}}}, {"id": "HJxW5p8sOS", "original": null, "number": 2, "cdate": 1570626953114, "ddate": null, "tcdate": 1570626953114, "tmdate": 1573723119584, "tddate": null, "forum": "r1e9GCNKvH", "replyto": "r1e9GCNKvH", "invitation": "ICLR.cc/2020/Conference/Paper1011/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #1", "review": "\nNotes: \n\n  -RNN network pruning has proven to be challenging using the techniques often used with other network types.  \n\n  -One issue is that the performance of an LSTM/GRU can hinge on a few activated gates and can lead to more concentration of influence than would be seen in a feedforward network without parameter sharing between layers.  \n\n  -New objective uses 64 points to prune the network (I assume this is just the size of the minibatch).  \n\n  -Result is a 95% sparse GRU cell.  \n\n  -New idea is based on keeping weights that propagate information through many time steps.  \n\n  -Encourages \"singular values of the temporal jacobian with respect to network weights to be non-degenerate\" (I suppose this means that the gradient flowing through time will contain multiple directions of variation)  \n\n  -Introduction does a good job of introducing the key ideas.  \n\n  -J_t is a temporal jacobian of size N x N (N is number units) at time step t.  \n\n  -Chi is the spectral norm of this temporal jacobian.  I'm a bit confused by this, because my understanding is that the spectral norm is the largest singular value, but equation 3 looks like a sum over singular values, making it more like a frobenius norm?  \n\n  -This jacobian isn't tractable, so paper approximates it using a first-order taylor expansion.  So basically the pruning just amounts to taking parameters with the largest gradient?  \n\n  -Section 2.4 is confusing and seems to come out of nowhere.  Is this suggesting that the technique isn't just pruning but adding a new normalization scheme?  On second reading, this is a normalization scheme effecting which parameters to prune.  The motivation for why the gradients are normalized like this is still confusing.  If you're willing to make a linear assumption, it seems like it's enough to consider the gradient on the parameter multiplied by the magnitude of the parameter to see the overall effect of removing it?  \n\n  -The results look good, but sequential mnist is a bit of a toy task.  I'd also like to see a more fine-grained analysis showing the tradeoff between the number of units removed and the performance.  \n\n  -The paper claims that L2 pruning requires more data, but it's unclear if this really matters since the whole dataset was used to train both methods initially.  \n\n  -On Table 2 the results of the technique don't seem that much better than \"Random\".  \n\n  Review: This paper presented a fast pruning algorithm for RNNs, which uses the norm of the gradient as a guide to pruning.  I'm borderline on this paper.  The idea of using the gradient is good, but the explanation of some aspects like the normalization is confusing and felt random.  Additionally the results, while better than some other pruning techniques on RNNs, don't seem to be that much better than random.  ", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper1011/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1011/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "One-Shot Pruning of Recurrent Neural Networks by Jacobian Spectrum Evaluation", "authors": ["Shunshi Zhang", "Bradly C. Stadie"], "authorids": ["matthew.zhang@mail.utoronto.ca", "bstadie@berkeley.edu"], "keywords": ["Pruning", "RNNs", "Sparsity"], "TL;DR": "New Objective for One-Shot Pruning Recurrent Neural Networks", "abstract": "  Recent advances in the sparse neural network literature have made it possible to prune many large feed forward and convolutional networks with only a small quantity of data. Yet, these same techniques often falter when applied to the problem of recovering sparse recurrent networks. These failures are quantitative: when pruned with recent techniques, RNNs typically obtain worse performance than they do under a simple random pruning scheme. The failures are also qualitative: the distribution of active weights in a pruned LSTM or GRU network tend to be concentrated in specific neurons and gates, and not well dispersed across the entire architecture. We seek to rectify both the quantitative and qualitative issues with recurrent network pruning by introducing a new recurrent pruning objective derived from the spectrum of the recurrent Jacobian. Our objective is data efficient (requiring only 64 data points to prune the network), easy to implement, and produces 95 % sparse GRUs that significantly improve on existing baselines. We evaluate on sequential MNIST, Billion Words, and Wikitext. ", "pdf": "/pdf/ec8730f5965a0e9fab955521dc143850be6a34f2.pdf", "paperhash": "zhang|oneshot_pruning_of_recurrent_neural_networks_by_jacobian_spectrum_evaluation", "_bibtex": "@inproceedings{\nZhang2020One-Shot,\ntitle={One-Shot Pruning of Recurrent Neural Networks by Jacobian Spectrum Evaluation},\nauthor={Shunshi Zhang and Bradly C. Stadie},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=r1e9GCNKvH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/b1cc4a8f9cd7ea0bfd1e181add92ce79e552b63d.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "r1e9GCNKvH", "replyto": "r1e9GCNKvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1011/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1011/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575633632355, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1011/Reviewers"], "noninvitees": [], "tcdate": 1570237743691, "tmdate": 1575633632369, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1011/-/Official_Review"}}}, {"id": "HJg30sq9jS", "original": null, "number": 5, "cdate": 1573723091556, "ddate": null, "tcdate": 1573723091556, "tmdate": 1573723091556, "tddate": null, "forum": "r1e9GCNKvH", "replyto": "SyejPNkXjS", "invitation": "ICLR.cc/2020/Conference/Paper1011/-/Official_Comment", "content": {"title": "Response to rebuttal", "comment": "\n1.  I reread the new section 3.2.  I think it's much better now, as it is very clear that the pruning criteria is a ranking over sensitivity scores.  In the original draft I found that part quite confusing.  \n\n2.  The point about other pruning methods failing for RNNs makes sense to me.  I'm still wary of a paper which has results which are really not that much better than random.  Part of .  Although for sequential mnist for 1600 units, the improvement just barely exceeds 1 stdv.  Thus it's somewhat unlikely to just be random variation between experiments.  \n\nOverall I'm still quite borderline, because I feel like another round of refinement could improve the paper.  At the same time, I think the 3/10 score is too low, and I do think that rejecting this paper could hold back progress in this area.  So I'll raise my score to 6/10.  \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1011/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1011/AnonReviewer1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "One-Shot Pruning of Recurrent Neural Networks by Jacobian Spectrum Evaluation", "authors": ["Shunshi Zhang", "Bradly C. Stadie"], "authorids": ["matthew.zhang@mail.utoronto.ca", "bstadie@berkeley.edu"], "keywords": ["Pruning", "RNNs", "Sparsity"], "TL;DR": "New Objective for One-Shot Pruning Recurrent Neural Networks", "abstract": "  Recent advances in the sparse neural network literature have made it possible to prune many large feed forward and convolutional networks with only a small quantity of data. Yet, these same techniques often falter when applied to the problem of recovering sparse recurrent networks. These failures are quantitative: when pruned with recent techniques, RNNs typically obtain worse performance than they do under a simple random pruning scheme. The failures are also qualitative: the distribution of active weights in a pruned LSTM or GRU network tend to be concentrated in specific neurons and gates, and not well dispersed across the entire architecture. We seek to rectify both the quantitative and qualitative issues with recurrent network pruning by introducing a new recurrent pruning objective derived from the spectrum of the recurrent Jacobian. Our objective is data efficient (requiring only 64 data points to prune the network), easy to implement, and produces 95 % sparse GRUs that significantly improve on existing baselines. We evaluate on sequential MNIST, Billion Words, and Wikitext. ", "pdf": "/pdf/ec8730f5965a0e9fab955521dc143850be6a34f2.pdf", "paperhash": "zhang|oneshot_pruning_of_recurrent_neural_networks_by_jacobian_spectrum_evaluation", "_bibtex": "@inproceedings{\nZhang2020One-Shot,\ntitle={One-Shot Pruning of Recurrent Neural Networks by Jacobian Spectrum Evaluation},\nauthor={Shunshi Zhang and Bradly C. Stadie},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=r1e9GCNKvH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/b1cc4a8f9cd7ea0bfd1e181add92ce79e552b63d.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1e9GCNKvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1011/Authors", "ICLR.cc/2020/Conference/Paper1011/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1011/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1011/Reviewers", "ICLR.cc/2020/Conference/Paper1011/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1011/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1011/Authors|ICLR.cc/2020/Conference/Paper1011/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504162655, "tmdate": 1576860557246, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1011/Authors", "ICLR.cc/2020/Conference/Paper1011/Reviewers", "ICLR.cc/2020/Conference/Paper1011/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1011/-/Official_Comment"}}}, {"id": "SylVhHMYiH", "original": null, "number": 4, "cdate": 1573623212506, "ddate": null, "tcdate": 1573623212506, "tmdate": 1573623212506, "tddate": null, "forum": "r1e9GCNKvH", "replyto": "SyejPNkXjS", "invitation": "ICLR.cc/2020/Conference/Paper1011/-/Official_Comment", "content": {"title": "Additional Feedback", "comment": "Dear Reviewer 1, we wanted to see if you had a chance to review the changes we've made to the paper. We think we addressed many of your concerns, and would be interested in hearing any other useful feedback you might have."}, "signatures": ["ICLR.cc/2020/Conference/Paper1011/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1011/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "One-Shot Pruning of Recurrent Neural Networks by Jacobian Spectrum Evaluation", "authors": ["Shunshi Zhang", "Bradly C. Stadie"], "authorids": ["matthew.zhang@mail.utoronto.ca", "bstadie@berkeley.edu"], "keywords": ["Pruning", "RNNs", "Sparsity"], "TL;DR": "New Objective for One-Shot Pruning Recurrent Neural Networks", "abstract": "  Recent advances in the sparse neural network literature have made it possible to prune many large feed forward and convolutional networks with only a small quantity of data. Yet, these same techniques often falter when applied to the problem of recovering sparse recurrent networks. These failures are quantitative: when pruned with recent techniques, RNNs typically obtain worse performance than they do under a simple random pruning scheme. The failures are also qualitative: the distribution of active weights in a pruned LSTM or GRU network tend to be concentrated in specific neurons and gates, and not well dispersed across the entire architecture. We seek to rectify both the quantitative and qualitative issues with recurrent network pruning by introducing a new recurrent pruning objective derived from the spectrum of the recurrent Jacobian. Our objective is data efficient (requiring only 64 data points to prune the network), easy to implement, and produces 95 % sparse GRUs that significantly improve on existing baselines. We evaluate on sequential MNIST, Billion Words, and Wikitext. ", "pdf": "/pdf/ec8730f5965a0e9fab955521dc143850be6a34f2.pdf", "paperhash": "zhang|oneshot_pruning_of_recurrent_neural_networks_by_jacobian_spectrum_evaluation", "_bibtex": "@inproceedings{\nZhang2020One-Shot,\ntitle={One-Shot Pruning of Recurrent Neural Networks by Jacobian Spectrum Evaluation},\nauthor={Shunshi Zhang and Bradly C. Stadie},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=r1e9GCNKvH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/b1cc4a8f9cd7ea0bfd1e181add92ce79e552b63d.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1e9GCNKvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1011/Authors", "ICLR.cc/2020/Conference/Paper1011/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1011/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1011/Reviewers", "ICLR.cc/2020/Conference/Paper1011/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1011/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1011/Authors|ICLR.cc/2020/Conference/Paper1011/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504162655, "tmdate": 1576860557246, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1011/Authors", "ICLR.cc/2020/Conference/Paper1011/Reviewers", "ICLR.cc/2020/Conference/Paper1011/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1011/-/Official_Comment"}}}, {"id": "rJgAqEkmjS", "original": null, "number": 3, "cdate": 1573217429768, "ddate": null, "tcdate": 1573217429768, "tmdate": 1573217479179, "tddate": null, "forum": "r1e9GCNKvH", "replyto": "rklCPNPKur", "invitation": "ICLR.cc/2020/Conference/Paper1011/-/Official_Comment", "content": {"title": "Response to Reviewer 2", "comment": "We thank the reviewer for their insights and comments.\n\nWe are happy to know that you found the paper well-motivated and were able to follow the derivations. We put substantial effort into making the derivations easy to follow. It is great feedback to learn that these efforts were worthwhile. \n\nWith respect to each of your comments:\nRuntime of Sparse Networks: At high sparsity (above 90%), pruned networks typically run faster than their dense counterparts even without adaptions such as block-sparsity. Tensorflow and other standard libraries have operations specifically dedicated to fast sparse multiplication and autodifferentiation. Thank you for calling attention to this. \n\n\nRedundancy in Parameters: Although there may be groupings of similar parameters with respect to Jacobian contribution, capturing second-order correlations is nontrivial and adds significantly to the runtime of the algorithm. We may explore such ideas in a sequel of our work.\n\nInitializing a smaller sparse network: Even with random pruning, we find that large sparse networks are more parameter efficient than small dense networks. For example, on PTB, a 95% sparse network achieves 47.2 train perplexity, compared to an equivalently parameterized dense network performance of 48.4. Consequently, there is a need to efficiently sparsify RNNs, even if we observe a drop in performance before and after sparsification.\n\nComments on Figure 1: Random pruning is already evenly distributed across gates and type (for a 400 unit GRU with 500k parameters, the unevenness across gates and types is statistically negligible). This is precisely why random pruning obtains such strong performance compared to our algorithm. However, homogeneity across gate/type is not the sole contributor to our algorithm's success, as shown in our paper's analysis.\n\nComments on Figure 2: For standard, variance-preserving initializations (He et. al.), the singular values tend to be less than 1. For more irregular, high variance distributions, this assumption does not hold. Our algorithm performs poorly on such distributions, as shown in the appendix.\n\nStylistic feedback has been incorporated in a revision of the document. We apologize for any distraction this caused. We were having fun with the writing but were perhaps too overwrought at times. "}, "signatures": ["ICLR.cc/2020/Conference/Paper1011/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1011/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "One-Shot Pruning of Recurrent Neural Networks by Jacobian Spectrum Evaluation", "authors": ["Shunshi Zhang", "Bradly C. Stadie"], "authorids": ["matthew.zhang@mail.utoronto.ca", "bstadie@berkeley.edu"], "keywords": ["Pruning", "RNNs", "Sparsity"], "TL;DR": "New Objective for One-Shot Pruning Recurrent Neural Networks", "abstract": "  Recent advances in the sparse neural network literature have made it possible to prune many large feed forward and convolutional networks with only a small quantity of data. Yet, these same techniques often falter when applied to the problem of recovering sparse recurrent networks. These failures are quantitative: when pruned with recent techniques, RNNs typically obtain worse performance than they do under a simple random pruning scheme. The failures are also qualitative: the distribution of active weights in a pruned LSTM or GRU network tend to be concentrated in specific neurons and gates, and not well dispersed across the entire architecture. We seek to rectify both the quantitative and qualitative issues with recurrent network pruning by introducing a new recurrent pruning objective derived from the spectrum of the recurrent Jacobian. Our objective is data efficient (requiring only 64 data points to prune the network), easy to implement, and produces 95 % sparse GRUs that significantly improve on existing baselines. We evaluate on sequential MNIST, Billion Words, and Wikitext. ", "pdf": "/pdf/ec8730f5965a0e9fab955521dc143850be6a34f2.pdf", "paperhash": "zhang|oneshot_pruning_of_recurrent_neural_networks_by_jacobian_spectrum_evaluation", "_bibtex": "@inproceedings{\nZhang2020One-Shot,\ntitle={One-Shot Pruning of Recurrent Neural Networks by Jacobian Spectrum Evaluation},\nauthor={Shunshi Zhang and Bradly C. Stadie},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=r1e9GCNKvH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/b1cc4a8f9cd7ea0bfd1e181add92ce79e552b63d.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1e9GCNKvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1011/Authors", "ICLR.cc/2020/Conference/Paper1011/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1011/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1011/Reviewers", "ICLR.cc/2020/Conference/Paper1011/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1011/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1011/Authors|ICLR.cc/2020/Conference/Paper1011/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504162655, "tmdate": 1576860557246, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1011/Authors", "ICLR.cc/2020/Conference/Paper1011/Reviewers", "ICLR.cc/2020/Conference/Paper1011/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1011/-/Official_Comment"}}}, {"id": "SyejPNkXjS", "original": null, "number": 2, "cdate": 1573217378528, "ddate": null, "tcdate": 1573217378528, "tmdate": 1573217378528, "tddate": null, "forum": "r1e9GCNKvH", "replyto": "HJxW5p8sOS", "invitation": "ICLR.cc/2020/Conference/Paper1011/-/Official_Comment", "content": {"title": "Response to Reviewer 1", "comment": "We thank the reviewer for their careful analysis of our paper. The review's insightful comments helped us to substantially improve the clarity of our work. We would also like to thank you for speaking about the strength of our experimental results (with some interesting caveats, of course, about random pruning).\n\nThe reviewer has enumerated three primary hesitations with our paper. We would like to respond to those as follows:\n    1. \"the explanation of some aspects like the normalization is confusing and felt random\"\n    \n    Architecture choices such as the activation function applied to each gate will affect the resulting variance of each parameter's score, irrespective of actual contribution to the Jacobian spectrum. Nonetheless, we agree that the importance of this section was overstated in our original draft, and that the motivation could be further elaborated. The updated paper no longer emphasizes this section (see the end of section 2.3). \n    \n    2. \"Additionally the results, while better than some other pruning techniques on RNNs, don't seem to be that much better than random.\"\n    The performance of random pruning has been surprising, and is the focus of a good deal of analysis in our paper. When discussing random pruning of recurrent networks, it is crucial to keep in mind that most algorithms that are not adapted to RNNs fail to improve upon random pruning. Indeed we are the first algorithm to systematically improve on random pruning for sequential models, especially at large parameter scales. We hope to see future papers revisit this topic, and explore the performance of random pruning more deeply. \n    \n    3. \"I'd also like to see a more fine-grained analysis showing the tradeoff between the number of units removed and the performance.\"\n    We have included an ablation experiment on Penn Treebank showing the tradeoff between parameters removed, and performance of the resulting network. This supplements the analysis performed on sequential MNIST. See the revised paper for more comments (end of section 3.2).\n\nWe would also like to respond to several of the smaller concerns that were raised.\n\n - Pruning amounts to taking parameters with the largest gradient:\n Our method estimates the parameter impact on the Jacobian singular values through first order expansion. While the Jacobian itself is tractable, optimizing it with binary search on sparse masks is not. Hence, we take the gradient of the Jacobian as our approximation. Our method is second-order with respect to the data and parameters.\n\n - L2 Pruning: We intended to state that L2 pruning required significantly more resources during training than our method. The ambiguity in our paper has been clarified (second paragraph of 3.1). \n\n - Stylistic Errors: These have been corrected in a revision of the paper. We thank the reviewer for taking the time to point these out.\n \nPlease let us know if we have addressed the review's concerns enough to merit a change in score. If you are still borderline about our paper clearing the bar for acceptance, please let us know if there's any other analysis that could help. "}, "signatures": ["ICLR.cc/2020/Conference/Paper1011/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1011/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "One-Shot Pruning of Recurrent Neural Networks by Jacobian Spectrum Evaluation", "authors": ["Shunshi Zhang", "Bradly C. Stadie"], "authorids": ["matthew.zhang@mail.utoronto.ca", "bstadie@berkeley.edu"], "keywords": ["Pruning", "RNNs", "Sparsity"], "TL;DR": "New Objective for One-Shot Pruning Recurrent Neural Networks", "abstract": "  Recent advances in the sparse neural network literature have made it possible to prune many large feed forward and convolutional networks with only a small quantity of data. Yet, these same techniques often falter when applied to the problem of recovering sparse recurrent networks. These failures are quantitative: when pruned with recent techniques, RNNs typically obtain worse performance than they do under a simple random pruning scheme. The failures are also qualitative: the distribution of active weights in a pruned LSTM or GRU network tend to be concentrated in specific neurons and gates, and not well dispersed across the entire architecture. We seek to rectify both the quantitative and qualitative issues with recurrent network pruning by introducing a new recurrent pruning objective derived from the spectrum of the recurrent Jacobian. Our objective is data efficient (requiring only 64 data points to prune the network), easy to implement, and produces 95 % sparse GRUs that significantly improve on existing baselines. We evaluate on sequential MNIST, Billion Words, and Wikitext. ", "pdf": "/pdf/ec8730f5965a0e9fab955521dc143850be6a34f2.pdf", "paperhash": "zhang|oneshot_pruning_of_recurrent_neural_networks_by_jacobian_spectrum_evaluation", "_bibtex": "@inproceedings{\nZhang2020One-Shot,\ntitle={One-Shot Pruning of Recurrent Neural Networks by Jacobian Spectrum Evaluation},\nauthor={Shunshi Zhang and Bradly C. Stadie},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=r1e9GCNKvH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/b1cc4a8f9cd7ea0bfd1e181add92ce79e552b63d.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1e9GCNKvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1011/Authors", "ICLR.cc/2020/Conference/Paper1011/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1011/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1011/Reviewers", "ICLR.cc/2020/Conference/Paper1011/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1011/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1011/Authors|ICLR.cc/2020/Conference/Paper1011/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504162655, "tmdate": 1576860557246, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1011/Authors", "ICLR.cc/2020/Conference/Paper1011/Reviewers", "ICLR.cc/2020/Conference/Paper1011/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1011/-/Official_Comment"}}}, {"id": "rkx9M4J7sB", "original": null, "number": 1, "cdate": 1573217297679, "ddate": null, "tcdate": 1573217297679, "tmdate": 1573217297679, "tddate": null, "forum": "r1e9GCNKvH", "replyto": "rkevyhvpYS", "invitation": "ICLR.cc/2020/Conference/Paper1011/-/Official_Comment", "content": {"title": "Response to Reviewer 3", "comment": "We thank the reviewer for their generous assessment of our paper. We agree with their analysis, and appreciate their comment on \"the neatness of deriving closed-form analytical expressions for a fast and simple pruning method\". This, in our opinion, is one of our work's main strengths.\n\nWe believe that this reviewer's endorsement is a worthwhile sign, since it means that researchers without extensive knowledge of this particular problem are able to follow our paper's main arguments."}, "signatures": ["ICLR.cc/2020/Conference/Paper1011/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1011/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "One-Shot Pruning of Recurrent Neural Networks by Jacobian Spectrum Evaluation", "authors": ["Shunshi Zhang", "Bradly C. Stadie"], "authorids": ["matthew.zhang@mail.utoronto.ca", "bstadie@berkeley.edu"], "keywords": ["Pruning", "RNNs", "Sparsity"], "TL;DR": "New Objective for One-Shot Pruning Recurrent Neural Networks", "abstract": "  Recent advances in the sparse neural network literature have made it possible to prune many large feed forward and convolutional networks with only a small quantity of data. Yet, these same techniques often falter when applied to the problem of recovering sparse recurrent networks. These failures are quantitative: when pruned with recent techniques, RNNs typically obtain worse performance than they do under a simple random pruning scheme. The failures are also qualitative: the distribution of active weights in a pruned LSTM or GRU network tend to be concentrated in specific neurons and gates, and not well dispersed across the entire architecture. We seek to rectify both the quantitative and qualitative issues with recurrent network pruning by introducing a new recurrent pruning objective derived from the spectrum of the recurrent Jacobian. Our objective is data efficient (requiring only 64 data points to prune the network), easy to implement, and produces 95 % sparse GRUs that significantly improve on existing baselines. We evaluate on sequential MNIST, Billion Words, and Wikitext. ", "pdf": "/pdf/ec8730f5965a0e9fab955521dc143850be6a34f2.pdf", "paperhash": "zhang|oneshot_pruning_of_recurrent_neural_networks_by_jacobian_spectrum_evaluation", "_bibtex": "@inproceedings{\nZhang2020One-Shot,\ntitle={One-Shot Pruning of Recurrent Neural Networks by Jacobian Spectrum Evaluation},\nauthor={Shunshi Zhang and Bradly C. Stadie},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=r1e9GCNKvH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/b1cc4a8f9cd7ea0bfd1e181add92ce79e552b63d.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1e9GCNKvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1011/Authors", "ICLR.cc/2020/Conference/Paper1011/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1011/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1011/Reviewers", "ICLR.cc/2020/Conference/Paper1011/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1011/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1011/Authors|ICLR.cc/2020/Conference/Paper1011/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504162655, "tmdate": 1576860557246, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1011/Authors", "ICLR.cc/2020/Conference/Paper1011/Reviewers", "ICLR.cc/2020/Conference/Paper1011/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1011/-/Official_Comment"}}}, {"id": "rklCPNPKur", "original": null, "number": 1, "cdate": 1570497638087, "ddate": null, "tcdate": 1570497638087, "tmdate": 1572972523790, "tddate": null, "forum": "r1e9GCNKvH", "replyto": "r1e9GCNKvH", "invitation": "ICLR.cc/2020/Conference/Paper1011/-/Official_Review", "content": {"rating": "6: Weak Accept", "experience_assessment": "I do not know much about this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "OVERALL:\n\nI should first say that this is reasonably far outside my wheelhouse.\nI have never worked on RNNs or pruning.\nI also have no familiarity with the data sets used.\n\nAll these things being said, I can follow the derivations, and the idea\nseems reasonable and well-motivated, and pruning is interesting\nfor both scientific and practical reasons, and this technique seems to help a substantial amount,\nso I'm inclined to vote for acceptance, with the understanding that perhaps better informed reviewers\n will in the future point out something I have missed.\n\nDETAILED NOTES:\n\n> overparameterized networks require more storage capacity and are computationally more expensive than their pruned counterparts\nI'm with you on the storage capacity, but do any of these pruned networks actually run faster than their non-pruned counterparts?\nI thought you had to work really hard to prune to some kind of block-sparse representation to realize any speed gains.\nThis question is not rhetorical - I know very little about this topic.\n\n> Work for the present volume began by asking the question...\nI like this paragraph for motivation, but perhaps 'volume' is slightly overwrought?\n\n> For our pruning objective, we simply take the\nK weights with largest sensitivity scores, as those represent the parameters which most affect the\nJacobian objective near the initialization.\nIs there some notion of redundancy, where certain sets of parameters affect the jacobian in the same way,\nso that all but 1 element of the set could be pruned, or something?\n\nIn line 13 of algorithm 1, why do we need to sort if in the next step we just mask out\neverything with sensitivity less than D tilde k?\n\nMaybe this is a dumb question, but if you're pruning at initialization, why not\njust initialize a smaller network in such a way that you wouldn't choose to prune any of its parameters?\nAm I misunderstanding what you're doing?\n\n\n> In the prequel, we postulated t\nThe prequel?\n\nFig 1 is interesting, but it raises the question of whether you could recover a simpler\nalgorithm by just modifying random pruning so that it evenly distributes 'prunes' across\ngate and Type.\n\nIn fig 2, why are all the singular values less than 1?\nIt's not obvious to me why that should be true, unless you enforce it w/ the initialization.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1011/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1011/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "One-Shot Pruning of Recurrent Neural Networks by Jacobian Spectrum Evaluation", "authors": ["Shunshi Zhang", "Bradly C. Stadie"], "authorids": ["matthew.zhang@mail.utoronto.ca", "bstadie@berkeley.edu"], "keywords": ["Pruning", "RNNs", "Sparsity"], "TL;DR": "New Objective for One-Shot Pruning Recurrent Neural Networks", "abstract": "  Recent advances in the sparse neural network literature have made it possible to prune many large feed forward and convolutional networks with only a small quantity of data. Yet, these same techniques often falter when applied to the problem of recovering sparse recurrent networks. These failures are quantitative: when pruned with recent techniques, RNNs typically obtain worse performance than they do under a simple random pruning scheme. The failures are also qualitative: the distribution of active weights in a pruned LSTM or GRU network tend to be concentrated in specific neurons and gates, and not well dispersed across the entire architecture. We seek to rectify both the quantitative and qualitative issues with recurrent network pruning by introducing a new recurrent pruning objective derived from the spectrum of the recurrent Jacobian. Our objective is data efficient (requiring only 64 data points to prune the network), easy to implement, and produces 95 % sparse GRUs that significantly improve on existing baselines. We evaluate on sequential MNIST, Billion Words, and Wikitext. ", "pdf": "/pdf/ec8730f5965a0e9fab955521dc143850be6a34f2.pdf", "paperhash": "zhang|oneshot_pruning_of_recurrent_neural_networks_by_jacobian_spectrum_evaluation", "_bibtex": "@inproceedings{\nZhang2020One-Shot,\ntitle={One-Shot Pruning of Recurrent Neural Networks by Jacobian Spectrum Evaluation},\nauthor={Shunshi Zhang and Bradly C. Stadie},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=r1e9GCNKvH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/b1cc4a8f9cd7ea0bfd1e181add92ce79e552b63d.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "r1e9GCNKvH", "replyto": "r1e9GCNKvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1011/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1011/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575633632355, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1011/Reviewers"], "noninvitees": [], "tcdate": 1570237743691, "tmdate": 1575633632369, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1011/-/Official_Review"}}}, {"id": "rkevyhvpYS", "original": null, "number": 3, "cdate": 1571810271287, "ddate": null, "tcdate": 1571810271287, "tmdate": 1572972523722, "tddate": null, "forum": "r1e9GCNKvH", "replyto": "r1e9GCNKvH", "invitation": "ICLR.cc/2020/Conference/Paper1011/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "Although I worked and published on network pruning in the past, I found little overlap between my knowledge and the content of this paper. I feel therefore that I can only provide a very superficial feedback.\n\nThe authors generalize recently-developed network pruning techniques that were developed in he context of feed-forward networks, and do not require iterative pruning-retraining cycles, to RNNs. Since RNNs apply the same matrix on their hidden vector multiple times, approximate closed-form expressions for pruning criteria can be derived analytically. The pruning criterion is based on the spectrum of the Jacobian matrix of the (N+1)-th hidden unit with respect to the N-th one. A closed-form algorithm for pruning is presented, and the method is evaluated on several datasets.\n\nWhile I appreciate the neatness of deriving closed-form analytical expressions for a fast and simple pruning method, I feel that am not in a position to rate the paper. My apologies to the Authors and the Editors."}, "signatures": ["ICLR.cc/2020/Conference/Paper1011/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1011/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "One-Shot Pruning of Recurrent Neural Networks by Jacobian Spectrum Evaluation", "authors": ["Shunshi Zhang", "Bradly C. Stadie"], "authorids": ["matthew.zhang@mail.utoronto.ca", "bstadie@berkeley.edu"], "keywords": ["Pruning", "RNNs", "Sparsity"], "TL;DR": "New Objective for One-Shot Pruning Recurrent Neural Networks", "abstract": "  Recent advances in the sparse neural network literature have made it possible to prune many large feed forward and convolutional networks with only a small quantity of data. Yet, these same techniques often falter when applied to the problem of recovering sparse recurrent networks. These failures are quantitative: when pruned with recent techniques, RNNs typically obtain worse performance than they do under a simple random pruning scheme. The failures are also qualitative: the distribution of active weights in a pruned LSTM or GRU network tend to be concentrated in specific neurons and gates, and not well dispersed across the entire architecture. We seek to rectify both the quantitative and qualitative issues with recurrent network pruning by introducing a new recurrent pruning objective derived from the spectrum of the recurrent Jacobian. Our objective is data efficient (requiring only 64 data points to prune the network), easy to implement, and produces 95 % sparse GRUs that significantly improve on existing baselines. We evaluate on sequential MNIST, Billion Words, and Wikitext. ", "pdf": "/pdf/ec8730f5965a0e9fab955521dc143850be6a34f2.pdf", "paperhash": "zhang|oneshot_pruning_of_recurrent_neural_networks_by_jacobian_spectrum_evaluation", "_bibtex": "@inproceedings{\nZhang2020One-Shot,\ntitle={One-Shot Pruning of Recurrent Neural Networks by Jacobian Spectrum Evaluation},\nauthor={Shunshi Zhang and Bradly C. Stadie},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=r1e9GCNKvH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/b1cc4a8f9cd7ea0bfd1e181add92ce79e552b63d.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "r1e9GCNKvH", "replyto": "r1e9GCNKvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1011/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1011/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575633632355, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1011/Reviewers"], "noninvitees": [], "tcdate": 1570237743691, "tmdate": 1575633632369, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1011/-/Official_Review"}}}], "count": 11}