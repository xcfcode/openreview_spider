{"notes": [{"id": "JI2TGOehNT0", "original": "0qioept9x_", "number": 3288, "cdate": 1601308365223, "ddate": null, "tcdate": 1601308365223, "tmdate": 1614985720595, "tddate": null, "forum": "JI2TGOehNT0", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Combining Imitation and Reinforcement Learning with Free Energy Principle", "authorids": ["~Ryoya_Ogishima1", "karino@isi.imi.i.u-tokyo.ac.jp", "~Yasuo_Kuniyoshi1"], "authors": ["Ryoya Ogishima", "Izumi Karino", "Yasuo Kuniyoshi"], "keywords": ["Imitation", "Reinforcement Learning", "Free Energy Principle"], "abstract": "Imitation Learning (IL) and Reinforcement Learning (RL) from high dimensional sensory inputs are often introduced as separate problems, but a more realistic problem setting is how to merge the techniques so that the agent can reduce exploration costs by partially imitating experts at the same time it maximizes its return. Even when the experts are suboptimal (e.g. Experts learned halfway with other RL methods or human-crafted experts), it is expected that the agent outperforms the suboptimal experts\u2019 performance. In this paper, we propose to address the issue by using and theoretically extending Free Energy Principle, a unified brain theory that explains perception, action and model learning in a Bayesian probabilistic way. We find that both IL and RL can be achieved based on the same free energy objective function. Our results show that our approach is promising in visual control tasks especially with sparse-reward environments.", "one-sentence_summary": "Extending Free Energy Principle to achieve imitation reinforcement learning for sparse reward problems with suboptimal experts", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ogishima|combining_imitation_and_reinforcement_learning_with_free_energy_principle", "supplementary_material": "/attachment/3938ad292c9c9665152d7f19e0a7c48b5dbc2ee8.zip", "pdf": "/pdf/cfcf91db3909637376a517eac03f5e0c2dc9488a.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=KvdHIDeYmK", "_bibtex": "@misc{\nogishima2021combining,\ntitle={Combining Imitation and Reinforcement Learning with Free Energy Principle},\nauthor={Ryoya Ogishima and Izumi Karino and Yasuo Kuniyoshi},\nyear={2021},\nurl={https://openreview.net/forum?id=JI2TGOehNT0}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 11, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "-9KJJrF7d5W", "original": null, "number": 1, "cdate": 1610040420892, "ddate": null, "tcdate": 1610040420892, "tmdate": 1610474019615, "tddate": null, "forum": "JI2TGOehNT0", "replyto": "JI2TGOehNT0", "invitation": "ICLR.cc/2021/Conference/Paper3288/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "This paper proposes a new algorithm that combines imitation learning and reinforcement learning, based on an extension of the free energy principal. The expert's demonstrations are encoded as a policy prior, and a posterior policy is inferred by maximizing expected rewards. While at a high-level this is a promising direction, all the reviewers found the paper difficult to follow and verify its claims. This mostly due to a use of unusual and non-conistent notations. The authors are advised to take into account the issues about clarity that the reviewers raised and improve the readability of their paper accordingly."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Combining Imitation and Reinforcement Learning with Free Energy Principle", "authorids": ["~Ryoya_Ogishima1", "karino@isi.imi.i.u-tokyo.ac.jp", "~Yasuo_Kuniyoshi1"], "authors": ["Ryoya Ogishima", "Izumi Karino", "Yasuo Kuniyoshi"], "keywords": ["Imitation", "Reinforcement Learning", "Free Energy Principle"], "abstract": "Imitation Learning (IL) and Reinforcement Learning (RL) from high dimensional sensory inputs are often introduced as separate problems, but a more realistic problem setting is how to merge the techniques so that the agent can reduce exploration costs by partially imitating experts at the same time it maximizes its return. Even when the experts are suboptimal (e.g. Experts learned halfway with other RL methods or human-crafted experts), it is expected that the agent outperforms the suboptimal experts\u2019 performance. In this paper, we propose to address the issue by using and theoretically extending Free Energy Principle, a unified brain theory that explains perception, action and model learning in a Bayesian probabilistic way. We find that both IL and RL can be achieved based on the same free energy objective function. Our results show that our approach is promising in visual control tasks especially with sparse-reward environments.", "one-sentence_summary": "Extending Free Energy Principle to achieve imitation reinforcement learning for sparse reward problems with suboptimal experts", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ogishima|combining_imitation_and_reinforcement_learning_with_free_energy_principle", "supplementary_material": "/attachment/3938ad292c9c9665152d7f19e0a7c48b5dbc2ee8.zip", "pdf": "/pdf/cfcf91db3909637376a517eac03f5e0c2dc9488a.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=KvdHIDeYmK", "_bibtex": "@misc{\nogishima2021combining,\ntitle={Combining Imitation and Reinforcement Learning with Free Energy Principle},\nauthor={Ryoya Ogishima and Izumi Karino and Yasuo Kuniyoshi},\nyear={2021},\nurl={https://openreview.net/forum?id=JI2TGOehNT0}\n}"}, "tags": [], "invitation": {"reply": {"forum": "JI2TGOehNT0", "replyto": "JI2TGOehNT0", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040420878, "tmdate": 1610474019598, "id": "ICLR.cc/2021/Conference/Paper3288/-/Decision"}}}, {"id": "BK_EjD3dSY", "original": null, "number": 4, "cdate": 1604182078562, "ddate": null, "tcdate": 1604182078562, "tmdate": 1606798390877, "tddate": null, "forum": "JI2TGOehNT0", "replyto": "JI2TGOehNT0", "invitation": "ICLR.cc/2021/Conference/Paper3288/-/Official_Review", "content": {"title": "Review", "review": "This paper  introduces two different interpretations of free energy minimization as a form of behavior cloning and reinforcement learning.  \n\nStrength:\nThis approach seems to have significant gains on the environments evaluated.\nThe approach appears novel to my knowledge.\n\nWeaknesses:\nI found that I was confused by the presentation of section 3.1. In particular, I think the authors should clarify the difference between prior and posterior policies in both the RL and imitation learning setting as they appear to be different.\n\nWhy does equation 17 to 18 follow? Isn't the posterior policy different than the policy prior, leading to the likelihood to the next state this is distinct from the prior probability of the next state?\n\nFor equation 22 to 23, is the assumption that the likelihood of the next state is proportional to inverse exponentiated reward? I think the statement should be said in the text.\n\nHow does the approach compare with other approaches that encourage entropy in the policy? Such as something like soft Q learning? Or some type of curiosity?\n\nCan this approach be evaluated on more realistic environments other than deepmind control? \n\nPost Rebuttal Update:\n\nDue to the remaining confusion among reviewers about the equations in the manuscript, I maintain my score.\n", "rating": "5: Marginally below acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2021/Conference/Paper3288/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3288/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Combining Imitation and Reinforcement Learning with Free Energy Principle", "authorids": ["~Ryoya_Ogishima1", "karino@isi.imi.i.u-tokyo.ac.jp", "~Yasuo_Kuniyoshi1"], "authors": ["Ryoya Ogishima", "Izumi Karino", "Yasuo Kuniyoshi"], "keywords": ["Imitation", "Reinforcement Learning", "Free Energy Principle"], "abstract": "Imitation Learning (IL) and Reinforcement Learning (RL) from high dimensional sensory inputs are often introduced as separate problems, but a more realistic problem setting is how to merge the techniques so that the agent can reduce exploration costs by partially imitating experts at the same time it maximizes its return. Even when the experts are suboptimal (e.g. Experts learned halfway with other RL methods or human-crafted experts), it is expected that the agent outperforms the suboptimal experts\u2019 performance. In this paper, we propose to address the issue by using and theoretically extending Free Energy Principle, a unified brain theory that explains perception, action and model learning in a Bayesian probabilistic way. We find that both IL and RL can be achieved based on the same free energy objective function. Our results show that our approach is promising in visual control tasks especially with sparse-reward environments.", "one-sentence_summary": "Extending Free Energy Principle to achieve imitation reinforcement learning for sparse reward problems with suboptimal experts", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ogishima|combining_imitation_and_reinforcement_learning_with_free_energy_principle", "supplementary_material": "/attachment/3938ad292c9c9665152d7f19e0a7c48b5dbc2ee8.zip", "pdf": "/pdf/cfcf91db3909637376a517eac03f5e0c2dc9488a.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=KvdHIDeYmK", "_bibtex": "@misc{\nogishima2021combining,\ntitle={Combining Imitation and Reinforcement Learning with Free Energy Principle},\nauthor={Ryoya Ogishima and Izumi Karino and Yasuo Kuniyoshi},\nyear={2021},\nurl={https://openreview.net/forum?id=JI2TGOehNT0}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "JI2TGOehNT0", "replyto": "JI2TGOehNT0", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3288/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538078470, "tmdate": 1606915778408, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3288/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3288/-/Official_Review"}}}, {"id": "y-9WVIi9fv", "original": null, "number": 7, "cdate": 1606305202060, "ddate": null, "tcdate": 1606305202060, "tmdate": 1606305614124, "tddate": null, "forum": "JI2TGOehNT0", "replyto": "ZVt-8QyLs4c", "invitation": "ICLR.cc/2021/Conference/Paper3288/-/Official_Comment", "content": {"title": "Let me explain in details", "comment": "Thanks for the reply. Let me explain more in details.\n\nQ1. What are the functionals that are minimized? Where do the theta, phi, and psi actually appear?\n\nA1.\nEq.41, 42, 43 are all minimized, but updated parameters are different. Let me explain this step by step. Please look at section 3.3 while you read the followings (especially eq. 33-40).\n\nWe model a probability with Deep Neural Networks that output the mean and standard deviation of the probability and regard the output as Gaussian distribution. For example, policy posterior is modeled as a network that takes s_t and h_t as inputs and calculates through several hidden layers and outputs the Gaussian distribution of a_t.\n\nTheta is a group/set of parameters consisting the probabilities of state prior, observation/reward likelihood and policy prior. \nPhi is a group of parameters consisting the probability of state posterior. \nPsi is a group of parameters consisting the probability of policy posterior. \nOmega is a group of parameters consisting the probability of value network. \nFor example, equation 38 can also be written as q(a_t|s_t, h_t; psi) as you suggested.\n\nGroup of parameters theta, phi and psi are updated (gradient descent) to minimize Eq. 41+42.  Group of parameters omega is updated to minimize Eq 43. \n\nIn summary, eq.41 + 42 (F_IL + F_RL) is minimized to update a set of parameters theta, phi and psi. Eq. 43 (L) is minimized to update a set of parameters omega.\n\n\nQ2. What are the motivation to justify equation 41-43?\n\nA2. \nEq. 41 and 42 are the free energy objectives and eq. 43 is a loss for the value function (approximator of the sum of expected free energy at time step t+2 ~ infinity). \n\nAbout eq. 41 and 42:\nAccording to Free Energy Principle (please see section 2 or  reference of Friston's work for more details), by minimizing the free energy, the agent can learn variational (state) posterior, model (prior and likelihood), and action (policy). If you look at eq.7, you can understand that the expected free energy is derived by imagining 1  time step ahead. Through the derivation in section 3.1, we (mathematically) extended the free energy and the expected free energy to be readily used for imitation and RL. If you look at eq. 27, you can understand that the curly F is a sum of free energy and expected free energies (t+1 to infinitiy). In other words, the curly F explains now (t) and future (t+1 to infinity; the agent has not experienced this yet). Eq. 29 and 30 are both derived from eq. 27. The reason why we separate these two is that we have different formulation (G^IL and G^RL) depending on IL or RL, both of which are from the same mathematical amount (imagining 1 time step ahead from F from eq. 10). Eq. 41 and 42 are the same as eq. 29 and 30 respectively.\n\nAbout eq. 43:\nThis is not a free energy objective. This loss is for learning a value function which tries to approximate G^{t+2} + \u2026 G^{infinity}. The reason why we introduce this is that it is impossible to add up the amounts until infinity. To learn the value function, we use the exactly same way as temporal difference learning of Deep Q Network. Please see this prior work for the validity.\n\n\nThank you very much for your comments! We will update the paper for the camera ready version so that every reader can understand."}, "signatures": ["ICLR.cc/2021/Conference/Paper3288/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3288/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Combining Imitation and Reinforcement Learning with Free Energy Principle", "authorids": ["~Ryoya_Ogishima1", "karino@isi.imi.i.u-tokyo.ac.jp", "~Yasuo_Kuniyoshi1"], "authors": ["Ryoya Ogishima", "Izumi Karino", "Yasuo Kuniyoshi"], "keywords": ["Imitation", "Reinforcement Learning", "Free Energy Principle"], "abstract": "Imitation Learning (IL) and Reinforcement Learning (RL) from high dimensional sensory inputs are often introduced as separate problems, but a more realistic problem setting is how to merge the techniques so that the agent can reduce exploration costs by partially imitating experts at the same time it maximizes its return. Even when the experts are suboptimal (e.g. Experts learned halfway with other RL methods or human-crafted experts), it is expected that the agent outperforms the suboptimal experts\u2019 performance. In this paper, we propose to address the issue by using and theoretically extending Free Energy Principle, a unified brain theory that explains perception, action and model learning in a Bayesian probabilistic way. We find that both IL and RL can be achieved based on the same free energy objective function. Our results show that our approach is promising in visual control tasks especially with sparse-reward environments.", "one-sentence_summary": "Extending Free Energy Principle to achieve imitation reinforcement learning for sparse reward problems with suboptimal experts", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ogishima|combining_imitation_and_reinforcement_learning_with_free_energy_principle", "supplementary_material": "/attachment/3938ad292c9c9665152d7f19e0a7c48b5dbc2ee8.zip", "pdf": "/pdf/cfcf91db3909637376a517eac03f5e0c2dc9488a.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=KvdHIDeYmK", "_bibtex": "@misc{\nogishima2021combining,\ntitle={Combining Imitation and Reinforcement Learning with Free Energy Principle},\nauthor={Ryoya Ogishima and Izumi Karino and Yasuo Kuniyoshi},\nyear={2021},\nurl={https://openreview.net/forum?id=JI2TGOehNT0}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "JI2TGOehNT0", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3288/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3288/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3288/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3288/Authors|ICLR.cc/2021/Conference/Paper3288/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3288/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923839060, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3288/-/Official_Comment"}}}, {"id": "ZVt-8QyLs4c", "original": null, "number": 6, "cdate": 1606256785140, "ddate": null, "tcdate": 1606256785140, "tmdate": 1606256785140, "tddate": null, "forum": "JI2TGOehNT0", "replyto": "5PvLqA2R68", "invitation": "ICLR.cc/2021/Conference/Paper3288/-/Official_Comment", "content": {"title": "still not clear to me", "comment": "Thanks for the reply. I still can't understand exactly what are the functionals that are minimized. Is it all equations 41+42+43 ?  And where do the $\\theta, \\phi$ and $\\psi$ actually appear? It would be more readable if they appear with a notation such as $f(\\cdot; \\theta)$.\n\nIn other words, the following part in your reply is still not understandable to me: \"you can see that F_IL + F_RL are minimized along parameters theta, phi and psi. L is simply a loss along parameter omega for learning a value function which is used to calculate F_RL as in eq. 42.\" \n\n In addition, the motivation to justify equations 41-43 is also unclear. Unfortunately, at this point I keep my score unchanged."}, "signatures": ["ICLR.cc/2021/Conference/Paper3288/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3288/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Combining Imitation and Reinforcement Learning with Free Energy Principle", "authorids": ["~Ryoya_Ogishima1", "karino@isi.imi.i.u-tokyo.ac.jp", "~Yasuo_Kuniyoshi1"], "authors": ["Ryoya Ogishima", "Izumi Karino", "Yasuo Kuniyoshi"], "keywords": ["Imitation", "Reinforcement Learning", "Free Energy Principle"], "abstract": "Imitation Learning (IL) and Reinforcement Learning (RL) from high dimensional sensory inputs are often introduced as separate problems, but a more realistic problem setting is how to merge the techniques so that the agent can reduce exploration costs by partially imitating experts at the same time it maximizes its return. Even when the experts are suboptimal (e.g. Experts learned halfway with other RL methods or human-crafted experts), it is expected that the agent outperforms the suboptimal experts\u2019 performance. In this paper, we propose to address the issue by using and theoretically extending Free Energy Principle, a unified brain theory that explains perception, action and model learning in a Bayesian probabilistic way. We find that both IL and RL can be achieved based on the same free energy objective function. Our results show that our approach is promising in visual control tasks especially with sparse-reward environments.", "one-sentence_summary": "Extending Free Energy Principle to achieve imitation reinforcement learning for sparse reward problems with suboptimal experts", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ogishima|combining_imitation_and_reinforcement_learning_with_free_energy_principle", "supplementary_material": "/attachment/3938ad292c9c9665152d7f19e0a7c48b5dbc2ee8.zip", "pdf": "/pdf/cfcf91db3909637376a517eac03f5e0c2dc9488a.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=KvdHIDeYmK", "_bibtex": "@misc{\nogishima2021combining,\ntitle={Combining Imitation and Reinforcement Learning with Free Energy Principle},\nauthor={Ryoya Ogishima and Izumi Karino and Yasuo Kuniyoshi},\nyear={2021},\nurl={https://openreview.net/forum?id=JI2TGOehNT0}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "JI2TGOehNT0", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3288/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3288/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3288/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3288/Authors|ICLR.cc/2021/Conference/Paper3288/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3288/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923839060, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3288/-/Official_Comment"}}}, {"id": "5PvLqA2R68", "original": null, "number": 5, "cdate": 1606168643505, "ddate": null, "tcdate": 1606168643505, "tmdate": 1606168643505, "tddate": null, "forum": "JI2TGOehNT0", "replyto": "6aOeaNg6_Ni", "invitation": "ICLR.cc/2021/Conference/Paper3288/-/Official_Comment", "content": {"title": "Response to Reviewer 1", "comment": "We thank the reviewer for the comments. We have revised the paper according to the suggestions and would like to clarify several things:\n\n\u30fbIt is unclear what are the minimized loss functions\n> I have updated the paper, and please find eq. 41-43 and Algorithm 1 in Appendix. If you look at Algorithm 1, you can see that F_IL + F_RL are minimized along parameters theta, phi and psi. L is simply a loss along parameter omega for learning a value function which is used to calculate F_RL as in eq. 42. Thus, the contributions of F_IL (incl. G_{t+1}^{IL}) and F_RL (incl. G_{t+1}^{RL}) are the same. F_IL and F_RL are derived from eq. 27 (sum of free energy at future time steps).\n\n\u30fbNotations are not always consistent:\n> I\u2019ve updated all F_IL, F_RL to curly F. I\u2019ve added a subscript for all F and G.\n\n\u30fbUnclear sentence \"in RL, using the value function to predict rewards in the long-term future is essential to avoid a local minimum and achieve the desired goal.\" \n> What I mean by local minimum here is the case where an agent cannot achieve the global minimum of the loss by pursuing rewards in a short time horizon. For example, in \"mountain car problem\", the agent needs to temporarily take negative-reward actions (not local minimum) to gain momentum to go up the hill, which leads to higher rewards in the end.\n\n\u30fbTypos\n> I fixed the typos. Thanks for pointing these out."}, "signatures": ["ICLR.cc/2021/Conference/Paper3288/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3288/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Combining Imitation and Reinforcement Learning with Free Energy Principle", "authorids": ["~Ryoya_Ogishima1", "karino@isi.imi.i.u-tokyo.ac.jp", "~Yasuo_Kuniyoshi1"], "authors": ["Ryoya Ogishima", "Izumi Karino", "Yasuo Kuniyoshi"], "keywords": ["Imitation", "Reinforcement Learning", "Free Energy Principle"], "abstract": "Imitation Learning (IL) and Reinforcement Learning (RL) from high dimensional sensory inputs are often introduced as separate problems, but a more realistic problem setting is how to merge the techniques so that the agent can reduce exploration costs by partially imitating experts at the same time it maximizes its return. Even when the experts are suboptimal (e.g. Experts learned halfway with other RL methods or human-crafted experts), it is expected that the agent outperforms the suboptimal experts\u2019 performance. In this paper, we propose to address the issue by using and theoretically extending Free Energy Principle, a unified brain theory that explains perception, action and model learning in a Bayesian probabilistic way. We find that both IL and RL can be achieved based on the same free energy objective function. Our results show that our approach is promising in visual control tasks especially with sparse-reward environments.", "one-sentence_summary": "Extending Free Energy Principle to achieve imitation reinforcement learning for sparse reward problems with suboptimal experts", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ogishima|combining_imitation_and_reinforcement_learning_with_free_energy_principle", "supplementary_material": "/attachment/3938ad292c9c9665152d7f19e0a7c48b5dbc2ee8.zip", "pdf": "/pdf/cfcf91db3909637376a517eac03f5e0c2dc9488a.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=KvdHIDeYmK", "_bibtex": "@misc{\nogishima2021combining,\ntitle={Combining Imitation and Reinforcement Learning with Free Energy Principle},\nauthor={Ryoya Ogishima and Izumi Karino and Yasuo Kuniyoshi},\nyear={2021},\nurl={https://openreview.net/forum?id=JI2TGOehNT0}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "JI2TGOehNT0", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3288/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3288/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3288/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3288/Authors|ICLR.cc/2021/Conference/Paper3288/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3288/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923839060, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3288/-/Official_Comment"}}}, {"id": "nXqw0t6hpfY", "original": null, "number": 4, "cdate": 1606168315610, "ddate": null, "tcdate": 1606168315610, "tmdate": 1606168315610, "tddate": null, "forum": "JI2TGOehNT0", "replyto": "JTk4S_Fv1EY", "invitation": "ICLR.cc/2021/Conference/Paper3288/-/Official_Comment", "content": {"title": "Response to Reviewer 4", "comment": "We thank the reviewer for the comments. We have revised the paper according to the suggestions and would like to clarify several things:\n\n\u30fbMath notation consistency\n> I have updated the paper for the notations to be more consistent. It would be great if you could have a look at sections 2&3 and Algorithm 1 in Appendix.\n\n\u30fbGradient of loss functions\n> I have added more explanation in section 3.3 so that readers can understand the gradient / how the loss functions are minimized along which network parameters.\n\n\u30fbClarification on why Imitation does not need longer term objective\n> As the expert data is created as demonstrations that maximizes long-term return beforehand, what the agent has to do in imitation learning is simply to match its policy to demonstrations at each time step without looking ahead. In contrast, in RL, the agent has to look at future rewards. \n\n\u30fbEvidence on achieving better performance\n> I have updated the abstract. The main contribution of this paper is to propose a FEP-based framework that can deal with imitation and RL with the same objectives. Proving the method to be SOTA is not the scope of this paper.\n\n\u30fbExpert data for \u201cPlaNet with demonstrations\u201d\u00a0in section 4.2\n> As expert demonstration data do not require reward information, we used the same experts from non-sparse environments. For Cheetah and Walker, we used PlaNet at final performance as the experts. For Quadruped, we trained Soft Actor Critic in classic env (not visual control) and I rendered the expert demonstrations and used them as the expert.\n\n\u30fbAlgorithm in section 4.4\n>This section is about ablation studies. I have updated the paper so that readers can understand what each method is about. For example, RL only means the agent only uses F_RL as an objective function, ignoring F_IL.\n\n\u30fbEq. 4 on page 2\n> As is written, F is minimized along the parameters of the variational posterior q(s_t). For all the objective functions of FENet, I have updated section 3.3 to show each objective function is minimized along which parameters.\n\n\u30fbSection 3.1 mentions \u201cwe give a_t a prior in Section 2.2\u201d\n> Not \u201ca prior\u201d, but \u201ca priori\u201d. As the expression was confusing, I have updated this part to be more straightforward.\n\n\u30fbEq. 14, is it equal to the variational posterior as an approximation of p(s_{t+1}|o_{t+1}) on page 2 ?\n> Yes\n\n\u30fbFigure 1 may lack good explanation\n> I have updated Figure 1.\n\n\u30fbConfusion of s_t\n> I have updated the paper so that  s_t means the stochastic part only in section 3.3 and Algorithm1 (the last part of method description). In anywhere before section 3.3, s_t always means the entire hidden state. I hope this flow is natural and readers will not get confused."}, "signatures": ["ICLR.cc/2021/Conference/Paper3288/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3288/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Combining Imitation and Reinforcement Learning with Free Energy Principle", "authorids": ["~Ryoya_Ogishima1", "karino@isi.imi.i.u-tokyo.ac.jp", "~Yasuo_Kuniyoshi1"], "authors": ["Ryoya Ogishima", "Izumi Karino", "Yasuo Kuniyoshi"], "keywords": ["Imitation", "Reinforcement Learning", "Free Energy Principle"], "abstract": "Imitation Learning (IL) and Reinforcement Learning (RL) from high dimensional sensory inputs are often introduced as separate problems, but a more realistic problem setting is how to merge the techniques so that the agent can reduce exploration costs by partially imitating experts at the same time it maximizes its return. Even when the experts are suboptimal (e.g. Experts learned halfway with other RL methods or human-crafted experts), it is expected that the agent outperforms the suboptimal experts\u2019 performance. In this paper, we propose to address the issue by using and theoretically extending Free Energy Principle, a unified brain theory that explains perception, action and model learning in a Bayesian probabilistic way. We find that both IL and RL can be achieved based on the same free energy objective function. Our results show that our approach is promising in visual control tasks especially with sparse-reward environments.", "one-sentence_summary": "Extending Free Energy Principle to achieve imitation reinforcement learning for sparse reward problems with suboptimal experts", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ogishima|combining_imitation_and_reinforcement_learning_with_free_energy_principle", "supplementary_material": "/attachment/3938ad292c9c9665152d7f19e0a7c48b5dbc2ee8.zip", "pdf": "/pdf/cfcf91db3909637376a517eac03f5e0c2dc9488a.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=KvdHIDeYmK", "_bibtex": "@misc{\nogishima2021combining,\ntitle={Combining Imitation and Reinforcement Learning with Free Energy Principle},\nauthor={Ryoya Ogishima and Izumi Karino and Yasuo Kuniyoshi},\nyear={2021},\nurl={https://openreview.net/forum?id=JI2TGOehNT0}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "JI2TGOehNT0", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3288/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3288/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3288/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3288/Authors|ICLR.cc/2021/Conference/Paper3288/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3288/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923839060, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3288/-/Official_Comment"}}}, {"id": "yAUXXkOb9tv", "original": null, "number": 3, "cdate": 1606168112969, "ddate": null, "tcdate": 1606168112969, "tmdate": 1606168112969, "tddate": null, "forum": "JI2TGOehNT0", "replyto": "Z-8KAV9hQJS", "invitation": "ICLR.cc/2021/Conference/Paper3288/-/Official_Comment", "content": {"title": "Response to Reviewer 2", "comment": "We thank the reviewer for the comments. We have revised the paper according to the suggestions and would like to clarify several things:\n\n\u30fbConnection with standard RL contexts\n> I have updated the paper. Connection with optimality variable in RL as inference is now mentioned in \u201cActive Inference\u201d in section 2.2. Connection with Entropy RL is now mentioned in \u201cExpected Free Energy for RL\u201d in section 3.1. Also, if I rephrase FENet in a more RL way, it would be \u201clearning a demonstration policy, and then using this demonstration policy as a prior on an RL objective policy\u201d while maximizing the entropy of the policy. The interesting thing about this paper is that both demonstration policy and RL objective policy are trained on the same loss of the Free Energy.\n\n\u30fbConfusion of s_t to refer to both the entire hidden state and just the stochastic part\n> I have updated the paper so that  s_t means the stochastic part only in section 3.3 and Algorithm1 (the last part of method description). In anywhere before section 3.3, s_t always means the entire hidden state. I hope this flow is natural and readers will not get confused.\n\n\u30fbDefine the RL problem\n> This paper solves POMDP. PlaNet paper says individual image observations generally do not reveal the full state of the environment and uses visual control tasks of Cheetah and Walker as one of the POMDP environments. For example, in visual control tasks, velocity information can be regarded as non/partially observable. If you wanted to talk about occlusion, there is no occlusion in Cheetah/Walker, but still they can be regarded as POMDP environments.\n\n\u30fbExperimental comparison with model-based RL\n> It is correct that as you said the state transition function of FENet is learned but never used for planning, only for inferring the hidden state of the world. Please note that PlaNet is a model-based RL using RSSM (PlaNet does Model Predictive Control). In this paper, FENet is compared with PlaNet as one example of model-based RL.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3288/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3288/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Combining Imitation and Reinforcement Learning with Free Energy Principle", "authorids": ["~Ryoya_Ogishima1", "karino@isi.imi.i.u-tokyo.ac.jp", "~Yasuo_Kuniyoshi1"], "authors": ["Ryoya Ogishima", "Izumi Karino", "Yasuo Kuniyoshi"], "keywords": ["Imitation", "Reinforcement Learning", "Free Energy Principle"], "abstract": "Imitation Learning (IL) and Reinforcement Learning (RL) from high dimensional sensory inputs are often introduced as separate problems, but a more realistic problem setting is how to merge the techniques so that the agent can reduce exploration costs by partially imitating experts at the same time it maximizes its return. Even when the experts are suboptimal (e.g. Experts learned halfway with other RL methods or human-crafted experts), it is expected that the agent outperforms the suboptimal experts\u2019 performance. In this paper, we propose to address the issue by using and theoretically extending Free Energy Principle, a unified brain theory that explains perception, action and model learning in a Bayesian probabilistic way. We find that both IL and RL can be achieved based on the same free energy objective function. Our results show that our approach is promising in visual control tasks especially with sparse-reward environments.", "one-sentence_summary": "Extending Free Energy Principle to achieve imitation reinforcement learning for sparse reward problems with suboptimal experts", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ogishima|combining_imitation_and_reinforcement_learning_with_free_energy_principle", "supplementary_material": "/attachment/3938ad292c9c9665152d7f19e0a7c48b5dbc2ee8.zip", "pdf": "/pdf/cfcf91db3909637376a517eac03f5e0c2dc9488a.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=KvdHIDeYmK", "_bibtex": "@misc{\nogishima2021combining,\ntitle={Combining Imitation and Reinforcement Learning with Free Energy Principle},\nauthor={Ryoya Ogishima and Izumi Karino and Yasuo Kuniyoshi},\nyear={2021},\nurl={https://openreview.net/forum?id=JI2TGOehNT0}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "JI2TGOehNT0", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3288/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3288/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3288/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3288/Authors|ICLR.cc/2021/Conference/Paper3288/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3288/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923839060, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3288/-/Official_Comment"}}}, {"id": "-UN09bQeVF3", "original": null, "number": 2, "cdate": 1606167954449, "ddate": null, "tcdate": 1606167954449, "tmdate": 1606167954449, "tddate": null, "forum": "JI2TGOehNT0", "replyto": "BK_EjD3dSY", "invitation": "ICLR.cc/2021/Conference/Paper3288/-/Official_Comment", "content": {"title": "Response to Reviewer 3", "comment": "We thank the reviewer for the comments. We have revised the paper according to the suggestions and would like to clarify several things:\n\n\u30fbThe difference between prior and posterior policies in both the RL and imitation learning setting\n> As you can see in Algorithm 1 in Appendix, there are 2 phases (imitation and RL) in one iteration. In imitation learning, the policy prior is trained to match expert data (minimizing F) while the policy posterior is trained to match the policy prior (minimizing G^IL). In RL, the policy prior is trained to match agent data (minimizing F) while the policy posterior is trained to maximize reward and explore the world at the same time it tries to match the policy prior. As I have updated section 3, it would be great if you could read this part again.\n\n\u30fbEquation 18 to 19 (eq. 17 to 18 in the first draft)\n> The policy posterior is different from the policy prior, but in this paper, only the policy posterior is used not only when the agent moves in real, but also when the agent makes inference as in Figure 1. Thus, likelihood to the next state and prior probability of the next state are identical.\n\n\u30fbThe assumption that the likelihood of the next state is proportional to inverse exponentiated reward.\n> It is correct. I have updated the paper to state this assumption in \u201cExpected Free Energy for RL\u201d in section 3.1.\n\n\u30fbConnection with entropy-maximizing policy\n> FENet maximizes the entropy of the posterior policy. I have updated the paper to explain this connection in \u201cExpected Free Energy for RL\u201d in section 3.1.\n\n\u30fbMore realistic environments other than deepmind control\n> It is possible to try FENet on more realistic environments. As the purpose of this paper is to introduce how to theoretically extend Free Energy, more practical experiments are left for future work.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3288/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3288/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Combining Imitation and Reinforcement Learning with Free Energy Principle", "authorids": ["~Ryoya_Ogishima1", "karino@isi.imi.i.u-tokyo.ac.jp", "~Yasuo_Kuniyoshi1"], "authors": ["Ryoya Ogishima", "Izumi Karino", "Yasuo Kuniyoshi"], "keywords": ["Imitation", "Reinforcement Learning", "Free Energy Principle"], "abstract": "Imitation Learning (IL) and Reinforcement Learning (RL) from high dimensional sensory inputs are often introduced as separate problems, but a more realistic problem setting is how to merge the techniques so that the agent can reduce exploration costs by partially imitating experts at the same time it maximizes its return. Even when the experts are suboptimal (e.g. Experts learned halfway with other RL methods or human-crafted experts), it is expected that the agent outperforms the suboptimal experts\u2019 performance. In this paper, we propose to address the issue by using and theoretically extending Free Energy Principle, a unified brain theory that explains perception, action and model learning in a Bayesian probabilistic way. We find that both IL and RL can be achieved based on the same free energy objective function. Our results show that our approach is promising in visual control tasks especially with sparse-reward environments.", "one-sentence_summary": "Extending Free Energy Principle to achieve imitation reinforcement learning for sparse reward problems with suboptimal experts", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ogishima|combining_imitation_and_reinforcement_learning_with_free_energy_principle", "supplementary_material": "/attachment/3938ad292c9c9665152d7f19e0a7c48b5dbc2ee8.zip", "pdf": "/pdf/cfcf91db3909637376a517eac03f5e0c2dc9488a.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=KvdHIDeYmK", "_bibtex": "@misc{\nogishima2021combining,\ntitle={Combining Imitation and Reinforcement Learning with Free Energy Principle},\nauthor={Ryoya Ogishima and Izumi Karino and Yasuo Kuniyoshi},\nyear={2021},\nurl={https://openreview.net/forum?id=JI2TGOehNT0}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "JI2TGOehNT0", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3288/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3288/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3288/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3288/Authors|ICLR.cc/2021/Conference/Paper3288/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3288/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923839060, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3288/-/Official_Comment"}}}, {"id": "6aOeaNg6_Ni", "original": null, "number": 1, "cdate": 1603206463805, "ddate": null, "tcdate": 1603206463805, "tmdate": 1605024028293, "tddate": null, "forum": "JI2TGOehNT0", "replyto": "JI2TGOehNT0", "invitation": "ICLR.cc/2021/Conference/Paper3288/-/Official_Review", "content": {"title": "My main concerns are related to the clarity of the paper", "review": "This paper aims at merging Imitation Learning (IL) and Reinforcement Learning (RL) from high dimensional sensory inputs so that the agent can make use of expert trajectories, even when the experts are suboptimal. This paper aims at addressing this by theoretically using the \"Free Energy Principle\", which the authors define in the abstract as a unified brain theory that explains perception. \n\nThe paper tackles an important problem and develops many theoretical functionals. The approach is then tested on a few benchmarks from the DeepMind Control Suite where the approach is reported to work well.\n\nMy main concerns are related to the clarity of the paper, which does not allow me to understand some key parts.\n\nIt is unclear what are the minimized loss functions: it is mentioned that equations 27-29 sum up all objective functions. Since F_t and G_{t+1}^{RL} are appearing twice, does it mean that those losses have a two times more important contribution than for instance G_{t+1}^{IL}? In addition how exactly are equations 27-29 derived from the other equations?\n\nNotations are not always consistent:\n- F_{IL} F_{RL} are sometimes with curly F sometimes not (equations 27-29 and the few lines that follow Figure 1 page 5).\n- First line equation 7 and first line equation 10: Why is there a subscript for F and not G? In other parts of the paper, a subscript is used for G as well.\n\nSome sentences are unclear:\n- \"in RL, using the value function to predict rewards in the long-term future is essential to avoid a local minimum and achieve the desired goal.\" What kind of local minimum does that refer to?\n\nA few typos:\n- \"Then We (...)\"\n- \"the agent easily fall down\"", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3288/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3288/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Combining Imitation and Reinforcement Learning with Free Energy Principle", "authorids": ["~Ryoya_Ogishima1", "karino@isi.imi.i.u-tokyo.ac.jp", "~Yasuo_Kuniyoshi1"], "authors": ["Ryoya Ogishima", "Izumi Karino", "Yasuo Kuniyoshi"], "keywords": ["Imitation", "Reinforcement Learning", "Free Energy Principle"], "abstract": "Imitation Learning (IL) and Reinforcement Learning (RL) from high dimensional sensory inputs are often introduced as separate problems, but a more realistic problem setting is how to merge the techniques so that the agent can reduce exploration costs by partially imitating experts at the same time it maximizes its return. Even when the experts are suboptimal (e.g. Experts learned halfway with other RL methods or human-crafted experts), it is expected that the agent outperforms the suboptimal experts\u2019 performance. In this paper, we propose to address the issue by using and theoretically extending Free Energy Principle, a unified brain theory that explains perception, action and model learning in a Bayesian probabilistic way. We find that both IL and RL can be achieved based on the same free energy objective function. Our results show that our approach is promising in visual control tasks especially with sparse-reward environments.", "one-sentence_summary": "Extending Free Energy Principle to achieve imitation reinforcement learning for sparse reward problems with suboptimal experts", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ogishima|combining_imitation_and_reinforcement_learning_with_free_energy_principle", "supplementary_material": "/attachment/3938ad292c9c9665152d7f19e0a7c48b5dbc2ee8.zip", "pdf": "/pdf/cfcf91db3909637376a517eac03f5e0c2dc9488a.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=KvdHIDeYmK", "_bibtex": "@misc{\nogishima2021combining,\ntitle={Combining Imitation and Reinforcement Learning with Free Energy Principle},\nauthor={Ryoya Ogishima and Izumi Karino and Yasuo Kuniyoshi},\nyear={2021},\nurl={https://openreview.net/forum?id=JI2TGOehNT0}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "JI2TGOehNT0", "replyto": "JI2TGOehNT0", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3288/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538078470, "tmdate": 1606915778408, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3288/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3288/-/Official_Review"}}}, {"id": "JTk4S_Fv1EY", "original": null, "number": 2, "cdate": 1603953611766, "ddate": null, "tcdate": 1603953611766, "tmdate": 1605024028226, "tddate": null, "forum": "JI2TGOehNT0", "replyto": "JI2TGOehNT0", "invitation": "ICLR.cc/2021/Conference/Paper3288/-/Official_Review", "content": {"title": "An interesting paper shows theoretical framework to explain imitation learning and RL with free energy principle", "review": "This manuscript develops RL algorithms that can learn reward from expert data and even exceed the expert performance. It introduces free energy framework that combines ideas of imitation learning and reinforcement learning in a Bayesian probabilistic way. The methodology is well written. It is interesting that they find the algorithm can deal with both imitation learning and RL by minimizing the same Free energy objective function.  The expert data provides policy prior. Then, the policy posterior will try to match the policy prior as the imitation learning, and at the same time, it is motivated to maximize the reward and explore the environment by maximizing the KL divergence between the state posterior and the state posterior given imaged observations. \n\nWith the proposed objective function, the algorithm applies existing recurrent state space model to solve the problem. Though it applies existing model, it would enhance the paper if it can show further analysis of the algorithm in this context, particularly from perspective of imitation learning. And the paper may need to enhance the empirical study by testing more problems and provide convincing analysis.\n\nThe math notations could be more consistent, as in Section 3 it widely uses variables like q(s_{\\tau}), q(s_{\\tau}|o_{\\tau}), p(a_{t}|s_{t}), but in Appendix, there is no  q(s_{\\tau}), it defines the variables like q_{\\phi}(s_{t}|h_{t}, o_{t}), p_{\\theta}(a_{t}|s_{t}, h_{t}) etc. \n\nIt may lack details on the the gradient of energy function. In Algorithm 1, there are important steps to update the parameters, it is not straightforward to find how to get the gradient from the functions 27 - 29.\n\nIn section 3.2, it may need further clarification on why imitation learning doesn\u2019t need longer term objective. The assumption seems not straightforward, as usually the agent mimics the expert behavior to guarantee its expected long term reward is similar as the expert.\n\nThough the abstract mentions the proposed algorithm can achieve better performance than standard IL or RL, there isn\u2019t sufficient evidence. It may need theoretical analysis or rich empirical results. The experiments could study more simulation problems, and may compare with state-of-the-art inverse RL algorithms. Other questions related to experiments include:\n- How to set up the expert data for  \u201cPlaNet with demonstrations\u201d in Section 4.2?\n- Can experiments show the reward shaping benefit using virtualization to compare different reward estimations and the ground truth?\n- The behavior cloning algorithms compared in Section 4.3 seem basic in the field, it may be worth comparing with \u201cPlaNet with suboptimal expert data\u201d, or other inverse RL algorithms. \n- In Section 4.4, it may need to provide reference to the algorithms in study. Are they known algorithms, eg. which imitation RL algorithm is used,  which algorithm is the imitation pretrained RL (which algorithm is used to learn model and which RL is employed  after that), and which RL algorithm is used in \u2018RL only\u2019. \n\nSome questions are written below.\n- On page 2, it could make clear which parameters are optimized to minimize the Free Energy function of Eq. (4)\n- In Eq. (8) when it replaces p(s_{\\tau}|o_{\\tau}) with q(s_{\\tau}|o_{\\tau}), it may be better to explain how to get the approximation and whether there is error bound.\n- It would be interesting to know the convergence analysis of Algorithm 1, is it always convergent.\n- Section 3.1 mentions we give a_t a prior in Section 2.2, but it seems there isn't a prior of a_t in section 2.2.\n- It may be worth explaining q(s_{\\tau}) in Eq. 14, is it equal to the variational posterior  as an approximation of p(s_{t+1}|o_{t+1})  on page 2.?\n- It may need explanation on how to reach Eq.15 from Eq.12? Is it directly derived from KL(q(o_{\\tau}, s_{\\tau}, a_{\\tau})||p(o_{\\tau}, s_{\\tau}, a_{\\tau})?\n- Figure 1 may lack good explanation, as the graph looks complicated with many math notations, not self-explanatory.\n- The notation $s_{t}$ in section 3.3 may be confusing as $s_{t}$ refers to both hidden state and the stochastic part of it. In Figure 1, the deterministic transition only depends on h_t and a_t, while it depends on s_{t-1} too in  Eq.30.\n- In quadruped-walk experiments, like Figure 3,  it seems the solution is not convergent yet. Why do FEnet and planet w/demo have less number of iterations?\n\n", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper3288/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3288/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Combining Imitation and Reinforcement Learning with Free Energy Principle", "authorids": ["~Ryoya_Ogishima1", "karino@isi.imi.i.u-tokyo.ac.jp", "~Yasuo_Kuniyoshi1"], "authors": ["Ryoya Ogishima", "Izumi Karino", "Yasuo Kuniyoshi"], "keywords": ["Imitation", "Reinforcement Learning", "Free Energy Principle"], "abstract": "Imitation Learning (IL) and Reinforcement Learning (RL) from high dimensional sensory inputs are often introduced as separate problems, but a more realistic problem setting is how to merge the techniques so that the agent can reduce exploration costs by partially imitating experts at the same time it maximizes its return. Even when the experts are suboptimal (e.g. Experts learned halfway with other RL methods or human-crafted experts), it is expected that the agent outperforms the suboptimal experts\u2019 performance. In this paper, we propose to address the issue by using and theoretically extending Free Energy Principle, a unified brain theory that explains perception, action and model learning in a Bayesian probabilistic way. We find that both IL and RL can be achieved based on the same free energy objective function. Our results show that our approach is promising in visual control tasks especially with sparse-reward environments.", "one-sentence_summary": "Extending Free Energy Principle to achieve imitation reinforcement learning for sparse reward problems with suboptimal experts", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ogishima|combining_imitation_and_reinforcement_learning_with_free_energy_principle", "supplementary_material": "/attachment/3938ad292c9c9665152d7f19e0a7c48b5dbc2ee8.zip", "pdf": "/pdf/cfcf91db3909637376a517eac03f5e0c2dc9488a.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=KvdHIDeYmK", "_bibtex": "@misc{\nogishima2021combining,\ntitle={Combining Imitation and Reinforcement Learning with Free Energy Principle},\nauthor={Ryoya Ogishima and Izumi Karino and Yasuo Kuniyoshi},\nyear={2021},\nurl={https://openreview.net/forum?id=JI2TGOehNT0}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "JI2TGOehNT0", "replyto": "JI2TGOehNT0", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3288/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538078470, "tmdate": 1606915778408, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3288/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3288/-/Official_Review"}}}, {"id": "Z-8KAV9hQJS", "original": null, "number": 3, "cdate": 1604022520787, "ddate": null, "tcdate": 1604022520787, "tmdate": 1605024028138, "tddate": null, "forum": "JI2TGOehNT0", "replyto": "JI2TGOehNT0", "invitation": "ICLR.cc/2021/Conference/Paper3288/-/Official_Review", "content": {"title": "Interesting idea, needs clearer communication and experimental section", "review": "This paper extends and explains how to apply the \"free energy principle\" and active inference to RL and imitation learning. They implement a neural network approximation of losses derived this way and test on some control tasks. Importantly the tasks focus on here are imitation + control tasks. That is, there is both a reward signal but also demonstration trajectories. The demonstrations may be suboptimal. The compare against PLaNet, a latent planning based approach.\n\nIt was difficult to evaluate this work well, as I found the approach difficult to follow. My primary concern about this paper is connecting it with other work and ensuring the terminology and approach is as clear as possible.\n\nFirstly, in the initial derivation up to equation 9. By defining the observation prior $p(o) \\propto \\exp r(o)$ I believe this results in the same thing as \"RL as inference\" (e.g. [1] which states that \"formulation proposed by Friston (2009) is similar to the maximum entropy approach outlined in this survey\"). It would be good to make this connection clear for the reader. In particular, at least from my perspective, connecting to existing maximum entropy RL is helpful for understanding this work.\n\nHowever, this leads to a concern of terminology. I realize the use of the term \"observation\" for $p(o)$ is to maintain the connection with the free energy formulation, but if the \"observation\" is really just the reward, then in the context of an RL paper this seems to lead to a lot of confusion if when you say \"observation\" you really just mean reward. In other places observation seems to mean the observation in a POMDP (e.g. in the algorithm, where reward is a separate variable).\n\nAnother source of confusion is using $s_t$ to refer to both the entire hidden state and just the stochastic part (3.3).\n\nIt would helpful to define the RL problem formally that is attempted to be solved here. It appears that is a partially observed MDP (which makes the use of \"observation\" even more confusing). It is not clear the the test environments are particularly partially observable, since the image reveals the full state of the world and these tasks have been solved with models that don't deal with partially observable states.\n\nI think another view of this paper is that it is applying maximum entropy RL approach to a partially observed MDP, learning a demonstration policy, and then using this demonstration policy as a prior on an RL objective policy. Approaching this topic from a different perspective is helpful, but it would be useful to make connection with existing literature much clearer and state an alternative way of viewing the approach from a more standard RL point of view. Alternatively, if these statements are incorrect contrasting what is different would benefit the reader.\n\nThe experimental comparisons seem weak for a few reasons. There seems to be a conflation between model-based RL solutions and RL approaches dealing with partial observability. As far as I can understand the approach introduced here is the second, a state transition function is learned but this never used for planning, only for inferring the hidden state of the world. For this reason, the only comparison being with a model-based control approach seems lacking. It would be interesting to compare against other model-free approaches that incorporate demonstrations such as [3] (this does not deal with partial observability, but it's not clear these problems are partially observable) or [4] ([2] is an example of using a GAIL objective as in [4] + and a reward as is done here). Finally, it appears one of the significant benefits of the algorithm introduced here is to work well with partially observed MDPs, it would be good to construct environments which test this more clearly.\n\nOverall, I think this work introduces interesting ideas. However, it needs to connect this with existing work such as RL as inference view of RL and make the approach clearer. The experimental section would benefit for more comparison with other methods and experiments on environments which are more straightforwardly partially observable.\n\n[1] Levine, Sergey. \"Reinforcement learning and control as probabilistic inference: Tutorial and review.\" arXiv preprint arXiv:1805.00909 (2018).\n\n[2] Zhu, Yuke, et al. \"Reinforcement and imitation learning for diverse visuomotor skills.\" arXiv preprint arXiv:1802.09564 (2018).\n\n[3] Vecerik, Mel, et al. \"Leveraging demonstrations for deep reinforcement learning on robotics problems with sparse rewards.\" arXiv preprint arXiv:1707.08817 (2017).\n\n[4] Torabi, Faraz, Garrett Warnell, and Peter Stone. \"Generative adversarial imitation from observation.\" arXiv preprint arXiv:1807.06158 (2018).", "rating": "5: Marginally below acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2021/Conference/Paper3288/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3288/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Combining Imitation and Reinforcement Learning with Free Energy Principle", "authorids": ["~Ryoya_Ogishima1", "karino@isi.imi.i.u-tokyo.ac.jp", "~Yasuo_Kuniyoshi1"], "authors": ["Ryoya Ogishima", "Izumi Karino", "Yasuo Kuniyoshi"], "keywords": ["Imitation", "Reinforcement Learning", "Free Energy Principle"], "abstract": "Imitation Learning (IL) and Reinforcement Learning (RL) from high dimensional sensory inputs are often introduced as separate problems, but a more realistic problem setting is how to merge the techniques so that the agent can reduce exploration costs by partially imitating experts at the same time it maximizes its return. Even when the experts are suboptimal (e.g. Experts learned halfway with other RL methods or human-crafted experts), it is expected that the agent outperforms the suboptimal experts\u2019 performance. In this paper, we propose to address the issue by using and theoretically extending Free Energy Principle, a unified brain theory that explains perception, action and model learning in a Bayesian probabilistic way. We find that both IL and RL can be achieved based on the same free energy objective function. Our results show that our approach is promising in visual control tasks especially with sparse-reward environments.", "one-sentence_summary": "Extending Free Energy Principle to achieve imitation reinforcement learning for sparse reward problems with suboptimal experts", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ogishima|combining_imitation_and_reinforcement_learning_with_free_energy_principle", "supplementary_material": "/attachment/3938ad292c9c9665152d7f19e0a7c48b5dbc2ee8.zip", "pdf": "/pdf/cfcf91db3909637376a517eac03f5e0c2dc9488a.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=KvdHIDeYmK", "_bibtex": "@misc{\nogishima2021combining,\ntitle={Combining Imitation and Reinforcement Learning with Free Energy Principle},\nauthor={Ryoya Ogishima and Izumi Karino and Yasuo Kuniyoshi},\nyear={2021},\nurl={https://openreview.net/forum?id=JI2TGOehNT0}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "JI2TGOehNT0", "replyto": "JI2TGOehNT0", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3288/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538078470, "tmdate": 1606915778408, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3288/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3288/-/Official_Review"}}}], "count": 12}