{"notes": [{"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396548450, "tcdate": 1486396548450, "number": 1, "id": "ByhY3ML_l", "invitation": "ICLR.cc/2017/conference/-/paper374/acceptance", "forum": "SJgWQPcxl", "replyto": "SJgWQPcxl", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision", "comment": "The presented approach builds heavily on recent work, but does provide some novelty. The presentation is generally all right, but there are parts of the manuscript that the reviewers feel needed/needs work. All reviewers note that the evaluation and experimental work could be improved."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Multi-view Generative Adversarial Networks", "abstract": "Learning over multi-view data is a challenging problem with strong practical applications. Most related studies focus on the classification point of view and assume that all the views are available at any time. We consider an extension of this framework in two directions. First, based on the BiGAN model, the Multi-view BiGAN (MV-BiGAN) is able to perform density estimation from multi-view inputs. Second, it can deal with missing views and is able to update its prediction when additional views are provided. We illustrate these properties on a set of experiments over different datasets.", "pdf": "/pdf/87d768f43bbef2b334ebeedd863bbd4c3f218cf6.pdf", "TL;DR": "We describe the MV-BiGAN model able to perform density estimation from multiple views, and to update its prediction when additional views are provided", "paperhash": "chen|multiview_generative_adversarial_networks", "conflicts": ["lip6.fr"], "keywords": ["Deep learning", "Supervised Learning"], "authors": ["Micka\u00ebl Chen", "Ludovic Denoyer"], "authorids": ["mickael.chen@lip6.fr", "ludovic.denoyer@lip6.fr"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396548929, "id": "ICLR.cc/2017/conference/-/paper374/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "SJgWQPcxl", "replyto": "SJgWQPcxl", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396548929}}}, {"tddate": null, "tmdate": 1482595709626, "tcdate": 1482595709626, "number": 3, "id": "SkUt6z2Vl", "invitation": "ICLR.cc/2017/conference/-/paper374/official/review", "forum": "SJgWQPcxl", "replyto": "SJgWQPcxl", "signatures": ["ICLR.cc/2017/conference/paper374/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper374/AnonReviewer2"], "content": {"title": "", "rating": "3: Clear rejection", "review": "The paper proposed conditional biGAN and its extension to multi-view biGAN. The main idea of conditional biGAN is to matching the latent variable distributions of two encoders, each of which are conditioned on the observation (\\tilde{x}) and the output (y), respectively, in addition to standard biGAN formulation.\n\nThe description on MV-GAN require more revision. Specifically, the definition on aggregating model, \\Phi, mapping v and a variable s should be clarified. Looking at Equation (8), I can't find a term that constrains the output domain of function v to be the same as a data domain. \n\nExperimental results are not convincing. In most generation results, the observation is not very well preserved. For example, in Figure 6 the second row, background changes significantly from the observation. We also observe such behavior in digit generation example. Preserving attributes like gender is interesting but doesn't seem to be a strong indication that the model learn to correlate observation and the output through latent variable. ", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Multi-view Generative Adversarial Networks", "abstract": "Learning over multi-view data is a challenging problem with strong practical applications. Most related studies focus on the classification point of view and assume that all the views are available at any time. We consider an extension of this framework in two directions. First, based on the BiGAN model, the Multi-view BiGAN (MV-BiGAN) is able to perform density estimation from multi-view inputs. Second, it can deal with missing views and is able to update its prediction when additional views are provided. We illustrate these properties on a set of experiments over different datasets.", "pdf": "/pdf/87d768f43bbef2b334ebeedd863bbd4c3f218cf6.pdf", "TL;DR": "We describe the MV-BiGAN model able to perform density estimation from multiple views, and to update its prediction when additional views are provided", "paperhash": "chen|multiview_generative_adversarial_networks", "conflicts": ["lip6.fr"], "keywords": ["Deep learning", "Supervised Learning"], "authors": ["Micka\u00ebl Chen", "Ludovic Denoyer"], "authorids": ["mickael.chen@lip6.fr", "ludovic.denoyer@lip6.fr"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482595710272, "id": "ICLR.cc/2017/conference/-/paper374/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper374/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper374/AnonReviewer3", "ICLR.cc/2017/conference/paper374/AnonReviewer1", "ICLR.cc/2017/conference/paper374/AnonReviewer2"], "reply": {"forum": "SJgWQPcxl", "replyto": "SJgWQPcxl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper374/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper374/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482595710272}}}, {"tddate": null, "tmdate": 1481932914500, "tcdate": 1481932914500, "number": 2, "id": "rkqugZfVe", "invitation": "ICLR.cc/2017/conference/-/paper374/official/review", "forum": "SJgWQPcxl", "replyto": "SJgWQPcxl", "signatures": ["ICLR.cc/2017/conference/paper374/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper374/AnonReviewer1"], "content": {"title": "nice paper but seems somehow incomplete", "rating": "5: Marginally below acceptance threshold", "review": "This paper builds in the bidirectional GAN (BiGAN) to obtain an extension which can handle multiple views of data. To this end, the authors extend the BiGAN for multiple view aggregation. This is an easy task and just requires the introduction of the additional distribution and accompanying discriminator. The main challenging and novel part is in regularizing the model to avoid instabilities. The authors propose a novel KL divergence based constraint.\n\nAs mentioned above, the approach builds quite heavily on previous ones but it has enough novel elements, in particular the constraint for regularization. This constraint is a reasonable assumption an in practice seems to work well. One downside is that it comes with a parameter \\lambda which controls its strength and which is not obvious how to find efficiently. For example, for the MNIST data it takes a very small value, 10^-5 and for the CELEBA it's 10^-3. It could be that the results are not very sensitive to this value, but there's no discussion concerning this aspect. \n\nApart from the regularization term, the rest of the model construction is well motivated, I agree with the authors that a multi-view approach employing GANs is an interesting topic to consider. \n\nPresentation is in general good although at parts readability is hindered. I feel that the notation is unnecessarily complicated, and some parts of the text too. Furthermore, it wasn't immediately obvious to me what is considered as \\tilde{x} and what is y in the experiments. \n\nAdditionally, most of the discussion on the well-studied area of multi-view learning (intro and sec. 6) omits reference to important prior work which is not based on neural networks. Indeed, some of the issues mentioned as common in today's methods (discrete outputs only, no density estimation...) do not actually exist in many non-neural network approaches. There are too many works to suggest including in the discussion, but I guess the most relevant ones are in the field of probabilistic and non-linear multi-view learning, which is also what MV-BiGAN is doing.\n\nThe experiments presented in the paper are nice illustrations but unfortunately insufficient. Firstly, they only cover the task of generating (small) images and correspond to non-real world settings (I'd actually consider all of the experiments as toy experiments). Furthermore, the most difficult of these experiments (sec. 5.3) is quite unconvincing. If Fig. 6 shows some of the best results that can be achieved, then this is rather disappointing. Important details are also missing: what is exactly the attribute vector used? How many instances exist?\n\nThe two other experiments on 5.1 and 5.2 are well executed. It's important that the authors show a comparison with \\lambda=0. \n\nHowever, beyond showing the validity of the KL term, one can't conclude much about the overall merit of the method as a multi-view learning approach, given the above experiments. \n\nOverall this was a nice paper to read, but seems somehow incomplete.\n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Multi-view Generative Adversarial Networks", "abstract": "Learning over multi-view data is a challenging problem with strong practical applications. Most related studies focus on the classification point of view and assume that all the views are available at any time. We consider an extension of this framework in two directions. First, based on the BiGAN model, the Multi-view BiGAN (MV-BiGAN) is able to perform density estimation from multi-view inputs. Second, it can deal with missing views and is able to update its prediction when additional views are provided. We illustrate these properties on a set of experiments over different datasets.", "pdf": "/pdf/87d768f43bbef2b334ebeedd863bbd4c3f218cf6.pdf", "TL;DR": "We describe the MV-BiGAN model able to perform density estimation from multiple views, and to update its prediction when additional views are provided", "paperhash": "chen|multiview_generative_adversarial_networks", "conflicts": ["lip6.fr"], "keywords": ["Deep learning", "Supervised Learning"], "authors": ["Micka\u00ebl Chen", "Ludovic Denoyer"], "authorids": ["mickael.chen@lip6.fr", "ludovic.denoyer@lip6.fr"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482595710272, "id": "ICLR.cc/2017/conference/-/paper374/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper374/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper374/AnonReviewer3", "ICLR.cc/2017/conference/paper374/AnonReviewer1", "ICLR.cc/2017/conference/paper374/AnonReviewer2"], "reply": {"forum": "SJgWQPcxl", "replyto": "SJgWQPcxl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper374/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper374/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482595710272}}}, {"tddate": null, "tmdate": 1481919816850, "tcdate": 1481919816850, "number": 1, "id": "S1Z8Ta-Nx", "invitation": "ICLR.cc/2017/conference/-/paper374/official/review", "forum": "SJgWQPcxl", "replyto": "SJgWQPcxl", "signatures": ["ICLR.cc/2017/conference/paper374/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper374/AnonReviewer3"], "content": {"title": "nice extensions to Bi-GAN", "rating": "6: Marginally above acceptance threshold", "review": "This paper presents extensions of bidirectional generative adversarial networks to the conditional setting and multi-view setting. The methods are well-motivated, the mathematical derivations appear to be correct, and the presentation is clear enough to me. \n\nI would suggest this paper to be accepted. However, I find it somewhat limited to only present results for generation tasks. I think a main advantage of using bi-GAN (rather than the standard GAN) is the additional inference model that can learn useful features. I am curious about how good the features are for some other supervised (or semi-supervised) learning tasks and what have they really learned.\n\nI also find it interesting that the counterpart of these models under the VAE framework have also been proposed\n- Kihyuk Sohn and Honglak Lee and Xinchen Yan. Learning Structured Output Representation using Deep Conditional Generative Models. NIPS 2015.  (*** contitional VAE ***)\n- Weiran Wang, Xinchen Yan, Honglak Lee, and Karen Livescu. Deep Variational Canonical Correlation Analysis. In submission to ICLR 2017. (*** sort of multi-view VAE ***)\n\nIt would be nice to have discussions and comparisons in future work. ", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Multi-view Generative Adversarial Networks", "abstract": "Learning over multi-view data is a challenging problem with strong practical applications. Most related studies focus on the classification point of view and assume that all the views are available at any time. We consider an extension of this framework in two directions. First, based on the BiGAN model, the Multi-view BiGAN (MV-BiGAN) is able to perform density estimation from multi-view inputs. Second, it can deal with missing views and is able to update its prediction when additional views are provided. We illustrate these properties on a set of experiments over different datasets.", "pdf": "/pdf/87d768f43bbef2b334ebeedd863bbd4c3f218cf6.pdf", "TL;DR": "We describe the MV-BiGAN model able to perform density estimation from multiple views, and to update its prediction when additional views are provided", "paperhash": "chen|multiview_generative_adversarial_networks", "conflicts": ["lip6.fr"], "keywords": ["Deep learning", "Supervised Learning"], "authors": ["Micka\u00ebl Chen", "Ludovic Denoyer"], "authorids": ["mickael.chen@lip6.fr", "ludovic.denoyer@lip6.fr"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482595710272, "id": "ICLR.cc/2017/conference/-/paper374/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper374/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper374/AnonReviewer3", "ICLR.cc/2017/conference/paper374/AnonReviewer1", "ICLR.cc/2017/conference/paper374/AnonReviewer2"], "reply": {"forum": "SJgWQPcxl", "replyto": "SJgWQPcxl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper374/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper374/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482595710272}}}, {"tddate": null, "tmdate": 1481127899293, "tcdate": 1481127899288, "number": 3, "id": "H1QkdnHmg", "invitation": "ICLR.cc/2017/conference/-/paper374/public/comment", "forum": "SJgWQPcxl", "replyto": "BktgfAkmg", "signatures": ["~Mickael_Chen1"], "readers": ["everyone"], "writers": ["~Mickael_Chen1"], "content": {"title": "re: have you used the latent features for some tasks", "comment": "Thank you for your questions. Actually, we only use the MV-GAN for generation, and a future work will be to see if the learned distributions can be used for other tasks e.g semi-supervised learning."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Multi-view Generative Adversarial Networks", "abstract": "Learning over multi-view data is a challenging problem with strong practical applications. Most related studies focus on the classification point of view and assume that all the views are available at any time. We consider an extension of this framework in two directions. First, based on the BiGAN model, the Multi-view BiGAN (MV-BiGAN) is able to perform density estimation from multi-view inputs. Second, it can deal with missing views and is able to update its prediction when additional views are provided. We illustrate these properties on a set of experiments over different datasets.", "pdf": "/pdf/87d768f43bbef2b334ebeedd863bbd4c3f218cf6.pdf", "TL;DR": "We describe the MV-BiGAN model able to perform density estimation from multiple views, and to update its prediction when additional views are provided", "paperhash": "chen|multiview_generative_adversarial_networks", "conflicts": ["lip6.fr"], "keywords": ["Deep learning", "Supervised Learning"], "authors": ["Micka\u00ebl Chen", "Ludovic Denoyer"], "authorids": ["mickael.chen@lip6.fr", "ludovic.denoyer@lip6.fr"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287602052, "id": "ICLR.cc/2017/conference/-/paper374/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SJgWQPcxl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper374/reviewers", "ICLR.cc/2017/conference/paper374/areachairs"], "cdate": 1485287602052}}}, {"tddate": null, "tmdate": 1481127772742, "tcdate": 1481127772735, "number": 1, "id": "SkSwvnSme", "invitation": "ICLR.cc/2017/conference/-/paper374/public/comment", "forum": "SJgWQPcxl", "replyto": "BJgb6j1mx", "signatures": ["~Mickael_Chen1"], "readers": ["everyone"], "writers": ["~Mickael_Chen1"], "content": {"title": "re: A few questions", "comment": "Thank you for the questions:\n\nQ1: n_k refer to the number of features of view k. We are only pointing out that all observations for each view k must be vectors in \\mathbb{R}^n_k\n\nQ2: In the case of the MV-BiGAN, \\tilde{x} is a set of views. Therefore p(z|\\tilde{x}) is approximated with a neural network that takes a set as input. \\Psi is a learned function which goal is to aggregate  the differents views into a single vector. In our experiments, \\phi are either linear transformations or convolutional networks that are learned together with the MV-BiGAN.\n\nQ3: They are two main distinctions between standard CGAN proposed by Mirza & Osindero, 2014 and CV-BiGAN.\nThe first difference concerns the generation process: In CGAN, a noise is added at the input level. (It is concatenated with the input  and then used to generate an output). The CV-BiGAN instead only uses an input which is mapped to a distribution in the latent space. The generator then uses one latent vector sampled from this distribution. Let us illustrate the two processes:\nCGAN: G(x + z) -> y\nCV-BiGAN: G(x) -> mu,sigma ~> z -> y\n\nThe second difference concerns the discriminator: \nCGAN's discriminator learns to differentiate fake pairs (x,y) where x is sampled using G(y) from real pairs (x,y) directly drawn from the dataset. Depending on the data, this task can be too easy for the discriminator, especially when x and y are both images. This can hamper the learning process.\nCV-BiGAN decomposes the task using two discriminators, one for (x,z) and one for (z,y). Then, the discriminators never have access to both the input and the output at the same time making learning easier.\n\nThe paper has been updated and now includes this discussion"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Multi-view Generative Adversarial Networks", "abstract": "Learning over multi-view data is a challenging problem with strong practical applications. Most related studies focus on the classification point of view and assume that all the views are available at any time. We consider an extension of this framework in two directions. First, based on the BiGAN model, the Multi-view BiGAN (MV-BiGAN) is able to perform density estimation from multi-view inputs. Second, it can deal with missing views and is able to update its prediction when additional views are provided. We illustrate these properties on a set of experiments over different datasets.", "pdf": "/pdf/87d768f43bbef2b334ebeedd863bbd4c3f218cf6.pdf", "TL;DR": "We describe the MV-BiGAN model able to perform density estimation from multiple views, and to update its prediction when additional views are provided", "paperhash": "chen|multiview_generative_adversarial_networks", "conflicts": ["lip6.fr"], "keywords": ["Deep learning", "Supervised Learning"], "authors": ["Micka\u00ebl Chen", "Ludovic Denoyer"], "authorids": ["mickael.chen@lip6.fr", "ludovic.denoyer@lip6.fr"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287602052, "id": "ICLR.cc/2017/conference/-/paper374/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SJgWQPcxl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper374/reviewers", "ICLR.cc/2017/conference/paper374/areachairs"], "cdate": 1485287602052}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1481127042015, "tcdate": 1478288120210, "number": 374, "id": "SJgWQPcxl", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "SJgWQPcxl", "signatures": ["~Ludovic_Denoyer1"], "readers": ["everyone"], "content": {"title": "Multi-view Generative Adversarial Networks", "abstract": "Learning over multi-view data is a challenging problem with strong practical applications. Most related studies focus on the classification point of view and assume that all the views are available at any time. We consider an extension of this framework in two directions. First, based on the BiGAN model, the Multi-view BiGAN (MV-BiGAN) is able to perform density estimation from multi-view inputs. Second, it can deal with missing views and is able to update its prediction when additional views are provided. We illustrate these properties on a set of experiments over different datasets.", "pdf": "/pdf/87d768f43bbef2b334ebeedd863bbd4c3f218cf6.pdf", "TL;DR": "We describe the MV-BiGAN model able to perform density estimation from multiple views, and to update its prediction when additional views are provided", "paperhash": "chen|multiview_generative_adversarial_networks", "conflicts": ["lip6.fr"], "keywords": ["Deep learning", "Supervised Learning"], "authors": ["Micka\u00ebl Chen", "Ludovic Denoyer"], "authorids": ["mickael.chen@lip6.fr", "ludovic.denoyer@lip6.fr"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "tmdate": 1480741361490, "tcdate": 1480741361486, "number": 2, "id": "BktgfAkmg", "invitation": "ICLR.cc/2017/conference/-/paper374/pre-review/question", "forum": "SJgWQPcxl", "replyto": "SJgWQPcxl", "signatures": ["ICLR.cc/2017/conference/paper374/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper374/AnonReviewer3"], "content": {"title": "have you used the latent features for some tasks", "question": "I have not found major problems with the formulation, derivation etc. \n\nAt this point, I am curious, have you used the latent low-dimensional features for some downstream tasks? A quick scan indicates the authors are mostly focusing on generation tasks.\n\nIt would be good to consider using those features for the multi-view representation learning tasks, such as those in the deep variational CCA paper. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Multi-view Generative Adversarial Networks", "abstract": "Learning over multi-view data is a challenging problem with strong practical applications. Most related studies focus on the classification point of view and assume that all the views are available at any time. We consider an extension of this framework in two directions. First, based on the BiGAN model, the Multi-view BiGAN (MV-BiGAN) is able to perform density estimation from multi-view inputs. Second, it can deal with missing views and is able to update its prediction when additional views are provided. We illustrate these properties on a set of experiments over different datasets.", "pdf": "/pdf/87d768f43bbef2b334ebeedd863bbd4c3f218cf6.pdf", "TL;DR": "We describe the MV-BiGAN model able to perform density estimation from multiple views, and to update its prediction when additional views are provided", "paperhash": "chen|multiview_generative_adversarial_networks", "conflicts": ["lip6.fr"], "keywords": ["Deep learning", "Supervised Learning"], "authors": ["Micka\u00ebl Chen", "Ludovic Denoyer"], "authorids": ["mickael.chen@lip6.fr", "ludovic.denoyer@lip6.fr"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959315107, "id": "ICLR.cc/2017/conference/-/paper374/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper374/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper374/AnonReviewer1", "ICLR.cc/2017/conference/paper374/AnonReviewer3"], "reply": {"forum": "SJgWQPcxl", "replyto": "SJgWQPcxl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper374/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper374/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959315107}}}, {"tddate": null, "tmdate": 1480731895604, "tcdate": 1480731895599, "number": 1, "id": "BJgb6j1mx", "invitation": "ICLR.cc/2017/conference/-/paper374/pre-review/question", "forum": "SJgWQPcxl", "replyto": "SJgWQPcxl", "signatures": ["ICLR.cc/2017/conference/paper374/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper374/AnonReviewer1"], "content": {"title": "A few questions", "question": "I have a few questions regarding this paper:\n\n1) Does n_k refer to the number of points observed for view k, or to the number of features with which each point is represented? \n\n2) Why is \\phi used? What is the benefit from the basic approach where we let the latent space z deal with the representation of \\tilde{x}? Also, how is \\phi parameterized? Doesn't the consideration of \\phi (instead of just having \\tilde{x}) make training more tricky (i.e. it'd be easier to learn p(z|\\tilde{x}) rather than p(z | \\Psi))?\n\n3) I'd be interested to see more discussion on CGAN vs CV-BiGAN; from the current discussion it seems that CV-BiGAN is like CGAN but with an extra mapping (following BiGAN). Is there a further distinction?\n\nThank you"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Multi-view Generative Adversarial Networks", "abstract": "Learning over multi-view data is a challenging problem with strong practical applications. Most related studies focus on the classification point of view and assume that all the views are available at any time. We consider an extension of this framework in two directions. First, based on the BiGAN model, the Multi-view BiGAN (MV-BiGAN) is able to perform density estimation from multi-view inputs. Second, it can deal with missing views and is able to update its prediction when additional views are provided. We illustrate these properties on a set of experiments over different datasets.", "pdf": "/pdf/87d768f43bbef2b334ebeedd863bbd4c3f218cf6.pdf", "TL;DR": "We describe the MV-BiGAN model able to perform density estimation from multiple views, and to update its prediction when additional views are provided", "paperhash": "chen|multiview_generative_adversarial_networks", "conflicts": ["lip6.fr"], "keywords": ["Deep learning", "Supervised Learning"], "authors": ["Micka\u00ebl Chen", "Ludovic Denoyer"], "authorids": ["mickael.chen@lip6.fr", "ludovic.denoyer@lip6.fr"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959315107, "id": "ICLR.cc/2017/conference/-/paper374/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper374/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper374/AnonReviewer1", "ICLR.cc/2017/conference/paper374/AnonReviewer3"], "reply": {"forum": "SJgWQPcxl", "replyto": "SJgWQPcxl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper374/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper374/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959315107}}}], "count": 9}