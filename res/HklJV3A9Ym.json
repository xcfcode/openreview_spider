{"notes": [{"id": "HklJV3A9Ym", "original": "HyxxU7o9K7", "number": 1411, "cdate": 1538087974687, "ddate": null, "tcdate": 1538087974687, "tmdate": 1545355427312, "tddate": null, "forum": "HklJV3A9Ym", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Approximation capability of neural networks on sets of probability measures and tree-structured data", "abstract": "This paper extends the proof of density of neural networks in the space of continuous (or even measurable) functions on Euclidean spaces to functions on compact sets of probability measures.\nBy doing so the work parallels a more then a decade old results on mean-map embedding of probability measures in reproducing kernel Hilbert spaces.  \nThe work has wide practical consequences for multi-instance learning, where it theoretically justifies some recently proposed constructions.\nThe result is then extended to Cartesian products, yielding universal approximation theorem for tree-structured domains, which naturally occur in data-exchange formats like JSON, XML, YAML, AVRO, and ProtoBuffer. This has important practical implications, as it enables to automatically create an architecture of neural networks for processing structured data (AutoML paradigms), as demonstrated by an accompanied library for JSON format.", "keywords": ["multi-instance learning", "hierarchical models", "universal approximation theorem"], "authorids": ["pevnak@gmail.com", "vojta.kovarik@gmail.com"], "authors": ["Tom\u00e1\u0161 Pevn\u00fd", "Vojt\u011bch Kova\u0159\u00edk"], "TL;DR": "This paper extends the proof of density of neural networks in the space of continuous (or even measurable) functions on Euclidean spaces to functions on compact sets of probability measures. ", "pdf": "/pdf/3367b7bea6c06087eb81ba185c9c6321e8a57e6b.pdf", "paperhash": "pevn\u00fd|approximation_capability_of_neural_networks_on_sets_of_probability_measures_and_treestructured_data", "_bibtex": "@misc{\npevn\u00fd2019approximation,\ntitle={Approximation capability of neural networks on sets of probability measures and tree-structured data},\nauthor={Tom\u00e1\u0161 Pevn\u00fd and Vojt\u011bch Kova\u0159\u00edk},\nyear={2019},\nurl={https://openreview.net/forum?id=HklJV3A9Ym},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "rJlyUaeNlN", "original": null, "number": 1, "cdate": 1544977734710, "ddate": null, "tcdate": 1544977734710, "tmdate": 1545354488658, "tddate": null, "forum": "HklJV3A9Ym", "replyto": "HklJV3A9Ym", "invitation": "ICLR.cc/2019/Conference/-/Paper1411/Meta_Review", "content": {"metareview": "Several reviewers thought the results were not surprising in light of existing universality results, and thought the results were of limited relevance, given that the formalization is not quite in line with real-world networks for MIL. The authors draw out some further justifications in the rebuttal. These should be reintegrated. I agree with the general criticisms regarding relevance to ICLR. Ultimately, this work may belong in a journal.", "confidence": "3: The area chair is somewhat confident", "recommendation": "Reject", "title": "Rewrite needed to address importance of result"}, "signatures": ["ICLR.cc/2019/Conference/Paper1411/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper1411/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Approximation capability of neural networks on sets of probability measures and tree-structured data", "abstract": "This paper extends the proof of density of neural networks in the space of continuous (or even measurable) functions on Euclidean spaces to functions on compact sets of probability measures.\nBy doing so the work parallels a more then a decade old results on mean-map embedding of probability measures in reproducing kernel Hilbert spaces.  \nThe work has wide practical consequences for multi-instance learning, where it theoretically justifies some recently proposed constructions.\nThe result is then extended to Cartesian products, yielding universal approximation theorem for tree-structured domains, which naturally occur in data-exchange formats like JSON, XML, YAML, AVRO, and ProtoBuffer. This has important practical implications, as it enables to automatically create an architecture of neural networks for processing structured data (AutoML paradigms), as demonstrated by an accompanied library for JSON format.", "keywords": ["multi-instance learning", "hierarchical models", "universal approximation theorem"], "authorids": ["pevnak@gmail.com", "vojta.kovarik@gmail.com"], "authors": ["Tom\u00e1\u0161 Pevn\u00fd", "Vojt\u011bch Kova\u0159\u00edk"], "TL;DR": "This paper extends the proof of density of neural networks in the space of continuous (or even measurable) functions on Euclidean spaces to functions on compact sets of probability measures. ", "pdf": "/pdf/3367b7bea6c06087eb81ba185c9c6321e8a57e6b.pdf", "paperhash": "pevn\u00fd|approximation_capability_of_neural_networks_on_sets_of_probability_measures_and_treestructured_data", "_bibtex": "@misc{\npevn\u00fd2019approximation,\ntitle={Approximation capability of neural networks on sets of probability measures and tree-structured data},\nauthor={Tom\u00e1\u0161 Pevn\u00fd and Vojt\u011bch Kova\u0159\u00edk},\nyear={2019},\nurl={https://openreview.net/forum?id=HklJV3A9Ym},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1411/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545352849015, "tddate": null, "super": null, "final": null, "reply": {"forum": "HklJV3A9Ym", "replyto": "HklJV3A9Ym", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1411/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper1411/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1411/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545352849015}}}, {"id": "B1lSnuSWAm", "original": null, "number": 5, "cdate": 1542703276946, "ddate": null, "tcdate": 1542703276946, "tmdate": 1542710535576, "tddate": null, "forum": "HklJV3A9Ym", "replyto": "SkxnuIAp3m", "invitation": "ICLR.cc/2019/Conference/-/Paper1411/Official_Comment", "content": {"title": "Key properties should have proofs even if they aren't surprising.", "comment": "We do not dispute the novelty of the proof, yet we believe that as the number of applications of AI grows, it becomes important to prove even expected results, as the lack of a proof can help us spot the unsound constructions quicker. The proof itself is important for the field of multi-instance learning, since it has been shown in [1] that the MIL NN architecture it addresses is a considerable improvement over the prior art on a wide range (20) of problems.\n\n[1] Pevn\u00fd, Tom\u00e1\u0161, and Petr Somol. \"Using neural network formalism to solve multiple-instance problems.\" International Symposium on Neural Networks. Springer, Cham, 2017."}, "signatures": ["ICLR.cc/2019/Conference/Paper1411/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1411/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1411/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Approximation capability of neural networks on sets of probability measures and tree-structured data", "abstract": "This paper extends the proof of density of neural networks in the space of continuous (or even measurable) functions on Euclidean spaces to functions on compact sets of probability measures.\nBy doing so the work parallels a more then a decade old results on mean-map embedding of probability measures in reproducing kernel Hilbert spaces.  \nThe work has wide practical consequences for multi-instance learning, where it theoretically justifies some recently proposed constructions.\nThe result is then extended to Cartesian products, yielding universal approximation theorem for tree-structured domains, which naturally occur in data-exchange formats like JSON, XML, YAML, AVRO, and ProtoBuffer. This has important practical implications, as it enables to automatically create an architecture of neural networks for processing structured data (AutoML paradigms), as demonstrated by an accompanied library for JSON format.", "keywords": ["multi-instance learning", "hierarchical models", "universal approximation theorem"], "authorids": ["pevnak@gmail.com", "vojta.kovarik@gmail.com"], "authors": ["Tom\u00e1\u0161 Pevn\u00fd", "Vojt\u011bch Kova\u0159\u00edk"], "TL;DR": "This paper extends the proof of density of neural networks in the space of continuous (or even measurable) functions on Euclidean spaces to functions on compact sets of probability measures. ", "pdf": "/pdf/3367b7bea6c06087eb81ba185c9c6321e8a57e6b.pdf", "paperhash": "pevn\u00fd|approximation_capability_of_neural_networks_on_sets_of_probability_measures_and_treestructured_data", "_bibtex": "@misc{\npevn\u00fd2019approximation,\ntitle={Approximation capability of neural networks on sets of probability measures and tree-structured data},\nauthor={Tom\u00e1\u0161 Pevn\u00fd and Vojt\u011bch Kova\u0159\u00edk},\nyear={2019},\nurl={https://openreview.net/forum?id=HklJV3A9Ym},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1411/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621626237, "tddate": null, "super": null, "final": null, "reply": {"forum": "HklJV3A9Ym", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1411/Authors", "ICLR.cc/2019/Conference/Paper1411/Reviewers", "ICLR.cc/2019/Conference/Paper1411/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1411/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1411/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1411/Authors|ICLR.cc/2019/Conference/Paper1411/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1411/Reviewers", "ICLR.cc/2019/Conference/Paper1411/Authors", "ICLR.cc/2019/Conference/Paper1411/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621626237}}}, {"id": "BJeq4UXeR7", "original": null, "number": 4, "cdate": 1542628914005, "ddate": null, "tcdate": 1542628914005, "tmdate": 1542628914005, "tddate": null, "forum": "HklJV3A9Ym", "replyto": "BJgiQFiNhX", "invitation": "ICLR.cc/2019/Conference/-/Paper1411/Official_Comment", "content": {"title": "importance", "comment": "Without disagreeing with the arguments regarding novelty and bag sizes, we would like to add that for the purposes of MIL NN, being able to work with general probability measures is more general than being able to work with functions in L^p(mu) as in [1], since mu has to be fixed and this only gives measures which are absolutely continuous w.r.t. mu. We also hope that for application to MIL NN, our result should be more accessible than [1] --- while our Theorem 5 gives the approximation property for MIL NN directly, some additional effort is required before being able to apply [1] to specific scenarios (the amount of said effort being quite dependent on the readers background).\n\n\n[1] Rossi, Fabrice, and Brieuc Conan-Guez. \"Functional multi-layer perceptron: a non-linear tool for functional data analysis.\" Neural networks 18.1 (2005): 45-60."}, "signatures": ["ICLR.cc/2019/Conference/Paper1411/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1411/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1411/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Approximation capability of neural networks on sets of probability measures and tree-structured data", "abstract": "This paper extends the proof of density of neural networks in the space of continuous (or even measurable) functions on Euclidean spaces to functions on compact sets of probability measures.\nBy doing so the work parallels a more then a decade old results on mean-map embedding of probability measures in reproducing kernel Hilbert spaces.  \nThe work has wide practical consequences for multi-instance learning, where it theoretically justifies some recently proposed constructions.\nThe result is then extended to Cartesian products, yielding universal approximation theorem for tree-structured domains, which naturally occur in data-exchange formats like JSON, XML, YAML, AVRO, and ProtoBuffer. This has important practical implications, as it enables to automatically create an architecture of neural networks for processing structured data (AutoML paradigms), as demonstrated by an accompanied library for JSON format.", "keywords": ["multi-instance learning", "hierarchical models", "universal approximation theorem"], "authorids": ["pevnak@gmail.com", "vojta.kovarik@gmail.com"], "authors": ["Tom\u00e1\u0161 Pevn\u00fd", "Vojt\u011bch Kova\u0159\u00edk"], "TL;DR": "This paper extends the proof of density of neural networks in the space of continuous (or even measurable) functions on Euclidean spaces to functions on compact sets of probability measures. ", "pdf": "/pdf/3367b7bea6c06087eb81ba185c9c6321e8a57e6b.pdf", "paperhash": "pevn\u00fd|approximation_capability_of_neural_networks_on_sets_of_probability_measures_and_treestructured_data", "_bibtex": "@misc{\npevn\u00fd2019approximation,\ntitle={Approximation capability of neural networks on sets of probability measures and tree-structured data},\nauthor={Tom\u00e1\u0161 Pevn\u00fd and Vojt\u011bch Kova\u0159\u00edk},\nyear={2019},\nurl={https://openreview.net/forum?id=HklJV3A9Ym},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1411/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621626237, "tddate": null, "super": null, "final": null, "reply": {"forum": "HklJV3A9Ym", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1411/Authors", "ICLR.cc/2019/Conference/Paper1411/Reviewers", "ICLR.cc/2019/Conference/Paper1411/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1411/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1411/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1411/Authors|ICLR.cc/2019/Conference/Paper1411/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1411/Reviewers", "ICLR.cc/2019/Conference/Paper1411/Authors", "ICLR.cc/2019/Conference/Paper1411/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621626237}}}, {"id": "BJekexQOTQ", "original": null, "number": 2, "cdate": 1542103015105, "ddate": null, "tcdate": 1542103015105, "tmdate": 1542103015105, "tddate": null, "forum": "HklJV3A9Ym", "replyto": "HkxRkOgRnX", "invitation": "ICLR.cc/2019/Conference/-/Paper1411/Official_Comment", "content": {"title": "Relevance to learning representation", "comment": "The truth is that this work has been inspired by difficulty to use neural networks on security-related problems. As has been written in the introduction, most methods (multi-layer perceptron, convolutional neural networks) assumes samples to have a fixed euclidean dimension, or (recurrent neural networks) being sequences of vectors of a fixed dimension. \n\nBut in many domains where you ingesting data using APIs, they comes typically in form of JSON documents (see for example https://www.threatcrowd.org/searchApi/v2/ip/report/?ip=188.40.75.132). This type of data can be elegantly processed by the proposed framework (and the accompanying library). Therefore we believe that it is relevant to ICLR. "}, "signatures": ["ICLR.cc/2019/Conference/Paper1411/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1411/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1411/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Approximation capability of neural networks on sets of probability measures and tree-structured data", "abstract": "This paper extends the proof of density of neural networks in the space of continuous (or even measurable) functions on Euclidean spaces to functions on compact sets of probability measures.\nBy doing so the work parallels a more then a decade old results on mean-map embedding of probability measures in reproducing kernel Hilbert spaces.  \nThe work has wide practical consequences for multi-instance learning, where it theoretically justifies some recently proposed constructions.\nThe result is then extended to Cartesian products, yielding universal approximation theorem for tree-structured domains, which naturally occur in data-exchange formats like JSON, XML, YAML, AVRO, and ProtoBuffer. This has important practical implications, as it enables to automatically create an architecture of neural networks for processing structured data (AutoML paradigms), as demonstrated by an accompanied library for JSON format.", "keywords": ["multi-instance learning", "hierarchical models", "universal approximation theorem"], "authorids": ["pevnak@gmail.com", "vojta.kovarik@gmail.com"], "authors": ["Tom\u00e1\u0161 Pevn\u00fd", "Vojt\u011bch Kova\u0159\u00edk"], "TL;DR": "This paper extends the proof of density of neural networks in the space of continuous (or even measurable) functions on Euclidean spaces to functions on compact sets of probability measures. ", "pdf": "/pdf/3367b7bea6c06087eb81ba185c9c6321e8a57e6b.pdf", "paperhash": "pevn\u00fd|approximation_capability_of_neural_networks_on_sets_of_probability_measures_and_treestructured_data", "_bibtex": "@misc{\npevn\u00fd2019approximation,\ntitle={Approximation capability of neural networks on sets of probability measures and tree-structured data},\nauthor={Tom\u00e1\u0161 Pevn\u00fd and Vojt\u011bch Kova\u0159\u00edk},\nyear={2019},\nurl={https://openreview.net/forum?id=HklJV3A9Ym},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1411/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621626237, "tddate": null, "super": null, "final": null, "reply": {"forum": "HklJV3A9Ym", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1411/Authors", "ICLR.cc/2019/Conference/Paper1411/Reviewers", "ICLR.cc/2019/Conference/Paper1411/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1411/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1411/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1411/Authors|ICLR.cc/2019/Conference/Paper1411/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1411/Reviewers", "ICLR.cc/2019/Conference/Paper1411/Authors", "ICLR.cc/2019/Conference/Paper1411/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621626237}}}, {"id": "HkeHul4P6Q", "original": null, "number": 1, "cdate": 1542041708897, "ddate": null, "tcdate": 1542041708897, "tmdate": 1542041708897, "tddate": null, "forum": "HklJV3A9Ym", "replyto": "HklJV3A9Ym", "invitation": "ICLR.cc/2019/Conference/-/Paper1411/Official_Comment", "content": {"title": "Justifying the usefulness", "comment": "Dear Reviewers,\n\nwe admit that the results aren't \"surprising\". But taking into account the recent paper [1], we believe the results are important. Ref. [1], published last year at NIPS, studies the same approach as described in our paper (previously independently proposed in [2, 3]), but justifies the construction only for a limited case of probability distributions over finite sets. Our paper fills this gap by extending the justification to probability distributions with infinite support.\n\nThe construction seems to be versatile, as it has been recently used in many cited papers, for example in [4] (cited 37 times) it is used within a reasoning module, in [5] (cited 155 times) it is used to learn messages in message passing algorithms for graphs, and in [6] (cited 273 times) it is used for 3D scene recognition.\n\nTaking the above into account, we think that the proof has its place.\n\n[1] Zaheer, Manzil, et al. \"Deep sets.\" Advances in Neural Information Processing Systems. 2017.\n\n[2] Edwards, Harrison, and Amos Storkey. \"Towards a neural statistician.\" arXiv preprint arXiv:1606.02185 (2016).\n\n[3] Pevny, Tomas, and Petr Somol. \"Using Neural Network Formalism to Solve Multiple-Instance Problems.\" arXiv preprint arXiv:1609.07257 (2016).\n\n[4] Santoro, Adam, et al. \"A simple neural network module for relational reasoning.\" Advances in neural information processing systems. 2017\n\n[5] Lin, Guosheng, et al. \"Deeply learning the messages in message passing inference.\" Advances in Neural Information Processing Systems. 2015.\n\n[6] (Qi, Charles R., et al. \"Pointnet: Deep learning on point sets for 3d classification and segmentation.\" Proc. Computer Vision and Pattern Recognition (CVPR), 2017."}, "signatures": ["ICLR.cc/2019/Conference/Paper1411/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1411/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1411/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Approximation capability of neural networks on sets of probability measures and tree-structured data", "abstract": "This paper extends the proof of density of neural networks in the space of continuous (or even measurable) functions on Euclidean spaces to functions on compact sets of probability measures.\nBy doing so the work parallels a more then a decade old results on mean-map embedding of probability measures in reproducing kernel Hilbert spaces.  \nThe work has wide practical consequences for multi-instance learning, where it theoretically justifies some recently proposed constructions.\nThe result is then extended to Cartesian products, yielding universal approximation theorem for tree-structured domains, which naturally occur in data-exchange formats like JSON, XML, YAML, AVRO, and ProtoBuffer. This has important practical implications, as it enables to automatically create an architecture of neural networks for processing structured data (AutoML paradigms), as demonstrated by an accompanied library for JSON format.", "keywords": ["multi-instance learning", "hierarchical models", "universal approximation theorem"], "authorids": ["pevnak@gmail.com", "vojta.kovarik@gmail.com"], "authors": ["Tom\u00e1\u0161 Pevn\u00fd", "Vojt\u011bch Kova\u0159\u00edk"], "TL;DR": "This paper extends the proof of density of neural networks in the space of continuous (or even measurable) functions on Euclidean spaces to functions on compact sets of probability measures. ", "pdf": "/pdf/3367b7bea6c06087eb81ba185c9c6321e8a57e6b.pdf", "paperhash": "pevn\u00fd|approximation_capability_of_neural_networks_on_sets_of_probability_measures_and_treestructured_data", "_bibtex": "@misc{\npevn\u00fd2019approximation,\ntitle={Approximation capability of neural networks on sets of probability measures and tree-structured data},\nauthor={Tom\u00e1\u0161 Pevn\u00fd and Vojt\u011bch Kova\u0159\u00edk},\nyear={2019},\nurl={https://openreview.net/forum?id=HklJV3A9Ym},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1411/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621626237, "tddate": null, "super": null, "final": null, "reply": {"forum": "HklJV3A9Ym", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1411/Authors", "ICLR.cc/2019/Conference/Paper1411/Reviewers", "ICLR.cc/2019/Conference/Paper1411/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1411/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1411/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1411/Authors|ICLR.cc/2019/Conference/Paper1411/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1411/Reviewers", "ICLR.cc/2019/Conference/Paper1411/Authors", "ICLR.cc/2019/Conference/Paper1411/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621626237}}}, {"id": "HkxRkOgRnX", "original": null, "number": 3, "cdate": 1541437414404, "ddate": null, "tcdate": 1541437414404, "tmdate": 1541533154402, "tddate": null, "forum": "HklJV3A9Ym", "replyto": "HklJV3A9Ym", "invitation": "ICLR.cc/2019/Conference/-/Paper1411/Official_Review", "content": {"title": "Useful result on universality. Probably not extremely relevant to ICLR", "review": "The paper investigates the approximation properties of a family of neural networks designed to address multi-instance learning (MIL) problems. The authors show that results well-known for standard one layer architectures extend to the MIL models considered. The authors focus on tree-structured domains showing that their analysis applies to these relevant settings. \n\nThe paper is well written and easy to follow. In particular the theoretical analysis is clear and pleasant to read. \n\nThe main concern is related to the relevance of the result to ICLR. As the authors themselves state, the result is not surprising given the standard universality result of one-layer neural networks (and indeed Thm. 2 heavily relies on this fact to prove the universality of MIL architectures). In this sense the current work might be more suited to a journal venue. \n\n", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1411/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Approximation capability of neural networks on sets of probability measures and tree-structured data", "abstract": "This paper extends the proof of density of neural networks in the space of continuous (or even measurable) functions on Euclidean spaces to functions on compact sets of probability measures.\nBy doing so the work parallels a more then a decade old results on mean-map embedding of probability measures in reproducing kernel Hilbert spaces.  \nThe work has wide practical consequences for multi-instance learning, where it theoretically justifies some recently proposed constructions.\nThe result is then extended to Cartesian products, yielding universal approximation theorem for tree-structured domains, which naturally occur in data-exchange formats like JSON, XML, YAML, AVRO, and ProtoBuffer. This has important practical implications, as it enables to automatically create an architecture of neural networks for processing structured data (AutoML paradigms), as demonstrated by an accompanied library for JSON format.", "keywords": ["multi-instance learning", "hierarchical models", "universal approximation theorem"], "authorids": ["pevnak@gmail.com", "vojta.kovarik@gmail.com"], "authors": ["Tom\u00e1\u0161 Pevn\u00fd", "Vojt\u011bch Kova\u0159\u00edk"], "TL;DR": "This paper extends the proof of density of neural networks in the space of continuous (or even measurable) functions on Euclidean spaces to functions on compact sets of probability measures. ", "pdf": "/pdf/3367b7bea6c06087eb81ba185c9c6321e8a57e6b.pdf", "paperhash": "pevn\u00fd|approximation_capability_of_neural_networks_on_sets_of_probability_measures_and_treestructured_data", "_bibtex": "@misc{\npevn\u00fd2019approximation,\ntitle={Approximation capability of neural networks on sets of probability measures and tree-structured data},\nauthor={Tom\u00e1\u0161 Pevn\u00fd and Vojt\u011bch Kova\u0159\u00edk},\nyear={2019},\nurl={https://openreview.net/forum?id=HklJV3A9Ym},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1411/Official_Review", "cdate": 1542234235360, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HklJV3A9Ym", "replyto": "HklJV3A9Ym", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1411/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335941322, "tmdate": 1552335941322, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1411/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "SkxnuIAp3m", "original": null, "number": 2, "cdate": 1541428851927, "ddate": null, "tcdate": 1541428851927, "tmdate": 1541533154195, "tddate": null, "forum": "HklJV3A9Ym", "replyto": "HklJV3A9Ym", "invitation": "ICLR.cc/2019/Conference/-/Paper1411/Official_Review", "content": {"title": "Interesting but...", "review": "This paper generalizes the universal approximation theorem (usually stated for real functions on some Euclidean space) to real functions on the space of measures (at least a compact set of proba. measures).\n\nThis result might be interesting but not really surprising and the paper does not put any new theoretical ideas or proof techniques. The proof is actually almost identical than in the original paper of Hornik, Stinchcombe and White (89) [and not the 91 paper of Hornik as indicated in the paper], the only difference being a trick on the density of f\\circ h instead of just considering cos() function.\n\nAll in all, the contributions is interesting but really incremental", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper1411/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Approximation capability of neural networks on sets of probability measures and tree-structured data", "abstract": "This paper extends the proof of density of neural networks in the space of continuous (or even measurable) functions on Euclidean spaces to functions on compact sets of probability measures.\nBy doing so the work parallels a more then a decade old results on mean-map embedding of probability measures in reproducing kernel Hilbert spaces.  \nThe work has wide practical consequences for multi-instance learning, where it theoretically justifies some recently proposed constructions.\nThe result is then extended to Cartesian products, yielding universal approximation theorem for tree-structured domains, which naturally occur in data-exchange formats like JSON, XML, YAML, AVRO, and ProtoBuffer. This has important practical implications, as it enables to automatically create an architecture of neural networks for processing structured data (AutoML paradigms), as demonstrated by an accompanied library for JSON format.", "keywords": ["multi-instance learning", "hierarchical models", "universal approximation theorem"], "authorids": ["pevnak@gmail.com", "vojta.kovarik@gmail.com"], "authors": ["Tom\u00e1\u0161 Pevn\u00fd", "Vojt\u011bch Kova\u0159\u00edk"], "TL;DR": "This paper extends the proof of density of neural networks in the space of continuous (or even measurable) functions on Euclidean spaces to functions on compact sets of probability measures. ", "pdf": "/pdf/3367b7bea6c06087eb81ba185c9c6321e8a57e6b.pdf", "paperhash": "pevn\u00fd|approximation_capability_of_neural_networks_on_sets_of_probability_measures_and_treestructured_data", "_bibtex": "@misc{\npevn\u00fd2019approximation,\ntitle={Approximation capability of neural networks on sets of probability measures and tree-structured data},\nauthor={Tom\u00e1\u0161 Pevn\u00fd and Vojt\u011bch Kova\u0159\u00edk},\nyear={2019},\nurl={https://openreview.net/forum?id=HklJV3A9Ym},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1411/Official_Review", "cdate": 1542234235360, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HklJV3A9Ym", "replyto": "HklJV3A9Ym", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1411/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335941322, "tmdate": 1552335941322, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1411/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "BJgiQFiNhX", "original": null, "number": 1, "cdate": 1540827426948, "ddate": null, "tcdate": 1540827426948, "tmdate": 1541533153985, "tddate": null, "forum": "HklJV3A9Ym", "replyto": "HklJV3A9Ym", "invitation": "ICLR.cc/2019/Conference/-/Paper1411/Official_Review", "content": {"title": "The paper proposes a quite straightforward extension of standard results about universal approximation by neural networks on complex domains.", "review": "The authors study in this paper the approximation capabilities of neural networks for real valued functions on probability measure spaces (and on tree structured domains). \n\nThe first step of the paper consists in extending standard NN results to probability measure spaces, that is rather than having finite dimensional vectors as inputs, the NN considered here have probability measures as inputs. The extension to this case is straightforward and closely related to older extension on infinite dimensional spaces (see for instance the seminal paper of Stinchcombe https://doi.org/10.1016/S0893-6080(98)00108-7 and e.g. http://dx.doi.org/10.1016/j.neunet.2004.07.001 for an application to NN with functional inputs). Nothing quite new here.\n\nIn addition, and exactly as in the case of functional inputs, the real world neural networks do not implement what is covered by the theorem but only an approximation of it. This is acknowledged by the authors at the end of Section 2 but in a way that is close to hand waving. Indeed while the probability distribution point is valuable and gives interesting tools in the MIL context, the truth is that we have no reason to assume the bag sizes will grow to infinite or even will be under our control. In fact there are many situations were the bag sizes are part of the data (for instance when a text is embedded in a vector space word by word and then represented as a bag of vectors). Thus proving some form of universal approximation in the multiple instance learning context would need to take this fact into account, something that is not done at all here. \n\nTherefore I believe the contribution of this paper to be somewhat limited. ", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper1411/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Approximation capability of neural networks on sets of probability measures and tree-structured data", "abstract": "This paper extends the proof of density of neural networks in the space of continuous (or even measurable) functions on Euclidean spaces to functions on compact sets of probability measures.\nBy doing so the work parallels a more then a decade old results on mean-map embedding of probability measures in reproducing kernel Hilbert spaces.  \nThe work has wide practical consequences for multi-instance learning, where it theoretically justifies some recently proposed constructions.\nThe result is then extended to Cartesian products, yielding universal approximation theorem for tree-structured domains, which naturally occur in data-exchange formats like JSON, XML, YAML, AVRO, and ProtoBuffer. This has important practical implications, as it enables to automatically create an architecture of neural networks for processing structured data (AutoML paradigms), as demonstrated by an accompanied library for JSON format.", "keywords": ["multi-instance learning", "hierarchical models", "universal approximation theorem"], "authorids": ["pevnak@gmail.com", "vojta.kovarik@gmail.com"], "authors": ["Tom\u00e1\u0161 Pevn\u00fd", "Vojt\u011bch Kova\u0159\u00edk"], "TL;DR": "This paper extends the proof of density of neural networks in the space of continuous (or even measurable) functions on Euclidean spaces to functions on compact sets of probability measures. ", "pdf": "/pdf/3367b7bea6c06087eb81ba185c9c6321e8a57e6b.pdf", "paperhash": "pevn\u00fd|approximation_capability_of_neural_networks_on_sets_of_probability_measures_and_treestructured_data", "_bibtex": "@misc{\npevn\u00fd2019approximation,\ntitle={Approximation capability of neural networks on sets of probability measures and tree-structured data},\nauthor={Tom\u00e1\u0161 Pevn\u00fd and Vojt\u011bch Kova\u0159\u00edk},\nyear={2019},\nurl={https://openreview.net/forum?id=HklJV3A9Ym},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1411/Official_Review", "cdate": 1542234235360, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HklJV3A9Ym", "replyto": "HklJV3A9Ym", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1411/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335941322, "tmdate": 1552335941322, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1411/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 9}