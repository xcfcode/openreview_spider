{"notes": [{"id": "NjF772F4ZZR", "original": "moyrKqc4Z8w", "number": 1698, "cdate": 1601308187853, "ddate": null, "tcdate": 1601308187853, "tmdate": 1615805721856, "tddate": null, "forum": "NjF772F4ZZR", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Learning the Pareto Front with Hypernetworks", "authorids": ["~Aviv_Navon1", "~Aviv_Shamsian1", "~Ethan_Fetaya1", "~Gal_Chechik1"], "authors": ["Aviv Navon", "Aviv Shamsian", "Ethan Fetaya", "Gal Chechik"], "keywords": ["Multi-objective optimization", "multi-task learning"], "abstract": "Multi-objective optimization (MOO) problems are prevalent in machine learning. These problems have a set of optimal solutions, called the Pareto front, where each point on the front represents a different trade-off between possibly conflicting objectives. Recent MOO methods can target a specific desired ray in loss space however, most approaches still face two grave limitations: (i) A separate model has to be trained for each point on the front; and (ii) The exact trade-off must be known before the optimization process. Here, we tackle the problem of learning the entire Pareto front, with the capability of selecting a desired operating point on the front after training. We call this new setup Pareto-Front Learning (PFL).\n\nWe describe an approach to PFL implemented using HyperNetworks, which we term Pareto HyperNetworks (PHNs). PHN learns the entire Pareto front simultaneously using a single hypernetwork, which receives as input a desired preference vector and returns a Pareto-optimal model whose loss vector is in the desired ray. The unified model is runtime efficient compared to training multiple models and generalizes to new operating points not used during training. We evaluate our method on a wide set of problems, from multi-task regression and classification to fairness. PHNs learn the entire Pareto front at roughly the same time as learning a single point on the front and at the same time reach a better solution set. PFL opens the door to new applications where models are selected based on preferences that are only available at run time.", "one-sentence_summary": "A novel approach for learning the entire Pareto front using hypernetworks", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "navon|learning_the_pareto_front_with_hypernetworks", "supplementary_material": "", "pdf": "/pdf/9c01e8c47f7e80e87af0175ac2a5e9a356f518bd.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nnavon2021learning,\ntitle={Learning the Pareto Front with Hypernetworks},\nauthor={Aviv Navon and Aviv Shamsian and Ethan Fetaya and Gal Chechik},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=NjF772F4ZZR}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 11, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "o9f0OkpUk6A", "original": null, "number": 1, "cdate": 1610040505193, "ddate": null, "tcdate": 1610040505193, "tmdate": 1610474112373, "tddate": null, "forum": "NjF772F4ZZR", "replyto": "NjF772F4ZZR", "invitation": "ICLR.cc/2021/Conference/Paper1698/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Poster)", "comment": "The paper proposes a hyper-net method for multi-objective optimization, which trains a neural network that maps preference vector to the corresponding Pareto solution. The proposed idea is interesting and useful, although the evaluation of the work is not overwhelming convincing. The writing of the work can be further improved. \n\nAlso, the basic idea of the work is the almost the same as a concurrent work \"Lin et al 2020. controllable pareto multi-task learning\" which is also submitted to this conference. The paper cited that paper briefly, \"... The proposed method is conceptually similar to our approach...\",  which is too vague and brief. We urge the author to provide a through discussion on the detailed difference and similarity of the works, including empirical comparisons when necessary. "}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning the Pareto Front with Hypernetworks", "authorids": ["~Aviv_Navon1", "~Aviv_Shamsian1", "~Ethan_Fetaya1", "~Gal_Chechik1"], "authors": ["Aviv Navon", "Aviv Shamsian", "Ethan Fetaya", "Gal Chechik"], "keywords": ["Multi-objective optimization", "multi-task learning"], "abstract": "Multi-objective optimization (MOO) problems are prevalent in machine learning. These problems have a set of optimal solutions, called the Pareto front, where each point on the front represents a different trade-off between possibly conflicting objectives. Recent MOO methods can target a specific desired ray in loss space however, most approaches still face two grave limitations: (i) A separate model has to be trained for each point on the front; and (ii) The exact trade-off must be known before the optimization process. Here, we tackle the problem of learning the entire Pareto front, with the capability of selecting a desired operating point on the front after training. We call this new setup Pareto-Front Learning (PFL).\n\nWe describe an approach to PFL implemented using HyperNetworks, which we term Pareto HyperNetworks (PHNs). PHN learns the entire Pareto front simultaneously using a single hypernetwork, which receives as input a desired preference vector and returns a Pareto-optimal model whose loss vector is in the desired ray. The unified model is runtime efficient compared to training multiple models and generalizes to new operating points not used during training. We evaluate our method on a wide set of problems, from multi-task regression and classification to fairness. PHNs learn the entire Pareto front at roughly the same time as learning a single point on the front and at the same time reach a better solution set. PFL opens the door to new applications where models are selected based on preferences that are only available at run time.", "one-sentence_summary": "A novel approach for learning the entire Pareto front using hypernetworks", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "navon|learning_the_pareto_front_with_hypernetworks", "supplementary_material": "", "pdf": "/pdf/9c01e8c47f7e80e87af0175ac2a5e9a356f518bd.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nnavon2021learning,\ntitle={Learning the Pareto Front with Hypernetworks},\nauthor={Aviv Navon and Aviv Shamsian and Ethan Fetaya and Gal Chechik},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=NjF772F4ZZR}\n}"}, "tags": [], "invitation": {"reply": {"forum": "NjF772F4ZZR", "replyto": "NjF772F4ZZR", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040505180, "tmdate": 1610474112357, "id": "ICLR.cc/2021/Conference/Paper1698/-/Decision"}}}, {"id": "K5tA3-pFp-", "original": null, "number": 4, "cdate": 1603901229133, "ddate": null, "tcdate": 1603901229133, "tmdate": 1606750171711, "tddate": null, "forum": "NjF772F4ZZR", "replyto": "NjF772F4ZZR", "invitation": "ICLR.cc/2021/Conference/Paper1698/-/Official_Review", "content": {"title": "Learning the Pareto Front", "review": "The paper proposes a method for multi-objective optimization. The key idea is to learn the entire Pareto front at once by training a hypernetwork that takes preference vector as an inputs and outputs network parameters, which corresponds to a point on the Pareto set with the desired trade-off specified by the preference vector. Specifically, the hypernetwork is a multi-head network where each head outputs a weight tensor of a module in the target network. The method improves HV from the baselines, in several multi-task learning problems, including image classification, regression and, mixed classification and regression.\n\n+) The main contribution of this work is to learn a continuous function that maps a preference vector to network parameters that corresponds to the desired trade-off. The trained hypernetwork generalizes to preference vectors unseen during training so that the required training time for getting models of an arbitrary trade-off is reduced.\n\n+) Compared to prior work, CPMTL, which starts from a Pareto optimal point and extends Pareto front locally around the point, the proposed method attempts to train a single hypernetwork that represents the entire Pareto front.\n\n+) Pareto front provides insights on the trade-off relation between tasks but it usually requires repetitive training of models under different preference settings. I think this method can be useful when understanding the relations among tasks by significantly reducing the training time to obtain the Pareto front.\n\n-) One concern is the scalability to the number of tasks. The amount of reduce in the training time of the proposed method over the base methods depends on how much the method generalize to the preference vectors unseen during training. For example, ideally, the model trained using the preference vectors [0,1] and [1,0] generalizes to [0.5,0.5]. In other words, it is desired that a hypernetwork is trained using less number of samples while generalizes well to arbitrary trade-offs, relying on the smoothness of the hypernetwork. Otherwise, the training requires more number of iterations to sample sufficient number of preference vectors that covers the whole preference vector space to match the original performance, where its size grows exponentially to the number of tasks. Current manuscript lacks experimental or theoretical analysis on this generalization performance.\n\n-) Relating to the above point, I think evaluation measure HV and uniformity are not sufficient to evaluate the overall behavior. HV measures the quality of exploration, but higher HV does not necessarily mean that every model of method A dominates method B. For example, in the left figure of Figure 2, some points of EPO dominates PHN-EPO though HV is higher for PHN-EPO (Table 1). This implies that the proposed method reduces training time at cost of possible degrading movement of Pareto front. In other words, depending on the hyperparameter alpha used for the Dirichlet distribution, under the comparable training time to the baseline, the baseline method may dominate its PHN counter part at certain preferences.\n\n-) And therefore, I think one needs to investigate the accuracy plot (as Figure 2) together with HV and uniformity to better understand the behavior of the method. Some are missing in the current manuscript (e.g. NYUv2)\n\n-) Another concern is the scalability to the target model capacity. In the current manuscript, all the experiments are performed using architectures with small number of parameters (up to \\~0.37M of ENNet), which is much smaller than popular architectures such as ResNet50 (\\~26M). In addition, the required size of the hypernetwork grows at least proportional to the target model capacity,\nand the amount of training data and the training time likely increase accordingly.\n\n-) HyperNetwork require some overhead of memory and computation cost during the test time.\n\n-) Comparison with important prior work CPMTL is necessary.\n\nI would recommend 'accept'. Though I have concerns about the scalability and lack of analysis, I think this work has some advantages stated above.\n\nQuestions:\n1. Discussion on the generalization performance to unseen preference vectors\n2. Comparison with CPMTL\n3. Accuracy plot for NYUv2\n\n___\nThanks for the response. After reading the authors' response and other reviews, I would like to keep my recommendation.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1698/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1698/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning the Pareto Front with Hypernetworks", "authorids": ["~Aviv_Navon1", "~Aviv_Shamsian1", "~Ethan_Fetaya1", "~Gal_Chechik1"], "authors": ["Aviv Navon", "Aviv Shamsian", "Ethan Fetaya", "Gal Chechik"], "keywords": ["Multi-objective optimization", "multi-task learning"], "abstract": "Multi-objective optimization (MOO) problems are prevalent in machine learning. These problems have a set of optimal solutions, called the Pareto front, where each point on the front represents a different trade-off between possibly conflicting objectives. Recent MOO methods can target a specific desired ray in loss space however, most approaches still face two grave limitations: (i) A separate model has to be trained for each point on the front; and (ii) The exact trade-off must be known before the optimization process. Here, we tackle the problem of learning the entire Pareto front, with the capability of selecting a desired operating point on the front after training. We call this new setup Pareto-Front Learning (PFL).\n\nWe describe an approach to PFL implemented using HyperNetworks, which we term Pareto HyperNetworks (PHNs). PHN learns the entire Pareto front simultaneously using a single hypernetwork, which receives as input a desired preference vector and returns a Pareto-optimal model whose loss vector is in the desired ray. The unified model is runtime efficient compared to training multiple models and generalizes to new operating points not used during training. We evaluate our method on a wide set of problems, from multi-task regression and classification to fairness. PHNs learn the entire Pareto front at roughly the same time as learning a single point on the front and at the same time reach a better solution set. PFL opens the door to new applications where models are selected based on preferences that are only available at run time.", "one-sentence_summary": "A novel approach for learning the entire Pareto front using hypernetworks", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "navon|learning_the_pareto_front_with_hypernetworks", "supplementary_material": "", "pdf": "/pdf/9c01e8c47f7e80e87af0175ac2a5e9a356f518bd.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nnavon2021learning,\ntitle={Learning the Pareto Front with Hypernetworks},\nauthor={Aviv Navon and Aviv Shamsian and Ethan Fetaya and Gal Chechik},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=NjF772F4ZZR}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "NjF772F4ZZR", "replyto": "NjF772F4ZZR", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1698/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538112755, "tmdate": 1606915801537, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1698/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1698/-/Official_Review"}}}, {"id": "mTLHGIO13Ge", "original": null, "number": 3, "cdate": 1603896625057, "ddate": null, "tcdate": 1603896625057, "tmdate": 1606640663536, "tddate": null, "forum": "NjF772F4ZZR", "replyto": "NjF772F4ZZR", "invitation": "ICLR.cc/2021/Conference/Paper1698/-/Official_Review", "content": {"title": "The idea of learning the entire Pareto front is interesting. However, the organization and experiments need more elaboration.", "review": "Thanks for the efforts of the authors. After reading the response and other reviews,  I raise my score from 5 to 6. However, the proposed algorithm lacks analysis. So I cannot improve my score further. \n___________________________________________________\n\nSummary: \nThe paper proposes using a hypernetwork to learn the entire Pareto front of a multi-objective optimization.  It develops two approaches using linear scalarization and exact Pareto optimal (EPO), respectively. Experiments on several multi-task learning tasks show its advantage in terms of hypervolume and uniformity.\n\nStrengths: \n+ It is very interesting and useful that the paper tries to learn the entire Pareto front directly.\n+ The paper proposes using hypernetworks to learn the Pareto front for multi-task learning. \n\nWeaknesses: \n- Algorithm 1 is the main contribution. The pseudocode and its descriptions are not clear. For example, it is better to use two algorithms to descript PHN-LS and PHN-EPO, respectively. r is the weights for LS, while it is the ray for EPO. However, r should be pre-given. How and why PHN could optimize the multi-objective optimization for any r? What does \u201cDir(\\alpha)\u201d mean?\n-PHN is a solving method for MOO problems. So the paper should verify that it could learn the entire Pareto front. The paper may test with MOO problems with known Pareto fronts. The results in Figure 2 cannot testify to this claim. This is more important than the experiments on the multi-task learning.\n\nMinor comments:\n1.\tThe legend in Figure2 should be colored.\n2.\tThe captions of the subfigures in Figure 4 should be under them. \n3.\tTypos:\nsampled \u201cform\u201d the m-dimensional-> sampled \u201cfrom\u201d the m-dimensional, \u201cminimas\u201d->\u201dminima\u201d, longer \u201cthen\u201d-> longer \u201cthan\u201d, \u201cfollwoing\" ->\u201cfollowing\"\n", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1698/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1698/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning the Pareto Front with Hypernetworks", "authorids": ["~Aviv_Navon1", "~Aviv_Shamsian1", "~Ethan_Fetaya1", "~Gal_Chechik1"], "authors": ["Aviv Navon", "Aviv Shamsian", "Ethan Fetaya", "Gal Chechik"], "keywords": ["Multi-objective optimization", "multi-task learning"], "abstract": "Multi-objective optimization (MOO) problems are prevalent in machine learning. These problems have a set of optimal solutions, called the Pareto front, where each point on the front represents a different trade-off between possibly conflicting objectives. Recent MOO methods can target a specific desired ray in loss space however, most approaches still face two grave limitations: (i) A separate model has to be trained for each point on the front; and (ii) The exact trade-off must be known before the optimization process. Here, we tackle the problem of learning the entire Pareto front, with the capability of selecting a desired operating point on the front after training. We call this new setup Pareto-Front Learning (PFL).\n\nWe describe an approach to PFL implemented using HyperNetworks, which we term Pareto HyperNetworks (PHNs). PHN learns the entire Pareto front simultaneously using a single hypernetwork, which receives as input a desired preference vector and returns a Pareto-optimal model whose loss vector is in the desired ray. The unified model is runtime efficient compared to training multiple models and generalizes to new operating points not used during training. We evaluate our method on a wide set of problems, from multi-task regression and classification to fairness. PHNs learn the entire Pareto front at roughly the same time as learning a single point on the front and at the same time reach a better solution set. PFL opens the door to new applications where models are selected based on preferences that are only available at run time.", "one-sentence_summary": "A novel approach for learning the entire Pareto front using hypernetworks", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "navon|learning_the_pareto_front_with_hypernetworks", "supplementary_material": "", "pdf": "/pdf/9c01e8c47f7e80e87af0175ac2a5e9a356f518bd.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nnavon2021learning,\ntitle={Learning the Pareto Front with Hypernetworks},\nauthor={Aviv Navon and Aviv Shamsian and Ethan Fetaya and Gal Chechik},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=NjF772F4ZZR}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "NjF772F4ZZR", "replyto": "NjF772F4ZZR", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1698/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538112755, "tmdate": 1606915801537, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1698/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1698/-/Official_Review"}}}, {"id": "M0ohlA8T8X", "original": null, "number": 2, "cdate": 1603893837754, "ddate": null, "tcdate": 1603893837754, "tmdate": 1606568219883, "tddate": null, "forum": "NjF772F4ZZR", "replyto": "NjF772F4ZZR", "invitation": "ICLR.cc/2021/Conference/Paper1698/-/Official_Review", "content": {"title": "Good paper with minor issues", "review": "The paper proposed a novel algorithm for MOO, which combines preference-based methods with hypernetworks, in order to encapsulate the preferences in the network input.\nThe paper is well written, the proposed method is clear, and the experiments are sufficient. \nMy concerns are the following.\n\nFirst, in the abstract you say that\n\"Recent optimization algorithms can target a specific desired ray in loss space,\nbut still face two grave limitations: (i) A separate model has to be learned for\neach point on the front; and (ii) The exact trade-off must be known prior to the\noptimization process. \"\n\nThis is not entirely true. In Reinforcement Learning (RL) manifold-based approach such as\n\nParisi et al, \"Multi-objective Reinforcement Learning through Continuous Pareto Manifold Approximation\"\nYang et al, \"A Generalized Algorithm for Multi-Objective Reinforcement Learning and Policy Adaptation\"\n\ndo not need neither a separate model, nor to know the trade-off a priori. The work of Parisi et al learns a parametrization producing infinitely many solutions at the same time using a specific loss, while Yang et al train a neural network which takes preferences over the objectives as input, and generalizes over them. \nDespite being tested only on MORL, these algorithms can easily be extended to MOO.\nTherefore, I suggest you to rephrase your abstract and introduction to mention these methods.\n\nIn particular, the work of Yang et al is very reminiscent of your algorithm, since both include the preference vector in the network input. \n\nNonetheless, I feel the paper has enough novely thanks to the use of hypernetworks, EPO, and its application to MOO.\nThe evaluation is sufficient, but I would suggest to move the evaluation of evolutionary algorithms to the main section. Evolutionary algorithms are extremely popular in MOO and thanks to a wide variety of fitness functions can learn any frontier, even though they may require longer run time (as your experiments clearly show).\n\nFinally, a bit more analysis on concave frontier would be beneficial. In Figure 1 you show that LS fails in finding concave frontiers, but it seems to me that all experiments have convex frontier. To my understanding, EPO should address this limitation (\"The recent EPO algorithm can guarantee convergence to a local Pareto optimal point on a specified ray r, addressing the theoretical limitations of LS\"). Am I correct? A bit more discussion and experiments with concave frontiers would be a nice addition to the paper.\n\nOverall, I am leaning to accept it but the authors should address the above issues.\n\n**EDIT **\nThe authors have addressed my concerns, and I have increased my score.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1698/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1698/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning the Pareto Front with Hypernetworks", "authorids": ["~Aviv_Navon1", "~Aviv_Shamsian1", "~Ethan_Fetaya1", "~Gal_Chechik1"], "authors": ["Aviv Navon", "Aviv Shamsian", "Ethan Fetaya", "Gal Chechik"], "keywords": ["Multi-objective optimization", "multi-task learning"], "abstract": "Multi-objective optimization (MOO) problems are prevalent in machine learning. These problems have a set of optimal solutions, called the Pareto front, where each point on the front represents a different trade-off between possibly conflicting objectives. Recent MOO methods can target a specific desired ray in loss space however, most approaches still face two grave limitations: (i) A separate model has to be trained for each point on the front; and (ii) The exact trade-off must be known before the optimization process. Here, we tackle the problem of learning the entire Pareto front, with the capability of selecting a desired operating point on the front after training. We call this new setup Pareto-Front Learning (PFL).\n\nWe describe an approach to PFL implemented using HyperNetworks, which we term Pareto HyperNetworks (PHNs). PHN learns the entire Pareto front simultaneously using a single hypernetwork, which receives as input a desired preference vector and returns a Pareto-optimal model whose loss vector is in the desired ray. The unified model is runtime efficient compared to training multiple models and generalizes to new operating points not used during training. We evaluate our method on a wide set of problems, from multi-task regression and classification to fairness. PHNs learn the entire Pareto front at roughly the same time as learning a single point on the front and at the same time reach a better solution set. PFL opens the door to new applications where models are selected based on preferences that are only available at run time.", "one-sentence_summary": "A novel approach for learning the entire Pareto front using hypernetworks", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "navon|learning_the_pareto_front_with_hypernetworks", "supplementary_material": "", "pdf": "/pdf/9c01e8c47f7e80e87af0175ac2a5e9a356f518bd.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nnavon2021learning,\ntitle={Learning the Pareto Front with Hypernetworks},\nauthor={Aviv Navon and Aviv Shamsian and Ethan Fetaya and Gal Chechik},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=NjF772F4ZZR}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "NjF772F4ZZR", "replyto": "NjF772F4ZZR", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1698/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538112755, "tmdate": 1606915801537, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1698/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1698/-/Official_Review"}}}, {"id": "MWyCGHBdyAN", "original": null, "number": 5, "cdate": 1605452842275, "ddate": null, "tcdate": 1605452842275, "tmdate": 1605711814454, "tddate": null, "forum": "NjF772F4ZZR", "replyto": "mTLHGIO13Ge", "invitation": "ICLR.cc/2021/Conference/Paper1698/-/Official_Comment", "content": {"title": "Response to AnonReviewer3", "comment": "Thank you for your helpful and thoughtful feedback. We provide our response below.\n\n**Clarify inference and training procedures:**\n\nFollowing your comment we revised the text [Section 4] to better explain our training and inference procedures: During training, we sample $r$ at each iteration from the Dirichlet distribution with parameter $\\alpha$ ($Dir(\\alpha)$). During inference, the user is free to choose a preferred operating point (preference vector $r$) that is simply given as input to the hypernetwork. The PHN outputs model weights tuned for that preference vector.\n\n**Add experiments with known Pareto front:**\n\nFollowing your comments, we made the following changes: First, we added two new experiments on two popular MOO benchmarks [2,3] which have a known, non-convex Pareto front [Appendix C.1, Figure 7]. The evaluation shows that PHN can learn the entire Pareto front in these problems.\n\nSecond, we added a section (5.1) that discusses in more detail the illustrative example of Figure 1. This problem is a well-known MOO benchmark with a non-convex front [1].\n\nFinally, we point out that experiments on MTL are important as they show that the method works on real challenging problems.\n\n**Graphics:**\n\nClarification of figure 3 (revised manuscript) - the colors represent different preference vectors $r$ in the objective space. The legend is used to distinguish between the different methods.\n\n**Typos:**\n\nThank you, we fixed all typos and grammar issues.\n\n**Citations:**\n\n[1] Carlos Manuel Mira da Fonseca.Multiobjective genetic algorithms with application to control engineering problems. PhD thesis, University of Sheffield, 1995.\n\n[2] Yu G Evtushenko and Mikhail Anatol\u2019evich Posypkin. Nonuniform covering method as applied to multicriteria optimization problems with guaranteed accuracy. Computational Mathematics and Mathematical Physics, 53(2):144\u2013157, 2013.\n\n[3] Eckart Zitzler, Kalyanmoy Deb, and Lothar Thiele. Comparison of multiobjective evolutionary algorithms: Empirical results. Evolutionary computation, 8(2):173\u2013195, 2000.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1698/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1698/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning the Pareto Front with Hypernetworks", "authorids": ["~Aviv_Navon1", "~Aviv_Shamsian1", "~Ethan_Fetaya1", "~Gal_Chechik1"], "authors": ["Aviv Navon", "Aviv Shamsian", "Ethan Fetaya", "Gal Chechik"], "keywords": ["Multi-objective optimization", "multi-task learning"], "abstract": "Multi-objective optimization (MOO) problems are prevalent in machine learning. These problems have a set of optimal solutions, called the Pareto front, where each point on the front represents a different trade-off between possibly conflicting objectives. Recent MOO methods can target a specific desired ray in loss space however, most approaches still face two grave limitations: (i) A separate model has to be trained for each point on the front; and (ii) The exact trade-off must be known before the optimization process. Here, we tackle the problem of learning the entire Pareto front, with the capability of selecting a desired operating point on the front after training. We call this new setup Pareto-Front Learning (PFL).\n\nWe describe an approach to PFL implemented using HyperNetworks, which we term Pareto HyperNetworks (PHNs). PHN learns the entire Pareto front simultaneously using a single hypernetwork, which receives as input a desired preference vector and returns a Pareto-optimal model whose loss vector is in the desired ray. The unified model is runtime efficient compared to training multiple models and generalizes to new operating points not used during training. We evaluate our method on a wide set of problems, from multi-task regression and classification to fairness. PHNs learn the entire Pareto front at roughly the same time as learning a single point on the front and at the same time reach a better solution set. PFL opens the door to new applications where models are selected based on preferences that are only available at run time.", "one-sentence_summary": "A novel approach for learning the entire Pareto front using hypernetworks", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "navon|learning_the_pareto_front_with_hypernetworks", "supplementary_material": "", "pdf": "/pdf/9c01e8c47f7e80e87af0175ac2a5e9a356f518bd.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nnavon2021learning,\ntitle={Learning the Pareto Front with Hypernetworks},\nauthor={Aviv Navon and Aviv Shamsian and Ethan Fetaya and Gal Chechik},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=NjF772F4ZZR}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "NjF772F4ZZR", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1698/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1698/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1698/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1698/Authors|ICLR.cc/2021/Conference/Paper1698/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1698/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923856760, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1698/-/Official_Comment"}}}, {"id": "CzNbFJX2ZPX", "original": null, "number": 7, "cdate": 1605453385467, "ddate": null, "tcdate": 1605453385467, "tmdate": 1605453385467, "tddate": null, "forum": "NjF772F4ZZR", "replyto": "hQluav46dvD", "invitation": "ICLR.cc/2021/Conference/Paper1698/-/Official_Comment", "content": {"title": "Response to AnonReviewer1", "comment": "Thank you for your helpful and thoughtful feedback. We provide our response below.\n\n**Recent similar work:**\n\nThank you for pointing out this highly relevant work. We stress that it is a concurrent work that was published on arxiv after our own work was submitted to ICLR. It was also submitted to ICLR 2021 (https://openreview.net/forum?id=5mhViEOQxaV). We find it encouraging that other researchers find the problem addressed in our paper, namely approximating the entire Pareto front using a single model, interesting and relevant. We revise our paper to acknowledge this work. \n\n**Elaborate on EPO:**\n\nWe added more details on the EPO approach [Section 2] and how we use it in our proposed method PHN-EPO [Section 4].\n\n**Typos:**\n\nThank you, we fixed all typos and grammar issues.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1698/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1698/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning the Pareto Front with Hypernetworks", "authorids": ["~Aviv_Navon1", "~Aviv_Shamsian1", "~Ethan_Fetaya1", "~Gal_Chechik1"], "authors": ["Aviv Navon", "Aviv Shamsian", "Ethan Fetaya", "Gal Chechik"], "keywords": ["Multi-objective optimization", "multi-task learning"], "abstract": "Multi-objective optimization (MOO) problems are prevalent in machine learning. These problems have a set of optimal solutions, called the Pareto front, where each point on the front represents a different trade-off between possibly conflicting objectives. Recent MOO methods can target a specific desired ray in loss space however, most approaches still face two grave limitations: (i) A separate model has to be trained for each point on the front; and (ii) The exact trade-off must be known before the optimization process. Here, we tackle the problem of learning the entire Pareto front, with the capability of selecting a desired operating point on the front after training. We call this new setup Pareto-Front Learning (PFL).\n\nWe describe an approach to PFL implemented using HyperNetworks, which we term Pareto HyperNetworks (PHNs). PHN learns the entire Pareto front simultaneously using a single hypernetwork, which receives as input a desired preference vector and returns a Pareto-optimal model whose loss vector is in the desired ray. The unified model is runtime efficient compared to training multiple models and generalizes to new operating points not used during training. We evaluate our method on a wide set of problems, from multi-task regression and classification to fairness. PHNs learn the entire Pareto front at roughly the same time as learning a single point on the front and at the same time reach a better solution set. PFL opens the door to new applications where models are selected based on preferences that are only available at run time.", "one-sentence_summary": "A novel approach for learning the entire Pareto front using hypernetworks", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "navon|learning_the_pareto_front_with_hypernetworks", "supplementary_material": "", "pdf": "/pdf/9c01e8c47f7e80e87af0175ac2a5e9a356f518bd.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nnavon2021learning,\ntitle={Learning the Pareto Front with Hypernetworks},\nauthor={Aviv Navon and Aviv Shamsian and Ethan Fetaya and Gal Chechik},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=NjF772F4ZZR}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "NjF772F4ZZR", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1698/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1698/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1698/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1698/Authors|ICLR.cc/2021/Conference/Paper1698/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1698/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923856760, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1698/-/Official_Comment"}}}, {"id": "qWCWtzd0qOb", "original": null, "number": 6, "cdate": 1605453045601, "ddate": null, "tcdate": 1605453045601, "tmdate": 1605453295038, "tddate": null, "forum": "NjF772F4ZZR", "replyto": "M0ohlA8T8X", "invitation": "ICLR.cc/2021/Conference/Paper1698/-/Official_Comment", "content": {"title": "Response to AnonReviewer2", "comment": "Thank you for your helpful and thoughtful feedback. We provide our response below.\n\n**Additional relevant work:**\n\nThank you for bringing this to our attention. We acknowledged these relevant studies in the revised paper and rephrased the abstract and introduction accordingly.\n\n**Move EA evaluation to the main text:**\n\nFollowing your suggestion, we moved the evaluation of the genetic algorithm NSGA-III to the main text (Figure 1). We also discuss genetic algorithms in more detail in Sec. 5.1. \n\n**A bit more discussion and experiments with concave frontiers would be a nice addition to the paper:**\n\nEPO (as well as the other MOO approaches) can converge to the non-convex parts of the Pareto front. As a result, PHN-EPO can also learn non-convex frontiers.\nFollowing this comment, we made the following changes to the paper: First, figure 1 corresponds to a popular MOO benchmark [1] with a concave frontier. We now provide the details for Figure 1 in the main paper [Section 5.1]. Second, we also add two new experiments on two MOO benchmarks [2,3] that have known, non-convex Pareto fronts. The results are qualitatively similar to Figure 1, and are provided in Appendix C.1, Figure 7. The evaluations demonstrate that PHN-EPO can generate preference-specific solutions along the entire Pareto front, even for non-convex problems.\n\n**Citations:**\n\n[1] Carlos Manuel Mira da Fonseca.Multiobjective genetic algorithms with application to control engineering problems. PhD thesis, University of Sheffield, 1995.\n\n[2] Yu G Evtushenko and Mikhail Anatol\u2019evich Posypkin. Nonuniform covering method as applied to multicriteria optimization problems with guaranteed accuracy.Computational Mathematics and Mathematical Physics, 53(2):144\u2013157, 2013.\n\n[3] Eckart Zitzler, Kalyanmoy Deb, and Lothar Thiele.  Comparison of multiobjective evolutionary algorithms: Empirical results.Evolutionary computation, 8(2):173\u2013195, 2000. \n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1698/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1698/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning the Pareto Front with Hypernetworks", "authorids": ["~Aviv_Navon1", "~Aviv_Shamsian1", "~Ethan_Fetaya1", "~Gal_Chechik1"], "authors": ["Aviv Navon", "Aviv Shamsian", "Ethan Fetaya", "Gal Chechik"], "keywords": ["Multi-objective optimization", "multi-task learning"], "abstract": "Multi-objective optimization (MOO) problems are prevalent in machine learning. These problems have a set of optimal solutions, called the Pareto front, where each point on the front represents a different trade-off between possibly conflicting objectives. Recent MOO methods can target a specific desired ray in loss space however, most approaches still face two grave limitations: (i) A separate model has to be trained for each point on the front; and (ii) The exact trade-off must be known before the optimization process. Here, we tackle the problem of learning the entire Pareto front, with the capability of selecting a desired operating point on the front after training. We call this new setup Pareto-Front Learning (PFL).\n\nWe describe an approach to PFL implemented using HyperNetworks, which we term Pareto HyperNetworks (PHNs). PHN learns the entire Pareto front simultaneously using a single hypernetwork, which receives as input a desired preference vector and returns a Pareto-optimal model whose loss vector is in the desired ray. The unified model is runtime efficient compared to training multiple models and generalizes to new operating points not used during training. We evaluate our method on a wide set of problems, from multi-task regression and classification to fairness. PHNs learn the entire Pareto front at roughly the same time as learning a single point on the front and at the same time reach a better solution set. PFL opens the door to new applications where models are selected based on preferences that are only available at run time.", "one-sentence_summary": "A novel approach for learning the entire Pareto front using hypernetworks", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "navon|learning_the_pareto_front_with_hypernetworks", "supplementary_material": "", "pdf": "/pdf/9c01e8c47f7e80e87af0175ac2a5e9a356f518bd.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nnavon2021learning,\ntitle={Learning the Pareto Front with Hypernetworks},\nauthor={Aviv Navon and Aviv Shamsian and Ethan Fetaya and Gal Chechik},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=NjF772F4ZZR}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "NjF772F4ZZR", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1698/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1698/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1698/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1698/Authors|ICLR.cc/2021/Conference/Paper1698/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1698/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923856760, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1698/-/Official_Comment"}}}, {"id": "gMbzwHeghW", "original": null, "number": 4, "cdate": 1605452575407, "ddate": null, "tcdate": 1605452575407, "tmdate": 1605453250288, "tddate": null, "forum": "NjF772F4ZZR", "replyto": "_FuQdofSBVs", "invitation": "ICLR.cc/2021/Conference/Paper1698/-/Official_Comment", "content": {"title": "Response to AnonReviewer4 - Part 2", "comment": "**Memory and computation cost during inference:**\n\nIn the standard setting, in which a decision-maker (DM) selects one operating point, there is no overhead at test time. In that case, the HN is used once to generate the desired operating point (target model weights) and only this model is used later on, with no overhead. If the DM wants **flexibility at real-time**, it comes with the cost of memory and computational overhead.\n\n**Comparison with CPMTL:**\n\nIn our original submission, we compared with CPMTL on all multi-MNIST experiments, and found that our approach PHN outperforms it in terms of HV [Table 1]. Following this comment, in the revised paper we add an additional comparison to CPMTL on the NYUv2 experiment [please see Table 3]. The evaluation shows that our approach provides better coverage of the solution space and trade-off curve (measured by HV). We note that CPMTL is evaluated on 55 models (5 initial solutions + 10 extended models per initial solution).\n\n**Citations:**\n\n[1] Guerreiro, Andreia P., Carlos M. Fonseca, and Lu\u00eds Paquete. \"The Hypervolume Indicator: Problems and Algorithms.\" arXiv preprint arXiv:2005.00515 (2020).\n\n[2] Deb, Kalyanmoy. Multi-objective optimization using evolutionary algorithms. Vol. 16. John Wiley & Sons, 2001.\n\n[3] Zitzler, Eckart, Dimo Brockhoff, and Lothar Thiele. \"The hypervolume indicator revisited: On the design of Pareto-compliant indicators via weighted integration.\" In International Conference on Evolutionary Multi-Criterion Optimization, pp. 862-876. Springer, Berlin, Heidelberg, 2007.\n\n[4] Luan, Shangzhen, Chen Chen, Baochang Zhang, Jungong Han, and Jianzhuang Liu. \"Gabor convolutional networks.\" IEEE Transactions on Image Processing 27, no. 9 (2018): 4357-4366.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1698/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1698/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning the Pareto Front with Hypernetworks", "authorids": ["~Aviv_Navon1", "~Aviv_Shamsian1", "~Ethan_Fetaya1", "~Gal_Chechik1"], "authors": ["Aviv Navon", "Aviv Shamsian", "Ethan Fetaya", "Gal Chechik"], "keywords": ["Multi-objective optimization", "multi-task learning"], "abstract": "Multi-objective optimization (MOO) problems are prevalent in machine learning. These problems have a set of optimal solutions, called the Pareto front, where each point on the front represents a different trade-off between possibly conflicting objectives. Recent MOO methods can target a specific desired ray in loss space however, most approaches still face two grave limitations: (i) A separate model has to be trained for each point on the front; and (ii) The exact trade-off must be known before the optimization process. Here, we tackle the problem of learning the entire Pareto front, with the capability of selecting a desired operating point on the front after training. We call this new setup Pareto-Front Learning (PFL).\n\nWe describe an approach to PFL implemented using HyperNetworks, which we term Pareto HyperNetworks (PHNs). PHN learns the entire Pareto front simultaneously using a single hypernetwork, which receives as input a desired preference vector and returns a Pareto-optimal model whose loss vector is in the desired ray. The unified model is runtime efficient compared to training multiple models and generalizes to new operating points not used during training. We evaluate our method on a wide set of problems, from multi-task regression and classification to fairness. PHNs learn the entire Pareto front at roughly the same time as learning a single point on the front and at the same time reach a better solution set. PFL opens the door to new applications where models are selected based on preferences that are only available at run time.", "one-sentence_summary": "A novel approach for learning the entire Pareto front using hypernetworks", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "navon|learning_the_pareto_front_with_hypernetworks", "supplementary_material": "", "pdf": "/pdf/9c01e8c47f7e80e87af0175ac2a5e9a356f518bd.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nnavon2021learning,\ntitle={Learning the Pareto Front with Hypernetworks},\nauthor={Aviv Navon and Aviv Shamsian and Ethan Fetaya and Gal Chechik},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=NjF772F4ZZR}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "NjF772F4ZZR", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1698/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1698/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1698/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1698/Authors|ICLR.cc/2021/Conference/Paper1698/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1698/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923856760, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1698/-/Official_Comment"}}}, {"id": "_FuQdofSBVs", "original": null, "number": 3, "cdate": 1605452427535, "ddate": null, "tcdate": 1605452427535, "tmdate": 1605453215058, "tddate": null, "forum": "NjF772F4ZZR", "replyto": "K5tA3-pFp-", "invitation": "ICLR.cc/2021/Conference/Paper1698/-/Official_Comment", "content": {"title": "Response to AnonReviewer4 - Part 1", "comment": "Thank you for your helpful and thoughtful feedback. We provide our response below.\n\n**Scalability (to tasks) and generalization:**\n\nThank you for the insightful observation. First, we stress that the experiments show that PHN generalizes well, because we test it on unseen preference directions. Following your suggestion, and with the purpose of quantifying generalization to rays that were not seen during training, we conducted an additional, controlled experiment on SARCOS dataset with 7 tasks. During training, we sample rays from a predefined grid. During inference, we measure HV and Uniformity on two sets of rays: (i) Randomly sampled train rays (ii) Randomly sampled test rays from a shifted grid, so they are most distant from the train grid. In both cases, test *samples* are different from training samples.  We find that testing with unseen test rays only causes a minor decrease in both HV and Uniformity compared with train rays. This is consistent with the reviewer's suggestion that the smoothness of the PHN allows it to generalize to unseen rays. Full details of this new experiment are now given in Appendix C.2. \n\nIn addition, we point out that most MOO and MTL benchmarks contain less than 7 tasks. Our SARCOS experiment already considered a setting with higher dimensionality (in terms of num. objectives) than typical MOOs.\n\n**Hypervolume as an evaluation measure:**\n\nThis comment raises a deep question about the right way to **evaluate a set of solutions**. HV is the standard, and most commonly used metric for evaluating a set of solutions, as required in MOO settings [1,2,3]. As such, and to be consistent with the literature, we adopted HV as our main metric through all experiments. Importantly, HV measures more than just exploration, but also the quality of the results, similar to AUC. Indeed, for some preferences, a baseline model might dominate PHN. However, the higher HV obtained by PHN suggests that, on average, PHN dominates the baselines. In Appendix C.4 we show that even when evaluated on the rays that are used to train the baselines, PHN can outperform all baselines (e.g., for all multi-MNIST experiments). This is a somewhat surprising results, as the baselines train a separate model for each ray, and is being tested on that same ray. We attributed this to the inductive bias effect of sharing weights across all preference rays. Lastly, we point out that the Dirichlet parameter alpha is only used at training time by PHN, and is not being used for inference or training. All baselines are always trained and tested on the same evenly spaced rays.\n\n**Alternative performance metrics, report accuracy on NYUv2:**\n\nWe adopted HV as our main metric to align with the MOO literature. There are several issues with using accuracy as a quality measure, which caused the community to avoid it, and prevents us from using it in this paper. (1) First, there is no standard way to evaluate accuracy on a set of solutions, as required in MOO problems, because the accuracy of different tasks should be aggregated, and there is no agreement about the weighted aggregation procedure. (2) Second, unlike HV, accuracy is only applicable to classification tasks. Specifically, it cannot be used to evaluate NYUv2 because it contains a regression task.\n\n**PHN scalability:**\n\nIndeed, the HN size grows linearly with the target network size. However, there are several simple extensions that can be applied to PHN, and make it scale to much larger networks: (i) First, one can learn only part of the target network parameters using HN. For instance, lower layers in vision applications tend to be highly conserved across classes and often even across tasks [4]. (ii) One can use different parts of the ray embedding to generate different weight tensors of the target network, mainly, provide each head of the HN with only a small part of the shared representation; or reduce the embedding size as shown in the ablation study in Appendix C.5.\n\nFollowing your comment, we added a discussion on the scalability of our method in Appendix B.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1698/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1698/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning the Pareto Front with Hypernetworks", "authorids": ["~Aviv_Navon1", "~Aviv_Shamsian1", "~Ethan_Fetaya1", "~Gal_Chechik1"], "authors": ["Aviv Navon", "Aviv Shamsian", "Ethan Fetaya", "Gal Chechik"], "keywords": ["Multi-objective optimization", "multi-task learning"], "abstract": "Multi-objective optimization (MOO) problems are prevalent in machine learning. These problems have a set of optimal solutions, called the Pareto front, where each point on the front represents a different trade-off between possibly conflicting objectives. Recent MOO methods can target a specific desired ray in loss space however, most approaches still face two grave limitations: (i) A separate model has to be trained for each point on the front; and (ii) The exact trade-off must be known before the optimization process. Here, we tackle the problem of learning the entire Pareto front, with the capability of selecting a desired operating point on the front after training. We call this new setup Pareto-Front Learning (PFL).\n\nWe describe an approach to PFL implemented using HyperNetworks, which we term Pareto HyperNetworks (PHNs). PHN learns the entire Pareto front simultaneously using a single hypernetwork, which receives as input a desired preference vector and returns a Pareto-optimal model whose loss vector is in the desired ray. The unified model is runtime efficient compared to training multiple models and generalizes to new operating points not used during training. We evaluate our method on a wide set of problems, from multi-task regression and classification to fairness. PHNs learn the entire Pareto front at roughly the same time as learning a single point on the front and at the same time reach a better solution set. PFL opens the door to new applications where models are selected based on preferences that are only available at run time.", "one-sentence_summary": "A novel approach for learning the entire Pareto front using hypernetworks", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "navon|learning_the_pareto_front_with_hypernetworks", "supplementary_material": "", "pdf": "/pdf/9c01e8c47f7e80e87af0175ac2a5e9a356f518bd.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nnavon2021learning,\ntitle={Learning the Pareto Front with Hypernetworks},\nauthor={Aviv Navon and Aviv Shamsian and Ethan Fetaya and Gal Chechik},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=NjF772F4ZZR}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "NjF772F4ZZR", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1698/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1698/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1698/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1698/Authors|ICLR.cc/2021/Conference/Paper1698/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1698/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923856760, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1698/-/Official_Comment"}}}, {"id": "Td-cCzuIyMw", "original": null, "number": 2, "cdate": 1605451815667, "ddate": null, "tcdate": 1605451815667, "tmdate": 1605451815667, "tddate": null, "forum": "NjF772F4ZZR", "replyto": "NjF772F4ZZR", "invitation": "ICLR.cc/2021/Conference/Paper1698/-/Official_Comment", "content": {"title": "To All Reviewers", "comment": "We thank the reviewers for their helpful and thoughtful feedback. We are encouraged that the reviewers find our work interesting and useful (R3, R4) with enough novelty (R2)  and the paper well-written (R1, R2). They also found the approach to significantly reduce the training time (R4), the experiments substantial (R1, R2), and the results promising (R1) and improve over baselines in several MTL problems (R4).\n\nWe revised the manuscript accordingly to resolve the reviewers\u2019 concerns, and add additional experiments and discussions to address the reviewers\u2019 suggestion. We provide a detailed description of the changes we made in our responses below.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1698/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1698/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning the Pareto Front with Hypernetworks", "authorids": ["~Aviv_Navon1", "~Aviv_Shamsian1", "~Ethan_Fetaya1", "~Gal_Chechik1"], "authors": ["Aviv Navon", "Aviv Shamsian", "Ethan Fetaya", "Gal Chechik"], "keywords": ["Multi-objective optimization", "multi-task learning"], "abstract": "Multi-objective optimization (MOO) problems are prevalent in machine learning. These problems have a set of optimal solutions, called the Pareto front, where each point on the front represents a different trade-off between possibly conflicting objectives. Recent MOO methods can target a specific desired ray in loss space however, most approaches still face two grave limitations: (i) A separate model has to be trained for each point on the front; and (ii) The exact trade-off must be known before the optimization process. Here, we tackle the problem of learning the entire Pareto front, with the capability of selecting a desired operating point on the front after training. We call this new setup Pareto-Front Learning (PFL).\n\nWe describe an approach to PFL implemented using HyperNetworks, which we term Pareto HyperNetworks (PHNs). PHN learns the entire Pareto front simultaneously using a single hypernetwork, which receives as input a desired preference vector and returns a Pareto-optimal model whose loss vector is in the desired ray. The unified model is runtime efficient compared to training multiple models and generalizes to new operating points not used during training. We evaluate our method on a wide set of problems, from multi-task regression and classification to fairness. PHNs learn the entire Pareto front at roughly the same time as learning a single point on the front and at the same time reach a better solution set. PFL opens the door to new applications where models are selected based on preferences that are only available at run time.", "one-sentence_summary": "A novel approach for learning the entire Pareto front using hypernetworks", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "navon|learning_the_pareto_front_with_hypernetworks", "supplementary_material": "", "pdf": "/pdf/9c01e8c47f7e80e87af0175ac2a5e9a356f518bd.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nnavon2021learning,\ntitle={Learning the Pareto Front with Hypernetworks},\nauthor={Aviv Navon and Aviv Shamsian and Ethan Fetaya and Gal Chechik},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=NjF772F4ZZR}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "NjF772F4ZZR", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1698/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1698/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1698/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1698/Authors|ICLR.cc/2021/Conference/Paper1698/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1698/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923856760, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1698/-/Official_Comment"}}}, {"id": "hQluav46dvD", "original": null, "number": 1, "cdate": 1603677370819, "ddate": null, "tcdate": 1603677370819, "tmdate": 1605024379863, "tddate": null, "forum": "NjF772F4ZZR", "replyto": "NjF772F4ZZR", "invitation": "ICLR.cc/2021/Conference/Paper1698/-/Official_Review", "content": {"title": "Learning the Pareto Front with Hypernetworks", "review": "This paper tracks the problem of learning the entire Pareto front to allow the user to select a desired Pareto optimal solution by one inference procedure without retraining the model. The high-level idea is to learn the entire Pareto front simultaneously using a single hyper network, which receives as input the desired preference vector and returns a Pareto-optimal solution whose loss vector is in the desired direction. The paper gives an early trial to build a toolbox to allow users to get a desired solution by a single inference procedure.\n\nStrength:\n1.\tThis paper is an early trial to use a hyper network to directly approximate the Pareto Optimal front, allowing practitioners to flexibly choose Pareto solutions conditioned on different preference vectors.\n2.\tThe paper is well-written and a substantial number of experiments are conducted, promising good results of the proposed method. \n\nFeedbacks:\n1.\tMy major concern is that there is a similar work [1] that shares the same spirit with the work.  The authors might want to clarify the differences or conduct some experimental comparisons.\n2.\tIt is better to attach more details about the EPO algorithm.\n3.\tThere are some typos or grammar issues, such as 1) Page 6, the title of the subsection \u2018Hyperparamter tuning\u2019 should be \u2018Hyperparameter tuning\u2019. 2) Page 6, \u2018We therefor\u2019 should be \u2018We therefore\u2019. 3) Page 6, \u2018as follow\u2019 should be \u2018as follows\u2019. 4) Page 7, \u2018are train and evaluate\u2019 should be \u2018are trained and evaluated\u2019.\n\n[1] X. Lin, Z. Yang, Q. Zhang, and S. Kwong, \u201cControllable Pareto multi-task learning,\u201d arXiv preprint arXiv: 2010.06313, 2020.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1698/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1698/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning the Pareto Front with Hypernetworks", "authorids": ["~Aviv_Navon1", "~Aviv_Shamsian1", "~Ethan_Fetaya1", "~Gal_Chechik1"], "authors": ["Aviv Navon", "Aviv Shamsian", "Ethan Fetaya", "Gal Chechik"], "keywords": ["Multi-objective optimization", "multi-task learning"], "abstract": "Multi-objective optimization (MOO) problems are prevalent in machine learning. These problems have a set of optimal solutions, called the Pareto front, where each point on the front represents a different trade-off between possibly conflicting objectives. Recent MOO methods can target a specific desired ray in loss space however, most approaches still face two grave limitations: (i) A separate model has to be trained for each point on the front; and (ii) The exact trade-off must be known before the optimization process. Here, we tackle the problem of learning the entire Pareto front, with the capability of selecting a desired operating point on the front after training. We call this new setup Pareto-Front Learning (PFL).\n\nWe describe an approach to PFL implemented using HyperNetworks, which we term Pareto HyperNetworks (PHNs). PHN learns the entire Pareto front simultaneously using a single hypernetwork, which receives as input a desired preference vector and returns a Pareto-optimal model whose loss vector is in the desired ray. The unified model is runtime efficient compared to training multiple models and generalizes to new operating points not used during training. We evaluate our method on a wide set of problems, from multi-task regression and classification to fairness. PHNs learn the entire Pareto front at roughly the same time as learning a single point on the front and at the same time reach a better solution set. PFL opens the door to new applications where models are selected based on preferences that are only available at run time.", "one-sentence_summary": "A novel approach for learning the entire Pareto front using hypernetworks", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "navon|learning_the_pareto_front_with_hypernetworks", "supplementary_material": "", "pdf": "/pdf/9c01e8c47f7e80e87af0175ac2a5e9a356f518bd.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nnavon2021learning,\ntitle={Learning the Pareto Front with Hypernetworks},\nauthor={Aviv Navon and Aviv Shamsian and Ethan Fetaya and Gal Chechik},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=NjF772F4ZZR}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "NjF772F4ZZR", "replyto": "NjF772F4ZZR", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1698/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538112755, "tmdate": 1606915801537, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1698/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1698/-/Official_Review"}}}], "count": 12}