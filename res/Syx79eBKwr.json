{"notes": [{"id": "Syx79eBKwr", "original": "BygAo0xKDS", "number": 2466, "cdate": 1569439883310, "ddate": null, "tcdate": 1569439883310, "tmdate": 1583912031920, "tddate": null, "forum": "Syx79eBKwr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["lingpenk@google.com", "cyprien@google.com", "leiyu@google.com", "lingwang@google.com", "zihangd@google.com", "dyogatama@google.com"], "title": "A Mutual Information Maximization Perspective of Language Representation Learning", "authors": ["Lingpeng Kong", "Cyprien de Masson d'Autume", "Lei Yu", "Wang Ling", "Zihang Dai", "Dani Yogatama"], "pdf": "/pdf/eb7c97ac3d4465bf40ae5911244e1fba78fd3314.pdf", "abstract": "We show state-of-the-art word representation learning methods maximize an objective function that is a lower bound on the mutual information between different parts of a word sequence (i.e., a sentence). Our formulation provides an alternative perspective that unifies classical word embedding models (e.g., Skip-gram) and modern contextual embeddings (e.g., BERT, XLNet). In addition to enhancing our theoretical understanding of these methods, our derivation leads to a principled framework that can be used to construct new self-supervised tasks. We provide an example by drawing inspirations from related methods based on mutual information maximization that have been successful in computer vision, and introduce a simple self-supervised objective that maximizes the mutual information between a global sentence representation and n-grams in the sentence. Our analysis offers a holistic view of representation learning methods to transfer knowledge and translate progress across multiple domains (e.g., natural language processing, computer vision, audio processing).", "keywords": [], "paperhash": "kong|a_mutual_information_maximization_perspective_of_language_representation_learning", "_bibtex": "@inproceedings{\nKong2020A,\ntitle={A Mutual Information Maximization Perspective of Language Representation Learning},\nauthor={Lingpeng Kong and Cyprien de Masson d'Autume and Lei Yu and Wang Ling and Zihang Dai and Dani Yogatama},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Syx79eBKwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/b76d744edf0d66b50fe0756cefd4c645652084c2.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 12, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "fTAWD1xiNf", "original": null, "number": 2, "cdate": 1582574028675, "ddate": null, "tcdate": 1582574028675, "tmdate": 1582574028675, "tddate": null, "forum": "Syx79eBKwr", "replyto": "Syx79eBKwr", "invitation": "ICLR.cc/2020/Conference/Paper2466/-/Public_Comment", "content": {"title": "Code for this paper", "comment": "Dear authors,\n\nGreetings!\n\nMay I ask will the code for this paper be possibly public?\n\nThank you so much!"}, "signatures": ["~Martin_Ma1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Martin_Ma1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["lingpenk@google.com", "cyprien@google.com", "leiyu@google.com", "lingwang@google.com", "zihangd@google.com", "dyogatama@google.com"], "title": "A Mutual Information Maximization Perspective of Language Representation Learning", "authors": ["Lingpeng Kong", "Cyprien de Masson d'Autume", "Lei Yu", "Wang Ling", "Zihang Dai", "Dani Yogatama"], "pdf": "/pdf/eb7c97ac3d4465bf40ae5911244e1fba78fd3314.pdf", "abstract": "We show state-of-the-art word representation learning methods maximize an objective function that is a lower bound on the mutual information between different parts of a word sequence (i.e., a sentence). Our formulation provides an alternative perspective that unifies classical word embedding models (e.g., Skip-gram) and modern contextual embeddings (e.g., BERT, XLNet). In addition to enhancing our theoretical understanding of these methods, our derivation leads to a principled framework that can be used to construct new self-supervised tasks. We provide an example by drawing inspirations from related methods based on mutual information maximization that have been successful in computer vision, and introduce a simple self-supervised objective that maximizes the mutual information between a global sentence representation and n-grams in the sentence. Our analysis offers a holistic view of representation learning methods to transfer knowledge and translate progress across multiple domains (e.g., natural language processing, computer vision, audio processing).", "keywords": [], "paperhash": "kong|a_mutual_information_maximization_perspective_of_language_representation_learning", "_bibtex": "@inproceedings{\nKong2020A,\ntitle={A Mutual Information Maximization Perspective of Language Representation Learning},\nauthor={Lingpeng Kong and Cyprien de Masson d'Autume and Lei Yu and Wang Ling and Zihang Dai and Dani Yogatama},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Syx79eBKwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/b76d744edf0d66b50fe0756cefd4c645652084c2.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Syx79eBKwr", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504179884, "tmdate": 1576860568739, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper2466/Authors", "ICLR.cc/2020/Conference/Paper2466/Reviewers", "ICLR.cc/2020/Conference/Paper2466/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2466/-/Public_Comment"}}}, {"id": "r4k0jo0fy9", "original": null, "number": 14, "cdate": 1581687650345, "ddate": null, "tcdate": 1581687650345, "tmdate": 1581687650345, "tddate": null, "forum": "Syx79eBKwr", "replyto": "OFTf9q2VP_", "invitation": "ICLR.cc/2020/Conference/Paper2466/-/Official_Comment", "content": {"title": "reply", "comment": "Thanks for the comments.\n\nFor 1, yes. The \\hat{x_{i:j}} follows the masking budget (15% of the sequence length) and there are several masked n-grams in this sentence.\n\nFor 2, in MLM g_{\\psi} is a simple lookup same as in the original BERT."}, "signatures": ["ICLR.cc/2020/Conference/Paper2466/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2466/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["lingpenk@google.com", "cyprien@google.com", "leiyu@google.com", "lingwang@google.com", "zihangd@google.com", "dyogatama@google.com"], "title": "A Mutual Information Maximization Perspective of Language Representation Learning", "authors": ["Lingpeng Kong", "Cyprien de Masson d'Autume", "Lei Yu", "Wang Ling", "Zihang Dai", "Dani Yogatama"], "pdf": "/pdf/eb7c97ac3d4465bf40ae5911244e1fba78fd3314.pdf", "abstract": "We show state-of-the-art word representation learning methods maximize an objective function that is a lower bound on the mutual information between different parts of a word sequence (i.e., a sentence). Our formulation provides an alternative perspective that unifies classical word embedding models (e.g., Skip-gram) and modern contextual embeddings (e.g., BERT, XLNet). In addition to enhancing our theoretical understanding of these methods, our derivation leads to a principled framework that can be used to construct new self-supervised tasks. We provide an example by drawing inspirations from related methods based on mutual information maximization that have been successful in computer vision, and introduce a simple self-supervised objective that maximizes the mutual information between a global sentence representation and n-grams in the sentence. Our analysis offers a holistic view of representation learning methods to transfer knowledge and translate progress across multiple domains (e.g., natural language processing, computer vision, audio processing).", "keywords": [], "paperhash": "kong|a_mutual_information_maximization_perspective_of_language_representation_learning", "_bibtex": "@inproceedings{\nKong2020A,\ntitle={A Mutual Information Maximization Perspective of Language Representation Learning},\nauthor={Lingpeng Kong and Cyprien de Masson d'Autume and Lei Yu and Wang Ling and Zihang Dai and Dani Yogatama},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Syx79eBKwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/b76d744edf0d66b50fe0756cefd4c645652084c2.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Syx79eBKwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2466/Authors", "ICLR.cc/2020/Conference/Paper2466/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2466/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2466/Reviewers", "ICLR.cc/2020/Conference/Paper2466/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2466/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2466/Authors|ICLR.cc/2020/Conference/Paper2466/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504140933, "tmdate": 1576860535026, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2466/Authors", "ICLR.cc/2020/Conference/Paper2466/Reviewers", "ICLR.cc/2020/Conference/Paper2466/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2466/-/Official_Comment"}}}, {"id": "OFTf9q2VP_", "original": null, "number": 1, "cdate": 1577110004901, "ddate": null, "tcdate": 1577110004901, "tmdate": 1577110004901, "tddate": null, "forum": "Syx79eBKwr", "replyto": "Syx79eBKwr", "invitation": "ICLR.cc/2020/Conference/Paper2466/-/Public_Comment", "content": {"title": "Questions about I_{DIM} and I_{MLM}.", "comment": "Dear authors,\n\nThank you for the interesting paper and congratulations on the acceptance at ICLR.\n\nAfter reading, I have two questions about I_{DIM} and I_{MLM}:\n1. According to the paper, \\hat{x_{i:j}} in I_{DIM} is a sentence masked at position i to j. I wonder whether \\hat{x_{i:j}} follows the masking budget (15% of the sequence length) and there are several masked n-grams in this sentence.\n2. In I_{MLM}, does g_{\\psi} give the contextualized word embeddings from the final layer? If so, I think the model should compute the masked sentence for g_{\\omega} and unmasked sentence for g_{\\psi} simultaneously."}, "signatures": ["~Zhengyan_Zhang1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Zhengyan_Zhang1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["lingpenk@google.com", "cyprien@google.com", "leiyu@google.com", "lingwang@google.com", "zihangd@google.com", "dyogatama@google.com"], "title": "A Mutual Information Maximization Perspective of Language Representation Learning", "authors": ["Lingpeng Kong", "Cyprien de Masson d'Autume", "Lei Yu", "Wang Ling", "Zihang Dai", "Dani Yogatama"], "pdf": "/pdf/eb7c97ac3d4465bf40ae5911244e1fba78fd3314.pdf", "abstract": "We show state-of-the-art word representation learning methods maximize an objective function that is a lower bound on the mutual information between different parts of a word sequence (i.e., a sentence). Our formulation provides an alternative perspective that unifies classical word embedding models (e.g., Skip-gram) and modern contextual embeddings (e.g., BERT, XLNet). In addition to enhancing our theoretical understanding of these methods, our derivation leads to a principled framework that can be used to construct new self-supervised tasks. We provide an example by drawing inspirations from related methods based on mutual information maximization that have been successful in computer vision, and introduce a simple self-supervised objective that maximizes the mutual information between a global sentence representation and n-grams in the sentence. Our analysis offers a holistic view of representation learning methods to transfer knowledge and translate progress across multiple domains (e.g., natural language processing, computer vision, audio processing).", "keywords": [], "paperhash": "kong|a_mutual_information_maximization_perspective_of_language_representation_learning", "_bibtex": "@inproceedings{\nKong2020A,\ntitle={A Mutual Information Maximization Perspective of Language Representation Learning},\nauthor={Lingpeng Kong and Cyprien de Masson d'Autume and Lei Yu and Wang Ling and Zihang Dai and Dani Yogatama},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Syx79eBKwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/b76d744edf0d66b50fe0756cefd4c645652084c2.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Syx79eBKwr", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504179884, "tmdate": 1576860568739, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper2466/Authors", "ICLR.cc/2020/Conference/Paper2466/Reviewers", "ICLR.cc/2020/Conference/Paper2466/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2466/-/Public_Comment"}}}, {"id": "KT8aePozXB", "original": null, "number": 1, "cdate": 1576798749711, "ddate": null, "tcdate": 1576798749711, "tmdate": 1576800886179, "tddate": null, "forum": "Syx79eBKwr", "replyto": "Syx79eBKwr", "invitation": "ICLR.cc/2020/Conference/Paper2466/-/Decision", "content": {"decision": "Accept (Spotlight)", "comment": "This paper explores several embedding models (Skip-gram, BERT, XLNet) and describes a framework for comparing, and in the end, unifying them.  The framework is such that it actually suggests new ways of creating embeddings, and draws connections to methodology from computer vision.\n\nOne of the reviewers had several questions about the derivations in your paper and was worried about the paper's clarity.  But all of the reviewers appreciated the contributions of the paper, which joins multiple seemingly disparite models under into one theoretical framework.\n\nThe reviewers were positive about the paper, and in particular were happy to see the active response of authors to their questions and willingness to update the paper with their suggested improvements.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["lingpenk@google.com", "cyprien@google.com", "leiyu@google.com", "lingwang@google.com", "zihangd@google.com", "dyogatama@google.com"], "title": "A Mutual Information Maximization Perspective of Language Representation Learning", "authors": ["Lingpeng Kong", "Cyprien de Masson d'Autume", "Lei Yu", "Wang Ling", "Zihang Dai", "Dani Yogatama"], "pdf": "/pdf/eb7c97ac3d4465bf40ae5911244e1fba78fd3314.pdf", "abstract": "We show state-of-the-art word representation learning methods maximize an objective function that is a lower bound on the mutual information between different parts of a word sequence (i.e., a sentence). Our formulation provides an alternative perspective that unifies classical word embedding models (e.g., Skip-gram) and modern contextual embeddings (e.g., BERT, XLNet). In addition to enhancing our theoretical understanding of these methods, our derivation leads to a principled framework that can be used to construct new self-supervised tasks. We provide an example by drawing inspirations from related methods based on mutual information maximization that have been successful in computer vision, and introduce a simple self-supervised objective that maximizes the mutual information between a global sentence representation and n-grams in the sentence. Our analysis offers a holistic view of representation learning methods to transfer knowledge and translate progress across multiple domains (e.g., natural language processing, computer vision, audio processing).", "keywords": [], "paperhash": "kong|a_mutual_information_maximization_perspective_of_language_representation_learning", "_bibtex": "@inproceedings{\nKong2020A,\ntitle={A Mutual Information Maximization Perspective of Language Representation Learning},\nauthor={Lingpeng Kong and Cyprien de Masson d'Autume and Lei Yu and Wang Ling and Zihang Dai and Dani Yogatama},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Syx79eBKwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/b76d744edf0d66b50fe0756cefd4c645652084c2.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "Syx79eBKwr", "replyto": "Syx79eBKwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795723719, "tmdate": 1576800275246, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2466/-/Decision"}}}, {"id": "SklzdV82jr", "original": null, "number": 9, "cdate": 1573835882097, "ddate": null, "tcdate": 1573835882097, "tmdate": 1573858299235, "tddate": null, "forum": "Syx79eBKwr", "replyto": "HyxUKCS2oB", "invitation": "ICLR.cc/2020/Conference/Paper2466/-/Official_Comment", "content": {"title": "response", "comment": "Thank you for the clarification. I hope we have answered your question above.\n\nRegarding novelty, the main contribution of the paper is a unifying framework of language representation learning models based on mutual information maximization. The framework also allows us to easily construct new self-supervised tasks and take inspirations from similar methods that have been successful in other domains. We use Deep InfoMax as an example to validate this claim, but training objectives derived from other methods such as AMDIM and CPC are also possible.\n\nPlease let us know if you have any other questions or concerns, and thank you for helping us improve the submission. "}, "signatures": ["ICLR.cc/2020/Conference/Paper2466/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2466/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["lingpenk@google.com", "cyprien@google.com", "leiyu@google.com", "lingwang@google.com", "zihangd@google.com", "dyogatama@google.com"], "title": "A Mutual Information Maximization Perspective of Language Representation Learning", "authors": ["Lingpeng Kong", "Cyprien de Masson d'Autume", "Lei Yu", "Wang Ling", "Zihang Dai", "Dani Yogatama"], "pdf": "/pdf/eb7c97ac3d4465bf40ae5911244e1fba78fd3314.pdf", "abstract": "We show state-of-the-art word representation learning methods maximize an objective function that is a lower bound on the mutual information between different parts of a word sequence (i.e., a sentence). Our formulation provides an alternative perspective that unifies classical word embedding models (e.g., Skip-gram) and modern contextual embeddings (e.g., BERT, XLNet). In addition to enhancing our theoretical understanding of these methods, our derivation leads to a principled framework that can be used to construct new self-supervised tasks. We provide an example by drawing inspirations from related methods based on mutual information maximization that have been successful in computer vision, and introduce a simple self-supervised objective that maximizes the mutual information between a global sentence representation and n-grams in the sentence. Our analysis offers a holistic view of representation learning methods to transfer knowledge and translate progress across multiple domains (e.g., natural language processing, computer vision, audio processing).", "keywords": [], "paperhash": "kong|a_mutual_information_maximization_perspective_of_language_representation_learning", "_bibtex": "@inproceedings{\nKong2020A,\ntitle={A Mutual Information Maximization Perspective of Language Representation Learning},\nauthor={Lingpeng Kong and Cyprien de Masson d'Autume and Lei Yu and Wang Ling and Zihang Dai and Dani Yogatama},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Syx79eBKwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/b76d744edf0d66b50fe0756cefd4c645652084c2.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Syx79eBKwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2466/Authors", "ICLR.cc/2020/Conference/Paper2466/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2466/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2466/Reviewers", "ICLR.cc/2020/Conference/Paper2466/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2466/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2466/Authors|ICLR.cc/2020/Conference/Paper2466/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504140933, "tmdate": 1576860535026, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2466/Authors", "ICLR.cc/2020/Conference/Paper2466/Reviewers", "ICLR.cc/2020/Conference/Paper2466/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2466/-/Official_Comment"}}}, {"id": "H1eWGbyRYB", "original": null, "number": 2, "cdate": 1571840265499, "ddate": null, "tcdate": 1571840265499, "tmdate": 1573834478364, "tddate": null, "forum": "Syx79eBKwr", "replyto": "Syx79eBKwr", "invitation": "ICLR.cc/2020/Conference/Paper2466/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "8: Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #3", "review": "The paper proposes to make a clear connection between the InfoNCE learning objective (which is a lower bound of the mutual information) and multiple language models like BERT and XLN. Then based on the observation that classical LM can be seen as instances of InfoNCE, they propose a new (InfoWord) model relying on the same principles, but taking inspiration from other models also based on InfoNCE. Mainly, the proposed model  differs both in the nature of the a and b variables used in InfoNCE, and also on the fact that it uses negative sampling instead of softmax. Experiments are made on two tasks and compared to a classical BERT model, and on the BERT-NCE model that is a BERT variant proposed by the authors which is somehow in-between BERT and InfoWord. They show that their approach works quite well. \n\nI have a very mitigated opinion on the paper. I) First, I really like the idea of trying to unify different models under the same learning principles, and then show that these models can be seen as specific instances of generic principles. But the way it is presented and explained lacks of clarity: for instance in Section 2, some notations are not well defined (e.g what is f?) . Moreover, the way classical models are casted under the InfoNCE principle is badly written: it assumes that readers have a very good knowledge of the models, and the paper does not show well the mapping between the loss function of each model and the InfoNCE criterion. It gives technical details that could (in my opinion) get ignored, and I would clearly prefer to catch the main differences between the different models that being flooded by technical details. So, my suggestion would be to improve the writing of this section to make the message stronger and relevant for a larger audience. II) The Infoword model can be seen as a simple instance of word masking based models, and as an extension of deep infomax for sequences (it would be certainly nice to describe a little bit what Deep InfoMax is to facilitate the reading).  Here again, the article moves from technical details (e.g \"hidden state of the first token (assumed to be a special start of sentence symbol \") without providing formal definitions. Having a first loss function after paragraph 4 could help to understand the principle of this model (before restricting the model to n-grams).  Moreover, the equation J_DIM seems to be wrong since it contains g_\\omega twice while I think (but maybe I am wrong) that it has also to be defined by g_\\psi. J_MLM is also not clear since x_i is never defined (I assume it is x_{i:i}). At last,  after unifying multiple models under one common learning objective, the authors propose to mix two different losses which is strange (the effect of the second term is slightly studied in the experimental section) without allowing us to understand why it is important to have this second loss function and why the first one is not sufficient enough. At last, I am pretty sure to not be able to reproduce the model described in the paper (adding a section on that in the supplementary material would help), and many concrete aspects are described too fast (like the way to sample negative pairs). \n\nConcerning the experimental section, experiments are convincing and show that the model is able to achieve a performance which is close to classical models. In my opinion, tis section has to be interpreted as  a proof that the proposed unified vision is a good way to easily define new and efficient models. \n\nTo summarize, the unification under the InfoNCE principle is interesting,  but the way the paper is written makes it very difficult to follow, and the description of the proposed model is unclear (making the experiments difficult to reproduce) and lacks of a better discussion about the interest of mixing multiple loss. \n\n\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"}, "signatures": ["ICLR.cc/2020/Conference/Paper2466/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2466/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["lingpenk@google.com", "cyprien@google.com", "leiyu@google.com", "lingwang@google.com", "zihangd@google.com", "dyogatama@google.com"], "title": "A Mutual Information Maximization Perspective of Language Representation Learning", "authors": ["Lingpeng Kong", "Cyprien de Masson d'Autume", "Lei Yu", "Wang Ling", "Zihang Dai", "Dani Yogatama"], "pdf": "/pdf/eb7c97ac3d4465bf40ae5911244e1fba78fd3314.pdf", "abstract": "We show state-of-the-art word representation learning methods maximize an objective function that is a lower bound on the mutual information between different parts of a word sequence (i.e., a sentence). Our formulation provides an alternative perspective that unifies classical word embedding models (e.g., Skip-gram) and modern contextual embeddings (e.g., BERT, XLNet). In addition to enhancing our theoretical understanding of these methods, our derivation leads to a principled framework that can be used to construct new self-supervised tasks. We provide an example by drawing inspirations from related methods based on mutual information maximization that have been successful in computer vision, and introduce a simple self-supervised objective that maximizes the mutual information between a global sentence representation and n-grams in the sentence. Our analysis offers a holistic view of representation learning methods to transfer knowledge and translate progress across multiple domains (e.g., natural language processing, computer vision, audio processing).", "keywords": [], "paperhash": "kong|a_mutual_information_maximization_perspective_of_language_representation_learning", "_bibtex": "@inproceedings{\nKong2020A,\ntitle={A Mutual Information Maximization Perspective of Language Representation Learning},\nauthor={Lingpeng Kong and Cyprien de Masson d'Autume and Lei Yu and Wang Ling and Zihang Dai and Dani Yogatama},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Syx79eBKwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/b76d744edf0d66b50fe0756cefd4c645652084c2.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Syx79eBKwr", "replyto": "Syx79eBKwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2466/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2466/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575704309240, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2466/Reviewers"], "noninvitees": [], "tcdate": 1570237722435, "tmdate": 1575704309251, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2466/-/Official_Review"}}}, {"id": "HyxUKCS2oB", "original": null, "number": 7, "cdate": 1573834366495, "ddate": null, "tcdate": 1573834366495, "tmdate": 1573834366495, "tddate": null, "forum": "Syx79eBKwr", "replyto": "rJlgPc-mjB", "invitation": "ICLR.cc/2020/Conference/Paper2466/-/Official_Comment", "content": {"title": "Official Blind Review #1", "comment": "Yes, it is."}, "signatures": ["ICLR.cc/2020/Conference/Paper2466/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2466/AnonReviewer2", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["lingpenk@google.com", "cyprien@google.com", "leiyu@google.com", "lingwang@google.com", "zihangd@google.com", "dyogatama@google.com"], "title": "A Mutual Information Maximization Perspective of Language Representation Learning", "authors": ["Lingpeng Kong", "Cyprien de Masson d'Autume", "Lei Yu", "Wang Ling", "Zihang Dai", "Dani Yogatama"], "pdf": "/pdf/eb7c97ac3d4465bf40ae5911244e1fba78fd3314.pdf", "abstract": "We show state-of-the-art word representation learning methods maximize an objective function that is a lower bound on the mutual information between different parts of a word sequence (i.e., a sentence). Our formulation provides an alternative perspective that unifies classical word embedding models (e.g., Skip-gram) and modern contextual embeddings (e.g., BERT, XLNet). In addition to enhancing our theoretical understanding of these methods, our derivation leads to a principled framework that can be used to construct new self-supervised tasks. We provide an example by drawing inspirations from related methods based on mutual information maximization that have been successful in computer vision, and introduce a simple self-supervised objective that maximizes the mutual information between a global sentence representation and n-grams in the sentence. Our analysis offers a holistic view of representation learning methods to transfer knowledge and translate progress across multiple domains (e.g., natural language processing, computer vision, audio processing).", "keywords": [], "paperhash": "kong|a_mutual_information_maximization_perspective_of_language_representation_learning", "_bibtex": "@inproceedings{\nKong2020A,\ntitle={A Mutual Information Maximization Perspective of Language Representation Learning},\nauthor={Lingpeng Kong and Cyprien de Masson d'Autume and Lei Yu and Wang Ling and Zihang Dai and Dani Yogatama},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Syx79eBKwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/b76d744edf0d66b50fe0756cefd4c645652084c2.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Syx79eBKwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2466/Authors", "ICLR.cc/2020/Conference/Paper2466/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2466/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2466/Reviewers", "ICLR.cc/2020/Conference/Paper2466/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2466/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2466/Authors|ICLR.cc/2020/Conference/Paper2466/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504140933, "tmdate": 1576860535026, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2466/Authors", "ICLR.cc/2020/Conference/Paper2466/Reviewers", "ICLR.cc/2020/Conference/Paper2466/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2466/-/Official_Comment"}}}, {"id": "B1eMJ9-msH", "original": null, "number": 2, "cdate": 1573226970331, "ddate": null, "tcdate": 1573226970331, "tmdate": 1573264766526, "tddate": null, "forum": "Syx79eBKwr", "replyto": "Hyemr5VVtB", "invitation": "ICLR.cc/2020/Conference/Paper2466/-/Official_Comment", "content": {"title": "response", "comment": "Thank you for your thoughtful review. We have updated Equation 1 and the paragraph above so that I(...) is consistently a function of two variables."}, "signatures": ["ICLR.cc/2020/Conference/Paper2466/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2466/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["lingpenk@google.com", "cyprien@google.com", "leiyu@google.com", "lingwang@google.com", "zihangd@google.com", "dyogatama@google.com"], "title": "A Mutual Information Maximization Perspective of Language Representation Learning", "authors": ["Lingpeng Kong", "Cyprien de Masson d'Autume", "Lei Yu", "Wang Ling", "Zihang Dai", "Dani Yogatama"], "pdf": "/pdf/eb7c97ac3d4465bf40ae5911244e1fba78fd3314.pdf", "abstract": "We show state-of-the-art word representation learning methods maximize an objective function that is a lower bound on the mutual information between different parts of a word sequence (i.e., a sentence). Our formulation provides an alternative perspective that unifies classical word embedding models (e.g., Skip-gram) and modern contextual embeddings (e.g., BERT, XLNet). In addition to enhancing our theoretical understanding of these methods, our derivation leads to a principled framework that can be used to construct new self-supervised tasks. We provide an example by drawing inspirations from related methods based on mutual information maximization that have been successful in computer vision, and introduce a simple self-supervised objective that maximizes the mutual information between a global sentence representation and n-grams in the sentence. Our analysis offers a holistic view of representation learning methods to transfer knowledge and translate progress across multiple domains (e.g., natural language processing, computer vision, audio processing).", "keywords": [], "paperhash": "kong|a_mutual_information_maximization_perspective_of_language_representation_learning", "_bibtex": "@inproceedings{\nKong2020A,\ntitle={A Mutual Information Maximization Perspective of Language Representation Learning},\nauthor={Lingpeng Kong and Cyprien de Masson d'Autume and Lei Yu and Wang Ling and Zihang Dai and Dani Yogatama},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Syx79eBKwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/b76d744edf0d66b50fe0756cefd4c645652084c2.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Syx79eBKwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2466/Authors", "ICLR.cc/2020/Conference/Paper2466/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2466/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2466/Reviewers", "ICLR.cc/2020/Conference/Paper2466/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2466/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2466/Authors|ICLR.cc/2020/Conference/Paper2466/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504140933, "tmdate": 1576860535026, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2466/Authors", "ICLR.cc/2020/Conference/Paper2466/Reviewers", "ICLR.cc/2020/Conference/Paper2466/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2466/-/Official_Comment"}}}, {"id": "r1xKd5Z7or", "original": null, "number": 4, "cdate": 1573227121109, "ddate": null, "tcdate": 1573227121109, "tmdate": 1573264755455, "tddate": null, "forum": "Syx79eBKwr", "replyto": "H1eWGbyRYB", "invitation": "ICLR.cc/2020/Conference/Paper2466/-/Official_Comment", "content": {"title": "response", "comment": "Thank you for your thoughtful review. \n\nWe have updated the paper based on your comments to improve clarity and reproducibility. We list a summary of our main changes below:\n- In order to make it easier for readers to understand the differences between different models and how they are related to InfoNCE, we have added a summary in Table 1.\n- We have improved notations by adding explicit definitions before they are used in Section 2 and Section 4, and added a short description of Deep InfoMax in Section 4.\n- We have included model and training hyperparameter details in Section 5.1 and Appendix B.\n- We added a motivation for mixing two different terms in the objective function. Our DIM is primarily designed to improve sentence and span representations. We combine it with MLM which is designed for learning (contextual) word representations, since our overall goal is to create better representations for both the sentence and each word in the sentence. We also note that Deep InfoMax for learning image representations mixes multiple terms in their objective function. We only take one of the terms from the full objective function and mix it with MLM.\n\nRegarding equation I_{DIM}, it is supposed to contain two g_{\\omega} and no g_{\\psi} as we use one network for encoding both the sentence and n-grams. This is not a typo.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2466/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2466/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["lingpenk@google.com", "cyprien@google.com", "leiyu@google.com", "lingwang@google.com", "zihangd@google.com", "dyogatama@google.com"], "title": "A Mutual Information Maximization Perspective of Language Representation Learning", "authors": ["Lingpeng Kong", "Cyprien de Masson d'Autume", "Lei Yu", "Wang Ling", "Zihang Dai", "Dani Yogatama"], "pdf": "/pdf/eb7c97ac3d4465bf40ae5911244e1fba78fd3314.pdf", "abstract": "We show state-of-the-art word representation learning methods maximize an objective function that is a lower bound on the mutual information between different parts of a word sequence (i.e., a sentence). Our formulation provides an alternative perspective that unifies classical word embedding models (e.g., Skip-gram) and modern contextual embeddings (e.g., BERT, XLNet). In addition to enhancing our theoretical understanding of these methods, our derivation leads to a principled framework that can be used to construct new self-supervised tasks. We provide an example by drawing inspirations from related methods based on mutual information maximization that have been successful in computer vision, and introduce a simple self-supervised objective that maximizes the mutual information between a global sentence representation and n-grams in the sentence. Our analysis offers a holistic view of representation learning methods to transfer knowledge and translate progress across multiple domains (e.g., natural language processing, computer vision, audio processing).", "keywords": [], "paperhash": "kong|a_mutual_information_maximization_perspective_of_language_representation_learning", "_bibtex": "@inproceedings{\nKong2020A,\ntitle={A Mutual Information Maximization Perspective of Language Representation Learning},\nauthor={Lingpeng Kong and Cyprien de Masson d'Autume and Lei Yu and Wang Ling and Zihang Dai and Dani Yogatama},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Syx79eBKwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/b76d744edf0d66b50fe0756cefd4c645652084c2.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Syx79eBKwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2466/Authors", "ICLR.cc/2020/Conference/Paper2466/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2466/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2466/Reviewers", "ICLR.cc/2020/Conference/Paper2466/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2466/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2466/Authors|ICLR.cc/2020/Conference/Paper2466/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504140933, "tmdate": 1576860535026, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2466/Authors", "ICLR.cc/2020/Conference/Paper2466/Reviewers", "ICLR.cc/2020/Conference/Paper2466/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2466/-/Official_Comment"}}}, {"id": "rJlgPc-mjB", "original": null, "number": 3, "cdate": 1573227095970, "ddate": null, "tcdate": 1573227095970, "tmdate": 1573264745349, "tddate": null, "forum": "Syx79eBKwr", "replyto": "Skx9YZG-qB", "invitation": "ICLR.cc/2020/Conference/Paper2466/-/Official_Comment", "content": {"title": "response", "comment": "Thank you for your thoughtful review. \n\nWe have updated notations in Equations 1 and 2. The expectations are now taken over random variables (A and B) and the function takes particular values (a and b) of these random variables.\n\nRegarding your comment about increasing bias and reducing variance, we did observe that the quality of the InfoWord representations is relatively stable across different runs in our experiments (as evaluated by performance on downstream tasks). Could you please clarify a bit more whether this is what you are asking?"}, "signatures": ["ICLR.cc/2020/Conference/Paper2466/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2466/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["lingpenk@google.com", "cyprien@google.com", "leiyu@google.com", "lingwang@google.com", "zihangd@google.com", "dyogatama@google.com"], "title": "A Mutual Information Maximization Perspective of Language Representation Learning", "authors": ["Lingpeng Kong", "Cyprien de Masson d'Autume", "Lei Yu", "Wang Ling", "Zihang Dai", "Dani Yogatama"], "pdf": "/pdf/eb7c97ac3d4465bf40ae5911244e1fba78fd3314.pdf", "abstract": "We show state-of-the-art word representation learning methods maximize an objective function that is a lower bound on the mutual information between different parts of a word sequence (i.e., a sentence). Our formulation provides an alternative perspective that unifies classical word embedding models (e.g., Skip-gram) and modern contextual embeddings (e.g., BERT, XLNet). In addition to enhancing our theoretical understanding of these methods, our derivation leads to a principled framework that can be used to construct new self-supervised tasks. We provide an example by drawing inspirations from related methods based on mutual information maximization that have been successful in computer vision, and introduce a simple self-supervised objective that maximizes the mutual information between a global sentence representation and n-grams in the sentence. Our analysis offers a holistic view of representation learning methods to transfer knowledge and translate progress across multiple domains (e.g., natural language processing, computer vision, audio processing).", "keywords": [], "paperhash": "kong|a_mutual_information_maximization_perspective_of_language_representation_learning", "_bibtex": "@inproceedings{\nKong2020A,\ntitle={A Mutual Information Maximization Perspective of Language Representation Learning},\nauthor={Lingpeng Kong and Cyprien de Masson d'Autume and Lei Yu and Wang Ling and Zihang Dai and Dani Yogatama},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Syx79eBKwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/b76d744edf0d66b50fe0756cefd4c645652084c2.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Syx79eBKwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2466/Authors", "ICLR.cc/2020/Conference/Paper2466/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2466/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2466/Reviewers", "ICLR.cc/2020/Conference/Paper2466/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2466/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2466/Authors|ICLR.cc/2020/Conference/Paper2466/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504140933, "tmdate": 1576860535026, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2466/Authors", "ICLR.cc/2020/Conference/Paper2466/Reviewers", "ICLR.cc/2020/Conference/Paper2466/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2466/-/Official_Comment"}}}, {"id": "Hyemr5VVtB", "original": null, "number": 1, "cdate": 1571207738784, "ddate": null, "tcdate": 1571207738784, "tmdate": 1572972334692, "tddate": null, "forum": "Syx79eBKwr", "replyto": "Syx79eBKwr", "invitation": "ICLR.cc/2020/Conference/Paper2466/-/Official_Review", "content": {"rating": "8: Accept", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "The paper gives a big picture view on training objectives used to obtain static and contextualized word embeddings. This is very handy since classical static word embeddings, such as SGNS and GloVe, have been studied theoretically in a number of works (e.g., Levy and Goldberg, 2014; Arora et al., 2016; Hashimoto et al., 2016; Gittens et al., 2017; Allen and Hospedales, 2019; Assylbekov and Takhanov, 2019), but not much has been done for the modern contextualized embedding models such ELMo and BERT - I personally know only the work of Wang and Cho (2019), and please correct me if I am wrong.\n\n\"There is nothing as practical as a good theory\", and the authors confirm this statement: their theory suggests them to modify the training objective of the masked language modeling in a certain way and this modification proves to benefit the embeddings in general when evaluated on standard tasks.\n\nI don't have any major issues to raise. A minor comment is that the mutual information I(., .) being a function of two variables suddenly became a function of a single variable in Eq. (1) and in the text which precedes it."}, "signatures": ["ICLR.cc/2020/Conference/Paper2466/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2466/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["lingpenk@google.com", "cyprien@google.com", "leiyu@google.com", "lingwang@google.com", "zihangd@google.com", "dyogatama@google.com"], "title": "A Mutual Information Maximization Perspective of Language Representation Learning", "authors": ["Lingpeng Kong", "Cyprien de Masson d'Autume", "Lei Yu", "Wang Ling", "Zihang Dai", "Dani Yogatama"], "pdf": "/pdf/eb7c97ac3d4465bf40ae5911244e1fba78fd3314.pdf", "abstract": "We show state-of-the-art word representation learning methods maximize an objective function that is a lower bound on the mutual information between different parts of a word sequence (i.e., a sentence). Our formulation provides an alternative perspective that unifies classical word embedding models (e.g., Skip-gram) and modern contextual embeddings (e.g., BERT, XLNet). In addition to enhancing our theoretical understanding of these methods, our derivation leads to a principled framework that can be used to construct new self-supervised tasks. We provide an example by drawing inspirations from related methods based on mutual information maximization that have been successful in computer vision, and introduce a simple self-supervised objective that maximizes the mutual information between a global sentence representation and n-grams in the sentence. Our analysis offers a holistic view of representation learning methods to transfer knowledge and translate progress across multiple domains (e.g., natural language processing, computer vision, audio processing).", "keywords": [], "paperhash": "kong|a_mutual_information_maximization_perspective_of_language_representation_learning", "_bibtex": "@inproceedings{\nKong2020A,\ntitle={A Mutual Information Maximization Perspective of Language Representation Learning},\nauthor={Lingpeng Kong and Cyprien de Masson d'Autume and Lei Yu and Wang Ling and Zihang Dai and Dani Yogatama},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Syx79eBKwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/b76d744edf0d66b50fe0756cefd4c645652084c2.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Syx79eBKwr", "replyto": "Syx79eBKwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2466/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2466/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575704309240, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2466/Reviewers"], "noninvitees": [], "tcdate": 1570237722435, "tmdate": 1575704309251, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2466/-/Official_Review"}}}, {"id": "Skx9YZG-qB", "original": null, "number": 3, "cdate": 1572049281625, "ddate": null, "tcdate": 1572049281625, "tmdate": 1572972334598, "tddate": null, "forum": "Syx79eBKwr", "replyto": "Syx79eBKwr", "invitation": "ICLR.cc/2020/Conference/Paper2466/-/Official_Review", "content": {"rating": "6: Weak Accept", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper first gives a concise yet precise summary of maximizing one of variational lower bounds of mutual information, InfoNCE, then it provides an alternative view to explain case by case why word embedding Skip-gram, BERT, XLNet work in practice can be viewed by InfoNCE framework, thus we have a good understand for these methods. Moreover it introduces a self-learning method  that maximizes the mutual information between a global sentence representation and n-grams in the sentence based on deep InfoMax framework instead. Experiments show that it is better then BERT and BERT-NCE. It's known that InfoNCE increases bias but reduce variance, the same is true for deep InfoMax. Do you observe this in your experiments? If so, please provide.\n\nThe paper is well-written and easy to follow. The originality is relative low though, since it is mainly an application of  deep InfoMax to language modeling, not inventing a new algorithm and applying to language modeling.\n\nIn equations 1 and 2, should a, b be written in capital? Since they represent random variables."}, "signatures": ["ICLR.cc/2020/Conference/Paper2466/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2466/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["lingpenk@google.com", "cyprien@google.com", "leiyu@google.com", "lingwang@google.com", "zihangd@google.com", "dyogatama@google.com"], "title": "A Mutual Information Maximization Perspective of Language Representation Learning", "authors": ["Lingpeng Kong", "Cyprien de Masson d'Autume", "Lei Yu", "Wang Ling", "Zihang Dai", "Dani Yogatama"], "pdf": "/pdf/eb7c97ac3d4465bf40ae5911244e1fba78fd3314.pdf", "abstract": "We show state-of-the-art word representation learning methods maximize an objective function that is a lower bound on the mutual information between different parts of a word sequence (i.e., a sentence). Our formulation provides an alternative perspective that unifies classical word embedding models (e.g., Skip-gram) and modern contextual embeddings (e.g., BERT, XLNet). In addition to enhancing our theoretical understanding of these methods, our derivation leads to a principled framework that can be used to construct new self-supervised tasks. We provide an example by drawing inspirations from related methods based on mutual information maximization that have been successful in computer vision, and introduce a simple self-supervised objective that maximizes the mutual information between a global sentence representation and n-grams in the sentence. Our analysis offers a holistic view of representation learning methods to transfer knowledge and translate progress across multiple domains (e.g., natural language processing, computer vision, audio processing).", "keywords": [], "paperhash": "kong|a_mutual_information_maximization_perspective_of_language_representation_learning", "_bibtex": "@inproceedings{\nKong2020A,\ntitle={A Mutual Information Maximization Perspective of Language Representation Learning},\nauthor={Lingpeng Kong and Cyprien de Masson d'Autume and Lei Yu and Wang Ling and Zihang Dai and Dani Yogatama},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Syx79eBKwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/b76d744edf0d66b50fe0756cefd4c645652084c2.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Syx79eBKwr", "replyto": "Syx79eBKwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2466/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2466/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575704309240, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2466/Reviewers"], "noninvitees": [], "tcdate": 1570237722435, "tmdate": 1575704309251, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2466/-/Official_Review"}}}], "count": 13}