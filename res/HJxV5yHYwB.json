{"notes": [{"id": "HJxV5yHYwB", "original": "SJxG6FAOwr", "number": 1875, "cdate": 1569439628411, "ddate": null, "tcdate": 1569439628411, "tmdate": 1577168235261, "tddate": null, "forum": "HJxV5yHYwB", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Solving single-objective tasks by preference multi-objective reinforcement learning", "authors": ["Jinsheng Ren", "Shangqi Guo", "Feng Chen"], "authorids": ["rjs17@mails.tsinghua.edu.cn", "gsq15@mails.tsinghua.edu.cn", "chenfeng@mail.tsinghua.edu.cn"], "keywords": ["reinforcement learning", "single-objective tasks", "multi-objectivization"], "TL;DR": "Solving complex single-objective tasks by preference multi-objective reinforcement learning.", "abstract": "There ubiquitously exist many single-objective tasks in the real world that are inevitably related to some other objectives and influenced by them. We call such task as the objective-constrained task, which is inherently a multi-objective problem. Due to the conflict among different objectives, a trade-off is needed. A common compromise is to design a scalar reward function through clarifying the relationship among these objectives using the prior knowledge of experts. However, reward engineering is extremely cumbersome. This will result in behaviors that optimize our reward function without actually satisfying our preferences. In this paper, we explicitly cast the objective-constrained task as preference multi-objective reinforcement learning, with the overall goal of finding a Pareto optimal policy. Combined with Trajectory Preference Domination we propose, a weight vector that reflects the agent's preference for each objective can be learned. We analyzed the feasibility of our algorithm in theory, and further proved in experiments its better performance compared to those that design the reward function by experts.", "pdf": "/pdf/e9af58618cf7042ebcdf22dd12157fff9f303c7d.pdf", "paperhash": "ren|solving_singleobjective_tasks_by_preference_multiobjective_reinforcement_learning", "original_pdf": "/attachment/e9af58618cf7042ebcdf22dd12157fff9f303c7d.pdf", "_bibtex": "@misc{\nren2020solving,\ntitle={Solving single-objective tasks by preference multi-objective reinforcement learning},\nauthor={Jinsheng Ren and Shangqi Guo and Feng Chen},\nyear={2020},\nurl={https://openreview.net/forum?id=HJxV5yHYwB}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 6, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "T0ytmBXEG6", "original": null, "number": 1, "cdate": 1576798734749, "ddate": null, "tcdate": 1576798734749, "tmdate": 1576800901639, "tddate": null, "forum": "HJxV5yHYwB", "replyto": "HJxV5yHYwB", "invitation": "ICLR.cc/2020/Conference/Paper1875/-/Decision", "content": {"decision": "Reject", "comment": "The paper considers planning through the lenses both of a single and multiple objectives. The paper then discusses the pareto frontiers of this optimization. While this is an interesting direction, the reviewers feel a more careful comparison to related work is needed.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Solving single-objective tasks by preference multi-objective reinforcement learning", "authors": ["Jinsheng Ren", "Shangqi Guo", "Feng Chen"], "authorids": ["rjs17@mails.tsinghua.edu.cn", "gsq15@mails.tsinghua.edu.cn", "chenfeng@mail.tsinghua.edu.cn"], "keywords": ["reinforcement learning", "single-objective tasks", "multi-objectivization"], "TL;DR": "Solving complex single-objective tasks by preference multi-objective reinforcement learning.", "abstract": "There ubiquitously exist many single-objective tasks in the real world that are inevitably related to some other objectives and influenced by them. We call such task as the objective-constrained task, which is inherently a multi-objective problem. Due to the conflict among different objectives, a trade-off is needed. A common compromise is to design a scalar reward function through clarifying the relationship among these objectives using the prior knowledge of experts. However, reward engineering is extremely cumbersome. This will result in behaviors that optimize our reward function without actually satisfying our preferences. In this paper, we explicitly cast the objective-constrained task as preference multi-objective reinforcement learning, with the overall goal of finding a Pareto optimal policy. Combined with Trajectory Preference Domination we propose, a weight vector that reflects the agent's preference for each objective can be learned. We analyzed the feasibility of our algorithm in theory, and further proved in experiments its better performance compared to those that design the reward function by experts.", "pdf": "/pdf/e9af58618cf7042ebcdf22dd12157fff9f303c7d.pdf", "paperhash": "ren|solving_singleobjective_tasks_by_preference_multiobjective_reinforcement_learning", "original_pdf": "/attachment/e9af58618cf7042ebcdf22dd12157fff9f303c7d.pdf", "_bibtex": "@misc{\nren2020solving,\ntitle={Solving single-objective tasks by preference multi-objective reinforcement learning},\nauthor={Jinsheng Ren and Shangqi Guo and Feng Chen},\nyear={2020},\nurl={https://openreview.net/forum?id=HJxV5yHYwB}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "HJxV5yHYwB", "replyto": "HJxV5yHYwB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795726055, "tmdate": 1576800278099, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1875/-/Decision"}}}, {"id": "Byg4aCzE5S", "original": null, "number": 2, "cdate": 1572249276488, "ddate": null, "tcdate": 1572249276488, "tmdate": 1574977782941, "tddate": null, "forum": "HJxV5yHYwB", "replyto": "HJxV5yHYwB", "invitation": "ICLR.cc/2020/Conference/Paper1875/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #3", "review": "Thank the authors for the response. I agree with R2 that the paper lacks comparisons with previous works. I will stick to my previous decision.\n----------------------------------------\nSummary\nThis paper presents a new approach for single-objective reinforcement learning by preferencing multi-objective reinforcement learning. The general idea is to first figure out a few important objectives, add some helper-objectives to the original problem, and learn the weights for each individual objective by trying to keep the same order as Pareto dominance. This paper has potential, but I lean to vote for rejecting this paper now, since it is still not ready. I might change my score based on the reviews from other reviewers.\nStrengths\n- The idea is novel. Learning weights for each objective by keeping the order as Pareto dominance is an interesting idea to me.\nWeaknesses\n- The lack of experiments. The authors tested their method in only one scenario, which makes me feel unsafe. Only testing on one simple scenario does not demonstrate the effectiveness. The authors are supposed to test their method on more (complex) scenarios to show the effectiveness of their method.\nPossible Improvements\nAs mentioned before, the proposed method can be tested on more scenarios (e.g., Deep Sea Treasure, SuperMario, etc.).", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper1875/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1875/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Solving single-objective tasks by preference multi-objective reinforcement learning", "authors": ["Jinsheng Ren", "Shangqi Guo", "Feng Chen"], "authorids": ["rjs17@mails.tsinghua.edu.cn", "gsq15@mails.tsinghua.edu.cn", "chenfeng@mail.tsinghua.edu.cn"], "keywords": ["reinforcement learning", "single-objective tasks", "multi-objectivization"], "TL;DR": "Solving complex single-objective tasks by preference multi-objective reinforcement learning.", "abstract": "There ubiquitously exist many single-objective tasks in the real world that are inevitably related to some other objectives and influenced by them. We call such task as the objective-constrained task, which is inherently a multi-objective problem. Due to the conflict among different objectives, a trade-off is needed. A common compromise is to design a scalar reward function through clarifying the relationship among these objectives using the prior knowledge of experts. However, reward engineering is extremely cumbersome. This will result in behaviors that optimize our reward function without actually satisfying our preferences. In this paper, we explicitly cast the objective-constrained task as preference multi-objective reinforcement learning, with the overall goal of finding a Pareto optimal policy. Combined with Trajectory Preference Domination we propose, a weight vector that reflects the agent's preference for each objective can be learned. We analyzed the feasibility of our algorithm in theory, and further proved in experiments its better performance compared to those that design the reward function by experts.", "pdf": "/pdf/e9af58618cf7042ebcdf22dd12157fff9f303c7d.pdf", "paperhash": "ren|solving_singleobjective_tasks_by_preference_multiobjective_reinforcement_learning", "original_pdf": "/attachment/e9af58618cf7042ebcdf22dd12157fff9f303c7d.pdf", "_bibtex": "@misc{\nren2020solving,\ntitle={Solving single-objective tasks by preference multi-objective reinforcement learning},\nauthor={Jinsheng Ren and Shangqi Guo and Feng Chen},\nyear={2020},\nurl={https://openreview.net/forum?id=HJxV5yHYwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJxV5yHYwB", "replyto": "HJxV5yHYwB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1875/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1875/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575501052360, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1875/Reviewers"], "noninvitees": [], "tcdate": 1570237731027, "tmdate": 1575501052371, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1875/-/Official_Review"}}}, {"id": "rygBjki6tB", "original": null, "number": 1, "cdate": 1571823517433, "ddate": null, "tcdate": 1571823517433, "tmdate": 1574079642843, "tddate": null, "forum": "HJxV5yHYwB", "replyto": "HJxV5yHYwB", "invitation": "ICLR.cc/2020/Conference/Paper1875/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #2", "review": "After Responses:\nI understand the differences that authors pointed to the relevant literature. However, it is still lacking comparisons to these relevant methods. The proposed method has not been compared with any of the existing literature. Hence, we do not have any idea how does it stand against the existing approaches. Hence, I believe the empirical study is still significantly lacking. I will stick to my decision. Main reason is as follows; I believe the idea is interesting but it needs a significant empirical work to be published. I recommend authors to improve empirical study and re-submit.\n-------\nThe submission is proposing a method for multi-objective RL such that the preference of tasks learned on the fly with the policy learning. The main idea is converting the multi-objective problem into single objective by scalar weighting. The weights are learned in a structured learning fashion by enforcing them to approximate the Pareto dominance relations.\n\nThe submission is interesting; however, its novelty is not even clear since authors did not discuss majority of the existing related work. \n\nAuthors can consult the AAMAS 2018 tutorial \"Multi-Objective Planning and Reinforcement Learning\" by Whiteson&Roijers for relevant papers. It is also important to note that there are other methods which learn weighting. Optimistic linear support is one of such methods. Hence, this is not the first of such approaches. Beyond RL, it is also studied extensively in supervised learning. For example, authors can see \"Multi-Task Learning as Multi-Objective Optimization\" from NeurIPS 2018.\n\nThe manuscript is also very hard to parse and understand. For example, Definition 2 uses but not define \"p\" in condition (2). Similarly, Lemma 1 states sth is \"far greater\" than something else. However, \"far greater\" is not really defined. I am also puzzled to understand the relevance of Theorem 1. It is beyond the scope of the manuscript, and also not really new.\n\nAuthors suggest a method to solve multi-objective optimization. However, there is no correctness proof. We do not know would the algorithm result in Pareto optimal solution even asymptotically. Arbitrary weights do not result in Pareto optimality.\n\nProposing a new toy problem is well-received. However, not providing any experiment beyond the proposed problem is problematic. Authors motivate their method using DOOM example. Why not provide experimental results on a challenging problem like DOOM?\n\nIn summary, I definitely appreciate the idea. However, it needs better literature search. Authors should position their paper properly with respect to existing literature. The theory should be revised and extended with convergence to Pareto optimality. Finally, more extensive experiments on existing problems comparing with existing baselines is needed.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper1875/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1875/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Solving single-objective tasks by preference multi-objective reinforcement learning", "authors": ["Jinsheng Ren", "Shangqi Guo", "Feng Chen"], "authorids": ["rjs17@mails.tsinghua.edu.cn", "gsq15@mails.tsinghua.edu.cn", "chenfeng@mail.tsinghua.edu.cn"], "keywords": ["reinforcement learning", "single-objective tasks", "multi-objectivization"], "TL;DR": "Solving complex single-objective tasks by preference multi-objective reinforcement learning.", "abstract": "There ubiquitously exist many single-objective tasks in the real world that are inevitably related to some other objectives and influenced by them. We call such task as the objective-constrained task, which is inherently a multi-objective problem. Due to the conflict among different objectives, a trade-off is needed. A common compromise is to design a scalar reward function through clarifying the relationship among these objectives using the prior knowledge of experts. However, reward engineering is extremely cumbersome. This will result in behaviors that optimize our reward function without actually satisfying our preferences. In this paper, we explicitly cast the objective-constrained task as preference multi-objective reinforcement learning, with the overall goal of finding a Pareto optimal policy. Combined with Trajectory Preference Domination we propose, a weight vector that reflects the agent's preference for each objective can be learned. We analyzed the feasibility of our algorithm in theory, and further proved in experiments its better performance compared to those that design the reward function by experts.", "pdf": "/pdf/e9af58618cf7042ebcdf22dd12157fff9f303c7d.pdf", "paperhash": "ren|solving_singleobjective_tasks_by_preference_multiobjective_reinforcement_learning", "original_pdf": "/attachment/e9af58618cf7042ebcdf22dd12157fff9f303c7d.pdf", "_bibtex": "@misc{\nren2020solving,\ntitle={Solving single-objective tasks by preference multi-objective reinforcement learning},\nauthor={Jinsheng Ren and Shangqi Guo and Feng Chen},\nyear={2020},\nurl={https://openreview.net/forum?id=HJxV5yHYwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJxV5yHYwB", "replyto": "HJxV5yHYwB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1875/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1875/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575501052360, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1875/Reviewers"], "noninvitees": [], "tcdate": 1570237731027, "tmdate": 1575501052371, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1875/-/Official_Review"}}}, {"id": "Hke2FZqFiH", "original": null, "number": 5, "cdate": 1573654916245, "ddate": null, "tcdate": 1573654916245, "tmdate": 1573655323307, "tddate": null, "forum": "HJxV5yHYwB", "replyto": "Syg1Lx9Yor", "invitation": "ICLR.cc/2020/Conference/Paper1875/-/Official_Comment", "content": {"title": "Author response to Reviewer #2 (part 2)", "comment": "\nQ4: \"I am also puzzled to understand the relevance of Theorem 1.\"\nA: In the process of weight optimization, the form of undiscounted cumulative return is adopted. However, the goal of the agent is to find a policy that maximizes the discounted cumulative return. Therefore, Theorem 1 proves that the optimal policy can also maximize the cumulative reward, although the reward fucntion is learned in the form of undiscounted accumulation. The recent work[5] also used undiscounting to optimize the reward fucntion, but it did not prove its rationality in theory, only showing its feasibility through many experiments, while we have proved its feasibility in theory for the first time.\n\t\nQ5: \"there is no correctness proof.\"\nA: The research of this paper can be seen as a kind of enrichment to the research on preference-based reinforcement learning[6], in which, as far as we have been concerned, no strict convergence proof has been provided. Here we provide a brief proof based on evolutionary algorithms. It is obvious that the optimal trajectory, which fits the preference Pareto dominance relations, can be sampled when an exploration rate $\\sigma > 0$ is used during interaction with the environment. Meanwhile, the trajectory buffer always maintains the optimal trajectories found over time. Given the two points above, the optimal weight vector can be acquired through minimizing the cross-entropy loss (formula 7). For more details, please refer to the proof of convergence of evolutionary algorithms[7].\n\t\nQ6: \"Why not provide experimental results on a challenging problem like DOOM?\"\nA: We agree that more experiments are helpful to show the effectiveness of our method. However, due to the limitations of time and computation resources, more experiments are left as future work.\n\t\nReferences:\n\t\n[1] Natarajan S, Tadepalli P. Dynamic preferences in multi-criteria reinforcement learning. In ICML, 2005.\n[2] Abels A, Roijers D M, Lenaerts T, et al. Dynamic Weights in Multi-Objective Deep Reinforcement Learning. In ICML, 2019.\n[3] Mossalam H, Assael Y M, Roijers D M, et al. Multi-objective deep reinforcement learning. arXiv preprint arXiv:1610.02707, 2016.\n[4] Sener O, Koltun V. Multi-task learning as multi-objective optimization. In Advances in Neural Information Processing Systems, 2018.\n[5] Christiano P F, Leike J, Brown T, et al. Deep reinforcement learning from human preferences. In Advances in Neural Information Processing Systems, 2017.\n[6] Wirth C, Akrour R, Neumann G, et al. A survey of preference-based reinforcement learning methods. The Journal of Machine Learning Research, 2017, 18(1): 4945-4990.\n[7] Rudolph G. Convergence properties of evolutionary algorithms. Kovac, 1997."}, "signatures": ["ICLR.cc/2020/Conference/Paper1875/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1875/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Solving single-objective tasks by preference multi-objective reinforcement learning", "authors": ["Jinsheng Ren", "Shangqi Guo", "Feng Chen"], "authorids": ["rjs17@mails.tsinghua.edu.cn", "gsq15@mails.tsinghua.edu.cn", "chenfeng@mail.tsinghua.edu.cn"], "keywords": ["reinforcement learning", "single-objective tasks", "multi-objectivization"], "TL;DR": "Solving complex single-objective tasks by preference multi-objective reinforcement learning.", "abstract": "There ubiquitously exist many single-objective tasks in the real world that are inevitably related to some other objectives and influenced by them. We call such task as the objective-constrained task, which is inherently a multi-objective problem. Due to the conflict among different objectives, a trade-off is needed. A common compromise is to design a scalar reward function through clarifying the relationship among these objectives using the prior knowledge of experts. However, reward engineering is extremely cumbersome. This will result in behaviors that optimize our reward function without actually satisfying our preferences. In this paper, we explicitly cast the objective-constrained task as preference multi-objective reinforcement learning, with the overall goal of finding a Pareto optimal policy. Combined with Trajectory Preference Domination we propose, a weight vector that reflects the agent's preference for each objective can be learned. We analyzed the feasibility of our algorithm in theory, and further proved in experiments its better performance compared to those that design the reward function by experts.", "pdf": "/pdf/e9af58618cf7042ebcdf22dd12157fff9f303c7d.pdf", "paperhash": "ren|solving_singleobjective_tasks_by_preference_multiobjective_reinforcement_learning", "original_pdf": "/attachment/e9af58618cf7042ebcdf22dd12157fff9f303c7d.pdf", "_bibtex": "@misc{\nren2020solving,\ntitle={Solving single-objective tasks by preference multi-objective reinforcement learning},\nauthor={Jinsheng Ren and Shangqi Guo and Feng Chen},\nyear={2020},\nurl={https://openreview.net/forum?id=HJxV5yHYwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJxV5yHYwB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1875/Authors", "ICLR.cc/2020/Conference/Paper1875/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1875/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1875/Reviewers", "ICLR.cc/2020/Conference/Paper1875/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1875/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1875/Authors|ICLR.cc/2020/Conference/Paper1875/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504149621, "tmdate": 1576860553130, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1875/Authors", "ICLR.cc/2020/Conference/Paper1875/Reviewers", "ICLR.cc/2020/Conference/Paper1875/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1875/-/Official_Comment"}}}, {"id": "SkxhzAFtiH", "original": null, "number": 2, "cdate": 1573654036321, "ddate": null, "tcdate": 1573654036321, "tmdate": 1573655230283, "tddate": null, "forum": "HJxV5yHYwB", "replyto": "Byg4aCzE5S", "invitation": "ICLR.cc/2020/Conference/Paper1875/-/Official_Comment", "content": {"title": "Author response to Review #3", "comment": "Thank you for providing the feedback. We hope the following address some of your concerns.\n\nWe agree that more experiments are helpful to show the effectiveness of our method. However, almost all benchmark scenarios (e.g., Deep Sea Treasure, SuperMario, etc), which have been widely used to measure the performance of MORL algorithms, are not suitable for evaluating our contribution. The reasons are as follows. 1) The problem setting is different. Although the problem proposed also requires a learning agent to optimize two or more objectives at the same time, the major difference is that we focus on problems with one single primary objective and several additional helper-objectives, in which the main concern is how to utilize the helper-objectives so that the primary objective can be more efficiently optimized. 2) The weight vector is time-variant. Different from most benchmark scenarios, where the weight vector is time-invariant, in our Efficient Delivery environment, in order to deliver as many packages as possible, it is necessary for the agent to weigh the importance of the three objectives in every state, according to the distances from the delivery location, the charging location and the acceleration location. \n\t\nIn addition, the feasibility of our algorithm has been theoretically analyzed. Although only one scenario was used, we believe that the effectiveness of our algorithm has been sufficiently demonstrated. Therefore, we sincerely hope that the reviewer can understand why only one scenario was used in this paper."}, "signatures": ["ICLR.cc/2020/Conference/Paper1875/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1875/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Solving single-objective tasks by preference multi-objective reinforcement learning", "authors": ["Jinsheng Ren", "Shangqi Guo", "Feng Chen"], "authorids": ["rjs17@mails.tsinghua.edu.cn", "gsq15@mails.tsinghua.edu.cn", "chenfeng@mail.tsinghua.edu.cn"], "keywords": ["reinforcement learning", "single-objective tasks", "multi-objectivization"], "TL;DR": "Solving complex single-objective tasks by preference multi-objective reinforcement learning.", "abstract": "There ubiquitously exist many single-objective tasks in the real world that are inevitably related to some other objectives and influenced by them. We call such task as the objective-constrained task, which is inherently a multi-objective problem. Due to the conflict among different objectives, a trade-off is needed. A common compromise is to design a scalar reward function through clarifying the relationship among these objectives using the prior knowledge of experts. However, reward engineering is extremely cumbersome. This will result in behaviors that optimize our reward function without actually satisfying our preferences. In this paper, we explicitly cast the objective-constrained task as preference multi-objective reinforcement learning, with the overall goal of finding a Pareto optimal policy. Combined with Trajectory Preference Domination we propose, a weight vector that reflects the agent's preference for each objective can be learned. We analyzed the feasibility of our algorithm in theory, and further proved in experiments its better performance compared to those that design the reward function by experts.", "pdf": "/pdf/e9af58618cf7042ebcdf22dd12157fff9f303c7d.pdf", "paperhash": "ren|solving_singleobjective_tasks_by_preference_multiobjective_reinforcement_learning", "original_pdf": "/attachment/e9af58618cf7042ebcdf22dd12157fff9f303c7d.pdf", "_bibtex": "@misc{\nren2020solving,\ntitle={Solving single-objective tasks by preference multi-objective reinforcement learning},\nauthor={Jinsheng Ren and Shangqi Guo and Feng Chen},\nyear={2020},\nurl={https://openreview.net/forum?id=HJxV5yHYwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJxV5yHYwB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1875/Authors", "ICLR.cc/2020/Conference/Paper1875/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1875/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1875/Reviewers", "ICLR.cc/2020/Conference/Paper1875/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1875/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1875/Authors|ICLR.cc/2020/Conference/Paper1875/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504149621, "tmdate": 1576860553130, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1875/Authors", "ICLR.cc/2020/Conference/Paper1875/Reviewers", "ICLR.cc/2020/Conference/Paper1875/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1875/-/Official_Comment"}}}, {"id": "Syg1Lx9Yor", "original": null, "number": 3, "cdate": 1573654599139, "ddate": null, "tcdate": 1573654599139, "tmdate": 1573654599139, "tddate": null, "forum": "HJxV5yHYwB", "replyto": "rygBjki6tB", "invitation": "ICLR.cc/2020/Conference/Paper1875/-/Official_Comment", "content": {"title": "Author response to Reviewer #2 (part 1)", "comment": "We appreciate the time you spent reviewing our submission and hope our response help address some of your concerns.\n\nWe believe the reviewer may have misunderstanded our paper to some extent. Different from the reviewer's summary \"The main idea is converting the multi-objective problem into single objective by scalar weighting\", we propose to transform a single-objective problem to a preference multi-objective problem with learnable dynamic weights. This main idea has been elaborated in the third paragraph of introduction.\n\t\nQ1: \"its novelty is not even clear since authors did not discuss majority of the existing related work.\"\nA: Similar to MORL, the problem proposed by us also requires a learning agent to optimize two or more objectives at the same time. The major differences are as follows. 1) The problem setting is different. We focus on problems with one single primary objective and several additional helper-objectives, in which the main concern is how to utilize the helper-objectives so that the primary objective can be more efficiently optimized. 2) The weight vector is time-variant. Different from most MORL algorithms, where the weight vector is time-invariant, in our Efficient Delivery environment, in order to deliver as many packages as possible, it is necessary for the agent to weigh the importance of the three objectives in every state, according to the distances from the delivery location, the charging location and the acceleration location. \n\t\nIn terms of the time-variant weight setting, there are two existing researches [1,2] that are somewhat similar to our work. However, the weights in these works are known in advance, and the research in this field mainly focuses on how to leverage transfer learning techniques to accelerate the learning process when the weights change over time. In contrast, the weights in our work are not clear and even ambiguous in the beginning of training, and the main idea is to let the agent learn the weights among these objectives. More vividly, the task in our setting can be considered as a series of multiple tasks with different weights in dynamic weights setting. Optimistic linear support[3] uses the concept of corner weights to pick the weights to use for creating scalarised instances. However, it is unknown to determine which corner weight to use by satisfying the users\u2019 preferences. Moreover, each corner weight is still time-invariant. The work[4] explicitly cast multi-task learning as multi-objective optimization, with the overall objective of finding a Pareto optimal solution. A set of learnable weights is used to adjust the gradient magnitude from each task to weigh the conflict between tasks. Unlike this, the weight in our work is a function of state-action pair, thus it is time-variant.\n\t\nQ2: \"Definition 2 uses but not define 'p' in condition (2).\"\nA: We are sorry for the mistake. The letter 'p' is the initial of primary, denoting the primary objective.\n\t\nQ3: \"Lemma 1 states sth is 'far greater' than something else. However, 'far greater' is not really defined.\"\nA: The 'far greater' is used in the expression '$r(s_0^1,a_0^1)+\\cdots + r(s_{k-1}^1,a_{k-1}^1) \\gg r(s_0^2,a_0^2)+\\cdots + r(s_{t-1}^2,a_{t-1}^2)$', which is between Definition 1 and Definition 2. The symbol $'\\gg'$ denotes our learning objective, taking '$a \\gg b$' for example, which can be satisfied by maximizing $\\left(a-b\\right)$."}, "signatures": ["ICLR.cc/2020/Conference/Paper1875/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1875/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Solving single-objective tasks by preference multi-objective reinforcement learning", "authors": ["Jinsheng Ren", "Shangqi Guo", "Feng Chen"], "authorids": ["rjs17@mails.tsinghua.edu.cn", "gsq15@mails.tsinghua.edu.cn", "chenfeng@mail.tsinghua.edu.cn"], "keywords": ["reinforcement learning", "single-objective tasks", "multi-objectivization"], "TL;DR": "Solving complex single-objective tasks by preference multi-objective reinforcement learning.", "abstract": "There ubiquitously exist many single-objective tasks in the real world that are inevitably related to some other objectives and influenced by them. We call such task as the objective-constrained task, which is inherently a multi-objective problem. Due to the conflict among different objectives, a trade-off is needed. A common compromise is to design a scalar reward function through clarifying the relationship among these objectives using the prior knowledge of experts. However, reward engineering is extremely cumbersome. This will result in behaviors that optimize our reward function without actually satisfying our preferences. In this paper, we explicitly cast the objective-constrained task as preference multi-objective reinforcement learning, with the overall goal of finding a Pareto optimal policy. Combined with Trajectory Preference Domination we propose, a weight vector that reflects the agent's preference for each objective can be learned. We analyzed the feasibility of our algorithm in theory, and further proved in experiments its better performance compared to those that design the reward function by experts.", "pdf": "/pdf/e9af58618cf7042ebcdf22dd12157fff9f303c7d.pdf", "paperhash": "ren|solving_singleobjective_tasks_by_preference_multiobjective_reinforcement_learning", "original_pdf": "/attachment/e9af58618cf7042ebcdf22dd12157fff9f303c7d.pdf", "_bibtex": "@misc{\nren2020solving,\ntitle={Solving single-objective tasks by preference multi-objective reinforcement learning},\nauthor={Jinsheng Ren and Shangqi Guo and Feng Chen},\nyear={2020},\nurl={https://openreview.net/forum?id=HJxV5yHYwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJxV5yHYwB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1875/Authors", "ICLR.cc/2020/Conference/Paper1875/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1875/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1875/Reviewers", "ICLR.cc/2020/Conference/Paper1875/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1875/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1875/Authors|ICLR.cc/2020/Conference/Paper1875/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504149621, "tmdate": 1576860553130, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1875/Authors", "ICLR.cc/2020/Conference/Paper1875/Reviewers", "ICLR.cc/2020/Conference/Paper1875/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1875/-/Official_Comment"}}}], "count": 7}