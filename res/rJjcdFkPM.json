{"notes": [{"tddate": null, "ddate": null, "tmdate": 1521652002470, "tcdate": 1518471315434, "number": 297, "cdate": 1518471315434, "id": "rJjcdFkPM", "invitation": "ICLR.cc/2018/Workshop/-/Submission", "forum": "rJjcdFkPM", "original": "HkpYwMZRb", "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop"], "content": {"title": "Gradients explode - Deep Networks are shallow - ResNet explained", "abstract": "Whereas it is believed that techniques such as Adam, batch normalization and, more recently, SeLU nonlinearities \"solve\" the exploding gradient problem, we show that this is not the case and that in a range of popular MLP architectures, exploding gradients exist and that they limit the depth to which networks can be effectively trained, both in theory and in practice. We explain why exploding gradients occur and highlight the *collapsing domain problem*, which can arise in architectures that avoid exploding gradients. \n\nResNets have significantly lower gradients and thus can circumvent the exploding gradient problem, enabling the effective training of much deeper networks, which we show is a consequence of a surprising mathematical property. By noticing that *any neural network is a residual network*, we devise the *residual trick*, which reveals that introducing skip connections simplifies the network mathematically, and that this simplicity may be the major cause for their success.", "pdf": "/pdf/643fc02825d8f480d8a15dbfac490e10cab075a2.pdf", "TL;DR": "We show that in contras to popular wisdom, the exploding gradient problem has not been solved and that it limits the depth to which MLPs can be effectively trained. We show why gradients explode and how ResNet handles them.", "paperhash": "philipp|gradients_explode_deep_networks_are_shallow_resnet_explained", "_bibtex": "@misc{\nphilipp2018gradients,\ntitle={Gradients explode - Deep Networks are shallow - ResNet explained},\nauthor={George Philipp, Dawn Song, Jaime G. Carbonell},\nyear={2018},\nurl={https://openreview.net/forum?id=HkpYwMZRb},\n}", "keywords": ["deep learning", "MLP", "ResNet", "residual network", "exploding gradient problem", "vanishing gradient problem", "effective depth", "batch normalization", "covariate shift"], "authors": ["George Philipp", "Dawn Song", "Jaime G. Carbonell"], "authorids": ["george.philipp@email.de", "dawnsong@gmail.com", "jgc@cs.cmu.edu"]}, "nonreaders": [], "details": {"replyCount": 1, "writable": false, "overwriting": [], "revisions": true, "tags": [], "original": {"tddate": null, "ddate": null, "tmdate": 1518730162653, "tcdate": 1509136261164, "number": 860, "cdate": 1518730162643, "id": "HkpYwMZRb", "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "forum": "HkpYwMZRb", "original": "Hy2FvG-RZ", "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference"], "content": {"title": "Gradients explode - Deep Networks are shallow - ResNet explained", "abstract": "Whereas it is believed that techniques such as Adam, batch normalization and, more recently, SeLU nonlinearities ``solve'' the exploding gradient problem, we show that this is not the case and that in a range of popular MLP architectures, exploding gradients exist and that they limit the depth to which networks can be effectively trained, both in theory and in practice. We explain why exploding gradients occur and highlight the {\\it collapsing domain problem}, which can arise in architectures that avoid exploding gradients. \n\nResNets have significantly lower gradients and thus can circumvent the exploding gradient problem, enabling the effective training of much deeper networks, which we show is a consequence of a surprising mathematical property. By noticing that {\\it any neural network is a residual network}, we devise the {\\it residual trick}, which reveals that introducing skip connections simplifies the network mathematically, and that this simplicity may be the major cause for their success.", "pdf": "/pdf/09157368b4158528b45bd5fa08fc01b47cece834.pdf", "TL;DR": "We show that in contras to popular wisdom, the exploding gradient problem has not been solved and that it limits the depth to which MLPs can be effectively trained. We show why gradients explode and how ResNet handles them.", "paperhash": "philipp|gradients_explode_deep_networks_are_shallow_resnet_explained", "_bibtex": "@misc{\nphilipp2018gradients,\ntitle={Gradients explode - Deep Networks are shallow - ResNet explained},\nauthor={George Philipp and Dawn Song and Jaime G. Carbonell},\nyear={2018},\nurl={https://openreview.net/forum?id=HkpYwMZRb},\n}", "keywords": ["deep learning", "MLP", "ResNet", "residual network", "exploding gradient problem", "vanishing gradient problem", "effective depth", "batch normalization", "covariate shift"], "authors": ["George Philipp", "Dawn Song", "Jaime G. Carbonell"], "authorids": ["george.philipp@email.de", "dawnsong@gmail.com", "jgc@cs.cmu.edu"]}, "nonreaders": []}, "originalWritable": false, "originalInvitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1506717071958, "id": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Conference"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference"]}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"authors": {"required": false, "order": 1, "values-regex": ".*", "description": "Comma separated list of author names, as they appear in the paper."}, "authorids": {"required": false, "order": 2, "values-regex": ".*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "cdate": 1506717071958}, "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1518472800000, "tmdate": 1518474081690, "id": "ICLR.cc/2018/Workshop/-/Submission", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Workshop"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2018/Workshop", "description": "Your authorized identity to be associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 9, "value-regex": "upload", "description": "Upload a PDF file that ends with .pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 8, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names. Please provide real names; identities will be anonymized."}, "keywords": {"order": 6, "values-regex": "(^$)|[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of keywords."}, "TL;DR": {"required": false, "order": 7, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,500}"}, "authorids": {"required": true, "order": 3, "values-regex": "([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,},){0,}([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,})", "description": "Comma separated list of author email addresses, lowercased, in the same order as above. For authors with existing OpenReview accounts, please make sure that the provided email address(es) match those listed in the author's profile. Please provide real emails; identities will be anonymized."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1526248800000, "cdate": 1518474081690}}, "tauthor": "ICLR.cc/2018/Workshop"}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521573625134, "tcdate": 1521573625134, "number": 343, "cdate": 1521573624794, "id": "r1ZWJJy5f", "invitation": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "forum": "rJjcdFkPM", "replyto": "rJjcdFkPM", "signatures": ["ICLR.cc/2018/Workshop/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Program_Chairs"], "content": {"decision": "Accept", "title": "ICLR 2018 Workshop Acceptance Decision", "comment": "This paper was invited to the workshop track based on reviews at the main conference."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Gradients explode - Deep Networks are shallow - ResNet explained", "abstract": "Whereas it is believed that techniques such as Adam, batch normalization and, more recently, SeLU nonlinearities \"solve\" the exploding gradient problem, we show that this is not the case and that in a range of popular MLP architectures, exploding gradients exist and that they limit the depth to which networks can be effectively trained, both in theory and in practice. We explain why exploding gradients occur and highlight the *collapsing domain problem*, which can arise in architectures that avoid exploding gradients. \n\nResNets have significantly lower gradients and thus can circumvent the exploding gradient problem, enabling the effective training of much deeper networks, which we show is a consequence of a surprising mathematical property. By noticing that *any neural network is a residual network*, we devise the *residual trick*, which reveals that introducing skip connections simplifies the network mathematically, and that this simplicity may be the major cause for their success.", "pdf": "/pdf/643fc02825d8f480d8a15dbfac490e10cab075a2.pdf", "TL;DR": "We show that in contras to popular wisdom, the exploding gradient problem has not been solved and that it limits the depth to which MLPs can be effectively trained. We show why gradients explode and how ResNet handles them.", "paperhash": "philipp|gradients_explode_deep_networks_are_shallow_resnet_explained", "_bibtex": "@misc{\nphilipp2018gradients,\ntitle={Gradients explode - Deep Networks are shallow - ResNet explained},\nauthor={George Philipp, Dawn Song, Jaime G. Carbonell},\nyear={2018},\nurl={https://openreview.net/forum?id=HkpYwMZRb},\n}", "keywords": ["deep learning", "MLP", "ResNet", "residual network", "exploding gradient problem", "vanishing gradient problem", "effective depth", "batch normalization", "covariate shift"], "authors": ["George Philipp", "Dawn Song", "Jaime G. Carbonell"], "authorids": ["george.philipp@email.de", "dawnsong@gmail.com", "jgc@cs.cmu.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518629844880, "id": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Program_Chairs"], "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Workshop/-/Submission", "writers": {"values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Workshop/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Workshop Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept", "Reject"]}}}, "nonreaders": [], "noninvitees": [], "cdate": 1518629844880}}}], "count": 2}