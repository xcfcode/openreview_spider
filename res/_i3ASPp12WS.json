{"notes": [{"id": "_i3ASPp12WS", "original": "1BW2HZC926", "number": 2704, "cdate": 1601308299664, "ddate": null, "tcdate": 1601308299664, "tmdate": 1615705116353, "tddate": null, "forum": "_i3ASPp12WS", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Online Adversarial Purification based on Self-supervised Learning", "authorids": ["~Changhao_Shi1", "~Chester_Holtz1", "~Gal_Mishne1"], "authors": ["Changhao Shi", "Chester Holtz", "Gal Mishne"], "keywords": ["Adversarial Robustness", "Self-Supervised Learning"], "abstract": "Deep neural networks are known to be vulnerable to adversarial examples, where a perturbation in the input space leads to an amplified shift in the latent network representation. In this paper, we combine canonical supervised learning with self-supervised representation learning, and present Self-supervised Online Adversarial Purification (SOAP), a novel defense strategy that uses a self-supervised loss to purify adversarial examples at test-time. Our approach leverages the label-independent nature of self-supervised signals and counters the adversarial perturbation with respect to the self-supervised tasks. SOAP yields competitive robust accuracy against state-of-the-art adversarial training and purification methods, with considerably less training complexity. In addition, our approach is robust even when adversaries are given the knowledge of the purification defense strategy. To the best of our knowledge, our paper is the first that generalizes the idea of using self-supervised signals to perform online test-time purification.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shi|online_adversarial_purification_based_on_selfsupervised_learning", "pdf": "/pdf/c72b2431912d9433eb862f7ff1c59d589191b939.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nshi2021online,\ntitle={Online Adversarial Purification based on Self-supervised Learning},\nauthor={Changhao Shi and Chester Holtz and Gal Mishne},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=_i3ASPp12WS}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 20, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "z956HDKXUf2", "original": null, "number": 1, "cdate": 1610040438274, "ddate": null, "tcdate": 1610040438274, "tmdate": 1610474039054, "tddate": null, "forum": "_i3ASPp12WS", "replyto": "_i3ASPp12WS", "invitation": "ICLR.cc/2021/Conference/Paper2704/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Poster)", "comment": "This paper presents a defense scheme for adversarial attacks, called self-supervised online adversarial purification (SOAP), by purifying the adversarial examples at test time. The novelty of this work is in its incorporation of self-supervised representation learning into adversarial defense through purification via optimizing an auxiliary self-supervised loss. This is done by jointly training the model on a self-supervised task while it is learning to perform the target classification task in a multi-task learning setting. Compared with existing adversarial defense schemes such as adversarial training and purification techniques, SOAP has a lower computation overhead during the training stage.\n\n**Strengths:**\n  * It is novel to incorporate self-supervised learning for adversarial purification at test time.\n  * SOAP\u2019s training stage based on multi-task learning incurs low computation overhead compared with the original classification task.\n\n**Weaknesses:**\n  * Although the proposed adversarial defense scheme is computationally cheaper than the other existing methods during the training stage, it does incur some overhead during test time. This may be undesirable for some applications in which efficiency during test time is an important factor to consider.\n  * The choice of a suitable self-supervised auxiliary task is somewhat ad hoc. The performance varies a lot for different auxiliary tasks.\n  * The experimental evaluation is only based on relatively small and unrealistic datasets even after new experiments on CIFAR-100 have been added by the authors.\n\nIt is said in the paper that SOAP can exploit a wider range of self-supervised signals for purification and hence conceptually can be applied to any format of data and not just images, given an appropriate self-supervised task. However, this claim has not been substantiated in the paper using non-image data.\n\nDespite some limitations and that some claims still need to be better substantiated, the paper presents some novel ideas which are expected to arouse interest for follow-up work in the adversarial attack and defense research community.\n"}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Online Adversarial Purification based on Self-supervised Learning", "authorids": ["~Changhao_Shi1", "~Chester_Holtz1", "~Gal_Mishne1"], "authors": ["Changhao Shi", "Chester Holtz", "Gal Mishne"], "keywords": ["Adversarial Robustness", "Self-Supervised Learning"], "abstract": "Deep neural networks are known to be vulnerable to adversarial examples, where a perturbation in the input space leads to an amplified shift in the latent network representation. In this paper, we combine canonical supervised learning with self-supervised representation learning, and present Self-supervised Online Adversarial Purification (SOAP), a novel defense strategy that uses a self-supervised loss to purify adversarial examples at test-time. Our approach leverages the label-independent nature of self-supervised signals and counters the adversarial perturbation with respect to the self-supervised tasks. SOAP yields competitive robust accuracy against state-of-the-art adversarial training and purification methods, with considerably less training complexity. In addition, our approach is robust even when adversaries are given the knowledge of the purification defense strategy. To the best of our knowledge, our paper is the first that generalizes the idea of using self-supervised signals to perform online test-time purification.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shi|online_adversarial_purification_based_on_selfsupervised_learning", "pdf": "/pdf/c72b2431912d9433eb862f7ff1c59d589191b939.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nshi2021online,\ntitle={Online Adversarial Purification based on Self-supervised Learning},\nauthor={Changhao Shi and Chester Holtz and Gal Mishne},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=_i3ASPp12WS}\n}"}, "tags": [], "invitation": {"reply": {"forum": "_i3ASPp12WS", "replyto": "_i3ASPp12WS", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040438261, "tmdate": 1610474039038, "id": "ICLR.cc/2021/Conference/Paper2704/-/Decision"}}}, {"id": "vp78c1ImyzJ", "original": null, "number": 1, "cdate": 1603739504086, "ddate": null, "tcdate": 1603739504086, "tmdate": 1606586488899, "tddate": null, "forum": "_i3ASPp12WS", "replyto": "_i3ASPp12WS", "invitation": "ICLR.cc/2021/Conference/Paper2704/-/Official_Review", "content": {"title": "Official Blind Review #2", "review": "[Summary]\nOnline defenses of adversarial examples is an old topic: Given an input x (potentially adversarially perturbed) at test time, we want to sanitize x to get x', on which the trained classifier $g \\circ f$ gives the correct answer. This paper proposes a new architecture for online defenses via self supervision. There are two new things in the proposal:\n\n1. There is an explicit representation function f, namely the classifier is decomposed into $g \\circ f$. And the auxiliary self-supervised component h works on the same representation. This thus creates a Y-shape architecture that is \"syntactically\" similar to the training structure in unsupervised domain adaptation (e.g., domain adversarial neural networks). This architecture for online defense seems new (as far as I know).\n\n2. The paper leverages an interesting hypothesis that for a common f, a large classification loss happens if and only if a large self-supervision loss happens. And this paper provides solid evidence to justify this -- namely in Section 4.1 (auxiliary-aware attacks), it evaluates the defense against an adversary that is aware of h, in order to create adversarial examples that explicitly breaks the hypothesis (i.e. large classification loss but small self-supervision loss).\n\n3. For the experiments -- the paper trained f, g, and h under Gaussian corruptions, and indeed found that this online purification strategy provides robustness under adversarial perturbations, even for auxiliary-aware attacks, which is interesting.\n\n[Assessment]\n1. My first worry is that the performance of the defense is still much worse than the performance from direct adversarial training (for example, check the MNIST numbers). For example, under PGD, on CNN architecture we can achieve 80%ish accuracy. Note that for MNST, a simple discretization can already achieve almost-perfect accuracy. This is especially the case if we consider auxiliary-aware attacks.\n\n2. Following (1), what worries me more is that online-purification still needs to be aware of the attack type. Namely if one looks into equation (4), the objective has encoded norm-based attacks within it. This makes the results less interesting.\n\n3. All in all, my major doubt is what is really the benefit of reduced training complexity if we cannot achieve better robustness, and also the defense still needs to be fully aware of the attack type? For these reasons, I vote for a weak reject.\n\n[Questions]\n1. Why do we need to know the results for FCN (fully connected networks)?\n\n2. I am not sure the numbers reported for adversarial training match the state of the art reported in the MNIST challenge leaderboard: https://github.com/MadryLab/mnist_challenge. There the SOTA MNIST model always has >88% accuracy (so I am a bit skeptical about DF can bring down the accuracy to 78% for PGD AT). Also, how about applying those attacks for the self-supervision defense? (that's an additional request). Similarly, for CIFAR10, as shown by https://github.com/MadryLab/cifar10_challenge, PGD AT is never under 43%, but in Table 2, the robust accuracy is only 2% under CW attack. This is suspicious.\n\n[Post rebuttal]\n\nAfter more discussion and reading through the revision, I think this is a good paper and will be useful to the community for an instance of test-time defenses.", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper2704/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2704/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Online Adversarial Purification based on Self-supervised Learning", "authorids": ["~Changhao_Shi1", "~Chester_Holtz1", "~Gal_Mishne1"], "authors": ["Changhao Shi", "Chester Holtz", "Gal Mishne"], "keywords": ["Adversarial Robustness", "Self-Supervised Learning"], "abstract": "Deep neural networks are known to be vulnerable to adversarial examples, where a perturbation in the input space leads to an amplified shift in the latent network representation. In this paper, we combine canonical supervised learning with self-supervised representation learning, and present Self-supervised Online Adversarial Purification (SOAP), a novel defense strategy that uses a self-supervised loss to purify adversarial examples at test-time. Our approach leverages the label-independent nature of self-supervised signals and counters the adversarial perturbation with respect to the self-supervised tasks. SOAP yields competitive robust accuracy against state-of-the-art adversarial training and purification methods, with considerably less training complexity. In addition, our approach is robust even when adversaries are given the knowledge of the purification defense strategy. To the best of our knowledge, our paper is the first that generalizes the idea of using self-supervised signals to perform online test-time purification.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shi|online_adversarial_purification_based_on_selfsupervised_learning", "pdf": "/pdf/c72b2431912d9433eb862f7ff1c59d589191b939.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nshi2021online,\ntitle={Online Adversarial Purification based on Self-supervised Learning},\nauthor={Changhao Shi and Chester Holtz and Gal Mishne},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=_i3ASPp12WS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "_i3ASPp12WS", "replyto": "_i3ASPp12WS", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2704/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538090357, "tmdate": 1606915783276, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2704/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2704/-/Official_Review"}}}, {"id": "po_6iM4JmD", "original": null, "number": 3, "cdate": 1604288647704, "ddate": null, "tcdate": 1604288647704, "tmdate": 1606293038513, "tddate": null, "forum": "_i3ASPp12WS", "replyto": "_i3ASPp12WS", "invitation": "ICLR.cc/2021/Conference/Paper2704/-/Official_Review", "content": {"title": "reasonable and interesting idea, but needs more empirical validation", "review": "###################\nSummary:\n\nThis paper studies adversarial defense by combing purification and self-supervised loss. During inference, the authors propose an online-purification method based on (clipped) iterative gradient ascent. The loss used by purification is from some pre-defined self-supervised tasks. During training, joint loss of softmax and self-supervised loss are used to match the purification process in inference. Experiments on MNIST10 and CIFAR10 demonstrate the effectiveness of the proposed method over several SOTA baselines. The evaluation considers both the white-box and black-box attack setup.\n\n###################\nPros\n\n1. The proposed method is well-motivated and reasonable.\n\n2. The paper is clear and easy-to-follow.\n\n\n###################\nCons\n\n1. What is the T  for the online purification? Large T will significantly slow down the test time efficiency.\n\n2. In Table 2, \"FGSM AT\" + \"PGD\", why it is \"37.4%\"? My understanding is this should be very small value, since multi-step PGD attack is pretty strong.\n\n3. I am curious to see the gain by purely online-purification, maybe using the encoder by \"PGD AT\".\n\n4. Seems like self-supervised tasks are pretty ad-hoc. Is there a principled way of selecting a good self-supervised task?\n\n5. The two datasets used in the paper represents limited visual patterns. I think larger-dataset needs to be used, like cifar100, tiny imagenet.\n\n######################### post-rebuttal\n\nI appreciate the additional explanations and experiments by the authors. I also read the public discussion threads. I raise my score to 6. Two things for future:\n- Make it work on bigger and more realistic images, imagenet, pascal, coco, etc. Now the adversarial community and deep learning community in general, highly relies on experiments, because theoretical guarantee is still mysterious. So we should push the field forward, by proving ideas on harder datasets. \n- Explore stronger attacks, particularly gradient-free attack to avoid the obfuscated gradients.\n\n\n\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2704/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2704/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Online Adversarial Purification based on Self-supervised Learning", "authorids": ["~Changhao_Shi1", "~Chester_Holtz1", "~Gal_Mishne1"], "authors": ["Changhao Shi", "Chester Holtz", "Gal Mishne"], "keywords": ["Adversarial Robustness", "Self-Supervised Learning"], "abstract": "Deep neural networks are known to be vulnerable to adversarial examples, where a perturbation in the input space leads to an amplified shift in the latent network representation. In this paper, we combine canonical supervised learning with self-supervised representation learning, and present Self-supervised Online Adversarial Purification (SOAP), a novel defense strategy that uses a self-supervised loss to purify adversarial examples at test-time. Our approach leverages the label-independent nature of self-supervised signals and counters the adversarial perturbation with respect to the self-supervised tasks. SOAP yields competitive robust accuracy against state-of-the-art adversarial training and purification methods, with considerably less training complexity. In addition, our approach is robust even when adversaries are given the knowledge of the purification defense strategy. To the best of our knowledge, our paper is the first that generalizes the idea of using self-supervised signals to perform online test-time purification.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shi|online_adversarial_purification_based_on_selfsupervised_learning", "pdf": "/pdf/c72b2431912d9433eb862f7ff1c59d589191b939.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nshi2021online,\ntitle={Online Adversarial Purification based on Self-supervised Learning},\nauthor={Changhao Shi and Chester Holtz and Gal Mishne},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=_i3ASPp12WS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "_i3ASPp12WS", "replyto": "_i3ASPp12WS", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2704/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538090357, "tmdate": 1606915783276, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2704/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2704/-/Official_Review"}}}, {"id": "pzuI7o2btWH", "original": null, "number": 19, "cdate": 1606208720964, "ddate": null, "tcdate": 1606208720964, "tmdate": 1606208720964, "tddate": null, "forum": "_i3ASPp12WS", "replyto": "Qp7GOEWxpcu", "invitation": "ICLR.cc/2021/Conference/Paper2704/-/Official_Comment", "content": {"title": "Thanks!", "comment": "Dear Authors.\n\nThank you very much for your reply, and for addressing my review comments.\n\nI think in my side all is clear now and I will keep my score.\n\nThanks!"}, "signatures": ["ICLR.cc/2021/Conference/Paper2704/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2704/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Online Adversarial Purification based on Self-supervised Learning", "authorids": ["~Changhao_Shi1", "~Chester_Holtz1", "~Gal_Mishne1"], "authors": ["Changhao Shi", "Chester Holtz", "Gal Mishne"], "keywords": ["Adversarial Robustness", "Self-Supervised Learning"], "abstract": "Deep neural networks are known to be vulnerable to adversarial examples, where a perturbation in the input space leads to an amplified shift in the latent network representation. In this paper, we combine canonical supervised learning with self-supervised representation learning, and present Self-supervised Online Adversarial Purification (SOAP), a novel defense strategy that uses a self-supervised loss to purify adversarial examples at test-time. Our approach leverages the label-independent nature of self-supervised signals and counters the adversarial perturbation with respect to the self-supervised tasks. SOAP yields competitive robust accuracy against state-of-the-art adversarial training and purification methods, with considerably less training complexity. In addition, our approach is robust even when adversaries are given the knowledge of the purification defense strategy. To the best of our knowledge, our paper is the first that generalizes the idea of using self-supervised signals to perform online test-time purification.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shi|online_adversarial_purification_based_on_selfsupervised_learning", "pdf": "/pdf/c72b2431912d9433eb862f7ff1c59d589191b939.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nshi2021online,\ntitle={Online Adversarial Purification based on Self-supervised Learning},\nauthor={Changhao Shi and Chester Holtz and Gal Mishne},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=_i3ASPp12WS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "_i3ASPp12WS", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2704/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2704/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2704/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2704/Authors|ICLR.cc/2021/Conference/Paper2704/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2704/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923845334, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2704/-/Official_Comment"}}}, {"id": "TnyDwMRZegn", "original": null, "number": 16, "cdate": 1606185012109, "ddate": null, "tcdate": 1606185012109, "tmdate": 1606187906599, "tddate": null, "forum": "_i3ASPp12WS", "replyto": "vp78c1ImyzJ", "invitation": "ICLR.cc/2021/Conference/Paper2704/-/Official_Comment", "content": {"title": "Update my score and rationales", "comment": "Thanks the authors. I have updated my score to 7. For two reasons: (1) This paper has provided clear and reasonable evidence for robustness in the white-box case. And (2) It may have a potential to provide the first example of adversarial robustness (that actually works in the white-box model) where at the training time we do not need explicit adversarial training. In that sense, I think either confirming or refuting the claim is of interest to the community."}, "signatures": ["ICLR.cc/2021/Conference/Paper2704/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2704/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Online Adversarial Purification based on Self-supervised Learning", "authorids": ["~Changhao_Shi1", "~Chester_Holtz1", "~Gal_Mishne1"], "authors": ["Changhao Shi", "Chester Holtz", "Gal Mishne"], "keywords": ["Adversarial Robustness", "Self-Supervised Learning"], "abstract": "Deep neural networks are known to be vulnerable to adversarial examples, where a perturbation in the input space leads to an amplified shift in the latent network representation. In this paper, we combine canonical supervised learning with self-supervised representation learning, and present Self-supervised Online Adversarial Purification (SOAP), a novel defense strategy that uses a self-supervised loss to purify adversarial examples at test-time. Our approach leverages the label-independent nature of self-supervised signals and counters the adversarial perturbation with respect to the self-supervised tasks. SOAP yields competitive robust accuracy against state-of-the-art adversarial training and purification methods, with considerably less training complexity. In addition, our approach is robust even when adversaries are given the knowledge of the purification defense strategy. To the best of our knowledge, our paper is the first that generalizes the idea of using self-supervised signals to perform online test-time purification.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shi|online_adversarial_purification_based_on_selfsupervised_learning", "pdf": "/pdf/c72b2431912d9433eb862f7ff1c59d589191b939.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nshi2021online,\ntitle={Online Adversarial Purification based on Self-supervised Learning},\nauthor={Changhao Shi and Chester Holtz and Gal Mishne},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=_i3ASPp12WS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "_i3ASPp12WS", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2704/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2704/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2704/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2704/Authors|ICLR.cc/2021/Conference/Paper2704/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2704/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923845334, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2704/-/Official_Comment"}}}, {"id": "ZXB3zCKroZ7", "original": null, "number": 15, "cdate": 1606162861801, "ddate": null, "tcdate": 1606162861801, "tmdate": 1606162861801, "tddate": null, "forum": "_i3ASPp12WS", "replyto": "a3UnR2XE_ez", "invitation": "ICLR.cc/2021/Conference/Paper2704/-/Official_Comment", "content": {"title": "You are right, apologies for the confusions", "comment": "I have a wrong memory about (2) and (3) (being a while after my last read). Yes objective (2) is natural training and only objective (3) (which is the online part) at the test-time has an adversarial training form. But isn't (3) a form of adversarial training? (Algorithm 1 and 2 are also adversarial training too). So the main evaluation is still \"norm bounded adversarial training\" against \"norm bounded attacks\". Can you try to defend against rotation-based attacks? I think that will demonstrate more surprising \"attack agnostic\".\n\nI think we are on the same page, but maybe I take a different perspective about what does it by \"attack agnostic\" (but I agree that your point about $\\ell_2$ attacks is interesting). "}, "signatures": ["ICLR.cc/2021/Conference/Paper2704/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2704/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Online Adversarial Purification based on Self-supervised Learning", "authorids": ["~Changhao_Shi1", "~Chester_Holtz1", "~Gal_Mishne1"], "authors": ["Changhao Shi", "Chester Holtz", "Gal Mishne"], "keywords": ["Adversarial Robustness", "Self-Supervised Learning"], "abstract": "Deep neural networks are known to be vulnerable to adversarial examples, where a perturbation in the input space leads to an amplified shift in the latent network representation. In this paper, we combine canonical supervised learning with self-supervised representation learning, and present Self-supervised Online Adversarial Purification (SOAP), a novel defense strategy that uses a self-supervised loss to purify adversarial examples at test-time. Our approach leverages the label-independent nature of self-supervised signals and counters the adversarial perturbation with respect to the self-supervised tasks. SOAP yields competitive robust accuracy against state-of-the-art adversarial training and purification methods, with considerably less training complexity. In addition, our approach is robust even when adversaries are given the knowledge of the purification defense strategy. To the best of our knowledge, our paper is the first that generalizes the idea of using self-supervised signals to perform online test-time purification.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shi|online_adversarial_purification_based_on_selfsupervised_learning", "pdf": "/pdf/c72b2431912d9433eb862f7ff1c59d589191b939.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nshi2021online,\ntitle={Online Adversarial Purification based on Self-supervised Learning},\nauthor={Changhao Shi and Chester Holtz and Gal Mishne},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=_i3ASPp12WS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "_i3ASPp12WS", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2704/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2704/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2704/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2704/Authors|ICLR.cc/2021/Conference/Paper2704/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2704/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923845334, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2704/-/Official_Comment"}}}, {"id": "a3UnR2XE_ez", "original": null, "number": 14, "cdate": 1606162030099, "ddate": null, "tcdate": 1606162030099, "tmdate": 1606162030099, "tddate": null, "forum": "_i3ASPp12WS", "replyto": "lXCEMfQmFfa", "invitation": "ICLR.cc/2021/Conference/Paper2704/-/Official_Comment", "content": {"title": "Further Clarification on the Training Strategy", "comment": "Thank you for your response. We would like to further clarify that our defense strategy is purely online and does not rely on adversarial examples during training. Note that the training objective is Equation.2 where x is a natural example (corrupted by Gaussian noises), and we minimize a joint classification and auxiliary loss. No adversarial training is used in this.\nAt test-time, we perform online purification based on Equation.4, which finds an l_inf bounded \u201ccorrection\u201d to the input image, again without knowledge of the attack type or its strength. We assume this performs well on l_2 attacks because the bounded purification perturbation is strong enough and the model is trained with noises to deal with residuals of imperfect purification.\nWe agree that combining adversarial training is a promising direction to explore, but at the current stage we found that, with online/test-time purification, doing auxiliary/multi-task training on noisy natural examples alone already gives us decent results. This is also for the sake of efficiency - SOAP is more efficient during training than traditional adversarial training since it does not perturb examples adversarially."}, "signatures": ["ICLR.cc/2021/Conference/Paper2704/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2704/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Online Adversarial Purification based on Self-supervised Learning", "authorids": ["~Changhao_Shi1", "~Chester_Holtz1", "~Gal_Mishne1"], "authors": ["Changhao Shi", "Chester Holtz", "Gal Mishne"], "keywords": ["Adversarial Robustness", "Self-Supervised Learning"], "abstract": "Deep neural networks are known to be vulnerable to adversarial examples, where a perturbation in the input space leads to an amplified shift in the latent network representation. In this paper, we combine canonical supervised learning with self-supervised representation learning, and present Self-supervised Online Adversarial Purification (SOAP), a novel defense strategy that uses a self-supervised loss to purify adversarial examples at test-time. Our approach leverages the label-independent nature of self-supervised signals and counters the adversarial perturbation with respect to the self-supervised tasks. SOAP yields competitive robust accuracy against state-of-the-art adversarial training and purification methods, with considerably less training complexity. In addition, our approach is robust even when adversaries are given the knowledge of the purification defense strategy. To the best of our knowledge, our paper is the first that generalizes the idea of using self-supervised signals to perform online test-time purification.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shi|online_adversarial_purification_based_on_selfsupervised_learning", "pdf": "/pdf/c72b2431912d9433eb862f7ff1c59d589191b939.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nshi2021online,\ntitle={Online Adversarial Purification based on Self-supervised Learning},\nauthor={Changhao Shi and Chester Holtz and Gal Mishne},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=_i3ASPp12WS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "_i3ASPp12WS", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2704/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2704/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2704/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2704/Authors|ICLR.cc/2021/Conference/Paper2704/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2704/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923845334, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2704/-/Official_Comment"}}}, {"id": "lXCEMfQmFfa", "original": null, "number": 13, "cdate": 1606149532496, "ddate": null, "tcdate": 1606149532496, "tmdate": 1606149586075, "tddate": null, "forum": "_i3ASPp12WS", "replyto": "DaMlzyR0zOj", "invitation": "ICLR.cc/2021/Conference/Paper2704/-/Official_Comment", "content": {"title": "Quick replies on the \"attack agnostic\" argument", "comment": "Thanks for the update. The fact that it can defend against $\\ell_2$ attacks, with only $\\ell_\\infty$ training in place, is a good point.\n\nTo push back a little, while I acknowledge that I have made very specific words of \"knowing the attack type\", what was (also) in my mind is that this whole process (note that the defense is actually training+purification, which are two algorithms), we are still doing adversarial training (and at least now, the training does not jump out of the norm-based adv training). So to certain extent from the empirical results, it is more \"agnostic\", but it is not the \"real agnostic\" in my mind (at least for the current evidence, maybe this is actually too much to hope for). For example, how about using your $\\ell_\\infty$-based defense to defend against rotation-based attacks (even just transfer attacks?) Nevertheless, I stress again that like the empirical evidence on defending $\\ell_2$ attacks.\n\nAlso, are you aware of any **theoretical evidence** why in this case training with $\\ell_\\infty$ can give $\\ell_2$ robustness as well? I am asking this because adv training has a well-justified theoretical evidence that on the 1-layer network it becomes large-margin training (moreover, see this paper https://arxiv.org/abs/2005.10190 for more theory). I worry a bit about the robustness on $\\ell_2$ will be brittle without any \"bullet-proof\" evidence (I know this is challenging, but I want to put this here).\n\nFinally, for this submission, I will need to talk to other reviewers in the post-rebuttal period, to finally decide my score. Thanks.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2704/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2704/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Online Adversarial Purification based on Self-supervised Learning", "authorids": ["~Changhao_Shi1", "~Chester_Holtz1", "~Gal_Mishne1"], "authors": ["Changhao Shi", "Chester Holtz", "Gal Mishne"], "keywords": ["Adversarial Robustness", "Self-Supervised Learning"], "abstract": "Deep neural networks are known to be vulnerable to adversarial examples, where a perturbation in the input space leads to an amplified shift in the latent network representation. In this paper, we combine canonical supervised learning with self-supervised representation learning, and present Self-supervised Online Adversarial Purification (SOAP), a novel defense strategy that uses a self-supervised loss to purify adversarial examples at test-time. Our approach leverages the label-independent nature of self-supervised signals and counters the adversarial perturbation with respect to the self-supervised tasks. SOAP yields competitive robust accuracy against state-of-the-art adversarial training and purification methods, with considerably less training complexity. In addition, our approach is robust even when adversaries are given the knowledge of the purification defense strategy. To the best of our knowledge, our paper is the first that generalizes the idea of using self-supervised signals to perform online test-time purification.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shi|online_adversarial_purification_based_on_selfsupervised_learning", "pdf": "/pdf/c72b2431912d9433eb862f7ff1c59d589191b939.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nshi2021online,\ntitle={Online Adversarial Purification based on Self-supervised Learning},\nauthor={Changhao Shi and Chester Holtz and Gal Mishne},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=_i3ASPp12WS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "_i3ASPp12WS", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2704/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2704/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2704/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2704/Authors|ICLR.cc/2021/Conference/Paper2704/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2704/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923845334, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2704/-/Official_Comment"}}}, {"id": "DaMlzyR0zOj", "original": null, "number": 12, "cdate": 1606128381646, "ddate": null, "tcdate": 1606128381646, "tmdate": 1606128381646, "tddate": null, "forum": "_i3ASPp12WS", "replyto": "vp78c1ImyzJ", "invitation": "ICLR.cc/2021/Conference/Paper2704/-/Official_Comment", "content": {"title": "Official Reply to Reviewer 2", "comment": "We thank you for your valuable feedback on our paper. We would like to address your concerns accordingly:\n\n1. By including the FCN results, we show that SOAP works for various architectures, and is not specific to CNNs. Though real-world visual understanding algorithms almost always rely on CNNs, the paradigm of SOAP is not specific to images and can easily generalize to other data formats, such as text and graphs. FCN or other architectures can be useful in those scenarios, combined with an appropriate self-supervised task.\n2. The reason that our CW results seem low compared to the results posted on the leaderboard is that the two attacks are actually different despite bearing the same name. Specifically, our CW attack is the optimization-based l_2 bounded (as is DF) version from the original paper (please see Sec. 4.1), while the leaderboard\u2019s CW attack is the PGD-based l_inf bounded version from the PGD paper. For optimization-based attacks, the accuracy of any classifier can always go down to 0 if the bound is large enough. We apologize for the lack of clarity.\n\nIn response to some other concerns:\n\n1. While it\u2019s true that SOAP does not out-perform PGD AT with CNNs on MNIST (we actually close the gap by tuning the CNNs more carefully), SOAP does out-perform PGD and FGSM AT on MNIST when the model capacity is low, and on CIFAR10 and CIFAR100 in general (which are more realistic image datasets). SOAP also achieves very competitive accuracies on MNIST on black-box attacks.\nWe also want to mention that, though not revealed in the paper, our experiments show that using uniform noises rather than Gaussian noises during training can improve the PGD accuracy of SOAP-DR to 90.13% with CNNs on MNIST. We choose to use Gaussian noises for overall performance on all considered attacks.\n2. Our defense is not based on knowledge of the attack. Rather we need to constrain the form of our defense (the perturbation used to purify the image) in order to solve the optimization problem. While our defense does parallel the l_inf bounded attack, note that in our experiments we show it performs well on both l_inf bounded attacks (FGSM, PGD) and l_2 bounded attacks (CW, DF). This is in contrast to AT which uses knowledge of the attack type (l_inf) and performs poorly on l_2 attacks. Although our purification is based on l_inf-norm perturbations in Eq. 4 (because the purifier needs some budget), we show that SOAP can defend at least against l_inf and l_2 and potentially more. We have further clarified this in Sec. 3.3.\n3. As above, our defense is not aware of the attack and we do obtain better accuracy on more difficult datasets than MNIST, e.g. CIFAR10 and CIFAR100. Note that we significantly out-perform AT on l_2 attacks.  \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2704/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2704/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Online Adversarial Purification based on Self-supervised Learning", "authorids": ["~Changhao_Shi1", "~Chester_Holtz1", "~Gal_Mishne1"], "authors": ["Changhao Shi", "Chester Holtz", "Gal Mishne"], "keywords": ["Adversarial Robustness", "Self-Supervised Learning"], "abstract": "Deep neural networks are known to be vulnerable to adversarial examples, where a perturbation in the input space leads to an amplified shift in the latent network representation. In this paper, we combine canonical supervised learning with self-supervised representation learning, and present Self-supervised Online Adversarial Purification (SOAP), a novel defense strategy that uses a self-supervised loss to purify adversarial examples at test-time. Our approach leverages the label-independent nature of self-supervised signals and counters the adversarial perturbation with respect to the self-supervised tasks. SOAP yields competitive robust accuracy against state-of-the-art adversarial training and purification methods, with considerably less training complexity. In addition, our approach is robust even when adversaries are given the knowledge of the purification defense strategy. To the best of our knowledge, our paper is the first that generalizes the idea of using self-supervised signals to perform online test-time purification.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shi|online_adversarial_purification_based_on_selfsupervised_learning", "pdf": "/pdf/c72b2431912d9433eb862f7ff1c59d589191b939.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nshi2021online,\ntitle={Online Adversarial Purification based on Self-supervised Learning},\nauthor={Changhao Shi and Chester Holtz and Gal Mishne},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=_i3ASPp12WS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "_i3ASPp12WS", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2704/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2704/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2704/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2704/Authors|ICLR.cc/2021/Conference/Paper2704/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2704/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923845334, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2704/-/Official_Comment"}}}, {"id": "MvOrXMw8VOv", "original": null, "number": 10, "cdate": 1606127091522, "ddate": null, "tcdate": 1606127091522, "tmdate": 1606127442903, "tddate": null, "forum": "_i3ASPp12WS", "replyto": "po_6iM4JmD", "invitation": "ICLR.cc/2021/Conference/Paper2704/-/Official_Comment", "content": {"title": "Official Reply to Reviewer 4", "comment": "We thank you for your valuable feedback on our paper. We would like to address your concerns accordingly\n\n1. We agree that a large T will diminish test-time efficiency. As shown in the Sec. 4 Experiment, the value of T is 5 in each case which is relatively small. Practically, we found that increasing the value of T beyond 5 doesn\u2019t help much. For more discussion on the choice of T, please see Sec. A2 in the appendix.\n2. You are absolutely right that PGD is stronger than FGSM, but it is possible that FGSM-AT achieves relatively high PGD accuracy on CIFAR10 (e.g. SAT accuracy in table 1(c) from Song et.al. 2019). We believe the reason for this is that label leaking (Kurakin et.al. 2017), an issue standard FGSM-AT suffers from, does not happen in our case due to the low capacity of ResNet18. When label leaking happens, as in other cases in our paper, we see FGSM accuracy that is higher than No-Atk accuracy as well as low PGD accuracy (almost zero).\n3. Our purification approach is purely online - the purification is performed at test-time. Also in contrast to Defense-GAN and Pixel-Defend which train the main model and the purifier separately, SOAP relies on the encoder f to learn a good auxiliary device h. It is not possible to use a different encoder with our auxiliary device h, since h operates with respect to the internal representation space (output of f), therefore the two need to be trained together. Thus a trained auxiliary device h cannot be simply composed onto a different network. We agree that it would be interesting to see how SOAP can be combined with AT for improved robustness in future work.\n4. The whole field of self-supervised learning is growing fast but a theoretical foundation is still lacking, which makes such selection of appropriate self-supervised tasks empirical at this point. Nevertheless, we discuss some of the principles of selecting appropriate self-supervised tasks in Sec. 3.4. Our main suggestion is that the selection self-supervision should be differentiable (wrt inputs), comprehensive and efficient, and appropriate to the dataset. For example, RP will not work with MNIST since digits such as  0, 1, 6, 8 and 9 can be invariant/interchangeable to 0 and 180 degrees.\n5. We have included experiments on CIFAR100 and we out-perform AT on both ResNet and WideResNet architectures, and on both l_2 and l_inf attacks."}, "signatures": ["ICLR.cc/2021/Conference/Paper2704/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2704/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Online Adversarial Purification based on Self-supervised Learning", "authorids": ["~Changhao_Shi1", "~Chester_Holtz1", "~Gal_Mishne1"], "authors": ["Changhao Shi", "Chester Holtz", "Gal Mishne"], "keywords": ["Adversarial Robustness", "Self-Supervised Learning"], "abstract": "Deep neural networks are known to be vulnerable to adversarial examples, where a perturbation in the input space leads to an amplified shift in the latent network representation. In this paper, we combine canonical supervised learning with self-supervised representation learning, and present Self-supervised Online Adversarial Purification (SOAP), a novel defense strategy that uses a self-supervised loss to purify adversarial examples at test-time. Our approach leverages the label-independent nature of self-supervised signals and counters the adversarial perturbation with respect to the self-supervised tasks. SOAP yields competitive robust accuracy against state-of-the-art adversarial training and purification methods, with considerably less training complexity. In addition, our approach is robust even when adversaries are given the knowledge of the purification defense strategy. To the best of our knowledge, our paper is the first that generalizes the idea of using self-supervised signals to perform online test-time purification.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shi|online_adversarial_purification_based_on_selfsupervised_learning", "pdf": "/pdf/c72b2431912d9433eb862f7ff1c59d589191b939.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nshi2021online,\ntitle={Online Adversarial Purification based on Self-supervised Learning},\nauthor={Changhao Shi and Chester Holtz and Gal Mishne},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=_i3ASPp12WS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "_i3ASPp12WS", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2704/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2704/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2704/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2704/Authors|ICLR.cc/2021/Conference/Paper2704/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2704/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923845334, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2704/-/Official_Comment"}}}, {"id": "Qp7GOEWxpcu", "original": null, "number": 11, "cdate": 1606127408039, "ddate": null, "tcdate": 1606127408039, "tmdate": 1606127408039, "tddate": null, "forum": "_i3ASPp12WS", "replyto": "dnmwi7rSNqz", "invitation": "ICLR.cc/2021/Conference/Paper2704/-/Official_Comment", "content": {"title": "Official Reply to Reviewer 3", "comment": "We thank you for your positive feedback on our paper. We would like to address your remaining concerns accordingly:\n\n1. We have included results for CIFAR100 in Table 3, demonstrating that we out-perform adversarial training on both l_2 and l_inf attacks and for two architectures (ResNet and WideResNet).\n2. We have added a visualization of adversarial and purified images for CIFAR10. Note that in figure 2, we leverage the autoencoder in the DR auxiliary task to visualize how the network \u2018sees\u2019 the adversarial examples and purified images. There is no equivalent possibility for the RP and LC auxiliary tasks. Therefore we include the adversarial images and their purified versions. \n3. We have added experimental results for CIFAR100, where LC mostly performs better than RP. An explanation for this is that LC is a more difficult task, and therefore leads to a \u201cricher\u201d representation space.\nIn addition, note that there are practical reasons why some self-supervised tasks cannot work on both datasets. For example, RP will not work on MNIST because digits such as  0, 1, 6, 8 and 9 can be invariant/interchangeable to 0 and 180 degrees of rotation. We have included a discussion on this in the paper.\nWe also want to stress that the scope of self-supervised tasks is not limited to the three we discussed. We encourage readers to explore more possibilities and choose one that is the most appropriate for the dataset.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2704/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2704/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Online Adversarial Purification based on Self-supervised Learning", "authorids": ["~Changhao_Shi1", "~Chester_Holtz1", "~Gal_Mishne1"], "authors": ["Changhao Shi", "Chester Holtz", "Gal Mishne"], "keywords": ["Adversarial Robustness", "Self-Supervised Learning"], "abstract": "Deep neural networks are known to be vulnerable to adversarial examples, where a perturbation in the input space leads to an amplified shift in the latent network representation. In this paper, we combine canonical supervised learning with self-supervised representation learning, and present Self-supervised Online Adversarial Purification (SOAP), a novel defense strategy that uses a self-supervised loss to purify adversarial examples at test-time. Our approach leverages the label-independent nature of self-supervised signals and counters the adversarial perturbation with respect to the self-supervised tasks. SOAP yields competitive robust accuracy against state-of-the-art adversarial training and purification methods, with considerably less training complexity. In addition, our approach is robust even when adversaries are given the knowledge of the purification defense strategy. To the best of our knowledge, our paper is the first that generalizes the idea of using self-supervised signals to perform online test-time purification.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shi|online_adversarial_purification_based_on_selfsupervised_learning", "pdf": "/pdf/c72b2431912d9433eb862f7ff1c59d589191b939.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nshi2021online,\ntitle={Online Adversarial Purification based on Self-supervised Learning},\nauthor={Changhao Shi and Chester Holtz and Gal Mishne},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=_i3ASPp12WS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "_i3ASPp12WS", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2704/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2704/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2704/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2704/Authors|ICLR.cc/2021/Conference/Paper2704/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2704/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923845334, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2704/-/Official_Comment"}}}, {"id": "i_BKiUeFH5J", "original": null, "number": 9, "cdate": 1606124630305, "ddate": null, "tcdate": 1606124630305, "tmdate": 1606124630305, "tddate": null, "forum": "_i3ASPp12WS", "replyto": "_i3ASPp12WS", "invitation": "ICLR.cc/2021/Conference/Paper2704/-/Official_Comment", "content": {"title": "General Reply", "comment": "We appreciate the detailed comments and recommendations provided by all reviewers. We have uploaded a new version of the paper that addresses issues brought up by all reviewers. First, we have added a new table presenting the results of applying SOAP to CIFAR-100. Notably, SOAP still achieves superior performance, regardless of the capacity of the network. In addition to Table 3, we have also provided further clarification in section 4.1 regarding the motivation for evaluating SOAP on white-box attacks, and additional interpretation of these results. We have also made clarifications regarding the choice of auxiliary tasks and scalability of the purification procedure, addressing points raised by Reviewer 2 and Reviewer 4."}, "signatures": ["ICLR.cc/2021/Conference/Paper2704/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2704/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Online Adversarial Purification based on Self-supervised Learning", "authorids": ["~Changhao_Shi1", "~Chester_Holtz1", "~Gal_Mishne1"], "authors": ["Changhao Shi", "Chester Holtz", "Gal Mishne"], "keywords": ["Adversarial Robustness", "Self-Supervised Learning"], "abstract": "Deep neural networks are known to be vulnerable to adversarial examples, where a perturbation in the input space leads to an amplified shift in the latent network representation. In this paper, we combine canonical supervised learning with self-supervised representation learning, and present Self-supervised Online Adversarial Purification (SOAP), a novel defense strategy that uses a self-supervised loss to purify adversarial examples at test-time. Our approach leverages the label-independent nature of self-supervised signals and counters the adversarial perturbation with respect to the self-supervised tasks. SOAP yields competitive robust accuracy against state-of-the-art adversarial training and purification methods, with considerably less training complexity. In addition, our approach is robust even when adversaries are given the knowledge of the purification defense strategy. To the best of our knowledge, our paper is the first that generalizes the idea of using self-supervised signals to perform online test-time purification.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shi|online_adversarial_purification_based_on_selfsupervised_learning", "pdf": "/pdf/c72b2431912d9433eb862f7ff1c59d589191b939.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nshi2021online,\ntitle={Online Adversarial Purification based on Self-supervised Learning},\nauthor={Changhao Shi and Chester Holtz and Gal Mishne},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=_i3ASPp12WS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "_i3ASPp12WS", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2704/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2704/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2704/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2704/Authors|ICLR.cc/2021/Conference/Paper2704/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2704/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923845334, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2704/-/Official_Comment"}}}, {"id": "DeE1PVpHFZX", "original": null, "number": 7, "cdate": 1605580158771, "ddate": null, "tcdate": 1605580158771, "tmdate": 1605580158771, "tddate": null, "forum": "_i3ASPp12WS", "replyto": "ciMKVVq1cFE", "invitation": "ICLR.cc/2021/Conference/Paper2704/-/Official_Comment", "content": {"title": "Thank You and Some Clarification", "comment": "We thank Nicholas for your attention and valuable comments on our paper, and we also want to thank Reviewer 2 for their clarification. (We will reply to all official reviewers in time along with the revised version of our paper. We did want to address Nicholas\u2019s concerns during the public discussion period.)\n\nReviewer 2 is correct about Section 4.1--we did not just claim SOAP is robust to auxiliary-aware / adaptive attacks but also proposed a targeted attack based on the full training model, including the auxiliary. This attack generates adversarial images from gradient ascent on the joint classification+auxiliary loss. The results are shown in Fig.4 for all three self-supervised auxiliary tasks. \n\nWe also wanted to clarify that we did not design SOAP with the explicit purpose of \u2018obfuscating gradients\u2019, but this was a side comment with respect to the auxiliary-aware attack we presented, i.e., that it is hard for adversarial gradients to simultaneously increase cross entropy loss and maintain a small auxiliary loss. \n\nTo reviewer 2, we would like to clarify (and will clarify this in the text of the submitted revision as well) that our defense is not based on knowledge of the attack. Rather we need to constrain the form of our defense (perturbation used to purify the image) in order to solve the optimization problem, so that the purified image resembles the natural input image, i.e. the perturbation is not unbounded. Alternative formulations to constrain the purification of Eq. 4 are beyond the scope of this work but an interesting topic for future work. Therefore, while our defense does parallel the l_inf bounded attack in its formulation, note that in our experiments we demonstrate it performs well on both l_inf bounded attack (FGSM, PGD) and l_2 bounded attack (CW, DF). (We will clarify and emphasize in the text that CW and DF reported in our results are l_2 bounded attacks). Therefore, although our purification is constrained based on an l_inf norm in Eq. 4, we show that SOAP can defend at the very least against l_inf and l_2 attacks. You could find some discussion on this topic in Sec. 3.3.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2704/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2704/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Online Adversarial Purification based on Self-supervised Learning", "authorids": ["~Changhao_Shi1", "~Chester_Holtz1", "~Gal_Mishne1"], "authors": ["Changhao Shi", "Chester Holtz", "Gal Mishne"], "keywords": ["Adversarial Robustness", "Self-Supervised Learning"], "abstract": "Deep neural networks are known to be vulnerable to adversarial examples, where a perturbation in the input space leads to an amplified shift in the latent network representation. In this paper, we combine canonical supervised learning with self-supervised representation learning, and present Self-supervised Online Adversarial Purification (SOAP), a novel defense strategy that uses a self-supervised loss to purify adversarial examples at test-time. Our approach leverages the label-independent nature of self-supervised signals and counters the adversarial perturbation with respect to the self-supervised tasks. SOAP yields competitive robust accuracy against state-of-the-art adversarial training and purification methods, with considerably less training complexity. In addition, our approach is robust even when adversaries are given the knowledge of the purification defense strategy. To the best of our knowledge, our paper is the first that generalizes the idea of using self-supervised signals to perform online test-time purification.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shi|online_adversarial_purification_based_on_selfsupervised_learning", "pdf": "/pdf/c72b2431912d9433eb862f7ff1c59d589191b939.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nshi2021online,\ntitle={Online Adversarial Purification based on Self-supervised Learning},\nauthor={Changhao Shi and Chester Holtz and Gal Mishne},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=_i3ASPp12WS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "_i3ASPp12WS", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2704/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2704/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2704/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2704/Authors|ICLR.cc/2021/Conference/Paper2704/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2704/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923845334, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2704/-/Official_Comment"}}}, {"id": "r7-cJNjPQsN", "original": null, "number": 6, "cdate": 1605198047447, "ddate": null, "tcdate": 1605198047447, "tmdate": 1605198098135, "tddate": null, "forum": "_i3ASPp12WS", "replyto": "REL_rbpTU78", "invitation": "ICLR.cc/2021/Conference/Paper2704/-/Official_Comment", "content": {"title": "A difference of this work with previous \"online\" defenses", "comment": "Agreed. Indeed that's the key question.\n\nPerhaps another useful and interesting point of this work is that the way it does \"online defense\", different from the previous work that I know of, is actually, again, **adversarial training**. This is encoded in the equations (3) and (4) in the draft. So at least intuitively, as long as ${\\cal L}_{\\text{aux}}$ is correlated with the main objective, this adversarial training works. And this is indeed why their equation (9) makes sense to me as a an adaptive attack (where they tried to explicitly break the correlation).\n\nOn the flip side, this adversarial training also triggers one of my main questions in my review, quote: *\"Following (1), what worries me more is that online-purification still needs to be aware of the attack type. Namely if one looks into equation (4), the objective has encoded norm-based attacks within it. This makes the results less interesting.\"*, and this actually leads to inferior adversarial robustness and increased inference cost (so the reduced training cost does not seem to be a good tradeoff)."}, "signatures": ["ICLR.cc/2021/Conference/Paper2704/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2704/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Online Adversarial Purification based on Self-supervised Learning", "authorids": ["~Changhao_Shi1", "~Chester_Holtz1", "~Gal_Mishne1"], "authors": ["Changhao Shi", "Chester Holtz", "Gal Mishne"], "keywords": ["Adversarial Robustness", "Self-Supervised Learning"], "abstract": "Deep neural networks are known to be vulnerable to adversarial examples, where a perturbation in the input space leads to an amplified shift in the latent network representation. In this paper, we combine canonical supervised learning with self-supervised representation learning, and present Self-supervised Online Adversarial Purification (SOAP), a novel defense strategy that uses a self-supervised loss to purify adversarial examples at test-time. Our approach leverages the label-independent nature of self-supervised signals and counters the adversarial perturbation with respect to the self-supervised tasks. SOAP yields competitive robust accuracy against state-of-the-art adversarial training and purification methods, with considerably less training complexity. In addition, our approach is robust even when adversaries are given the knowledge of the purification defense strategy. To the best of our knowledge, our paper is the first that generalizes the idea of using self-supervised signals to perform online test-time purification.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shi|online_adversarial_purification_based_on_selfsupervised_learning", "pdf": "/pdf/c72b2431912d9433eb862f7ff1c59d589191b939.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nshi2021online,\ntitle={Online Adversarial Purification based on Self-supervised Learning},\nauthor={Changhao Shi and Chester Holtz and Gal Mishne},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=_i3ASPp12WS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "_i3ASPp12WS", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2704/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2704/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2704/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2704/Authors|ICLR.cc/2021/Conference/Paper2704/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2704/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923845334, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2704/-/Official_Comment"}}}, {"id": "REL_rbpTU78", "original": null, "number": 3, "cdate": 1605166011011, "ddate": null, "tcdate": 1605166011011, "tmdate": 1605166011011, "tddate": null, "forum": "_i3ASPp12WS", "replyto": "BDuuzIKNsOW", "invitation": "ICLR.cc/2021/Conference/Paper2704/-/Public_Comment", "content": {"title": "That may very well be true.", "comment": "Agreed---It may very well be the case that an adversary can't benefit from this additional information. But if this turned out to be the case, it would be the first time I'm aware of this happening in this field. But every other defense that's made similar claims has shortly been broken by an adversary who has used this extra information. \n\nSo the key question then is as you say: does the paper present a convincing argument that adversaries will fail at this task in the future, given knowledge of the defense?"}, "signatures": ["~Nicholas_Carlini1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "~Nicholas_Carlini1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Online Adversarial Purification based on Self-supervised Learning", "authorids": ["~Changhao_Shi1", "~Chester_Holtz1", "~Gal_Mishne1"], "authors": ["Changhao Shi", "Chester Holtz", "Gal Mishne"], "keywords": ["Adversarial Robustness", "Self-Supervised Learning"], "abstract": "Deep neural networks are known to be vulnerable to adversarial examples, where a perturbation in the input space leads to an amplified shift in the latent network representation. In this paper, we combine canonical supervised learning with self-supervised representation learning, and present Self-supervised Online Adversarial Purification (SOAP), a novel defense strategy that uses a self-supervised loss to purify adversarial examples at test-time. Our approach leverages the label-independent nature of self-supervised signals and counters the adversarial perturbation with respect to the self-supervised tasks. SOAP yields competitive robust accuracy against state-of-the-art adversarial training and purification methods, with considerably less training complexity. In addition, our approach is robust even when adversaries are given the knowledge of the purification defense strategy. To the best of our knowledge, our paper is the first that generalizes the idea of using self-supervised signals to perform online test-time purification.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shi|online_adversarial_purification_based_on_selfsupervised_learning", "pdf": "/pdf/c72b2431912d9433eb862f7ff1c59d589191b939.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nshi2021online,\ntitle={Online Adversarial Purification based on Self-supervised Learning},\nauthor={Changhao Shi and Chester Holtz and Gal Mishne},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=_i3ASPp12WS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "_i3ASPp12WS", "readers": {"description": "User groups that will be able to read this comment.", "values": ["everyone"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed."}}, "expdate": 1605630600000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2704/Authors", "ICLR.cc/2021/Conference/Paper2704/Reviewers", "ICLR.cc/2021/Conference/Paper2704/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1605024960519, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2704/-/Public_Comment"}}}, {"id": "BDuuzIKNsOW", "original": null, "number": 5, "cdate": 1605122820603, "ddate": null, "tcdate": 1605122820603, "tmdate": 1605122820603, "tddate": null, "forum": "_i3ASPp12WS", "replyto": "sC4rdbszOW", "invitation": "ICLR.cc/2021/Conference/Paper2704/-/Official_Comment", "content": {"title": "Thanks, my understanding is slightly different", "comment": "I see your point. To me, for that paragraph, the authors indeed formulated a new objective that takes into consideration the knowledge of the auxiliary part, which is their equation (9). And they also mentioned in the beginning of the paragraph that,\nquote: *\"In this section, we introduce a more \u2018complete\u2019 whitebox adversary which is aware of the purification method, and show that it is not straightforward to attack SOAP even with the knowledge of the auxiliary task used for purification.\"*\n\nFrom there, the authors found that there is still robustness w.r.t. this objective, so they conclude that, quote *\"an adversary cannot benefit from the knowledge of the defense in a **straightforward** way\"*.\n\nOf course, one can question whether this objective is sufficient for such a purpose, which is where I think you are heading at."}, "signatures": ["ICLR.cc/2021/Conference/Paper2704/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2704/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Online Adversarial Purification based on Self-supervised Learning", "authorids": ["~Changhao_Shi1", "~Chester_Holtz1", "~Gal_Mishne1"], "authors": ["Changhao Shi", "Chester Holtz", "Gal Mishne"], "keywords": ["Adversarial Robustness", "Self-Supervised Learning"], "abstract": "Deep neural networks are known to be vulnerable to adversarial examples, where a perturbation in the input space leads to an amplified shift in the latent network representation. In this paper, we combine canonical supervised learning with self-supervised representation learning, and present Self-supervised Online Adversarial Purification (SOAP), a novel defense strategy that uses a self-supervised loss to purify adversarial examples at test-time. Our approach leverages the label-independent nature of self-supervised signals and counters the adversarial perturbation with respect to the self-supervised tasks. SOAP yields competitive robust accuracy against state-of-the-art adversarial training and purification methods, with considerably less training complexity. In addition, our approach is robust even when adversaries are given the knowledge of the purification defense strategy. To the best of our knowledge, our paper is the first that generalizes the idea of using self-supervised signals to perform online test-time purification.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shi|online_adversarial_purification_based_on_selfsupervised_learning", "pdf": "/pdf/c72b2431912d9433eb862f7ff1c59d589191b939.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nshi2021online,\ntitle={Online Adversarial Purification based on Self-supervised Learning},\nauthor={Changhao Shi and Chester Holtz and Gal Mishne},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=_i3ASPp12WS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "_i3ASPp12WS", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2704/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2704/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2704/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2704/Authors|ICLR.cc/2021/Conference/Paper2704/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2704/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923845334, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2704/-/Official_Comment"}}}, {"id": "sC4rdbszOW", "original": null, "number": 2, "cdate": 1605119546479, "ddate": null, "tcdate": 1605119546479, "tmdate": 1605119546479, "tddate": null, "forum": "_i3ASPp12WS", "replyto": "_WfLpX7KSXe", "invitation": "ICLR.cc/2021/Conference/Paper2704/-/Public_Comment", "content": {"title": "Their consideration is saying adaptive attacks can't be done", "comment": "It's under the section titled \"Auxiliary-aware attacks\" that the paper says \"an adversary cannot benefit from the knowledge\" and \"the auxiliary component of the adapted attacks obfuscates the cross entropy gradient\". So yes, while they have a section that tries to talk about it, the discussion is saying it can't be done."}, "signatures": ["~Nicholas_Carlini1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "~Nicholas_Carlini1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Online Adversarial Purification based on Self-supervised Learning", "authorids": ["~Changhao_Shi1", "~Chester_Holtz1", "~Gal_Mishne1"], "authors": ["Changhao Shi", "Chester Holtz", "Gal Mishne"], "keywords": ["Adversarial Robustness", "Self-Supervised Learning"], "abstract": "Deep neural networks are known to be vulnerable to adversarial examples, where a perturbation in the input space leads to an amplified shift in the latent network representation. In this paper, we combine canonical supervised learning with self-supervised representation learning, and present Self-supervised Online Adversarial Purification (SOAP), a novel defense strategy that uses a self-supervised loss to purify adversarial examples at test-time. Our approach leverages the label-independent nature of self-supervised signals and counters the adversarial perturbation with respect to the self-supervised tasks. SOAP yields competitive robust accuracy against state-of-the-art adversarial training and purification methods, with considerably less training complexity. In addition, our approach is robust even when adversaries are given the knowledge of the purification defense strategy. To the best of our knowledge, our paper is the first that generalizes the idea of using self-supervised signals to perform online test-time purification.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shi|online_adversarial_purification_based_on_selfsupervised_learning", "pdf": "/pdf/c72b2431912d9433eb862f7ff1c59d589191b939.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nshi2021online,\ntitle={Online Adversarial Purification based on Self-supervised Learning},\nauthor={Changhao Shi and Chester Holtz and Gal Mishne},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=_i3ASPp12WS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "_i3ASPp12WS", "readers": {"description": "User groups that will be able to read this comment.", "values": ["everyone"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed."}}, "expdate": 1605630600000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2704/Authors", "ICLR.cc/2021/Conference/Paper2704/Reviewers", "ICLR.cc/2021/Conference/Paper2704/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1605024960519, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2704/-/Public_Comment"}}}, {"id": "_WfLpX7KSXe", "original": null, "number": 4, "cdate": 1605111094552, "ddate": null, "tcdate": 1605111094552, "tmdate": 1605111094552, "tddate": null, "forum": "_i3ASPp12WS", "replyto": "ciMKVVq1cFE", "invitation": "ICLR.cc/2021/Conference/Paper2704/-/Official_Comment", "content": {"title": "Thanks Nicholas, they have considered auxiliary-aware attacks", "comment": "They have indeed considered adaptive attacks in the paper, see the section White-box attacks, paragraph \"Auxiliary-aware attacks\".\nIt might be useful if you can share your thoughts on that, too (since you are already here)."}, "signatures": ["ICLR.cc/2021/Conference/Paper2704/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2704/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Online Adversarial Purification based on Self-supervised Learning", "authorids": ["~Changhao_Shi1", "~Chester_Holtz1", "~Gal_Mishne1"], "authors": ["Changhao Shi", "Chester Holtz", "Gal Mishne"], "keywords": ["Adversarial Robustness", "Self-Supervised Learning"], "abstract": "Deep neural networks are known to be vulnerable to adversarial examples, where a perturbation in the input space leads to an amplified shift in the latent network representation. In this paper, we combine canonical supervised learning with self-supervised representation learning, and present Self-supervised Online Adversarial Purification (SOAP), a novel defense strategy that uses a self-supervised loss to purify adversarial examples at test-time. Our approach leverages the label-independent nature of self-supervised signals and counters the adversarial perturbation with respect to the self-supervised tasks. SOAP yields competitive robust accuracy against state-of-the-art adversarial training and purification methods, with considerably less training complexity. In addition, our approach is robust even when adversaries are given the knowledge of the purification defense strategy. To the best of our knowledge, our paper is the first that generalizes the idea of using self-supervised signals to perform online test-time purification.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shi|online_adversarial_purification_based_on_selfsupervised_learning", "pdf": "/pdf/c72b2431912d9433eb862f7ff1c59d589191b939.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nshi2021online,\ntitle={Online Adversarial Purification based on Self-supervised Learning},\nauthor={Changhao Shi and Chester Holtz and Gal Mishne},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=_i3ASPp12WS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "_i3ASPp12WS", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2704/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2704/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2704/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2704/Authors|ICLR.cc/2021/Conference/Paper2704/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2704/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923845334, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2704/-/Official_Comment"}}}, {"id": "ciMKVVq1cFE", "original": null, "number": 1, "cdate": 1605041470004, "ddate": null, "tcdate": 1605041470004, "tmdate": 1605041470004, "tddate": null, "forum": "_i3ASPp12WS", "replyto": "_i3ASPp12WS", "invitation": "ICLR.cc/2021/Conference/Paper2704/-/Public_Comment", "content": {"title": "Defense idea built around obfuscating gradients", "comment": "This paper is explicit that \"the reason [the defense is robust] is that the auxiliary component of the adapted attacks obfuscates the cross entropy gradient\". It is well known (Athalye et al. 2018) that intentionally obfuscating gradient information is not a good technique to increase robustness. To the best of my knowledge, all papers that try this approach have been broken.\n\nWhile the paper claims that \"an adversary cannot benefit from the knowledge of the defense in a straightforward way\", Tramer et al. 2020 has shown that in every case an adaptive attacker *can* take advantage of the particular details of a defense. It is necessary to actually demonstrate that adaptive attacks fail: claiming it without justification is insufficient."}, "signatures": ["~Nicholas_Carlini1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "~Nicholas_Carlini1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Online Adversarial Purification based on Self-supervised Learning", "authorids": ["~Changhao_Shi1", "~Chester_Holtz1", "~Gal_Mishne1"], "authors": ["Changhao Shi", "Chester Holtz", "Gal Mishne"], "keywords": ["Adversarial Robustness", "Self-Supervised Learning"], "abstract": "Deep neural networks are known to be vulnerable to adversarial examples, where a perturbation in the input space leads to an amplified shift in the latent network representation. In this paper, we combine canonical supervised learning with self-supervised representation learning, and present Self-supervised Online Adversarial Purification (SOAP), a novel defense strategy that uses a self-supervised loss to purify adversarial examples at test-time. Our approach leverages the label-independent nature of self-supervised signals and counters the adversarial perturbation with respect to the self-supervised tasks. SOAP yields competitive robust accuracy against state-of-the-art adversarial training and purification methods, with considerably less training complexity. In addition, our approach is robust even when adversaries are given the knowledge of the purification defense strategy. To the best of our knowledge, our paper is the first that generalizes the idea of using self-supervised signals to perform online test-time purification.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shi|online_adversarial_purification_based_on_selfsupervised_learning", "pdf": "/pdf/c72b2431912d9433eb862f7ff1c59d589191b939.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nshi2021online,\ntitle={Online Adversarial Purification based on Self-supervised Learning},\nauthor={Changhao Shi and Chester Holtz and Gal Mishne},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=_i3ASPp12WS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "_i3ASPp12WS", "readers": {"description": "User groups that will be able to read this comment.", "values": ["everyone"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed."}}, "expdate": 1605630600000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2704/Authors", "ICLR.cc/2021/Conference/Paper2704/Reviewers", "ICLR.cc/2021/Conference/Paper2704/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1605024960519, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2704/-/Public_Comment"}}}, {"id": "dnmwi7rSNqz", "original": null, "number": 2, "cdate": 1603882276013, "ddate": null, "tcdate": 1603882276013, "tmdate": 1605024149940, "tddate": null, "forum": "_i3ASPp12WS", "replyto": "_i3ASPp12WS", "invitation": "ICLR.cc/2021/Conference/Paper2704/-/Official_Review", "content": {"title": "Interesting method and results, I am missing results on larger and more complex datasets.", "review": "Summary: The paper introduces a defence for adversarial attack based on minimising a self-supervised loss on the test examples. Authors work under the assumption that minimising the self-supervised loss would be equivalent to minimising the supervised loss (to which they don't have access at test time). Authors evaluate their method on MNIST and CIFAR.\n\nStrengths:\n- The paper address the important topic of adversarial defence. Given the numerous adversarial attacks that have appeared in the last years and the deployment of novel classification methods into real world tools, I believe the field of adversarial defence is relevant.\n\n- Authors use in a smart way the self-supervised loss which is traditionally used for learning good representation as pretrainining networks. I think self-supervised learning should extend its usage over the traditional framework and this paper is one example of its potential.\n\n- Authors are also able to compute its own budget using the self-supervised loss, which I believe it's additional evidence that the usage of self-supervised learning for adversarial defence is interesting and useful. \n\n- Quantitative results show how the proposed method is competitive with methods.\n\n- Authors also evaluate the effectiveness of the method when faced with an attacker knowing which defence method is using. Although I consider the proposed attack a baseline attack (maybe other alternatives can be used), I believe it's a relevant result. \n\n\nWeaknesses:\n- I am missing evaluation of the method in larger scale datasets, or more natural images dataset. I think for this methods to be applicable and useful, authors should demonstrate its usefulness into real data. \n\n- I am missing some images of the CIFAR dataset similar to Figure 2. I know the supplementary material shows some, but it would be good to include some into the main paper.\n\n- I think authors should at least have one of the self-supervised methods (LC, RP or DR) show performance in both dataset. Given a new dataset, which methods should I select?\n\nConclusion: I believe the paper presents an interesting method with strong experimental results. The paper deserves acceptance as I believe it contains enough evidence to proof the effectiveness of the method. \n", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2704/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2704/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Online Adversarial Purification based on Self-supervised Learning", "authorids": ["~Changhao_Shi1", "~Chester_Holtz1", "~Gal_Mishne1"], "authors": ["Changhao Shi", "Chester Holtz", "Gal Mishne"], "keywords": ["Adversarial Robustness", "Self-Supervised Learning"], "abstract": "Deep neural networks are known to be vulnerable to adversarial examples, where a perturbation in the input space leads to an amplified shift in the latent network representation. In this paper, we combine canonical supervised learning with self-supervised representation learning, and present Self-supervised Online Adversarial Purification (SOAP), a novel defense strategy that uses a self-supervised loss to purify adversarial examples at test-time. Our approach leverages the label-independent nature of self-supervised signals and counters the adversarial perturbation with respect to the self-supervised tasks. SOAP yields competitive robust accuracy against state-of-the-art adversarial training and purification methods, with considerably less training complexity. In addition, our approach is robust even when adversaries are given the knowledge of the purification defense strategy. To the best of our knowledge, our paper is the first that generalizes the idea of using self-supervised signals to perform online test-time purification.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shi|online_adversarial_purification_based_on_selfsupervised_learning", "pdf": "/pdf/c72b2431912d9433eb862f7ff1c59d589191b939.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nshi2021online,\ntitle={Online Adversarial Purification based on Self-supervised Learning},\nauthor={Changhao Shi and Chester Holtz and Gal Mishne},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=_i3ASPp12WS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "_i3ASPp12WS", "replyto": "_i3ASPp12WS", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2704/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538090357, "tmdate": 1606915783276, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2704/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2704/-/Official_Review"}}}], "count": 21}