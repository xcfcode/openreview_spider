{"notes": [{"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396428610, "tcdate": 1486396428610, "number": 1, "id": "ryHMnfL_g", "invitation": "ICLR.cc/2017/conference/-/paper201/acceptance", "forum": "ByG8A7cee", "replyto": "ByG8A7cee", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision", "comment": "All of the reviewers point out clarity problems; while these may have been resolved in an updated version, the reviewers have not expressed that the matter is resolved. There are several questions raised about the use of perplexity, both whether the comparison is fair, and whether it is a valid proxy for more standard measures in NLP. The former seems to be more of an issue for this area chair, and the discussion did not convince me that it was adequately resolved."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Reference-Aware Language Models", "abstract": "We propose a general class of language models that treat reference as an explicit stochastic latent variable. This architecture allows models to create mentions of entities and their attributes by accessing external databases (required by, e.g., dialogue generation and recipe generation) and internal state (required by, e.g. language models which are aware of coreference). This facilitates the incorporation of information that can be accessed in predictable locations in databases or dis- course context, even when the targets of the reference may be rare words. Experiments on three tasks show our model variants outperform models based on deterministic attention.", "pdf": "/pdf/4460aa4b621276786054b8f3c3b654b9f487a92a.pdf", "TL;DR": "reference-aware language models", "paperhash": "yang|referenceaware_language_models", "conflicts": ["cs.cmu.edu", "google.com"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Zichao Yang", "Phil Blunsom", "Chris Dyer", "Wang Ling"], "authorids": ["zichaoy@cs.cmu.edu", "pblunsom@google.com", "cdyer@google.com", "lingwang@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396429129, "id": "ICLR.cc/2017/conference/-/paper201/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "ByG8A7cee", "replyto": "ByG8A7cee", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396429129}}}, {"tddate": null, "tmdate": 1484929928744, "tcdate": 1484929912178, "number": 3, "id": "SyxKihJPg", "invitation": "ICLR.cc/2017/conference/-/paper201/official/comment", "forum": "ByG8A7cee", "replyto": "r1I5-XnUe", "signatures": ["ICLR.cc/2017/conference/paper201/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper201/AnonReviewer1"], "content": {"title": "re: re: Review", "comment": "Thank you for the rebuttal.\n\nI appreciate the clarifications and revision you have made of the paper.\n\nHowever, I do not find sufficient grounds to change my scores.\n\nYou stated that the BLEU metric was validated for recipe generation in \"Globally coherent text generation with neural checklist\" by Kiddon et al. I've looked through their paper and indeed the BLEU metric is one of the automated metrics used. However, no study is carried out to correlate it with human evaluation, so I don't understand how you see that as supporting your argument. In fact, on page 8 the authors write \"... neither BLEU nor METEOR is suitable for evaluating generated text in terms of their adherence to the provided goal and the agenda ...\". Furthermore, in their paper a nearest-neighbour baseline outperforms all other models w.r.t. BLEU-4. This is worrisome, since your paper does not include this as a baseline.\n\nYou stated that the difference between your proposed model and the model proposed in \"A Network-based End-to-End Trainable Task-oriented Dialogue System\" by Wen et al., is that your model is trained end-to-end while Wen et al. needs an intermediate neural network to extract the table representation. I think this is mainly due to the difference between your tasks; for Wen et al.'s task, it is not possible to extract the table a priori and therefore it must be inferred online using a neural network trained with labelled data. Note, Wen et al. also use a fixed database (restaurant listings) across dialogues, for which no intermediate neural network is necessary."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Reference-Aware Language Models", "abstract": "We propose a general class of language models that treat reference as an explicit stochastic latent variable. This architecture allows models to create mentions of entities and their attributes by accessing external databases (required by, e.g., dialogue generation and recipe generation) and internal state (required by, e.g. language models which are aware of coreference). This facilitates the incorporation of information that can be accessed in predictable locations in databases or dis- course context, even when the targets of the reference may be rare words. Experiments on three tasks show our model variants outperform models based on deterministic attention.", "pdf": "/pdf/4460aa4b621276786054b8f3c3b654b9f487a92a.pdf", "TL;DR": "reference-aware language models", "paperhash": "yang|referenceaware_language_models", "conflicts": ["cs.cmu.edu", "google.com"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Zichao Yang", "Phil Blunsom", "Chris Dyer", "Wang Ling"], "authorids": ["zichaoy@cs.cmu.edu", "pblunsom@google.com", "cdyer@google.com", "lingwang@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287688047, "id": "ICLR.cc/2017/conference/-/paper201/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "ByG8A7cee", "writers": {"values-regex": "ICLR.cc/2017/conference/paper201/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper201/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper201/reviewers", "ICLR.cc/2017/conference/paper201/areachairs"], "cdate": 1485287688047}}}, {"tddate": null, "tmdate": 1484772596007, "tcdate": 1484693977302, "number": 6, "id": "B1WyfQ3Il", "invitation": "ICLR.cc/2017/conference/-/paper201/public/comment", "forum": "ByG8A7cee", "replyto": "BJBPqNGNg", "signatures": ["~Zichao_Yang1"], "readers": ["everyone"], "writers": ["~Zichao_Yang1"], "content": {"title": "re: Review", "comment": "1) We thank the reviewer for the constructive review and feedback. We have rewritten the paper to make it more readable. Please refer to our new draft for the revision.\n\n2) Coupling neural network with database is an interesting direction and we are the first (to the best of our knowledge) to explore in this direction. There lacks good data sets for this type of research.\n\n3) We want to point out that although the data sets (dialogues and recipes) we used are relatively small, they are new data sets that we build ourselves. For the coreference based language models, there is no standard data set either. Building the data sets is one of our contributions. We have provided the state of art baseline methods (seq2seq with attention and lstm language models) on these newly constructed data set and shown our model performs better than those baselines.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Reference-Aware Language Models", "abstract": "We propose a general class of language models that treat reference as an explicit stochastic latent variable. This architecture allows models to create mentions of entities and their attributes by accessing external databases (required by, e.g., dialogue generation and recipe generation) and internal state (required by, e.g. language models which are aware of coreference). This facilitates the incorporation of information that can be accessed in predictable locations in databases or dis- course context, even when the targets of the reference may be rare words. Experiments on three tasks show our model variants outperform models based on deterministic attention.", "pdf": "/pdf/4460aa4b621276786054b8f3c3b654b9f487a92a.pdf", "TL;DR": "reference-aware language models", "paperhash": "yang|referenceaware_language_models", "conflicts": ["cs.cmu.edu", "google.com"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Zichao Yang", "Phil Blunsom", "Chris Dyer", "Wang Ling"], "authorids": ["zichaoy@cs.cmu.edu", "pblunsom@google.com", "cdyer@google.com", "lingwang@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287688194, "id": "ICLR.cc/2017/conference/-/paper201/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "ByG8A7cee", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper201/reviewers", "ICLR.cc/2017/conference/paper201/areachairs"], "cdate": 1485287688194}}}, {"tddate": null, "tmdate": 1484694127803, "tcdate": 1484694127803, "number": 7, "id": "B1__z7hIl", "invitation": "ICLR.cc/2017/conference/-/paper201/public/comment", "forum": "ByG8A7cee", "replyto": "BJ_SQ64Vg", "signatures": ["~Zichao_Yang1"], "readers": ["everyone"], "writers": ["~Zichao_Yang1"], "content": {"title": "re: Review", "comment": "1) We thank the reviewer for the constructive review and feedback. We have rewritten the paper to make it more readable. Please refer to our new draft for the revision.\n\n2) By latent, we mainly mean the decision of copy is latent, so we marginalize the decision variable in the training process in dialogue and recipe task. For the coreference based language model, because there are so many decisions, marginalize them out is intractable, so we treat everything as given. But we measure the ppl as the product of all probabilities together, which gives us an upper bound on the ppl.\n\n3) The reviewer misunderstood the comparison between ordinary language models and reference-aware language models. The words refereed are not UNK, they actually appear in the context, and exit in the vocabulary. Our main argument is that explicitly considering the reference decision is helpful for language model and this is our main contribution.\n\n4) We propose the  \u201cEntity state update\u201d, because there are multiple mentions for each entity. In coreference resolution, we only care about which entity the next mention refers to. So we need to keep one state for each entity in the coreference resolution phrase. With this part, we can do coreference resolution together with language model. This is the main motivation with entity update process. Although we did not report the coreference resolution result in the paper, the model design enables us to do it and it is one of the future directions to explore.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Reference-Aware Language Models", "abstract": "We propose a general class of language models that treat reference as an explicit stochastic latent variable. This architecture allows models to create mentions of entities and their attributes by accessing external databases (required by, e.g., dialogue generation and recipe generation) and internal state (required by, e.g. language models which are aware of coreference). This facilitates the incorporation of information that can be accessed in predictable locations in databases or dis- course context, even when the targets of the reference may be rare words. Experiments on three tasks show our model variants outperform models based on deterministic attention.", "pdf": "/pdf/4460aa4b621276786054b8f3c3b654b9f487a92a.pdf", "TL;DR": "reference-aware language models", "paperhash": "yang|referenceaware_language_models", "conflicts": ["cs.cmu.edu", "google.com"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Zichao Yang", "Phil Blunsom", "Chris Dyer", "Wang Ling"], "authorids": ["zichaoy@cs.cmu.edu", "pblunsom@google.com", "cdyer@google.com", "lingwang@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287688194, "id": "ICLR.cc/2017/conference/-/paper201/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "ByG8A7cee", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper201/reviewers", "ICLR.cc/2017/conference/paper201/areachairs"], "cdate": 1485287688194}}}, {"tddate": null, "tmdate": 1484694061757, "tcdate": 1484693901949, "number": 5, "id": "r1I5-XnUe", "invitation": "ICLR.cc/2017/conference/-/paper201/public/comment", "forum": "ByG8A7cee", "replyto": "H1KxcSgVg", "signatures": ["~Zichao_Yang1"], "readers": ["everyone"], "writers": ["~Zichao_Yang1"], "content": {"title": "re: Review", "comment": "1) We thank the reviewer for the constructive review and feedback. We have rewritten our paper to make it more readable. Please refer to our new draft for the revision.\n\n2) We want to point out that although the data sets (dialogues and recipes) we used are relatively small, they are new data sets that we build ourselves. For the coreference based language models, there is no standard data set either. Building the data sets is one of our contributions. We don\u2019t agree with the reviewer\u2019s comment that more data would decrease the necessity for model innovation. Moreover, labeled data sets are hard and expensive to get.\n\n3) Human evaluation would be ideal, but setting up the experiments is troublesome. For dialogue, since the table entries are quite rare in the sentences, so other evaluation metrics such as BLEU, METEOR are not feasible for this task. We report both ppl and BLEU for recipe generation, the BLEU score correlates highly with human evaluation for recipe task, according to Kiddon et al., 2016. Coreference based LM is a pure language model, it is hard to define the generation task under this context. \n\n4) For \u201cAttention based decoder\u201d, we use attention mechanism over hidden states of the sentence encoder. We apply the attention only on previous turn of dialogue since it is mostly relevant.\n\n5) We propose the  \u201cEntity state update\u201d, because there are multiple mentions for each entity. In coreference resolution, we only care about which entity the next mention refers to. So we need to keep one state for each entity in the coreference resolution phrase. With this part, we can do coreference resolution together with language model. This is the main motivation with entity update process. Although we did not report the coreference resolution result in the paper, the model design enables us to do it and it is one of the future directions to explore.\n\n6) The work by Wen et al still has the table query part separate from the other neural network model, while our model is truly end-to-end.\n\n7)In coreference LM, $M$ is the number of mentions for each entity.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Reference-Aware Language Models", "abstract": "We propose a general class of language models that treat reference as an explicit stochastic latent variable. This architecture allows models to create mentions of entities and their attributes by accessing external databases (required by, e.g., dialogue generation and recipe generation) and internal state (required by, e.g. language models which are aware of coreference). This facilitates the incorporation of information that can be accessed in predictable locations in databases or dis- course context, even when the targets of the reference may be rare words. Experiments on three tasks show our model variants outperform models based on deterministic attention.", "pdf": "/pdf/4460aa4b621276786054b8f3c3b654b9f487a92a.pdf", "TL;DR": "reference-aware language models", "paperhash": "yang|referenceaware_language_models", "conflicts": ["cs.cmu.edu", "google.com"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Zichao Yang", "Phil Blunsom", "Chris Dyer", "Wang Ling"], "authorids": ["zichaoy@cs.cmu.edu", "pblunsom@google.com", "cdyer@google.com", "lingwang@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287688194, "id": "ICLR.cc/2017/conference/-/paper201/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "ByG8A7cee", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper201/reviewers", "ICLR.cc/2017/conference/paper201/areachairs"], "cdate": 1485287688194}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1484693730419, "tcdate": 1478274633607, "number": 201, "id": "ByG8A7cee", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "ByG8A7cee", "signatures": ["~Zichao_Yang1"], "readers": ["everyone"], "content": {"title": "Reference-Aware Language Models", "abstract": "We propose a general class of language models that treat reference as an explicit stochastic latent variable. This architecture allows models to create mentions of entities and their attributes by accessing external databases (required by, e.g., dialogue generation and recipe generation) and internal state (required by, e.g. language models which are aware of coreference). This facilitates the incorporation of information that can be accessed in predictable locations in databases or dis- course context, even when the targets of the reference may be rare words. Experiments on three tasks show our model variants outperform models based on deterministic attention.", "pdf": "/pdf/4460aa4b621276786054b8f3c3b654b9f487a92a.pdf", "TL;DR": "reference-aware language models", "paperhash": "yang|referenceaware_language_models", "conflicts": ["cs.cmu.edu", "google.com"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Zichao Yang", "Phil Blunsom", "Chris Dyer", "Wang Ling"], "authorids": ["zichaoy@cs.cmu.edu", "pblunsom@google.com", "cdyer@google.com", "lingwang@google.com"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 15, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "tmdate": 1482114242184, "tcdate": 1482113855853, "number": 3, "id": "BJ_SQ64Vg", "invitation": "ICLR.cc/2017/conference/-/paper201/official/review", "forum": "ByG8A7cee", "replyto": "ByG8A7cee", "signatures": ["ICLR.cc/2017/conference/paper201/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper201/AnonReviewer2"], "content": {"title": "", "rating": "6: Marginally above acceptance threshold", "review": "This paper presents a new type of language model that treats entity references as latent variables. The paper is structured as three specialized models for three applications: dialog generation with references to database entries, recipe generation with references to ingredients, and text generation with coreference mentions.\n\nDespite some opaqueness in details that I will discuss later, the paper does a great job making the main idea coming through, which I think is quite interesting and definitely worth pursuing further. But it seems the paper was rushed into the deadline, as there are a few major weaknesses.\n\nThe first major weakness is that the claimed latent variables are hardly latent in the actual empirical evaluation. As clarified by the authors via pre-review QAs, all mentions were assumed to be given to all model variants, and so, it would seem like an over-claim to call these variables as latent when they are in fact treated as observed variables. Is it because the models with latent variables were too difficult to train right?\n\nA related problem is the use of perplexity as an evaluation measure when comparing reference-aware language models to vanilla language models. Essentially the authors are comparing two language models defined over different event space, which is not a fair comparison. Because mentions were assumed to be given for the reference-aware language models, and because of the fact that mention generators are designed similar to a pointer network, the probability scores over mentions will naturally be higher, compared to the regular language model that needs to consider a much bigger vocabulary set. The effect is analogous to comparing language models with aggressive UNK (and a small vocabulary set) to a language models with no UNK (and a much larger vocabulary set).\n\nTo mitigate this problem, the authors need to perform one of the following additional evaluations: either assuming no mention boundaries and marginalizing over all possibilities (treating latent variables as truly latent), or showing other types of evaluation beyond perplexity, for example, BLEU, METEOR, human evaluation etc on the corresponding generation task.\n\nThe other major weakness is writing in terms of technical accuracy and completeness. I found many details opaque and confusing even after QAs. I wonder if the main challenge that hinders the quality of writing has something to do with having three very specialized models in one paper, each having a lot of details to be worked out, which may have not been extremely important for the main story of the paper, but nonetheless not negligible in order to understand what is going on with the paper.    Perhaps the authors can restructure the paper so that the most important details are clearly worked out in the main body of the paper, especially in terms of latent variable handling \u2014 how to make mention detection and conference resolution truly latent, and if and when entity update helps, which in the current version is not elaborated at all, as it is mentioned only very briefly for the third application (coreference resolution) without any empirical comparisons to motivate the update operation.\n\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Reference-Aware Language Models", "abstract": "We propose a general class of language models that treat reference as an explicit stochastic latent variable. This architecture allows models to create mentions of entities and their attributes by accessing external databases (required by, e.g., dialogue generation and recipe generation) and internal state (required by, e.g. language models which are aware of coreference). This facilitates the incorporation of information that can be accessed in predictable locations in databases or dis- course context, even when the targets of the reference may be rare words. Experiments on three tasks show our model variants outperform models based on deterministic attention.", "pdf": "/pdf/4460aa4b621276786054b8f3c3b654b9f487a92a.pdf", "TL;DR": "reference-aware language models", "paperhash": "yang|referenceaware_language_models", "conflicts": ["cs.cmu.edu", "google.com"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Zichao Yang", "Phil Blunsom", "Chris Dyer", "Wang Ling"], "authorids": ["zichaoy@cs.cmu.edu", "pblunsom@google.com", "cdyer@google.com", "lingwang@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512665644, "id": "ICLR.cc/2017/conference/-/paper201/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper201/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper201/AnonReviewer1", "ICLR.cc/2017/conference/paper201/AnonReviewer3", "ICLR.cc/2017/conference/paper201/AnonReviewer2"], "reply": {"forum": "ByG8A7cee", "replyto": "ByG8A7cee", "writers": {"values-regex": "ICLR.cc/2017/conference/paper201/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper201/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512665644}}}, {"tddate": null, "tmdate": 1481947740962, "tcdate": 1481947740962, "number": 2, "id": "BJBPqNGNg", "invitation": "ICLR.cc/2017/conference/-/paper201/official/review", "forum": "ByG8A7cee", "replyto": "ByG8A7cee", "signatures": ["ICLR.cc/2017/conference/paper201/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper201/AnonReviewer3"], "content": {"title": "", "rating": "5: Marginally below acceptance threshold", "review": "This paper explores 3 language modeling applications with an explicit modeling of reference expressions: dialog, receipt generation and coreferences. While these are important tasks for NLP and the authors have done a number of experiments, the paper is limited for a few reasons:\n\n1. This paper is not clearly written and is pretty hard to follow some details. In particular,  there are many obvious math errors, such as missing the marginalization sum in Eq (1), and P(z_{i,v}...) = 1 (should be 0 here) on page 5, pointer switch section.\n\n2. The major novelty seems to be the 2-dimensional attention from the table and the pointer to the 2-D table. These are more of a customization of existing work to a particular task with 2-D tables as a part of the input to seq2seq model with both attentions and pointer networks.\n\n3. The empirical results are not very conclusive yet, limited by either the relatively small data size, or the lack of well-established baseline for some new applications (e.g., the recipe generation task).\n\nOverall, this paper, as it is for now, is more suitable for a workshop rather than for the main conference.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Reference-Aware Language Models", "abstract": "We propose a general class of language models that treat reference as an explicit stochastic latent variable. This architecture allows models to create mentions of entities and their attributes by accessing external databases (required by, e.g., dialogue generation and recipe generation) and internal state (required by, e.g. language models which are aware of coreference). This facilitates the incorporation of information that can be accessed in predictable locations in databases or dis- course context, even when the targets of the reference may be rare words. Experiments on three tasks show our model variants outperform models based on deterministic attention.", "pdf": "/pdf/4460aa4b621276786054b8f3c3b654b9f487a92a.pdf", "TL;DR": "reference-aware language models", "paperhash": "yang|referenceaware_language_models", "conflicts": ["cs.cmu.edu", "google.com"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Zichao Yang", "Phil Blunsom", "Chris Dyer", "Wang Ling"], "authorids": ["zichaoy@cs.cmu.edu", "pblunsom@google.com", "cdyer@google.com", "lingwang@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512665644, "id": "ICLR.cc/2017/conference/-/paper201/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper201/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper201/AnonReviewer1", "ICLR.cc/2017/conference/paper201/AnonReviewer3", "ICLR.cc/2017/conference/paper201/AnonReviewer2"], "reply": {"forum": "ByG8A7cee", "replyto": "ByG8A7cee", "writers": {"values-regex": "ICLR.cc/2017/conference/paper201/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper201/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512665644}}}, {"tddate": null, "tmdate": 1481820952236, "tcdate": 1481820952232, "number": 2, "id": "B1e7jSxVe", "invitation": "ICLR.cc/2017/conference/-/paper201/official/comment", "forum": "ByG8A7cee", "replyto": "SyNV70HXe", "signatures": ["ICLR.cc/2017/conference/paper201/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper201/AnonReviewer1"], "content": {"title": "Re: Thanks for the comment.", "comment": "Having a large validation/test set, won't reduce the bias in the evaluation method (e.g. idiosyncratic aspects/features BLEU focuses on). I agree it should reduce variance (or noise), but my concern was mainly related to the bias."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Reference-Aware Language Models", "abstract": "We propose a general class of language models that treat reference as an explicit stochastic latent variable. This architecture allows models to create mentions of entities and their attributes by accessing external databases (required by, e.g., dialogue generation and recipe generation) and internal state (required by, e.g. language models which are aware of coreference). This facilitates the incorporation of information that can be accessed in predictable locations in databases or dis- course context, even when the targets of the reference may be rare words. Experiments on three tasks show our model variants outperform models based on deterministic attention.", "pdf": "/pdf/4460aa4b621276786054b8f3c3b654b9f487a92a.pdf", "TL;DR": "reference-aware language models", "paperhash": "yang|referenceaware_language_models", "conflicts": ["cs.cmu.edu", "google.com"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Zichao Yang", "Phil Blunsom", "Chris Dyer", "Wang Ling"], "authorids": ["zichaoy@cs.cmu.edu", "pblunsom@google.com", "cdyer@google.com", "lingwang@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287688047, "id": "ICLR.cc/2017/conference/-/paper201/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "ByG8A7cee", "writers": {"values-regex": "ICLR.cc/2017/conference/paper201/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper201/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper201/reviewers", "ICLR.cc/2017/conference/paper201/areachairs"], "cdate": 1485287688047}}}, {"tddate": null, "tmdate": 1481820656601, "tcdate": 1481820656592, "number": 1, "id": "H1KxcSgVg", "invitation": "ICLR.cc/2017/conference/-/paper201/official/review", "forum": "ByG8A7cee", "replyto": "ByG8A7cee", "signatures": ["ICLR.cc/2017/conference/paper201/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper201/AnonReviewer1"], "content": {"title": "Review", "rating": "5: Marginally below acceptance threshold", "review": "This paper introduces pointer-network neural networks, which are applied to referring expressions in three small-scale language modeling tasks: dialogue modeling, recipe modeling and news article modeling. When conditioned on the co-reference chain, the proposed models outperform standard sequence-to-sequence models with attention.\n\nThe proposed models are essentially variants of pointer networks with copy mechanisms (Gulcehre et al., 2016; Gu et al., 2016; Ling et al., 2016), which have been modified to take into account reference chains. As such, the main architectural novelty lies in 1) restricting the pointer mechanism to focus on co-referenced entities, 2) applying pointer mechanism to 2D arrays (tables), and 3) training with supervised alignments. Although useful in practice, these are minor contributions from an architectural perspective.\n\nThe empirical contributions are centred around measuring perplexity on the three language modeling tasks. Measuring perplexity is typical for standard language modeling tasks, but is really an unreliable proxy for dialogue modeling and recipe generation performance. In addition to this, both the dialogue and recipe tasks are tiny compared to standard language modeling tasks. This makes it difficult to evaluate the impact of the dialogue and recipe modeling results. For example, if one was to bootstrap from a larger corpus, it seems likely that a standard sequence-to-sequence model with attention would yield performance comparable to the proposed models (with enough data, the attention mechanism could learn to align referring entities by itself). The language modeling task on news article (Gigaword) seems to yield the most conclusive results. However, the dataset for this task is non-standard and results are provided for only a single baseline. Overall, this limits the conclusions we can draw from the empirical experiments.\n\n\nFinally, the paper itself contains many errors, including mathematical errors, grammatical errors and typos:\n- Eq. (1) is missing a sum over $z_i$.\n- \"into the a decoder LSTM\" -> \"into the decoder LSTM\"\n- \"denoted as his\" -> \"denoted as\"\n- \"Surprising,\" -> \"Surprisingly,\"\n- \"torkens\" -> \"tokens\"\n- \"if follows that the next token\" -> \"the next token\"\n- In the \"COREFERENCE BASED LANGUAGE MODEL\" sub-section, what does $M$ denote?\n- In the sentence: \"The attribute of each column is denoted as $s_c, where $c$ is the c-th attribute\". For these definitions to be make sense, $s_c$ has to be a one-hot vector. If yes, please clarify this in the text.\n- \"the weighted sum is performed\" -> \"the weighted sum is computed\"\n- \"a attribute\" -> \"an attribute\"\n- In the paragraph on Pointer Switch, change $p(z_{i,v} |s_{i,v}) = 1$ -> $p(z_{i,v} |s_{i,v}) = 0$.\n- In the \"Table Pointer\" paragraph, I assume you mean outer-product instead of cross-product? Otherwise, I don't see how the equations add up.\n\n\nOther comments:\n- For the \"Attention based decoder\", is the attention computed using the word embeddings themselves or the hidden states of the sentence encoder? Also, it applied only to the previous turn of the dialogue or to the entire dialogue history? Please clarify this.\n- What's the advantage of using an \"Entity state update\" rule, compared to a pointer network or copy network, which you used in the dialogue and recipe tasks? Please elaborate on this.\n- In the Related Work section, the following sentence is not quite accurate: \"For the task oriented dialogues, most of them embed the seq2seq model in traditional dialogue systems while our model queries the database directly.\". There are task-oriented dialogue models which do query databases during natural language generation. See, for example, \"A Network-based End-to-End Trainable Task-oriented Dialogue System\" by Wen et al.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Reference-Aware Language Models", "abstract": "We propose a general class of language models that treat reference as an explicit stochastic latent variable. This architecture allows models to create mentions of entities and their attributes by accessing external databases (required by, e.g., dialogue generation and recipe generation) and internal state (required by, e.g. language models which are aware of coreference). This facilitates the incorporation of information that can be accessed in predictable locations in databases or dis- course context, even when the targets of the reference may be rare words. Experiments on three tasks show our model variants outperform models based on deterministic attention.", "pdf": "/pdf/4460aa4b621276786054b8f3c3b654b9f487a92a.pdf", "TL;DR": "reference-aware language models", "paperhash": "yang|referenceaware_language_models", "conflicts": ["cs.cmu.edu", "google.com"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Zichao Yang", "Phil Blunsom", "Chris Dyer", "Wang Ling"], "authorids": ["zichaoy@cs.cmu.edu", "pblunsom@google.com", "cdyer@google.com", "lingwang@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512665644, "id": "ICLR.cc/2017/conference/-/paper201/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper201/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper201/AnonReviewer1", "ICLR.cc/2017/conference/paper201/AnonReviewer3", "ICLR.cc/2017/conference/paper201/AnonReviewer2"], "reply": {"forum": "ByG8A7cee", "replyto": "ByG8A7cee", "writers": {"values-regex": "ICLR.cc/2017/conference/paper201/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper201/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512665644}}}, {"tddate": null, "tmdate": 1481335550249, "tcdate": 1481335550244, "number": 4, "id": "BJLWQyt7x", "invitation": "ICLR.cc/2017/conference/-/paper201/public/comment", "forum": "ByG8A7cee", "replyto": "HJ3jgvwXe", "signatures": ["~Zichao_Yang1"], "readers": ["everyone"], "writers": ["~Zichao_Yang1"], "content": {"title": "Re: reference-chain", "comment": "Thanks for your question.\n\nThe reference-chain is obtained by automatical tools."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Reference-Aware Language Models", "abstract": "We propose a general class of language models that treat reference as an explicit stochastic latent variable. This architecture allows models to create mentions of entities and their attributes by accessing external databases (required by, e.g., dialogue generation and recipe generation) and internal state (required by, e.g. language models which are aware of coreference). This facilitates the incorporation of information that can be accessed in predictable locations in databases or dis- course context, even when the targets of the reference may be rare words. Experiments on three tasks show our model variants outperform models based on deterministic attention.", "pdf": "/pdf/4460aa4b621276786054b8f3c3b654b9f487a92a.pdf", "TL;DR": "reference-aware language models", "paperhash": "yang|referenceaware_language_models", "conflicts": ["cs.cmu.edu", "google.com"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Zichao Yang", "Phil Blunsom", "Chris Dyer", "Wang Ling"], "authorids": ["zichaoy@cs.cmu.edu", "pblunsom@google.com", "cdyer@google.com", "lingwang@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287688194, "id": "ICLR.cc/2017/conference/-/paper201/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "ByG8A7cee", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper201/reviewers", "ICLR.cc/2017/conference/paper201/areachairs"], "cdate": 1485287688194}}}, {"tddate": null, "tmdate": 1481236643745, "tcdate": 1481236643739, "number": 3, "id": "HJ3jgvwXe", "invitation": "ICLR.cc/2017/conference/-/paper201/public/comment", "forum": "ByG8A7cee", "replyto": "ByG8A7cee", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "Question about the reference-chain", "comment": "Hi, can you tell me the reference-chain is obtained by manual annotation or by some automatical tools such stanford corenlp ? "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Reference-Aware Language Models", "abstract": "We propose a general class of language models that treat reference as an explicit stochastic latent variable. This architecture allows models to create mentions of entities and their attributes by accessing external databases (required by, e.g., dialogue generation and recipe generation) and internal state (required by, e.g. language models which are aware of coreference). This facilitates the incorporation of information that can be accessed in predictable locations in databases or dis- course context, even when the targets of the reference may be rare words. Experiments on three tasks show our model variants outperform models based on deterministic attention.", "pdf": "/pdf/4460aa4b621276786054b8f3c3b654b9f487a92a.pdf", "TL;DR": "reference-aware language models", "paperhash": "yang|referenceaware_language_models", "conflicts": ["cs.cmu.edu", "google.com"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Zichao Yang", "Phil Blunsom", "Chris Dyer", "Wang Ling"], "authorids": ["zichaoy@cs.cmu.edu", "pblunsom@google.com", "cdyer@google.com", "lingwang@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287688194, "id": "ICLR.cc/2017/conference/-/paper201/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "ByG8A7cee", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper201/reviewers", "ICLR.cc/2017/conference/paper201/areachairs"], "cdate": 1485287688194}}}, {"tddate": null, "tmdate": 1481134892105, "tcdate": 1481134892100, "number": 2, "id": "SyNV70HXe", "invitation": "ICLR.cc/2017/conference/-/paper201/public/comment", "forum": "ByG8A7cee", "replyto": "ryGAtQAMe", "signatures": ["~Zichao_Yang1"], "readers": ["everyone"], "writers": ["~Zichao_Yang1"], "content": {"title": "Re: Evaluation", "comment": "Thanks for the comment.\n\nAlthough there can be some noise in BLEU, we think there is a large enough sample size in the val/test set to reduce the noise. Our main goal is to develop methods that can address references to existing structures."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Reference-Aware Language Models", "abstract": "We propose a general class of language models that treat reference as an explicit stochastic latent variable. This architecture allows models to create mentions of entities and their attributes by accessing external databases (required by, e.g., dialogue generation and recipe generation) and internal state (required by, e.g. language models which are aware of coreference). This facilitates the incorporation of information that can be accessed in predictable locations in databases or dis- course context, even when the targets of the reference may be rare words. Experiments on three tasks show our model variants outperform models based on deterministic attention.", "pdf": "/pdf/4460aa4b621276786054b8f3c3b654b9f487a92a.pdf", "TL;DR": "reference-aware language models", "paperhash": "yang|referenceaware_language_models", "conflicts": ["cs.cmu.edu", "google.com"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Zichao Yang", "Phil Blunsom", "Chris Dyer", "Wang Ling"], "authorids": ["zichaoy@cs.cmu.edu", "pblunsom@google.com", "cdyer@google.com", "lingwang@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287688194, "id": "ICLR.cc/2017/conference/-/paper201/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "ByG8A7cee", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper201/reviewers", "ICLR.cc/2017/conference/paper201/areachairs"], "cdate": 1485287688194}}}, {"tddate": null, "tmdate": 1480976411042, "tcdate": 1480976411037, "number": 1, "id": "rJ7QOPQ7e", "invitation": "ICLR.cc/2017/conference/-/paper201/public/comment", "forum": "ByG8A7cee", "replyto": "rJJvmfRfl", "signatures": ["~Zichao_Yang1"], "readers": ["everyone"], "writers": ["~Zichao_Yang1"], "content": {"title": "mention detections etc", "comment": "Thanks for your question.\n\n1) The mentions are given in the model. But we time the probability altogether to get an upper bound on perplexity. An ideal solution is to marginalize them out in the decoding/test process.\n2) Yes, we provide the co-reference chains for training and testing\n3) We used an intuitive way to update the entity state and it performs well, so we didn't experiment other ways to update the entity state. There are other more sophisticated ways, like using a LSTM to encode each entity chain. It's harder to implement."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Reference-Aware Language Models", "abstract": "We propose a general class of language models that treat reference as an explicit stochastic latent variable. This architecture allows models to create mentions of entities and their attributes by accessing external databases (required by, e.g., dialogue generation and recipe generation) and internal state (required by, e.g. language models which are aware of coreference). This facilitates the incorporation of information that can be accessed in predictable locations in databases or dis- course context, even when the targets of the reference may be rare words. Experiments on three tasks show our model variants outperform models based on deterministic attention.", "pdf": "/pdf/4460aa4b621276786054b8f3c3b654b9f487a92a.pdf", "TL;DR": "reference-aware language models", "paperhash": "yang|referenceaware_language_models", "conflicts": ["cs.cmu.edu", "google.com"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Zichao Yang", "Phil Blunsom", "Chris Dyer", "Wang Ling"], "authorids": ["zichaoy@cs.cmu.edu", "pblunsom@google.com", "cdyer@google.com", "lingwang@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287688194, "id": "ICLR.cc/2017/conference/-/paper201/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "ByG8A7cee", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper201/reviewers", "ICLR.cc/2017/conference/paper201/areachairs"], "cdate": 1485287688194}}}, {"tddate": null, "tmdate": 1480632777720, "tcdate": 1480632777714, "number": 2, "id": "ryGAtQAMe", "invitation": "ICLR.cc/2017/conference/-/paper201/pre-review/question", "forum": "ByG8A7cee", "replyto": "ByG8A7cee", "signatures": ["ICLR.cc/2017/conference/paper201/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper201/AnonReviewer1"], "content": {"title": "Evaluation", "question": "All the quantitative results are given in terms of perplexity and BLEU scores. It would have been more convincing to have some form of human evaluation.\n\nAlso, are there any empirical results showing that the BLEU metric can be applied to recipe modeling or similar tasks? After all, it is known that BLEU is very sensitive to pronoun overlap, which could be negatively correlated with outputting correct reference words."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Reference-Aware Language Models", "abstract": "We propose a general class of language models that treat reference as an explicit stochastic latent variable. This architecture allows models to create mentions of entities and their attributes by accessing external databases (required by, e.g., dialogue generation and recipe generation) and internal state (required by, e.g. language models which are aware of coreference). This facilitates the incorporation of information that can be accessed in predictable locations in databases or dis- course context, even when the targets of the reference may be rare words. Experiments on three tasks show our model variants outperform models based on deterministic attention.", "pdf": "/pdf/4460aa4b621276786054b8f3c3b654b9f487a92a.pdf", "TL;DR": "reference-aware language models", "paperhash": "yang|referenceaware_language_models", "conflicts": ["cs.cmu.edu", "google.com"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Zichao Yang", "Phil Blunsom", "Chris Dyer", "Wang Ling"], "authorids": ["zichaoy@cs.cmu.edu", "pblunsom@google.com", "cdyer@google.com", "lingwang@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959410060, "id": "ICLR.cc/2017/conference/-/paper201/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper201/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper201/AnonReviewer2", "ICLR.cc/2017/conference/paper201/AnonReviewer1"], "reply": {"forum": "ByG8A7cee", "replyto": "ByG8A7cee", "writers": {"values-regex": "ICLR.cc/2017/conference/paper201/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper201/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959410060}}}, {"tddate": null, "tmdate": 1480627031282, "tcdate": 1480627031278, "number": 1, "id": "rJJvmfRfl", "invitation": "ICLR.cc/2017/conference/-/paper201/pre-review/question", "forum": "ByG8A7cee", "replyto": "ByG8A7cee", "signatures": ["ICLR.cc/2017/conference/paper201/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper201/AnonReviewer2"], "content": {"title": "mention detection, coref chains, and entity state update", "question": "Hi, I have a few (mostly clarification) questions:\n\n(1) Does any of the models presented in Table 4, 5, 6 assume that the mention boundaries are given to the model during test?\n\n(2) Do you provide gold co-reference chains for training for the coref-based LM? \n\n(3) Do you provide gold co-reference chains for testing? \n\n(4) Also, can you comment on how much \"entity state update\" in section 2 helps improve the performance for the coref-based LM? \n\nThanks."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Reference-Aware Language Models", "abstract": "We propose a general class of language models that treat reference as an explicit stochastic latent variable. This architecture allows models to create mentions of entities and their attributes by accessing external databases (required by, e.g., dialogue generation and recipe generation) and internal state (required by, e.g. language models which are aware of coreference). This facilitates the incorporation of information that can be accessed in predictable locations in databases or dis- course context, even when the targets of the reference may be rare words. Experiments on three tasks show our model variants outperform models based on deterministic attention.", "pdf": "/pdf/4460aa4b621276786054b8f3c3b654b9f487a92a.pdf", "TL;DR": "reference-aware language models", "paperhash": "yang|referenceaware_language_models", "conflicts": ["cs.cmu.edu", "google.com"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Zichao Yang", "Phil Blunsom", "Chris Dyer", "Wang Ling"], "authorids": ["zichaoy@cs.cmu.edu", "pblunsom@google.com", "cdyer@google.com", "lingwang@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959410060, "id": "ICLR.cc/2017/conference/-/paper201/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper201/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper201/AnonReviewer2", "ICLR.cc/2017/conference/paper201/AnonReviewer1"], "reply": {"forum": "ByG8A7cee", "replyto": "ByG8A7cee", "writers": {"values-regex": "ICLR.cc/2017/conference/paper201/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper201/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959410060}}}], "count": 16}