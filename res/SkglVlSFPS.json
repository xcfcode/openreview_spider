{"notes": [{"id": "SkglVlSFPS", "original": "BJlLTrxYDB", "number": 2236, "cdate": 1569439784454, "ddate": null, "tcdate": 1569439784454, "tmdate": 1577168230491, "tddate": null, "forum": "SkglVlSFPS", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["pmilos@mimuw.edu.pl", "lukasz.kucinski@gmail.com", "konrad.czechowski@gmail.com", "p.kozakowski@mimuw.edu.pl", "maciej.klimek@gmail.com"], "title": "Uncertainty - sensitive learning and planning with ensembles", "authors": ["Piotr Mi\u0142o\u015b", "\u0141ukasz Kuci\u0144ski", "Konrad Czechowski", "Piotr Kozakowski", "Maciej Klimek"], "pdf": "/pdf/adbfde55dab1f41a282f6358c7e859af2f7707b7.pdf", "abstract": "We propose a reinforcement learning framework for discrete environments in which an agent optimizes its behavior on two timescales. For the short one, it uses tree search methods to perform tactical decisions. The long strategic level is handled with an ensemble of value functions learned using $TD$-like backups. Combining these two techniques brings synergies. The planning module performs \\textit{what-if} analysis allowing to avoid short-term pitfalls and boost backups of the value function. Notably, our method performs well in environments with sparse rewards where standard $TD(1)$ backups fail. On the other hand, the value functions compensate for inherent short-sightedness of planning. Importantly, we use ensembles to measure the epistemic uncertainty of value functions. This serves two purposes: a) it stabilizes planning, b) it guides exploration. \n\nWe evaluate our methods on discrete environments with sparse rewards: the Deep sea chain environment, toy Montezuma's Revenge, and Sokoban. In all the cases, we obtain speed-up of learning and boost to the final performance.", "code": "https://github.com/learningandplanningICLR/learningandplanning", "keywords": ["deep reinfocement learning", "mcts", "ensembles", "uncertainty"], "paperhash": "mio|uncertainty_sensitive_learning_and_planning_with_ensembles", "original_pdf": "/attachment/c3ab5a1f0a2ddc7731ae7cfd0abffb3e2b6965f3.pdf", "_bibtex": "@misc{\nmi{\\l}o{\\'s}2020uncertainty,\ntitle={Uncertainty - sensitive learning and planning with ensembles},\nauthor={Piotr Mi{\\l}o{\\'s} and {\\L}ukasz Kuci{\\'n}ski and Konrad Czechowski and Piotr Kozakowski and Maciej Klimek},\nyear={2020},\nurl={https://openreview.net/forum?id=SkglVlSFPS}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "BJzN0S-PQ", "original": null, "number": 1, "cdate": 1576798743941, "ddate": null, "tcdate": 1576798743941, "tmdate": 1576800892212, "tddate": null, "forum": "SkglVlSFPS", "replyto": "SkglVlSFPS", "invitation": "ICLR.cc/2020/Conference/Paper2236/-/Decision", "content": {"decision": "Reject", "comment": "The authors study planning problems with sparse rewards.                                                                           \nThey propose a tree search algorithm together with an ensemble of value                                                            \nfunctions to guide exploration in this setting.                                                                                    \nThe value predictions from the ensemble are combined in a risk sensitive way,                                                      \ntherefore biasing the search towards states with high uncertainty in value                                                         \nprediction.                                                                                                                        \nThe approach is applied to several grid-world environments.                                                                        \n                                                                                                                                   \nThe reviewers mostly criticized the presentation of the material, in particular                                                    \nthat the paper provided insufficient details on the proposed                                                                       \nmethod. Furthermore, the comparison to model-free RL methods was deemed somewhat                                                   \nlacking, as the proposed algorithm has access to the ground truth model.                                                           \nThe authors improved the manuscript in the rebuttal.                                                                               \n                                                                                                                                   \nBased on the reviews and my own reading I think that the paper in it's current                                                     \nform is below acceptance threshold. However, with further improved presentation                                                    \nand baselines for the experiments, this has potential to be an important contribution.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["pmilos@mimuw.edu.pl", "lukasz.kucinski@gmail.com", "konrad.czechowski@gmail.com", "p.kozakowski@mimuw.edu.pl", "maciej.klimek@gmail.com"], "title": "Uncertainty - sensitive learning and planning with ensembles", "authors": ["Piotr Mi\u0142o\u015b", "\u0141ukasz Kuci\u0144ski", "Konrad Czechowski", "Piotr Kozakowski", "Maciej Klimek"], "pdf": "/pdf/adbfde55dab1f41a282f6358c7e859af2f7707b7.pdf", "abstract": "We propose a reinforcement learning framework for discrete environments in which an agent optimizes its behavior on two timescales. For the short one, it uses tree search methods to perform tactical decisions. The long strategic level is handled with an ensemble of value functions learned using $TD$-like backups. Combining these two techniques brings synergies. The planning module performs \\textit{what-if} analysis allowing to avoid short-term pitfalls and boost backups of the value function. Notably, our method performs well in environments with sparse rewards where standard $TD(1)$ backups fail. On the other hand, the value functions compensate for inherent short-sightedness of planning. Importantly, we use ensembles to measure the epistemic uncertainty of value functions. This serves two purposes: a) it stabilizes planning, b) it guides exploration. \n\nWe evaluate our methods on discrete environments with sparse rewards: the Deep sea chain environment, toy Montezuma's Revenge, and Sokoban. In all the cases, we obtain speed-up of learning and boost to the final performance.", "code": "https://github.com/learningandplanningICLR/learningandplanning", "keywords": ["deep reinfocement learning", "mcts", "ensembles", "uncertainty"], "paperhash": "mio|uncertainty_sensitive_learning_and_planning_with_ensembles", "original_pdf": "/attachment/c3ab5a1f0a2ddc7731ae7cfd0abffb3e2b6965f3.pdf", "_bibtex": "@misc{\nmi{\\l}o{\\'s}2020uncertainty,\ntitle={Uncertainty - sensitive learning and planning with ensembles},\nauthor={Piotr Mi{\\l}o{\\'s} and {\\L}ukasz Kuci{\\'n}ski and Konrad Czechowski and Piotr Kozakowski and Maciej Klimek},\nyear={2020},\nurl={https://openreview.net/forum?id=SkglVlSFPS}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "SkglVlSFPS", "replyto": "SkglVlSFPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795708762, "tmdate": 1576800257285, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2236/-/Decision"}}}, {"id": "H1lRg1qtcS", "original": null, "number": 2, "cdate": 1572605686009, "ddate": null, "tcdate": 1572605686009, "tmdate": 1574350105479, "tddate": null, "forum": "SkglVlSFPS", "replyto": "SkglVlSFPS", "invitation": "ICLR.cc/2020/Conference/Paper2236/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #4", "review": "The authors propose to combine planning methods like MCTS with an ensemble of value functions to a) estimate the value of leaf nodes of the search tree and b) use the ensemble estimate of uncertainty to guide exploration during MCTS search. \nThe MCTS rollouts are also used as optimization targets for the value function.\n\nI believe this is a clear reject. On the one hand, the paper needs signficiantly more work on the writing and clarity. On the other hand I have several worries on the method and evaluation side. \n\nRegarding the presentation of the paper:\nOverall, the paper seems quite rushed. This is not a strong reason for rejection but should be improved in a future version. For example, punctuation and sentence structure is often wrong, the paper has only slighlty over 7 pages, a citation is undefined on p.7 and images and whitespace is formatted wrongly on occasion (e.g. top of page 6).\nMore importantly, on the content side, the experimental section is sufficiently clear and well written, however, the method description needs more detail and background information. The paper relies on several prior works which are referred to but not described (E.g. MCTS , the sampling mechanism by Osband et al. which they are using but not describing, the 'mask' from Osband et al which they are using but not describing).\nFurthermore, the algorithm itself is not described in sufficient detail:\n- How does the 'soft-penalization' work?\n- How exactly does the mechanism \"similar in fashing to\" Thomson sampling work?\n- Are you learning a model or do you have access to the true transition function?\n\nRegarding the method:\nI can't say anything definitive about the method as I'm not entirely clear how exactly it works. However, I have several worries that might need addressing:\n- It seems to me that the method relies on access to the _true_ transition and reward function and not on a learned model. This is a big difference to much of the prior work they compare against. This also makes the comparison against any pure model free method like PPO much less meaningful.\n- Similarly, manually avoiding dead-ends and loops is a very strong assumption \n- Also, being able to distinguish and use a fixed ratio of \"solved\" and \"unsolved\" episodes is a strong assumption. \n- The one main contribution seems to be a new way of how \\phi_a(x) is defined. Their particular choice needs a clearer motivation. Furthermore, if there is more contribution and differences to prior work, highlighting them more would help the reader understand the contribution. \n- As the work makes several strong assumptions regarding the environment and access to the model, significantly more work (e.g. ablation studies) is needed to clearly show which assumption and feature of the algorithm is important for performance (and ideally also why). For example (but that's just a first idea): To understand the impact of their choice of \\phi vs. their planning architecture, it would be be interesting to maybe train PPO using an exploration bonus based on \\phi. This would allow disentangling the contribution of: Access to the true model, \"discrete-environment-tricks\" like penalizing dead-ends, and exploration incentivication of \\phi. \n\nEdit:\nThank you for your response and the updated manuscript, which reads considerably better.\nI also agree with your point regarding the strength of assumption regarding \"solved\" and \"unsolved\" episodes.\n\nConsequently, I will raise my score to a \"weak reject\" to express that I think this is promising work.\n\nI do believe that ablation studies would add a lot to the paper as they would allow one to see which of the (many) added components help how much, for example between the selection function $\\phi$ and the various penalizations used. ", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper2236/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2236/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["pmilos@mimuw.edu.pl", "lukasz.kucinski@gmail.com", "konrad.czechowski@gmail.com", "p.kozakowski@mimuw.edu.pl", "maciej.klimek@gmail.com"], "title": "Uncertainty - sensitive learning and planning with ensembles", "authors": ["Piotr Mi\u0142o\u015b", "\u0141ukasz Kuci\u0144ski", "Konrad Czechowski", "Piotr Kozakowski", "Maciej Klimek"], "pdf": "/pdf/adbfde55dab1f41a282f6358c7e859af2f7707b7.pdf", "abstract": "We propose a reinforcement learning framework for discrete environments in which an agent optimizes its behavior on two timescales. For the short one, it uses tree search methods to perform tactical decisions. The long strategic level is handled with an ensemble of value functions learned using $TD$-like backups. Combining these two techniques brings synergies. The planning module performs \\textit{what-if} analysis allowing to avoid short-term pitfalls and boost backups of the value function. Notably, our method performs well in environments with sparse rewards where standard $TD(1)$ backups fail. On the other hand, the value functions compensate for inherent short-sightedness of planning. Importantly, we use ensembles to measure the epistemic uncertainty of value functions. This serves two purposes: a) it stabilizes planning, b) it guides exploration. \n\nWe evaluate our methods on discrete environments with sparse rewards: the Deep sea chain environment, toy Montezuma's Revenge, and Sokoban. In all the cases, we obtain speed-up of learning and boost to the final performance.", "code": "https://github.com/learningandplanningICLR/learningandplanning", "keywords": ["deep reinfocement learning", "mcts", "ensembles", "uncertainty"], "paperhash": "mio|uncertainty_sensitive_learning_and_planning_with_ensembles", "original_pdf": "/attachment/c3ab5a1f0a2ddc7731ae7cfd0abffb3e2b6965f3.pdf", "_bibtex": "@misc{\nmi{\\l}o{\\'s}2020uncertainty,\ntitle={Uncertainty - sensitive learning and planning with ensembles},\nauthor={Piotr Mi{\\l}o{\\'s} and {\\L}ukasz Kuci{\\'n}ski and Konrad Czechowski and Piotr Kozakowski and Maciej Klimek},\nyear={2020},\nurl={https://openreview.net/forum?id=SkglVlSFPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SkglVlSFPS", "replyto": "SkglVlSFPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2236/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2236/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575569734019, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2236/Reviewers"], "noninvitees": [], "tcdate": 1570237725748, "tmdate": 1575569734036, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2236/-/Official_Review"}}}, {"id": "Hyl3nCQnoH", "original": null, "number": 6, "cdate": 1573826228419, "ddate": null, "tcdate": 1573826228419, "tmdate": 1573826228419, "tddate": null, "forum": "SkglVlSFPS", "replyto": "HkgcGAHAFH", "invitation": "ICLR.cc/2020/Conference/Paper2236/-/Official_Comment", "content": {"title": "Answer to AnonReviewer1", "comment": "We thank for the review."}, "signatures": ["ICLR.cc/2020/Conference/Paper2236/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2236/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["pmilos@mimuw.edu.pl", "lukasz.kucinski@gmail.com", "konrad.czechowski@gmail.com", "p.kozakowski@mimuw.edu.pl", "maciej.klimek@gmail.com"], "title": "Uncertainty - sensitive learning and planning with ensembles", "authors": ["Piotr Mi\u0142o\u015b", "\u0141ukasz Kuci\u0144ski", "Konrad Czechowski", "Piotr Kozakowski", "Maciej Klimek"], "pdf": "/pdf/adbfde55dab1f41a282f6358c7e859af2f7707b7.pdf", "abstract": "We propose a reinforcement learning framework for discrete environments in which an agent optimizes its behavior on two timescales. For the short one, it uses tree search methods to perform tactical decisions. The long strategic level is handled with an ensemble of value functions learned using $TD$-like backups. Combining these two techniques brings synergies. The planning module performs \\textit{what-if} analysis allowing to avoid short-term pitfalls and boost backups of the value function. Notably, our method performs well in environments with sparse rewards where standard $TD(1)$ backups fail. On the other hand, the value functions compensate for inherent short-sightedness of planning. Importantly, we use ensembles to measure the epistemic uncertainty of value functions. This serves two purposes: a) it stabilizes planning, b) it guides exploration. \n\nWe evaluate our methods on discrete environments with sparse rewards: the Deep sea chain environment, toy Montezuma's Revenge, and Sokoban. In all the cases, we obtain speed-up of learning and boost to the final performance.", "code": "https://github.com/learningandplanningICLR/learningandplanning", "keywords": ["deep reinfocement learning", "mcts", "ensembles", "uncertainty"], "paperhash": "mio|uncertainty_sensitive_learning_and_planning_with_ensembles", "original_pdf": "/attachment/c3ab5a1f0a2ddc7731ae7cfd0abffb3e2b6965f3.pdf", "_bibtex": "@misc{\nmi{\\l}o{\\'s}2020uncertainty,\ntitle={Uncertainty - sensitive learning and planning with ensembles},\nauthor={Piotr Mi{\\l}o{\\'s} and {\\L}ukasz Kuci{\\'n}ski and Konrad Czechowski and Piotr Kozakowski and Maciej Klimek},\nyear={2020},\nurl={https://openreview.net/forum?id=SkglVlSFPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SkglVlSFPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2236/Authors", "ICLR.cc/2020/Conference/Paper2236/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2236/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2236/Reviewers", "ICLR.cc/2020/Conference/Paper2236/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2236/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2236/Authors|ICLR.cc/2020/Conference/Paper2236/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504144341, "tmdate": 1576860555008, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2236/Authors", "ICLR.cc/2020/Conference/Paper2236/Reviewers", "ICLR.cc/2020/Conference/Paper2236/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2236/-/Official_Comment"}}}, {"id": "HJxM0T73oS", "original": null, "number": 5, "cdate": 1573825994037, "ddate": null, "tcdate": 1573825994037, "tmdate": 1573825994037, "tddate": null, "forum": "SkglVlSFPS", "replyto": "H1lRg1qtcS", "invitation": "ICLR.cc/2020/Conference/Paper2236/-/Official_Comment", "content": {"title": "Answer to AnonReviewer4", "comment": "We thank the reviewer for a detailed review. We admit major deficiencies in the presentation of our work, which, we believe, improved significantly in the new uploaded version. \n\nAnswering detailed comments:\n- the presentation of the method is rewritten. We hope that it is much clearer and addresses the reviewers concerns. In particular, we explain MCTS, mask, sampling mechanism either in Section 2 or Appendix.\n- As to the reviewer concerns regarding 'soft-penalization'. We penalize loops on two levels, mcts planner and the episodes. This is  now explained in Section 2 and Appendix A (the relevant parameters are penalty_p and penalty_e).\n- Regarding assumptions: In our work we assume access to the prefect model (realised by the simulator), which, hopefully clearly, is now stated in the introduction. Having a model enables to avoid loops (independently of the fact how the model is obtained). \nWe admit that access to a model is a substantial assumption, however this is somewhat orthogonal to our main focus. In the future work, we would like to address learning models. There are a number of domains, like Sokoban, in which, we believe, learning of the model is much simpler than planning. In fact our very preliminary experiments indicate that learning model of Sokoban is is indeed likely to be doable. \n- We respectfully disagree with the statement that that having fixed ratio of \u201csolved\u201d and \u201cunsolved\u201d episodes is a strong assumption. For environments with a single (sparse) reward for completing a task (like the environments used in our experiments) it is a very natural concept. Furthermore, there is a growing body of literature concerning prioritised usage of \u201cgood\u201d episodes, see e.g. [1,2,3]. \n- Motivation behind particular choices of \\phi_a are now presented in Section 2.\n- We admit that  having ablations would be nice to have and we will prepare them for the camera ready version. We reckon that this does not diminish the value of the method itself.\n- We like the idea of including the exploration bonus into model-free training and state it in the future work section. \n\n[1] Junhyuk Oh, Yijie Guo, Satinder Singh, and Honglak Lee.  Self-imitation learning. ICML 2018\n[2] Kaixiang Lin, Jiayu Zhou, Ranking Policy Gradient. arXiv:1906.09674, 2019\n[3] Yijie Guo, Jongwook Choi, Marcin Moczulski, Samy Bengio, Mohammad Norouzi, Honglak Lee, Efficient Exploration with Self-Imitation Learning via Trajectory-Conditioned Policy. arXiv:1907.10247 (2019)\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2236/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2236/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["pmilos@mimuw.edu.pl", "lukasz.kucinski@gmail.com", "konrad.czechowski@gmail.com", "p.kozakowski@mimuw.edu.pl", "maciej.klimek@gmail.com"], "title": "Uncertainty - sensitive learning and planning with ensembles", "authors": ["Piotr Mi\u0142o\u015b", "\u0141ukasz Kuci\u0144ski", "Konrad Czechowski", "Piotr Kozakowski", "Maciej Klimek"], "pdf": "/pdf/adbfde55dab1f41a282f6358c7e859af2f7707b7.pdf", "abstract": "We propose a reinforcement learning framework for discrete environments in which an agent optimizes its behavior on two timescales. For the short one, it uses tree search methods to perform tactical decisions. The long strategic level is handled with an ensemble of value functions learned using $TD$-like backups. Combining these two techniques brings synergies. The planning module performs \\textit{what-if} analysis allowing to avoid short-term pitfalls and boost backups of the value function. Notably, our method performs well in environments with sparse rewards where standard $TD(1)$ backups fail. On the other hand, the value functions compensate for inherent short-sightedness of planning. Importantly, we use ensembles to measure the epistemic uncertainty of value functions. This serves two purposes: a) it stabilizes planning, b) it guides exploration. \n\nWe evaluate our methods on discrete environments with sparse rewards: the Deep sea chain environment, toy Montezuma's Revenge, and Sokoban. In all the cases, we obtain speed-up of learning and boost to the final performance.", "code": "https://github.com/learningandplanningICLR/learningandplanning", "keywords": ["deep reinfocement learning", "mcts", "ensembles", "uncertainty"], "paperhash": "mio|uncertainty_sensitive_learning_and_planning_with_ensembles", "original_pdf": "/attachment/c3ab5a1f0a2ddc7731ae7cfd0abffb3e2b6965f3.pdf", "_bibtex": "@misc{\nmi{\\l}o{\\'s}2020uncertainty,\ntitle={Uncertainty - sensitive learning and planning with ensembles},\nauthor={Piotr Mi{\\l}o{\\'s} and {\\L}ukasz Kuci{\\'n}ski and Konrad Czechowski and Piotr Kozakowski and Maciej Klimek},\nyear={2020},\nurl={https://openreview.net/forum?id=SkglVlSFPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SkglVlSFPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2236/Authors", "ICLR.cc/2020/Conference/Paper2236/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2236/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2236/Reviewers", "ICLR.cc/2020/Conference/Paper2236/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2236/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2236/Authors|ICLR.cc/2020/Conference/Paper2236/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504144341, "tmdate": 1576860555008, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2236/Authors", "ICLR.cc/2020/Conference/Paper2236/Reviewers", "ICLR.cc/2020/Conference/Paper2236/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2236/-/Official_Comment"}}}, {"id": "SJlz4TX3jH", "original": null, "number": 4, "cdate": 1573825833549, "ddate": null, "tcdate": 1573825833549, "tmdate": 1573825833549, "tddate": null, "forum": "SkglVlSFPS", "replyto": "HJlP92j69H", "invitation": "ICLR.cc/2020/Conference/Paper2236/-/Official_Comment", "content": {"title": "Answer to AnonReviewer3", "comment": "We thank for the review and comments. We admit various shortcomings especially in the text clarity. We have uploaded a new overhauled version of the paper, which we hope makes our work more accessible. \n\nBelow we address particular concerns of the reviewer:\n- \u201cmain algorithm 1, it really seems like more of a sketch\u201d - the description of the method has been rewritten. We provide pseudo-code for all the components of the method. This is done in Section 2 and Appendix A. \n- Figures descriptions, in particular Figures 1 and Figure 2, have been clarified.\n- We assume access to the perfect model (simulator so to speak), which is now, hopefully clearly, stated in the introduction. One could argue that in some cases, like Sokoban, the model is easy to learn, what is hard is planning. Having said that, learning models and using imperfect models is an exciting research direction. \n- As to the code, we have made some further clean-ups and improved the README file. Meanwhile, we have also been working on a completely new version of code, redesigned from scratch. We hope that it will also be of use for the community. \n- We made our best to clearly state our contribution as well as match our claims with the corresponding evidence. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2236/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2236/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["pmilos@mimuw.edu.pl", "lukasz.kucinski@gmail.com", "konrad.czechowski@gmail.com", "p.kozakowski@mimuw.edu.pl", "maciej.klimek@gmail.com"], "title": "Uncertainty - sensitive learning and planning with ensembles", "authors": ["Piotr Mi\u0142o\u015b", "\u0141ukasz Kuci\u0144ski", "Konrad Czechowski", "Piotr Kozakowski", "Maciej Klimek"], "pdf": "/pdf/adbfde55dab1f41a282f6358c7e859af2f7707b7.pdf", "abstract": "We propose a reinforcement learning framework for discrete environments in which an agent optimizes its behavior on two timescales. For the short one, it uses tree search methods to perform tactical decisions. The long strategic level is handled with an ensemble of value functions learned using $TD$-like backups. Combining these two techniques brings synergies. The planning module performs \\textit{what-if} analysis allowing to avoid short-term pitfalls and boost backups of the value function. Notably, our method performs well in environments with sparse rewards where standard $TD(1)$ backups fail. On the other hand, the value functions compensate for inherent short-sightedness of planning. Importantly, we use ensembles to measure the epistemic uncertainty of value functions. This serves two purposes: a) it stabilizes planning, b) it guides exploration. \n\nWe evaluate our methods on discrete environments with sparse rewards: the Deep sea chain environment, toy Montezuma's Revenge, and Sokoban. In all the cases, we obtain speed-up of learning and boost to the final performance.", "code": "https://github.com/learningandplanningICLR/learningandplanning", "keywords": ["deep reinfocement learning", "mcts", "ensembles", "uncertainty"], "paperhash": "mio|uncertainty_sensitive_learning_and_planning_with_ensembles", "original_pdf": "/attachment/c3ab5a1f0a2ddc7731ae7cfd0abffb3e2b6965f3.pdf", "_bibtex": "@misc{\nmi{\\l}o{\\'s}2020uncertainty,\ntitle={Uncertainty - sensitive learning and planning with ensembles},\nauthor={Piotr Mi{\\l}o{\\'s} and {\\L}ukasz Kuci{\\'n}ski and Konrad Czechowski and Piotr Kozakowski and Maciej Klimek},\nyear={2020},\nurl={https://openreview.net/forum?id=SkglVlSFPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SkglVlSFPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2236/Authors", "ICLR.cc/2020/Conference/Paper2236/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2236/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2236/Reviewers", "ICLR.cc/2020/Conference/Paper2236/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2236/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2236/Authors|ICLR.cc/2020/Conference/Paper2236/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504144341, "tmdate": 1576860555008, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2236/Authors", "ICLR.cc/2020/Conference/Paper2236/Reviewers", "ICLR.cc/2020/Conference/Paper2236/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2236/-/Official_Comment"}}}, {"id": "HkgcGAHAFH", "original": null, "number": 1, "cdate": 1571868177960, "ddate": null, "tcdate": 1571868177960, "tmdate": 1572972365218, "tddate": null, "forum": "SkglVlSFPS", "replyto": "SkglVlSFPS", "invitation": "ICLR.cc/2020/Conference/Paper2236/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper proposes an approach blending model-based, model-free methods and utilizing risk-sensitivity information in ensembles as part of the value estimation and exploration process. The exploration is based on risk- sensitivity measures such as moments and relative majority vote. There is a lot of work currently trying to marry the model free with model based approaches for integrated planning and learning as the authors have mentioned in the related work section of the paper and also called out similar methods and techniques. The authors have provided evidence via experiments in three environments and shown good results of using this blended approach. Code is also provided for others to further carry out explorations in this research area.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2236/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2236/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["pmilos@mimuw.edu.pl", "lukasz.kucinski@gmail.com", "konrad.czechowski@gmail.com", "p.kozakowski@mimuw.edu.pl", "maciej.klimek@gmail.com"], "title": "Uncertainty - sensitive learning and planning with ensembles", "authors": ["Piotr Mi\u0142o\u015b", "\u0141ukasz Kuci\u0144ski", "Konrad Czechowski", "Piotr Kozakowski", "Maciej Klimek"], "pdf": "/pdf/adbfde55dab1f41a282f6358c7e859af2f7707b7.pdf", "abstract": "We propose a reinforcement learning framework for discrete environments in which an agent optimizes its behavior on two timescales. For the short one, it uses tree search methods to perform tactical decisions. The long strategic level is handled with an ensemble of value functions learned using $TD$-like backups. Combining these two techniques brings synergies. The planning module performs \\textit{what-if} analysis allowing to avoid short-term pitfalls and boost backups of the value function. Notably, our method performs well in environments with sparse rewards where standard $TD(1)$ backups fail. On the other hand, the value functions compensate for inherent short-sightedness of planning. Importantly, we use ensembles to measure the epistemic uncertainty of value functions. This serves two purposes: a) it stabilizes planning, b) it guides exploration. \n\nWe evaluate our methods on discrete environments with sparse rewards: the Deep sea chain environment, toy Montezuma's Revenge, and Sokoban. In all the cases, we obtain speed-up of learning and boost to the final performance.", "code": "https://github.com/learningandplanningICLR/learningandplanning", "keywords": ["deep reinfocement learning", "mcts", "ensembles", "uncertainty"], "paperhash": "mio|uncertainty_sensitive_learning_and_planning_with_ensembles", "original_pdf": "/attachment/c3ab5a1f0a2ddc7731ae7cfd0abffb3e2b6965f3.pdf", "_bibtex": "@misc{\nmi{\\l}o{\\'s}2020uncertainty,\ntitle={Uncertainty - sensitive learning and planning with ensembles},\nauthor={Piotr Mi{\\l}o{\\'s} and {\\L}ukasz Kuci{\\'n}ski and Konrad Czechowski and Piotr Kozakowski and Maciej Klimek},\nyear={2020},\nurl={https://openreview.net/forum?id=SkglVlSFPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SkglVlSFPS", "replyto": "SkglVlSFPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2236/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2236/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575569734019, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2236/Reviewers"], "noninvitees": [], "tcdate": 1570237725748, "tmdate": 1575569734036, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2236/-/Official_Review"}}}, {"id": "HJlP92j69H", "original": null, "number": 3, "cdate": 1572875406656, "ddate": null, "tcdate": 1572875406656, "tmdate": 1572972365124, "tddate": null, "forum": "SkglVlSFPS", "replyto": "SkglVlSFPS", "invitation": "ICLR.cc/2020/Conference/Paper2236/-/Official_Review", "content": {"rating": "1: Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "Uncertainty-Sensitive Learning and Planning with Ensembles\n=====================================================\n\nThis paper investigates the use of uncertainty-aware estimates in solving planning problems (RL with access to simulator).\nThe proposed algorithm combines a learned model-free value estimate with MCTS planning.\nAn ensemble of neural networks is used to model posterior uncertainty in the value estimate and drive efficient exploration.\n\n\nThere are several things to like about this paper:\n- The paper takes on several core issues in RL/planning research, most notably the synthesis of dealing with model-based and model-free uncertainty in RL.\n- The general flavour of the paper + algorithm seems to be reasonable. The proposal to use ensemble uncertainty estimates to drive model-based MCTS is interesting, natural, and I think it's a good one.\n- The proposed structure of the paper is quite nice, there is mostly a linear and logical progression of complexity in the experiments. This is nice to see clear benefits of the approach on the simplest possible settings and build up from there.\n- The effort to open source code + implementation details is laudable.\n\nHowever, there are several places where this paper falls short:\n- In general, the claims and results of the paper are far too vague to be fully understood and replicated. Take the main algorithm 1, it really seems like more of a \"sketch\" of a very general family of algorithms, rather than a specific description of a clear algorithm.\n- This vagueness is spread throughout the plots and figures as well... note that Figure 1 has no indication of how many steps have been evaluated, and Figure 2 has no indication for what value K > 0 was actually used. The clarity does not improve in Sections 3.2 and 3.3 where quite inconsistent performance metrics and presentations are presented.\n- Generally, the writing could be tightened quite a lot. In particular I would encourage you to think about whether each statement you make is clearly supported by some theorem, experiment or plot in your paper. For example, on page 3 \"We found this mechanism to be beneficial... see Section 3.3\" but then it's not clear exactly what statement shows that particular part of the mechanism was helpful, versus other issues associated with ensemble learning. There are more than a few typos... the on(e) in Osband... akin to ??... might be obtained by choosing from (the) ensemble...\n- It would be very helpful to clarify that the agent is given access to a simulator... so that this is not exactly the typical RL setting of sequential decision making. This should appear early in the paper.\n- The code that is released with the paper is also quite confusing, it is not structured with a clear README and includes many sections of dead/commented code. I was hoping the code might rescue some of the clarity, but I think that still needs work.\n\nOverall, I do think there is some interesting material here...\nIt's an important problem, and the core building blocks of combining model, value and uncertainty for better exploration is interesting.\nHowever, I just think the actual paper is not clear enough on the details.\nMy belief is that going through this paper very methodically and carefully to make sure that every single detail + claim is rigorously supported would help this paper immensely.\nFor that reason I have to say that I think it's a \"reject\" in its current form."}, "signatures": ["ICLR.cc/2020/Conference/Paper2236/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2236/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["pmilos@mimuw.edu.pl", "lukasz.kucinski@gmail.com", "konrad.czechowski@gmail.com", "p.kozakowski@mimuw.edu.pl", "maciej.klimek@gmail.com"], "title": "Uncertainty - sensitive learning and planning with ensembles", "authors": ["Piotr Mi\u0142o\u015b", "\u0141ukasz Kuci\u0144ski", "Konrad Czechowski", "Piotr Kozakowski", "Maciej Klimek"], "pdf": "/pdf/adbfde55dab1f41a282f6358c7e859af2f7707b7.pdf", "abstract": "We propose a reinforcement learning framework for discrete environments in which an agent optimizes its behavior on two timescales. For the short one, it uses tree search methods to perform tactical decisions. The long strategic level is handled with an ensemble of value functions learned using $TD$-like backups. Combining these two techniques brings synergies. The planning module performs \\textit{what-if} analysis allowing to avoid short-term pitfalls and boost backups of the value function. Notably, our method performs well in environments with sparse rewards where standard $TD(1)$ backups fail. On the other hand, the value functions compensate for inherent short-sightedness of planning. Importantly, we use ensembles to measure the epistemic uncertainty of value functions. This serves two purposes: a) it stabilizes planning, b) it guides exploration. \n\nWe evaluate our methods on discrete environments with sparse rewards: the Deep sea chain environment, toy Montezuma's Revenge, and Sokoban. In all the cases, we obtain speed-up of learning and boost to the final performance.", "code": "https://github.com/learningandplanningICLR/learningandplanning", "keywords": ["deep reinfocement learning", "mcts", "ensembles", "uncertainty"], "paperhash": "mio|uncertainty_sensitive_learning_and_planning_with_ensembles", "original_pdf": "/attachment/c3ab5a1f0a2ddc7731ae7cfd0abffb3e2b6965f3.pdf", "_bibtex": "@misc{\nmi{\\l}o{\\'s}2020uncertainty,\ntitle={Uncertainty - sensitive learning and planning with ensembles},\nauthor={Piotr Mi{\\l}o{\\'s} and {\\L}ukasz Kuci{\\'n}ski and Konrad Czechowski and Piotr Kozakowski and Maciej Klimek},\nyear={2020},\nurl={https://openreview.net/forum?id=SkglVlSFPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SkglVlSFPS", "replyto": "SkglVlSFPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2236/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2236/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575569734019, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2236/Reviewers"], "noninvitees": [], "tcdate": 1570237725748, "tmdate": 1575569734036, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2236/-/Official_Review"}}}, {"id": "SJlvczd5ur", "original": null, "number": 1, "cdate": 1570566798987, "ddate": null, "tcdate": 1570566798987, "tmdate": 1570566798987, "tddate": null, "forum": "SkglVlSFPS", "replyto": "Byea19BhDS", "invitation": "ICLR.cc/2020/Conference/Paper2236/-/Official_Comment", "content": {"comment": "You're right. It took us somewhat longer to make a repo, which we are sure that is fully anonymous. Now it is up and running. ", "title": "repo working"}, "signatures": ["ICLR.cc/2020/Conference/Paper2236/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2236/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["pmilos@mimuw.edu.pl", "lukasz.kucinski@gmail.com", "konrad.czechowski@gmail.com", "p.kozakowski@mimuw.edu.pl", "maciej.klimek@gmail.com"], "title": "Uncertainty - sensitive learning and planning with ensembles", "authors": ["Piotr Mi\u0142o\u015b", "\u0141ukasz Kuci\u0144ski", "Konrad Czechowski", "Piotr Kozakowski", "Maciej Klimek"], "pdf": "/pdf/adbfde55dab1f41a282f6358c7e859af2f7707b7.pdf", "abstract": "We propose a reinforcement learning framework for discrete environments in which an agent optimizes its behavior on two timescales. For the short one, it uses tree search methods to perform tactical decisions. The long strategic level is handled with an ensemble of value functions learned using $TD$-like backups. Combining these two techniques brings synergies. The planning module performs \\textit{what-if} analysis allowing to avoid short-term pitfalls and boost backups of the value function. Notably, our method performs well in environments with sparse rewards where standard $TD(1)$ backups fail. On the other hand, the value functions compensate for inherent short-sightedness of planning. Importantly, we use ensembles to measure the epistemic uncertainty of value functions. This serves two purposes: a) it stabilizes planning, b) it guides exploration. \n\nWe evaluate our methods on discrete environments with sparse rewards: the Deep sea chain environment, toy Montezuma's Revenge, and Sokoban. In all the cases, we obtain speed-up of learning and boost to the final performance.", "code": "https://github.com/learningandplanningICLR/learningandplanning", "keywords": ["deep reinfocement learning", "mcts", "ensembles", "uncertainty"], "paperhash": "mio|uncertainty_sensitive_learning_and_planning_with_ensembles", "original_pdf": "/attachment/c3ab5a1f0a2ddc7731ae7cfd0abffb3e2b6965f3.pdf", "_bibtex": "@misc{\nmi{\\l}o{\\'s}2020uncertainty,\ntitle={Uncertainty - sensitive learning and planning with ensembles},\nauthor={Piotr Mi{\\l}o{\\'s} and {\\L}ukasz Kuci{\\'n}ski and Konrad Czechowski and Piotr Kozakowski and Maciej Klimek},\nyear={2020},\nurl={https://openreview.net/forum?id=SkglVlSFPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SkglVlSFPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2236/Authors", "ICLR.cc/2020/Conference/Paper2236/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2236/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2236/Reviewers", "ICLR.cc/2020/Conference/Paper2236/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2236/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2236/Authors|ICLR.cc/2020/Conference/Paper2236/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504144341, "tmdate": 1576860555008, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2236/Authors", "ICLR.cc/2020/Conference/Paper2236/Reviewers", "ICLR.cc/2020/Conference/Paper2236/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2236/-/Official_Comment"}}}, {"id": "Byea19BhDS", "original": null, "number": 1, "cdate": 1569638885204, "ddate": null, "tcdate": 1569638885204, "tmdate": 1569638885204, "tddate": null, "forum": "SkglVlSFPS", "replyto": "SkglVlSFPS", "invitation": "ICLR.cc/2020/Conference/Paper2236/-/Public_Comment", "content": {"comment": "Hi,\n       \nNo code is present in the repo of the github link. It is not fair to provide a placeholder link for code submissions (which impact the review process) and submit code taking considerable buffer time after submission deadline.", "title": "No code in provided github link even after 60 hours of submission deadline "}, "signatures": ["~Anthony_Wittmer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Anthony_Wittmer1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["pmilos@mimuw.edu.pl", "lukasz.kucinski@gmail.com", "konrad.czechowski@gmail.com", "p.kozakowski@mimuw.edu.pl", "maciej.klimek@gmail.com"], "title": "Uncertainty - sensitive learning and planning with ensembles", "authors": ["Piotr Mi\u0142o\u015b", "\u0141ukasz Kuci\u0144ski", "Konrad Czechowski", "Piotr Kozakowski", "Maciej Klimek"], "pdf": "/pdf/adbfde55dab1f41a282f6358c7e859af2f7707b7.pdf", "abstract": "We propose a reinforcement learning framework for discrete environments in which an agent optimizes its behavior on two timescales. For the short one, it uses tree search methods to perform tactical decisions. The long strategic level is handled with an ensemble of value functions learned using $TD$-like backups. Combining these two techniques brings synergies. The planning module performs \\textit{what-if} analysis allowing to avoid short-term pitfalls and boost backups of the value function. Notably, our method performs well in environments with sparse rewards where standard $TD(1)$ backups fail. On the other hand, the value functions compensate for inherent short-sightedness of planning. Importantly, we use ensembles to measure the epistemic uncertainty of value functions. This serves two purposes: a) it stabilizes planning, b) it guides exploration. \n\nWe evaluate our methods on discrete environments with sparse rewards: the Deep sea chain environment, toy Montezuma's Revenge, and Sokoban. In all the cases, we obtain speed-up of learning and boost to the final performance.", "code": "https://github.com/learningandplanningICLR/learningandplanning", "keywords": ["deep reinfocement learning", "mcts", "ensembles", "uncertainty"], "paperhash": "mio|uncertainty_sensitive_learning_and_planning_with_ensembles", "original_pdf": "/attachment/c3ab5a1f0a2ddc7731ae7cfd0abffb3e2b6965f3.pdf", "_bibtex": "@misc{\nmi{\\l}o{\\'s}2020uncertainty,\ntitle={Uncertainty - sensitive learning and planning with ensembles},\nauthor={Piotr Mi{\\l}o{\\'s} and {\\L}ukasz Kuci{\\'n}ski and Konrad Czechowski and Piotr Kozakowski and Maciej Klimek},\nyear={2020},\nurl={https://openreview.net/forum?id=SkglVlSFPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SkglVlSFPS", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504183179, "tmdate": 1576860588150, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper2236/Authors", "ICLR.cc/2020/Conference/Paper2236/Reviewers", "ICLR.cc/2020/Conference/Paper2236/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2236/-/Public_Comment"}}}], "count": 10}