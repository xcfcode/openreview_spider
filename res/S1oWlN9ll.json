{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1488467226135, "tcdate": 1478275075169, "number": 203, "id": "S1oWlN9ll", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "S1oWlN9ll", "signatures": ["~LU_HOU1"], "readers": ["everyone"], "content": {"TL;DR": "", "title": "Loss-aware Binarization of Deep Networks", "abstract": "Deep neural network models, though very powerful and highly successful, are computationally expensive in terms of space and time. Recently, there have been a number of attempts on binarizing the network weights and activations. This greatly reduces the network size, and replaces the underlying multiplications to additions or even XNOR bit operations. However, existing binarization schemes are based on simple matrix approximations and ignore the effect of binarization on the loss. In this paper, we propose a proximal Newton algorithm with diagonal Hessian approximation that directly minimizes the loss w.r.t. the binarized weights. The underlying proximal step has an efficient closed-form solution, and the second-order information can be efficiently obtained from the second moments already computed by the Adam optimizer. Experiments on both feedforward and recurrent networks show that the proposed loss-aware binarization algorithm outperforms existing binarization schemes, and is also more robust for wide and deep networks.", "pdf": "/pdf/b9cdfec48f524ca349708a3a3b0cc6fcc10715d6.pdf", "paperhash": "hou|lossaware_binarization_of_deep_networks", "conflicts": ["cse.ust.hk"], "authorids": ["lhouab@cse.ust.hk", "qyaoaa@cse.ust.hk", "jamesk@cse.ust.hk"], "authors": ["Lu Hou", "Quanming Yao", "James T. Kwok"], "keywords": ["Deep learning", "Applications", "Optimization"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 21, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396430052, "tcdate": 1486396430052, "number": 1, "id": "Hk8fhMLOg", "invitation": "ICLR.cc/2017/conference/-/paper203/acceptance", "forum": "S1oWlN9ll", "replyto": "S1oWlN9ll", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"title": "ICLR committee final decision", "comment": "It's a simple contribution supported by empirical and theoretical analyses. After some discussion, all reviewers viewed the paper favourably.", "decision": "Accept (Poster)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Loss-aware Binarization of Deep Networks", "abstract": "Deep neural network models, though very powerful and highly successful, are computationally expensive in terms of space and time. Recently, there have been a number of attempts on binarizing the network weights and activations. This greatly reduces the network size, and replaces the underlying multiplications to additions or even XNOR bit operations. However, existing binarization schemes are based on simple matrix approximations and ignore the effect of binarization on the loss. In this paper, we propose a proximal Newton algorithm with diagonal Hessian approximation that directly minimizes the loss w.r.t. the binarized weights. The underlying proximal step has an efficient closed-form solution, and the second-order information can be efficiently obtained from the second moments already computed by the Adam optimizer. Experiments on both feedforward and recurrent networks show that the proposed loss-aware binarization algorithm outperforms existing binarization schemes, and is also more robust for wide and deep networks.", "pdf": "/pdf/b9cdfec48f524ca349708a3a3b0cc6fcc10715d6.pdf", "paperhash": "hou|lossaware_binarization_of_deep_networks", "conflicts": ["cse.ust.hk"], "authorids": ["lhouab@cse.ust.hk", "qyaoaa@cse.ust.hk", "jamesk@cse.ust.hk"], "authors": ["Lu Hou", "Quanming Yao", "James T. Kwok"], "keywords": ["Deep learning", "Applications", "Optimization"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396430561, "id": "ICLR.cc/2017/conference/-/paper203/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "S1oWlN9ll", "replyto": "S1oWlN9ll", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396430561}}}, {"tddate": null, "tmdate": 1484986900203, "tcdate": 1484986900203, "number": 2, "id": "SJ3Gq5xwl", "invitation": "ICLR.cc/2017/conference/-/paper203/official/comment", "forum": "S1oWlN9ll", "replyto": "rkRnyzHLl", "signatures": ["ICLR.cc/2017/conference/paper203/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper203/AnonReviewer1"], "content": {"title": "Comments after rebuttal stage", "comment": "I agree with the authors on the challenge of theoretical analysis for DNN. However, the authors should place more emphasis on the development of algorithm instead of the theoretical contribution. Overall, the paper makes solid contribution and additional experiments/plots are convincing and I decide to raise the score. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Loss-aware Binarization of Deep Networks", "abstract": "Deep neural network models, though very powerful and highly successful, are computationally expensive in terms of space and time. Recently, there have been a number of attempts on binarizing the network weights and activations. This greatly reduces the network size, and replaces the underlying multiplications to additions or even XNOR bit operations. However, existing binarization schemes are based on simple matrix approximations and ignore the effect of binarization on the loss. In this paper, we propose a proximal Newton algorithm with diagonal Hessian approximation that directly minimizes the loss w.r.t. the binarized weights. The underlying proximal step has an efficient closed-form solution, and the second-order information can be efficiently obtained from the second moments already computed by the Adam optimizer. Experiments on both feedforward and recurrent networks show that the proposed loss-aware binarization algorithm outperforms existing binarization schemes, and is also more robust for wide and deep networks.", "pdf": "/pdf/b9cdfec48f524ca349708a3a3b0cc6fcc10715d6.pdf", "paperhash": "hou|lossaware_binarization_of_deep_networks", "conflicts": ["cse.ust.hk"], "authorids": ["lhouab@cse.ust.hk", "qyaoaa@cse.ust.hk", "jamesk@cse.ust.hk"], "authors": ["Lu Hou", "Quanming Yao", "James T. Kwok"], "keywords": ["Deep learning", "Applications", "Optimization"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287687049, "id": "ICLR.cc/2017/conference/-/paper203/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "S1oWlN9ll", "writers": {"values-regex": "ICLR.cc/2017/conference/paper203/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper203/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper203/reviewers", "ICLR.cc/2017/conference/paper203/areachairs"], "cdate": 1485287687049}}}, {"tddate": null, "tmdate": 1484986713848, "tcdate": 1482823229980, "number": 3, "id": "S1IrUqyHg", "invitation": "ICLR.cc/2017/conference/-/paper203/official/review", "forum": "S1oWlN9ll", "replyto": "S1oWlN9ll", "signatures": ["ICLR.cc/2017/conference/paper203/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper203/AnonReviewer1"], "content": {"title": "", "rating": "7: Good paper, accept", "review": "This paper proposed a proximal (quasi-) Newton\u2019s method to learn binary DNN. The main contribution is to combine pre-conditioning with binarization in a proximal framework. It is interesting to have a proximal Newton\u2019s method to interpret the different DNN binarization schemes. This gives a new interpretation of existing approaches. However, the theoretical analysis is not very convincing or useful. The formulated optimization problem (3)-(4) is essentially a mixed integer programming. Even though the paper treats the integer part as a constraint and address it in proximal operators, the constraint set is still discrete and there is no guarantee that the proximal Newton algorithm could converge under practically useful conditions. In practice it is hard to verify the assumption [d_t^t]_k > \\beta in Theorem 3.1. This relation could be hard to hold in DNN as the loss surface could be extremely complicated.\n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Loss-aware Binarization of Deep Networks", "abstract": "Deep neural network models, though very powerful and highly successful, are computationally expensive in terms of space and time. Recently, there have been a number of attempts on binarizing the network weights and activations. This greatly reduces the network size, and replaces the underlying multiplications to additions or even XNOR bit operations. However, existing binarization schemes are based on simple matrix approximations and ignore the effect of binarization on the loss. In this paper, we propose a proximal Newton algorithm with diagonal Hessian approximation that directly minimizes the loss w.r.t. the binarized weights. The underlying proximal step has an efficient closed-form solution, and the second-order information can be efficiently obtained from the second moments already computed by the Adam optimizer. Experiments on both feedforward and recurrent networks show that the proposed loss-aware binarization algorithm outperforms existing binarization schemes, and is also more robust for wide and deep networks.", "pdf": "/pdf/b9cdfec48f524ca349708a3a3b0cc6fcc10715d6.pdf", "paperhash": "hou|lossaware_binarization_of_deep_networks", "conflicts": ["cse.ust.hk"], "authorids": ["lhouab@cse.ust.hk", "qyaoaa@cse.ust.hk", "jamesk@cse.ust.hk"], "authors": ["Lu Hou", "Quanming Yao", "James T. Kwok"], "keywords": ["Deep learning", "Applications", "Optimization"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482823230580, "id": "ICLR.cc/2017/conference/-/paper203/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper203/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper203/AnonReviewer2", "ICLR.cc/2017/conference/paper203/AnonReviewer3", "ICLR.cc/2017/conference/paper203/AnonReviewer1"], "reply": {"forum": "S1oWlN9ll", "replyto": "S1oWlN9ll", "writers": {"values-regex": "ICLR.cc/2017/conference/paper203/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper203/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482823230580}}}, {"tddate": null, "tmdate": 1484967358237, "tcdate": 1484967358237, "number": 15, "id": "S18TaSeDl", "invitation": "ICLR.cc/2017/conference/-/paper203/public/comment", "forum": "S1oWlN9ll", "replyto": "r1hYNZywe", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "Confused", "comment": "I'm confused now. The paper has not mentioned such \"transformation step\" and you have emphasized that \"BWN does not use ST estimator\" in the previous discussion.\n\nWhen we use end-to-end training, the optimization problem is to directly minimize the loss w.r.t the full precision weight, which is not the problem in (3) and (4).\n\nFor the proof, you can not define c_3 like this. There will possibly be an infinite number of steps and the overall minimum may converge to zero."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Loss-aware Binarization of Deep Networks", "abstract": "Deep neural network models, though very powerful and highly successful, are computationally expensive in terms of space and time. Recently, there have been a number of attempts on binarizing the network weights and activations. This greatly reduces the network size, and replaces the underlying multiplications to additions or even XNOR bit operations. However, existing binarization schemes are based on simple matrix approximations and ignore the effect of binarization on the loss. In this paper, we propose a proximal Newton algorithm with diagonal Hessian approximation that directly minimizes the loss w.r.t. the binarized weights. The underlying proximal step has an efficient closed-form solution, and the second-order information can be efficiently obtained from the second moments already computed by the Adam optimizer. Experiments on both feedforward and recurrent networks show that the proposed loss-aware binarization algorithm outperforms existing binarization schemes, and is also more robust for wide and deep networks.", "pdf": "/pdf/b9cdfec48f524ca349708a3a3b0cc6fcc10715d6.pdf", "paperhash": "hou|lossaware_binarization_of_deep_networks", "conflicts": ["cse.ust.hk"], "authorids": ["lhouab@cse.ust.hk", "qyaoaa@cse.ust.hk", "jamesk@cse.ust.hk"], "authors": ["Lu Hou", "Quanming Yao", "James T. Kwok"], "keywords": ["Deep learning", "Applications", "Optimization"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287687171, "id": "ICLR.cc/2017/conference/-/paper203/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "S1oWlN9ll", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper203/reviewers", "ICLR.cc/2017/conference/paper203/areachairs"], "cdate": 1485287687171}}}, {"tddate": null, "tmdate": 1484883105238, "tcdate": 1484883076432, "number": 14, "id": "r1hYNZywe", "invitation": "ICLR.cc/2017/conference/-/paper203/public/comment", "forum": "S1oWlN9ll", "replyto": "B1vXJ79Ug", "signatures": ["~LU_HOU1"], "readers": ["everyone"], "writers": ["~LU_HOU1"], "content": {"title": "reply", "comment": "- As discussed above, the problem with BWN is that the derivation of B(W) does not involve the loss function. Even with your interpretation of BWN as end-to-end training, it does not link the form of B(W) with the loss. As discussed above, learning of the W always *needs* to take the loss into account, and so this \"end-to-end training\" interpretation does not shed additional light. In other words, what we have been emphasizing throughout is that the derivation of B(W) in BWN is not loss-aware.\n\n- The BWN we compared is the original BWN. As discussed above, we used the straight-through estimator only on transforming the gradient w.r.t. \\tilde{W}_i to that w.r.t. to W_i.\n\n- The subscript t is forgotten in defining c_3. It can be slightly modified as \"Let c_3 = min_{k,l,t}([d_l^{t-1}]_k - \\beta) > 0 \\forall l,k,t\". Thanks for pointing this out. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Loss-aware Binarization of Deep Networks", "abstract": "Deep neural network models, though very powerful and highly successful, are computationally expensive in terms of space and time. Recently, there have been a number of attempts on binarizing the network weights and activations. This greatly reduces the network size, and replaces the underlying multiplications to additions or even XNOR bit operations. However, existing binarization schemes are based on simple matrix approximations and ignore the effect of binarization on the loss. In this paper, we propose a proximal Newton algorithm with diagonal Hessian approximation that directly minimizes the loss w.r.t. the binarized weights. The underlying proximal step has an efficient closed-form solution, and the second-order information can be efficiently obtained from the second moments already computed by the Adam optimizer. Experiments on both feedforward and recurrent networks show that the proposed loss-aware binarization algorithm outperforms existing binarization schemes, and is also more robust for wide and deep networks.", "pdf": "/pdf/b9cdfec48f524ca349708a3a3b0cc6fcc10715d6.pdf", "paperhash": "hou|lossaware_binarization_of_deep_networks", "conflicts": ["cse.ust.hk"], "authorids": ["lhouab@cse.ust.hk", "qyaoaa@cse.ust.hk", "jamesk@cse.ust.hk"], "authors": ["Lu Hou", "Quanming Yao", "James T. Kwok"], "keywords": ["Deep learning", "Applications", "Optimization"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287687171, "id": "ICLR.cc/2017/conference/-/paper203/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "S1oWlN9ll", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper203/reviewers", "ICLR.cc/2017/conference/paper203/areachairs"], "cdate": 1485287687171}}}, {"tddate": null, "tmdate": 1484562207553, "tcdate": 1484562207553, "number": 13, "id": "B1vXJ79Ug", "invitation": "ICLR.cc/2017/conference/-/paper203/public/comment", "forum": "S1oWlN9ll", "replyto": "BynBlfS8g", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "Emphasize the difference between the \"BWN\" in the paper and the original BWN + Proof of Theorem 3.1", "comment": "What I mean is that the optimization problem of BWN is min: l(B(W)) w.r.t W (See the previous comment \"Discussion about BWN\") and the straight-through estimator is used to pass back the gradient through the non-differentiable layer.\n\nThe \"BWN\" compared in the paper is not the original BWN and the author needs to emphasize the difference. Also, why not comparing with the original BWN on the same set of datasets?\n\nAlso, for c_3, the proof may be wrong even if we add back the superscript. We can have an counter example. Consider the case when \\sum c_3^t converges to a constant, the sequence of \\hat{w}_t can be like {w_a, w_b, w_a, w_b, ...}, which does not converge but will satisfy the final inequality."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Loss-aware Binarization of Deep Networks", "abstract": "Deep neural network models, though very powerful and highly successful, are computationally expensive in terms of space and time. Recently, there have been a number of attempts on binarizing the network weights and activations. This greatly reduces the network size, and replaces the underlying multiplications to additions or even XNOR bit operations. However, existing binarization schemes are based on simple matrix approximations and ignore the effect of binarization on the loss. In this paper, we propose a proximal Newton algorithm with diagonal Hessian approximation that directly minimizes the loss w.r.t. the binarized weights. The underlying proximal step has an efficient closed-form solution, and the second-order information can be efficiently obtained from the second moments already computed by the Adam optimizer. Experiments on both feedforward and recurrent networks show that the proposed loss-aware binarization algorithm outperforms existing binarization schemes, and is also more robust for wide and deep networks.", "pdf": "/pdf/b9cdfec48f524ca349708a3a3b0cc6fcc10715d6.pdf", "paperhash": "hou|lossaware_binarization_of_deep_networks", "conflicts": ["cse.ust.hk"], "authorids": ["lhouab@cse.ust.hk", "qyaoaa@cse.ust.hk", "jamesk@cse.ust.hk"], "authors": ["Lu Hou", "Quanming Yao", "James T. Kwok"], "keywords": ["Deep learning", "Applications", "Optimization"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287687171, "id": "ICLR.cc/2017/conference/-/paper203/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "S1oWlN9ll", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper203/reviewers", "ICLR.cc/2017/conference/paper203/areachairs"], "cdate": 1485287687171}}}, {"tddate": null, "tmdate": 1484230777557, "tcdate": 1484230582515, "number": 11, "id": "rkRnyzHLl", "invitation": "ICLR.cc/2017/conference/-/paper203/public/comment", "forum": "S1oWlN9ll", "replyto": "S1IrUqyHg", "signatures": ["~LU_HOU1"], "readers": ["everyone"], "writers": ["~LU_HOU1"], "content": {"title": "RE: AnonReviewer1's review", "comment": "Thank you for your thoughts.\n\n1. \"the constraint set is still discrete and there is no guarantee that the proximal Newton algorithm could converge under practically useful conditions. In practice it is hard to verify the assumption [d_t^l]_k > \\beta in Theorem 3.1.\"\n\n- Recall that \\ell is assumed to have Lipschitz-continuous gradient with modulus \\beta. If \\beta is known, we can adjust \\eta^t in step   21 so that \"[d_t^l]_k > \\beta\" holds.\n- In practice, \\beta may not be known. However, as deep networks are highly nonlinear, it is not unusual that convergence proofs need to make some assumptions, and our assumption can be considered as quite weak. For example, in the theoretical analysis of ADAM, the cost function is assumed to be convex (which is not possible in deep networks). \n- In practice, the proposed algorithm converges. As can be seen from the plots in https://www.dropbox.com/s/wkys181colu6yta/iclr_rebuttal.pdf?dl=0,  the training loss converges on all the data sets. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Loss-aware Binarization of Deep Networks", "abstract": "Deep neural network models, though very powerful and highly successful, are computationally expensive in terms of space and time. Recently, there have been a number of attempts on binarizing the network weights and activations. This greatly reduces the network size, and replaces the underlying multiplications to additions or even XNOR bit operations. However, existing binarization schemes are based on simple matrix approximations and ignore the effect of binarization on the loss. In this paper, we propose a proximal Newton algorithm with diagonal Hessian approximation that directly minimizes the loss w.r.t. the binarized weights. The underlying proximal step has an efficient closed-form solution, and the second-order information can be efficiently obtained from the second moments already computed by the Adam optimizer. Experiments on both feedforward and recurrent networks show that the proposed loss-aware binarization algorithm outperforms existing binarization schemes, and is also more robust for wide and deep networks.", "pdf": "/pdf/b9cdfec48f524ca349708a3a3b0cc6fcc10715d6.pdf", "paperhash": "hou|lossaware_binarization_of_deep_networks", "conflicts": ["cse.ust.hk"], "authorids": ["lhouab@cse.ust.hk", "qyaoaa@cse.ust.hk", "jamesk@cse.ust.hk"], "authors": ["Lu Hou", "Quanming Yao", "James T. Kwok"], "keywords": ["Deep learning", "Applications", "Optimization"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287687171, "id": "ICLR.cc/2017/conference/-/paper203/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "S1oWlN9ll", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper203/reviewers", "ICLR.cc/2017/conference/paper203/areachairs"], "cdate": 1485287687171}}}, {"tddate": null, "tmdate": 1484230724236, "tcdate": 1484230724236, "number": 12, "id": "BynBlfS8g", "invitation": "ICLR.cc/2017/conference/-/paper203/public/comment", "forum": "S1oWlN9ll", "replyto": "r1mM_XCEg", "signatures": ["~LU_HOU1"], "readers": ["everyone"], "writers": ["~LU_HOU1"], "content": {"title": "reply", "comment": "- Note that the core idea of BWN is on how to find the \\alpha and B. The straight-through estimator is not used in the derivation of this binarization operation, and is also not used in forward propagation and backpropagation of the gradients (w.r.t. the binarized weights). The straight-through estimator is only used to transform the gradient w.r.t. \\tilde{W}_i to that w.r.t. to W_i (here, we follow the BWN paper notation). \n\n- As for c_3, we can add back a superscript for clarity, but that does not affect the argument.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Loss-aware Binarization of Deep Networks", "abstract": "Deep neural network models, though very powerful and highly successful, are computationally expensive in terms of space and time. Recently, there have been a number of attempts on binarizing the network weights and activations. This greatly reduces the network size, and replaces the underlying multiplications to additions or even XNOR bit operations. However, existing binarization schemes are based on simple matrix approximations and ignore the effect of binarization on the loss. In this paper, we propose a proximal Newton algorithm with diagonal Hessian approximation that directly minimizes the loss w.r.t. the binarized weights. The underlying proximal step has an efficient closed-form solution, and the second-order information can be efficiently obtained from the second moments already computed by the Adam optimizer. Experiments on both feedforward and recurrent networks show that the proposed loss-aware binarization algorithm outperforms existing binarization schemes, and is also more robust for wide and deep networks.", "pdf": "/pdf/b9cdfec48f524ca349708a3a3b0cc6fcc10715d6.pdf", "paperhash": "hou|lossaware_binarization_of_deep_networks", "conflicts": ["cse.ust.hk"], "authorids": ["lhouab@cse.ust.hk", "qyaoaa@cse.ust.hk", "jamesk@cse.ust.hk"], "authors": ["Lu Hou", "Quanming Yao", "James T. Kwok"], "keywords": ["Deep learning", "Applications", "Optimization"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287687171, "id": "ICLR.cc/2017/conference/-/paper203/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "S1oWlN9ll", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper203/reviewers", "ICLR.cc/2017/conference/paper203/areachairs"], "cdate": 1485287687171}}}, {"tddate": null, "tmdate": 1482729482854, "tcdate": 1482729482854, "number": 10, "id": "r1mM_XCEg", "invitation": "ICLR.cc/2017/conference/-/paper203/public/comment", "forum": "S1oWlN9ll", "replyto": "SyyLLDTVg", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "To make two of my points clear", "comment": "1. The paper has wrongly interpreted BWN in the related work. The wrong interpretation will certainly mislead the reader.\nBWN DOES USE the \"straight-through estimator\". However, the paper has not mentioned a single word of the technique when introducing BWN in the related work.\nIn fact, as clearly stated in Section 3.1, Paragraph \"Training Binary-Weights-Networks\" of [1], BWN passes back the gradient of the non-differentiable binarization layer through the \"straight-through estimator\". The implementation here https://github.com/allenai/XNOR-Net/blob/master/util.lua#L78 also shows this.\nI think it's the very basic of an academic paper to precisely state and review the related works.\n\n2. Proof of Theorem 3.1 is wrong unless some further assumptions are made for the sequence $d_l^t$. The $c_3$ in formula (11) of the paper should actually be $c_3^{t-1}$ since there is no assumption that $d_l^t$ will be fixed.\n\n[1] M. Rastegari, V. Ordonez, J. Redmon, and A. Farhadi. XNOR-Net: ImageNet classification using\nbinary convolutional neural networks. In ECCV, 2016."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Loss-aware Binarization of Deep Networks", "abstract": "Deep neural network models, though very powerful and highly successful, are computationally expensive in terms of space and time. Recently, there have been a number of attempts on binarizing the network weights and activations. This greatly reduces the network size, and replaces the underlying multiplications to additions or even XNOR bit operations. However, existing binarization schemes are based on simple matrix approximations and ignore the effect of binarization on the loss. In this paper, we propose a proximal Newton algorithm with diagonal Hessian approximation that directly minimizes the loss w.r.t. the binarized weights. The underlying proximal step has an efficient closed-form solution, and the second-order information can be efficiently obtained from the second moments already computed by the Adam optimizer. Experiments on both feedforward and recurrent networks show that the proposed loss-aware binarization algorithm outperforms existing binarization schemes, and is also more robust for wide and deep networks.", "pdf": "/pdf/b9cdfec48f524ca349708a3a3b0cc6fcc10715d6.pdf", "paperhash": "hou|lossaware_binarization_of_deep_networks", "conflicts": ["cse.ust.hk"], "authorids": ["lhouab@cse.ust.hk", "qyaoaa@cse.ust.hk", "jamesk@cse.ust.hk"], "authors": ["Lu Hou", "Quanming Yao", "James T. Kwok"], "keywords": ["Deep learning", "Applications", "Optimization"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287687171, "id": "ICLR.cc/2017/conference/-/paper203/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "S1oWlN9ll", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper203/reviewers", "ICLR.cc/2017/conference/paper203/areachairs"], "cdate": 1485287687171}}}, {"tddate": null, "tmdate": 1482679878946, "tcdate": 1482679878946, "number": 9, "id": "SyyLLDTVg", "invitation": "ICLR.cc/2017/conference/-/paper203/public/comment", "forum": "S1oWlN9ll", "replyto": "Hknx3Nc4l", "signatures": ["~LU_HOU1"], "readers": ["everyone"], "writers": ["~LU_HOU1"], "content": {"title": "RE: Discussion about BWN", "comment": "As discussed in the paper and also the reply above, BWN simply minimizes the difference between the real-valued weight w and the binary weight \\alpha b, while the proposed method also takes the loss function into account on deriving the optimal \\alpha and b.\n\nAs for discussions on the related works (e.g., on how BWN works and whether the necessary condition in (Pascanu et al., 2013) is \"appropriate to use it as a formal theoretical analysis\"), we'd suggest discussing it in some online forum or email the corresponding authors directly, rather than discussing it in this paper review platform. \n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Loss-aware Binarization of Deep Networks", "abstract": "Deep neural network models, though very powerful and highly successful, are computationally expensive in terms of space and time. Recently, there have been a number of attempts on binarizing the network weights and activations. This greatly reduces the network size, and replaces the underlying multiplications to additions or even XNOR bit operations. However, existing binarization schemes are based on simple matrix approximations and ignore the effect of binarization on the loss. In this paper, we propose a proximal Newton algorithm with diagonal Hessian approximation that directly minimizes the loss w.r.t. the binarized weights. The underlying proximal step has an efficient closed-form solution, and the second-order information can be efficiently obtained from the second moments already computed by the Adam optimizer. Experiments on both feedforward and recurrent networks show that the proposed loss-aware binarization algorithm outperforms existing binarization schemes, and is also more robust for wide and deep networks.", "pdf": "/pdf/b9cdfec48f524ca349708a3a3b0cc6fcc10715d6.pdf", "paperhash": "hou|lossaware_binarization_of_deep_networks", "conflicts": ["cse.ust.hk"], "authorids": ["lhouab@cse.ust.hk", "qyaoaa@cse.ust.hk", "jamesk@cse.ust.hk"], "authors": ["Lu Hou", "Quanming Yao", "James T. Kwok"], "keywords": ["Deep learning", "Applications", "Optimization"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287687171, "id": "ICLR.cc/2017/conference/-/paper203/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "S1oWlN9ll", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper203/reviewers", "ICLR.cc/2017/conference/paper203/areachairs"], "cdate": 1485287687171}}}, {"tddate": null, "tmdate": 1482475084059, "tcdate": 1482331579669, "number": 4, "id": "SyE6rG_Ex", "invitation": "ICLR.cc/2017/conference/-/paper203/public/comment", "forum": "S1oWlN9ll", "replyto": "ryHPrLy4e", "signatures": ["~LU_HOU1"], "readers": ["everyone"], "writers": ["~LU_HOU1"], "content": {"title": "RE: AnonReviewer2's review", "comment": "Thanks for your review and suggestions, for the questions:\n\n1. \"Is it necessary to discuss exploding/vanishing gradients when the RNN experiments are carried out by an LSTM, and handled by the cell error carousel? We see the desire to tie into proposition 2, but not clear that the degradation we see in the binary connect is related.\"\n\n- LSTM can handle the vanishing gradient problem, but does not address the the exploding gradient problem explicitly (see \"On the difficulty of training recurrent neural networks.\" ICML 2013). \n- The gradients of the LSTM are provided in Appendix E. As can be seen, the backpropagated gradients of LSTM still take the form of a      product of the Jacobian matrices like a vanilla RNN, and may explode when the scale of the weight matrix is large.\n- Proposition 2 suggests that a binary weight matrix (as produced by BinaryConnect) is more likely to suffer from the exploding gradient problem. \n- Empirically, for LSTM (with TS=100) on the \"War and Peace\" dataset, the gradient values obtained by the full-precision net, BWN and BPN are usually in the range [10^{-6}, 10^{-3}]. However, the gradient values obtained by BinaryConnect are [10^{-4}, 10^{-1}] when the gradients are clipped after each time step(Figure 1.(b)), and in [10^{28}, 10^{32}]when the gradients are clipped at the end of each back pass(Figure 2 in Appendix D.1). Hence, the binary LSTM produced by BinaryConnect suffers from the exploding gradients problem.\n- We also did a preliminary experiment that scales the binarized matrix (from BinaryConnect) with a scaling factor \\alpha < 1. The following shows the testing cross-entropy values on the \"War and Peace\" dataset.\n\\alpha=1(original): 2.942\n\\alpha=0.1:\t    1.543\n\\alpha=0.2:         1.422\n\\alpha=0.3:         1.441\nAs can be seen, using an appropriate scaling can improve performance. On the other hand, the proposed binarization scheme can learn this \\alpha automatically (section 3.2), and leads to an even better testing cross-entropy value of 1.291.\n\n2. \"Adam is used in the LSTM optimization, was gradient clipping really needed, or is the degradation of binary connect simply related to capacity?\"\n\n- All the network models share the same architecture and so have the same capacity. \n- We also tried varying the clipping threshold for BinaryConnect. Results can be found in Appendix D.1. As can be seen from Table 5 there, this brings little improvement to the performance of BinaryConnect.\n\nThanks again for your time and efforts."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Loss-aware Binarization of Deep Networks", "abstract": "Deep neural network models, though very powerful and highly successful, are computationally expensive in terms of space and time. Recently, there have been a number of attempts on binarizing the network weights and activations. This greatly reduces the network size, and replaces the underlying multiplications to additions or even XNOR bit operations. However, existing binarization schemes are based on simple matrix approximations and ignore the effect of binarization on the loss. In this paper, we propose a proximal Newton algorithm with diagonal Hessian approximation that directly minimizes the loss w.r.t. the binarized weights. The underlying proximal step has an efficient closed-form solution, and the second-order information can be efficiently obtained from the second moments already computed by the Adam optimizer. Experiments on both feedforward and recurrent networks show that the proposed loss-aware binarization algorithm outperforms existing binarization schemes, and is also more robust for wide and deep networks.", "pdf": "/pdf/b9cdfec48f524ca349708a3a3b0cc6fcc10715d6.pdf", "paperhash": "hou|lossaware_binarization_of_deep_networks", "conflicts": ["cse.ust.hk"], "authorids": ["lhouab@cse.ust.hk", "qyaoaa@cse.ust.hk", "jamesk@cse.ust.hk"], "authors": ["Lu Hou", "Quanming Yao", "James T. Kwok"], "keywords": ["Deep learning", "Applications", "Optimization"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287687171, "id": "ICLR.cc/2017/conference/-/paper203/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "S1oWlN9ll", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper203/reviewers", "ICLR.cc/2017/conference/paper203/areachairs"], "cdate": 1485287687171}}}, {"tddate": null, "tmdate": 1482472436177, "tcdate": 1482472436177, "number": 8, "id": "Hknx3Nc4l", "invitation": "ICLR.cc/2017/conference/-/paper203/public/comment", "forum": "S1oWlN9ll", "replyto": "BJIYFDKNe", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "Discussion about BWN ", "comment": "Here's my understanding of BWN:\nAssume we have possessed a full-precision weight W. The binarization process is to use a deterministic mapping B(W) to project the weight to the space of valid binary matrices.\nFor BWN, the binarization method is chosen to be B(W) = L1(W)/n sgn(W) and the training process is to minimize l(B(W)) w.r.t W. For training, we will first calculate the gradient w.r.t B(W) and then backprop through the non-differentiable layer B using the \"straight-through estimator\". We can refer to Section 3.1(Training BWN) of the XNOR-Net and also the implementation  https://github.com/allenai/XNOR-Net/blob/master/util.lua#L78 + the discussion https://github.com/allenai/XNOR-Net/issues/3. If we adopt this understanding, BWN is like to insert a binarization layer in the network and train the whole stuff end-to-end. Some possible error caused by binarization should be considered through the end2end training.\n\nIn the major optimization problem (5) discussed in the paper, there is nothing like full-precision matrix. I do not know how to define \"loss caused by binarization\" if we directly use (5). I think we should also add the full precision matrix W to the main problem if we want to discuss \"loss caused by binarization\".\n\nFor the proof, what I mean is that it may require some more assumptions, like the choice of the selected \\eta^t to hold.\n\nFor the theoretical analysis of the exploding gradient problem, I'm saying that a necessary condition is not sufficient and it's not so appropriate to use it as a formal theoretical analysis."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Loss-aware Binarization of Deep Networks", "abstract": "Deep neural network models, though very powerful and highly successful, are computationally expensive in terms of space and time. Recently, there have been a number of attempts on binarizing the network weights and activations. This greatly reduces the network size, and replaces the underlying multiplications to additions or even XNOR bit operations. However, existing binarization schemes are based on simple matrix approximations and ignore the effect of binarization on the loss. In this paper, we propose a proximal Newton algorithm with diagonal Hessian approximation that directly minimizes the loss w.r.t. the binarized weights. The underlying proximal step has an efficient closed-form solution, and the second-order information can be efficiently obtained from the second moments already computed by the Adam optimizer. Experiments on both feedforward and recurrent networks show that the proposed loss-aware binarization algorithm outperforms existing binarization schemes, and is also more robust for wide and deep networks.", "pdf": "/pdf/b9cdfec48f524ca349708a3a3b0cc6fcc10715d6.pdf", "paperhash": "hou|lossaware_binarization_of_deep_networks", "conflicts": ["cse.ust.hk"], "authorids": ["lhouab@cse.ust.hk", "qyaoaa@cse.ust.hk", "jamesk@cse.ust.hk"], "authors": ["Lu Hou", "Quanming Yao", "James T. Kwok"], "keywords": ["Deep learning", "Applications", "Optimization"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287687171, "id": "ICLR.cc/2017/conference/-/paper203/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "S1oWlN9ll", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper203/reviewers", "ICLR.cc/2017/conference/paper203/areachairs"], "cdate": 1485287687171}}}, {"tddate": null, "tmdate": 1482457592748, "tcdate": 1482418558120, "number": 7, "id": "BJIYFDKNe", "invitation": "ICLR.cc/2017/conference/-/paper203/public/comment", "forum": "S1oWlN9ll", "replyto": "ryJaB4uNx", "signatures": ["~LU_HOU1"], "readers": ["everyone"], "writers": ["~LU_HOU1"], "content": {"title": "RE: Some more questions", "comment": "Unfortunately, apparently you have quite some misunderstandings about the BWN paper and our submission.\n- \"Loss-aware\" means taking the loss into account during *binarization*. On the other hand, all methods, no matter using binarization or not or what form of binarization, have to take the loss into account during weight update. \n- BWN does NOT use straight-through estimator. We (and papers like (Courbariaux & Bengio., 2016)) need the the straight-through estimator so as to propagate the gradients through the sign function. When only the weights are binarized (as in BWN and our BPN), the activations are not binarized and so there is no sign function.\n- For c_3, we already assumed that it is positive (see the first sentence in Theorem 3.1). Empirically, this can be guaranteed by controlling the learning rate \\eta^t (see step 22 of Algorithm 1).\n- For vanishing gradients, the condition provided in (Pascanu et al., 2013) and this paper is a sufficient condition; whereas for exploding gradient, the condition is a necessary condition. For details, please read section 2.1 of (Pascanu et al., 2013).\n- For implementation, as discussed above, there is no need for the straight-through estimator when only the weights are binarized. Also, we only binarize the weights during the forward and backward propagations, but not during parameter update. This follows the suggestion in (Courbariaux et al., 2015). For details, please read that paper.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Loss-aware Binarization of Deep Networks", "abstract": "Deep neural network models, though very powerful and highly successful, are computationally expensive in terms of space and time. Recently, there have been a number of attempts on binarizing the network weights and activations. This greatly reduces the network size, and replaces the underlying multiplications to additions or even XNOR bit operations. However, existing binarization schemes are based on simple matrix approximations and ignore the effect of binarization on the loss. In this paper, we propose a proximal Newton algorithm with diagonal Hessian approximation that directly minimizes the loss w.r.t. the binarized weights. The underlying proximal step has an efficient closed-form solution, and the second-order information can be efficiently obtained from the second moments already computed by the Adam optimizer. Experiments on both feedforward and recurrent networks show that the proposed loss-aware binarization algorithm outperforms existing binarization schemes, and is also more robust for wide and deep networks.", "pdf": "/pdf/b9cdfec48f524ca349708a3a3b0cc6fcc10715d6.pdf", "paperhash": "hou|lossaware_binarization_of_deep_networks", "conflicts": ["cse.ust.hk"], "authorids": ["lhouab@cse.ust.hk", "qyaoaa@cse.ust.hk", "jamesk@cse.ust.hk"], "authors": ["Lu Hou", "Quanming Yao", "James T. Kwok"], "keywords": ["Deep learning", "Applications", "Optimization"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287687171, "id": "ICLR.cc/2017/conference/-/paper203/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "S1oWlN9ll", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper203/reviewers", "ICLR.cc/2017/conference/paper203/areachairs"], "cdate": 1485287687171}}}, {"tddate": null, "tmdate": 1482339767236, "tcdate": 1482339767236, "number": 6, "id": "ryJaB4uNx", "invitation": "ICLR.cc/2017/conference/-/paper203/public/comment", "forum": "S1oWlN9ll", "replyto": "SkvJFaP4x", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "Some more questions", "comment": "Q1: Meaning of \"loss-aware\"\nI'm not so clear what \"loss-aware\" means. I feel that BWN may also be \"loss-aware\" since it tries to directly calculate the gradient of the loss function w.r.t the original weights where the gradient of sign is approximated by the \"straight-through estimator\" [1].\n\nQ2: About the main optimization problem and the proof of Theorem 3.1\nThe main optimization problem discussed in the paper seems to be (5). According to Remark 3.1, we can regard the Binarization step in BWN as the proximal operator. BWN and BPN are viewed as solving the same objective and Theorem 3.1 should be the major theoretical result. Does the constant c3 in (11) remain positive and the same along the training process so we can sum up the inequalities?\n\nQ3: Exploding gradient\nThe condition of the largest singular value seems to be a necessary condition, which means that the long-term gradient will vanish if it is not satisfied [2]. It's not so appropriate to use arguments like Proposition 3.2 to analyze the problem of exploding gradient.\n\nQ4: Implementation\nI find that in the implementation, (7) is not used and the binary weight is replaced by the full-precision weight. Will this process break Theorem 3.1? Also, is \"straight-through estimator\" not used when only the weights are binarized?\n\n[1] M. Courbariaux and Y. Bengio, BinaryNet: Training deep neural networks with weights and activations constrained to +1 or \u22121. Arxiv. 2016\n[2] R. Pascanu, T. Mikolov, and Y. Bengio. On the difficulty of training recurrent neural networks. ICML. 2013"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Loss-aware Binarization of Deep Networks", "abstract": "Deep neural network models, though very powerful and highly successful, are computationally expensive in terms of space and time. Recently, there have been a number of attempts on binarizing the network weights and activations. This greatly reduces the network size, and replaces the underlying multiplications to additions or even XNOR bit operations. However, existing binarization schemes are based on simple matrix approximations and ignore the effect of binarization on the loss. In this paper, we propose a proximal Newton algorithm with diagonal Hessian approximation that directly minimizes the loss w.r.t. the binarized weights. The underlying proximal step has an efficient closed-form solution, and the second-order information can be efficiently obtained from the second moments already computed by the Adam optimizer. Experiments on both feedforward and recurrent networks show that the proposed loss-aware binarization algorithm outperforms existing binarization schemes, and is also more robust for wide and deep networks.", "pdf": "/pdf/b9cdfec48f524ca349708a3a3b0cc6fcc10715d6.pdf", "paperhash": "hou|lossaware_binarization_of_deep_networks", "conflicts": ["cse.ust.hk"], "authorids": ["lhouab@cse.ust.hk", "qyaoaa@cse.ust.hk", "jamesk@cse.ust.hk"], "authors": ["Lu Hou", "Quanming Yao", "James T. Kwok"], "keywords": ["Deep learning", "Applications", "Optimization"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287687171, "id": "ICLR.cc/2017/conference/-/paper203/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "S1oWlN9ll", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper203/reviewers", "ICLR.cc/2017/conference/paper203/areachairs"], "cdate": 1485287687171}}}, {"tddate": null, "tmdate": 1482331789347, "tcdate": 1482331789347, "number": 5, "id": "r1H58zuVx", "invitation": "ICLR.cc/2017/conference/-/paper203/public/comment", "forum": "S1oWlN9ll", "replyto": "H1ake7INx", "signatures": ["~LU_HOU1"], "readers": ["everyone"], "writers": ["~LU_HOU1"], "content": {"title": "RE: AnonReviewer3 review", "comment": "Thanks for your review and suggestions, we will add the suggested experiments in the future."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Loss-aware Binarization of Deep Networks", "abstract": "Deep neural network models, though very powerful and highly successful, are computationally expensive in terms of space and time. Recently, there have been a number of attempts on binarizing the network weights and activations. This greatly reduces the network size, and replaces the underlying multiplications to additions or even XNOR bit operations. However, existing binarization schemes are based on simple matrix approximations and ignore the effect of binarization on the loss. In this paper, we propose a proximal Newton algorithm with diagonal Hessian approximation that directly minimizes the loss w.r.t. the binarized weights. The underlying proximal step has an efficient closed-form solution, and the second-order information can be efficiently obtained from the second moments already computed by the Adam optimizer. Experiments on both feedforward and recurrent networks show that the proposed loss-aware binarization algorithm outperforms existing binarization schemes, and is also more robust for wide and deep networks.", "pdf": "/pdf/b9cdfec48f524ca349708a3a3b0cc6fcc10715d6.pdf", "paperhash": "hou|lossaware_binarization_of_deep_networks", "conflicts": ["cse.ust.hk"], "authorids": ["lhouab@cse.ust.hk", "qyaoaa@cse.ust.hk", "jamesk@cse.ust.hk"], "authors": ["Lu Hou", "Quanming Yao", "James T. Kwok"], "keywords": ["Deep learning", "Applications", "Optimization"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287687171, "id": "ICLR.cc/2017/conference/-/paper203/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "S1oWlN9ll", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper203/reviewers", "ICLR.cc/2017/conference/paper203/areachairs"], "cdate": 1485287687171}}}, {"tddate": null, "tmdate": 1482311902744, "tcdate": 1482311902744, "number": 3, "id": "SkvJFaP4x", "invitation": "ICLR.cc/2017/conference/-/paper203/public/comment", "forum": "S1oWlN9ll", "replyto": "SySt0X8Ee", "signatures": ["~LU_HOU1"], "readers": ["everyone"], "writers": ["~LU_HOU1"], "content": {"title": "RE: Is BWN also loss-aware?", "comment": "Thanks for your question, you may misunderstand BWN.  BWN simply minimizes the difference between the real-valued weight and the binarized weight. The loss is not considered. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Loss-aware Binarization of Deep Networks", "abstract": "Deep neural network models, though very powerful and highly successful, are computationally expensive in terms of space and time. Recently, there have been a number of attempts on binarizing the network weights and activations. This greatly reduces the network size, and replaces the underlying multiplications to additions or even XNOR bit operations. However, existing binarization schemes are based on simple matrix approximations and ignore the effect of binarization on the loss. In this paper, we propose a proximal Newton algorithm with diagonal Hessian approximation that directly minimizes the loss w.r.t. the binarized weights. The underlying proximal step has an efficient closed-form solution, and the second-order information can be efficiently obtained from the second moments already computed by the Adam optimizer. Experiments on both feedforward and recurrent networks show that the proposed loss-aware binarization algorithm outperforms existing binarization schemes, and is also more robust for wide and deep networks.", "pdf": "/pdf/b9cdfec48f524ca349708a3a3b0cc6fcc10715d6.pdf", "paperhash": "hou|lossaware_binarization_of_deep_networks", "conflicts": ["cse.ust.hk"], "authorids": ["lhouab@cse.ust.hk", "qyaoaa@cse.ust.hk", "jamesk@cse.ust.hk"], "authors": ["Lu Hou", "Quanming Yao", "James T. Kwok"], "keywords": ["Deep learning", "Applications", "Optimization"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287687171, "id": "ICLR.cc/2017/conference/-/paper203/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "S1oWlN9ll", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper203/reviewers", "ICLR.cc/2017/conference/paper203/areachairs"], "cdate": 1485287687171}}}, {"tddate": null, "tmdate": 1482206845322, "tcdate": 1482206845322, "number": 2, "id": "SySt0X8Ee", "invitation": "ICLR.cc/2017/conference/-/paper203/public/comment", "forum": "S1oWlN9ll", "replyto": "S1oWlN9ll", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "Is BWN also loss-aware?", "comment": "It seems not so fair to claim that the paper is the first \"loss-aware\" approach. The same optimization objective as BWN is used and the only difference is that the paper uses proximal Newton method instead of proximal gradient descent. It will be more convincing if the author could compare more with BWN. (In fact, BWN also has good performance in RNN)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Loss-aware Binarization of Deep Networks", "abstract": "Deep neural network models, though very powerful and highly successful, are computationally expensive in terms of space and time. Recently, there have been a number of attempts on binarizing the network weights and activations. This greatly reduces the network size, and replaces the underlying multiplications to additions or even XNOR bit operations. However, existing binarization schemes are based on simple matrix approximations and ignore the effect of binarization on the loss. In this paper, we propose a proximal Newton algorithm with diagonal Hessian approximation that directly minimizes the loss w.r.t. the binarized weights. The underlying proximal step has an efficient closed-form solution, and the second-order information can be efficiently obtained from the second moments already computed by the Adam optimizer. Experiments on both feedforward and recurrent networks show that the proposed loss-aware binarization algorithm outperforms existing binarization schemes, and is also more robust for wide and deep networks.", "pdf": "/pdf/b9cdfec48f524ca349708a3a3b0cc6fcc10715d6.pdf", "paperhash": "hou|lossaware_binarization_of_deep_networks", "conflicts": ["cse.ust.hk"], "authorids": ["lhouab@cse.ust.hk", "qyaoaa@cse.ust.hk", "jamesk@cse.ust.hk"], "authors": ["Lu Hou", "Quanming Yao", "James T. Kwok"], "keywords": ["Deep learning", "Applications", "Optimization"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287687171, "id": "ICLR.cc/2017/conference/-/paper203/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "S1oWlN9ll", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper203/reviewers", "ICLR.cc/2017/conference/paper203/areachairs"], "cdate": 1485287687171}}}, {"tddate": null, "tmdate": 1482203108737, "tcdate": 1482203108737, "number": 2, "id": "H1ake7INx", "invitation": "ICLR.cc/2017/conference/-/paper203/official/review", "forum": "S1oWlN9ll", "replyto": "S1oWlN9ll", "signatures": ["ICLR.cc/2017/conference/paper203/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper203/AnonReviewer3"], "content": {"title": "Novel 2nd order loss-aware binarization method for neural networks. Optimization performance is evaluated through the test error proxy.", "rating": "7: Good paper, accept", "review": "The paper presents a second-order method for training a neural networks while ensuring at the same time that weights (and activations) are binary. Through binarization, the method aims to achieve model compression for subsequent deployment on low-memory systems. The method is abbreviated BPN for \"binarization using proximal Newton algorithm\".\n\nThe method incorporates the supervised loss function directly in the binarization procedure, which is an important and desirable property. (Authors mention that existing weight binarization methods ignore the effect of binarization to the loss.) The method is clearly described and related analytically to the previously proposed weight binarization methods.\n\nThe experiments are extensive with multiple datasets and architectures, and demonstrate the generally higher performance of the proposed approach.\n\nA minor issue with the feed-forward network experiments is that only test errors are reported. Such information does not really give evidence for the higher optimization performance. (see also comment \"RE: AnonReviewer3's questions\" stating that all baselines achieve near perfect training accuracy.) Making the optimization problem harder (e.g. by including an explicit regularizer into the training objective, or by using a data extension scheme), and monitoring the training objective instead of the test error could be a more direct way of demonstrating superior optimization performance.\n\nThe superiority of BPN is however becoming more clearly apparent in the subsequent LSTM experiments.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Loss-aware Binarization of Deep Networks", "abstract": "Deep neural network models, though very powerful and highly successful, are computationally expensive in terms of space and time. Recently, there have been a number of attempts on binarizing the network weights and activations. This greatly reduces the network size, and replaces the underlying multiplications to additions or even XNOR bit operations. However, existing binarization schemes are based on simple matrix approximations and ignore the effect of binarization on the loss. In this paper, we propose a proximal Newton algorithm with diagonal Hessian approximation that directly minimizes the loss w.r.t. the binarized weights. The underlying proximal step has an efficient closed-form solution, and the second-order information can be efficiently obtained from the second moments already computed by the Adam optimizer. Experiments on both feedforward and recurrent networks show that the proposed loss-aware binarization algorithm outperforms existing binarization schemes, and is also more robust for wide and deep networks.", "pdf": "/pdf/b9cdfec48f524ca349708a3a3b0cc6fcc10715d6.pdf", "paperhash": "hou|lossaware_binarization_of_deep_networks", "conflicts": ["cse.ust.hk"], "authorids": ["lhouab@cse.ust.hk", "qyaoaa@cse.ust.hk", "jamesk@cse.ust.hk"], "authors": ["Lu Hou", "Quanming Yao", "James T. Kwok"], "keywords": ["Deep learning", "Applications", "Optimization"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482823230580, "id": "ICLR.cc/2017/conference/-/paper203/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper203/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper203/AnonReviewer2", "ICLR.cc/2017/conference/paper203/AnonReviewer3", "ICLR.cc/2017/conference/paper203/AnonReviewer1"], "reply": {"forum": "S1oWlN9ll", "replyto": "S1oWlN9ll", "writers": {"values-regex": "ICLR.cc/2017/conference/paper203/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper203/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482823230580}}}, {"tddate": null, "tmdate": 1481758045377, "tcdate": 1481758045368, "number": 1, "id": "ryHPrLy4e", "invitation": "ICLR.cc/2017/conference/-/paper203/official/review", "forum": "S1oWlN9ll", "replyto": "S1oWlN9ll", "signatures": ["ICLR.cc/2017/conference/paper203/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper203/AnonReviewer2"], "content": {"title": "Inclusion of loss in the binarization of deep networks using a proximal Newton algorithm, where the proximal term is an indicator function for the binarized weights. This is the reasonable next step on the recently published works for training and compressing the network simultaneously (Binary Connect and Binary weight Network). Shown that it outperforms those binarization schemes on Mnist, svhn and RNN LM task ", "rating": "7: Good paper, accept", "review": "Taking into account the loss in the binarization step through a proximal Newton algorithm is a nice idea. This is at least one approach to bringing in the missing loss in the binarization step, which has recently gone from a two step process of train and binarize to a single step simultaneous train/compress. Performance on a few small tasks show the benefit. It would be nice to see some results on substantial networks and tasks which really need compression on embedded systems (a point made in the introduction). Is it necessary to discuss exploding/vanishing gradients when the RNN experiments are carried out by an LSTM, and handled by the cell error carousel? We see the desire to tie into proposition 2, but not clear that the degradation we see in the binary connect is related. Adam is used in the LSTM optimization, was gradient clipping really needed, or is the degradation of binary connect simply related to capacity? For proposition 3.1, theorem 3.1 and proposition 3.2 put the pointers to proofs in appendix.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Loss-aware Binarization of Deep Networks", "abstract": "Deep neural network models, though very powerful and highly successful, are computationally expensive in terms of space and time. Recently, there have been a number of attempts on binarizing the network weights and activations. This greatly reduces the network size, and replaces the underlying multiplications to additions or even XNOR bit operations. However, existing binarization schemes are based on simple matrix approximations and ignore the effect of binarization on the loss. In this paper, we propose a proximal Newton algorithm with diagonal Hessian approximation that directly minimizes the loss w.r.t. the binarized weights. The underlying proximal step has an efficient closed-form solution, and the second-order information can be efficiently obtained from the second moments already computed by the Adam optimizer. Experiments on both feedforward and recurrent networks show that the proposed loss-aware binarization algorithm outperforms existing binarization schemes, and is also more robust for wide and deep networks.", "pdf": "/pdf/b9cdfec48f524ca349708a3a3b0cc6fcc10715d6.pdf", "paperhash": "hou|lossaware_binarization_of_deep_networks", "conflicts": ["cse.ust.hk"], "authorids": ["lhouab@cse.ust.hk", "qyaoaa@cse.ust.hk", "jamesk@cse.ust.hk"], "authors": ["Lu Hou", "Quanming Yao", "James T. Kwok"], "keywords": ["Deep learning", "Applications", "Optimization"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482823230580, "id": "ICLR.cc/2017/conference/-/paper203/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper203/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper203/AnonReviewer2", "ICLR.cc/2017/conference/paper203/AnonReviewer3", "ICLR.cc/2017/conference/paper203/AnonReviewer1"], "reply": {"forum": "S1oWlN9ll", "replyto": "S1oWlN9ll", "writers": {"values-regex": "ICLR.cc/2017/conference/paper203/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper203/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482823230580}}}, {"tddate": null, "tmdate": 1481524205563, "tcdate": 1481524163591, "number": 1, "id": "Sy36m6jmg", "invitation": "ICLR.cc/2017/conference/-/paper203/public/comment", "forum": "S1oWlN9ll", "replyto": "HJmUWVDXe", "signatures": ["~LU_HOU1"], "readers": ["everyone"], "writers": ["~LU_HOU1"], "content": {"title": "RE: AnonReviewer3's questions", "comment": "Thanks for your feedback and questions.   \n\n1. The training procedure for BPN2 is almost the same as BPN, with the following changes:\n- In feedforward propagation, the activations are binarized with the sign function (after step 7 in Algorithm 1).\n- In backward propagation, the gradient for the sign function is computed using straight-through estimator as in \"BinaryNet: Training deep neural networks with weights and activations constrained to +1 or -1\".\n\n2. For the FNN experiments, both BPN and BWN converge to near-zero training loss.\nMNIST:     \tfull-precision: 0.000\tBinaryConnect: 0.000\tBWN: 0.000\tBPN: 0.000\nCIFAR-10: \tfull-precision: 0.000\tBinaryConnect: 0.000\tBWN: 0.000\tBPN: 0.000\nSVHN:      \tfull-precision: 0.000\tBinaryConnect: 0.000\tBWN: 0.000\tBPN: 0.000\n\nFor the RNN experiments, BPN has lower training loss than BWN.\nWar and Peace:  full-precision: 1.095 \tBinaryConnect: 3.003\tBWN: 1.220\tBPN: 1.203\nLinux Kernel:   full-precision: 0.968\tBinaryConnect: 3.493\tBWN: 1.081 \tBPN: 1.062\n\nNote that Adam is used as the optimization solver for all the models. Hence, the difference in performance is due to the different binarization schemes used.\n\nConvergence is reached for  the full-precision network, BWN and BPN (specifically, the change of training and validation losses in successive iterations is smaller than 10^{-5}). However, BinaryConnect cannot converge in the RNN experiments. This is due to the exploding gradient problem as discussed section 3.2 and also supported by experimental results in section 3.4. \n\nPlease let us know if you have further questions.  Thanks again.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Loss-aware Binarization of Deep Networks", "abstract": "Deep neural network models, though very powerful and highly successful, are computationally expensive in terms of space and time. Recently, there have been a number of attempts on binarizing the network weights and activations. This greatly reduces the network size, and replaces the underlying multiplications to additions or even XNOR bit operations. However, existing binarization schemes are based on simple matrix approximations and ignore the effect of binarization on the loss. In this paper, we propose a proximal Newton algorithm with diagonal Hessian approximation that directly minimizes the loss w.r.t. the binarized weights. The underlying proximal step has an efficient closed-form solution, and the second-order information can be efficiently obtained from the second moments already computed by the Adam optimizer. Experiments on both feedforward and recurrent networks show that the proposed loss-aware binarization algorithm outperforms existing binarization schemes, and is also more robust for wide and deep networks.", "pdf": "/pdf/b9cdfec48f524ca349708a3a3b0cc6fcc10715d6.pdf", "paperhash": "hou|lossaware_binarization_of_deep_networks", "conflicts": ["cse.ust.hk"], "authorids": ["lhouab@cse.ust.hk", "qyaoaa@cse.ust.hk", "jamesk@cse.ust.hk"], "authors": ["Lu Hou", "Quanming Yao", "James T. Kwok"], "keywords": ["Deep learning", "Applications", "Optimization"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287687171, "id": "ICLR.cc/2017/conference/-/paper203/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "S1oWlN9ll", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper203/reviewers", "ICLR.cc/2017/conference/paper203/areachairs"], "cdate": 1485287687171}}}, {"tddate": null, "tmdate": 1481224523175, "tcdate": 1481224523170, "number": 1, "id": "HJmUWVDXe", "invitation": "ICLR.cc/2017/conference/-/paper203/pre-review/question", "forum": "S1oWlN9ll", "replyto": "S1oWlN9ll", "signatures": ["ICLR.cc/2017/conference/paper203/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper203/AnonReviewer3"], "content": {"title": "binarized activations + regularization", "question": "Do the binarized activations in the BPN2 setting affect the training procedure? Or does the training procedure remain the same as for BPN?\n\nWhat is the main source of regularization: binarization or good optimization? (i.e. is the training error of BPN higher or lower than the training error of BWN?) Does learning reach convergence in all cases (BPN, BWN, etc.), or is there an implicit early-stopping regularization due to the maximum number of epochs?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Loss-aware Binarization of Deep Networks", "abstract": "Deep neural network models, though very powerful and highly successful, are computationally expensive in terms of space and time. Recently, there have been a number of attempts on binarizing the network weights and activations. This greatly reduces the network size, and replaces the underlying multiplications to additions or even XNOR bit operations. However, existing binarization schemes are based on simple matrix approximations and ignore the effect of binarization on the loss. In this paper, we propose a proximal Newton algorithm with diagonal Hessian approximation that directly minimizes the loss w.r.t. the binarized weights. The underlying proximal step has an efficient closed-form solution, and the second-order information can be efficiently obtained from the second moments already computed by the Adam optimizer. Experiments on both feedforward and recurrent networks show that the proposed loss-aware binarization algorithm outperforms existing binarization schemes, and is also more robust for wide and deep networks.", "pdf": "/pdf/b9cdfec48f524ca349708a3a3b0cc6fcc10715d6.pdf", "paperhash": "hou|lossaware_binarization_of_deep_networks", "conflicts": ["cse.ust.hk"], "authorids": ["lhouab@cse.ust.hk", "qyaoaa@cse.ust.hk", "jamesk@cse.ust.hk"], "authors": ["Lu Hou", "Quanming Yao", "James T. Kwok"], "keywords": ["Deep learning", "Applications", "Optimization"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481224523670, "id": "ICLR.cc/2017/conference/-/paper203/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper203/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper203/AnonReviewer3"], "reply": {"forum": "S1oWlN9ll", "replyto": "S1oWlN9ll", "writers": {"values-regex": "ICLR.cc/2017/conference/paper203/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper203/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481224523670}}}], "count": 22}