{"notes": [{"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1457634952104, "tcdate": 1457634952104, "id": "GvV1OvLjQi1WDOmRiMQJ", "invitation": "ICLR.cc/2016/workshop/-/paper/159/review/11", "forum": "XL9vKJ98DCXB8D1RUGV0", "replyto": "XL9vKJ98DCXB8D1RUGV0", "signatures": ["ICLR.cc/2016/workshop/paper/159/reviewer/11"], "readers": ["everyone"], "writers": ["ICLR.cc/2016/workshop/paper/159/reviewer/11"], "content": {"title": "Interesting new method for training Helmholtz Machines using a persistent MCMC chain", "rating": "7: Good paper, accept", "review": "The authors present a new method to perform maximum likelihood training for Helmholtz machines. This paper follows up on recent work that jointly train a directed generative model p(h)p(x|h) and an approximate inference model q(h|x). The authors provide a concise summary of previous work and their mutual differences (e.g. Table 1). \n\nTheir new method maintains a (persistent) MCMC chain of latent configurations per training datapoint and it uses q(h|x) as a proposal distribution in a Metropolis Hastings style sampling algorithm. The proposed algorithm looks promising although the authors do not provide any in-depth analysis that highlights the potential strengths and weaknesses of the algorithm. For example: It seems plausible that the persistent Markov chain could deal with more complex posterior distributions p(h|x) than RWS or NVIL because these have to find high probability configurations p(h|x) by drawing only a few samples from (a typically factorial) q(h|x). It would therefore be interesting to measure the distance between the intractable p(h|x) and the approximate inference distribution q(h|x) by estimating KL(q|p) or by estimating the effective sampling size for samples h ~ q(h|x)  or by showing the final testset NLL estimates over the number of samples h from q (compared to other methods). It would also be interesting to see how this method compares to the others when deeper models are trained.\n\nIn summary: I think the paper presents an interesting method and provides sufficient experimental results for a workshop contribution. For a full conference or journal publication it would need to be extended. \n\nI also found some grammatical issues and I would recommend additional proofreading.\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "JOINT STOCHASTIC APPROXIMATION LEARNING OF HELMHOLTZ MACHINES", "abstract": "Though with progress, model learning and performing posterior inference still re-\nmains a common challenge for using deep generative models, especially for han-\ndling discrete hidden variables. This paper is mainly concerned with algorithms\nfor learning Helmholz machines, which is characterized by pairing the genera-\ntive model with an auxiliary inference model. A common drawback of previous\nlearning algorithms is that they indirectly optimize some bounds of the targeted\nmarginal log-likelihood. In contrast, we successfully develop a new class of al-\ngorithms, based on stochastic approximation (SA) theory of the Robbins-Monro\ntype, to directly optimize the marginal log-likelihood and simultaneously mini-\nmize the inclusive KL-divergence. The resulting learning algorithm is thus called\njoint SA (JSA). Moreover, we construct an effective MCMC operator for JSA. Our\nresults on the MNIST datasets demonstrate that the JSA\u2019s performance is consis-\ntently superior to that of competing algorithms like RWS, for learning a range of\ndifficult models.", "pdf": "/pdf/XL9vKJ98DCXB8D1RUGV0.pdf", "paperhash": "xu|joint_stochastic_approximation_learning_of_helmholtz_machines", "conflicts": ["tsinghua.edu"], "authors": ["Haotian Xu", "Zhijian Ou"], "authorids": ["xht13@mails.tsinghua.edu.cn", "ozj@tsinghua.edu.cn"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1456580025003, "ddate": null, "super": null, "final": null, "duedate": 1460725200000, "tcdate": 1456580025003, "id": "ICLR.cc/2016/workshop/-/paper/159/review/11", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "reply": {"pdf": null, "forum": "XL9vKJ98DCXB8D1RUGV0", "replyto": "XL9vKJ98DCXB8D1RUGV0", "writers": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)"}, "signatures": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "readers": ["everyone", "ICLR.cc/2016/workshop/paper/159/reviewer/11", "ICLR.cc/2016/workshop"], "expdate": 1468501200000}}}, {"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1457480986828, "tcdate": 1457480986828, "id": "5Qz2PyrAxCZgXpo7i3y5", "invitation": "ICLR.cc/2016/workshop/-/paper/159/review/12", "forum": "XL9vKJ98DCXB8D1RUGV0", "replyto": "XL9vKJ98DCXB8D1RUGV0", "signatures": ["ICLR.cc/2016/workshop/paper/159/reviewer/12"], "readers": ["everyone"], "writers": ["ICLR.cc/2016/workshop/paper/159/reviewer/12"], "content": {"title": "An interesting perspective, large memory requirements, article needs a bit of editing, experiments could use slightly better baselines", "rating": "6: Marginally above acceptance threshold", "review": "This is an interesting paper, offering a novel perspective on training directed graphical models.\n\nIt is known that learning in directed generative models using gradient ascent on the marginal log-likelihood requires one to obtain samples from the posterior probability distribution. The paper suggests getting those samples by running an MCMC chain that leaves the posterior probability invariant. The MCMC chain used is based on either independent proposals from an auxiliary distribution Q, and an MH accept-reject step, or on multiple-trial Metropolis independence sampler based on the same auxiliary distribution Q. The auxiliary distribution Q is also learnt by gradient descent on the KL divergence between the posterior and Q, where samples from the posterior are obtained in the same way as before.\n\nThis method of training is novel - previous methods used either MCMC chains based on Gibbs sampler (Neal, 1992 - unfortunately not cited in the article), or used optimization of a lower bound on log-likelihood, or biased estimates of the gradient of the log-likelihood.\nThe method is most directly comparable to the Reweighted Wake Sleep method, because ultimately the updates to the parameters follow the same equations every time the proposed transition is accepted (but reuse previous samples when the transition is rejected, which is an important difference from the RWS algorithm).\n\nOne drawback of the proposed method is that it requires to store a state of the MCMC chain, one state of latent variables configuration per datapoint in the dataset. It might not be too restrictive for smaller datasets, like MNIST, but is prohibitively expensive for larger datasets.\n\nThe experiments use a published implementation of RWS as a baseline. This is unfortunately not the best practice, as the implementation of the proposed algorithm might use slightly different initialization, hyperparameters, or length of training, which makes the contribution of the algorithm itself harder to separate. This is exacerbated by the fact that the difference in log-likelihoods of trained models is fairly small (although significant). It would be better to use exactly the same initialization and hyperparameters for the RWS implementation and for the proposed algorithm.\n\nAnother comparison is of the proposed algorithm (MIS version) to (non-reweighted) Wake-Sleep. In this comparison the proposed algorithm converges to significantly better performing models, indicating that storing the previous states of the MCMC chain, and following the proper Metropolis accept-reject step does provide a significant advantage.\n\nThe paper has multiple typos and grammar issues, and would benefit from additional editing.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "JOINT STOCHASTIC APPROXIMATION LEARNING OF HELMHOLTZ MACHINES", "abstract": "Though with progress, model learning and performing posterior inference still re-\nmains a common challenge for using deep generative models, especially for han-\ndling discrete hidden variables. This paper is mainly concerned with algorithms\nfor learning Helmholz machines, which is characterized by pairing the genera-\ntive model with an auxiliary inference model. A common drawback of previous\nlearning algorithms is that they indirectly optimize some bounds of the targeted\nmarginal log-likelihood. In contrast, we successfully develop a new class of al-\ngorithms, based on stochastic approximation (SA) theory of the Robbins-Monro\ntype, to directly optimize the marginal log-likelihood and simultaneously mini-\nmize the inclusive KL-divergence. The resulting learning algorithm is thus called\njoint SA (JSA). Moreover, we construct an effective MCMC operator for JSA. Our\nresults on the MNIST datasets demonstrate that the JSA\u2019s performance is consis-\ntently superior to that of competing algorithms like RWS, for learning a range of\ndifficult models.", "pdf": "/pdf/XL9vKJ98DCXB8D1RUGV0.pdf", "paperhash": "xu|joint_stochastic_approximation_learning_of_helmholtz_machines", "conflicts": ["tsinghua.edu"], "authors": ["Haotian Xu", "Zhijian Ou"], "authorids": ["xht13@mails.tsinghua.edu.cn", "ozj@tsinghua.edu.cn"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1456580024383, "ddate": null, "super": null, "final": null, "duedate": 1460725200000, "tcdate": 1456580024383, "id": "ICLR.cc/2016/workshop/-/paper/159/review/12", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "reply": {"pdf": null, "forum": "XL9vKJ98DCXB8D1RUGV0", "replyto": "XL9vKJ98DCXB8D1RUGV0", "writers": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)"}, "signatures": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "readers": ["everyone", "ICLR.cc/2016/workshop/paper/159/reviewer/12", "ICLR.cc/2016/workshop"], "expdate": 1468501200000}}}, {"tddate": null, "number": null, "replyto": null, "ddate": null, "cdate": null, "tmdate": 1455830726023, "tcdate": 1455830726023, "id": "XL9vKJ98DCXB8D1RUGV0", "invitation": "ICLR.cc/2016/workshop/-/submission", "forum": "XL9vKJ98DCXB8D1RUGV0", "signatures": ["~Haotian_Xu1"], "readers": ["everyone"], "writers": ["~Haotian_Xu1"], "content": {"CMT_id": "", "title": "JOINT STOCHASTIC APPROXIMATION LEARNING OF HELMHOLTZ MACHINES", "abstract": "Though with progress, model learning and performing posterior inference still re-\nmains a common challenge for using deep generative models, especially for han-\ndling discrete hidden variables. This paper is mainly concerned with algorithms\nfor learning Helmholz machines, which is characterized by pairing the genera-\ntive model with an auxiliary inference model. A common drawback of previous\nlearning algorithms is that they indirectly optimize some bounds of the targeted\nmarginal log-likelihood. In contrast, we successfully develop a new class of al-\ngorithms, based on stochastic approximation (SA) theory of the Robbins-Monro\ntype, to directly optimize the marginal log-likelihood and simultaneously mini-\nmize the inclusive KL-divergence. The resulting learning algorithm is thus called\njoint SA (JSA). Moreover, we construct an effective MCMC operator for JSA. Our\nresults on the MNIST datasets demonstrate that the JSA\u2019s performance is consis-\ntently superior to that of competing algorithms like RWS, for learning a range of\ndifficult models.", "pdf": "/pdf/XL9vKJ98DCXB8D1RUGV0.pdf", "paperhash": "xu|joint_stochastic_approximation_learning_of_helmholtz_machines", "conflicts": ["tsinghua.edu"], "authors": ["Haotian Xu", "Zhijian Ou"], "authorids": ["xht13@mails.tsinghua.edu.cn", "ozj@tsinghua.edu.cn"]}, "nonreaders": [], "details": {"replyCount": 2, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1454464564200, "ddate": null, "super": null, "final": null, "duedate": 1455833700000, "tcdate": 1454464564200, "id": "ICLR.cc/2016/workshop/-/submission", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "readers": ["everyone"], "reply": {"pdf": null, "forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"order": 4, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv.", "value-regex": "upload|http://arxiv.org/pdf/.+"}, "title": {"order": 3, "description": "Title of paper.", "value-regex": ".{0,500}"}, "abstract": {"order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"order": 1, "description": "Comma separated list of author names, as they appear in the paper.", "value-regex": "[^,\\n]+(,[^,\\n]+)*"}, "author_emails": {"order": 2, "description": "Comma separated list of author email addresses, in the same order as above.", "value-regex": "[^,\\n]+(,[^,\\n]+)*"}, "conflicts": {"order": 100, "description": "Semi-colon separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.).", "value-regex": "^([a-zA-Z0-9][a-zA-Z0-9-_]{0,61}[a-zA-Z0-9]{0,1}\\.([a-zA-Z]{1,6}|[a-zA-Z0-9-]{1,30}\\.[a-zA-Z]{2,3}))+(;[a-zA-Z0-9][a-zA-Z0-9-_]{0,61}[a-zA-Z0-9]{0,1}\\.([a-zA-Z]{1,6}|[a-zA-Z0-9-]{1,30}\\.[a-zA-Z]{2,3}))*$"}, "CMT_id": {"order": 5, "value-regex": ".*", "description": "If the paper is a resubmission from the ICLR 2016 Conference Track, enter its CMT ID; otherwise, leave blank."}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "expdate": 1463609700000}}}], "count": 3}