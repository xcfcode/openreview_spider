{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1528124449244, "tcdate": 1518470952535, "number": 288, "cdate": 1518470952535, "id": "Syg4wtkvf", "invitation": "ICLR.cc/2018/Workshop/-/Submission", "forum": "Syg4wtkvf", "signatures": ["~mahdieh_abbasi1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop"], "content": {"title": "Out-distribution Training Confers Robustness to Deep Neural Networks", "abstract": "The easiness at which adversarial instances can be generated in deep neural networks raises some fundamental questions on their functioning and concerns on their use in critical systems. In this paper, we draw a connection between over-generalization and adversaries: a possible cause of adversaries lies in models designed to make decisions all over the input space, leading to inappropriate high-confidence decisions in parts of the input space not represented in the training set. We empirically show an augmented neural network, which is not trained on any types of adversaries, can increase the robustness by detecting black-box one-step adversaries, i.e. assimilated to out-distribution samples, and making generation of white-box one-step adversaries harder. ", "paperhash": "abbasi|outdistribution_training_confers_robustness_to_deep_neural_networks", "_bibtex": "@misc{\n  abbasi2018out-distribution,\n  title={Out-distribution Training Confers Robustness to Deep Neural Networks},\n  author={mahdieh abbasi and christian gagne},\n  year={2018},\n  url={https://openreview.net/forum?id=Syg4wtkvf}\n}", "authorids": ["mahdieh.abbasi.1@ulaval.ca", "christian.gagne@gel.ulaval.ca"], "authors": ["mahdieh abbasi", "christian gagne"], "keywords": [], "pdf": "/pdf/3ea3e0c54265a3555ba1b6d689e1aa8d99cebb6a.pdf"}, "nonreaders": [], "details": {"replyCount": 3, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1518472800000, "tmdate": 1518474081690, "id": "ICLR.cc/2018/Workshop/-/Submission", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Workshop"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2018/Workshop", "description": "Your authorized identity to be associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 9, "value-regex": "upload", "description": "Upload a PDF file that ends with .pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 8, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names. Please provide real names; identities will be anonymized."}, "keywords": {"order": 6, "values-regex": "(^$)|[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of keywords."}, "TL;DR": {"required": false, "order": 7, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,500}"}, "authorids": {"required": true, "order": 3, "values-regex": "([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,},){0,}([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,})", "description": "Comma separated list of author email addresses, lowercased, in the same order as above. For authors with existing OpenReview accounts, please make sure that the provided email address(es) match those listed in the author's profile. Please provide real emails; identities will be anonymized."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1526248800000, "cdate": 1518474081690}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582707586, "tcdate": 1520693603399, "number": 1, "cdate": 1520693603399, "id": "SkjvZOWYM", "invitation": "ICLR.cc/2018/Workshop/-/Paper288/Official_Review", "forum": "Syg4wtkvf", "replyto": "Syg4wtkvf", "signatures": ["ICLR.cc/2018/Workshop/Paper288/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper288/AnonReviewer2"], "content": {"title": "interesting observation but missing experiments, related work", "rating": "5: Marginally below acceptance threshold", "review": "This paper makes a simple observation that my training a classifier with an additional \"other\" class, and utilizing data from outside the training set as examples for this class, the final trained classifier is more robust to fast gradient sign (FGS) adversarial examples than a similar classifier without the \"other\" class training.\n\nSpecific comments/questions:\n- you begin referring to FGS and T-FGS without ever defining these terms or citing the work. Don't assume the reader knows the acronym for fast gradient sign, and regardless you need to cite this method. (I know this come up in the appendix, but this is too late, or at least direct the reader to appendix)\n- similarly, cuda-convnet CNN is referred to without reference. I assume you mean AlexNet (cuda-convnet  is an old and non-standard way of referring to this architecture), but you need to say this. Same goes for citation of VGG net.\n- table 1 is very confusing. What is the accuracy column referring to. It is clearly not accuracy of the test images since that is given by column\"clean test acc\". The other possibilities I can see is are (i) the accuracy of the dustbin classifier (i.e. how frequently it correctly classifies an adversarial example as \"other\" or (ii) the accuracy of overall network on adversarial examples where a classification is considered correct if either an image is classified as \"other\" or classified correctly as one of k real classes. In both these cases a higher accuracy is better, but the augmented network does worse on both cifar and mnist.  Please specify what this column refers to and also make clear in paper. \n- How does your method work against other attack methods? There are so many easy to run toolboxes for generating adversarial examples, I don't think there is any excuse for not running your approach with other attacks.\n- How does this idea relate to other related work.. for example, GAN models have been proposed where the discriminator classifies 1 of k classes (trained with real data) and also classifies generated images as \"other\".. that seems akin to this approach, but with the out of distribution samples coming from a generator rather than a fixed real dataset. Similarly, what happens if you run adversarial training methods with the same model architectures (i.e. generate adversarial examples and classify them as \"other\"). How does this compare to other methods of classifying out of distribution examples, and can any of them be used to classify adversarial examples?\n- what happens when the out of distribution dataset is varied? it would be interesting to see a couple different datasets used here.\n- what happens if you utilize the out of distribution samples in another way, e.g. asking the network to have uniform output distribution over predicted classes?\n\nThe main observation in the paper is interesting, but the experiments are lacking, relation to other work is missing and overall I don't come away from the paper understanding *why* their results show what they do. I would increase my score if the authors can address the questions above. \n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Out-distribution Training Confers Robustness to Deep Neural Networks", "abstract": "The easiness at which adversarial instances can be generated in deep neural networks raises some fundamental questions on their functioning and concerns on their use in critical systems. In this paper, we draw a connection between over-generalization and adversaries: a possible cause of adversaries lies in models designed to make decisions all over the input space, leading to inappropriate high-confidence decisions in parts of the input space not represented in the training set. We empirically show an augmented neural network, which is not trained on any types of adversaries, can increase the robustness by detecting black-box one-step adversaries, i.e. assimilated to out-distribution samples, and making generation of white-box one-step adversaries harder. ", "paperhash": "abbasi|outdistribution_training_confers_robustness_to_deep_neural_networks", "_bibtex": "@misc{\n  abbasi2018out-distribution,\n  title={Out-distribution Training Confers Robustness to Deep Neural Networks},\n  author={mahdieh abbasi and christian gagne},\n  year={2018},\n  url={https://openreview.net/forum?id=Syg4wtkvf}\n}", "authorids": ["mahdieh.abbasi.1@ulaval.ca", "christian.gagne@gel.ulaval.ca"], "authors": ["mahdieh abbasi", "christian gagne"], "keywords": [], "pdf": "/pdf/3ea3e0c54265a3555ba1b6d689e1aa8d99cebb6a.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582707355, "id": "ICLR.cc/2018/Workshop/-/Paper288/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper288/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper288/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper288/AnonReviewer1"], "reply": {"forum": "Syg4wtkvf", "replyto": "Syg4wtkvf", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper288/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper288/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582707355}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582683620, "tcdate": 1520722096565, "number": 2, "cdate": 1520722096565, "id": "HJdhl1MFM", "invitation": "ICLR.cc/2018/Workshop/-/Paper288/Official_Review", "forum": "Syg4wtkvf", "replyto": "Syg4wtkvf", "signatures": ["ICLR.cc/2018/Workshop/Paper288/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper288/AnonReviewer1"], "content": {"title": "unconvincing results. robustness is a too strong a claim!", "rating": "5: Marginally below acceptance threshold", "review": "I think the paper is well written, clear and on one of the biggest problems in the field. I cannot perfectly assess originality but I find the approach limited in scope and potential impact. \n\nKnowing that a neural network can shatter a dataset in arbitrary ways (e.g. learn a classifier to random labels) I find it highly unlikely that adding a small set of points from another fixed distribution whose distance to the current one is uncontrolled would help alleviate the problem much less \"confer robustness\". I don't see anything in the paper to convince me otherwise.\n\npros:\n- simple method that seems to help somewhat on the datasets they have tested on\n\ncons:\n- even for cifar10 the reduction seems too small and will probably diminish further if scaling it up to ImageNet\n- the idea seems adhoc and unconvincing", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Out-distribution Training Confers Robustness to Deep Neural Networks", "abstract": "The easiness at which adversarial instances can be generated in deep neural networks raises some fundamental questions on their functioning and concerns on their use in critical systems. In this paper, we draw a connection between over-generalization and adversaries: a possible cause of adversaries lies in models designed to make decisions all over the input space, leading to inappropriate high-confidence decisions in parts of the input space not represented in the training set. We empirically show an augmented neural network, which is not trained on any types of adversaries, can increase the robustness by detecting black-box one-step adversaries, i.e. assimilated to out-distribution samples, and making generation of white-box one-step adversaries harder. ", "paperhash": "abbasi|outdistribution_training_confers_robustness_to_deep_neural_networks", "_bibtex": "@misc{\n  abbasi2018out-distribution,\n  title={Out-distribution Training Confers Robustness to Deep Neural Networks},\n  author={mahdieh abbasi and christian gagne},\n  year={2018},\n  url={https://openreview.net/forum?id=Syg4wtkvf}\n}", "authorids": ["mahdieh.abbasi.1@ulaval.ca", "christian.gagne@gel.ulaval.ca"], "authors": ["mahdieh abbasi", "christian gagne"], "keywords": [], "pdf": "/pdf/3ea3e0c54265a3555ba1b6d689e1aa8d99cebb6a.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582707355, "id": "ICLR.cc/2018/Workshop/-/Paper288/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper288/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper288/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper288/AnonReviewer1"], "reply": {"forum": "Syg4wtkvf", "replyto": "Syg4wtkvf", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper288/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper288/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582707355}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521573587677, "tcdate": 1521573587677, "number": 191, "cdate": 1521573587343, "id": "rJhACACFz", "invitation": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "forum": "Syg4wtkvf", "replyto": "Syg4wtkvf", "signatures": ["ICLR.cc/2018/Workshop/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Program_Chairs"], "content": {"decision": "Reject", "title": "ICLR 2018 Workshop Acceptance Decision", "comment": "Based on the reviews, this paper has not been accepted for presentation at the ICLR workshop. However, the conversation and updates can continue to appear here on OpenReview."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Out-distribution Training Confers Robustness to Deep Neural Networks", "abstract": "The easiness at which adversarial instances can be generated in deep neural networks raises some fundamental questions on their functioning and concerns on their use in critical systems. In this paper, we draw a connection between over-generalization and adversaries: a possible cause of adversaries lies in models designed to make decisions all over the input space, leading to inappropriate high-confidence decisions in parts of the input space not represented in the training set. We empirically show an augmented neural network, which is not trained on any types of adversaries, can increase the robustness by detecting black-box one-step adversaries, i.e. assimilated to out-distribution samples, and making generation of white-box one-step adversaries harder. ", "paperhash": "abbasi|outdistribution_training_confers_robustness_to_deep_neural_networks", "_bibtex": "@misc{\n  abbasi2018out-distribution,\n  title={Out-distribution Training Confers Robustness to Deep Neural Networks},\n  author={mahdieh abbasi and christian gagne},\n  year={2018},\n  url={https://openreview.net/forum?id=Syg4wtkvf}\n}", "authorids": ["mahdieh.abbasi.1@ulaval.ca", "christian.gagne@gel.ulaval.ca"], "authors": ["mahdieh abbasi", "christian gagne"], "keywords": [], "pdf": "/pdf/3ea3e0c54265a3555ba1b6d689e1aa8d99cebb6a.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518629844880, "id": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Program_Chairs"], "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Workshop/-/Submission", "writers": {"values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Workshop/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Workshop Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept", "Reject"]}}}, "nonreaders": [], "noninvitees": [], "cdate": 1518629844880}}}], "count": 4}