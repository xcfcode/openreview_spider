{"notes": [{"id": "_HsKf3YaWpG", "original": "cqjeNo5u-Lq", "number": 1682, "cdate": 1601308186191, "ddate": null, "tcdate": 1601308186191, "tmdate": 1614985680948, "tddate": null, "forum": "_HsKf3YaWpG", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Uniform Priors for Data-Efficient Transfer", "authorids": ["~Samarth_Sinha1", "~Karsten_Roth1", "~Anirudh_Goyal1", "~Marzyeh_Ghassemi1", "~Hugo_Larochelle1", "~Animesh_Garg1"], "authors": ["Samarth Sinha", "Karsten Roth", "Anirudh Goyal", "Marzyeh Ghassemi", "Hugo Larochelle", "Animesh Garg"], "keywords": ["Meta Learning", "Deep Metric Learning", "Transfer Learning"], "abstract": "Deep Neural Networks have shown great promise on a variety of downstream applications; but their ability to adapt and generalize to new data and tasks remains a challenging problem. However, the ability to perform few or zero-shot adaptation to novel tasks is important for the scalability and deployment of machine learning models.  It is therefore crucial to understand what makes for good, transferable features in deep networks that best allow for such adaptation.  In this paper, we shed light on this by showing that features that are most transferable have high uniformity in the embedding space and propose a uniformity regularization scheme that encourages better transfer and feature reuse.  We evaluate the regularization on its ability to facilitate adaptation to unseen tasks and data, for which we conduct a thorough experimental study covering four relevant, and distinct domains:  few-shot Meta-Learning, Deep Metric Learning, Zero-Shot Domain Adaptation, as well as Out-of-Distribution classification.   Across all experiments,  we show that uniformity regularization consistently offers benefits over baseline methods and is able to achieve state-of-the-art performance in Deep Metric Learning and Meta-Learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "sinha|uniform_priors_for_dataefficient_transfer", "one-sentence_summary": "We observe that the uniformity of embeddings is important for better transfer and adaptation, and propose a simple technique to promote uniformity which improve meta learning, deep metric learning, zero-shot domain adaptation and OOD generalization.", "pdf": "/pdf/56dba903a0cf8efcffb726406bf47b861dd084ee.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=cT6INmZ3om", "_bibtex": "@misc{\nsinha2021uniform,\ntitle={Uniform Priors for Data-Efficient Transfer},\nauthor={Samarth Sinha and Karsten Roth and Anirudh Goyal and Marzyeh Ghassemi and Hugo Larochelle and Animesh Garg},\nyear={2021},\nurl={https://openreview.net/forum?id=_HsKf3YaWpG}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 14, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "3o5m1JLR97", "original": null, "number": 1, "cdate": 1610040472988, "ddate": null, "tcdate": 1610040472988, "tmdate": 1610474077194, "tddate": null, "forum": "_HsKf3YaWpG", "replyto": "_HsKf3YaWpG", "invitation": "ICLR.cc/2021/Conference/Paper1682/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "The authors argue that uniform priors for the high-level latent representations improve transferability, which is beneficial in a number of tasks involving transference. The approach is evaluated on deep metric learning, zero-shot domain adaptation and few-shot meta-learning.\n\nPro:\n- A simple yet effective method\n- Signifiant gains in experimental study\n\nCons:\n- Close variants of this approach were proposed in previous works, and so the novelty of the current work is limited.\n- There is no accompanying analysis which may shed new light on the advantages of the approach."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Uniform Priors for Data-Efficient Transfer", "authorids": ["~Samarth_Sinha1", "~Karsten_Roth1", "~Anirudh_Goyal1", "~Marzyeh_Ghassemi1", "~Hugo_Larochelle1", "~Animesh_Garg1"], "authors": ["Samarth Sinha", "Karsten Roth", "Anirudh Goyal", "Marzyeh Ghassemi", "Hugo Larochelle", "Animesh Garg"], "keywords": ["Meta Learning", "Deep Metric Learning", "Transfer Learning"], "abstract": "Deep Neural Networks have shown great promise on a variety of downstream applications; but their ability to adapt and generalize to new data and tasks remains a challenging problem. However, the ability to perform few or zero-shot adaptation to novel tasks is important for the scalability and deployment of machine learning models.  It is therefore crucial to understand what makes for good, transferable features in deep networks that best allow for such adaptation.  In this paper, we shed light on this by showing that features that are most transferable have high uniformity in the embedding space and propose a uniformity regularization scheme that encourages better transfer and feature reuse.  We evaluate the regularization on its ability to facilitate adaptation to unseen tasks and data, for which we conduct a thorough experimental study covering four relevant, and distinct domains:  few-shot Meta-Learning, Deep Metric Learning, Zero-Shot Domain Adaptation, as well as Out-of-Distribution classification.   Across all experiments,  we show that uniformity regularization consistently offers benefits over baseline methods and is able to achieve state-of-the-art performance in Deep Metric Learning and Meta-Learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "sinha|uniform_priors_for_dataefficient_transfer", "one-sentence_summary": "We observe that the uniformity of embeddings is important for better transfer and adaptation, and propose a simple technique to promote uniformity which improve meta learning, deep metric learning, zero-shot domain adaptation and OOD generalization.", "pdf": "/pdf/56dba903a0cf8efcffb726406bf47b861dd084ee.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=cT6INmZ3om", "_bibtex": "@misc{\nsinha2021uniform,\ntitle={Uniform Priors for Data-Efficient Transfer},\nauthor={Samarth Sinha and Karsten Roth and Anirudh Goyal and Marzyeh Ghassemi and Hugo Larochelle and Animesh Garg},\nyear={2021},\nurl={https://openreview.net/forum?id=_HsKf3YaWpG}\n}"}, "tags": [], "invitation": {"reply": {"forum": "_HsKf3YaWpG", "replyto": "_HsKf3YaWpG", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040472973, "tmdate": 1610474077178, "id": "ICLR.cc/2021/Conference/Paper1682/-/Decision"}}}, {"id": "XHwU-CklU4", "original": null, "number": 1, "cdate": 1603904191106, "ddate": null, "tcdate": 1603904191106, "tmdate": 1607370600531, "tddate": null, "forum": "_HsKf3YaWpG", "replyto": "_HsKf3YaWpG", "invitation": "ICLR.cc/2021/Conference/Paper1682/-/Official_Review", "content": {"title": "Review of AnonReviewer2", "review": "Summary:This paper broadly discusses about learning good transferable features in deep networks to perform few or zero-shot \nadaptation to novel tasks. The paper proposes a uniformity regularization scheme that encourages better transfer and feature reuse. The method is validated on several benchmark datasets.\n\n+ves:\n+ The paper is overall well-written and easy to follow. \n+ The paper has shown experimentations on different datasets under different learning setups, including deep metric learning and zero shot domain adaptation.\n+ The overall idea of using a uniform regularizer on manifold is interesting and supported by theoretical considerations coming from GAN and VAE-like formulations.\n\nConcerns:\n- Equation 6, which is the final proposed loss, seems incomplete, which seems like a key issue. As I understand, Eqn 6 must consider the discriminator loss described in Eqn 4 too. Considering only the generator (that is the network q(z|x) till the last layer) is a necessary condition but may not be a sufficient condition to optimize the end-to-end network. I will be happy to know if I have missed something here.\n- The paper seems to have missed some key references that have addressed similar problems - [1] and [2] listed at the end of this review. Both of these efforts show task transferability for computer vision tasks. This is important, considering the similarity of objectives.\n- The paper states that it performs transfer between MNIST, SVHN and USPS (model trained on source data and tested on target data). It was not clear how MNIST --> SVHN was done. Wouldn't there be a mismatch of input channels in this case (SVHN is color, while MNIST is not). \n- A feature space visualization of the proposed method and other baselines would have been very useful to directly compare and understand the claim and advantages of imposing uniform regularizer.\n- One broader question that may be relevant (may not directly relate to a decision on the paper): How will uniformity on task space be affected if we leveraged a method such as Mixup [3]? How will the task space, in that case, look like? \n\nMinor comments:\n1.  Abstract: learn-ing --> learning,  transfer-able -->  transferable\n2. Please check the line \"... for improved generalization: For Unsupervised Representation Learning\"\n3. Section Uniformity Regularization: isn't \\U(-\\aplha, \\beta) a correct formulation considering the previous \\U(-\\alpha, \\beta) in section Prior Matching?\n4. Page 7 Zero shot Domain Adaption: Please remove ``(\" from the ``(LeNet''\n\nReferences:\n[1] Zero-shot task transfer, Pal et al, CVPR 2019\n[2] LSM: Learning Subspace Minimization for Low-level Vision, Tang et al, CVPR 2020\n[3] mixup: BEYOND EMPIRICAL RISK MINIMIZATION, Zhang et al, ICLR 2018\n\nPOST-REBUTTAL:\nI thank the authors for their response.\n\nAt the outset, I am satisfied with the authors' responses to my questions - all the questions were answered. I do agree with other reviewers that the idea is incremental. Learning the prior across tasks is not very novel, as pointed out in references cited by other reviewers. However, on the bright side, the authors have done a good job in answering the questions, and the comprehensive experimental results are promising. The overall idea looks a bit incremental from the GAN literature side but maybe a good lead for meta and incremental learning literature.\n\nI change my decision to \"Weak Accept\".", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1682/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1682/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Uniform Priors for Data-Efficient Transfer", "authorids": ["~Samarth_Sinha1", "~Karsten_Roth1", "~Anirudh_Goyal1", "~Marzyeh_Ghassemi1", "~Hugo_Larochelle1", "~Animesh_Garg1"], "authors": ["Samarth Sinha", "Karsten Roth", "Anirudh Goyal", "Marzyeh Ghassemi", "Hugo Larochelle", "Animesh Garg"], "keywords": ["Meta Learning", "Deep Metric Learning", "Transfer Learning"], "abstract": "Deep Neural Networks have shown great promise on a variety of downstream applications; but their ability to adapt and generalize to new data and tasks remains a challenging problem. However, the ability to perform few or zero-shot adaptation to novel tasks is important for the scalability and deployment of machine learning models.  It is therefore crucial to understand what makes for good, transferable features in deep networks that best allow for such adaptation.  In this paper, we shed light on this by showing that features that are most transferable have high uniformity in the embedding space and propose a uniformity regularization scheme that encourages better transfer and feature reuse.  We evaluate the regularization on its ability to facilitate adaptation to unseen tasks and data, for which we conduct a thorough experimental study covering four relevant, and distinct domains:  few-shot Meta-Learning, Deep Metric Learning, Zero-Shot Domain Adaptation, as well as Out-of-Distribution classification.   Across all experiments,  we show that uniformity regularization consistently offers benefits over baseline methods and is able to achieve state-of-the-art performance in Deep Metric Learning and Meta-Learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "sinha|uniform_priors_for_dataefficient_transfer", "one-sentence_summary": "We observe that the uniformity of embeddings is important for better transfer and adaptation, and propose a simple technique to promote uniformity which improve meta learning, deep metric learning, zero-shot domain adaptation and OOD generalization.", "pdf": "/pdf/56dba903a0cf8efcffb726406bf47b861dd084ee.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=cT6INmZ3om", "_bibtex": "@misc{\nsinha2021uniform,\ntitle={Uniform Priors for Data-Efficient Transfer},\nauthor={Samarth Sinha and Karsten Roth and Anirudh Goyal and Marzyeh Ghassemi and Hugo Larochelle and Animesh Garg},\nyear={2021},\nurl={https://openreview.net/forum?id=_HsKf3YaWpG}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "_HsKf3YaWpG", "replyto": "_HsKf3YaWpG", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1682/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538113134, "tmdate": 1606915792826, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1682/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1682/-/Official_Review"}}}, {"id": "fAuRlM7gpFg", "original": null, "number": 11, "cdate": 1606131554960, "ddate": null, "tcdate": 1606131554960, "tmdate": 1606131554960, "tddate": null, "forum": "_HsKf3YaWpG", "replyto": "XHwU-CklU4", "invitation": "ICLR.cc/2021/Conference/Paper1682/-/Official_Comment", "content": {"title": "Discussion", "comment": "Kindly let us know if our response below addressed your concerns. We are happy to answer if there are additional issues/questions."}, "signatures": ["ICLR.cc/2021/Conference/Paper1682/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1682/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Uniform Priors for Data-Efficient Transfer", "authorids": ["~Samarth_Sinha1", "~Karsten_Roth1", "~Anirudh_Goyal1", "~Marzyeh_Ghassemi1", "~Hugo_Larochelle1", "~Animesh_Garg1"], "authors": ["Samarth Sinha", "Karsten Roth", "Anirudh Goyal", "Marzyeh Ghassemi", "Hugo Larochelle", "Animesh Garg"], "keywords": ["Meta Learning", "Deep Metric Learning", "Transfer Learning"], "abstract": "Deep Neural Networks have shown great promise on a variety of downstream applications; but their ability to adapt and generalize to new data and tasks remains a challenging problem. However, the ability to perform few or zero-shot adaptation to novel tasks is important for the scalability and deployment of machine learning models.  It is therefore crucial to understand what makes for good, transferable features in deep networks that best allow for such adaptation.  In this paper, we shed light on this by showing that features that are most transferable have high uniformity in the embedding space and propose a uniformity regularization scheme that encourages better transfer and feature reuse.  We evaluate the regularization on its ability to facilitate adaptation to unseen tasks and data, for which we conduct a thorough experimental study covering four relevant, and distinct domains:  few-shot Meta-Learning, Deep Metric Learning, Zero-Shot Domain Adaptation, as well as Out-of-Distribution classification.   Across all experiments,  we show that uniformity regularization consistently offers benefits over baseline methods and is able to achieve state-of-the-art performance in Deep Metric Learning and Meta-Learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "sinha|uniform_priors_for_dataefficient_transfer", "one-sentence_summary": "We observe that the uniformity of embeddings is important for better transfer and adaptation, and propose a simple technique to promote uniformity which improve meta learning, deep metric learning, zero-shot domain adaptation and OOD generalization.", "pdf": "/pdf/56dba903a0cf8efcffb726406bf47b861dd084ee.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=cT6INmZ3om", "_bibtex": "@misc{\nsinha2021uniform,\ntitle={Uniform Priors for Data-Efficient Transfer},\nauthor={Samarth Sinha and Karsten Roth and Anirudh Goyal and Marzyeh Ghassemi and Hugo Larochelle and Animesh Garg},\nyear={2021},\nurl={https://openreview.net/forum?id=_HsKf3YaWpG}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "_HsKf3YaWpG", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1682/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1682/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1682/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1682/Authors|ICLR.cc/2021/Conference/Paper1682/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1682/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923856941, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1682/-/Official_Comment"}}}, {"id": "s_RIGEEgYC", "original": null, "number": 10, "cdate": 1606131537469, "ddate": null, "tcdate": 1606131537469, "tmdate": 1606131537469, "tddate": null, "forum": "_HsKf3YaWpG", "replyto": "oTbIk6nG4Bq", "invitation": "ICLR.cc/2021/Conference/Paper1682/-/Official_Comment", "content": {"title": "Discussion", "comment": "Kindly let us know if our response below addressed your concerns. We are happy to answer if there are additional issues/questions."}, "signatures": ["ICLR.cc/2021/Conference/Paper1682/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1682/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Uniform Priors for Data-Efficient Transfer", "authorids": ["~Samarth_Sinha1", "~Karsten_Roth1", "~Anirudh_Goyal1", "~Marzyeh_Ghassemi1", "~Hugo_Larochelle1", "~Animesh_Garg1"], "authors": ["Samarth Sinha", "Karsten Roth", "Anirudh Goyal", "Marzyeh Ghassemi", "Hugo Larochelle", "Animesh Garg"], "keywords": ["Meta Learning", "Deep Metric Learning", "Transfer Learning"], "abstract": "Deep Neural Networks have shown great promise on a variety of downstream applications; but their ability to adapt and generalize to new data and tasks remains a challenging problem. However, the ability to perform few or zero-shot adaptation to novel tasks is important for the scalability and deployment of machine learning models.  It is therefore crucial to understand what makes for good, transferable features in deep networks that best allow for such adaptation.  In this paper, we shed light on this by showing that features that are most transferable have high uniformity in the embedding space and propose a uniformity regularization scheme that encourages better transfer and feature reuse.  We evaluate the regularization on its ability to facilitate adaptation to unseen tasks and data, for which we conduct a thorough experimental study covering four relevant, and distinct domains:  few-shot Meta-Learning, Deep Metric Learning, Zero-Shot Domain Adaptation, as well as Out-of-Distribution classification.   Across all experiments,  we show that uniformity regularization consistently offers benefits over baseline methods and is able to achieve state-of-the-art performance in Deep Metric Learning and Meta-Learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "sinha|uniform_priors_for_dataefficient_transfer", "one-sentence_summary": "We observe that the uniformity of embeddings is important for better transfer and adaptation, and propose a simple technique to promote uniformity which improve meta learning, deep metric learning, zero-shot domain adaptation and OOD generalization.", "pdf": "/pdf/56dba903a0cf8efcffb726406bf47b861dd084ee.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=cT6INmZ3om", "_bibtex": "@misc{\nsinha2021uniform,\ntitle={Uniform Priors for Data-Efficient Transfer},\nauthor={Samarth Sinha and Karsten Roth and Anirudh Goyal and Marzyeh Ghassemi and Hugo Larochelle and Animesh Garg},\nyear={2021},\nurl={https://openreview.net/forum?id=_HsKf3YaWpG}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "_HsKf3YaWpG", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1682/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1682/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1682/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1682/Authors|ICLR.cc/2021/Conference/Paper1682/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1682/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923856941, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1682/-/Official_Comment"}}}, {"id": "4-v3jp-N8L-", "original": null, "number": 9, "cdate": 1606131515293, "ddate": null, "tcdate": 1606131515293, "tmdate": 1606131515293, "tddate": null, "forum": "_HsKf3YaWpG", "replyto": "MPALyO-UEVD", "invitation": "ICLR.cc/2021/Conference/Paper1682/-/Official_Comment", "content": {"title": "Discussion", "comment": "Kindly let us know if our response below addressed your concerns. We are happy to answer if there are additional issues/questions."}, "signatures": ["ICLR.cc/2021/Conference/Paper1682/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1682/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Uniform Priors for Data-Efficient Transfer", "authorids": ["~Samarth_Sinha1", "~Karsten_Roth1", "~Anirudh_Goyal1", "~Marzyeh_Ghassemi1", "~Hugo_Larochelle1", "~Animesh_Garg1"], "authors": ["Samarth Sinha", "Karsten Roth", "Anirudh Goyal", "Marzyeh Ghassemi", "Hugo Larochelle", "Animesh Garg"], "keywords": ["Meta Learning", "Deep Metric Learning", "Transfer Learning"], "abstract": "Deep Neural Networks have shown great promise on a variety of downstream applications; but their ability to adapt and generalize to new data and tasks remains a challenging problem. However, the ability to perform few or zero-shot adaptation to novel tasks is important for the scalability and deployment of machine learning models.  It is therefore crucial to understand what makes for good, transferable features in deep networks that best allow for such adaptation.  In this paper, we shed light on this by showing that features that are most transferable have high uniformity in the embedding space and propose a uniformity regularization scheme that encourages better transfer and feature reuse.  We evaluate the regularization on its ability to facilitate adaptation to unseen tasks and data, for which we conduct a thorough experimental study covering four relevant, and distinct domains:  few-shot Meta-Learning, Deep Metric Learning, Zero-Shot Domain Adaptation, as well as Out-of-Distribution classification.   Across all experiments,  we show that uniformity regularization consistently offers benefits over baseline methods and is able to achieve state-of-the-art performance in Deep Metric Learning and Meta-Learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "sinha|uniform_priors_for_dataefficient_transfer", "one-sentence_summary": "We observe that the uniformity of embeddings is important for better transfer and adaptation, and propose a simple technique to promote uniformity which improve meta learning, deep metric learning, zero-shot domain adaptation and OOD generalization.", "pdf": "/pdf/56dba903a0cf8efcffb726406bf47b861dd084ee.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=cT6INmZ3om", "_bibtex": "@misc{\nsinha2021uniform,\ntitle={Uniform Priors for Data-Efficient Transfer},\nauthor={Samarth Sinha and Karsten Roth and Anirudh Goyal and Marzyeh Ghassemi and Hugo Larochelle and Animesh Garg},\nyear={2021},\nurl={https://openreview.net/forum?id=_HsKf3YaWpG}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "_HsKf3YaWpG", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1682/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1682/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1682/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1682/Authors|ICLR.cc/2021/Conference/Paper1682/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1682/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923856941, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1682/-/Official_Comment"}}}, {"id": "H-0gNaURDl", "original": null, "number": 8, "cdate": 1606131491296, "ddate": null, "tcdate": 1606131491296, "tmdate": 1606131491296, "tddate": null, "forum": "_HsKf3YaWpG", "replyto": "zIMmJsNyKJs", "invitation": "ICLR.cc/2021/Conference/Paper1682/-/Official_Comment", "content": {"title": "Discussion", "comment": "Kindly let us know if our response below addressed your concerns. We are happy to answer if there are additional issues/questions."}, "signatures": ["ICLR.cc/2021/Conference/Paper1682/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1682/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Uniform Priors for Data-Efficient Transfer", "authorids": ["~Samarth_Sinha1", "~Karsten_Roth1", "~Anirudh_Goyal1", "~Marzyeh_Ghassemi1", "~Hugo_Larochelle1", "~Animesh_Garg1"], "authors": ["Samarth Sinha", "Karsten Roth", "Anirudh Goyal", "Marzyeh Ghassemi", "Hugo Larochelle", "Animesh Garg"], "keywords": ["Meta Learning", "Deep Metric Learning", "Transfer Learning"], "abstract": "Deep Neural Networks have shown great promise on a variety of downstream applications; but their ability to adapt and generalize to new data and tasks remains a challenging problem. However, the ability to perform few or zero-shot adaptation to novel tasks is important for the scalability and deployment of machine learning models.  It is therefore crucial to understand what makes for good, transferable features in deep networks that best allow for such adaptation.  In this paper, we shed light on this by showing that features that are most transferable have high uniformity in the embedding space and propose a uniformity regularization scheme that encourages better transfer and feature reuse.  We evaluate the regularization on its ability to facilitate adaptation to unseen tasks and data, for which we conduct a thorough experimental study covering four relevant, and distinct domains:  few-shot Meta-Learning, Deep Metric Learning, Zero-Shot Domain Adaptation, as well as Out-of-Distribution classification.   Across all experiments,  we show that uniformity regularization consistently offers benefits over baseline methods and is able to achieve state-of-the-art performance in Deep Metric Learning and Meta-Learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "sinha|uniform_priors_for_dataefficient_transfer", "one-sentence_summary": "We observe that the uniformity of embeddings is important for better transfer and adaptation, and propose a simple technique to promote uniformity which improve meta learning, deep metric learning, zero-shot domain adaptation and OOD generalization.", "pdf": "/pdf/56dba903a0cf8efcffb726406bf47b861dd084ee.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=cT6INmZ3om", "_bibtex": "@misc{\nsinha2021uniform,\ntitle={Uniform Priors for Data-Efficient Transfer},\nauthor={Samarth Sinha and Karsten Roth and Anirudh Goyal and Marzyeh Ghassemi and Hugo Larochelle and Animesh Garg},\nyear={2021},\nurl={https://openreview.net/forum?id=_HsKf3YaWpG}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "_HsKf3YaWpG", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1682/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1682/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1682/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1682/Authors|ICLR.cc/2021/Conference/Paper1682/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1682/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923856941, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1682/-/Official_Comment"}}}, {"id": "eGhIuzGkaU", "original": null, "number": 6, "cdate": 1605808874360, "ddate": null, "tcdate": 1605808874360, "tmdate": 1605808907243, "tddate": null, "forum": "_HsKf3YaWpG", "replyto": "_HsKf3YaWpG", "invitation": "ICLR.cc/2021/Conference/Paper1682/-/Official_Comment", "content": {"title": "Overall Comment", "comment": "We thank each of the reviewers for their time to review the paper and appreciating the simplicity of our approach, the consistent and wide-ranging applicability, our comprehensive experimentation as well as the writing of the paper! \n\nWe have addressed each reviewer individually. We have added the following to the manuscript:\n\n* Added UMAP visualizations, as suggested by 3 reviewers, on both the training and test distributions of the data. We qualitatively see that adding regularization doesn\u2019t hurt the learning of the training distribution, but it visually increases the distribution density of the features. We have included these visualizations in the paper (Section 4.6, Figure 1).\n* We quantitatively evaluated changes in feature space density following Roth et al., where we quantitatively find an increased density as well when applying uniformity regularization. This is in line with insights made by Roth et al., which show that \u201can increased embedding space density is linked to stronger generalisation.\u201d (in section 5 of Roth et al.). We have added this discussion in the paper (Section 4.6).\n* Added direct comparisons with Wang & Isola on 2 meta-learners over 4 datasets. We see consistent improvements over this baseline (Table 2).\n* Added two more popular regularization techniques: Dropout and L2 regularization on 2 meta-learners over 4 datasets. We see consistent improvements over both of the baselines (Table 2).\n* Finally, we have added multiple clarifications in the paper and better differentiated our work from previous papers.\n\n\n_Roth et al., 2020, Revisiting Training Strategies and Generalization Performance in Deep Metric Learning, ICML 2020_\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1682/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1682/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Uniform Priors for Data-Efficient Transfer", "authorids": ["~Samarth_Sinha1", "~Karsten_Roth1", "~Anirudh_Goyal1", "~Marzyeh_Ghassemi1", "~Hugo_Larochelle1", "~Animesh_Garg1"], "authors": ["Samarth Sinha", "Karsten Roth", "Anirudh Goyal", "Marzyeh Ghassemi", "Hugo Larochelle", "Animesh Garg"], "keywords": ["Meta Learning", "Deep Metric Learning", "Transfer Learning"], "abstract": "Deep Neural Networks have shown great promise on a variety of downstream applications; but their ability to adapt and generalize to new data and tasks remains a challenging problem. However, the ability to perform few or zero-shot adaptation to novel tasks is important for the scalability and deployment of machine learning models.  It is therefore crucial to understand what makes for good, transferable features in deep networks that best allow for such adaptation.  In this paper, we shed light on this by showing that features that are most transferable have high uniformity in the embedding space and propose a uniformity regularization scheme that encourages better transfer and feature reuse.  We evaluate the regularization on its ability to facilitate adaptation to unseen tasks and data, for which we conduct a thorough experimental study covering four relevant, and distinct domains:  few-shot Meta-Learning, Deep Metric Learning, Zero-Shot Domain Adaptation, as well as Out-of-Distribution classification.   Across all experiments,  we show that uniformity regularization consistently offers benefits over baseline methods and is able to achieve state-of-the-art performance in Deep Metric Learning and Meta-Learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "sinha|uniform_priors_for_dataefficient_transfer", "one-sentence_summary": "We observe that the uniformity of embeddings is important for better transfer and adaptation, and propose a simple technique to promote uniformity which improve meta learning, deep metric learning, zero-shot domain adaptation and OOD generalization.", "pdf": "/pdf/56dba903a0cf8efcffb726406bf47b861dd084ee.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=cT6INmZ3om", "_bibtex": "@misc{\nsinha2021uniform,\ntitle={Uniform Priors for Data-Efficient Transfer},\nauthor={Samarth Sinha and Karsten Roth and Anirudh Goyal and Marzyeh Ghassemi and Hugo Larochelle and Animesh Garg},\nyear={2021},\nurl={https://openreview.net/forum?id=_HsKf3YaWpG}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "_HsKf3YaWpG", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1682/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1682/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1682/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1682/Authors|ICLR.cc/2021/Conference/Paper1682/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1682/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923856941, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1682/-/Official_Comment"}}}, {"id": "XHL-UbPlcza", "original": null, "number": 5, "cdate": 1605808114870, "ddate": null, "tcdate": 1605808114870, "tmdate": 1605808114870, "tddate": null, "forum": "_HsKf3YaWpG", "replyto": "XHwU-CklU4", "invitation": "ICLR.cc/2021/Conference/Paper1682/-/Official_Comment", "content": {"title": "Response to AnonReviewer2", "comment": "Thank you for your review, we have addressed each point individually.\n\n__1)  \u201cEquation 6, which is the final proposed loss, seems incomplete, which seems like a key issue. As I understand, Eqn 6 must consider the discriminator loss described in Eqn 4 too. Considering only the generator (that is the network q(z|x) till the last layer) is a necessary condition but may not be a sufficient condition to optimize the end-to-end network. I will be happy to know if I have missed something here.\u201d__\n\nThank you for the correction! We have fixed Equation 6 where we have written the full min-max objective of the task-network and Discriminator.\n\n\n__2)  \u201cThe paper seems to have missed some key references that have addressed similar problems - [1] and [2] listed at the end of this review. Both of these efforts show task transferability for computer vision tasks. This is important, considering the similarity of objectives.\u201d__\n\nThank you for the reference! We have added the references in the related work. We do note that our method is strictly different from the mentioned works, with the main commonality being the study of generalization, however in different setups. \nWe have experimentally shown that our method can work synergistically with many other methods and across diverse experiments.\n\n\n__3) \u201c The paper states that it performs transfer between MNIST, SVHN and USPS (model trained on source data and tested on target data). It was not clear how MNIST --> SVHN was done. Wouldn't there be a mismatch of input channels in this case (SVHN is color, while MNIST is not).\u201d__\n\nThis is a common task in zero-shot domain adaptation (Tzeng et al., Table 2). To perform experiments with SVHN -> MNIST, we simply repeat the single channel of the MNIST images 3-times for testing. \nTzeng et al., 2017 Adversarial Discriminative Domain Adaptation, CVPR 2017\n\n__4)  \u201cA feature space visualization of the proposed method and other baselines would have been very useful to directly compare and understand the claim and advantages of imposing uniform regularizer.\u201d__\n\nThank you for the suggestion. We have included UMAPs of the features on the training and test data for our deep metric learning study. We see that by adding uniformity regularization, improvements on the density of the embeddings can be seen, especially notable on the test data. \nThis shows that uniformity regularization does indeed have an influence on the feature space. \nTo further quantify this influence, we have also quantitatively evaluated the embedding space density following Roth et al. over the non-reduced features, as notable context was likely lost during the process of significant dimensionality reduction. We find that applying uniformity regularization significantly changes the embedding space density, which has been marked as a potential driver for generalization in Roth et al.. \n\n_Roth et al., 2020, Revisiting Training Strategies and Generalization Performance in Deep Metric Learning_\n\n__5)  \u201cOne broader question that may be relevant (may not directly relate to a decision on the paper): How will uniformity on task space be affected if we leveraged a method such as Mixup [3]? How will the task space, in that case, look like?\u201d__\n\nThe effect of MixUp can be related to confidence regularization using \u201cVicinall Risk Minimization\u201d, where the method works by reducing model over-confidence and undesirable oscillations when making predictions on samples differing from those encountered during training, which they achieve by training on linear combinations of images and labels. \nWe are strictly different from MixUp as we encourage uniformity in the feature space of the models which improves generalization on many downstream tasks, instead of performing confidence regularization. \nMixUp is commonly used for such confidence regularization of models in semi-supervised literature.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1682/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1682/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Uniform Priors for Data-Efficient Transfer", "authorids": ["~Samarth_Sinha1", "~Karsten_Roth1", "~Anirudh_Goyal1", "~Marzyeh_Ghassemi1", "~Hugo_Larochelle1", "~Animesh_Garg1"], "authors": ["Samarth Sinha", "Karsten Roth", "Anirudh Goyal", "Marzyeh Ghassemi", "Hugo Larochelle", "Animesh Garg"], "keywords": ["Meta Learning", "Deep Metric Learning", "Transfer Learning"], "abstract": "Deep Neural Networks have shown great promise on a variety of downstream applications; but their ability to adapt and generalize to new data and tasks remains a challenging problem. However, the ability to perform few or zero-shot adaptation to novel tasks is important for the scalability and deployment of machine learning models.  It is therefore crucial to understand what makes for good, transferable features in deep networks that best allow for such adaptation.  In this paper, we shed light on this by showing that features that are most transferable have high uniformity in the embedding space and propose a uniformity regularization scheme that encourages better transfer and feature reuse.  We evaluate the regularization on its ability to facilitate adaptation to unseen tasks and data, for which we conduct a thorough experimental study covering four relevant, and distinct domains:  few-shot Meta-Learning, Deep Metric Learning, Zero-Shot Domain Adaptation, as well as Out-of-Distribution classification.   Across all experiments,  we show that uniformity regularization consistently offers benefits over baseline methods and is able to achieve state-of-the-art performance in Deep Metric Learning and Meta-Learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "sinha|uniform_priors_for_dataefficient_transfer", "one-sentence_summary": "We observe that the uniformity of embeddings is important for better transfer and adaptation, and propose a simple technique to promote uniformity which improve meta learning, deep metric learning, zero-shot domain adaptation and OOD generalization.", "pdf": "/pdf/56dba903a0cf8efcffb726406bf47b861dd084ee.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=cT6INmZ3om", "_bibtex": "@misc{\nsinha2021uniform,\ntitle={Uniform Priors for Data-Efficient Transfer},\nauthor={Samarth Sinha and Karsten Roth and Anirudh Goyal and Marzyeh Ghassemi and Hugo Larochelle and Animesh Garg},\nyear={2021},\nurl={https://openreview.net/forum?id=_HsKf3YaWpG}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "_HsKf3YaWpG", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1682/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1682/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1682/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1682/Authors|ICLR.cc/2021/Conference/Paper1682/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1682/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923856941, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1682/-/Official_Comment"}}}, {"id": "bkXZdAFbHu_", "original": null, "number": 4, "cdate": 1605808026002, "ddate": null, "tcdate": 1605808026002, "tmdate": 1605808026002, "tddate": null, "forum": "_HsKf3YaWpG", "replyto": "oTbIk6nG4Bq", "invitation": "ICLR.cc/2021/Conference/Paper1682/-/Official_Comment", "content": {"title": "Response to AnonReviewer1", "comment": "Thank you for your review, we have addressed each point individually.\n\n__1)  \u201cOne lacking aspect is that the authors provide no evidence on how the method works, neither quantitatively (the distance between uniform distribution and learned feature distribution) nor qualitatively (e.g. t-sne visualization on the learned feature).\u201d... \u201cthe authors fail to dig deeper into it, and lack sufficient ablation experiments to demonstrate the method works as expected\u201d... \u201cthis paper may contain some ideas that publishable, however the authors fail to dig deeper into it, and lack sufficient ablation experiments to demonstrate the method works as expected.\u201d__\n\nTab. 1 actually shows that there is evidence that as the variance of the prior increases, the model is able to perform better than the baseline. As the variance of a gaussian increases (over a limited interval, which one can realistically assume), loosely speaking, the distribution gets more \u201cuniform\u201d. And when we place a uniform prior, we see that the network performs the best. This further motivates our findings, showing that the uniformity in the learned posterior is key to learning a generalizable model.\nWe have further provided direct comparisons with Wang & Isola on baseline meta-learning tasks in Table 2 where we see that uniformity regularization consistently performs better than the baseline on both meta-learners (which requires generalization) and across 4 datasets. \nWe have also included UMAP plots of the embeddings on the training and test data in our deep metric learning where the test data is previously unseen by the model. We see that by adding uniformity regularization, improvements on the density of the embeddings can be seen, especially notable on the test data. \nThis shows that uniformity regularization does indeed have an influence on the feature space. \nTo further quantify this influence, we have also quantitatively evaluated the embedding space density following Roth et al. over the non-reduced features, as notable context was likely lost during the process of significant dimensionality reduction.We find that applying uniformity regularization significantly changes the embedding space density, which has been marked as a potential driver for generalization in Roth et al.. \nWe have also added baselines of common regularization techniques, dropout and L2 regularization, in Table 2, and continue to see consistent improvement using uniformity regularization instead.\n\n_Roth et al., 2020, Revisiting Training Strategies and Generalization Performance in Deep Metric Learning, ICML 2020_\n\n\n\n\n__2)  \u201cAnother point could improve is that I suspect the effect of uniformity is quite like imposing margin on loss function (such as AM-softmax, arcface, etc), it is better to discuss and compare with them.\u201d__\n\nOur approach is much more fundamental than using margin-based methods in that it actively tries to map features to a Uniform Prior, and can be used alongside such methods very well and is not in direct competition (see DML experiments). We directly experiment with very related, proxy-based baselines and see improvements over them (\u201cSoftmax\u201d, Table 3).\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1682/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1682/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Uniform Priors for Data-Efficient Transfer", "authorids": ["~Samarth_Sinha1", "~Karsten_Roth1", "~Anirudh_Goyal1", "~Marzyeh_Ghassemi1", "~Hugo_Larochelle1", "~Animesh_Garg1"], "authors": ["Samarth Sinha", "Karsten Roth", "Anirudh Goyal", "Marzyeh Ghassemi", "Hugo Larochelle", "Animesh Garg"], "keywords": ["Meta Learning", "Deep Metric Learning", "Transfer Learning"], "abstract": "Deep Neural Networks have shown great promise on a variety of downstream applications; but their ability to adapt and generalize to new data and tasks remains a challenging problem. However, the ability to perform few or zero-shot adaptation to novel tasks is important for the scalability and deployment of machine learning models.  It is therefore crucial to understand what makes for good, transferable features in deep networks that best allow for such adaptation.  In this paper, we shed light on this by showing that features that are most transferable have high uniformity in the embedding space and propose a uniformity regularization scheme that encourages better transfer and feature reuse.  We evaluate the regularization on its ability to facilitate adaptation to unseen tasks and data, for which we conduct a thorough experimental study covering four relevant, and distinct domains:  few-shot Meta-Learning, Deep Metric Learning, Zero-Shot Domain Adaptation, as well as Out-of-Distribution classification.   Across all experiments,  we show that uniformity regularization consistently offers benefits over baseline methods and is able to achieve state-of-the-art performance in Deep Metric Learning and Meta-Learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "sinha|uniform_priors_for_dataefficient_transfer", "one-sentence_summary": "We observe that the uniformity of embeddings is important for better transfer and adaptation, and propose a simple technique to promote uniformity which improve meta learning, deep metric learning, zero-shot domain adaptation and OOD generalization.", "pdf": "/pdf/56dba903a0cf8efcffb726406bf47b861dd084ee.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=cT6INmZ3om", "_bibtex": "@misc{\nsinha2021uniform,\ntitle={Uniform Priors for Data-Efficient Transfer},\nauthor={Samarth Sinha and Karsten Roth and Anirudh Goyal and Marzyeh Ghassemi and Hugo Larochelle and Animesh Garg},\nyear={2021},\nurl={https://openreview.net/forum?id=_HsKf3YaWpG}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "_HsKf3YaWpG", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1682/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1682/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1682/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1682/Authors|ICLR.cc/2021/Conference/Paper1682/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1682/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923856941, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1682/-/Official_Comment"}}}, {"id": "SynvsSoFhe7", "original": null, "number": 3, "cdate": 1605807947707, "ddate": null, "tcdate": 1605807947707, "tmdate": 1605807947707, "tddate": null, "forum": "_HsKf3YaWpG", "replyto": "MPALyO-UEVD", "invitation": "ICLR.cc/2021/Conference/Paper1682/-/Official_Comment", "content": {"title": "Response to AnonReviewer3", "comment": "Thank you for your review, we have addressed each point individually.\n\n__1)  \u201cThe idea of maximizing the entropy of the learned representation to increase its generalization capability has been explored before. In addition to the representation learning perspective in [Wang and Isola] and [2], [1] explicitly studied entropy maximization regularizations for generalization in meta-learning, although their regularization objectives are different. I would appreciate it if the authors could compare their proposed regularization with the ones in [1].\u201d__\n\n[1] suggest to either add an entropy term to the output probabilities of the meta-learner, a setting depending on the classification nature of the specific meta-learning task at hand, or by defining \u201cinequality measures\u201d over each task loss to learn less task-dependent representations. The relation to entropy maximization is much less clear, the performance relies on the specific choice of inequality metric and application is still limited to a multi-task setting akin to episodic Meta-Learning. Our approach is much more generic, being able to introduce the notion of uniformity over a much larger variety of cases, independent of the specific task setup and having a clearer link to feature uniformity/max-entropy.\n\nWe have adjusted the draft to reference [1] (Related Work section), and have expanded the introduction to differentiate our work more from [Wang and Isola] (see introduction). We also have added direct comparisons to Wang & Isola in table 2, where we see that adding uniformity regularization consistently outperforms Wang & Isola for both meta-learners on 4 different datasets. This is especially interesting as our regularization approach is not limited to hyperspherical representation spaces.\nWe have also added baselines for common regularization techniques, dropout and L2 regularization,  in Table 2, and continue to see consistent improvement using uniformity regularization instead.\n\n\n__2)  \u201cFor uniformity regularisation, the authors propose an adversarial learning scheme. This way of inducing uniformity in the representations is also not new. For e.g. Hjelm et al [2] used the exact strategy to induce uniformity in their self-supervised representations learning method.\u201d__\n\nOur work shares architectural similarities with Hjelm et al., who apply a similar adversarial objective to impose certain properties on the features learned through self-supervised representation learning. We note that, similar to Hjelm et al., we do not claim novelty in the adversarial optimization scheme, which was already introduced for Adversarial Autoencoders in Makhzani et al..\nHowever, while Hjelm et al. also matched their representations to a uniform distribution, no detailed reasons for this specific choice were given. Instead, to the best of our knowledge, our work is the first to show that it is exactly the uniformity of feature distributions introduced by our uniformity regularization that facilitate fast adaptation and transfer to novel data and tasks in neural networks, regardless of the specific application domain.\nWe have included differentiation to Hjelm et al. in our Related Work section.\n\n\n_Hjelm et al.: Learning deep representations by mutual information estimation\nand maximization._\n\n_Makhzani et al.: Adversarial Autoencoders._\n\n\n__3)  \u201cAs noted by the authors, [Wang and Isola] show that the self-supervised contrastive objectives also induce uniformity in the learned representations (although in hypersphere). Such contrastive objectives have recently been used to improve transferability of meta learned representations for e.g. in [3, 4]. I would appreciate it if authors could comment on how those regularizations are different and why they should not be compared with the proposed method.\u201d__\n\nWang & Isola primarily state that specifically encouraging uniformity over a hypersphere is beneficial to self-supervised contrastive learning. They only theoretically show that asymptotically, optimizing contrastive losses optimizes for these uniformity quantities. Practically, Wang & Isola show that optimization of proxies to this uniformity property can further improve generalization of such contrastive approaches.\nThis is similar to our motivation, in which we wish to encourage the notion of uniformity for improved generalization. However, our regularization matches a more explicit uniformity prior over *any* feature space, and as such is not limited to hyperspherical representation spaces. This opens up application to a much wider range of problem settings, which we experimentally support.\n\nFinally, we have added direct comparisons to Wang & Isola in table 2, where we see that adding uniformity regularization consistently outperforms Wang & Isola for both meta-learners on 4 different datasets.\n\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1682/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1682/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Uniform Priors for Data-Efficient Transfer", "authorids": ["~Samarth_Sinha1", "~Karsten_Roth1", "~Anirudh_Goyal1", "~Marzyeh_Ghassemi1", "~Hugo_Larochelle1", "~Animesh_Garg1"], "authors": ["Samarth Sinha", "Karsten Roth", "Anirudh Goyal", "Marzyeh Ghassemi", "Hugo Larochelle", "Animesh Garg"], "keywords": ["Meta Learning", "Deep Metric Learning", "Transfer Learning"], "abstract": "Deep Neural Networks have shown great promise on a variety of downstream applications; but their ability to adapt and generalize to new data and tasks remains a challenging problem. However, the ability to perform few or zero-shot adaptation to novel tasks is important for the scalability and deployment of machine learning models.  It is therefore crucial to understand what makes for good, transferable features in deep networks that best allow for such adaptation.  In this paper, we shed light on this by showing that features that are most transferable have high uniformity in the embedding space and propose a uniformity regularization scheme that encourages better transfer and feature reuse.  We evaluate the regularization on its ability to facilitate adaptation to unseen tasks and data, for which we conduct a thorough experimental study covering four relevant, and distinct domains:  few-shot Meta-Learning, Deep Metric Learning, Zero-Shot Domain Adaptation, as well as Out-of-Distribution classification.   Across all experiments,  we show that uniformity regularization consistently offers benefits over baseline methods and is able to achieve state-of-the-art performance in Deep Metric Learning and Meta-Learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "sinha|uniform_priors_for_dataefficient_transfer", "one-sentence_summary": "We observe that the uniformity of embeddings is important for better transfer and adaptation, and propose a simple technique to promote uniformity which improve meta learning, deep metric learning, zero-shot domain adaptation and OOD generalization.", "pdf": "/pdf/56dba903a0cf8efcffb726406bf47b861dd084ee.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=cT6INmZ3om", "_bibtex": "@misc{\nsinha2021uniform,\ntitle={Uniform Priors for Data-Efficient Transfer},\nauthor={Samarth Sinha and Karsten Roth and Anirudh Goyal and Marzyeh Ghassemi and Hugo Larochelle and Animesh Garg},\nyear={2021},\nurl={https://openreview.net/forum?id=_HsKf3YaWpG}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "_HsKf3YaWpG", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1682/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1682/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1682/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1682/Authors|ICLR.cc/2021/Conference/Paper1682/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1682/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923856941, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1682/-/Official_Comment"}}}, {"id": "y-FmtWmcwcG", "original": null, "number": 2, "cdate": 1605807796591, "ddate": null, "tcdate": 1605807796591, "tmdate": 1605807796591, "tddate": null, "forum": "_HsKf3YaWpG", "replyto": "zIMmJsNyKJs", "invitation": "ICLR.cc/2021/Conference/Paper1682/-/Official_Comment", "content": {"title": "Response to AnonReviewer4", "comment": "Thank you for your review, we have addressed each point individually.\n\n\n__1)  \u201cThe observation that uniformity helps transference was already observed by previous works, as the authors acknowledge. From that, the derivation of the regularization term is relatively straightforward\u201d.__\n\nWe do not see how the idea of aligning arbitrary feature spaces with a Uniformity Prior would follow \u201crelatively straightforward\u201d from the previous works. It would be appreciated if the reviewer could clarify this statement.\n\nIn addition, we believe that it should not matter how difficult an idea is, much rather should it matter that, to the best of our knowledge, our paper is the first to examine the direct benefits of aligning the feature spaces with a uniform distribution prior for improved adaptation and generalization to novel tasks and data, which we further support with experiments across multiple diverse application domains. \nBy directly encouraging uniformity on the features of the learned model, we set the state-of-the-art on two large scale diverse tasks in meta-learning and deep metric learning.\n\n\n__2)  \u201cThere is no qualitative analysis to illustrate and provide insights about why uniformity helps nor how the regularization term changes the latent representations.\u201d__\n\nThank you for the suggestion. We include some UMAP plots of the embeddings on the training and test data in our deep metric learning where the test data is previously unseen by the model. We see that by adding uniformity regularization, features are encouraged to follow a higher density distribution, which is especially visible on the test data. \nAs the reduction in dimensionality can lose important high-dimensional context, we also quantitatively evaluate the embedding space density on the training data following Roth et al., for which we see a notable increase in the feature space density (30%), which in turn can be linked to improved generalization, as found by Roth et al., 2020.\n\nWe have made sure to add this discussion as well as the visualizations to our draft (Section 4.6, Figure 1).\n\n\n_Roth et al., 2020, Revisiting Training Strategies and Generalization Performance in Deep Metric Learning, ICML 2020_\n\n\n__3) \u201cThe actual methods used in different tasks are often not explained. For example, the method for the out-of-distribution task is not explained. Please provide concise yet helpful explanations.\u201d__\n\nThe performance reported is the accuracy of the model, when the data is changed in a similar way as the generalization experiments in Sinha et al.. More specifically, we randomly apply the following transformations to each image on the test set to measure the generalization ability of the model: randomly translate [-4, 4] pixel, randomly rotate by [-30, 30] degrees and randomly scale by a factor of [0.75, 1.25.]. \nWe have updated the draft (Section 4.5) to include more detail.\n\n\n_Sinha et al.: DIBS: Diversity inducing Information Bottleneck in Model Ensembles_\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1682/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1682/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Uniform Priors for Data-Efficient Transfer", "authorids": ["~Samarth_Sinha1", "~Karsten_Roth1", "~Anirudh_Goyal1", "~Marzyeh_Ghassemi1", "~Hugo_Larochelle1", "~Animesh_Garg1"], "authors": ["Samarth Sinha", "Karsten Roth", "Anirudh Goyal", "Marzyeh Ghassemi", "Hugo Larochelle", "Animesh Garg"], "keywords": ["Meta Learning", "Deep Metric Learning", "Transfer Learning"], "abstract": "Deep Neural Networks have shown great promise on a variety of downstream applications; but their ability to adapt and generalize to new data and tasks remains a challenging problem. However, the ability to perform few or zero-shot adaptation to novel tasks is important for the scalability and deployment of machine learning models.  It is therefore crucial to understand what makes for good, transferable features in deep networks that best allow for such adaptation.  In this paper, we shed light on this by showing that features that are most transferable have high uniformity in the embedding space and propose a uniformity regularization scheme that encourages better transfer and feature reuse.  We evaluate the regularization on its ability to facilitate adaptation to unseen tasks and data, for which we conduct a thorough experimental study covering four relevant, and distinct domains:  few-shot Meta-Learning, Deep Metric Learning, Zero-Shot Domain Adaptation, as well as Out-of-Distribution classification.   Across all experiments,  we show that uniformity regularization consistently offers benefits over baseline methods and is able to achieve state-of-the-art performance in Deep Metric Learning and Meta-Learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "sinha|uniform_priors_for_dataefficient_transfer", "one-sentence_summary": "We observe that the uniformity of embeddings is important for better transfer and adaptation, and propose a simple technique to promote uniformity which improve meta learning, deep metric learning, zero-shot domain adaptation and OOD generalization.", "pdf": "/pdf/56dba903a0cf8efcffb726406bf47b861dd084ee.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=cT6INmZ3om", "_bibtex": "@misc{\nsinha2021uniform,\ntitle={Uniform Priors for Data-Efficient Transfer},\nauthor={Samarth Sinha and Karsten Roth and Anirudh Goyal and Marzyeh Ghassemi and Hugo Larochelle and Animesh Garg},\nyear={2021},\nurl={https://openreview.net/forum?id=_HsKf3YaWpG}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "_HsKf3YaWpG", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1682/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1682/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1682/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1682/Authors|ICLR.cc/2021/Conference/Paper1682/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1682/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923856941, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1682/-/Official_Comment"}}}, {"id": "oTbIk6nG4Bq", "original": null, "number": 2, "cdate": 1603978262679, "ddate": null, "tcdate": 1603978262679, "tmdate": 1605024383716, "tddate": null, "forum": "_HsKf3YaWpG", "replyto": "_HsKf3YaWpG", "invitation": "ICLR.cc/2021/Conference/Paper1682/-/Official_Review", "content": {"title": "review", "review": "In this paper, the authors claimed that uniformity in embedding space if the key for good generalization, and then propose an adversarial training based method to improve the uniformity of feature space. The claim is from previous work, thus the key contribution is the way to impose such regularization. The method itself makes sense to me. \n\nOne lacking aspect is that the authors provide no evidence on how the method works, neither quantitatively (the distance between uniform distribution and learned feature distribution) nor qualitatively (e.g. t-sne visualization on the learned feature). \n\nAnother point could improve is that I suspect the effect of uniformity is quite like imposing margin on loss function (such as AM-softmax, arcface, etc), it is better to discuss and compare with them. \n\nI am not familiar with the dataset and SOTA performance used in evaluation. The results look reasonable to me, and could demonstrate the effectiveness of the proposed method.\n\nAbove all, I think this paper may contain some ideas that publishable, however the authors fail to dig deeper into it, and lack sufficient ablation experiments to demonstrate the method works as expected. ", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1682/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1682/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Uniform Priors for Data-Efficient Transfer", "authorids": ["~Samarth_Sinha1", "~Karsten_Roth1", "~Anirudh_Goyal1", "~Marzyeh_Ghassemi1", "~Hugo_Larochelle1", "~Animesh_Garg1"], "authors": ["Samarth Sinha", "Karsten Roth", "Anirudh Goyal", "Marzyeh Ghassemi", "Hugo Larochelle", "Animesh Garg"], "keywords": ["Meta Learning", "Deep Metric Learning", "Transfer Learning"], "abstract": "Deep Neural Networks have shown great promise on a variety of downstream applications; but their ability to adapt and generalize to new data and tasks remains a challenging problem. However, the ability to perform few or zero-shot adaptation to novel tasks is important for the scalability and deployment of machine learning models.  It is therefore crucial to understand what makes for good, transferable features in deep networks that best allow for such adaptation.  In this paper, we shed light on this by showing that features that are most transferable have high uniformity in the embedding space and propose a uniformity regularization scheme that encourages better transfer and feature reuse.  We evaluate the regularization on its ability to facilitate adaptation to unseen tasks and data, for which we conduct a thorough experimental study covering four relevant, and distinct domains:  few-shot Meta-Learning, Deep Metric Learning, Zero-Shot Domain Adaptation, as well as Out-of-Distribution classification.   Across all experiments,  we show that uniformity regularization consistently offers benefits over baseline methods and is able to achieve state-of-the-art performance in Deep Metric Learning and Meta-Learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "sinha|uniform_priors_for_dataefficient_transfer", "one-sentence_summary": "We observe that the uniformity of embeddings is important for better transfer and adaptation, and propose a simple technique to promote uniformity which improve meta learning, deep metric learning, zero-shot domain adaptation and OOD generalization.", "pdf": "/pdf/56dba903a0cf8efcffb726406bf47b861dd084ee.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=cT6INmZ3om", "_bibtex": "@misc{\nsinha2021uniform,\ntitle={Uniform Priors for Data-Efficient Transfer},\nauthor={Samarth Sinha and Karsten Roth and Anirudh Goyal and Marzyeh Ghassemi and Hugo Larochelle and Animesh Garg},\nyear={2021},\nurl={https://openreview.net/forum?id=_HsKf3YaWpG}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "_HsKf3YaWpG", "replyto": "_HsKf3YaWpG", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1682/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538113134, "tmdate": 1606915792826, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1682/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1682/-/Official_Review"}}}, {"id": "zIMmJsNyKJs", "original": null, "number": 4, "cdate": 1604019941867, "ddate": null, "tcdate": 1604019941867, "tmdate": 1605024383649, "tddate": null, "forum": "_HsKf3YaWpG", "replyto": "_HsKf3YaWpG", "invitation": "ICLR.cc/2021/Conference/Paper1682/-/Official_Review", "content": {"title": "Simple yet effective regularization trick with numerous applications", "review": "The authors argue that uniform priors for the high-level latent representations improve transferability, which is beneficial in a number of tasks involving transference. The approach is evaluated on deep metric learning, zero-shot domain adaptation and few-shot meta-learning. The authors propose a uniformity regularization term on the latent representation, implemented as an adversarial discrepancy. The results show consistent improvement in the different tasks.\n\nStrengths\n- The method is simple, yet effective.\n- The paper is easy to follow and well presented.\n- The approach is relevant and can be applied to multiple problems.\n- The evaluation is comprehensive, showing consistent moderate gains.\n\nWeaknesses\n- The observation that uniformity helps transference was already observed by previous works, as the authors acknowledge. From that, the derivation of the regularization term is relatively straightforward.\n- There is no qualitative analysis to illustrate and provide insights about why uniformity helps nor how the regularization term changes the latent representations.\n- The actual methods used in different tasks are often not explained. For example, the method for the out-of-distribution task is not explained. Please provide concise yet helpful explanations.\n\nIn my opinion, the work shows an interesting contribution, but it would benefit from qualitative analysis that could provide more insights.", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1682/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1682/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Uniform Priors for Data-Efficient Transfer", "authorids": ["~Samarth_Sinha1", "~Karsten_Roth1", "~Anirudh_Goyal1", "~Marzyeh_Ghassemi1", "~Hugo_Larochelle1", "~Animesh_Garg1"], "authors": ["Samarth Sinha", "Karsten Roth", "Anirudh Goyal", "Marzyeh Ghassemi", "Hugo Larochelle", "Animesh Garg"], "keywords": ["Meta Learning", "Deep Metric Learning", "Transfer Learning"], "abstract": "Deep Neural Networks have shown great promise on a variety of downstream applications; but their ability to adapt and generalize to new data and tasks remains a challenging problem. However, the ability to perform few or zero-shot adaptation to novel tasks is important for the scalability and deployment of machine learning models.  It is therefore crucial to understand what makes for good, transferable features in deep networks that best allow for such adaptation.  In this paper, we shed light on this by showing that features that are most transferable have high uniformity in the embedding space and propose a uniformity regularization scheme that encourages better transfer and feature reuse.  We evaluate the regularization on its ability to facilitate adaptation to unseen tasks and data, for which we conduct a thorough experimental study covering four relevant, and distinct domains:  few-shot Meta-Learning, Deep Metric Learning, Zero-Shot Domain Adaptation, as well as Out-of-Distribution classification.   Across all experiments,  we show that uniformity regularization consistently offers benefits over baseline methods and is able to achieve state-of-the-art performance in Deep Metric Learning and Meta-Learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "sinha|uniform_priors_for_dataefficient_transfer", "one-sentence_summary": "We observe that the uniformity of embeddings is important for better transfer and adaptation, and propose a simple technique to promote uniformity which improve meta learning, deep metric learning, zero-shot domain adaptation and OOD generalization.", "pdf": "/pdf/56dba903a0cf8efcffb726406bf47b861dd084ee.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=cT6INmZ3om", "_bibtex": "@misc{\nsinha2021uniform,\ntitle={Uniform Priors for Data-Efficient Transfer},\nauthor={Samarth Sinha and Karsten Roth and Anirudh Goyal and Marzyeh Ghassemi and Hugo Larochelle and Animesh Garg},\nyear={2021},\nurl={https://openreview.net/forum?id=_HsKf3YaWpG}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "_HsKf3YaWpG", "replyto": "_HsKf3YaWpG", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1682/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538113134, "tmdate": 1606915792826, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1682/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1682/-/Official_Review"}}}, {"id": "MPALyO-UEVD", "original": null, "number": 3, "cdate": 1603996568911, "ddate": null, "tcdate": 1603996568911, "tmdate": 1605024383581, "tddate": null, "forum": "_HsKf3YaWpG", "replyto": "_HsKf3YaWpG", "invitation": "ICLR.cc/2021/Conference/Paper1682/-/Official_Review", "content": {"title": "A simple and effective approach, however some comparisons are missing.", "review": "#### Summary\n\nThe authors propose a regularization technique that maximizes the entropy of the learned representation by regularising it with uniformity prior. The uniformity prior is imposed via an adversarial objective function.\n\n#### Strong Points\n\n1. The proposed regularization can be easily added to the existing frameworks to improve their generalization ability.\n2. The methods improve upon the results of the baselines.\n3. The paper is well-written and easy to follow.\n\n#### Weak Points\n\nAlthough the method clearly improves the results, my main concern is with the novelty of the proposed method. Secondly, the paper needs to position itself better with respect to related work which proposes more or less similar regularisations for meta-learning frameworks. \n\n1. The idea of maximizing the entropy of the learned representation to increase its generalization capability has been explored before. In addition to the representation learning perspective in [Wang and Isola] and [2], [1] explicitly studied entropy maximization regularizations for generalization in meta-learning, although their regularization objectives are different. I would appreciate it if the authors could compare their proposed regularization with the ones in [1].\n\n2. For uniformity regularisation, the authors propose an adversarial learning scheme. This way of inducing uniformity in the representations is also not new. For e.g. Hjelm et al [2] used the exact strategy to induce uniformity in their self-supervised representations learning method.\n\n3. As noted by the authors, [Wang and Isola] show that the self-supervised contrastive objectives also induce uniformity in the learned representations (although in hypersphere). Such contrastive objectives have recently been used to improve transferability of meta learned representations for e.g. in [3, 4]. I would appreciate it if authors could comment on how those regularizations are different and why they should not be compared with the proposed method.\n\n\n[1] Jamal et al. Task-Agnostic Meta-Learning for Few-shot Learning.\n\n[2] Hjelm et al. Learning Deep Representations by Mutual Information Estimation and Maximization.\n\n[3] Medina et al. Self-Supervised Prototypical Transfer Learning for Few-Shot Classification.\n\n[4] Doersch et al. CrossTransformers: spatially-aware few-shot transfer.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1682/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1682/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Uniform Priors for Data-Efficient Transfer", "authorids": ["~Samarth_Sinha1", "~Karsten_Roth1", "~Anirudh_Goyal1", "~Marzyeh_Ghassemi1", "~Hugo_Larochelle1", "~Animesh_Garg1"], "authors": ["Samarth Sinha", "Karsten Roth", "Anirudh Goyal", "Marzyeh Ghassemi", "Hugo Larochelle", "Animesh Garg"], "keywords": ["Meta Learning", "Deep Metric Learning", "Transfer Learning"], "abstract": "Deep Neural Networks have shown great promise on a variety of downstream applications; but their ability to adapt and generalize to new data and tasks remains a challenging problem. However, the ability to perform few or zero-shot adaptation to novel tasks is important for the scalability and deployment of machine learning models.  It is therefore crucial to understand what makes for good, transferable features in deep networks that best allow for such adaptation.  In this paper, we shed light on this by showing that features that are most transferable have high uniformity in the embedding space and propose a uniformity regularization scheme that encourages better transfer and feature reuse.  We evaluate the regularization on its ability to facilitate adaptation to unseen tasks and data, for which we conduct a thorough experimental study covering four relevant, and distinct domains:  few-shot Meta-Learning, Deep Metric Learning, Zero-Shot Domain Adaptation, as well as Out-of-Distribution classification.   Across all experiments,  we show that uniformity regularization consistently offers benefits over baseline methods and is able to achieve state-of-the-art performance in Deep Metric Learning and Meta-Learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "sinha|uniform_priors_for_dataefficient_transfer", "one-sentence_summary": "We observe that the uniformity of embeddings is important for better transfer and adaptation, and propose a simple technique to promote uniformity which improve meta learning, deep metric learning, zero-shot domain adaptation and OOD generalization.", "pdf": "/pdf/56dba903a0cf8efcffb726406bf47b861dd084ee.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=cT6INmZ3om", "_bibtex": "@misc{\nsinha2021uniform,\ntitle={Uniform Priors for Data-Efficient Transfer},\nauthor={Samarth Sinha and Karsten Roth and Anirudh Goyal and Marzyeh Ghassemi and Hugo Larochelle and Animesh Garg},\nyear={2021},\nurl={https://openreview.net/forum?id=_HsKf3YaWpG}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "_HsKf3YaWpG", "replyto": "_HsKf3YaWpG", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1682/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538113134, "tmdate": 1606915792826, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1682/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1682/-/Official_Review"}}}], "count": 15}