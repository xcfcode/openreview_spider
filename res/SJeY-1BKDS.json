{"notes": [{"id": "SJeY-1BKDS", "original": "BJe1OnodDH", "number": 1552, "cdate": 1569439489470, "ddate": null, "tcdate": 1569439489470, "tmdate": 1583912050682, "tddate": null, "forum": "SJeY-1BKDS", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Understanding l4-based Dictionary Learning: Interpretation, Stability, and Robustness", "authors": ["Yuexiang Zhai", "Hermish Mehta", "Zhengyuan Zhou", "Yi Ma"], "authorids": ["ysz@berkeley.edu", "hermish@berkeley.edu", "zyzhou@stanford.edu", "yima@eecs.berkeley.edu"], "keywords": ["L4-norm Maximization", "Robust Dictionary Learning"], "TL;DR": "We compare the l4-norm based dictionary learning with PCA, ICA and show its stability as well as robustness.", "abstract": "Recently, the $\\ell^4$-norm maximization has been proposed to solve the sparse dictionary learning (SDL) problem. The simple MSP (matching, stretching, and projection) algorithm proposed by \\cite{zhai2019a} has proved surprisingly efficient and effective.  This paper aims to better understand this algorithm from its strong geometric and statistical connections with the classic PCA and ICA, as well as their associated fixed-point style algorithms. Such connections provide a unified way of viewing problems that pursue {\\em principal}, {\\em independent}, or {\\em sparse} components of high-dimensional data. Our studies reveal additional good properties of $\\ell^4$-maximization: not only is the MSP algorithm for sparse coding insensitive to small noise, but it is also robust to outliers and resilient to sparse corruptions. We provide statistical justification for such inherently nice properties. To corroborate the theoretical analysis, we also provide extensive and compelling experimental evidence with both synthetic data and real images.", "pdf": "/pdf/82f69ebb81f2a166efaed6c58908c26ecb0bde6b.pdf", "paperhash": "zhai|understanding_l4based_dictionary_learning_interpretation_stability_and_robustness", "code": "https://github.com/hermish/ZMZM-ICLR-2020", "_bibtex": "@inproceedings{\nZhai2020Understanding,\ntitle={Understanding l4-based Dictionary Learning: Interpretation, Stability, and Robustness},\nauthor={Yuexiang Zhai and Hermish Mehta and Zhengyuan Zhou and Yi Ma},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJeY-1BKDS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/5f236c0def40e09eb75fca84045199eb7f5ab47c.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 5, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "B7ZbWZM_af", "original": null, "number": 1, "cdate": 1576798726259, "ddate": null, "tcdate": 1576798726259, "tmdate": 1576800910220, "tddate": null, "forum": "SJeY-1BKDS", "replyto": "SJeY-1BKDS", "invitation": "ICLR.cc/2020/Conference/Paper1552/-/Decision", "content": {"decision": "Accept (Poster)", "comment": "Main content:\n\nBlind review #3 summarizes it well:\n\nThis paper presents results on Dictionary Learning through l4 maximization. The authors base this paper heavily off of the formulation and algorithm in Zhai et. al. (2019) \"Complete dictionary learning via l4-norm maximization over the orthogonal group\". The paper draws connections between complete dictionary learning, PCA, and ICA by pointing out similarities between the objectives functions that are optimized as well as the algorithms used. The paper further presents results on dictionary learning in the presence of different types of noise (AWGN, sparse corruptions, outliers) and show that the l4 objective is robust to different types of noise. Finally the authors apply different types of noise to synthetic and real images and show that the dictionaries that they learn are robust to the noise applied.\n\n--\n\nDiscussion:\n\nReviews agree about the interesting work, including the connections of complete dictionary learning with classic PCA and ICA (after further clarification during the rebuttal period). Additional empirical strengthening during the rebuttal period also addressed a reviewer concern.\n\n--\n\nRecommendation and justification:\n\nAs review #3 wrote, \"Overall this paper makes significant contributions by extending the work in [Zhai et. al's (2019) \"Complete dictionary learning via l4-norm maximization over the orthogonal group\"] to noisy dictionary learning settings\".", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Understanding l4-based Dictionary Learning: Interpretation, Stability, and Robustness", "authors": ["Yuexiang Zhai", "Hermish Mehta", "Zhengyuan Zhou", "Yi Ma"], "authorids": ["ysz@berkeley.edu", "hermish@berkeley.edu", "zyzhou@stanford.edu", "yima@eecs.berkeley.edu"], "keywords": ["L4-norm Maximization", "Robust Dictionary Learning"], "TL;DR": "We compare the l4-norm based dictionary learning with PCA, ICA and show its stability as well as robustness.", "abstract": "Recently, the $\\ell^4$-norm maximization has been proposed to solve the sparse dictionary learning (SDL) problem. The simple MSP (matching, stretching, and projection) algorithm proposed by \\cite{zhai2019a} has proved surprisingly efficient and effective.  This paper aims to better understand this algorithm from its strong geometric and statistical connections with the classic PCA and ICA, as well as their associated fixed-point style algorithms. Such connections provide a unified way of viewing problems that pursue {\\em principal}, {\\em independent}, or {\\em sparse} components of high-dimensional data. Our studies reveal additional good properties of $\\ell^4$-maximization: not only is the MSP algorithm for sparse coding insensitive to small noise, but it is also robust to outliers and resilient to sparse corruptions. We provide statistical justification for such inherently nice properties. To corroborate the theoretical analysis, we also provide extensive and compelling experimental evidence with both synthetic data and real images.", "pdf": "/pdf/82f69ebb81f2a166efaed6c58908c26ecb0bde6b.pdf", "paperhash": "zhai|understanding_l4based_dictionary_learning_interpretation_stability_and_robustness", "code": "https://github.com/hermish/ZMZM-ICLR-2020", "_bibtex": "@inproceedings{\nZhai2020Understanding,\ntitle={Understanding l4-based Dictionary Learning: Interpretation, Stability, and Robustness},\nauthor={Yuexiang Zhai and Hermish Mehta and Zhengyuan Zhou and Yi Ma},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJeY-1BKDS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/5f236c0def40e09eb75fca84045199eb7f5ab47c.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "SJeY-1BKDS", "replyto": "SJeY-1BKDS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795710071, "tmdate": 1576800258987, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1552/-/Decision"}}}, {"id": "H1gO_IImcB", "original": null, "number": 2, "cdate": 1572197999969, "ddate": null, "tcdate": 1572197999969, "tmdate": 1574443571360, "tddate": null, "forum": "SJeY-1BKDS", "replyto": "SJeY-1BKDS", "invitation": "ICLR.cc/2020/Conference/Paper1552/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "8: Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #3", "review": "This paper presents results on Dictionary Learning through l4 maximization. The authors base this paper heavily off of the formulation and algorithm in Zhai et. al. (2019) \"Complete dictionary learning via l4-norm maximization over the orthogonal group\". The paper draws connections between complete dictionary learning, PCA, and ICA by pointing out similarities between the objectives functions that are optimized as well as the algorithms used. The paper further presents results on dictionary learning in the presence of different types of noise (AWGN, sparse corruptions, outliers) and show that the l4 objective is robust to different types of noise. Finally the authors apply different types of noise to synthetic and real images and show that the dictionaries that they learn are robust to the noise applied.\n\nOverall this paper makes significant contributions by extending the work in the paper referenced above to noisy dictionary learning settings and I would vote to accept based on these results.\n\nThe connections between Complete Dictionary Learning, PCA and ICA are interesting, but the algorithmic analogies seem superficial in my opinion. There are a lot of algorithms which follow a projected/proximal gradient descent scheme. If there are any deeper connections between the specific algorithms discussed, they should be spelled out more clearly. One point of clarification that I would like to raise is the similarity between the kurtosis and l4 objectives. This paper could be strengthened by delineating the conditions under which one would learn an ICA basis vs a Complete Dictionary. It seems to me that the only difference is in the generative model, and that maximizing the same objective under different data conditions could return an ICA basis or a Complete Dictionary. \n\nThe robustness theory and experiments on synthetic data are reasonable and demonstrate that complete dictionary learning is robust to the different noise conditions. I would like to how this technique compares to other complete dictionary learning algorithms (ER-SpUD, Complete dictionary learning over the sphere - Sun, Qu, Wright 2015) and whether the l4 objective is unique in providing this robustness. Another central claim of Zhai et. al. 2019 seems to be that l4 maximization is able to recover the entire dictionary at once, vs other algorithms that recover the dictionary one column at a time. To test this, I would like to see runtime evaluations and comparisons to other algorithms. While the claim of recovering the entire dictionary is true, it seems to me that requiring an SVD at each iteration would be very expensive. I am not completely convinced that the approach of estimating the entire dictionary would indeed be faster.\n\nTo summarize, I believe this paper would be a good addition to the literature on l4 maximization algorithms for dictionary learning. I am willing to adjust my score based on responses to the above concerns.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper1552/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1552/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Understanding l4-based Dictionary Learning: Interpretation, Stability, and Robustness", "authors": ["Yuexiang Zhai", "Hermish Mehta", "Zhengyuan Zhou", "Yi Ma"], "authorids": ["ysz@berkeley.edu", "hermish@berkeley.edu", "zyzhou@stanford.edu", "yima@eecs.berkeley.edu"], "keywords": ["L4-norm Maximization", "Robust Dictionary Learning"], "TL;DR": "We compare the l4-norm based dictionary learning with PCA, ICA and show its stability as well as robustness.", "abstract": "Recently, the $\\ell^4$-norm maximization has been proposed to solve the sparse dictionary learning (SDL) problem. The simple MSP (matching, stretching, and projection) algorithm proposed by \\cite{zhai2019a} has proved surprisingly efficient and effective.  This paper aims to better understand this algorithm from its strong geometric and statistical connections with the classic PCA and ICA, as well as their associated fixed-point style algorithms. Such connections provide a unified way of viewing problems that pursue {\\em principal}, {\\em independent}, or {\\em sparse} components of high-dimensional data. Our studies reveal additional good properties of $\\ell^4$-maximization: not only is the MSP algorithm for sparse coding insensitive to small noise, but it is also robust to outliers and resilient to sparse corruptions. We provide statistical justification for such inherently nice properties. To corroborate the theoretical analysis, we also provide extensive and compelling experimental evidence with both synthetic data and real images.", "pdf": "/pdf/82f69ebb81f2a166efaed6c58908c26ecb0bde6b.pdf", "paperhash": "zhai|understanding_l4based_dictionary_learning_interpretation_stability_and_robustness", "code": "https://github.com/hermish/ZMZM-ICLR-2020", "_bibtex": "@inproceedings{\nZhai2020Understanding,\ntitle={Understanding l4-based Dictionary Learning: Interpretation, Stability, and Robustness},\nauthor={Yuexiang Zhai and Hermish Mehta and Zhengyuan Zhou and Yi Ma},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJeY-1BKDS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/5f236c0def40e09eb75fca84045199eb7f5ab47c.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SJeY-1BKDS", "replyto": "SJeY-1BKDS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1552/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1552/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575943114030, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1552/Reviewers"], "noninvitees": [], "tcdate": 1570237735718, "tmdate": 1575943114045, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1552/-/Official_Review"}}}, {"id": "B1x_np7Ctr", "original": null, "number": 1, "cdate": 1571859887716, "ddate": null, "tcdate": 1571859887716, "tmdate": 1574312935553, "tddate": null, "forum": "SJeY-1BKDS", "replyto": "SJeY-1BKDS", "invitation": "ICLR.cc/2020/Conference/Paper1552/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #1", "review": "This paper explores the recently proposed $\\ell^4$-norm maximization approach for solving the sparse dictionary learning (SDL) problem. Unlike other previously proposed methods that recover the dictionary one row/column at a time, for an orthonormal dictionary, the $\\ell^4$-norm maximization approach is known to recover the entire dictionary once for all. \n\nThis paper shows that $\\ell^4$-norm maximization has close connections with the PCA and ICA problem. Furthermore, focusing on the MSP algorithm for solving the  $\\ell^4$-norm maximization formulation, the paper highlights the connections of this fixed-point style algorithm with such algorithms for PCA and ICA. Subsequently, the paper studies the behavior of the MSP algorithm in the presence of noise, outliers, and sparse corruption. Unlike PCA, surprisingly, the MSP algorithm is shown to be robust to outliers and sparse corruption.\n\nOverall, the paper makes a nice effort towards better understanding the relatively new $\\ell^4$-norm maximization approach and its connection with other well-understood problems in the literature. Moreover, the paper takes the right step by studying the effect of non-ideal signal measurements on the underlying goal of dictionary learning. That said, the reviewer feels that, in the current form, the results in the paper are not novel enough to warrant an acceptance to ICLR. The connection of the $\\ell^4$-norm maximization formulation with ICA have been previously noted in other paper, so this would hardly qualify as a novel contribution. The analysis of the MSP algorithm in the presence of noise, outlier, and sparse corruption is not comprehensive enough. It would have been nice if the authors had provided a non-asymptotic analysis of the MSP algorithm in the presence of non-ideal measurements. Also, it is not clear how interesting the outlier formulation presented in the paper is. Shouldn't one consider outliers that go beyond the Gaussian distribution, ideally arbitrary outliers?", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper1552/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1552/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Understanding l4-based Dictionary Learning: Interpretation, Stability, and Robustness", "authors": ["Yuexiang Zhai", "Hermish Mehta", "Zhengyuan Zhou", "Yi Ma"], "authorids": ["ysz@berkeley.edu", "hermish@berkeley.edu", "zyzhou@stanford.edu", "yima@eecs.berkeley.edu"], "keywords": ["L4-norm Maximization", "Robust Dictionary Learning"], "TL;DR": "We compare the l4-norm based dictionary learning with PCA, ICA and show its stability as well as robustness.", "abstract": "Recently, the $\\ell^4$-norm maximization has been proposed to solve the sparse dictionary learning (SDL) problem. The simple MSP (matching, stretching, and projection) algorithm proposed by \\cite{zhai2019a} has proved surprisingly efficient and effective.  This paper aims to better understand this algorithm from its strong geometric and statistical connections with the classic PCA and ICA, as well as their associated fixed-point style algorithms. Such connections provide a unified way of viewing problems that pursue {\\em principal}, {\\em independent}, or {\\em sparse} components of high-dimensional data. Our studies reveal additional good properties of $\\ell^4$-maximization: not only is the MSP algorithm for sparse coding insensitive to small noise, but it is also robust to outliers and resilient to sparse corruptions. We provide statistical justification for such inherently nice properties. To corroborate the theoretical analysis, we also provide extensive and compelling experimental evidence with both synthetic data and real images.", "pdf": "/pdf/82f69ebb81f2a166efaed6c58908c26ecb0bde6b.pdf", "paperhash": "zhai|understanding_l4based_dictionary_learning_interpretation_stability_and_robustness", "code": "https://github.com/hermish/ZMZM-ICLR-2020", "_bibtex": "@inproceedings{\nZhai2020Understanding,\ntitle={Understanding l4-based Dictionary Learning: Interpretation, Stability, and Robustness},\nauthor={Yuexiang Zhai and Hermish Mehta and Zhengyuan Zhou and Yi Ma},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJeY-1BKDS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/5f236c0def40e09eb75fca84045199eb7f5ab47c.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SJeY-1BKDS", "replyto": "SJeY-1BKDS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1552/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1552/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575943114030, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1552/Reviewers"], "noninvitees": [], "tcdate": 1570237735718, "tmdate": 1575943114045, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1552/-/Official_Review"}}}, {"id": "r1lCqgAWjH", "original": null, "number": 2, "cdate": 1573146773633, "ddate": null, "tcdate": 1573146773633, "tmdate": 1573624113696, "tddate": null, "forum": "SJeY-1BKDS", "replyto": "B1x_np7Ctr", "invitation": "ICLR.cc/2020/Conference/Paper1552/-/Official_Comment", "content": {"title": "Thanks & Will extend non-asymptotic analysis", "comment": "Thanks for your detailed review and for your overall positive evaluation! In what follows, we provide more detailed responses to each of your comments. We have also performed more analyses per your subsequent suggestions. And we hope you find the updated draft adequately addresses your concerns.\n\nWe should have made our novelty more clear and hope to clarify the point here. We are aware that Zhai et al. [1] has already pointed out the connection between $\\ell^4$-maximization and ICA. However, this connection is merely at the formulation level and is *not* a novelty we claim for this paper. Instead, our work goes beyond and establishes the connections more at the algorithmic level and in particular, our work provides a unified understanding on how such efficient power-iteration like algorithms (FastICA and MSP) can be established under the unified framework of maximizing a convex function over a compact set. We consider this unification as valuable for the community, since the general result for power methods in maximizing convex function over a compact set only appears recently Journee et al. [2] and we believe this unified view will encourage further research in this direction.\n\nThanks for the suggestions on on which our paper can be improved with non-asymptotic concentration results. We did not include such results in the first submission due to limited space (as we prepared the paper for a 8-page version), but we already know there is no technical difficulty in reaching such results (which we have mentioned in footnote 9 of our paper.) As per your suggestion, we will provide non-asymptotic measure concentration results of the MSP algorithm in the updated version.\n     \nIn addition, we will also provide some extra clarifications in the regime of non-Gaussian outliers in the updated version, please stay tuned. \n\nThanks again for your thoughtful comments, we will try our best to clarify all your concerns about our paper in the coming version. \n\nReferences:\n[1] Zhai, Yuexiang, Zitong Yang, Zhenyu Liao, John Wright, and Yi Ma. \"Complete Dictionary Learning via $\\ell^ 4$-Norm Maximization over the Orthogonal Group.\" arXiv preprint arXiv:1906.02435, 2019\n[2] Journ\u00e9e, Michel, Yurii Nesterov, Peter Richt\u00e1rik, and Rodolphe Sepulchre. \"Generalized power method for sparse principal component analysis.\" Journal of Machine Learning Research 11, no. Feb (2010): 517 - 553."}, "signatures": ["ICLR.cc/2020/Conference/Paper1552/Authors"], "readers": ["ICLR.cc/2020/Conference/Paper1552/Authors", "ICLR.cc/2020/Conference/Paper1552/Reviewers", "ICLR.cc/2020/Conference/Paper1552/Area_Chairs", "everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1552/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Understanding l4-based Dictionary Learning: Interpretation, Stability, and Robustness", "authors": ["Yuexiang Zhai", "Hermish Mehta", "Zhengyuan Zhou", "Yi Ma"], "authorids": ["ysz@berkeley.edu", "hermish@berkeley.edu", "zyzhou@stanford.edu", "yima@eecs.berkeley.edu"], "keywords": ["L4-norm Maximization", "Robust Dictionary Learning"], "TL;DR": "We compare the l4-norm based dictionary learning with PCA, ICA and show its stability as well as robustness.", "abstract": "Recently, the $\\ell^4$-norm maximization has been proposed to solve the sparse dictionary learning (SDL) problem. The simple MSP (matching, stretching, and projection) algorithm proposed by \\cite{zhai2019a} has proved surprisingly efficient and effective.  This paper aims to better understand this algorithm from its strong geometric and statistical connections with the classic PCA and ICA, as well as their associated fixed-point style algorithms. Such connections provide a unified way of viewing problems that pursue {\\em principal}, {\\em independent}, or {\\em sparse} components of high-dimensional data. Our studies reveal additional good properties of $\\ell^4$-maximization: not only is the MSP algorithm for sparse coding insensitive to small noise, but it is also robust to outliers and resilient to sparse corruptions. We provide statistical justification for such inherently nice properties. To corroborate the theoretical analysis, we also provide extensive and compelling experimental evidence with both synthetic data and real images.", "pdf": "/pdf/82f69ebb81f2a166efaed6c58908c26ecb0bde6b.pdf", "paperhash": "zhai|understanding_l4based_dictionary_learning_interpretation_stability_and_robustness", "code": "https://github.com/hermish/ZMZM-ICLR-2020", "_bibtex": "@inproceedings{\nZhai2020Understanding,\ntitle={Understanding l4-based Dictionary Learning: Interpretation, Stability, and Robustness},\nauthor={Yuexiang Zhai and Hermish Mehta and Zhengyuan Zhou and Yi Ma},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJeY-1BKDS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/5f236c0def40e09eb75fca84045199eb7f5ab47c.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJeY-1BKDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1552/Authors", "ICLR.cc/2020/Conference/Paper1552/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1552/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1552/Reviewers", "ICLR.cc/2020/Conference/Paper1552/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1552/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1552/Authors|ICLR.cc/2020/Conference/Paper1552/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504154331, "tmdate": 1576860559009, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1552/Authors", "ICLR.cc/2020/Conference/Paper1552/Reviewers", "ICLR.cc/2020/Conference/Paper1552/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1552/-/Official_Comment"}}}, {"id": "H1e1hkCWoH", "original": null, "number": 1, "cdate": 1573146535474, "ddate": null, "tcdate": 1573146535474, "tmdate": 1573624104404, "tddate": null, "forum": "SJeY-1BKDS", "replyto": "H1gO_IImcB", "invitation": "ICLR.cc/2020/Conference/Paper1552/-/Official_Comment", "content": {"title": "Thanks & Will update draft accordingly", "comment": "Thank you for your detailed reading and for your positive opinion! Below, we provide a point-to-point response and hope to address your concerns.\n\nYes, we could have made the connection point more clear. Certainly, as you pointed out, there are a lot of algorithms which follow a projected/proximal gradient descent schemes. However, there is something interesting and nontrivial here: among all projected/proximal gradient descent methods, MSP algorithms lie in the optimization regime that allows the step-size to be infinite (i.e. MSP acts like a power iteration method) and hence resulting in more efficient algorithms than the traditional gradient descent type methods. Moreover, we make the comparison between MSP, Power-iteration, and FastICA (as stated in Table 1 of the paper) to illustrate the intuition behind the efficiency of the MSP algorithm. \n\nWe highly appreciate the ``ICA-basis-versus-dictionary-learning\" comment and it is also very surprising for us to see how $\\ell^4$-norm maximization can be derived and justified from different generative models -- ICA and Dictionary Learning. Qualitatively, we think such similarity comes from non-Gaussian property of the sparsity assumption -- as maximizing kurtosis promotes non-Gaussianity of the data which coincides with the sparse ground truth of Dictionary Learning problem. We will include a discussion on our intuition on why this occurs, because we certainly agree with you this is an intriguing phenomenon. Characterizing the exact conditions under which they coincide may be beyond the scope of the current paper and will leave the more quantitative analysis for future work.\n\nThank you for pointing this out, a very good point that we need to demonstrate. In the updated draft, we will provide more comparisons between the $\\ell^4$ formulation and the previous $\\ell^1$ based methods in terms of robustness.\n \n It is known that $\\ell^1$ minimization itself is not robust to noise or outliers (hence many works in the literature on Lasso for noise measurements and error correction for $\\ell^1$ minimization, see paper Wright et al. [1] and references therein.) In addition, learning the dictionary column by column is less robust than learning the entire dictionary holistically, as the error may propagate while the latter can leverage global information to denoise much more effectively. \n\nRegarding the run-time concerns, this is another good point that we did not address in the initial submission and thank you for the suggestion. We will also include run-time comparisons in the updated draft.\n\nThank you again for your review! Per your comments, we will update our paper according to your advice, please stay tuned.\n\nWe hope we have addressed all your concerns.\n\nReferences:\n[1] Wright, John, and Yi Ma. \"Dense Error Correction Via $\\ell^ 1$-Minimization.\" IEEE Transactions on Information Theory 56.7 (2010): 3540-3560."}, "signatures": ["ICLR.cc/2020/Conference/Paper1552/Authors"], "readers": ["ICLR.cc/2020/Conference/Paper1552/Authors", "ICLR.cc/2020/Conference/Paper1552/Reviewers", "ICLR.cc/2020/Conference/Paper1552/Area_Chairs", "everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1552/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Understanding l4-based Dictionary Learning: Interpretation, Stability, and Robustness", "authors": ["Yuexiang Zhai", "Hermish Mehta", "Zhengyuan Zhou", "Yi Ma"], "authorids": ["ysz@berkeley.edu", "hermish@berkeley.edu", "zyzhou@stanford.edu", "yima@eecs.berkeley.edu"], "keywords": ["L4-norm Maximization", "Robust Dictionary Learning"], "TL;DR": "We compare the l4-norm based dictionary learning with PCA, ICA and show its stability as well as robustness.", "abstract": "Recently, the $\\ell^4$-norm maximization has been proposed to solve the sparse dictionary learning (SDL) problem. The simple MSP (matching, stretching, and projection) algorithm proposed by \\cite{zhai2019a} has proved surprisingly efficient and effective.  This paper aims to better understand this algorithm from its strong geometric and statistical connections with the classic PCA and ICA, as well as their associated fixed-point style algorithms. Such connections provide a unified way of viewing problems that pursue {\\em principal}, {\\em independent}, or {\\em sparse} components of high-dimensional data. Our studies reveal additional good properties of $\\ell^4$-maximization: not only is the MSP algorithm for sparse coding insensitive to small noise, but it is also robust to outliers and resilient to sparse corruptions. We provide statistical justification for such inherently nice properties. To corroborate the theoretical analysis, we also provide extensive and compelling experimental evidence with both synthetic data and real images.", "pdf": "/pdf/82f69ebb81f2a166efaed6c58908c26ecb0bde6b.pdf", "paperhash": "zhai|understanding_l4based_dictionary_learning_interpretation_stability_and_robustness", "code": "https://github.com/hermish/ZMZM-ICLR-2020", "_bibtex": "@inproceedings{\nZhai2020Understanding,\ntitle={Understanding l4-based Dictionary Learning: Interpretation, Stability, and Robustness},\nauthor={Yuexiang Zhai and Hermish Mehta and Zhengyuan Zhou and Yi Ma},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJeY-1BKDS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/5f236c0def40e09eb75fca84045199eb7f5ab47c.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJeY-1BKDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1552/Authors", "ICLR.cc/2020/Conference/Paper1552/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1552/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1552/Reviewers", "ICLR.cc/2020/Conference/Paper1552/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1552/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1552/Authors|ICLR.cc/2020/Conference/Paper1552/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504154331, "tmdate": 1576860559009, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1552/Authors", "ICLR.cc/2020/Conference/Paper1552/Reviewers", "ICLR.cc/2020/Conference/Paper1552/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1552/-/Official_Comment"}}}], "count": 6}