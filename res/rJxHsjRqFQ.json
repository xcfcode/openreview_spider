{"notes": [{"id": "rJxHsjRqFQ", "original": "r1e-inoqF7", "number": 620, "cdate": 1538087837075, "ddate": null, "tcdate": 1538087837075, "tmdate": 1556977696457, "tddate": null, "forum": "rJxHsjRqFQ", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Hyperbolic Attention Networks", "abstract": "Recent approaches have successfully demonstrated the benefits of learning the parameters of shallow networks in hyperbolic space. We extend this line of work by imposing hyperbolic geometry on the embeddings used to compute the ubiquitous attention mechanisms for different neural networks architectures. By only changing the geometry of embedding of object representations, we can use the embedding space more efficiently without increasing the number of parameters of the model. Mainly as the number of objects grows exponentially for any semantic distance from the query, hyperbolic geometry  --as opposed to Euclidean geometry-- can encode those objects without having any interference. Our method shows improvements in generalization on neural machine translation on WMT'14 (English to German), learning on graphs (both on synthetic and real-world graph tasks) and visual question answering (CLEVR) tasks while keeping the neural representations compact.", "keywords": ["Hyperbolic Geometry", "Attention Methods", "Reasoning on Graphs", "Relation Learning", "Scale Free Graphs", "Transformers", "Power Law"], "authorids": ["ca9lar@gmail.com", "mdenil@google.com", "mateuszm@google.com", "alirazavi@google.com", "razp@google.com", "kmh@google.com", "vbapst@google.com", "drapso@google.com", "adamsantoro@google.com", "nandodefreitas@google.com"], "authors": ["Caglar Gulcehre", "Misha Denil", "Mateusz Malinowski", "Ali Razavi", "Razvan Pascanu", "Karl Moritz Hermann", "Peter Battaglia", "Victor Bapst", "David Raposo", "Adam Santoro", "Nando de Freitas"], "TL;DR": "We propose to incorporate inductive biases and operations coming from hyperbolic geometry to improve the attention mechanism of the neural networks.", "pdf": "/pdf/a363c6a7c4aaaadaaa0840b157dc28a84a295c34.pdf", "paperhash": "gulcehre|hyperbolic_attention_networks", "_bibtex": "@inproceedings{\ngulcehre2018hyperbolic,\ntitle={Hyperbolic Attention Networks},\nauthor={Caglar Gulcehre and Misha Denil and Mateusz Malinowski and Ali Razavi and Razvan Pascanu and Karl Moritz Hermann and Peter Battaglia and Victor Bapst and David Raposo and Adam Santoro and Nando de Freitas},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJxHsjRqFQ},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 11, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "B1extQeflE", "original": null, "number": 1, "cdate": 1544844151688, "ddate": null, "tcdate": 1544844151688, "tmdate": 1545354473220, "tddate": null, "forum": "rJxHsjRqFQ", "replyto": "rJxHsjRqFQ", "invitation": "ICLR.cc/2019/Conference/-/Paper620/Meta_Review", "content": {"metareview": "Reviewers all agree that this is a strong submission.\nI also believe it is interesting that only by changing the geometry of embeddings, they can use the space more efficiently without increasing the number of parameters.", "confidence": "5: The area chair is absolutely certain", "recommendation": "Accept (Poster)", "title": "Strong and interesting paper"}, "signatures": ["ICLR.cc/2019/Conference/Paper620/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper620/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hyperbolic Attention Networks", "abstract": "Recent approaches have successfully demonstrated the benefits of learning the parameters of shallow networks in hyperbolic space. We extend this line of work by imposing hyperbolic geometry on the embeddings used to compute the ubiquitous attention mechanisms for different neural networks architectures. By only changing the geometry of embedding of object representations, we can use the embedding space more efficiently without increasing the number of parameters of the model. Mainly as the number of objects grows exponentially for any semantic distance from the query, hyperbolic geometry  --as opposed to Euclidean geometry-- can encode those objects without having any interference. Our method shows improvements in generalization on neural machine translation on WMT'14 (English to German), learning on graphs (both on synthetic and real-world graph tasks) and visual question answering (CLEVR) tasks while keeping the neural representations compact.", "keywords": ["Hyperbolic Geometry", "Attention Methods", "Reasoning on Graphs", "Relation Learning", "Scale Free Graphs", "Transformers", "Power Law"], "authorids": ["ca9lar@gmail.com", "mdenil@google.com", "mateuszm@google.com", "alirazavi@google.com", "razp@google.com", "kmh@google.com", "vbapst@google.com", "drapso@google.com", "adamsantoro@google.com", "nandodefreitas@google.com"], "authors": ["Caglar Gulcehre", "Misha Denil", "Mateusz Malinowski", "Ali Razavi", "Razvan Pascanu", "Karl Moritz Hermann", "Peter Battaglia", "Victor Bapst", "David Raposo", "Adam Santoro", "Nando de Freitas"], "TL;DR": "We propose to incorporate inductive biases and operations coming from hyperbolic geometry to improve the attention mechanism of the neural networks.", "pdf": "/pdf/a363c6a7c4aaaadaaa0840b157dc28a84a295c34.pdf", "paperhash": "gulcehre|hyperbolic_attention_networks", "_bibtex": "@inproceedings{\ngulcehre2018hyperbolic,\ntitle={Hyperbolic Attention Networks},\nauthor={Caglar Gulcehre and Misha Denil and Mateusz Malinowski and Ali Razavi and Razvan Pascanu and Karl Moritz Hermann and Peter Battaglia and Victor Bapst and David Raposo and Adam Santoro and Nando de Freitas},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJxHsjRqFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper620/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353150445, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJxHsjRqFQ", "replyto": "rJxHsjRqFQ", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper620/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper620/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper620/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353150445}}}, {"id": "H1lSV55B0m", "original": null, "number": 6, "cdate": 1542986285250, "ddate": null, "tcdate": 1542986285250, "tmdate": 1542986285250, "tddate": null, "forum": "rJxHsjRqFQ", "replyto": "ryl7AYUH3Q", "invitation": "ICLR.cc/2019/Conference/-/Paper620/Official_Comment", "content": {"title": "On the Experiments with the Synthetic Graph Datasets", "comment": "Thank you for your remarkable feedback and comments about our paper.\n\n> Question: In Figure 3 (Center), the number of nodes 1000 and 1200 are pretty close. How about the results on 500 nodes and 2000 nodes? It seems the accuracy difference increases as the number of nodes increases. Is this true? \n\nThe difference in performance between the hyperbolic and Euclidean models is negligible if the graph is small enough (e.g. 200 nodes) and we would expect this trend to continue for graphs of size 2000 or larger, with the gap between hyperbolic and Euclidean models growing as the size of the graph increases. This is mainly because learning on larger graphs would require more capacity. We will add the experiments on graphs with 2000 nodes or larger to the camera-ready version of the paper."}, "signatures": ["ICLR.cc/2019/Conference/Paper620/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper620/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper620/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hyperbolic Attention Networks", "abstract": "Recent approaches have successfully demonstrated the benefits of learning the parameters of shallow networks in hyperbolic space. We extend this line of work by imposing hyperbolic geometry on the embeddings used to compute the ubiquitous attention mechanisms for different neural networks architectures. By only changing the geometry of embedding of object representations, we can use the embedding space more efficiently without increasing the number of parameters of the model. Mainly as the number of objects grows exponentially for any semantic distance from the query, hyperbolic geometry  --as opposed to Euclidean geometry-- can encode those objects without having any interference. Our method shows improvements in generalization on neural machine translation on WMT'14 (English to German), learning on graphs (both on synthetic and real-world graph tasks) and visual question answering (CLEVR) tasks while keeping the neural representations compact.", "keywords": ["Hyperbolic Geometry", "Attention Methods", "Reasoning on Graphs", "Relation Learning", "Scale Free Graphs", "Transformers", "Power Law"], "authorids": ["ca9lar@gmail.com", "mdenil@google.com", "mateuszm@google.com", "alirazavi@google.com", "razp@google.com", "kmh@google.com", "vbapst@google.com", "drapso@google.com", "adamsantoro@google.com", "nandodefreitas@google.com"], "authors": ["Caglar Gulcehre", "Misha Denil", "Mateusz Malinowski", "Ali Razavi", "Razvan Pascanu", "Karl Moritz Hermann", "Peter Battaglia", "Victor Bapst", "David Raposo", "Adam Santoro", "Nando de Freitas"], "TL;DR": "We propose to incorporate inductive biases and operations coming from hyperbolic geometry to improve the attention mechanism of the neural networks.", "pdf": "/pdf/a363c6a7c4aaaadaaa0840b157dc28a84a295c34.pdf", "paperhash": "gulcehre|hyperbolic_attention_networks", "_bibtex": "@inproceedings{\ngulcehre2018hyperbolic,\ntitle={Hyperbolic Attention Networks},\nauthor={Caglar Gulcehre and Misha Denil and Mateusz Malinowski and Ali Razavi and Razvan Pascanu and Karl Moritz Hermann and Peter Battaglia and Victor Bapst and David Raposo and Adam Santoro and Nando de Freitas},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJxHsjRqFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper620/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621624067, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJxHsjRqFQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper620/Authors", "ICLR.cc/2019/Conference/Paper620/Reviewers", "ICLR.cc/2019/Conference/Paper620/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper620/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper620/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper620/Authors|ICLR.cc/2019/Conference/Paper620/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper620/Reviewers", "ICLR.cc/2019/Conference/Paper620/Authors", "ICLR.cc/2019/Conference/Paper620/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621624067}}}, {"id": "HyeEJ59H0Q", "original": null, "number": 5, "cdate": 1542986203779, "ddate": null, "tcdate": 1542986203779, "tmdate": 1542986203779, "tddate": null, "forum": "rJxHsjRqFQ", "replyto": "Hylg-SL927", "invitation": "ICLR.cc/2019/Conference/-/Paper620/Official_Comment", "content": {"title": "On Experiments with other VQA Models and Datasets", "comment": "\nWe really appreciate your feedback and comments about our paper.\n\n> Baselines: The authors main contribution is the matching and aggregation operator. It always feels like the multi-modal community is divided between VQA and CLEVR datasets, but there should be a lot in common between them. Specifically, what is called here the matching operator, had several variants in VQA, such as Multimodal Compact Bilinear Pooling by Fukui et al., or Multi-modal Factorized Bilinear Pooling by You et al. etc. I think the paper would benefit from adding other variants of matching functions. \nDatasets: I think the approach might work as well in VQA dataset, which I find more interesting than clever because of the real-world nature of it. You can plug it into methods like MFB, or as pairwise potentials in Structured Attentions by Zhu et al, or High-Order attention by Schwartz et al.\n\nIn our work, we show the results of our hyperbolic module on a wide variety of different problems -- NMT, CLEVR, and graph problems. We used CLEVR because it has many relational questions that we hypothesize may benefit from representing in hyperbolic space. In contrast, the real-world VQA consists of somewhat shorter, and non-relational questions. Moreover, most of the challenges in the VQA dataset are about better visual representation that, in our work, we would like to abstract that away. Additionally, existing highly-performing architectures on CLEVR such as RN, also build upon the relational inductive biases, and hence there is a direct link between our module and these works. Nonetheless, we agree with the reviewers that the highly-performing VQA architectures may also benefit from our module, which we leave as possible future work."}, "signatures": ["ICLR.cc/2019/Conference/Paper620/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper620/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper620/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hyperbolic Attention Networks", "abstract": "Recent approaches have successfully demonstrated the benefits of learning the parameters of shallow networks in hyperbolic space. We extend this line of work by imposing hyperbolic geometry on the embeddings used to compute the ubiquitous attention mechanisms for different neural networks architectures. By only changing the geometry of embedding of object representations, we can use the embedding space more efficiently without increasing the number of parameters of the model. Mainly as the number of objects grows exponentially for any semantic distance from the query, hyperbolic geometry  --as opposed to Euclidean geometry-- can encode those objects without having any interference. Our method shows improvements in generalization on neural machine translation on WMT'14 (English to German), learning on graphs (both on synthetic and real-world graph tasks) and visual question answering (CLEVR) tasks while keeping the neural representations compact.", "keywords": ["Hyperbolic Geometry", "Attention Methods", "Reasoning on Graphs", "Relation Learning", "Scale Free Graphs", "Transformers", "Power Law"], "authorids": ["ca9lar@gmail.com", "mdenil@google.com", "mateuszm@google.com", "alirazavi@google.com", "razp@google.com", "kmh@google.com", "vbapst@google.com", "drapso@google.com", "adamsantoro@google.com", "nandodefreitas@google.com"], "authors": ["Caglar Gulcehre", "Misha Denil", "Mateusz Malinowski", "Ali Razavi", "Razvan Pascanu", "Karl Moritz Hermann", "Peter Battaglia", "Victor Bapst", "David Raposo", "Adam Santoro", "Nando de Freitas"], "TL;DR": "We propose to incorporate inductive biases and operations coming from hyperbolic geometry to improve the attention mechanism of the neural networks.", "pdf": "/pdf/a363c6a7c4aaaadaaa0840b157dc28a84a295c34.pdf", "paperhash": "gulcehre|hyperbolic_attention_networks", "_bibtex": "@inproceedings{\ngulcehre2018hyperbolic,\ntitle={Hyperbolic Attention Networks},\nauthor={Caglar Gulcehre and Misha Denil and Mateusz Malinowski and Ali Razavi and Razvan Pascanu and Karl Moritz Hermann and Peter Battaglia and Victor Bapst and David Raposo and Adam Santoro and Nando de Freitas},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJxHsjRqFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper620/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621624067, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJxHsjRqFQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper620/Authors", "ICLR.cc/2019/Conference/Paper620/Reviewers", "ICLR.cc/2019/Conference/Paper620/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper620/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper620/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper620/Authors|ICLR.cc/2019/Conference/Paper620/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper620/Reviewers", "ICLR.cc/2019/Conference/Paper620/Authors", "ICLR.cc/2019/Conference/Paper620/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621624067}}}, {"id": "S1xA9YqSAQ", "original": null, "number": 4, "cdate": 1542986134244, "ddate": null, "tcdate": 1542986134244, "tmdate": 1542986134244, "tddate": null, "forum": "rJxHsjRqFQ", "replyto": "H1g2vUL9nQ", "invitation": "ICLR.cc/2019/Conference/-/Paper620/Official_Comment", "content": {"title": "A Response to Improves small models but not large ones, reasonable but not very strong experimental comparisons", "comment": "\nThank you for your constructive feedback and comments about our paper.\n\n> The novelty of this paper is replacing the Euclidean metric with another existing metric, which has already been used in previous ML models. So the contribution is limited.\n\nWe present a method to ensure that the activations of a neural network can be interpreted as points in hyperbolic space, which is not guaranteed apriori. We show that by use of hyperbolic geometry to compute the attention, it is possible to reason over relations and graphs more efficiently and  with more compact representations. \n\nWe are among the first to adopt the inductive biases from hyperbolic geometry to improve the attention mechanisms of neural networks, and we have done so in a general and modular way that can be used directly in any existing attention based architecture.\n\n> As explicitly claimed in the paper and also reflected by the experimental results (e.g., Transformer-Big in Table 2). The hyperbolic metric only brings noticeable improvement to small neural nets with limited compacity on relatively small datasets. When applied it to most SOTA models (which are usually large/deep/wide neural networks) on larger datasets, it loses the advantage. This fact might seriously limit the application of the proposed technique.\n\nA comparison between network compression techniques and hyperbolic attention is an exciting direction for future work.  Especially since in principle they are orthogonal approaches, and perhaps combining them could be more effective than either in isolation.\n\n> Although hyperbolic metric reflects the power-law distribution, which is a very natural assumption verified on many kinds of real data (social networks and physical statistics), I am not fully convinced that it still holds on an embedding space produced by a neural net (since attention are usually applied to the outputs of a neural net). \n\nIt is important to note that the use of hyperbolic geometry is not an assumption about how the activations behave, it is a structure that is imposed on the activations as a modelling choice.   We use hyperbolic geometry to provide power law structure as an inductive bias to the model, not as an assumption about how the activations would behave in the absence of this imposed structure. The improvements we have observed on the graph tasks also indicate this performance gain.\n\n> In the experiments, does the model with the proposed metric cost similar training/inference time comparing to the baselines? What is the trade-off between improvements and extra time costs?\n\n Indeed the costs are similar. All the operations that we use are simple element-wise scalar operations. These operations have a negligible contribution to the total computational cost.\n\n> I notice that the results of the proposed attention in Table 2 are ~0.5% higher than the results from the earlier arXiv version of this paper. What is the reason for the improvements? Did you increase the training steps?\n\nWe observed that mentioned improvements mainly after training our model longer. In the latest version we trained our models for as many steps as the original Transformer [1].  We made this change after speaking with the authors of \u201cAttention is All You Need\u201d paper, who suggested this as a way to improve the performance of the model. This accounts for the increased performance compared to the Arxiv version of the paper.\n\n[1] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). Attention is all you need. In Advances in Neural Information Processing Systems(pp. 5998-6008).\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper620/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper620/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper620/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hyperbolic Attention Networks", "abstract": "Recent approaches have successfully demonstrated the benefits of learning the parameters of shallow networks in hyperbolic space. We extend this line of work by imposing hyperbolic geometry on the embeddings used to compute the ubiquitous attention mechanisms for different neural networks architectures. By only changing the geometry of embedding of object representations, we can use the embedding space more efficiently without increasing the number of parameters of the model. Mainly as the number of objects grows exponentially for any semantic distance from the query, hyperbolic geometry  --as opposed to Euclidean geometry-- can encode those objects without having any interference. Our method shows improvements in generalization on neural machine translation on WMT'14 (English to German), learning on graphs (both on synthetic and real-world graph tasks) and visual question answering (CLEVR) tasks while keeping the neural representations compact.", "keywords": ["Hyperbolic Geometry", "Attention Methods", "Reasoning on Graphs", "Relation Learning", "Scale Free Graphs", "Transformers", "Power Law"], "authorids": ["ca9lar@gmail.com", "mdenil@google.com", "mateuszm@google.com", "alirazavi@google.com", "razp@google.com", "kmh@google.com", "vbapst@google.com", "drapso@google.com", "adamsantoro@google.com", "nandodefreitas@google.com"], "authors": ["Caglar Gulcehre", "Misha Denil", "Mateusz Malinowski", "Ali Razavi", "Razvan Pascanu", "Karl Moritz Hermann", "Peter Battaglia", "Victor Bapst", "David Raposo", "Adam Santoro", "Nando de Freitas"], "TL;DR": "We propose to incorporate inductive biases and operations coming from hyperbolic geometry to improve the attention mechanism of the neural networks.", "pdf": "/pdf/a363c6a7c4aaaadaaa0840b157dc28a84a295c34.pdf", "paperhash": "gulcehre|hyperbolic_attention_networks", "_bibtex": "@inproceedings{\ngulcehre2018hyperbolic,\ntitle={Hyperbolic Attention Networks},\nauthor={Caglar Gulcehre and Misha Denil and Mateusz Malinowski and Ali Razavi and Razvan Pascanu and Karl Moritz Hermann and Peter Battaglia and Victor Bapst and David Raposo and Adam Santoro and Nando de Freitas},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJxHsjRqFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper620/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621624067, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJxHsjRqFQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper620/Authors", "ICLR.cc/2019/Conference/Paper620/Reviewers", "ICLR.cc/2019/Conference/Paper620/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper620/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper620/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper620/Authors|ICLR.cc/2019/Conference/Paper620/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper620/Reviewers", "ICLR.cc/2019/Conference/Paper620/Authors", "ICLR.cc/2019/Conference/Paper620/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621624067}}}, {"id": "rJe2zKqBA7", "original": null, "number": 3, "cdate": 1542986004396, "ddate": null, "tcdate": 1542986004396, "tmdate": 1542986004396, "tddate": null, "forum": "rJxHsjRqFQ", "replyto": "B1eVP5WS6m", "invitation": "ICLR.cc/2019/Conference/-/Paper620/Official_Comment", "content": {"title": "Relevant Work - We will cite this paper", "comment": "> This is quite an interesting work, utilizing hyperbolic geometry for more efficient representation. It reminds me of a previous work \"Lie Access Neural Turing Machines\" that proposed to use general manifolds as the \"index space\" of memory items, which are attended to like in standard attention. Could you comment on the relation of your paper to that work?\n\nThanks for pointing out this paper, we were not aware of it. \n\nIt indeed seems to be related to our method but with important distinctions. Yang et al introduces a memory access mechanism for NTM model where key and values of the memory are parametrized on a differentiable Lie-group manifold. To compute the matching function Yang et al also uses the distances between the key and value. Both Lie Access NTMs (LA-NTM) and our paper are closely related to each other in the sense of introducing attention-mechanisms by using certain geometric tools. However, the goals and the motivation of each work are quite different. LA-NTM represents uses the manifold of Lie-group actions to be able to learn better access mechanisms for the memory. Our paper focuses on improving the learning of relations in the data by combining attention mechanism with the hyperbolic inductive biases. It would be interesting to combine both approaches.\n\n We will cite this work and relate it to our method in our revised manuscript. \n"}, "signatures": ["ICLR.cc/2019/Conference/Paper620/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper620/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper620/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hyperbolic Attention Networks", "abstract": "Recent approaches have successfully demonstrated the benefits of learning the parameters of shallow networks in hyperbolic space. We extend this line of work by imposing hyperbolic geometry on the embeddings used to compute the ubiquitous attention mechanisms for different neural networks architectures. By only changing the geometry of embedding of object representations, we can use the embedding space more efficiently without increasing the number of parameters of the model. Mainly as the number of objects grows exponentially for any semantic distance from the query, hyperbolic geometry  --as opposed to Euclidean geometry-- can encode those objects without having any interference. Our method shows improvements in generalization on neural machine translation on WMT'14 (English to German), learning on graphs (both on synthetic and real-world graph tasks) and visual question answering (CLEVR) tasks while keeping the neural representations compact.", "keywords": ["Hyperbolic Geometry", "Attention Methods", "Reasoning on Graphs", "Relation Learning", "Scale Free Graphs", "Transformers", "Power Law"], "authorids": ["ca9lar@gmail.com", "mdenil@google.com", "mateuszm@google.com", "alirazavi@google.com", "razp@google.com", "kmh@google.com", "vbapst@google.com", "drapso@google.com", "adamsantoro@google.com", "nandodefreitas@google.com"], "authors": ["Caglar Gulcehre", "Misha Denil", "Mateusz Malinowski", "Ali Razavi", "Razvan Pascanu", "Karl Moritz Hermann", "Peter Battaglia", "Victor Bapst", "David Raposo", "Adam Santoro", "Nando de Freitas"], "TL;DR": "We propose to incorporate inductive biases and operations coming from hyperbolic geometry to improve the attention mechanism of the neural networks.", "pdf": "/pdf/a363c6a7c4aaaadaaa0840b157dc28a84a295c34.pdf", "paperhash": "gulcehre|hyperbolic_attention_networks", "_bibtex": "@inproceedings{\ngulcehre2018hyperbolic,\ntitle={Hyperbolic Attention Networks},\nauthor={Caglar Gulcehre and Misha Denil and Mateusz Malinowski and Ali Razavi and Razvan Pascanu and Karl Moritz Hermann and Peter Battaglia and Victor Bapst and David Raposo and Adam Santoro and Nando de Freitas},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJxHsjRqFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper620/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621624067, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJxHsjRqFQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper620/Authors", "ICLR.cc/2019/Conference/Paper620/Reviewers", "ICLR.cc/2019/Conference/Paper620/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper620/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper620/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper620/Authors|ICLR.cc/2019/Conference/Paper620/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper620/Reviewers", "ICLR.cc/2019/Conference/Paper620/Authors", "ICLR.cc/2019/Conference/Paper620/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621624067}}}, {"id": "B1eVP5WS6m", "original": null, "number": 2, "cdate": 1541900892097, "ddate": null, "tcdate": 1541900892097, "tmdate": 1541900892097, "tddate": null, "forum": "rJxHsjRqFQ", "replyto": "rJxHsjRqFQ", "invitation": "ICLR.cc/2019/Conference/-/Paper620/Public_Comment", "content": {"comment": "Dear authors,\n\nThis is quite an interesting work, utilizing hyperbolic geometry for more efficient representation. It reminds me of a previous work \"Lie Access Neural Turing Machines\" that proposed to use general manifolds as the \"index space\" of memory items, which are attended to like in standard attention. Could you comment on the relation of your paper to that work?\n\nG. Yang and A. Rush. Lie-Access Neural Turing Machines. https://openreview.net/forum?id=Byiy-Pqlx&noteId=Byiy-Pqlx", "title": "Reminiscent of \"Lie Access Neural Turing Machines\""}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hyperbolic Attention Networks", "abstract": "Recent approaches have successfully demonstrated the benefits of learning the parameters of shallow networks in hyperbolic space. We extend this line of work by imposing hyperbolic geometry on the embeddings used to compute the ubiquitous attention mechanisms for different neural networks architectures. By only changing the geometry of embedding of object representations, we can use the embedding space more efficiently without increasing the number of parameters of the model. Mainly as the number of objects grows exponentially for any semantic distance from the query, hyperbolic geometry  --as opposed to Euclidean geometry-- can encode those objects without having any interference. Our method shows improvements in generalization on neural machine translation on WMT'14 (English to German), learning on graphs (both on synthetic and real-world graph tasks) and visual question answering (CLEVR) tasks while keeping the neural representations compact.", "keywords": ["Hyperbolic Geometry", "Attention Methods", "Reasoning on Graphs", "Relation Learning", "Scale Free Graphs", "Transformers", "Power Law"], "authorids": ["ca9lar@gmail.com", "mdenil@google.com", "mateuszm@google.com", "alirazavi@google.com", "razp@google.com", "kmh@google.com", "vbapst@google.com", "drapso@google.com", "adamsantoro@google.com", "nandodefreitas@google.com"], "authors": ["Caglar Gulcehre", "Misha Denil", "Mateusz Malinowski", "Ali Razavi", "Razvan Pascanu", "Karl Moritz Hermann", "Peter Battaglia", "Victor Bapst", "David Raposo", "Adam Santoro", "Nando de Freitas"], "TL;DR": "We propose to incorporate inductive biases and operations coming from hyperbolic geometry to improve the attention mechanism of the neural networks.", "pdf": "/pdf/a363c6a7c4aaaadaaa0840b157dc28a84a295c34.pdf", "paperhash": "gulcehre|hyperbolic_attention_networks", "_bibtex": "@inproceedings{\ngulcehre2018hyperbolic,\ntitle={Hyperbolic Attention Networks},\nauthor={Caglar Gulcehre and Misha Denil and Mateusz Malinowski and Ali Razavi and Razvan Pascanu and Karl Moritz Hermann and Peter Battaglia and Victor Bapst and David Raposo and Adam Santoro and Nando de Freitas},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJxHsjRqFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper620/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311792716, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "rJxHsjRqFQ", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper620/Authors", "ICLR.cc/2019/Conference/Paper620/Reviewers", "ICLR.cc/2019/Conference/Paper620/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper620/Authors", "ICLR.cc/2019/Conference/Paper620/Reviewers", "ICLR.cc/2019/Conference/Paper620/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311792716}}}, {"id": "H1g2vUL9nQ", "original": null, "number": 3, "cdate": 1541199459542, "ddate": null, "tcdate": 1541199459542, "tmdate": 1541533837492, "tddate": null, "forum": "rJxHsjRqFQ", "replyto": "rJxHsjRqFQ", "invitation": "ICLR.cc/2019/Conference/-/Paper620/Official_Review", "content": {"title": "Applying a new metric to attention mechanism, improves small models but not large ones, reasonable but not very strong experimental comparisons.", "review": "This paper replaces the dot-product similarity used in attention mechanisms with the negative hyperbolic distance, and applies this new attention to the existing Transformer model, graph attention networks (GAT), and Relation Networks (RN). Accordingly, they use Einstein midpoint to compute the aggregation weights of attention in hyperbolic space. The idea of using hyperbolic rather than Euclidean space is based on the assumption that the input embeddings (neural net activations) are on the hyperbolic manifold, which follows power-law distribution and can be seem as a smooth description of tree-like hierarchy of data points. This assumption might hold for small neural networks with relatively low dimensional output. One main reason why this paper adopts the hyperbolic space is that the volume of hyperbolic space grows exponentially with the increase of radius while that of Euclidean space grows only polynomially. Using hyperbolic distance can increase the capacity of networks and handle the complexity of data. Experiments on Transformer and relation network show that Transformer, GAT and RN with the new attention metric produce better performance than Euclidean distance.\n\nPros:\n\n1. Comparing to the existing methods using representations for shallow models in hyperbolic geometry, this paper extends the idea to deep neural networks. \n2. The proposed attention mechanism can be easily applied to many of existing networks to enhance their capacity.\n3. The experiments show several interesting results: 1) hyperbolic recursive transformer (RT) is consistently superior to Euclidean RT across the tasks in this paper; 2) hyperbolic space substantially benefits the low-capacity networks (i.e., low-dimensionality hidden state); 3) Einstein midpoint is better than Euclidean aggregation in hyperbolic space; 4) using sigmoid rather than softmax to compute attention weight may achieve better effectiveness on some tasks for the reason that the attention weights over different entities ay do not compete with each other.\n\n\nCons:\n\n1. The novelty of this paper is replacing the Euclidean metric with another existing metric, which has already been used in previous ML models. So the contribution is limited.\n2. As explicitly claimed in the paper and also reflected by the experimental results (e.g., Transformer-Big in Table 2). The hyperbolic metric only brings noticeable improvement to small neural nets with limited compacity on relatively small datasets. When applied it to most SOTA models (which are usually large/deep/wide neural networks) on larger datasets, it loses the advantage. This fact might seriously limit the application of the proposed technique.\n3. Small models are preferred for inference especially on edge devices. But model compression and knowledge distillation can make small models having similar performance as large models, which might be much better than directly training a small model with the proposed metric.\n4. Although hyperbolic metric reflects the power-law distribution, which is a very natural assumption verified on many kinds of real data (social networks and physical statistics), I am not fully convinced that it still holds on an embedding space produced by a neural net (since attention are usually applied to the outputs of a neural net). \n5. In the experiments, does the model with the proposed metric cost similar training/inference time comparing to the baselines? What is the trade-off between improvements and extra time costs? I notice that the results of the proposed attention in Table 2 are ~0.5% higher than the results from the earlier arXiv version of this paper. What is the reason for the improvements? Did you increase the training steps?", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper620/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hyperbolic Attention Networks", "abstract": "Recent approaches have successfully demonstrated the benefits of learning the parameters of shallow networks in hyperbolic space. We extend this line of work by imposing hyperbolic geometry on the embeddings used to compute the ubiquitous attention mechanisms for different neural networks architectures. By only changing the geometry of embedding of object representations, we can use the embedding space more efficiently without increasing the number of parameters of the model. Mainly as the number of objects grows exponentially for any semantic distance from the query, hyperbolic geometry  --as opposed to Euclidean geometry-- can encode those objects without having any interference. Our method shows improvements in generalization on neural machine translation on WMT'14 (English to German), learning on graphs (both on synthetic and real-world graph tasks) and visual question answering (CLEVR) tasks while keeping the neural representations compact.", "keywords": ["Hyperbolic Geometry", "Attention Methods", "Reasoning on Graphs", "Relation Learning", "Scale Free Graphs", "Transformers", "Power Law"], "authorids": ["ca9lar@gmail.com", "mdenil@google.com", "mateuszm@google.com", "alirazavi@google.com", "razp@google.com", "kmh@google.com", "vbapst@google.com", "drapso@google.com", "adamsantoro@google.com", "nandodefreitas@google.com"], "authors": ["Caglar Gulcehre", "Misha Denil", "Mateusz Malinowski", "Ali Razavi", "Razvan Pascanu", "Karl Moritz Hermann", "Peter Battaglia", "Victor Bapst", "David Raposo", "Adam Santoro", "Nando de Freitas"], "TL;DR": "We propose to incorporate inductive biases and operations coming from hyperbolic geometry to improve the attention mechanism of the neural networks.", "pdf": "/pdf/a363c6a7c4aaaadaaa0840b157dc28a84a295c34.pdf", "paperhash": "gulcehre|hyperbolic_attention_networks", "_bibtex": "@inproceedings{\ngulcehre2018hyperbolic,\ntitle={Hyperbolic Attention Networks},\nauthor={Caglar Gulcehre and Misha Denil and Mateusz Malinowski and Ali Razavi and Razvan Pascanu and Karl Moritz Hermann and Peter Battaglia and Victor Bapst and David Raposo and Adam Santoro and Nando de Freitas},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJxHsjRqFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper620/Official_Review", "cdate": 1542234418045, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "rJxHsjRqFQ", "replyto": "rJxHsjRqFQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper620/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335765330, "tmdate": 1552335765330, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper620/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "Hylg-SL927", "original": null, "number": 2, "cdate": 1541199095721, "ddate": null, "tcdate": 1541199095721, "tmdate": 1541533837243, "tddate": null, "forum": "rJxHsjRqFQ", "replyto": "rJxHsjRqFQ", "invitation": "ICLR.cc/2019/Conference/-/Paper620/Official_Review", "content": {"title": "Refreshing approach for matching and aggregating", "review": "The authors propose a novel approach to improve relational-attention by changing the matching and aggregation functions to use hyperbolic geometric. By doing so the network can exploit the metric structure the functions live on.  Method was evaluated and showed improvements over baselines on wide range of tasks including translation, graph learning, and visual question answering.\n\nPros: \n* High quality paper. \n* Hyperbolic matching function is novel and interesting.\n* Even though the subject isn\u2019t trivial, the intuition was described well. \n* The evaluation is comprehensive on several relational related tasks. \n\nCons:\n* Baselines: The authors main contribution is the matching and aggregation operator. It always feels like the multi-modal community is divided between VQA and CLEVR datasets, but there should be a lot in common between them. Specifically, what is called here the matching operator, had several variants in VQA, such as Multimodal Compact Bilinear Pooling by Fukui et al., or Multi-modal Factorized Bilinear Pooling\u00a0by You et al. etc. I think the paper would benefit from adding other variants of matching functions. \n* Datasets: I think the approach might work as well in VQA dataset, which I find more interesting than clever because of the real-world nature of it. You can plug it into methods like MFB, or as pairwise potentials in Structured Attentions\u00a0by Zhu et al, or High-Order attention by Schwartz et al\n\nConclusion:\nA better matching and aggregating operations are always important, it can potentially improve performance in many challenges. The proposed method is novel and interesting, therefore I will be happy to see this paper as part of ICLR. ", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper620/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hyperbolic Attention Networks", "abstract": "Recent approaches have successfully demonstrated the benefits of learning the parameters of shallow networks in hyperbolic space. We extend this line of work by imposing hyperbolic geometry on the embeddings used to compute the ubiquitous attention mechanisms for different neural networks architectures. By only changing the geometry of embedding of object representations, we can use the embedding space more efficiently without increasing the number of parameters of the model. Mainly as the number of objects grows exponentially for any semantic distance from the query, hyperbolic geometry  --as opposed to Euclidean geometry-- can encode those objects without having any interference. Our method shows improvements in generalization on neural machine translation on WMT'14 (English to German), learning on graphs (both on synthetic and real-world graph tasks) and visual question answering (CLEVR) tasks while keeping the neural representations compact.", "keywords": ["Hyperbolic Geometry", "Attention Methods", "Reasoning on Graphs", "Relation Learning", "Scale Free Graphs", "Transformers", "Power Law"], "authorids": ["ca9lar@gmail.com", "mdenil@google.com", "mateuszm@google.com", "alirazavi@google.com", "razp@google.com", "kmh@google.com", "vbapst@google.com", "drapso@google.com", "adamsantoro@google.com", "nandodefreitas@google.com"], "authors": ["Caglar Gulcehre", "Misha Denil", "Mateusz Malinowski", "Ali Razavi", "Razvan Pascanu", "Karl Moritz Hermann", "Peter Battaglia", "Victor Bapst", "David Raposo", "Adam Santoro", "Nando de Freitas"], "TL;DR": "We propose to incorporate inductive biases and operations coming from hyperbolic geometry to improve the attention mechanism of the neural networks.", "pdf": "/pdf/a363c6a7c4aaaadaaa0840b157dc28a84a295c34.pdf", "paperhash": "gulcehre|hyperbolic_attention_networks", "_bibtex": "@inproceedings{\ngulcehre2018hyperbolic,\ntitle={Hyperbolic Attention Networks},\nauthor={Caglar Gulcehre and Misha Denil and Mateusz Malinowski and Ali Razavi and Razvan Pascanu and Karl Moritz Hermann and Peter Battaglia and Victor Bapst and David Raposo and Adam Santoro and Nando de Freitas},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJxHsjRqFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper620/Official_Review", "cdate": 1542234418045, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "rJxHsjRqFQ", "replyto": "rJxHsjRqFQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper620/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335765330, "tmdate": 1552335765330, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper620/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "ryl7AYUH3Q", "original": null, "number": 1, "cdate": 1540872650733, "ddate": null, "tcdate": 1540872650733, "tmdate": 1541533836999, "tddate": null, "forum": "rJxHsjRqFQ", "replyto": "rJxHsjRqFQ", "invitation": "ICLR.cc/2019/Conference/-/Paper620/Official_Review", "content": {"title": "ACCEPTABLE", "review": "\nThe authors proposed to exploit hyperbolic geometry in computing the attention mechanisms for neural networks. Specifically, they break the attention read operation into two parts: matching and aggregation. In matching step, they use the hyperbolic distance to quantify the macthing between a query and a key; in the aggregation step, they use the Einstein midpoint. Their experiments results based on synthetic and real-world data shows the new method outperforms the traditional method based on Euclidean distance. This paper is acceptable.\n\n\nQuestion: In Figure 3(Center), the number of nodes 1000 and 1200 are pretty close. How about the results on 500 nodes and 2000 nodes? It seems the accuracy difference increases as the number of nodes increases. Is this true? \n\n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper620/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hyperbolic Attention Networks", "abstract": "Recent approaches have successfully demonstrated the benefits of learning the parameters of shallow networks in hyperbolic space. We extend this line of work by imposing hyperbolic geometry on the embeddings used to compute the ubiquitous attention mechanisms for different neural networks architectures. By only changing the geometry of embedding of object representations, we can use the embedding space more efficiently without increasing the number of parameters of the model. Mainly as the number of objects grows exponentially for any semantic distance from the query, hyperbolic geometry  --as opposed to Euclidean geometry-- can encode those objects without having any interference. Our method shows improvements in generalization on neural machine translation on WMT'14 (English to German), learning on graphs (both on synthetic and real-world graph tasks) and visual question answering (CLEVR) tasks while keeping the neural representations compact.", "keywords": ["Hyperbolic Geometry", "Attention Methods", "Reasoning on Graphs", "Relation Learning", "Scale Free Graphs", "Transformers", "Power Law"], "authorids": ["ca9lar@gmail.com", "mdenil@google.com", "mateuszm@google.com", "alirazavi@google.com", "razp@google.com", "kmh@google.com", "vbapst@google.com", "drapso@google.com", "adamsantoro@google.com", "nandodefreitas@google.com"], "authors": ["Caglar Gulcehre", "Misha Denil", "Mateusz Malinowski", "Ali Razavi", "Razvan Pascanu", "Karl Moritz Hermann", "Peter Battaglia", "Victor Bapst", "David Raposo", "Adam Santoro", "Nando de Freitas"], "TL;DR": "We propose to incorporate inductive biases and operations coming from hyperbolic geometry to improve the attention mechanism of the neural networks.", "pdf": "/pdf/a363c6a7c4aaaadaaa0840b157dc28a84a295c34.pdf", "paperhash": "gulcehre|hyperbolic_attention_networks", "_bibtex": "@inproceedings{\ngulcehre2018hyperbolic,\ntitle={Hyperbolic Attention Networks},\nauthor={Caglar Gulcehre and Misha Denil and Mateusz Malinowski and Ali Razavi and Razvan Pascanu and Karl Moritz Hermann and Peter Battaglia and Victor Bapst and David Raposo and Adam Santoro and Nando de Freitas},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJxHsjRqFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper620/Official_Review", "cdate": 1542234418045, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "rJxHsjRqFQ", "replyto": "rJxHsjRqFQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper620/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335765330, "tmdate": 1552335765330, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper620/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "BygxP5uf9Q", "original": null, "number": 1, "cdate": 1538587224230, "ddate": null, "tcdate": 1538587224230, "tmdate": 1538587224230, "tddate": null, "forum": "rJxHsjRqFQ", "replyto": "BylT_-kTY7", "invitation": "ICLR.cc/2019/Conference/-/Paper620/Official_Comment", "content": {"title": "Results on Pubmed and PPI", "comment": "Hi,\n\nThank you for your interest in our paper.\n\nWe have provided results on VQA (CLEVR and Sort-of-CLEVR), Neural Machine Translation (WMT'14 En-De), Graph Classification Tasks (synthetic with different sizes), finally on transductive graph tasks such as Citeseer and Cora tasks in our paper. We covered a wide range of possible tasks that an attention mechanism with different architectures can be applied to. We have provided both extensive analysis and promising results on the tasks that we explored in our paper. Our goal in this paper was to show the generality of our approach on a wide range of tasks.\n\nFor the time being, we do not have any plans to provide more experiments on other datasets besides the ones that are already presented in the paper.\n\nBest,"}, "signatures": ["ICLR.cc/2019/Conference/Paper620/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper620/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper620/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hyperbolic Attention Networks", "abstract": "Recent approaches have successfully demonstrated the benefits of learning the parameters of shallow networks in hyperbolic space. We extend this line of work by imposing hyperbolic geometry on the embeddings used to compute the ubiquitous attention mechanisms for different neural networks architectures. By only changing the geometry of embedding of object representations, we can use the embedding space more efficiently without increasing the number of parameters of the model. Mainly as the number of objects grows exponentially for any semantic distance from the query, hyperbolic geometry  --as opposed to Euclidean geometry-- can encode those objects without having any interference. Our method shows improvements in generalization on neural machine translation on WMT'14 (English to German), learning on graphs (both on synthetic and real-world graph tasks) and visual question answering (CLEVR) tasks while keeping the neural representations compact.", "keywords": ["Hyperbolic Geometry", "Attention Methods", "Reasoning on Graphs", "Relation Learning", "Scale Free Graphs", "Transformers", "Power Law"], "authorids": ["ca9lar@gmail.com", "mdenil@google.com", "mateuszm@google.com", "alirazavi@google.com", "razp@google.com", "kmh@google.com", "vbapst@google.com", "drapso@google.com", "adamsantoro@google.com", "nandodefreitas@google.com"], "authors": ["Caglar Gulcehre", "Misha Denil", "Mateusz Malinowski", "Ali Razavi", "Razvan Pascanu", "Karl Moritz Hermann", "Peter Battaglia", "Victor Bapst", "David Raposo", "Adam Santoro", "Nando de Freitas"], "TL;DR": "We propose to incorporate inductive biases and operations coming from hyperbolic geometry to improve the attention mechanism of the neural networks.", "pdf": "/pdf/a363c6a7c4aaaadaaa0840b157dc28a84a295c34.pdf", "paperhash": "gulcehre|hyperbolic_attention_networks", "_bibtex": "@inproceedings{\ngulcehre2018hyperbolic,\ntitle={Hyperbolic Attention Networks},\nauthor={Caglar Gulcehre and Misha Denil and Mateusz Malinowski and Ali Razavi and Razvan Pascanu and Karl Moritz Hermann and Peter Battaglia and Victor Bapst and David Raposo and Adam Santoro and Nando de Freitas},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJxHsjRqFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper620/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621624067, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJxHsjRqFQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper620/Authors", "ICLR.cc/2019/Conference/Paper620/Reviewers", "ICLR.cc/2019/Conference/Paper620/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper620/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper620/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper620/Authors|ICLR.cc/2019/Conference/Paper620/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper620/Reviewers", "ICLR.cc/2019/Conference/Paper620/Authors", "ICLR.cc/2019/Conference/Paper620/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621624067}}}, {"id": "BylT_-kTY7", "original": null, "number": 1, "cdate": 1538220405280, "ddate": null, "tcdate": 1538220405280, "tmdate": 1538220466440, "tddate": null, "forum": "rJxHsjRqFQ", "replyto": "rJxHsjRqFQ", "invitation": "ICLR.cc/2019/Conference/-/Paper620/Public_Comment", "content": {"comment": "I would like to ask you whether you are still working on these two datasets or not tending to compare with other models on the two datasets?", "title": "Not have results of Pubmed and PPI"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper620/Reviewers/Unsubmitted"], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hyperbolic Attention Networks", "abstract": "Recent approaches have successfully demonstrated the benefits of learning the parameters of shallow networks in hyperbolic space. We extend this line of work by imposing hyperbolic geometry on the embeddings used to compute the ubiquitous attention mechanisms for different neural networks architectures. By only changing the geometry of embedding of object representations, we can use the embedding space more efficiently without increasing the number of parameters of the model. Mainly as the number of objects grows exponentially for any semantic distance from the query, hyperbolic geometry  --as opposed to Euclidean geometry-- can encode those objects without having any interference. Our method shows improvements in generalization on neural machine translation on WMT'14 (English to German), learning on graphs (both on synthetic and real-world graph tasks) and visual question answering (CLEVR) tasks while keeping the neural representations compact.", "keywords": ["Hyperbolic Geometry", "Attention Methods", "Reasoning on Graphs", "Relation Learning", "Scale Free Graphs", "Transformers", "Power Law"], "authorids": ["ca9lar@gmail.com", "mdenil@google.com", "mateuszm@google.com", "alirazavi@google.com", "razp@google.com", "kmh@google.com", "vbapst@google.com", "drapso@google.com", "adamsantoro@google.com", "nandodefreitas@google.com"], "authors": ["Caglar Gulcehre", "Misha Denil", "Mateusz Malinowski", "Ali Razavi", "Razvan Pascanu", "Karl Moritz Hermann", "Peter Battaglia", "Victor Bapst", "David Raposo", "Adam Santoro", "Nando de Freitas"], "TL;DR": "We propose to incorporate inductive biases and operations coming from hyperbolic geometry to improve the attention mechanism of the neural networks.", "pdf": "/pdf/a363c6a7c4aaaadaaa0840b157dc28a84a295c34.pdf", "paperhash": "gulcehre|hyperbolic_attention_networks", "_bibtex": "@inproceedings{\ngulcehre2018hyperbolic,\ntitle={Hyperbolic Attention Networks},\nauthor={Caglar Gulcehre and Misha Denil and Mateusz Malinowski and Ali Razavi and Razvan Pascanu and Karl Moritz Hermann and Peter Battaglia and Victor Bapst and David Raposo and Adam Santoro and Nando de Freitas},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJxHsjRqFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper620/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311792716, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "rJxHsjRqFQ", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper620/Authors", "ICLR.cc/2019/Conference/Paper620/Reviewers", "ICLR.cc/2019/Conference/Paper620/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper620/Authors", "ICLR.cc/2019/Conference/Paper620/Reviewers", "ICLR.cc/2019/Conference/Paper620/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311792716}}}], "count": 12}