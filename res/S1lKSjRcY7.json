{"notes": [{"id": "SylUS03elV", "original": null, "number": 1, "cdate": 1544764990120, "ddate": null, "tcdate": 1544764990120, "tmdate": 1545949619250, "tddate": null, "forum": "S1lKSjRcY7", "replyto": "S1lKSjRcY7", "invitation": "ICLR.cc/2019/Conference/-/Paper105/Meta_Review", "content": {"title": "Sensible ideas scattered throughout, but does not engage with similar earlier work.", "metareview": "Strengths: This paper provides a useful review of some of the recent work on gradient estimators for discrete variables, and proposes both a computationally more efficient variant of one, and a new estimator based on piecewise linear functions.\n\nWeaknesses:  Many new ideas are scattered throughout the paper.  The notation is a bit dense.  Comparisons to RELAX, which had better results than REBAR, are missing.  Finally, it seems that REBAR was trained with a fixed temperature, instead of optimizing it during training, which is one of the main benefits of the method.\n\nPoints of contention: Only R1 mentioned the omission of REBAR and RELAX.  A discussion and a few comparisons to REBAR were added to the paper, but only in a few experiments.\n\nConsensus:  This paper is borderline.  I agree with R1: quality 6, clarity 8, originality 6, significance 4.  All reviewers agreed that this was a decent paper but I think that R2 and R3 were relatively unfamiliar with the existing literature.\n\nUpdate for clarification:\n=====================\n\nThis section has been added to clarify the reasons for rejection.  The abstract of the paper states:\n\n\"We show that the commonly used Gumbel-Softmax estimator is biased and propose a simple method to reduce it. We also derive a simpler piece-wise linear continuous relaxation that also possesses reduced bias. We demonstrate empirically that reduced bias leads to a better\nperformance in variational inference and on binary optimization tasks.\"\n\nThe fact that Gumbel-Softmax is biased is well-known.  Reducing its bias was the motivation for developing the _exactly_ unbiased REBAR method, which already has similar asymptotic complexity.  A major side-benefit of using an exactly unbiased estimator is that the estimator's hyperparameters can be automatically tuned to reduce variance, as in REBAR and RELAX.\n\nThis paper focuses on methods for reducing bias and variance, but hardly discusses related methods that already achieved its stated aims. This a major weakness of the paper.  The experiments only compared with REBAR, and did not even tune the temperature to reduce variance (removing one of its major advantages).\n\nThis reject decision is not made mainly on lack of experiments or state-of-the-art results.  It's because the idea of reducing the bias of continuous-relaxation-based gradient estimators has already been fruitfully explored, and zero-bias CR estimators have been developed, but this work mostly ignores them.  However, thorough experiments are always going to be necessary for a paper proposing biased estimators, because there are already many such estimators, and little theory to say which ones will work well in which situations.\n\nSuggestions to improve the paper:  Run experiments on all methods that directly measure bias and variance.  Incorporate discussion of REBAR throughout, not just in an appendix.  Run comparisons against REBAR and RELAX without crippling their ability to reduce variance.   Do more to characterize when different estimators will be expected to be effective.", "recommendation": "Reject", "confidence": "3: The area chair is somewhat confident"}, "signatures": ["ICLR.cc/2019/Conference/Paper105/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper105/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improved Gradient Estimators for Stochastic Discrete Variables", "abstract": "In many applications we seek to optimize an expectation with respect to a distribution over discrete variables. Estimating gradients of such objectives with respect to the distribution parameters is a challenging problem. We analyze existing solutions including finite-difference (FD) estimators and continuous relaxation (CR) estimators in terms of bias and variance. We show that the commonly used Gumbel-Softmax estimator is biased and propose a simple method to reduce it. We also derive a simpler piece-wise linear continuous relaxation that also possesses reduced bias. We demonstrate empirically that reduced bias leads to a better performance in variational inference and on binary optimization tasks.", "keywords": ["continuous relaxation", "discrete stochastic variables", "reparameterization trick", "variational inference", "discrete optimization", "stochastic gradient estimation"], "authorids": ["eandriyash@dwavesys.com", "avahdat@dwavesys.com", "wgm@dwavesys.com"], "authors": ["Evgeny Andriyash", "Arash Vahdat", "Bill Macready"], "TL;DR": "We propose simple ways to reduce bias and complexity of stochastic gradient estimators used for learning distributions over discrete variables.", "pdf": "/pdf/ae83aaf46d270292953663aef66b0b6342cf97b4.pdf", "paperhash": "andriyash|improved_gradient_estimators_for_stochastic_discrete_variables", "_bibtex": "@misc{\nandriyash2019improved,\ntitle={Improved Gradient Estimators for Stochastic Discrete Variables},\nauthor={Evgeny Andriyash and Arash Vahdat and Bill Macready},\nyear={2019},\nurl={https://openreview.net/forum?id=S1lKSjRcY7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper105/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353334479, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1lKSjRcY7", "replyto": "S1lKSjRcY7", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper105/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper105/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper105/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353334479}}}, {"id": "S1lKSjRcY7", "original": "rklCznKKtX", "number": 105, "cdate": 1538087744825, "ddate": null, "tcdate": 1538087744825, "tmdate": 1545355390940, "tddate": null, "forum": "S1lKSjRcY7", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Improved Gradient Estimators for Stochastic Discrete Variables", "abstract": "In many applications we seek to optimize an expectation with respect to a distribution over discrete variables. Estimating gradients of such objectives with respect to the distribution parameters is a challenging problem. We analyze existing solutions including finite-difference (FD) estimators and continuous relaxation (CR) estimators in terms of bias and variance. We show that the commonly used Gumbel-Softmax estimator is biased and propose a simple method to reduce it. We also derive a simpler piece-wise linear continuous relaxation that also possesses reduced bias. We demonstrate empirically that reduced bias leads to a better performance in variational inference and on binary optimization tasks.", "keywords": ["continuous relaxation", "discrete stochastic variables", "reparameterization trick", "variational inference", "discrete optimization", "stochastic gradient estimation"], "authorids": ["eandriyash@dwavesys.com", "avahdat@dwavesys.com", "wgm@dwavesys.com"], "authors": ["Evgeny Andriyash", "Arash Vahdat", "Bill Macready"], "TL;DR": "We propose simple ways to reduce bias and complexity of stochastic gradient estimators used for learning distributions over discrete variables.", "pdf": "/pdf/ae83aaf46d270292953663aef66b0b6342cf97b4.pdf", "paperhash": "andriyash|improved_gradient_estimators_for_stochastic_discrete_variables", "_bibtex": "@misc{\nandriyash2019improved,\ntitle={Improved Gradient Estimators for Stochastic Discrete Variables},\nauthor={Evgeny Andriyash and Arash Vahdat and Bill Macready},\nyear={2019},\nurl={https://openreview.net/forum?id=S1lKSjRcY7},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "r1lMeCKcnm", "original": null, "number": 3, "cdate": 1541213674488, "ddate": null, "tcdate": 1541213674488, "tmdate": 1543273331041, "tddate": null, "forum": "S1lKSjRcY7", "replyto": "S1lKSjRcY7", "invitation": "ICLR.cc/2019/Conference/-/Paper105/Official_Review", "content": {"title": "Well written; contributions intuitively explained and motivated", "review": "After revision:\nThe authors have addressed all points in my review. Although I will not be increasing the score, these fixes certainly increase the confidence of my evaluation and I think it deserves to be accepted.\n\n====================\n\nSummary: This paper analyzes finite-difference and continuous relaxation gradient estimators for discrete random variables and from their analysis develop improvements to these existing methods. They empirically demonstrate the improvement by evaluating the gradient estimators on toy tasks and an autoencoding task.\n\nWriting: I found this paper very well written and explained. It covered an extensive background concisely while introducing all necessary ideas to understand the contributions of the paper.\n\nComments: Overall, I found the ideas presented in this paper interesting and novel, and results sufficiently strong to support the ideas. Though the contributions are not groundbreaking, they will certainly be useful to researchers in this space. I have some minor comments relating to notation and related work.\n\n- I found the notation in Section 3.2 to be a little confusing, namely that $\\zeta$ appears as both a random variable and a continuous function (that takes in one variable in the paragraph after eq11, but takes in two variables in eq15). I understand that the authors may have done this to suppress extra notation, but I found this section harder to understand than the rest due to this choice. There is also a small typo in eq2 where the $\\phi$ from $l_\\phi$ is dropped.\n\n- I think it would be useful to compare IGSM and PWL against a score-function gradient estimator (maybe REBAR, given the similarity in experiment setup). The authors do contextualize the line of work concerning score-function gradient estimators. However, since SF estimators are unbiased but high variance and the authors aim to reduce bias at the cost of variance, I think evaluating SF baselines will better contextualize the tradeoffs made in this paper.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper105/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Improved Gradient Estimators for Stochastic Discrete Variables", "abstract": "In many applications we seek to optimize an expectation with respect to a distribution over discrete variables. Estimating gradients of such objectives with respect to the distribution parameters is a challenging problem. We analyze existing solutions including finite-difference (FD) estimators and continuous relaxation (CR) estimators in terms of bias and variance. We show that the commonly used Gumbel-Softmax estimator is biased and propose a simple method to reduce it. We also derive a simpler piece-wise linear continuous relaxation that also possesses reduced bias. We demonstrate empirically that reduced bias leads to a better performance in variational inference and on binary optimization tasks.", "keywords": ["continuous relaxation", "discrete stochastic variables", "reparameterization trick", "variational inference", "discrete optimization", "stochastic gradient estimation"], "authorids": ["eandriyash@dwavesys.com", "avahdat@dwavesys.com", "wgm@dwavesys.com"], "authors": ["Evgeny Andriyash", "Arash Vahdat", "Bill Macready"], "TL;DR": "We propose simple ways to reduce bias and complexity of stochastic gradient estimators used for learning distributions over discrete variables.", "pdf": "/pdf/ae83aaf46d270292953663aef66b0b6342cf97b4.pdf", "paperhash": "andriyash|improved_gradient_estimators_for_stochastic_discrete_variables", "_bibtex": "@misc{\nandriyash2019improved,\ntitle={Improved Gradient Estimators for Stochastic Discrete Variables},\nauthor={Evgeny Andriyash and Arash Vahdat and Bill Macready},\nyear={2019},\nurl={https://openreview.net/forum?id=S1lKSjRcY7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper105/Official_Review", "cdate": 1542234536734, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "S1lKSjRcY7", "replyto": "S1lKSjRcY7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper105/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335650411, "tmdate": 1552335650411, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper105/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "SklhF5_oaQ", "original": null, "number": 1, "cdate": 1542322820299, "ddate": null, "tcdate": 1542322820299, "tmdate": 1542323101841, "tddate": null, "forum": "S1lKSjRcY7", "replyto": "r1lMeCKcnm", "invitation": "ICLR.cc/2019/Conference/-/Paper105/Official_Comment", "content": {"title": "Response to Reviewer 2", "comment": "Thank you for the constructive comments and suggestions. We have updated the paper:\n- We clarified the notation in Section 3.2, by removing this ambiguity of using $\\zeta$ symbol. \n- We also corrected the typo $l$ -> $l_\\phi$ (thank you for pointing it out!). \n- We added experimental results using REBAR in training VAE in Fig. 4, 5 and Appendix E. We agree that these experiments   provide another useful  comparison with the state-of-the-art."}, "signatures": ["ICLR.cc/2019/Conference/Paper105/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper105/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper105/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improved Gradient Estimators for Stochastic Discrete Variables", "abstract": "In many applications we seek to optimize an expectation with respect to a distribution over discrete variables. Estimating gradients of such objectives with respect to the distribution parameters is a challenging problem. We analyze existing solutions including finite-difference (FD) estimators and continuous relaxation (CR) estimators in terms of bias and variance. We show that the commonly used Gumbel-Softmax estimator is biased and propose a simple method to reduce it. We also derive a simpler piece-wise linear continuous relaxation that also possesses reduced bias. We demonstrate empirically that reduced bias leads to a better performance in variational inference and on binary optimization tasks.", "keywords": ["continuous relaxation", "discrete stochastic variables", "reparameterization trick", "variational inference", "discrete optimization", "stochastic gradient estimation"], "authorids": ["eandriyash@dwavesys.com", "avahdat@dwavesys.com", "wgm@dwavesys.com"], "authors": ["Evgeny Andriyash", "Arash Vahdat", "Bill Macready"], "TL;DR": "We propose simple ways to reduce bias and complexity of stochastic gradient estimators used for learning distributions over discrete variables.", "pdf": "/pdf/ae83aaf46d270292953663aef66b0b6342cf97b4.pdf", "paperhash": "andriyash|improved_gradient_estimators_for_stochastic_discrete_variables", "_bibtex": "@misc{\nandriyash2019improved,\ntitle={Improved Gradient Estimators for Stochastic Discrete Variables},\nauthor={Evgeny Andriyash and Arash Vahdat and Bill Macready},\nyear={2019},\nurl={https://openreview.net/forum?id=S1lKSjRcY7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper105/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621619441, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1lKSjRcY7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper105/Authors", "ICLR.cc/2019/Conference/Paper105/Reviewers", "ICLR.cc/2019/Conference/Paper105/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper105/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper105/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper105/Authors|ICLR.cc/2019/Conference/Paper105/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper105/Reviewers", "ICLR.cc/2019/Conference/Paper105/Authors", "ICLR.cc/2019/Conference/Paper105/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621619441}}}, {"id": "HklqFsdiTX", "original": null, "number": 3, "cdate": 1542323073911, "ddate": null, "tcdate": 1542323073911, "tmdate": 1542323073911, "tddate": null, "forum": "S1lKSjRcY7", "replyto": "HJg9OkW_hQ", "invitation": "ICLR.cc/2019/Conference/-/Paper105/Official_Comment", "content": {"title": "Response to Reviewer 1", "comment": "Thank you for the helpful suggestions. We have updated the paper. Please find our responses below:\n\n\"1. the paper content is a little disjointed: the improvement over RAM has not much relation with later improvements. It seems the paper is stacking different things into the paper.\" - We agree that improvement to RAM has little to do with CR estimators discussed afterwards. However, sampled RAM does have a connection to CR: both estimators evaluate the gradient only through a subset of stochastic variables $z_i$. Sampled RAM chooses this subset explicitly and evaluates the gradients via FD.  CR samples relaxed variables and effectively only a subset of them will deviate from {0,1} (in the binary variable case) and will possess non-zero gradients. We have added a clarifying sentence to the text.\n\n\"1. The REBAR estimator [Tucker et al., 2017] and the LAX estimator [Grathwohl et al., 2018] use continuous approximation and correct it to be unbiased. These papers in this thread are not well discussed in the paper. They are not compared in the experiment either.\" - We have added a comparison to REBAR in Fig. 4, 5 and Appendix E.\n\n\n\"2. In the equation 7 and above: what does 4 mean? When beta \\neq 4, do you still get unbiased estimation? My understanding is that the estimator is unbiased only when beta=4. (correct me if I'm wrong)\" - We added a factor of 4 so that the probability of keeping a variable, $p = [4 q (1-q)]/\\beta$,  is consistent with CR estimators. For example, in the case of the PWL estimator we have chosen to parameterize the slope as $\\alpha = \\beta/[4 q(1-q)]$ (above Eq (16)) which leads to the probability for the variable to have non-zero gradient equal to $p = [4 q (1-q)]/\\beta$.  Other than that there is no special meaning to having it there. Eq (7) contains the factor $\\beta/4$ just to compensate for our parameterization of $p$, so that $\\E[ \\beta \\zeta/4] = q (1-q)$ and on average Eq. (7) gives the same gradient as Eq. (3). So Eq. (7) is unbiased for every value of $\\beta$.\n\n\n\"3. The paper argues that the variance of the estimator is mostly decided by the variance of q(zeta)^-1 when the function is smooth. I feel this argument is not very clear. First, what do you mean by saying the function is smooth? The derivative is near a constant in [0, 1]?\" - We agree that this statement about smoothness of function $f(\\zeta)$ is a bit vague. We have replaced it in the paper with \u201cIf the derivative $\\partial_\\zeta f(\\zeta)$ does not change significantly in the interval $\\zeta \\in [0, 1]$ then the variance of  this  estimate  is  controlled  by ... \u201d.\n\n\"4. In the PWL development, the paper argues that we can choose alpha_i \\approx 1/(q_i(1-q_i)) to minimize the variance. However, my understanding is, the smaller alpha_i, the smaller variance.\" - Thank you for pointing this out! We agree that in order to minimize the variance one has to minimize variance of each term independently, which can be done by choosing the smallest possible $\\alpha_i$. We have removed this argument from the paper.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper105/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper105/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper105/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improved Gradient Estimators for Stochastic Discrete Variables", "abstract": "In many applications we seek to optimize an expectation with respect to a distribution over discrete variables. Estimating gradients of such objectives with respect to the distribution parameters is a challenging problem. We analyze existing solutions including finite-difference (FD) estimators and continuous relaxation (CR) estimators in terms of bias and variance. We show that the commonly used Gumbel-Softmax estimator is biased and propose a simple method to reduce it. We also derive a simpler piece-wise linear continuous relaxation that also possesses reduced bias. We demonstrate empirically that reduced bias leads to a better performance in variational inference and on binary optimization tasks.", "keywords": ["continuous relaxation", "discrete stochastic variables", "reparameterization trick", "variational inference", "discrete optimization", "stochastic gradient estimation"], "authorids": ["eandriyash@dwavesys.com", "avahdat@dwavesys.com", "wgm@dwavesys.com"], "authors": ["Evgeny Andriyash", "Arash Vahdat", "Bill Macready"], "TL;DR": "We propose simple ways to reduce bias and complexity of stochastic gradient estimators used for learning distributions over discrete variables.", "pdf": "/pdf/ae83aaf46d270292953663aef66b0b6342cf97b4.pdf", "paperhash": "andriyash|improved_gradient_estimators_for_stochastic_discrete_variables", "_bibtex": "@misc{\nandriyash2019improved,\ntitle={Improved Gradient Estimators for Stochastic Discrete Variables},\nauthor={Evgeny Andriyash and Arash Vahdat and Bill Macready},\nyear={2019},\nurl={https://openreview.net/forum?id=S1lKSjRcY7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper105/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621619441, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1lKSjRcY7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper105/Authors", "ICLR.cc/2019/Conference/Paper105/Reviewers", "ICLR.cc/2019/Conference/Paper105/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper105/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper105/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper105/Authors|ICLR.cc/2019/Conference/Paper105/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper105/Reviewers", "ICLR.cc/2019/Conference/Paper105/Authors", "ICLR.cc/2019/Conference/Paper105/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621619441}}}, {"id": "HyeW6cuoaX", "original": null, "number": 2, "cdate": 1542322873042, "ddate": null, "tcdate": 1542322873042, "tmdate": 1542322873042, "tddate": null, "forum": "S1lKSjRcY7", "replyto": "SkesQF453Q", "invitation": "ICLR.cc/2019/Conference/-/Paper105/Official_Comment", "content": {"title": "Response to Reviewer 3", "comment": "Thank you for the clarifying questions. Please find our responses below:\n\n1. \"How does the dimension of the variables affect the bias and variance of the proposed estimator?\" - For the case of a factorial posterior distribution over binary variables, our proposed improved estimator is given by Eq. (13). The bias here comes from the the deviation of relaxed $\\zeta_{\\setminus i}$ from binary $z_{\\setminus i}$. The magnitude of this bias depends on the function $f(z)$ that we are minimizing, but in general the bias is expected to grow with the number of variables. The main contribution to the variance of Eq. (13) comes from the terms $\\partial  \\zeta_i / \\partial \\rho_i$. Since these terms are independent (they depend on different $\\rho_i \\in U[0,1]$), the variance of the sum is expected to grow linearly with $M$ in general. This means that relative standard deviation (standard deviation / mean) scales as $\\sim 1/\\sqrt{M}$ in general.\n\n2. \"[Are] the proposed estimators applicable to hierarchical models with multi-discrete latent variables?\"  - We show in the Appendix C, the proposed improved estimators can be applied to hierarchical models (Bayesian network distributions): one just needs to replace $\\partial_{q_i} \\zeta_i$ with $\\partial_{\\rho_i} \\zeta_i$ throughout the hierarchy.\n\n3. \"What's the performance of the proposed method compared with the others in terms of running time?\" - We do not report running times in the paper, but in our experiments we saw similar running times for improved continuous relaxations versus the original ones (single function evaluation). In the case of finite-difference estimators, RAM involves $M$ function evaluations, Sampled RAM uses varying number of function evaluations (roughly $ M/10$ by the end of training), ARM uses 2 function evaluations. However, due to the GPU parallelization and small size of the neural networks in this work, we do not observe a significant variation in running time.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper105/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper105/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper105/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improved Gradient Estimators for Stochastic Discrete Variables", "abstract": "In many applications we seek to optimize an expectation with respect to a distribution over discrete variables. Estimating gradients of such objectives with respect to the distribution parameters is a challenging problem. We analyze existing solutions including finite-difference (FD) estimators and continuous relaxation (CR) estimators in terms of bias and variance. We show that the commonly used Gumbel-Softmax estimator is biased and propose a simple method to reduce it. We also derive a simpler piece-wise linear continuous relaxation that also possesses reduced bias. We demonstrate empirically that reduced bias leads to a better performance in variational inference and on binary optimization tasks.", "keywords": ["continuous relaxation", "discrete stochastic variables", "reparameterization trick", "variational inference", "discrete optimization", "stochastic gradient estimation"], "authorids": ["eandriyash@dwavesys.com", "avahdat@dwavesys.com", "wgm@dwavesys.com"], "authors": ["Evgeny Andriyash", "Arash Vahdat", "Bill Macready"], "TL;DR": "We propose simple ways to reduce bias and complexity of stochastic gradient estimators used for learning distributions over discrete variables.", "pdf": "/pdf/ae83aaf46d270292953663aef66b0b6342cf97b4.pdf", "paperhash": "andriyash|improved_gradient_estimators_for_stochastic_discrete_variables", "_bibtex": "@misc{\nandriyash2019improved,\ntitle={Improved Gradient Estimators for Stochastic Discrete Variables},\nauthor={Evgeny Andriyash and Arash Vahdat and Bill Macready},\nyear={2019},\nurl={https://openreview.net/forum?id=S1lKSjRcY7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper105/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621619441, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1lKSjRcY7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper105/Authors", "ICLR.cc/2019/Conference/Paper105/Reviewers", "ICLR.cc/2019/Conference/Paper105/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper105/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper105/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper105/Authors|ICLR.cc/2019/Conference/Paper105/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper105/Reviewers", "ICLR.cc/2019/Conference/Paper105/Authors", "ICLR.cc/2019/Conference/Paper105/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621619441}}}, {"id": "SkesQF453Q", "original": null, "number": 2, "cdate": 1541191971503, "ddate": null, "tcdate": 1541191971503, "tmdate": 1541534278629, "tddate": null, "forum": "S1lKSjRcY7", "replyto": "S1lKSjRcY7", "invitation": "ICLR.cc/2019/Conference/-/Paper105/Official_Review", "content": {"title": "The paper proposed to reduce the computation of the Re-parameterization and Marginalization method and the bias of Continuous Relaxation estimator.", "review": "The paper proposed a modification to RAM that allows us to trade decreased computational cost for increased variance. It also proposes an improved continuous relaxation (ICR) estimator to reduce the bias of CR, which is extended to categorical variables.\nThe proposed piece-wise linear relaxation (PWL) can be considered as the inverse CDF of the random variable is very interesting. The ICR estimators can also be extended to categorical variables. \nThe paper is well written. I have some questions:\n1.\tHow does the dimension of the variables affect the bias and variance of the proposed estimator?\n2.\tDose the proposed estimators applicable to hierarchical models with multi-discrete latent variables?\n3.\t What\u2019s the performance of the proposed method compared with the others in terms of running time?\n", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper105/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improved Gradient Estimators for Stochastic Discrete Variables", "abstract": "In many applications we seek to optimize an expectation with respect to a distribution over discrete variables. Estimating gradients of such objectives with respect to the distribution parameters is a challenging problem. We analyze existing solutions including finite-difference (FD) estimators and continuous relaxation (CR) estimators in terms of bias and variance. We show that the commonly used Gumbel-Softmax estimator is biased and propose a simple method to reduce it. We also derive a simpler piece-wise linear continuous relaxation that also possesses reduced bias. We demonstrate empirically that reduced bias leads to a better performance in variational inference and on binary optimization tasks.", "keywords": ["continuous relaxation", "discrete stochastic variables", "reparameterization trick", "variational inference", "discrete optimization", "stochastic gradient estimation"], "authorids": ["eandriyash@dwavesys.com", "avahdat@dwavesys.com", "wgm@dwavesys.com"], "authors": ["Evgeny Andriyash", "Arash Vahdat", "Bill Macready"], "TL;DR": "We propose simple ways to reduce bias and complexity of stochastic gradient estimators used for learning distributions over discrete variables.", "pdf": "/pdf/ae83aaf46d270292953663aef66b0b6342cf97b4.pdf", "paperhash": "andriyash|improved_gradient_estimators_for_stochastic_discrete_variables", "_bibtex": "@misc{\nandriyash2019improved,\ntitle={Improved Gradient Estimators for Stochastic Discrete Variables},\nauthor={Evgeny Andriyash and Arash Vahdat and Bill Macready},\nyear={2019},\nurl={https://openreview.net/forum?id=S1lKSjRcY7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper105/Official_Review", "cdate": 1542234536734, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "S1lKSjRcY7", "replyto": "S1lKSjRcY7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper105/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335650411, "tmdate": 1552335650411, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper105/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "HJg9OkW_hQ", "original": null, "number": 1, "cdate": 1541046129821, "ddate": null, "tcdate": 1541046129821, "tmdate": 1541534278428, "tddate": null, "forum": "S1lKSjRcY7", "replyto": "S1lKSjRcY7", "invitation": "ICLR.cc/2019/Conference/-/Paper105/Official_Review", "content": {"title": "A paper reviewing and improving different types of gradient estimators", "review": "The papers studies estimators of gradients taken from expectations with respect to the distribution parameters. The paper has studied two main types of estimators, Finite Difference and Continuous Relaxation. The paper made several improvements to existing estimators. \n\nMy rating of the paper in different aspects (quality 6, clarity 8, originality 6, significance 4). \n\nPros: \n1. The paper has made a nice introduction of FD and CR estimators. The improvements over previous estimators are concrete -- it is generally clear to see the benefit of these improvements. \n\n2. The first method reduces the running time of the RAM estimator. The second method (IGM) reduces the bias of GM estimator. The first improvement avoids many function evaluations when the probability is extreme. The second improvement helps to correct bias introduced by continuous approximation of \\zeta_i itself. \n\nCons: \n1. the paper content is a little disjointed: the improvement over RAM has not much relation with later improvements. It seems the paper is stacking different things into the paper. \n\n2. All these improvements are not very significant considering a few previous papers on this topic. Some arguments are not rigorous. (see details below)\n\n3. A few important papers are not well discussed and omitted from the experiment section. \n\nDetailed comments\n\n1. The REBAR estimator [Tucker et al., 2017] and the LAX estimator [Grathwohl et al., 2018] use continuous approximation and correct it to be unbiased. These papers in this thread are not well discussed in the paper. They are not compared in the experiment either.  \n\n2. In the equation 7 and above: what does 4 mean? When beta \\neq 4, do you still get unbiased estimation? My understanding is that the estimator is unbiased only when beta=4. (correct me if I'm wrong)\n\n3. The paper argues that the variance of the estimator is mostly decided by the variance of q(zeta)^-1 when the function is smooth. I feel this argument is not very clear. First, what do you mean by saying the function is smooth? The derivative is near a constant in [0, 1]? \n\n4. In the PWL development, the paper argues that we can choose alpha_i \\approx 1/(q_i(1-q_i)) to minimize the variance. However, my understanding is, the smaller alpha_i, the smaller variance.\n\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper105/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improved Gradient Estimators for Stochastic Discrete Variables", "abstract": "In many applications we seek to optimize an expectation with respect to a distribution over discrete variables. Estimating gradients of such objectives with respect to the distribution parameters is a challenging problem. We analyze existing solutions including finite-difference (FD) estimators and continuous relaxation (CR) estimators in terms of bias and variance. We show that the commonly used Gumbel-Softmax estimator is biased and propose a simple method to reduce it. We also derive a simpler piece-wise linear continuous relaxation that also possesses reduced bias. We demonstrate empirically that reduced bias leads to a better performance in variational inference and on binary optimization tasks.", "keywords": ["continuous relaxation", "discrete stochastic variables", "reparameterization trick", "variational inference", "discrete optimization", "stochastic gradient estimation"], "authorids": ["eandriyash@dwavesys.com", "avahdat@dwavesys.com", "wgm@dwavesys.com"], "authors": ["Evgeny Andriyash", "Arash Vahdat", "Bill Macready"], "TL;DR": "We propose simple ways to reduce bias and complexity of stochastic gradient estimators used for learning distributions over discrete variables.", "pdf": "/pdf/ae83aaf46d270292953663aef66b0b6342cf97b4.pdf", "paperhash": "andriyash|improved_gradient_estimators_for_stochastic_discrete_variables", "_bibtex": "@misc{\nandriyash2019improved,\ntitle={Improved Gradient Estimators for Stochastic Discrete Variables},\nauthor={Evgeny Andriyash and Arash Vahdat and Bill Macready},\nyear={2019},\nurl={https://openreview.net/forum?id=S1lKSjRcY7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper105/Official_Review", "cdate": 1542234536734, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "S1lKSjRcY7", "replyto": "S1lKSjRcY7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper105/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335650411, "tmdate": 1552335650411, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper105/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 8}