{"notes": [{"tddate": null, "replyto": null, "ddate": null, "cdate": null, "tmdate": 1488570646880, "tcdate": 1478377861671, "number": 589, "id": "BJAFbaolg", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "BJAFbaolg", "signatures": ["~Pascal_Vincent1"], "readers": ["everyone"], "content": {"title": "Learning to Generate Samples from Noise through Infusion Training", "abstract": "In this work, we investigate a novel training procedure to learn a generative model as the transition operator of a Markov chain, such that, when applied repeatedly on an unstructured random noise sample, it will denoise it into a sample that matches the target distribution from the training set. The novel training procedure to learn this progressive denoising operation involves sampling from a slightly different chain than the model chain used for generation in the absence of a denoising target. In the training chain we infuse information from the training target example that we would like the chains to reach with a high probability. The thus learned transition operator is able to produce quality and varied samples in a small number of steps. Experiments show competitive results compared to the samples generated with a basic Generative Adversarial Net. ", "pdf": "/pdf/388d81ae0dd044220d7a880fbc2b8d904de2eedf.pdf", "TL;DR": "We learn a markov transition operator acting on inputspace, to denoise random noise into a target distribution. We use a novel target injection technique to guide the training.", "paperhash": "bordes|learning_to_generate_samples_from_noise_through_infusion_training", "keywords": ["Deep learning", "Unsupervised Learning"], "conflicts": ["umontreal.ca", "polymtl.ca"], "authors": ["Florian Bordes", "Sina Honari", "Pascal Vincent"], "authorids": ["florian.bordes@umontreal.ca", "sina.honari@umontreal.ca", "pascal.vincent@umontreal.ca"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 19, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396693623, "tcdate": 1486396693623, "number": 1, "id": "HyCfpz8_g", "invitation": "ICLR.cc/2017/conference/-/paper589/acceptance", "forum": "BJAFbaolg", "replyto": "BJAFbaolg", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"title": "ICLR committee final decision", "comment": "Infusion training is a new, somewhat heuristic, procedure for training deep generative models. It's an interesting novel idea and a good paper, which has also been improved after the authors have been responsive to reviewer feedback.", "decision": "Accept (Poster)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Generate Samples from Noise through Infusion Training", "abstract": "In this work, we investigate a novel training procedure to learn a generative model as the transition operator of a Markov chain, such that, when applied repeatedly on an unstructured random noise sample, it will denoise it into a sample that matches the target distribution from the training set. The novel training procedure to learn this progressive denoising operation involves sampling from a slightly different chain than the model chain used for generation in the absence of a denoising target. In the training chain we infuse information from the training target example that we would like the chains to reach with a high probability. The thus learned transition operator is able to produce quality and varied samples in a small number of steps. Experiments show competitive results compared to the samples generated with a basic Generative Adversarial Net. ", "pdf": "/pdf/388d81ae0dd044220d7a880fbc2b8d904de2eedf.pdf", "TL;DR": "We learn a markov transition operator acting on inputspace, to denoise random noise into a target distribution. We use a novel target injection technique to guide the training.", "paperhash": "bordes|learning_to_generate_samples_from_noise_through_infusion_training", "keywords": ["Deep learning", "Unsupervised Learning"], "conflicts": ["umontreal.ca", "polymtl.ca"], "authors": ["Florian Bordes", "Sina Honari", "Pascal Vincent"], "authorids": ["florian.bordes@umontreal.ca", "sina.honari@umontreal.ca", "pascal.vincent@umontreal.ca"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396694126, "id": "ICLR.cc/2017/conference/-/paper589/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "BJAFbaolg", "replyto": "BJAFbaolg", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396694126}}}, {"tddate": null, "tmdate": 1485462713800, "tcdate": 1485462713800, "number": 8, "id": "HkGa3Cwvg", "invitation": "ICLR.cc/2017/conference/-/paper589/public/comment", "forum": "BJAFbaolg", "replyto": "HJc3yWePg", "signatures": ["~Pascal_Vincent1"], "readers": ["everyone"], "writers": ["~Pascal_Vincent1"], "content": {"title": "Further clarifications regarding data preprocessing", "comment": "The only preprocessing we did was to divide the integer pixel values 0...255 by 256 to obtain floating point numbers in range [0,1). Log likelihoods were evaluated in this space, which we believe is the same space/scale that others' works that we compare to had used. \n\nWe did not add uniform noise to the targets during training of our models, so in principle some degree of overfitting on the 8-bit quantization was possible (though expected to be mild as output Gaussians probably need to have a significantly larger width than this quantization to account for remaining uncertainty). To verify that, we did rerun the evaluation of our trained model when adding uniform noise in [0, 1/256] to our test data (that had been scaled to range [0,1) ).\n\nThis had only a small effect on the log-likelihood estimates: For the diagonal Gaussian observation model the estimates dropped by about 13 nats:\n1836 nats (instead of our previous 1848 nats), as estimated by IS, and 1350 instead of 1363 for the lower bound.\n\nWhen using the isotropic Gaussian observation model (with hyper-optimized fixed variance) the estimated log likelihood actually improved a little: from 409 to 413 nats.\n\nThank you for pointing this quantization issue out: we will make sure to add the uniform noise to guard againt any potential quantization overfitting also during training from now on.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Generate Samples from Noise through Infusion Training", "abstract": "In this work, we investigate a novel training procedure to learn a generative model as the transition operator of a Markov chain, such that, when applied repeatedly on an unstructured random noise sample, it will denoise it into a sample that matches the target distribution from the training set. The novel training procedure to learn this progressive denoising operation involves sampling from a slightly different chain than the model chain used for generation in the absence of a denoising target. In the training chain we infuse information from the training target example that we would like the chains to reach with a high probability. The thus learned transition operator is able to produce quality and varied samples in a small number of steps. Experiments show competitive results compared to the samples generated with a basic Generative Adversarial Net. ", "pdf": "/pdf/388d81ae0dd044220d7a880fbc2b8d904de2eedf.pdf", "TL;DR": "We learn a markov transition operator acting on inputspace, to denoise random noise into a target distribution. We use a novel target injection technique to guide the training.", "paperhash": "bordes|learning_to_generate_samples_from_noise_through_infusion_training", "keywords": ["Deep learning", "Unsupervised Learning"], "conflicts": ["umontreal.ca", "polymtl.ca"], "authors": ["Florian Bordes", "Sina Honari", "Pascal Vincent"], "authorids": ["florian.bordes@umontreal.ca", "sina.honari@umontreal.ca", "pascal.vincent@umontreal.ca"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287509389, "id": "ICLR.cc/2017/conference/-/paper589/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJAFbaolg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper589/reviewers", "ICLR.cc/2017/conference/paper589/areachairs"], "cdate": 1485287509389}}}, {"tddate": null, "tmdate": 1484947390866, "tcdate": 1481930427623, "number": 3, "id": "rkNaLgzVe", "invitation": "ICLR.cc/2017/conference/-/paper589/official/review", "forum": "BJAFbaolg", "replyto": "BJAFbaolg", "signatures": ["ICLR.cc/2017/conference/paper589/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper589/AnonReviewer1"], "content": {"title": "interesting idea", "rating": "8: Top 50% of accepted papers, clear accept", "review": "This paper trains a generative model which transforms noise into model samples by a gradual denoising process. It is similar to a generative model based on diffusion. Unlike the diffusion approach:\n- It uses only a small number of denoising steps, and is thus far more computationally efficient.\n- Rather than consisting of a reverse trajectory, the conditional chain for the approximate posterior jumps to q(z(0) | x), and then runs in the same direction as the generative model. This allows the inference chain to behave like a perturbation around the generative model, that pulls it towards the data. (This also seems somewhat related to ladder networks.)\n- There is no tractable variational bound on the log likelihood.\n\nI liked the idea, and found the visual sample quality given a short chain impressive. The inpainting results were particularly nice, since one shot inpainting is not possible under most generative modeling frameworks. It would be much more convincing to have a log likelihood comparison that doesn't depend on Parzen likelihoods.\n\nDetailed comments follow:\n\nSec. 2:\n\"theta(0) the\" -> \"theta(0) be the\"\n\"theta(t) the\" -> \"theta(t) be the\"\n\"what we will be using\" -> \"which we will be doing\"\nI like that you infer q(z^0|x), and then run inference in the same order as the generative chain. This reminds me slightly of ladder networks.\n\"q*. Having learned\" -> \"q*. [paragraph break] Having learned\"\nSec 3.3:\n\"learn to inverse\" -> \"learn to reverse\"\nSec. 4:\n\"For each experiments\" -> \"For each experiment\"\nHow sensitive are your results to infusion rate?\nSec. 5: \"appears to provide more accurate models\" I don't think you showed this -- there's no direct comparison to the Sohl-Dickstein paper.\nFig 4. -- neat!\n", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Generate Samples from Noise through Infusion Training", "abstract": "In this work, we investigate a novel training procedure to learn a generative model as the transition operator of a Markov chain, such that, when applied repeatedly on an unstructured random noise sample, it will denoise it into a sample that matches the target distribution from the training set. The novel training procedure to learn this progressive denoising operation involves sampling from a slightly different chain than the model chain used for generation in the absence of a denoising target. In the training chain we infuse information from the training target example that we would like the chains to reach with a high probability. The thus learned transition operator is able to produce quality and varied samples in a small number of steps. Experiments show competitive results compared to the samples generated with a basic Generative Adversarial Net. ", "pdf": "/pdf/388d81ae0dd044220d7a880fbc2b8d904de2eedf.pdf", "TL;DR": "We learn a markov transition operator acting on inputspace, to denoise random noise into a target distribution. We use a novel target injection technique to guide the training.", "paperhash": "bordes|learning_to_generate_samples_from_noise_through_infusion_training", "keywords": ["Deep learning", "Unsupervised Learning"], "conflicts": ["umontreal.ca", "polymtl.ca"], "authors": ["Florian Bordes", "Sina Honari", "Pascal Vincent"], "authorids": ["florian.bordes@umontreal.ca", "sina.honari@umontreal.ca", "pascal.vincent@umontreal.ca"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512532994, "id": "ICLR.cc/2017/conference/-/paper589/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper589/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper589/AnonReviewer3", "ICLR.cc/2017/conference/paper589/AnonReviewer4", "ICLR.cc/2017/conference/paper589/AnonReviewer1"], "reply": {"forum": "BJAFbaolg", "replyto": "BJAFbaolg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper589/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper589/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512532994}}}, {"tddate": null, "tmdate": 1484947377987, "tcdate": 1484947377987, "number": 5, "id": "HJc3yWePg", "invitation": "ICLR.cc/2017/conference/-/paper589/official/comment", "forum": "BJAFbaolg", "replyto": "ByPCSpkvx", "signatures": ["ICLR.cc/2017/conference/paper589/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper589/AnonReviewer1"], "content": {"title": "log likelihood lower bound", "comment": "Thanks, that addresses my concern. I have raised my score by one point.\n\nI still find the reported log likelihood bounds very unexpectedly high, and unexpectedly consistent between training and test sets. I am giving them the benefit of the doubt. As further basic sanity checks though, could you confirm:\n- Are you adding uniform noise in [0,1] to the pixel values? Otherwise, it is possible that the model is learning the 8 bit quantization of the training data. (This was a problem for the diffusion model paper -- see footnote in Section 3 of the Sohl-Dickstein et al, 2015 paper.)\n- If you are rescaling your data, eg to have variance 1, are you including a factor of log(scaling factor) per dimension in your log likelihood estimates?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Generate Samples from Noise through Infusion Training", "abstract": "In this work, we investigate a novel training procedure to learn a generative model as the transition operator of a Markov chain, such that, when applied repeatedly on an unstructured random noise sample, it will denoise it into a sample that matches the target distribution from the training set. The novel training procedure to learn this progressive denoising operation involves sampling from a slightly different chain than the model chain used for generation in the absence of a denoising target. In the training chain we infuse information from the training target example that we would like the chains to reach with a high probability. The thus learned transition operator is able to produce quality and varied samples in a small number of steps. Experiments show competitive results compared to the samples generated with a basic Generative Adversarial Net. ", "pdf": "/pdf/388d81ae0dd044220d7a880fbc2b8d904de2eedf.pdf", "TL;DR": "We learn a markov transition operator acting on inputspace, to denoise random noise into a target distribution. We use a novel target injection technique to guide the training.", "paperhash": "bordes|learning_to_generate_samples_from_noise_through_infusion_training", "keywords": ["Deep learning", "Unsupervised Learning"], "conflicts": ["umontreal.ca", "polymtl.ca"], "authors": ["Florian Bordes", "Sina Honari", "Pascal Vincent"], "authorids": ["florian.bordes@umontreal.ca", "sina.honari@umontreal.ca", "pascal.vincent@umontreal.ca"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287509264, "id": "ICLR.cc/2017/conference/-/paper589/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "BJAFbaolg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper589/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper589/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper589/reviewers", "ICLR.cc/2017/conference/paper589/areachairs"], "cdate": 1485287509264}}}, {"tddate": null, "tmdate": 1484946263148, "tcdate": 1481916287217, "number": 2, "id": "H1PYkpbEx", "invitation": "ICLR.cc/2017/conference/-/paper589/official/review", "forum": "BJAFbaolg", "replyto": "BJAFbaolg", "signatures": ["ICLR.cc/2017/conference/paper589/AnonReviewer4"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper589/AnonReviewer4"], "content": {"title": "Clearly written paper pursuing an interesting idea. Some shortcomings with respect to the evaluation and comparison to prior work", "rating": "7: Good paper, accept", "review": "The paper presents a method for training a generative model via an iterative denoising procedure. The denoising process is initialized with a random sample from a crude approximation to the data distribution and produces a high quality sample via multiple denoising steps. Training is performed by setting-up a Markov chain that slowly blends propositions from the current denoising model with a real example from the data distribution; using this chain the current denoising model is updated towards reproducing the changed, \"better\", samples from the blending process.\n\nThis is a clearly written paper that considers an interesting approach for training generative models. I was intrigued by the simplicity of the presented approach and really enjoyed reading the paper.\nThe proposed method is novel although it has clear ties to other recent work aiming to use denoising models for sampling from distributions such as the work by Sohl-Dickstein and the recent work on using DAEs as generative models.\nI think this general direction of research is important. The proposed procedure takes inspiration from the perspective of generating samples by minimizing an energy function via transitions along a Markov chain and, if successful, it can potentially sidestep many problems of current procedures for training directed generative models such as:\n- convergence and mode coverage problems as in generative adversarial networks\n- problems with modeling multi-modal distributions which can arise when a too restrictive approximate inference model is paired with a powerful generative model\n\nThat being said, another method that seems promising for addressing these issues that also has some superficially similarity to the presented work is the idea of combining Hamiltonian Monte Carlo inference with variational inference as in [1]. As such I am not entirely convinced that the method presented here will be able to perform better than the mentioned paper; although it might be simpler to train. Similarly, although I agree that using a MCMC chain to generate samples via a MC-EM like procedure is likely very costly I am not convinced such a procedure won't at least also work reasonably well for the simple MNIST example. In general a more direct comparison between different inference methods using an MCMC chain like procedure would be nice to have but I understand that this is perhaps out of the scope of this paper. One thing that I would have expected, however, is a direct comparison to the procedure from Sohl-Dickstein in terms of sampling steps and generation quality as it is so directly related.\n\nOther major points (good and bad):\n- Although in general the method is explained well some training details are missing. Most importantly it is never mentioned how alpha or omega are set (I am assuming omega is 0.01 as that is the increase mentioned in the experimental setup). It is also unclear how alpha affects the capabilities of the generator. While it intuitively seems reasonable to use a small alpha over many steps to ensure slow blending of the two distributions it is not clear how necessary this is or at what point the procedure would break (I assume alpha = 1 won't work as the generator then would have to magically denoise a sample from the relatively uninformative draw from p0 ?). The authors do mention in one of the figure captions that the denoising model does not produce good samples in only 1-2 steps but that might also be an artifact of training the model with small alpha (at least I see no a priori reason for this). More experiments should be carried out here.\n- No infusion chains or generating chains are shown for any of the more complicated data distributions, this is unfortunate as I feel these would be interesting to look at.\n- The paper does a good job at evaluating the model with respect to several different metrics. The bound on the log-likelihood is nice to have as well!\n- Unfortunately the current approach does not come with any theoretical guarantees. It is unclear for what choices of alpha the procedure will work and whether there is some deeper connection to MCMC sampling or energy based models. In my eyes this does not subtract from the value of the paper but would perhaps be worth a short sentence in the conclusion.\n\nMinor points:\n- The second reference seems broken\n- Figure 3 starts at 100 epochs and, as a result, contains little information. Perhaps it would be more useful to show the complete training procedure and put the x-axis on a log-scale ?\n- The explanation regarding the convolutional networks you use makes no sense to me. You write that you use the same structure as in the \"Improved GANs\" paper which, unlike your model, generates samples from a fixed length random input. I thus suppose you don't really use a generator with 1 fully connected network followed by up-convolutions but rather have several stages of convolutions followed by a fully connected layer and then up-convolutions ?\n- The choice of parametrizing the variance via a sigmoid output unit is somewhat unusual, was there a specific reason for this choice ?\n- footnote 1 contains errors: \"This allow to\" -> \"allows to\",  \"few informations\" -> \"little information\". \"This force the network\" -> \"forces\"\n- Page 1 error: etc...\n- Page 4 error: \"operator should to learn\"\n\n[1] Markov Chain Monte Carlo and Variational Inference: Bridging the Gap, Tim Salimans and Diedrik P. Kingma and Max Welling, ICML 2015\n\n\n>>> Update <<<<\nCopied here from my response below: \n\nI believe the response of the authors clarifies all open issues. I strongly believe the paper should be accepted to the conference. The only remaining issue I have with the paper is that, as the authors acknowledge the architecture of the generator is likely highly sub-optimal and might hamper the performance of the method in the evaluation. This however does not at all subtract from any of the main points of the paper.\n\nI am thus keeping my score as a clear accept. I want to emphasize that I believe the paper should be published (just in case the review process results in some form of cut-off threshold that is high due to overall \"inflated\" review scores).\n", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Generate Samples from Noise through Infusion Training", "abstract": "In this work, we investigate a novel training procedure to learn a generative model as the transition operator of a Markov chain, such that, when applied repeatedly on an unstructured random noise sample, it will denoise it into a sample that matches the target distribution from the training set. The novel training procedure to learn this progressive denoising operation involves sampling from a slightly different chain than the model chain used for generation in the absence of a denoising target. In the training chain we infuse information from the training target example that we would like the chains to reach with a high probability. The thus learned transition operator is able to produce quality and varied samples in a small number of steps. Experiments show competitive results compared to the samples generated with a basic Generative Adversarial Net. ", "pdf": "/pdf/388d81ae0dd044220d7a880fbc2b8d904de2eedf.pdf", "TL;DR": "We learn a markov transition operator acting on inputspace, to denoise random noise into a target distribution. We use a novel target injection technique to guide the training.", "paperhash": "bordes|learning_to_generate_samples_from_noise_through_infusion_training", "keywords": ["Deep learning", "Unsupervised Learning"], "conflicts": ["umontreal.ca", "polymtl.ca"], "authors": ["Florian Bordes", "Sina Honari", "Pascal Vincent"], "authorids": ["florian.bordes@umontreal.ca", "sina.honari@umontreal.ca", "pascal.vincent@umontreal.ca"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512532994, "id": "ICLR.cc/2017/conference/-/paper589/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper589/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper589/AnonReviewer3", "ICLR.cc/2017/conference/paper589/AnonReviewer4", "ICLR.cc/2017/conference/paper589/AnonReviewer1"], "reply": {"forum": "BJAFbaolg", "replyto": "BJAFbaolg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper589/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper589/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512532994}}}, {"tddate": null, "tmdate": 1484946172498, "tcdate": 1484946172498, "number": 4, "id": "SyEZoeewx", "invitation": "ICLR.cc/2017/conference/-/paper589/official/comment", "forum": "BJAFbaolg", "replyto": "Bk093Zv8x", "signatures": ["ICLR.cc/2017/conference/paper589/AnonReviewer4"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper589/AnonReviewer4"], "content": {"title": "Response to rebuttal", "comment": "I believe the response of the authors clarifies all open issues. I strongly believe the paper should be accepted to the conference. The only remaining issue I have with the paper is that, as the authors acknowledge the architecture of the generator is likely highly sub-optimal and might hamper the performance of the method in the evaluation. This however does not at all subtract from any of the main points of the paper.\n\nI am thus keeping my score as a clear accept. I want to emphasize that I believe the paper should be published (just in case the review process results in some form of cut-off threshold that is high due to overall \"inflated\" review scores).\n\nRegarding your response:\nThanks, all responses make sense to me. \n\n- As described above the architecture for the generator probably makes your model look worse than it should :). I would encourage you to try different choices in future work. Perhaps a good starting point would be to take inspiration from Salimans recent work on VAEs, simply using a PixelCNN like structure, or starting from an autoencoder as in recent work by Dosovitskiy.\n\n- Regarding the sigmoid output: fair enough, as you wrote I don't think it matters much. Using a softplus or parametrizing the log of the variance would just have been more natural choices (that is they would have been more in-tune with what people typically do).\n\n- I really like the infusion chains for the other datasets. It looks like your method is promising in case you can combine it with some of the more recent advances for generative models.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Generate Samples from Noise through Infusion Training", "abstract": "In this work, we investigate a novel training procedure to learn a generative model as the transition operator of a Markov chain, such that, when applied repeatedly on an unstructured random noise sample, it will denoise it into a sample that matches the target distribution from the training set. The novel training procedure to learn this progressive denoising operation involves sampling from a slightly different chain than the model chain used for generation in the absence of a denoising target. In the training chain we infuse information from the training target example that we would like the chains to reach with a high probability. The thus learned transition operator is able to produce quality and varied samples in a small number of steps. Experiments show competitive results compared to the samples generated with a basic Generative Adversarial Net. ", "pdf": "/pdf/388d81ae0dd044220d7a880fbc2b8d904de2eedf.pdf", "TL;DR": "We learn a markov transition operator acting on inputspace, to denoise random noise into a target distribution. We use a novel target injection technique to guide the training.", "paperhash": "bordes|learning_to_generate_samples_from_noise_through_infusion_training", "keywords": ["Deep learning", "Unsupervised Learning"], "conflicts": ["umontreal.ca", "polymtl.ca"], "authors": ["Florian Bordes", "Sina Honari", "Pascal Vincent"], "authorids": ["florian.bordes@umontreal.ca", "sina.honari@umontreal.ca", "pascal.vincent@umontreal.ca"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287509264, "id": "ICLR.cc/2017/conference/-/paper589/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "BJAFbaolg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper589/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper589/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper589/reviewers", "ICLR.cc/2017/conference/paper589/areachairs"], "cdate": 1485287509264}}}, {"tddate": null, "tmdate": 1484932559149, "tcdate": 1484932559149, "number": 7, "id": "ByPCSpkvx", "invitation": "ICLR.cc/2017/conference/-/paper589/public/comment", "forum": "BJAFbaolg", "replyto": "HJq7kV1wx", "signatures": ["~Pascal_Vincent1"], "readers": ["everyone"], "writers": ["~Pascal_Vincent1"], "content": {"title": "Response to reviewer ", "comment": "In the text introducing q, we define delta_xi as \"a highly concentrated unimodal distribution around x_i such as a Dirac-delta, or a Gaussian with small variance\".\nWe did not actually use a delta function, but a Gaussian with a small variance (treated as a hyperpatameter adjusted using the validation set; in practice it was around 10^-4). The choice of delta notation can indeed be confusing in this context. We will clarify this in the paper. Thanks!"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Generate Samples from Noise through Infusion Training", "abstract": "In this work, we investigate a novel training procedure to learn a generative model as the transition operator of a Markov chain, such that, when applied repeatedly on an unstructured random noise sample, it will denoise it into a sample that matches the target distribution from the training set. The novel training procedure to learn this progressive denoising operation involves sampling from a slightly different chain than the model chain used for generation in the absence of a denoising target. In the training chain we infuse information from the training target example that we would like the chains to reach with a high probability. The thus learned transition operator is able to produce quality and varied samples in a small number of steps. Experiments show competitive results compared to the samples generated with a basic Generative Adversarial Net. ", "pdf": "/pdf/388d81ae0dd044220d7a880fbc2b8d904de2eedf.pdf", "TL;DR": "We learn a markov transition operator acting on inputspace, to denoise random noise into a target distribution. We use a novel target injection technique to guide the training.", "paperhash": "bordes|learning_to_generate_samples_from_noise_through_infusion_training", "keywords": ["Deep learning", "Unsupervised Learning"], "conflicts": ["umontreal.ca", "polymtl.ca"], "authors": ["Florian Bordes", "Sina Honari", "Pascal Vincent"], "authorids": ["florian.bordes@umontreal.ca", "sina.honari@umontreal.ca", "pascal.vincent@umontreal.ca"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287509389, "id": "ICLR.cc/2017/conference/-/paper589/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJAFbaolg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper589/reviewers", "ICLR.cc/2017/conference/paper589/areachairs"], "cdate": 1485287509389}}}, {"tddate": null, "tmdate": 1484893986448, "tcdate": 1484893986448, "number": 3, "id": "HJq7kV1wx", "invitation": "ICLR.cc/2017/conference/-/paper589/official/comment", "forum": "BJAFbaolg", "replyto": "HymvcWvLg", "signatures": ["ICLR.cc/2017/conference/paper589/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper589/AnonReviewer1"], "content": {"title": "log likelihood lower bound?", "comment": "I appreciate the inclusion of better and more extensive quantitative metrics. I am open to raising my score one point as a result, but I have a point of concern/confusion. I wonder if you could clarify how you are computing your log likelihood lower bound.\n\nIn Equation 3 you define a lower bound that includes the term log[q(z-tilde | x )]. In Section 2.3.1 you define q(z-tilde | x) such that it includes a mixture of delta functions. So, Equation 3 involves taking the expectation of the log of a delta function. Depending on how you define the delta function, this corresponds to a lower bound of -infinity on the log likelihood (differential entropy of a delta function is -infinity). How are you using this bound to produce reasonable log likelihood values?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Generate Samples from Noise through Infusion Training", "abstract": "In this work, we investigate a novel training procedure to learn a generative model as the transition operator of a Markov chain, such that, when applied repeatedly on an unstructured random noise sample, it will denoise it into a sample that matches the target distribution from the training set. The novel training procedure to learn this progressive denoising operation involves sampling from a slightly different chain than the model chain used for generation in the absence of a denoising target. In the training chain we infuse information from the training target example that we would like the chains to reach with a high probability. The thus learned transition operator is able to produce quality and varied samples in a small number of steps. Experiments show competitive results compared to the samples generated with a basic Generative Adversarial Net. ", "pdf": "/pdf/388d81ae0dd044220d7a880fbc2b8d904de2eedf.pdf", "TL;DR": "We learn a markov transition operator acting on inputspace, to denoise random noise into a target distribution. We use a novel target injection technique to guide the training.", "paperhash": "bordes|learning_to_generate_samples_from_noise_through_infusion_training", "keywords": ["Deep learning", "Unsupervised Learning"], "conflicts": ["umontreal.ca", "polymtl.ca"], "authors": ["Florian Bordes", "Sina Honari", "Pascal Vincent"], "authorids": ["florian.bordes@umontreal.ca", "sina.honari@umontreal.ca", "pascal.vincent@umontreal.ca"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287509264, "id": "ICLR.cc/2017/conference/-/paper589/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "BJAFbaolg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper589/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper589/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper589/reviewers", "ICLR.cc/2017/conference/paper589/areachairs"], "cdate": 1485287509264}}}, {"tddate": null, "tmdate": 1484361199273, "tcdate": 1484361199273, "number": 6, "id": "HJwx0WvIe", "invitation": "ICLR.cc/2017/conference/-/paper589/public/comment", "forum": "BJAFbaolg", "replyto": "B1IXim1Ex", "signatures": ["~Pascal_Vincent1"], "readers": ["everyone"], "writers": ["~Pascal_Vincent1"], "content": {"title": "Authors response", "comment": "Thank you for your careful, well informed, and very useful review.  \n\nRegarding log-likelihood estimates, to improve upon the limitations of the Parzen estimator we had for our first revision implemented a lower-bound estimator and used it for model evaluation. Although we initially feared it might suffer from too high variance, following your suggestion, we have since implemented an importance sampling based estimator. And it was able to measure significantly higher log likelihood, thank you for the suggestion! Future work will include a proper AIS estimator for our model.\n\n>> \u201cHowever, the authors should point out that the numbers taken from Wu et al. (2016) are not representative of the performance of a VAE. (From the paper: \u201cTherefore, the log-likelihood values we report should not be compared directly against networks which have a more flexible observation model.\u201d \u201cSuch observation models can easily achieve much higher log-likelihood scores, [\u2026].\u201d) \u201c\n\nThank you for pointing this out! We updated the results table with the performance obtained when using an isotropic gaussian output (which naturally does yields poorer log-likelihood) to be more comparable with Wu et al. (2016). We clarify this issue in the caption of the table.\n\nRegarding the work of Rezende & Mohamed, 2015 it is indeed a very interesting and much related work. Thank you for pointing it out. We now reference it in the paper, and mention in the conclusion that we will perform empirical comparisons with it as part of our future work. \nWhile the variational approach of Rezende & Mohamed indeed stands on more solid theoretical ground, the statistical/computational performance tradeoffs need to be assessed empirically. Also a situation where our more heuristic approach could prove interesting: it can straightforwardly be used in discretes spaces as well (infusion using mixtures of multinomials, which we did in some of our early unreported experiments), while extension of Rezende & Mohamed, 2015 to discrete spaces does not look as obvious. \n\nWe also corrected the typos and added the missing reference. Thank you very much!"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Generate Samples from Noise through Infusion Training", "abstract": "In this work, we investigate a novel training procedure to learn a generative model as the transition operator of a Markov chain, such that, when applied repeatedly on an unstructured random noise sample, it will denoise it into a sample that matches the target distribution from the training set. The novel training procedure to learn this progressive denoising operation involves sampling from a slightly different chain than the model chain used for generation in the absence of a denoising target. In the training chain we infuse information from the training target example that we would like the chains to reach with a high probability. The thus learned transition operator is able to produce quality and varied samples in a small number of steps. Experiments show competitive results compared to the samples generated with a basic Generative Adversarial Net. ", "pdf": "/pdf/388d81ae0dd044220d7a880fbc2b8d904de2eedf.pdf", "TL;DR": "We learn a markov transition operator acting on inputspace, to denoise random noise into a target distribution. We use a novel target injection technique to guide the training.", "paperhash": "bordes|learning_to_generate_samples_from_noise_through_infusion_training", "keywords": ["Deep learning", "Unsupervised Learning"], "conflicts": ["umontreal.ca", "polymtl.ca"], "authors": ["Florian Bordes", "Sina Honari", "Pascal Vincent"], "authorids": ["florian.bordes@umontreal.ca", "sina.honari@umontreal.ca", "pascal.vincent@umontreal.ca"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287509389, "id": "ICLR.cc/2017/conference/-/paper589/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJAFbaolg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper589/reviewers", "ICLR.cc/2017/conference/paper589/areachairs"], "cdate": 1485287509389}}}, {"tddate": null, "tmdate": 1484360915224, "tcdate": 1484360853666, "number": 5, "id": "Bk093Zv8x", "invitation": "ICLR.cc/2017/conference/-/paper589/public/comment", "forum": "BJAFbaolg", "replyto": "H1PYkpbEx", "signatures": ["~Pascal_Vincent1"], "readers": ["everyone"], "writers": ["~Pascal_Vincent1"], "content": {"title": "Authors response", "comment": "Thanks for your careful and very useful review. We corrected the typos that you mentioned. We address your comments below:\n\n>> \"another method that seems promising for addressing these issues that also has some superficially similarity to the presented work is the idea of combining Hamiltonian Monte Carlo inference with variational inference as in [1]. As such I am not entirely convinced that the method presented here will be able to perform better than the mentioned paper; although it might be simpler to train. Similarly, although I agree that using a MCMC chain to generate samples via a MC-EM like procedure is likely very costly I am not convinced such a procedure won't at least also work reasonably well for the simple MNIST example. In general a more direct comparison between different inference methods using an MCMC chain like procedure would be nice to have but I understand that this is perhaps out of the scope of this paper.\" \n\nThank you for pointing out the connection with the very interesting work of Salimans, Kingma & Welling, which we now include in our related works section. It will indeed be interesting to compare our more heuristic infusion approach with this method. We leave this as future work. \n\n>> \u201cOne thing that I would have expected, however, is a direct comparison to the procedure from Sohl-Dickstein in terms of sampling steps and generation quality as it is so directly related.\u201d \n\nNote that on MNIST, the log-likelihood estimation (Parzen-based) that they report for their diffusion model (317 bits = 220 nats) is worse than what we obtain with our model (312 nats estimated with Parzen; 409 nats with the importance-sampling estimator). And our samples look arguably better.\nIt obviously was (and still is) our plan to do a more extensive side-by-side comparison. But to be able to run experiments, starting from their code proved challenging, so we instead chose to prioritize developing and implementing proper log-likelihood estimation for our method to enable more trustworthy comparisons in the future before reimplementing their method in our framework. \n\n\n>> \u201cUnfortunately the current approach does not come with any theoretical guarantees. [...] whether there is some deeper connection to MCMC sampling or energy based models. In my eyes this does not subtract from the value of the paper but would perhaps be worth a short sentence in the conclusion.\u201d\n\nWe added the following sentence to our conclusion:\n\u201cWhile we empirically verified that the proposed infusion training procedure does result in increasing log-likielihood estimates, it remains a heuristic surrogate for the intractable likelihood, one for which we did not here derive any theoretical guarantee. Future work shall attempt to do so, or at least to clarify the relationship and quantify the compromises achieved with respect to other Markov Chain methods including Sohl-Dickstein et al. 2015 and  Salimans et al. 2015.\u201d\n\n\n>> \u201dAlthough in general the method is explained well some training details are missing. Most importantly it is never mentioned how alpha or omega are set.[...]\u201d\n\nWe added more details in the appendix, regarding how to set the infusion rate, together with experimental curves. \n\n>> \u201d No infusion chains or generating chains are shown for any of the more complicated data distributions, this is unfortunate as I feel these would be interesting to look at.\u201d\n\nWe added chains on CIFAR10 and CelebA in the Appendix of the paper.\n\n>> \u201dThe explanation regarding the convolutional networks you use makes no sense to me. You write that you use the same structure as in the \"Improved GANs\" paper which, unlike your model, generates samples from a fixed length random input. I thus suppose you don't really use a generator with 1 fully connected network followed by up-convolutions but rather have several stages of convolutions followed by a fully connected layer and then up-convolutions ?\u201d\n\n\nWe really did use a generator with 1 fully connected network followed by up-convolutions. The main difference with the structure of  \"Improved GANs\" paper is the input size (We use a matrix of the image size instead of a fixed length random vector).  We agree that this kind of architecture is probably suboptimal from the auto-encoder point of view. We didn\u2019t devote much energy in this work on optimizing the architecture, as our main focus was the new training method. We think there is place for a lot of architectural improvement (convolutional, resnet, ...) we leave this as future work.\n\n>> \u201cThe choice of parametrizing the variance via a sigmoid output unit is somewhat unusual, was there a specific reason for this choice ?\u201d \n\nWe just thought it made sense from a stability perspective to have the variance in a finite range that we could control. We didn\u2019t try anything else, but other choices are certainly possible and would likely work just as well.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Generate Samples from Noise through Infusion Training", "abstract": "In this work, we investigate a novel training procedure to learn a generative model as the transition operator of a Markov chain, such that, when applied repeatedly on an unstructured random noise sample, it will denoise it into a sample that matches the target distribution from the training set. The novel training procedure to learn this progressive denoising operation involves sampling from a slightly different chain than the model chain used for generation in the absence of a denoising target. In the training chain we infuse information from the training target example that we would like the chains to reach with a high probability. The thus learned transition operator is able to produce quality and varied samples in a small number of steps. Experiments show competitive results compared to the samples generated with a basic Generative Adversarial Net. ", "pdf": "/pdf/388d81ae0dd044220d7a880fbc2b8d904de2eedf.pdf", "TL;DR": "We learn a markov transition operator acting on inputspace, to denoise random noise into a target distribution. We use a novel target injection technique to guide the training.", "paperhash": "bordes|learning_to_generate_samples_from_noise_through_infusion_training", "keywords": ["Deep learning", "Unsupervised Learning"], "conflicts": ["umontreal.ca", "polymtl.ca"], "authors": ["Florian Bordes", "Sina Honari", "Pascal Vincent"], "authorids": ["florian.bordes@umontreal.ca", "sina.honari@umontreal.ca", "pascal.vincent@umontreal.ca"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287509389, "id": "ICLR.cc/2017/conference/-/paper589/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJAFbaolg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper589/reviewers", "ICLR.cc/2017/conference/paper589/areachairs"], "cdate": 1485287509389}}}, {"tddate": null, "tmdate": 1484360282854, "tcdate": 1484360282854, "number": 4, "id": "HymvcWvLg", "invitation": "ICLR.cc/2017/conference/-/paper589/public/comment", "forum": "BJAFbaolg", "replyto": "rkNaLgzVe", "signatures": ["~Pascal_Vincent1"], "readers": ["everyone"], "writers": ["~Pascal_Vincent1"], "content": {"title": "Authors response", "comment": "Thanks for your careful review, and useful suggestions.\n\n>> \u201cIt would be much more convincing to have a log likelihood comparison that doesn't depend on Parzen likelihoods.\u201d\n\n\nWe updated our paper with a lower bound estimate of the log-likelihood as well as an Importance Sampling estimate.\n\n>>  \u201cHow sensitive are your results to infusion rate?\u201d \n\nIn the last revision, we add an appendix showing the impact of the infusion rate on the lower bound and in term of sampling quality.\n\nWe also corrected the typos that you mentioned. Thank you!"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Generate Samples from Noise through Infusion Training", "abstract": "In this work, we investigate a novel training procedure to learn a generative model as the transition operator of a Markov chain, such that, when applied repeatedly on an unstructured random noise sample, it will denoise it into a sample that matches the target distribution from the training set. The novel training procedure to learn this progressive denoising operation involves sampling from a slightly different chain than the model chain used for generation in the absence of a denoising target. In the training chain we infuse information from the training target example that we would like the chains to reach with a high probability. The thus learned transition operator is able to produce quality and varied samples in a small number of steps. Experiments show competitive results compared to the samples generated with a basic Generative Adversarial Net. ", "pdf": "/pdf/388d81ae0dd044220d7a880fbc2b8d904de2eedf.pdf", "TL;DR": "We learn a markov transition operator acting on inputspace, to denoise random noise into a target distribution. We use a novel target injection technique to guide the training.", "paperhash": "bordes|learning_to_generate_samples_from_noise_through_infusion_training", "keywords": ["Deep learning", "Unsupervised Learning"], "conflicts": ["umontreal.ca", "polymtl.ca"], "authors": ["Florian Bordes", "Sina Honari", "Pascal Vincent"], "authorids": ["florian.bordes@umontreal.ca", "sina.honari@umontreal.ca", "pascal.vincent@umontreal.ca"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287509389, "id": "ICLR.cc/2017/conference/-/paper589/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJAFbaolg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper589/reviewers", "ICLR.cc/2017/conference/paper589/areachairs"], "cdate": 1485287509389}}}, {"tddate": null, "tmdate": 1484360044678, "tcdate": 1484360044678, "number": 3, "id": "rkHuF-w8x", "invitation": "ICLR.cc/2017/conference/-/paper589/public/comment", "forum": "BJAFbaolg", "replyto": "BJAFbaolg", "signatures": ["~Pascal_Vincent1"], "readers": ["everyone"], "writers": ["~Pascal_Vincent1"], "content": {"title": "Change-log", "comment": "We updated the paper, the main changes are:\n\nAdded better log-likelihood estimates (one stochastic lower bound and one based on importance sampling)\n\nAdded curves showing that log-likelihood bound improves as infusion training progresses \n\nAdded references to related and relevant works: Rezende & Mohamed, 2015;  Salimans et al. 2015; Dinh et al. 2014; Wu et al. 2016.\n\nAdded results tables for likelihood comparison with models from the literature (Parzen estimates by Sohl-Dickstein et al. 2015 and AIS estimates by Wu et al. 2016)\n\nAdded further experimental details.\n\nAdded an Appendix containing details regarding the infusion rate schedule as well as examples of infused and sampled chains on cifar10 and celebA\n\nCorrected the typos mentioned by the reviewers\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Generate Samples from Noise through Infusion Training", "abstract": "In this work, we investigate a novel training procedure to learn a generative model as the transition operator of a Markov chain, such that, when applied repeatedly on an unstructured random noise sample, it will denoise it into a sample that matches the target distribution from the training set. The novel training procedure to learn this progressive denoising operation involves sampling from a slightly different chain than the model chain used for generation in the absence of a denoising target. In the training chain we infuse information from the training target example that we would like the chains to reach with a high probability. The thus learned transition operator is able to produce quality and varied samples in a small number of steps. Experiments show competitive results compared to the samples generated with a basic Generative Adversarial Net. ", "pdf": "/pdf/388d81ae0dd044220d7a880fbc2b8d904de2eedf.pdf", "TL;DR": "We learn a markov transition operator acting on inputspace, to denoise random noise into a target distribution. We use a novel target injection technique to guide the training.", "paperhash": "bordes|learning_to_generate_samples_from_noise_through_infusion_training", "keywords": ["Deep learning", "Unsupervised Learning"], "conflicts": ["umontreal.ca", "polymtl.ca"], "authors": ["Florian Bordes", "Sina Honari", "Pascal Vincent"], "authorids": ["florian.bordes@umontreal.ca", "sina.honari@umontreal.ca", "pascal.vincent@umontreal.ca"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287509389, "id": "ICLR.cc/2017/conference/-/paper589/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJAFbaolg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper589/reviewers", "ICLR.cc/2017/conference/paper589/areachairs"], "cdate": 1485287509389}}}, {"tddate": null, "tmdate": 1481916471755, "tcdate": 1481916471755, "number": 3, "id": "rJxSxpZNl", "invitation": "ICLR.cc/2017/conference/-/paper589/pre-review/question", "forum": "BJAFbaolg", "replyto": "BJAFbaolg", "signatures": ["ICLR.cc/2017/conference/paper589/AnonReviewer4"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper589/AnonReviewer4"], "content": {"title": "-", "question": "The reviewer apologizes for not entering a pre-review question in time. I was under the impression that this is an optional step for clearing up initially discovered flaws. Please see my review for my main questions/points of criticism. I will gladly respond to any answers during the review period."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Generate Samples from Noise through Infusion Training", "abstract": "In this work, we investigate a novel training procedure to learn a generative model as the transition operator of a Markov chain, such that, when applied repeatedly on an unstructured random noise sample, it will denoise it into a sample that matches the target distribution from the training set. The novel training procedure to learn this progressive denoising operation involves sampling from a slightly different chain than the model chain used for generation in the absence of a denoising target. In the training chain we infuse information from the training target example that we would like the chains to reach with a high probability. The thus learned transition operator is able to produce quality and varied samples in a small number of steps. Experiments show competitive results compared to the samples generated with a basic Generative Adversarial Net. ", "pdf": "/pdf/388d81ae0dd044220d7a880fbc2b8d904de2eedf.pdf", "TL;DR": "We learn a markov transition operator acting on inputspace, to denoise random noise into a target distribution. We use a novel target injection technique to guide the training.", "paperhash": "bordes|learning_to_generate_samples_from_noise_through_infusion_training", "keywords": ["Deep learning", "Unsupervised Learning"], "conflicts": ["umontreal.ca", "polymtl.ca"], "authors": ["Florian Bordes", "Sina Honari", "Pascal Vincent"], "authorids": ["florian.bordes@umontreal.ca", "sina.honari@umontreal.ca", "pascal.vincent@umontreal.ca"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481916472198, "id": "ICLR.cc/2017/conference/-/paper589/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper589/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper589/AnonReviewer1", "ICLR.cc/2017/conference/paper589/AnonReviewer3", "ICLR.cc/2017/conference/paper589/AnonReviewer4"], "reply": {"forum": "BJAFbaolg", "replyto": "BJAFbaolg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper589/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper589/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481916472198}}}, {"tddate": null, "tmdate": 1481747230051, "tcdate": 1481747230043, "number": 1, "id": "B1IXim1Ex", "invitation": "ICLR.cc/2017/conference/-/paper589/official/review", "forum": "BJAFbaolg", "replyto": "BJAFbaolg", "signatures": ["ICLR.cc/2017/conference/paper589/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper589/AnonReviewer3"], "content": {"title": "Interesting idea with lacking theoretical motivation and limited empirical evaluation", "rating": "6: Marginally above acceptance threshold", "review": "Summary:\nThis paper introduces a heuristic approach for training a deep directed generative model, where similar to the transition operator of a Markov chain each layer samples from the same conditional distribution. Similar to optimizing a variational lower bound, the approach is to approximate the gradient by replacing the posterior over latents with an alternative distribution. However, the approximating distribution is not updated to improve the lower bound but heuristically constructed in each step. A further difference to variational optimization is that the conditional distributions are optimized greedily rather than following the gradient of the joint log-likelihood.\n\nReview:\nThe proposed approach is interesting and to me seems worth exploring more. Given that there are approaches for training the same class of models which are 1) theoretically more sound, 2) of similar computational complexity, and 3) work well in practice (e.g. Rezende & Mohamed, 2015), I am nevertheless not sure of its potential to generate impact. My bigger concern, however, is that the empirical evaluation is still quite limited.\n\nI appreciate the authors included proper estimates of the log-likelihood. This will enable and encourage future comparisons with this method on continuous MNIST. However, the authors should point out that the numbers taken from Wu et al. (2016) are not representative of the performance of a VAE. (From the paper: \u201cTherefore, the log-likelihood values we report should not be compared directly against networks which have a more flexible observation model.\u201d \u201cSuch observation models can easily achieve much higher log-likelihood scores, [\u2026].\u201d)\n\nComparisons with inpainting results using other methods would have been nice. How practical is the proposed approach compared to other approaches? Similar to the diffusion approach by Sohl-Dickstein et al. (2015), the proposed approach seems to be both efficient and effective for inpainting. Not making this a bigger point and performing the proper evaluations seems like a missed opportunity.\n\nMinor:\n\u2013\u00a0I am missing citations for \u201cordered visible dimension sampling\u201d\n\u2013\u00a0Typos and frequent incorrect use of \\citet and \\citep", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Generate Samples from Noise through Infusion Training", "abstract": "In this work, we investigate a novel training procedure to learn a generative model as the transition operator of a Markov chain, such that, when applied repeatedly on an unstructured random noise sample, it will denoise it into a sample that matches the target distribution from the training set. The novel training procedure to learn this progressive denoising operation involves sampling from a slightly different chain than the model chain used for generation in the absence of a denoising target. In the training chain we infuse information from the training target example that we would like the chains to reach with a high probability. The thus learned transition operator is able to produce quality and varied samples in a small number of steps. Experiments show competitive results compared to the samples generated with a basic Generative Adversarial Net. ", "pdf": "/pdf/388d81ae0dd044220d7a880fbc2b8d904de2eedf.pdf", "TL;DR": "We learn a markov transition operator acting on inputspace, to denoise random noise into a target distribution. We use a novel target injection technique to guide the training.", "paperhash": "bordes|learning_to_generate_samples_from_noise_through_infusion_training", "keywords": ["Deep learning", "Unsupervised Learning"], "conflicts": ["umontreal.ca", "polymtl.ca"], "authors": ["Florian Bordes", "Sina Honari", "Pascal Vincent"], "authorids": ["florian.bordes@umontreal.ca", "sina.honari@umontreal.ca", "pascal.vincent@umontreal.ca"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512532994, "id": "ICLR.cc/2017/conference/-/paper589/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper589/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper589/AnonReviewer3", "ICLR.cc/2017/conference/paper589/AnonReviewer4", "ICLR.cc/2017/conference/paper589/AnonReviewer1"], "reply": {"forum": "BJAFbaolg", "replyto": "BJAFbaolg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper589/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper589/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512532994}}}, {"tddate": null, "tmdate": 1481744445393, "tcdate": 1481744445384, "number": 2, "id": "ryrHxmyNg", "invitation": "ICLR.cc/2017/conference/-/paper589/official/comment", "forum": "BJAFbaolg", "replyto": "BkjtN3EXg", "signatures": ["ICLR.cc/2017/conference/paper589/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper589/AnonReviewer3"], "content": {"title": "Importance sampling", "comment": "Note that I asked about importance sampling, not AIS. Importance sampling would have been as easy to implement as the lower bound the authors have evaluated now.\n\nThat said, I don't think AIS would have been too much ask either. The paper mentioned by the authors performs a fairly straight-forward application of AIS and HMC. (This is not to say that I don\u2019t value the empirical contributions in that paper, to the contrary.)\n\nAs the authors know, Parzen window estimates and appearance of samples are not reliable. The estimate for \u201cinfusion training\u201d is much better than that of true MNIST samples (243 nats, see Theis et al., 2016), so how should we interpret these results? The table gives the misleading impression of much improved density estimation performance and that a proper quantitative comparison has been performed.\n\nI appreciate that the authors will include better estimates."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Generate Samples from Noise through Infusion Training", "abstract": "In this work, we investigate a novel training procedure to learn a generative model as the transition operator of a Markov chain, such that, when applied repeatedly on an unstructured random noise sample, it will denoise it into a sample that matches the target distribution from the training set. The novel training procedure to learn this progressive denoising operation involves sampling from a slightly different chain than the model chain used for generation in the absence of a denoising target. In the training chain we infuse information from the training target example that we would like the chains to reach with a high probability. The thus learned transition operator is able to produce quality and varied samples in a small number of steps. Experiments show competitive results compared to the samples generated with a basic Generative Adversarial Net. ", "pdf": "/pdf/388d81ae0dd044220d7a880fbc2b8d904de2eedf.pdf", "TL;DR": "We learn a markov transition operator acting on inputspace, to denoise random noise into a target distribution. We use a novel target injection technique to guide the training.", "paperhash": "bordes|learning_to_generate_samples_from_noise_through_infusion_training", "keywords": ["Deep learning", "Unsupervised Learning"], "conflicts": ["umontreal.ca", "polymtl.ca"], "authors": ["Florian Bordes", "Sina Honari", "Pascal Vincent"], "authorids": ["florian.bordes@umontreal.ca", "sina.honari@umontreal.ca", "pascal.vincent@umontreal.ca"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287509264, "id": "ICLR.cc/2017/conference/-/paper589/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "BJAFbaolg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper589/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper589/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper589/reviewers", "ICLR.cc/2017/conference/paper589/areachairs"], "cdate": 1485287509264}}}, {"tddate": null, "tmdate": 1481061507198, "tcdate": 1481061507192, "number": 2, "id": "BkjtN3EXg", "invitation": "ICLR.cc/2017/conference/-/paper589/public/comment", "forum": "BJAFbaolg", "replyto": "H1m81EJXl", "signatures": ["~Pascal_Vincent1"], "readers": ["everyone"], "writers": ["~Pascal_Vincent1"], "content": {"title": "Reply to evaluation of log-likelihood", "comment": "This statement was meant merely as an informal motivation and explanation for our training method; our ultimate goal is to develop a novel approach to learn a good generative model. The numerical Parzen estimator results we provide in Table 1 are (albeit imprecise) lower bounds on the average *test* log-likelihood, which measures the ability to generalize to new data. Despite its limitations, it allowed for comparison with previously published results that used the same estimation method (such as the initial GAN paper). Trainset log likelihood is typically easy to overfit, which is not the goal, and did not lend itself to comparison with other works. In addition to the numerical results of the Parzen estimators on test data, we take the subjective quality of generated samples as visible evidence that our training algorithm was able to learn adequately from the training set. \n\n\nThis being said, better alternatives to estimate the log-likelihood are clearly possible. Since our initial submission we implemented a more precise and tighter log-likelihood lower bound estimation method (detailed below). It yields an average test log-likelihood around 920 nats on MNIST  for our model (while the Parzen estimation method reported 314 nats). A training curve of this lower bound monitored as infusion training progresses can be found here (for now on google drive) \nhttps://drive.google.com/file/d/0B7qF0fRANBqSdnNJZTN3Zkx4OFE/view?usp=sharing. \nWe are updating the paper with these novel results.\nThis result is comparable to the very recent Annealed Importance Sampling results reported in [1]: around 600 nats for a GAN model and around 1000 for a VAE model. \n\n\nWe want to point out that coming up with a good estimator of the log likelihood of such high dimensional sampling models is a far from trivial and still an active area of research. Appropriate forms of Annealed Importance Sampling, likely to yield more accurate estimates, have only been advocated very recently for such models in [1], another parallel submission to ICLR. And while we are currently working out the details of how to implement it for our approach, we cannot reasonably be expected to provide results with a method that has simultaneously been proposed in another ICLR submission.\n\n\n[1] Y. Wu, Y. Burda, R. Salakhutdinov, R. Grosse, On the Quantitative Analysis of Decoder-Based Generative Models, submitted to ICLR2017.\n\n\nLog likelihood lower bound stochastic estimator we used:\nlog p(x) >= E_{q( z | x )} [ log p( z, x ) - log q( z | x ) ]\nwhere the expectation is estimated by taking an average over several samples drawn from chain  q( z | x ) where z=(z^0,z^1,...,z^(T-1))\nand where\nlog p( z , x) = log p(z^0) + [ sum_{t=1}^{T-1} log p(z^(t)|z^(t-1) ] + log p(x|z^(T-1))\nand similarly\nlog q( z | x) = log q(z^0 |x) +  sum_{t=1}^{T-1} log q(z^(t)|z^(t-1), x)  \n\n\nWe are working on integrating this into the paper."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Generate Samples from Noise through Infusion Training", "abstract": "In this work, we investigate a novel training procedure to learn a generative model as the transition operator of a Markov chain, such that, when applied repeatedly on an unstructured random noise sample, it will denoise it into a sample that matches the target distribution from the training set. The novel training procedure to learn this progressive denoising operation involves sampling from a slightly different chain than the model chain used for generation in the absence of a denoising target. In the training chain we infuse information from the training target example that we would like the chains to reach with a high probability. The thus learned transition operator is able to produce quality and varied samples in a small number of steps. Experiments show competitive results compared to the samples generated with a basic Generative Adversarial Net. ", "pdf": "/pdf/388d81ae0dd044220d7a880fbc2b8d904de2eedf.pdf", "TL;DR": "We learn a markov transition operator acting on inputspace, to denoise random noise into a target distribution. We use a novel target injection technique to guide the training.", "paperhash": "bordes|learning_to_generate_samples_from_noise_through_infusion_training", "keywords": ["Deep learning", "Unsupervised Learning"], "conflicts": ["umontreal.ca", "polymtl.ca"], "authors": ["Florian Bordes", "Sina Honari", "Pascal Vincent"], "authorids": ["florian.bordes@umontreal.ca", "sina.honari@umontreal.ca", "pascal.vincent@umontreal.ca"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287509389, "id": "ICLR.cc/2017/conference/-/paper589/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJAFbaolg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper589/reviewers", "ICLR.cc/2017/conference/paper589/areachairs"], "cdate": 1485287509389}}}, {"tddate": null, "tmdate": 1481060327713, "tcdate": 1481060327553, "number": 1, "id": "By1eehN7g", "invitation": "ICLR.cc/2017/conference/-/paper589/public/comment", "forum": "BJAFbaolg", "replyto": "B1yjtshGe", "signatures": ["~Pascal_Vincent1"], "readers": ["everyone"], "writers": ["~Pascal_Vincent1"], "content": {"title": "Answer regarding time-dependence of infusion chain", "comment": "In our initial experiments, only the variance sigma^2 of the Gaussians output by the network was dependent on t. Specifically we had used sigma^2 = epsilon + sigmoid(...)/(tau0+t)^2 with tau0=2 and espilon=1e-5. But in more recent experiments, we removed the dependence on t (simply using sigma^2 = epsilon + beta * sigmoid(...) ) which turned out to work just as well provided constants epsilon and beta are properly adjusted.\n\nThe primary reason we do not learn an entirely different conditional distribution for each t but instead have them all share parameters is a practical one: to keep the memory footprint of the whole model down to a manageable size. One potential theoretical reason why we might want p^(t)( z^(t) | Z^(t-1) ) to be computed the same way at every time step, is if we understand them as corresponding to (stochastic) gradient descent steps on an implicit energy function, similar to what a MCMC method with Langevin dynamics might do (based on an explicit energy). A gradient step is the same \u201coperation\u201d, iterated several times during gradient descent. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Generate Samples from Noise through Infusion Training", "abstract": "In this work, we investigate a novel training procedure to learn a generative model as the transition operator of a Markov chain, such that, when applied repeatedly on an unstructured random noise sample, it will denoise it into a sample that matches the target distribution from the training set. The novel training procedure to learn this progressive denoising operation involves sampling from a slightly different chain than the model chain used for generation in the absence of a denoising target. In the training chain we infuse information from the training target example that we would like the chains to reach with a high probability. The thus learned transition operator is able to produce quality and varied samples in a small number of steps. Experiments show competitive results compared to the samples generated with a basic Generative Adversarial Net. ", "pdf": "/pdf/388d81ae0dd044220d7a880fbc2b8d904de2eedf.pdf", "TL;DR": "We learn a markov transition operator acting on inputspace, to denoise random noise into a target distribution. We use a novel target injection technique to guide the training.", "paperhash": "bordes|learning_to_generate_samples_from_noise_through_infusion_training", "keywords": ["Deep learning", "Unsupervised Learning"], "conflicts": ["umontreal.ca", "polymtl.ca"], "authors": ["Florian Bordes", "Sina Honari", "Pascal Vincent"], "authorids": ["florian.bordes@umontreal.ca", "sina.honari@umontreal.ca", "pascal.vincent@umontreal.ca"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287509389, "id": "ICLR.cc/2017/conference/-/paper589/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJAFbaolg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper589/reviewers", "ICLR.cc/2017/conference/paper589/areachairs"], "cdate": 1485287509389}}}, {"tddate": null, "tmdate": 1480699722891, "tcdate": 1480699722886, "number": 2, "id": "H1m81EJXl", "invitation": "ICLR.cc/2017/conference/-/paper589/pre-review/question", "forum": "BJAFbaolg", "replyto": "BJAFbaolg", "signatures": ["ICLR.cc/2017/conference/paper589/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper589/AnonReviewer3"], "content": {"title": "Using IS to evaluate log-likelihood", "question": "The authors state the goal of training as \u201csamples from [the training set] are likely of being generated under the model sampling chain.\u201d Unfortunately none of the experiments performed is appropriate to verify if this goal has been achieved. Have you tried using importance sampling to evaluate log-likelihoods?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Generate Samples from Noise through Infusion Training", "abstract": "In this work, we investigate a novel training procedure to learn a generative model as the transition operator of a Markov chain, such that, when applied repeatedly on an unstructured random noise sample, it will denoise it into a sample that matches the target distribution from the training set. The novel training procedure to learn this progressive denoising operation involves sampling from a slightly different chain than the model chain used for generation in the absence of a denoising target. In the training chain we infuse information from the training target example that we would like the chains to reach with a high probability. The thus learned transition operator is able to produce quality and varied samples in a small number of steps. Experiments show competitive results compared to the samples generated with a basic Generative Adversarial Net. ", "pdf": "/pdf/388d81ae0dd044220d7a880fbc2b8d904de2eedf.pdf", "TL;DR": "We learn a markov transition operator acting on inputspace, to denoise random noise into a target distribution. We use a novel target injection technique to guide the training.", "paperhash": "bordes|learning_to_generate_samples_from_noise_through_infusion_training", "keywords": ["Deep learning", "Unsupervised Learning"], "conflicts": ["umontreal.ca", "polymtl.ca"], "authors": ["Florian Bordes", "Sina Honari", "Pascal Vincent"], "authorids": ["florian.bordes@umontreal.ca", "sina.honari@umontreal.ca", "pascal.vincent@umontreal.ca"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481916472198, "id": "ICLR.cc/2017/conference/-/paper589/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper589/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper589/AnonReviewer1", "ICLR.cc/2017/conference/paper589/AnonReviewer3", "ICLR.cc/2017/conference/paper589/AnonReviewer4"], "reply": {"forum": "BJAFbaolg", "replyto": "BJAFbaolg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper589/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper589/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481916472198}}}, {"tddate": null, "tmdate": 1480534423501, "tcdate": 1480534423496, "number": 1, "id": "B1yjtshGe", "invitation": "ICLR.cc/2017/conference/-/paper589/pre-review/question", "forum": "BJAFbaolg", "replyto": "BJAFbaolg", "signatures": ["ICLR.cc/2017/conference/paper589/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper589/AnonReviewer1"], "content": {"title": "time dependence of infusion chain", "question": "I enjoyed reading this paper. One point of confusion I had was about the way in which the step-number dependence was incorporated into p^(t)( z^(t) | Z^(t-1) ). In theory, you could learn a different conditional distribution for each t. In the first paragraph in Section 4, it seems like you are actually using the same function for every value of t though. Then, at the end of that paragraph, you note that you have a hard-coded t dependence in the variance, but you suggest that this is only for reasons of stability and ease of training.\n\nCould you clarify what the t dependence of p^(t)( z^(t) | Z^(t-1) ) is in your experiments? If you believe that p^(t)( z^(t) | Z^(t-1) ) should be the same distribution at every time step, could you talk a little bit about why?\n\nThanks!"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Generate Samples from Noise through Infusion Training", "abstract": "In this work, we investigate a novel training procedure to learn a generative model as the transition operator of a Markov chain, such that, when applied repeatedly on an unstructured random noise sample, it will denoise it into a sample that matches the target distribution from the training set. The novel training procedure to learn this progressive denoising operation involves sampling from a slightly different chain than the model chain used for generation in the absence of a denoising target. In the training chain we infuse information from the training target example that we would like the chains to reach with a high probability. The thus learned transition operator is able to produce quality and varied samples in a small number of steps. Experiments show competitive results compared to the samples generated with a basic Generative Adversarial Net. ", "pdf": "/pdf/388d81ae0dd044220d7a880fbc2b8d904de2eedf.pdf", "TL;DR": "We learn a markov transition operator acting on inputspace, to denoise random noise into a target distribution. We use a novel target injection technique to guide the training.", "paperhash": "bordes|learning_to_generate_samples_from_noise_through_infusion_training", "keywords": ["Deep learning", "Unsupervised Learning"], "conflicts": ["umontreal.ca", "polymtl.ca"], "authors": ["Florian Bordes", "Sina Honari", "Pascal Vincent"], "authorids": ["florian.bordes@umontreal.ca", "sina.honari@umontreal.ca", "pascal.vincent@umontreal.ca"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481916472198, "id": "ICLR.cc/2017/conference/-/paper589/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper589/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper589/AnonReviewer1", "ICLR.cc/2017/conference/paper589/AnonReviewer3", "ICLR.cc/2017/conference/paper589/AnonReviewer4"], "reply": {"forum": "BJAFbaolg", "replyto": "BJAFbaolg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper589/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper589/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481916472198}}}], "count": 20}