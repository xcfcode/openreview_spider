{"notes": [{"id": "BkxDthVtvS", "original": "BklQKcSnIr", "number": 82, "cdate": 1569438847189, "ddate": null, "tcdate": 1569438847189, "tmdate": 1577168255055, "tddate": null, "forum": "BkxDthVtvS", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"abstract": "A key difference from existing works is that our equivarification method can be applied without knowledge of the detailed functions of a layer in a neural network, and hence, can be generalized to any feedforward neural networks. Although the network size scales up, the constructed equivariant neural network does not increase the complexity of the network compared with the original one, in terms of the number of parameters. As an illustration, we build an equivariant neural network for image classification by equivarifying a convolutional neural network. Results show that our proposed method significantly reduces the design and training complexity, yet preserving the learning performance in terms of accuracy.", "title": "Equivariant neural networks and equivarification", "code": "https://github.com/symplecticgeometry/equivariant-neural-networks-and-equivarification", "keywords": ["equivariant", "invariant", "neural network", "equivarification"], "pdf": "/pdf/9dfd9da1e9291567a85e8be6233f74fa0369ff49.pdf", "authors": ["Erkao Bao", "Linqi Song"], "authorids": ["baoerkao@gmail.com", "linqi.song@cityu.edu.hk"], "paperhash": "bao|equivariant_neural_networks_and_equivarification", "original_pdf": "/attachment/41b9060ac63df6941c71f3ffc33f7b7fec3b8f69.pdf", "_bibtex": "@misc{\nbao2020equivariant,\ntitle={Equivariant neural networks and equivarification},\nauthor={Erkao Bao and Linqi Song},\nyear={2020},\nurl={https://openreview.net/forum?id=BkxDthVtvS}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 12, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "kXKxC_civf", "original": null, "number": 1, "cdate": 1576798686950, "ddate": null, "tcdate": 1576798686950, "tmdate": 1576800948119, "tddate": null, "forum": "BkxDthVtvS", "replyto": "BkxDthVtvS", "invitation": "ICLR.cc/2020/Conference/Paper82/-/Decision", "content": {"decision": "Reject", "comment": "This paper proposes a way to construct group equivariant neural networks from pre-trained non-equivariant networks. The equivarification is done with respect to known finite groups, and  can be done globally or layer-wise. The authors discuss their approach in the context of the image data domain. The paper is theoretically sound and proposes a novel perspective on equivarification, however, the reviewers agree that the experimental section should be strengthened and connections with other approaches (e.g. the work by Cohen and Welling) should be made clearer. The reviewers also had concerns about the computational cost of the equivarification method proposed in this paper. While the authors\u2019 revision addressed some of the reviewers\u2019 concerns, it was not enough to accept the paper this time round. Hence, unfortunately I recommend a rejection.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"abstract": "A key difference from existing works is that our equivarification method can be applied without knowledge of the detailed functions of a layer in a neural network, and hence, can be generalized to any feedforward neural networks. Although the network size scales up, the constructed equivariant neural network does not increase the complexity of the network compared with the original one, in terms of the number of parameters. As an illustration, we build an equivariant neural network for image classification by equivarifying a convolutional neural network. Results show that our proposed method significantly reduces the design and training complexity, yet preserving the learning performance in terms of accuracy.", "title": "Equivariant neural networks and equivarification", "code": "https://github.com/symplecticgeometry/equivariant-neural-networks-and-equivarification", "keywords": ["equivariant", "invariant", "neural network", "equivarification"], "pdf": "/pdf/9dfd9da1e9291567a85e8be6233f74fa0369ff49.pdf", "authors": ["Erkao Bao", "Linqi Song"], "authorids": ["baoerkao@gmail.com", "linqi.song@cityu.edu.hk"], "paperhash": "bao|equivariant_neural_networks_and_equivarification", "original_pdf": "/attachment/41b9060ac63df6941c71f3ffc33f7b7fec3b8f69.pdf", "_bibtex": "@misc{\nbao2020equivariant,\ntitle={Equivariant neural networks and equivarification},\nauthor={Erkao Bao and Linqi Song},\nyear={2020},\nurl={https://openreview.net/forum?id=BkxDthVtvS}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "BkxDthVtvS", "replyto": "BkxDthVtvS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795706396, "tmdate": 1576800254441, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper82/-/Decision"}}}, {"id": "H1lsO14PtB", "original": null, "number": 1, "cdate": 1571401587174, "ddate": null, "tcdate": 1571401587174, "tmdate": 1574422952602, "tddate": null, "forum": "BkxDthVtvS", "replyto": "BkxDthVtvS", "invitation": "ICLR.cc/2020/Conference/Paper82/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #3", "review": "In this work, the authors employ concepts from group theory to turn an arbitrary feed forward neural network into an equivariant one, i.e. a network whose output transforms in a way that is consistent with the transformation of the input. To this end, the authors first introduce the basic concepts of group theory required to follow their work and provide a comprehensive definition of equivariance. They then explain how to equivarify (w.r.t. a finite group G) a given neural network, and present experimental results on rotated MNIST digits to support their approach.\n\nI find that the proposed approach is very elegant and addresses a highly important question, namely how to devise or modify an architecture such that it preserves symmetries. In my opinion, the current paper makes an interesting theoretical contribution towards achieving this goal, but it also has several shortcomings that are detailed below. Based on these shortcomings, I recommend rejecting the paper but I would be willing to increase the score if these points were addressed in sufficient detail.\n\nMajor comments:\n\n1) Scaling\nThe authors mention in the abstract that \u2018although the network size scales up, the constructed equivariant neural network does not increase the complexity of the network compared with the original one in terms of the number of the parameters.\u2019 Based on Eq. (3.0.2) and Fig. 3, it is my understanding that n evaluations of each data point (input to a layer) are required for a cyclic group of order n. If the outputs of a layer are then concatenated, the input dimension of the subsequent layer grows by a factor of n. I would therefore argue that out-the-box application of the proposed approach does increase the number of variables dramatically and that the abstract is misleading in this respect. The authors briefly comment on this point with one sentence in the third paragraph of the introduction. However, I would appreciate if this point was addressed in more detail, for example in a dedicated paragraph after the theory is introduced. Please also address the question of whether variable sharing is essential from an equivariance point of view, or whether it\u2019s simply a necessity to prevent an explosion of the number of parameters. Furthermore, convolutions encode translational symmetry which may be beneficial for the current application but may not be desirable for other datasets. A few comments for clarification would be very helpful. Finally, the equivarified network seems to increase the required number of computations significantly compared to the original one, which I find worth a comment .\n\n2) Experiment\nThe authors only consider a convolutional architecture and say that \u2018in order to illustrate flexibility we choose not to simply equivarify the original network\u2019. However, to me one of the main advantages of the paper seems to be that you can take this approach to equivarify any FFN. It would therefore be interesting to see this approach be applied to different networks starting with a simpler one, e.g. a 2-layer MLP. The authors could then compare the original network to the equivarified one with and without variable sharing. That would not only help the reader understand the approach better but also be much more in line with the main motivation of the paper. Then adding a second experiment, e.g. a convolutional architecture, to demonstrate flexibility would be very interesting. With regard to Fig. 4, I think there may be better ways of summarising the results than dumping 160 numbers of which only 4 seem to be of interest. The message seems to be that the network yields identical probabilities irrespective of the degree of rotation. What I find surprising is that all numbers are actually identical (shifted by 10). Is this by construction?\n\n3) Limitations\nAs indicated in the second paragraph of Sec. 4, this approach is limited to finite groups and the authors only consider image rotations w.r.t. the cyclic group of degree 4. Although I appreciate that this is meant to serve as a toy problem to illustrate that the approach works, I do not think that rotations by a constant angle are very interesting. What would be really interesting is equivariance w.r.t continuous rotations (Lie Groups), e.g. the SO(2) in this particular case. I doubt that an extension to the SO(2) is straightforward within the current theoretical framework. However, even if that is the case, I would appreciate if the authors could comment on this in a paragraph.\n\nMinor comments:\n\ni) There are many typos and grammar mistakes in the paper:\n\u2018any feedforward networks\u2019 -> \u2018any feedforward network\u2019.\n\u2018enables to design\u2019 -> \u2018enables us to design\u2019\n\u2018our proposed equivarification method use\u2019 -> \u2018our proposed equivarification method uses\u2019\n\u2018traditional sense multiplication\u2019 -> \u2018traditional sense of multiplication\u2019\n\u2018a group G acts\u2019 -> \u2018a group G that acts\u2019\n\u2018neural network that defined on the\u2019 -> \u2018neural network that is defined on the\u2019\n\u2018which achieves promising performance\u2019 -> \u2018which achieve promising performance\u2019\n\u2018supplymentary material\u2019 -> \u2018supplementary material\u2019 \nEtc.\n\nii) I think there may be a mistake in the 3rd point of Definition 3.0.3: For consistency with the previous definitions and with Fig. 1, shouldn\u2019t F map from X to Z and \\hat F from X to \\hat Z? \n\niii) Last paragraph of Sec 3: \u2018then after the identification \\hat F becomes a map from Z to...'.  Should it be \u2018a map from X to ..\u2019?\n\niv) In Definition 3.0.3 you define the tuple (\\hat Z, T, p) to be a G-equivarification, but in the paragraph below you call the G-product itself a G-equivarification (without including T and p). \n\nv) Footnote 2: You could correct for that and present the theory shifting by g instead of g^-1 to make it easier for the reader to follow. Or, at least, give a reference to the footnote earlier on in Example 4.0.1 to avoid confusion.\n\nvi) Unless there is a special reason for this, I would suggest changing the enumeration of definitions, lemmas, examples and equations, i.e. (3.0.1) -> (3.1), etc...\n\n\n*********************************************************\nI increased my rating based on the authors addressing many of my comments. \n*********************************************************\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper82/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper82/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"abstract": "A key difference from existing works is that our equivarification method can be applied without knowledge of the detailed functions of a layer in a neural network, and hence, can be generalized to any feedforward neural networks. Although the network size scales up, the constructed equivariant neural network does not increase the complexity of the network compared with the original one, in terms of the number of parameters. As an illustration, we build an equivariant neural network for image classification by equivarifying a convolutional neural network. Results show that our proposed method significantly reduces the design and training complexity, yet preserving the learning performance in terms of accuracy.", "title": "Equivariant neural networks and equivarification", "code": "https://github.com/symplecticgeometry/equivariant-neural-networks-and-equivarification", "keywords": ["equivariant", "invariant", "neural network", "equivarification"], "pdf": "/pdf/9dfd9da1e9291567a85e8be6233f74fa0369ff49.pdf", "authors": ["Erkao Bao", "Linqi Song"], "authorids": ["baoerkao@gmail.com", "linqi.song@cityu.edu.hk"], "paperhash": "bao|equivariant_neural_networks_and_equivarification", "original_pdf": "/attachment/41b9060ac63df6941c71f3ffc33f7b7fec3b8f69.pdf", "_bibtex": "@misc{\nbao2020equivariant,\ntitle={Equivariant neural networks and equivarification},\nauthor={Erkao Bao and Linqi Song},\nyear={2020},\nurl={https://openreview.net/forum?id=BkxDthVtvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BkxDthVtvS", "replyto": "BkxDthVtvS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper82/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper82/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575382680139, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper82/Reviewers"], "noninvitees": [], "tcdate": 1570237757341, "tmdate": 1575382680150, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper82/-/Official_Review"}}}, {"id": "r1eMHtR4jB", "original": null, "number": 6, "cdate": 1573345593558, "ddate": null, "tcdate": 1573345593558, "tmdate": 1573358309436, "tddate": null, "forum": "BkxDthVtvS", "replyto": "BkxDthVtvS", "invitation": "ICLR.cc/2020/Conference/Paper82/-/Official_Comment", "content": {"title": "about revision and general respond to all the reviews", "comment": "We made a revision. This is not the final version. We are willing to make further modifications if necessary to make the paper more readable, and more fair to other works. \n\nAbout the changes we made: \n    We corrected some terrible typos in the theory part and added some remarks to make it more readable. Most of the changes are marked in the blue color.\n\n    We largely rewrote the introduction part. In particular, we removed those sentences that are confusing. We added more \n    heuristic arguments to compare our approach again the data augmentation approach. \n\n    We cited more works, and tried to make it more fair. \n\n    We added an acknowledgement. \n\nOur general respond the referees' comments: \n\nThanks for all your time and patience. We really appreciate. We find the comments are fair and honest. We find our paper is much user friendly after revision based on your comments.\n\n     1. difference between our approach vs data augmentation:\n          They are definitely quite different. In short, using data augmentation, the network is in general not equivariant. We \n          added more arguments in the revised version.\n\n    2. experiment limitation:\n          The experiment that we carry out is mainly to justify visually the equivariance of our network. It is a toy model. We \n          believe that there will be a lot of cool applications of our theory in the future. After all, we feel that the main \n          contribution of the paper is theoretical.\n\n    3. novelty:\n        Theoretically, I think the approach is the most general one. \n        In many other papers, when they say equivariant, they actually mean invariant. But we are not.\n        \n        F is a map from X to Z, where X has a group G action.\n            To achieve invariance, one can take an average (or other aggregation) of F over the group action.\n            To achieve equivariance, we choose to enlarge the space Z.\n   \n   4. sanity:\n        The theory part is short and easy to check. The experiment outputs vectors in Fig 4 where all vectors are related by \n        shifts. In the github code jupyter notebook v2, one can also see the version with flips added in, where the 80 \n       dimensional vectors are also related by shifts. \n\n    5. limitation on the group:\n        The theory works for all groups, but the implementation is only for finite groups (see newly added Remark 3.3). "}, "signatures": ["ICLR.cc/2020/Conference/Paper82/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper82/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"abstract": "A key difference from existing works is that our equivarification method can be applied without knowledge of the detailed functions of a layer in a neural network, and hence, can be generalized to any feedforward neural networks. Although the network size scales up, the constructed equivariant neural network does not increase the complexity of the network compared with the original one, in terms of the number of parameters. As an illustration, we build an equivariant neural network for image classification by equivarifying a convolutional neural network. Results show that our proposed method significantly reduces the design and training complexity, yet preserving the learning performance in terms of accuracy.", "title": "Equivariant neural networks and equivarification", "code": "https://github.com/symplecticgeometry/equivariant-neural-networks-and-equivarification", "keywords": ["equivariant", "invariant", "neural network", "equivarification"], "pdf": "/pdf/9dfd9da1e9291567a85e8be6233f74fa0369ff49.pdf", "authors": ["Erkao Bao", "Linqi Song"], "authorids": ["baoerkao@gmail.com", "linqi.song@cityu.edu.hk"], "paperhash": "bao|equivariant_neural_networks_and_equivarification", "original_pdf": "/attachment/41b9060ac63df6941c71f3ffc33f7b7fec3b8f69.pdf", "_bibtex": "@misc{\nbao2020equivariant,\ntitle={Equivariant neural networks and equivarification},\nauthor={Erkao Bao and Linqi Song},\nyear={2020},\nurl={https://openreview.net/forum?id=BkxDthVtvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BkxDthVtvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper82/Authors", "ICLR.cc/2020/Conference/Paper82/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper82/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper82/Reviewers", "ICLR.cc/2020/Conference/Paper82/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper82/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper82/Authors|ICLR.cc/2020/Conference/Paper82/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504176663, "tmdate": 1576860545305, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper82/Authors", "ICLR.cc/2020/Conference/Paper82/Reviewers", "ICLR.cc/2020/Conference/Paper82/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper82/-/Official_Comment"}}}, {"id": "Hyl5yipNir", "original": null, "number": 5, "cdate": 1573341922013, "ddate": null, "tcdate": 1573341922013, "tmdate": 1573341922013, "tddate": null, "forum": "BkxDthVtvS", "replyto": "rkl4q1YWjr", "invitation": "ICLR.cc/2020/Conference/Paper82/-/Official_Comment", "content": {"title": "respond to references", "comment": "I am reading the papers you mentioned. But I only see the only pdfs. Do you have the bibtex? Thanks."}, "signatures": ["ICLR.cc/2020/Conference/Paper82/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper82/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"abstract": "A key difference from existing works is that our equivarification method can be applied without knowledge of the detailed functions of a layer in a neural network, and hence, can be generalized to any feedforward neural networks. Although the network size scales up, the constructed equivariant neural network does not increase the complexity of the network compared with the original one, in terms of the number of parameters. As an illustration, we build an equivariant neural network for image classification by equivarifying a convolutional neural network. Results show that our proposed method significantly reduces the design and training complexity, yet preserving the learning performance in terms of accuracy.", "title": "Equivariant neural networks and equivarification", "code": "https://github.com/symplecticgeometry/equivariant-neural-networks-and-equivarification", "keywords": ["equivariant", "invariant", "neural network", "equivarification"], "pdf": "/pdf/9dfd9da1e9291567a85e8be6233f74fa0369ff49.pdf", "authors": ["Erkao Bao", "Linqi Song"], "authorids": ["baoerkao@gmail.com", "linqi.song@cityu.edu.hk"], "paperhash": "bao|equivariant_neural_networks_and_equivarification", "original_pdf": "/attachment/41b9060ac63df6941c71f3ffc33f7b7fec3b8f69.pdf", "_bibtex": "@misc{\nbao2020equivariant,\ntitle={Equivariant neural networks and equivarification},\nauthor={Erkao Bao and Linqi Song},\nyear={2020},\nurl={https://openreview.net/forum?id=BkxDthVtvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BkxDthVtvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper82/Authors", "ICLR.cc/2020/Conference/Paper82/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper82/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper82/Reviewers", "ICLR.cc/2020/Conference/Paper82/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper82/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper82/Authors|ICLR.cc/2020/Conference/Paper82/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504176663, "tmdate": 1576860545305, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper82/Authors", "ICLR.cc/2020/Conference/Paper82/Reviewers", "ICLR.cc/2020/Conference/Paper82/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper82/-/Official_Comment"}}}, {"id": "rylvUchEiS", "original": null, "number": 4, "cdate": 1573337679057, "ddate": null, "tcdate": 1573337679057, "tmdate": 1573337679057, "tddate": null, "forum": "BkxDthVtvS", "replyto": "Bkgdz79Mor", "invitation": "ICLR.cc/2020/Conference/Paper82/-/Official_Comment", "content": {"title": "respond to Official Blind Review #2", "comment": "Thanks for the feedback. I just looked at the two papers you mentioned (steerable one and the gauge one). Unfortunately, all the papers look very similar in this area. It is hard to tell exact what is going on in a paper without a rigorous definition. (of course, I will cite them).\n\nThe main contribution of our paper is bringing in the  Definition (3.0.1 or 3.1 in the coming version) of a G-product, and Lemma 3.0.2 (or 3.2 in the coming version).\n\nWhen you say Z^V, I do not understand what exactly you mean. Do you mean V is a vector space, and there is a group homomorphism (a representation) form G to the GL(V), the space of general linear transforms on V. If so, what is Z^V? \n\nIn our paper, Z^{\\times G} is the space of maps from G to Z. In the case when G is a compact Lie Group, we can restrict  Z^{\\times G} to be the space of smooth maps from Z to G to make it smaller and respect the differential structures. When G is a non-compact, we can further restrict to the compact supported maps.\n\nIt would be ideal that we can read all papers in this field and make it clear what the connections are. But on the other hand, this paper is not doing a special case study, and we have to worry about whether our paper is a special case of other papers. We provide a general and simple framework for equivariant network. I briefly go through all the papers that are mentioned to me, and I don't see any construction as general as this one.\n\nI want to stress that, the definition 3.1 and lemma 3.2 are the cleanest thing that I have seen in this equivariant area. We did not make any additional and unnecessary choices. "}, "signatures": ["ICLR.cc/2020/Conference/Paper82/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper82/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"abstract": "A key difference from existing works is that our equivarification method can be applied without knowledge of the detailed functions of a layer in a neural network, and hence, can be generalized to any feedforward neural networks. Although the network size scales up, the constructed equivariant neural network does not increase the complexity of the network compared with the original one, in terms of the number of parameters. As an illustration, we build an equivariant neural network for image classification by equivarifying a convolutional neural network. Results show that our proposed method significantly reduces the design and training complexity, yet preserving the learning performance in terms of accuracy.", "title": "Equivariant neural networks and equivarification", "code": "https://github.com/symplecticgeometry/equivariant-neural-networks-and-equivarification", "keywords": ["equivariant", "invariant", "neural network", "equivarification"], "pdf": "/pdf/9dfd9da1e9291567a85e8be6233f74fa0369ff49.pdf", "authors": ["Erkao Bao", "Linqi Song"], "authorids": ["baoerkao@gmail.com", "linqi.song@cityu.edu.hk"], "paperhash": "bao|equivariant_neural_networks_and_equivarification", "original_pdf": "/attachment/41b9060ac63df6941c71f3ffc33f7b7fec3b8f69.pdf", "_bibtex": "@misc{\nbao2020equivariant,\ntitle={Equivariant neural networks and equivarification},\nauthor={Erkao Bao and Linqi Song},\nyear={2020},\nurl={https://openreview.net/forum?id=BkxDthVtvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BkxDthVtvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper82/Authors", "ICLR.cc/2020/Conference/Paper82/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper82/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper82/Reviewers", "ICLR.cc/2020/Conference/Paper82/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper82/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper82/Authors|ICLR.cc/2020/Conference/Paper82/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504176663, "tmdate": 1576860545305, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper82/Authors", "ICLR.cc/2020/Conference/Paper82/Reviewers", "ICLR.cc/2020/Conference/Paper82/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper82/-/Official_Comment"}}}, {"id": "Bkgdz79Mor", "original": null, "number": 4, "cdate": 1573196559600, "ddate": null, "tcdate": 1573196559600, "tmdate": 1573196559600, "tddate": null, "forum": "BkxDthVtvS", "replyto": "BkxDthVtvS", "invitation": "ICLR.cc/2020/Conference/Paper82/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #2", "review": "The paper adds an interesting new perspective to equivariant neural nets. However, the actual construction looks equivalent to steerable neural nets to me (see the papers by Cohen and Welling). The generalization of steerable nets has been published under the name \"gauge equivariant neural nets\", it would be very interesting to chart out the exact connections between these concepts. \n\nThe authors mention that Z^{\\times G} is not the only possible lifting space. I believe that the general case would be Z^V where V is a representation of G. \n\nMany of the earlier papers on equivariant nets were written in the language of representation theory. It is interesting that similar nets can be constructed by purely group theoretic methods, but I really think that ultimately they are same thing. Consequently, I would expect all the experimental results to be identical.\n\nWhat would make this paper really valuable for didactic purposes is if these connections were carefully mapped out and presented with intuitive diagrams and examples.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper82/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper82/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"abstract": "A key difference from existing works is that our equivarification method can be applied without knowledge of the detailed functions of a layer in a neural network, and hence, can be generalized to any feedforward neural networks. Although the network size scales up, the constructed equivariant neural network does not increase the complexity of the network compared with the original one, in terms of the number of parameters. As an illustration, we build an equivariant neural network for image classification by equivarifying a convolutional neural network. Results show that our proposed method significantly reduces the design and training complexity, yet preserving the learning performance in terms of accuracy.", "title": "Equivariant neural networks and equivarification", "code": "https://github.com/symplecticgeometry/equivariant-neural-networks-and-equivarification", "keywords": ["equivariant", "invariant", "neural network", "equivarification"], "pdf": "/pdf/9dfd9da1e9291567a85e8be6233f74fa0369ff49.pdf", "authors": ["Erkao Bao", "Linqi Song"], "authorids": ["baoerkao@gmail.com", "linqi.song@cityu.edu.hk"], "paperhash": "bao|equivariant_neural_networks_and_equivarification", "original_pdf": "/attachment/41b9060ac63df6941c71f3ffc33f7b7fec3b8f69.pdf", "_bibtex": "@misc{\nbao2020equivariant,\ntitle={Equivariant neural networks and equivarification},\nauthor={Erkao Bao and Linqi Song},\nyear={2020},\nurl={https://openreview.net/forum?id=BkxDthVtvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BkxDthVtvS", "replyto": "BkxDthVtvS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper82/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper82/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575382680139, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper82/Reviewers"], "noninvitees": [], "tcdate": 1570237757341, "tmdate": 1575382680150, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper82/-/Official_Review"}}}, {"id": "SkgCrTWMjr", "original": null, "number": 3, "cdate": 1573162310079, "ddate": null, "tcdate": 1573162310079, "tmdate": 1573162310079, "tddate": null, "forum": "BkxDthVtvS", "replyto": "B1xIzwVAcB", "invitation": "ICLR.cc/2020/Conference/Paper82/-/Official_Comment", "content": {"title": "respond to Official Blind Review #4 ", "comment": "Thank you very much for your time reviewing this paper. We really appreciate all the comments. We are sorry that there are some typos in the current version which makes the reading unnecessarily harder. We are fixing this in the next version (coming in 1-2 days) and we are also making changes reflecting your valuable comments.\n\nQ. After reading the paper, I don\u2019t understand the ... Could the authors elaborate on this? \n\n\tA. this method vs data augmentation:\n\tData augmentation does not guarantee equivariance. \n\n\tIn the plain version of the equivarification method, we don't need to do it layer by layer as you suggested.\n\n\tAs for the experiment that we carried out, in short, to guarantee equivariance, we make each layer equivariant. More details, for the plain version, doing a layer by layer equivarification is equivalent to do a global equivarification. \n\nQ. After reading this section, I don\u2019t understand the proposed fine-tuning procedure (pre-train, finetune and test): (1) what is the accuracy of the pre-trained network that was started from? \n\n\tA. By pre-train accuracy, do you mean the accuracy on the training data before fine tune? Since this is a baby model, we actually did not check that. We only look at the loss in the training case. We also did not do the fine tune. Since our main goal is not to provide a specific equivariant neural network for image recognization, but to provide a general process to produce equivariant neural networks by modifying existing networks. Also, we are planing to do a full analysis and comparison in a following up paper.\n\nQ. (2) how is the initial network fine-tuned and modified? (as the authors mention that during training the samples are not rotated). Also, I am confused with the first sentence on page 8: \u2018the complexity of the constructed network does not grow in terms of the number of parameters\u2019. It would be useful if the results in Fig. 4 are more clearly illustrated.\n\nIs the order or increased computation 4x4x4? It would be useful to compare the method (computation & performance) with a baseline where the dataset is enlarged with data augmentation. The authors mention in the introduction that this increases training overhead, whereas the proposed practical method increases the computation at inference as well as the memory footprint of the model and the forward pass. It would be useful if the authors compared empirically with baselines with (1) data augmentation (2) network with an increased number of parameters (same as the proposed one).\n\n\tA. The initial network is not fine tuned. We just want to provide a general method to modify neural networks and we also did not carry out careful and fair comparison of our method against others (such as data augmentation). We will make the sentence \u2018the complexity of the constructed network does not grow in terms of the number of parameters\u2019 clearer in the revised version. We will make the Fig. 4 more clearly illustrated.\n\n- If digits 6 or 9 are rotated the label changes, how does the proposed method handle this?\n\n\tA. we don't label the rotated 6 the same as an unrotated 9.\n\n- page 8, conclusion: The authors claim ... with such baselines (see above).\n\n\tA. agree. We will change the wording. The paper is mainly a proposed method with a theoretical proof and an experiment justification. It does not deal with the comparison with previous work, which requires a lot of experiments and tests, otherwise, finding one or two examples and comparing the results won't be fair. \n\n- Page 1: it is mentioned that \u2018the number of parameters can be the same as the original network\u2019 but the experiments do not include such architecture. After reading the paper I don\u2019t understand how such a network can be implemented and whether it works.\n\n\tA. The experiment include a more interesting example, in which case, it is hard to compare the original network and the new network. \n\n\tOn the other hand, one can apply lemma 3.0.3 to the whole neural network, i.e., make F to be the original neural network. Then in the proof of lemma 3.0.3, the construction of \\hat F is our equivarification. Clearly \\hat F has the same number of parameters as F. For detailed tensorflow implementation, see https://github.com/symplecticgeometry/equivariant-neural-networks-and-equivarification/blob/master/src/equivariant_cnn%20V2.ipynb\n\n\t[106] drop X1_hat, X2_hat, and rewrite X6_hat as \n\n    X6_hat = equivarification(X0_hat, graphs, 'basic_graph', \n                              ['conv3', 'dense', 'logi'], \n                              layer_paremeters, \n                              'permutation')\n\n\u2014 Minor \u2014\nA. Will change these accordingly."}, "signatures": ["ICLR.cc/2020/Conference/Paper82/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper82/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"abstract": "A key difference from existing works is that our equivarification method can be applied without knowledge of the detailed functions of a layer in a neural network, and hence, can be generalized to any feedforward neural networks. Although the network size scales up, the constructed equivariant neural network does not increase the complexity of the network compared with the original one, in terms of the number of parameters. As an illustration, we build an equivariant neural network for image classification by equivarifying a convolutional neural network. Results show that our proposed method significantly reduces the design and training complexity, yet preserving the learning performance in terms of accuracy.", "title": "Equivariant neural networks and equivarification", "code": "https://github.com/symplecticgeometry/equivariant-neural-networks-and-equivarification", "keywords": ["equivariant", "invariant", "neural network", "equivarification"], "pdf": "/pdf/9dfd9da1e9291567a85e8be6233f74fa0369ff49.pdf", "authors": ["Erkao Bao", "Linqi Song"], "authorids": ["baoerkao@gmail.com", "linqi.song@cityu.edu.hk"], "paperhash": "bao|equivariant_neural_networks_and_equivarification", "original_pdf": "/attachment/41b9060ac63df6941c71f3ffc33f7b7fec3b8f69.pdf", "_bibtex": "@misc{\nbao2020equivariant,\ntitle={Equivariant neural networks and equivarification},\nauthor={Erkao Bao and Linqi Song},\nyear={2020},\nurl={https://openreview.net/forum?id=BkxDthVtvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BkxDthVtvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper82/Authors", "ICLR.cc/2020/Conference/Paper82/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper82/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper82/Reviewers", "ICLR.cc/2020/Conference/Paper82/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper82/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper82/Authors|ICLR.cc/2020/Conference/Paper82/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504176663, "tmdate": 1576860545305, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper82/Authors", "ICLR.cc/2020/Conference/Paper82/Reviewers", "ICLR.cc/2020/Conference/Paper82/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper82/-/Official_Comment"}}}, {"id": "rkl4q1YWjr", "original": null, "number": 1, "cdate": 1573126027749, "ddate": null, "tcdate": 1573126027749, "tmdate": 1573126027749, "tddate": null, "forum": "BkxDthVtvS", "replyto": "BkxDthVtvS", "invitation": "ICLR.cc/2020/Conference/Paper82/-/Public_Comment", "content": {"title": "Recent References to be included.", "comment": "The authors should also consult the recent work of Kondor (Risi Kondor. N-body networks: a covariant hierarchical neural network architecture for learning atomic potentials)  and Murugan et al (SO(2)-equivariance in Neural networks using tensor nonlinearity, BMVC 2019) where other ideas for group equivarification are considered.\n\n"}, "signatures": ["~K_V_Subrahmanyam1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~K_V_Subrahmanyam1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"abstract": "A key difference from existing works is that our equivarification method can be applied without knowledge of the detailed functions of a layer in a neural network, and hence, can be generalized to any feedforward neural networks. Although the network size scales up, the constructed equivariant neural network does not increase the complexity of the network compared with the original one, in terms of the number of parameters. As an illustration, we build an equivariant neural network for image classification by equivarifying a convolutional neural network. Results show that our proposed method significantly reduces the design and training complexity, yet preserving the learning performance in terms of accuracy.", "title": "Equivariant neural networks and equivarification", "code": "https://github.com/symplecticgeometry/equivariant-neural-networks-and-equivarification", "keywords": ["equivariant", "invariant", "neural network", "equivarification"], "pdf": "/pdf/9dfd9da1e9291567a85e8be6233f74fa0369ff49.pdf", "authors": ["Erkao Bao", "Linqi Song"], "authorids": ["baoerkao@gmail.com", "linqi.song@cityu.edu.hk"], "paperhash": "bao|equivariant_neural_networks_and_equivarification", "original_pdf": "/attachment/41b9060ac63df6941c71f3ffc33f7b7fec3b8f69.pdf", "_bibtex": "@misc{\nbao2020equivariant,\ntitle={Equivariant neural networks and equivarification},\nauthor={Erkao Bao and Linqi Song},\nyear={2020},\nurl={https://openreview.net/forum?id=BkxDthVtvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BkxDthVtvS", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504214206, "tmdate": 1576860578673, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper82/Authors", "ICLR.cc/2020/Conference/Paper82/Reviewers", "ICLR.cc/2020/Conference/Paper82/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper82/-/Public_Comment"}}}, {"id": "ByxdtdAesB", "original": null, "number": 2, "cdate": 1573083264252, "ddate": null, "tcdate": 1573083264252, "tmdate": 1573083264252, "tddate": null, "forum": "BkxDthVtvS", "replyto": "r1xeNwATKH", "invitation": "ICLR.cc/2020/Conference/Paper82/-/Official_Comment", "content": {"title": "Respond to Official Blind Review #1 ", "comment": "We appreciate the time you spent reviewing this paper. We are sorry that some unfortunate typos make the paper harder to read. \n\nWeaknesses:\n-- The experiment is nice but very limited and does not demonstrate the benefits of having an equivariant network. For example, the authors do not report the accuracy of recovering the original (0) rotation.\n\nA. We are planning to do a careful analysis of the experiments and comparisons in a following up paper. \n\n-- The novelty of the work is questionable. While the development is different, the final example for equivarification of a neural network is very similar to the existing works by Cohen and Welling.\n\nA. We have read a few existing works by Cohen and Welling, and we also cited their papers. Actually it is their spherical cnn paper that brought us into this area. But frankly speaking, we don't understand how they achieve equivariance. The only point in their papers that we understand is to achieve invariance by taking average (in the spherical cnn paper, taking average means integrating against a Haar measure). But even this, it is not so clear to me how it is carriered out. For this reason, we could not justify the novelty of our paper since I don't understand what is in their papers. On the other hand, we propose a criterion to test equivariance without understanding a paper or reading a code. In Figure 4 of this paper, we print out all the predicted probabilities. And one can see that all the four 40-dimensional vectors are exactly related by shifting. We marked out one component by yellow, but it is true for all other components. Our neural network certainly passed this test. One can view this from this code on github \nhttps://github.com/symplecticgeometry/equivariant-neural-networks-and-equivarification/blob/master/src/equivariant_cnn%20V2.ipynb\nThere, we also include horizontal flip and vertical flip, and one gets a vector in R^80. One can check that for the same image under different rotations/flips, the probabilities are the same except the shifts.\n\nWith these being said, our best understanding of the difference of our paper compared to the Cohen and welling's papers is:\nthey try to achieve invariance by taking average; while, we achieve equivariance by enlarging the space.\n\n-- There are other works on equivarification that are missed by this paper. For example, consider the following paper:\nLenssen, J. E., Fey, M., & Libuschewski, P. (2018). Group equivariant capsule networks. In NeurIPS.\n\nA. Thanks a lot bring up this paper. We will cite it properly in the revision (coming in 1-2 days.)\n\n-- The layer-wise equivariant method does have extra computational overheads.\nA. Agree.\n\n-- The fact that we have to specify the groups that we want to make the network equivariant with respect to is a limitation. The promise of capsule networks, in contrast, is to \"ideally\" learn the pose (variation) vectors in a data-driven way.\nSabour, S., Frosst, N., & Hinton, G. E. (2017). Dynamic routing between capsules. In NeurIPS.\n\nA. That is quite interesting. We will cite it. \n\n-- The following statements need more explanation:\n  * \"However, these may require extra training overhead as the augmented data increase.\"\nA. We will make it more clear. We think what we wrote here is confusing. In a task that just to predict the numbers not the angles, our neural network (the plain version) has the approximately computing complexity as the data augmentation one. But because we guarantee invariance (a special type of equivariance), while data augmentation does not, we are using the neurons more \"economically\". "}, "signatures": ["ICLR.cc/2020/Conference/Paper82/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper82/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"abstract": "A key difference from existing works is that our equivarification method can be applied without knowledge of the detailed functions of a layer in a neural network, and hence, can be generalized to any feedforward neural networks. Although the network size scales up, the constructed equivariant neural network does not increase the complexity of the network compared with the original one, in terms of the number of parameters. As an illustration, we build an equivariant neural network for image classification by equivarifying a convolutional neural network. Results show that our proposed method significantly reduces the design and training complexity, yet preserving the learning performance in terms of accuracy.", "title": "Equivariant neural networks and equivarification", "code": "https://github.com/symplecticgeometry/equivariant-neural-networks-and-equivarification", "keywords": ["equivariant", "invariant", "neural network", "equivarification"], "pdf": "/pdf/9dfd9da1e9291567a85e8be6233f74fa0369ff49.pdf", "authors": ["Erkao Bao", "Linqi Song"], "authorids": ["baoerkao@gmail.com", "linqi.song@cityu.edu.hk"], "paperhash": "bao|equivariant_neural_networks_and_equivarification", "original_pdf": "/attachment/41b9060ac63df6941c71f3ffc33f7b7fec3b8f69.pdf", "_bibtex": "@misc{\nbao2020equivariant,\ntitle={Equivariant neural networks and equivarification},\nauthor={Erkao Bao and Linqi Song},\nyear={2020},\nurl={https://openreview.net/forum?id=BkxDthVtvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BkxDthVtvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper82/Authors", "ICLR.cc/2020/Conference/Paper82/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper82/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper82/Reviewers", "ICLR.cc/2020/Conference/Paper82/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper82/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper82/Authors|ICLR.cc/2020/Conference/Paper82/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504176663, "tmdate": 1576860545305, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper82/Authors", "ICLR.cc/2020/Conference/Paper82/Reviewers", "ICLR.cc/2020/Conference/Paper82/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper82/-/Official_Comment"}}}, {"id": "ryltsMnlsr", "original": null, "number": 1, "cdate": 1573073568709, "ddate": null, "tcdate": 1573073568709, "tmdate": 1573073568709, "tddate": null, "forum": "BkxDthVtvS", "replyto": "H1lsO14PtB", "invitation": "ICLR.cc/2020/Conference/Paper82/-/Official_Comment", "content": {"title": "respond to Official Blind Review #3", "comment": "We really appreciate your time reading the paper and the constructive comments.\n\n1) Scaling\n\nA. You are right. If only do a global equivarification using a group of order n, then the number of parameters is the same as the original one. However, if we concatenate, then the number of variables grow by a factor of n. We will address this more precisely in the revised version (coming soon).\n\nWe were not trying to mislead readers (even though it is a little confusing.). We want to mention that to simply achieve equivariant, we can do a global equivarification of the neural network (not a layer by layer one), and this naive version does not increase number of parameters, but as you mentioned it does seem increase the computation complexity.\n\n\n2) Experiment\n\nA. The experiments you suggested make sense. However, since we are planning to equivarify a few well-known neural networks, such as vgg, resnet in a following up paper (with additional authors) and do more comparison there, we do not include many experiments in this paper. I don't think there is surficent amount of time for us in this tight time period to do a clear and fair comparison to include into this paper. \n\nWith regard to Fig. 4, I agree with what you said that it is not the best way to present data. But we do have our own reasons: I have read a few other articles about equivariant neural networks, and to be frank, I don't fully understand how they achieve the equivariance. I am including this picture also as a suggestion to test equivariance in neural networks. One does not need to read the paper or check the code. One can simply print out the all the probability predictions for one image and its rotations, then we can check whether it is equivariant or not. Yes, they are all shifted by 10 locations by construction. In the code, https://github.com/symplecticgeometry/equivariant-neural-networks-and-equivarification/blob/master/src/equivariant_cnn%20V2.ipynb\nwe included the horizontal and vertical flips and one can see that the probabilities are also shifted, but in a more complicated way. \n\n\n3) Limitations\n\nA. That is a good point. We will add a comment about it. In the paper, the example is only based on cylic group of order 4. In the code, we also have the dehedral4 group (rotation by 90, horizontal and vertical flips) of order 8. Even for finite groups, there are already a lot of applications. In NLP, for certain languagues, there are masculine vs. feminine symmetries (a group of order 2). Since this equivarification method is not picky about original neural networks, it does seem to have a lot of applications. \n\nAs for other groups, Lemma 3.0.2 does not require any thing about finiteness. For a topological group, it is natural to require that the Z^{\\times G} in the definition 3.0.1 to be the space of continuous maps from G to Z; for a Lie group, we can use the space of smooth maps from G to Z; for non-compact Lie groups, we can use the space of compactly supported smooth maps from G to Z. It is the implementation part that is less trivial. For implementation purpose, all the continuous groups need to be approximated by a finite thing (maybe a subgroup). We are fine with the approximation, but of course, after this approximation, we should only hope approximately equivariance instead of strictly equivariance. We did not set up the math framework to allow approximately equivariant. (A type of neural network is approximately equivariant, if as we approximate the group finer and finer, it becomes more and more equivariant.) All we wanted for this paper is to set up some simple foundations for equivariant neural network, and then people build more sophiscated things on top of it. Also for non-compact Lie groups, the approximate equivariant neural network is more subtle. \n\n\nMinor comments:\n\ni) \nA. Thanks a lot! We will change accordingly.\n\nii) \nA. Sorry for the terrible typo.\n\niii)\nA. Sorry for the terrible typo.\n\niv) \nA. Will change accordingly.\n\nv) \nA. In the new code in github, we correct it to agree with the paper.\n\nvi) \nA. Will change accordingly."}, "signatures": ["ICLR.cc/2020/Conference/Paper82/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper82/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"abstract": "A key difference from existing works is that our equivarification method can be applied without knowledge of the detailed functions of a layer in a neural network, and hence, can be generalized to any feedforward neural networks. Although the network size scales up, the constructed equivariant neural network does not increase the complexity of the network compared with the original one, in terms of the number of parameters. As an illustration, we build an equivariant neural network for image classification by equivarifying a convolutional neural network. Results show that our proposed method significantly reduces the design and training complexity, yet preserving the learning performance in terms of accuracy.", "title": "Equivariant neural networks and equivarification", "code": "https://github.com/symplecticgeometry/equivariant-neural-networks-and-equivarification", "keywords": ["equivariant", "invariant", "neural network", "equivarification"], "pdf": "/pdf/9dfd9da1e9291567a85e8be6233f74fa0369ff49.pdf", "authors": ["Erkao Bao", "Linqi Song"], "authorids": ["baoerkao@gmail.com", "linqi.song@cityu.edu.hk"], "paperhash": "bao|equivariant_neural_networks_and_equivarification", "original_pdf": "/attachment/41b9060ac63df6941c71f3ffc33f7b7fec3b8f69.pdf", "_bibtex": "@misc{\nbao2020equivariant,\ntitle={Equivariant neural networks and equivarification},\nauthor={Erkao Bao and Linqi Song},\nyear={2020},\nurl={https://openreview.net/forum?id=BkxDthVtvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BkxDthVtvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper82/Authors", "ICLR.cc/2020/Conference/Paper82/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper82/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper82/Reviewers", "ICLR.cc/2020/Conference/Paper82/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper82/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper82/Authors|ICLR.cc/2020/Conference/Paper82/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504176663, "tmdate": 1576860545305, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper82/Authors", "ICLR.cc/2020/Conference/Paper82/Reviewers", "ICLR.cc/2020/Conference/Paper82/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper82/-/Official_Comment"}}}, {"id": "r1xeNwATKH", "original": null, "number": 2, "cdate": 1571837736019, "ddate": null, "tcdate": 1571837736019, "tmdate": 1572972640892, "tddate": null, "forum": "BkxDthVtvS", "replyto": "BkxDthVtvS", "invitation": "ICLR.cc/2020/Conference/Paper82/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "In this paper, the authors propose a method for making a neural network equivariant. Their method also can be applied to make each layer equivariant too. \n\nStrengths:\n-- The paper is very well written and easy to follow with clear notation. \n\n-- The derivations seem to be correct.\n\n\nWeaknesses:\n-- The experiment is nice but very limited and does not demonstrate the benefits of having an equivariant network. For example, the authors do not report the accuracy of recovering the original (0) rotation.\n\n-- The novelty of the work is questionable. While the development is different, the final example for equivarification of a neural network is very similar to the existing works by Cohen and Welling.\n\n-- There are other works on equivarification that are missed by this paper. For example, consider the following paper:\nLenssen, J. E., Fey, M., & Libuschewski, P. (2018). Group equivariant capsule networks. In NeurIPS.\n\n-- The layer-wise equivariant method does have extra computational overheads.\n\n-- The fact that we have to specify the groups that we want to make the network equivariant with respect to is a limitation. The promise of capsule networks, in contrast, is to \"ideally\" learn the pose (variation) vectors in a data-driven way.\nSabour, S., Frosst, N., & Hinton, G. E. (2017). Dynamic routing between capsules. In NeurIPS.\n\n-- The following statements need more explanation:\n  * \"However, these may require extra training overhead as the augmented data increase.\""}, "signatures": ["ICLR.cc/2020/Conference/Paper82/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper82/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"abstract": "A key difference from existing works is that our equivarification method can be applied without knowledge of the detailed functions of a layer in a neural network, and hence, can be generalized to any feedforward neural networks. Although the network size scales up, the constructed equivariant neural network does not increase the complexity of the network compared with the original one, in terms of the number of parameters. As an illustration, we build an equivariant neural network for image classification by equivarifying a convolutional neural network. Results show that our proposed method significantly reduces the design and training complexity, yet preserving the learning performance in terms of accuracy.", "title": "Equivariant neural networks and equivarification", "code": "https://github.com/symplecticgeometry/equivariant-neural-networks-and-equivarification", "keywords": ["equivariant", "invariant", "neural network", "equivarification"], "pdf": "/pdf/9dfd9da1e9291567a85e8be6233f74fa0369ff49.pdf", "authors": ["Erkao Bao", "Linqi Song"], "authorids": ["baoerkao@gmail.com", "linqi.song@cityu.edu.hk"], "paperhash": "bao|equivariant_neural_networks_and_equivarification", "original_pdf": "/attachment/41b9060ac63df6941c71f3ffc33f7b7fec3b8f69.pdf", "_bibtex": "@misc{\nbao2020equivariant,\ntitle={Equivariant neural networks and equivarification},\nauthor={Erkao Bao and Linqi Song},\nyear={2020},\nurl={https://openreview.net/forum?id=BkxDthVtvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BkxDthVtvS", "replyto": "BkxDthVtvS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper82/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper82/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575382680139, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper82/Reviewers"], "noninvitees": [], "tcdate": 1570237757341, "tmdate": 1575382680150, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper82/-/Official_Review"}}}, {"id": "B1xIzwVAcB", "original": null, "number": 3, "cdate": 1572910861519, "ddate": null, "tcdate": 1572910861519, "tmdate": 1572972640847, "tddate": null, "forum": "BkxDthVtvS", "replyto": "BkxDthVtvS", "invitation": "ICLR.cc/2020/Conference/Paper82/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #4", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "Motivated by group action theory, this paper proposes a method to obtain \u2018equivariant\u2019 neural nets given trained one, where \u2018equivariant\u2019 refers to a network that gives identical output if certain symmetry of the dataset is performed on the input (for example, if we rotate a sample the predicted class should not change). \n\nAfter reading the paper, I don\u2019t understand the experiments section. In particular, it is not clear to me how the proposed method differs from regular data augmentation, as to my understanding, the input to conv1 is copied 4 times and performed rotation for 0, 90, 180 and 270 degrees and the 4 times increased number of parameters (in-depth) of conv1 are shared. Furthermore, the same rotations are performed to the input to the second layer-conv 2: as the augmentation is cyclic I don\u2019t understand why the authors perform this operation second time. Could the authors elaborate on this? After reading this section, I don\u2019t understand the proposed fine-tuning procedure (pre-train, finetune and test): (1) what is the accuracy of the pre-trained network that was started from? (2) how is the initial network fine-tuned and modified? (as the authors mention that during training the samples are not rotated). Also, I am confused with the first sentence on page 8: \u2018the complexity of the constructed network does not grow in terms of the number of parameters\u2019. It would be useful if the results in Fig. 4 are more clearly illustrated.\n\nIs the order or increased computation 4x4x4? It would be useful to compare the method (computation & performance) with a baseline where the dataset is enlarged with data augmentation. The authors mention in the introduction that this increases training overhead, whereas the proposed practical method increases the computation at inference as well as the memory footprint of the model and the forward pass. It would be useful if the authors compared empirically with baselines with (1) data augmentation (2) network with an increased number of parameters (same as the proposed one).\n\nIn summary, the idea of using group action theory seems interesting. However after reading the paper, it is not clear to me how the idea is carried out, and although the authors provide theoretical justification, it is not clear how this connects with the practical proposed method and whether it outperforms standard data augmentation (see above). Moreover, I find the writing of the paper quite unclear (see discussion above and examples below).\n\n- If digits 6 or 9 are rotated the label changes, how does the proposed method handle this?\n- page 8, conclusion: The authors claim that the proposed approach yields a \u2018significant reduction in the design and training complexity\u2019. I don\u2019t understand relative to what this comparison refers to, as the regular data augmentation approach is more straightforward in my opinion. Also, given that this is pointed as an important contribution, in my opinion, an empirical comparison must be done with such baselines (see above).\n- Page 1: it is mentioned that \u2018the number of parameters can be the same as the original network\u2019 but the experiments do not include such architecture. After reading the paper I don\u2019t understand how such a network can be implemented and whether it works.\n\n\u2014 Minor \u2014\n- Page 1 & 1par-Pg2: I don\u2019t understand what the authors mean by \u2018uniformly *across layers* of NN? \n- Page 2: In these existing works, \u2026 I don\u2019t understand this sentence\n- Page 2: our .. method use -> uses\n- Page 2: map over the orbits. I don\u2019t understand this\n- Page 2: the first truly equivariant NN. After reading Sec 5 I don\u2019t understand this point.\n- Sec. 4: how to equivarifying -> equivarify\n- Page 4: \u2018pick a generator\u2019, would recommend elaborating this term or only mentioning g as an element of G for clarity for readers unfamiliar with group theory\n- What is the testing accuracy if rotated for different angles than trained (e.g. 45 degrees)?"}, "signatures": ["ICLR.cc/2020/Conference/Paper82/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper82/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"abstract": "A key difference from existing works is that our equivarification method can be applied without knowledge of the detailed functions of a layer in a neural network, and hence, can be generalized to any feedforward neural networks. Although the network size scales up, the constructed equivariant neural network does not increase the complexity of the network compared with the original one, in terms of the number of parameters. As an illustration, we build an equivariant neural network for image classification by equivarifying a convolutional neural network. Results show that our proposed method significantly reduces the design and training complexity, yet preserving the learning performance in terms of accuracy.", "title": "Equivariant neural networks and equivarification", "code": "https://github.com/symplecticgeometry/equivariant-neural-networks-and-equivarification", "keywords": ["equivariant", "invariant", "neural network", "equivarification"], "pdf": "/pdf/9dfd9da1e9291567a85e8be6233f74fa0369ff49.pdf", "authors": ["Erkao Bao", "Linqi Song"], "authorids": ["baoerkao@gmail.com", "linqi.song@cityu.edu.hk"], "paperhash": "bao|equivariant_neural_networks_and_equivarification", "original_pdf": "/attachment/41b9060ac63df6941c71f3ffc33f7b7fec3b8f69.pdf", "_bibtex": "@misc{\nbao2020equivariant,\ntitle={Equivariant neural networks and equivarification},\nauthor={Erkao Bao and Linqi Song},\nyear={2020},\nurl={https://openreview.net/forum?id=BkxDthVtvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BkxDthVtvS", "replyto": "BkxDthVtvS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper82/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper82/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575382680139, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper82/Reviewers"], "noninvitees": [], "tcdate": 1570237757341, "tmdate": 1575382680150, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper82/-/Official_Review"}}}], "count": 13}