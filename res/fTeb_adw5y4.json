{"notes": [{"id": "fTeb_adw5y4", "original": "h8sYev-T5oo", "number": 2946, "cdate": 1601308326754, "ddate": null, "tcdate": 1601308326754, "tmdate": 1614985762530, "tddate": null, "forum": "fTeb_adw5y4", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Improving Calibration through the Relationship with Adversarial Robustness", "authorids": ["~Yao_Qin1", "~Xuezhi_Wang3", "~Alex_Beutel1", "~Ed_Chi1"], "authors": ["Yao Qin", "Xuezhi Wang", "Alex Beutel", "Ed Chi"], "keywords": ["Calibration", "Uncertainty Estimates", "Adversarial Robustness"], "abstract": "Neural networks lack adversarial robustness -- they are vulnerable to adversarial examples that through small perturbations to inputs cause incorrect predictions. Further, trust is undermined when models give miscalibrated uncertainty estimates, i.e.  the predicted probability is not a good indicator of how much we should trust our model.  In this paper, we study the connection between adversarial robustness and calibration on four classification networks and datasets. We find that the inputs for which the model is sensitive to small perturbations (are easily attacked) are more likely to have poorly calibrated predictions. Based on this insight, we examine if calibration can be improved by addressing those adversarially unrobust inputs. To this end, we propose Adversarial Robustness based Adaptive Label Smoothing (AR-AdaLS) that integrates the correlations of adversarial robustness and uncertainty into training by adaptively softening labels for an example based on how easily it can be attacked by an adversary. We find that our method, taking the adversarial robustness of the in-distribution data into consideration, leads to better calibration over the model even under distributional shifts. In addition, AR-AdaLS can also be applied to an ensemble model to further improve model's calibration.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "qin|improving_calibration_through_the_relationship_with_adversarial_robustness", "supplementary_material": "/attachment/bbfd4615961f5c2e4041661265d1d2d5060bcfe2.zip", "pdf": "/pdf/cdcef30fc5bc532a48436f64878059dd115ecd96.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=wfki-EBdjD", "_bibtex": "@misc{\nqin2021improving,\ntitle={Improving Calibration through the Relationship with Adversarial Robustness},\nauthor={Yao Qin and Xuezhi Wang and Alex Beutel and Ed Chi},\nyear={2021},\nurl={https://openreview.net/forum?id=fTeb_adw5y4}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "7IcLkvgoJZf", "original": null, "number": 1, "cdate": 1610040371293, "ddate": null, "tcdate": 1610040371293, "tmdate": 1610473962796, "tddate": null, "forum": "fTeb_adw5y4", "replyto": "fTeb_adw5y4", "invitation": "ICLR.cc/2021/Conference/Paper2946/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "This paper improves calibration of neural networks by investing its connection to adversarial robustness. Two reviewers suggested acceptance, and two did rejection. As the authors and some reviewers highlighted, AC also agreed that the correlation between adversarial robustness and calibration is interesting to explore. However, as R1 pointed out, AC also thinks that the experimental results are not strong enough to meet the high standard of ICLR, e.g., Mixup often outperforms the proposed method (without further post-processing) and the proposed method does not outperform the deep ensemble (although deep ensemble is expensive and both method can be combined). Due to this, AC doubts whether adversarial robustness is indeed the best way to improve calibration (it can be useful though). Hence, AC recommends rejection."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improving Calibration through the Relationship with Adversarial Robustness", "authorids": ["~Yao_Qin1", "~Xuezhi_Wang3", "~Alex_Beutel1", "~Ed_Chi1"], "authors": ["Yao Qin", "Xuezhi Wang", "Alex Beutel", "Ed Chi"], "keywords": ["Calibration", "Uncertainty Estimates", "Adversarial Robustness"], "abstract": "Neural networks lack adversarial robustness -- they are vulnerable to adversarial examples that through small perturbations to inputs cause incorrect predictions. Further, trust is undermined when models give miscalibrated uncertainty estimates, i.e.  the predicted probability is not a good indicator of how much we should trust our model.  In this paper, we study the connection between adversarial robustness and calibration on four classification networks and datasets. We find that the inputs for which the model is sensitive to small perturbations (are easily attacked) are more likely to have poorly calibrated predictions. Based on this insight, we examine if calibration can be improved by addressing those adversarially unrobust inputs. To this end, we propose Adversarial Robustness based Adaptive Label Smoothing (AR-AdaLS) that integrates the correlations of adversarial robustness and uncertainty into training by adaptively softening labels for an example based on how easily it can be attacked by an adversary. We find that our method, taking the adversarial robustness of the in-distribution data into consideration, leads to better calibration over the model even under distributional shifts. In addition, AR-AdaLS can also be applied to an ensemble model to further improve model's calibration.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "qin|improving_calibration_through_the_relationship_with_adversarial_robustness", "supplementary_material": "/attachment/bbfd4615961f5c2e4041661265d1d2d5060bcfe2.zip", "pdf": "/pdf/cdcef30fc5bc532a48436f64878059dd115ecd96.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=wfki-EBdjD", "_bibtex": "@misc{\nqin2021improving,\ntitle={Improving Calibration through the Relationship with Adversarial Robustness},\nauthor={Yao Qin and Xuezhi Wang and Alex Beutel and Ed Chi},\nyear={2021},\nurl={https://openreview.net/forum?id=fTeb_adw5y4}\n}"}, "tags": [], "invitation": {"reply": {"forum": "fTeb_adw5y4", "replyto": "fTeb_adw5y4", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040371279, "tmdate": 1610473962777, "id": "ICLR.cc/2021/Conference/Paper2946/-/Decision"}}}, {"id": "LV8EMmAsEY6", "original": null, "number": 4, "cdate": 1604020361737, "ddate": null, "tcdate": 1604020361737, "tmdate": 1607008971893, "tddate": null, "forum": "fTeb_adw5y4", "replyto": "fTeb_adw5y4", "invitation": "ICLR.cc/2021/Conference/Paper2946/-/Official_Review", "content": {"title": "Improving Calibration with Adversarial Information", "review": "\nThe author addressed my concerns. I\u2019ll keep the score 6.\n\n=======================\n\nSummary:\n\nThe paper studied the relationship between adversarial robustness and calibration, then use the findings to improve label smoothing method. An adaptive label smoothing method (AR-AdaLS) is proposed to improve the calibration performance. Combining AR-AdaLS and deep ensemble can further improve the performance of deep ensemble method.\n\nStrength:\n\n1. The idea is novel and easy to understand. Experiments show the positive relationship between high adversarial robustness and better calibration. This work connects two field of studies.\n2. AR-AdaLS improves the performance of label smoothing. AR-AdaLS can also be used to improve deep ensemble.\n3. Improving calibration of deep neural networks is an important task.\n\nWeakness:\n\n1. Comparing the performances of LS and AR-AdaLS on CIFAR100 and CIFAR100-c, the improvements are actually not significant on CIFAR100, but AR-AdaLS is more computationally expensive. AR-AdaLS (on-the-fly) generates adversarial examples during training. This is very computationally expensive and will restrict the ability of the method to scale up to large dataset.\n2. AR-AdaLS does not perform better than deep ensemble, which is the state-of-the-art (claimed by the author).\n3. Three datasets are used in the paper but it seems that not all datasets are used to compare the performances of proposed model and other baselines.\n\n\nClarity and Correctness:\n\nThe paper is well written and easy to follow. The experiments look convincing. Boxplots comparison of ECE on CIFAR10 and ImageNet are provided.\n\nReproducibility:\n\nDetails of training and pseudocode is given but code is not available.\n\nQuestions:\n\nThe comparison of performances of ensemble models on CIFAR10 is provided but the comparisons on CIFAR100 and ImageNet are not. Why is that?\n\nConclusion:\n\nThe idea is novel and interesting but the empirical performance is not outstanding. Overall, I'm ok to accept the paper. I like the idea of using adversarial robustness to improve calibration. This paper reveals some interesting connections between the two things. \n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2946/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2946/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improving Calibration through the Relationship with Adversarial Robustness", "authorids": ["~Yao_Qin1", "~Xuezhi_Wang3", "~Alex_Beutel1", "~Ed_Chi1"], "authors": ["Yao Qin", "Xuezhi Wang", "Alex Beutel", "Ed Chi"], "keywords": ["Calibration", "Uncertainty Estimates", "Adversarial Robustness"], "abstract": "Neural networks lack adversarial robustness -- they are vulnerable to adversarial examples that through small perturbations to inputs cause incorrect predictions. Further, trust is undermined when models give miscalibrated uncertainty estimates, i.e.  the predicted probability is not a good indicator of how much we should trust our model.  In this paper, we study the connection between adversarial robustness and calibration on four classification networks and datasets. We find that the inputs for which the model is sensitive to small perturbations (are easily attacked) are more likely to have poorly calibrated predictions. Based on this insight, we examine if calibration can be improved by addressing those adversarially unrobust inputs. To this end, we propose Adversarial Robustness based Adaptive Label Smoothing (AR-AdaLS) that integrates the correlations of adversarial robustness and uncertainty into training by adaptively softening labels for an example based on how easily it can be attacked by an adversary. We find that our method, taking the adversarial robustness of the in-distribution data into consideration, leads to better calibration over the model even under distributional shifts. In addition, AR-AdaLS can also be applied to an ensemble model to further improve model's calibration.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "qin|improving_calibration_through_the_relationship_with_adversarial_robustness", "supplementary_material": "/attachment/bbfd4615961f5c2e4041661265d1d2d5060bcfe2.zip", "pdf": "/pdf/cdcef30fc5bc532a48436f64878059dd115ecd96.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=wfki-EBdjD", "_bibtex": "@misc{\nqin2021improving,\ntitle={Improving Calibration through the Relationship with Adversarial Robustness},\nauthor={Yao Qin and Xuezhi Wang and Alex Beutel and Ed Chi},\nyear={2021},\nurl={https://openreview.net/forum?id=fTeb_adw5y4}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "fTeb_adw5y4", "replyto": "fTeb_adw5y4", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2946/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538085505, "tmdate": 1606915764445, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2946/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2946/-/Official_Review"}}}, {"id": "141LpOkuuxm", "original": null, "number": 3, "cdate": 1603897224798, "ddate": null, "tcdate": 1603897224798, "tmdate": 1606497267361, "tddate": null, "forum": "fTeb_adw5y4", "replyto": "fTeb_adw5y4", "invitation": "ICLR.cc/2021/Conference/Paper2946/-/Official_Review", "content": {"title": "Missing baselines, incomplete experiments, only small and unclear benefits", "review": "The authors propose an extension to label smoothing, where the smoothing parameter is determined based on the miscalibration of the validation set. They then compare the performance of their method in terms of calibration under domain shift to a small set of baselines. \n\nI have 3 main concerns regarding missing baselines, missing experiments and only marginal benefits over state-of-the-art.\n\nMy first main concern is that the authors limit their comparison to rather dated baselines (2018 and older), when there has been a lot of active research in this field in the past year. In particular, I am disappointed that while the authors cite recent work on MixUp showing its benefits for calibration (Thulasidasan et al., NeurIPS 2019) they do not include MixUp as baseline (where also inputs are smoothed rather than labels only). Other notable recent work that should be included as baseline is Verified Uncertainty Calibration, Kumar et al., NeurIPS 2019, which has been shown to be superior to Temperature Scaling.  In addition, a baseline which also exploits links between calibration and adversarial, Stutz et al., ICML 2020, should be included as prior work.\nAnother aspect of the presented work is the exploration of constructing an ensemble of neural nets trained with label smoothing. In light of this, it is crucial to also compare the presented approach to Mix-n-Match, Zhang et al., ICML 2020, who present work on ensemble methods for uncertainty calibration using label smoothing. \n\n My other main concern is a whole set of missing experiments investigating calibration in truly OOD scenarios. Snook et al, on whose work this paper heavily builds, point out that in addition to the domain drift scenarios explored by the authors, it is crucial to investigate performance in truly OOD scenarios, where the test data is drawn from a distribution far a way from the training distribution. Since the model is per definition not able to make a correct prediction, in this scenario entropy is used to quantify model the quality of the predictive uncertainty. The authors should add these experiments for all datasets. \nIn this context, it would also interesting to investigate the quality of predictive uncertainty of OOD detection methods. While such comparisons are not always meaningful, a model strongly related to label smoothing/MixUp is Hendrycks et al., ICLR 2019, where a GAN is trained to learn OOD samples, where in MixUp inputs are smoothed to generate OOD samples. A comparison to this approach would also be interesting. \n\nFinally, even though important baselines as well as experiments for truly OOD scenarios are missing, benefits over a simple ensemble of vanillas remain unclear. For large-scale data (i.e. Imagenet), ensemble of Vanilla perform consistently better than the proposed method in terms of ECE under domain shift. For CIFAR-10, performance is comparable to deep ensembles and only for AR-AdaLS of Ensemble for CIFAR-10 there is a marginal improvement for a subset of perturbation strengths. I would have liked to see also evaluation of ECE under domain shift for CIFAR-100 and SVHN. Also missing is an evaluation of AR-AdaLS of Ensemble for Imagenet. Finally, results for non-image data, e.g. using a recurrent architecture (as in Snoek et al) are missing. \n\nOther concerns and open questions include: \nAlthough the authors explain the hyperparameters they use in the appendix, a detailed sensitivity analysis is missing to understand how sensitive the method is with respect to alpha and R. \n\nThe authors use \u201eEnsemble of Vanilla\u201c as baseline - however, Snoek et al. have shown that it is actually deep ensembles that perform best; in addition to the ensemble effect they are also trained using adversarials. What is the performance of actual deep ensembles rather than ensemble of vanilla?\n\nA conceptual concern is that in the proposed approach the validation set becomes part of the training set since it is used during training to update epsilon and thus is not anymore the independent set that may be necessary to tune other hyperparameters/for early stopping; I wonder whether this data leak may lead to problems related to overfitting?\n\nPlease report a proper scoring rule in addition ECE (e.g. Brier score).\n\nA minor point ist that different colors are used for AR-AdaLS  and Ensemble of Vanilla in figure 3 and figure 4.\n\nUnfortunately the authors do not provide any code, which make reproducibility difficult. \n\n\n\n####post rebuttal####\nThe contribution of this paper is marginal only. The link between adversarial robustness and calibration has been explored previously: Snoek et al. NeurIPS 2019 have shown that adversarial training as part of deep ensembles leads to better calibration under domain shift. Unfortunately the authors do not compare their approach to these deep ensembles, but only an ensemble of differently initialised vanilla networks without adversarial training, which they call deep ensembles (section 5.1). Also in terms of label smoothing the contribution is marginal: in their rebuttal the authors show that MixUp training - a different implementation of label smoothing combined with input smoothing - has a better performance than their method (ECE of 1.8 MixUp vs 2.3 their method for CIFAR-100); they do show that further post-processing improves their method, but this is likely true for MixUp too (results not shown). For ImageNet results for MixUP are not shown, nor for calibration under domain shift where MixUp is likely to perform well too.\n Taken together, this suggests that the link between adversarial robustness and calibration is mainly a link between OOD samples and calibration: generating OOD samples with input smoothing in MixUp works very well compared to the proposed approach, as does adversarial training in deep ensembles (both of which was shown in prior work). In summary, the proposed approach lacks novelty and performs worse than baselines for complex datasets.\n\nLack of code during the reviewing phase means it is not possible to review reproducibility of results.", "rating": "2: Strong rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper2946/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2946/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improving Calibration through the Relationship with Adversarial Robustness", "authorids": ["~Yao_Qin1", "~Xuezhi_Wang3", "~Alex_Beutel1", "~Ed_Chi1"], "authors": ["Yao Qin", "Xuezhi Wang", "Alex Beutel", "Ed Chi"], "keywords": ["Calibration", "Uncertainty Estimates", "Adversarial Robustness"], "abstract": "Neural networks lack adversarial robustness -- they are vulnerable to adversarial examples that through small perturbations to inputs cause incorrect predictions. Further, trust is undermined when models give miscalibrated uncertainty estimates, i.e.  the predicted probability is not a good indicator of how much we should trust our model.  In this paper, we study the connection between adversarial robustness and calibration on four classification networks and datasets. We find that the inputs for which the model is sensitive to small perturbations (are easily attacked) are more likely to have poorly calibrated predictions. Based on this insight, we examine if calibration can be improved by addressing those adversarially unrobust inputs. To this end, we propose Adversarial Robustness based Adaptive Label Smoothing (AR-AdaLS) that integrates the correlations of adversarial robustness and uncertainty into training by adaptively softening labels for an example based on how easily it can be attacked by an adversary. We find that our method, taking the adversarial robustness of the in-distribution data into consideration, leads to better calibration over the model even under distributional shifts. In addition, AR-AdaLS can also be applied to an ensemble model to further improve model's calibration.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "qin|improving_calibration_through_the_relationship_with_adversarial_robustness", "supplementary_material": "/attachment/bbfd4615961f5c2e4041661265d1d2d5060bcfe2.zip", "pdf": "/pdf/cdcef30fc5bc532a48436f64878059dd115ecd96.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=wfki-EBdjD", "_bibtex": "@misc{\nqin2021improving,\ntitle={Improving Calibration through the Relationship with Adversarial Robustness},\nauthor={Yao Qin and Xuezhi Wang and Alex Beutel and Ed Chi},\nyear={2021},\nurl={https://openreview.net/forum?id=fTeb_adw5y4}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "fTeb_adw5y4", "replyto": "fTeb_adw5y4", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2946/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538085505, "tmdate": 1606915764445, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2946/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2946/-/Official_Review"}}}, {"id": "h9-gyachAjU", "original": null, "number": 1, "cdate": 1603386983301, "ddate": null, "tcdate": 1603386983301, "tmdate": 1606491568440, "tddate": null, "forum": "fTeb_adw5y4", "replyto": "fTeb_adw5y4", "invitation": "ICLR.cc/2021/Conference/Paper2946/-/Official_Review", "content": {"title": "a useful connection and compelling results but not fully explored", "review": "Summary: This paper proposes a new method (AR-AdaLS) for label smoothing to improve deep network calibration. In particular, the authors draw a connection between lack of calibration (overconfidence) and examples which are prone to adversarial attacks. They show that by generating smoothed targets based on the adversarial robustness of an example, they can further improve model calibration beyond traditional label smoothing.\n\nPros: The paper demonstrates a clear connection between per-example calibration error (in general, overconfidence) and a lack of adversarial robustness. \n\nMethod and results are clear and seem to be well situated in the literature.\n\nThey demonstrate the outperformance of their method relative to Label Smoothing in calibrating across several datasets, model architectures, and domain shifts. \n\nCons: I'm not an expert in this field, but the novelty of the adversarial robustness - overconfidence connection is not entirely clear to me. Muller et al. describe the behavior of label smoothing as necessarily reducing the difference between logits of two classes and thereby the confidence of the prediction, particularly for outliers. This would seem to be the same effect you describe, but perhaps less explicitly.\n\nThe methods used as baselines, label smoothing and temperature scaling, are attractive for their simplicity and ease of training. While you note that your model has the same latency at inference, it seems important to know how much extra computation is required in training.\n\nThe improvement of AR-AdaLS relative to vanilla ensemble (and ensemble vanilla relative to AR-AdaLS of ensemble) is meaningfully different for ImageNet. You mention this may be due to larger dataset/less overfitting but it would be useful to better understand this - how do other forms of regularization relate to AR-AdaLS?\n\nI find the discussion of Figure 5 somewhat unsatisfying - it's not entirely clear to me why averaging the predictions of 5 well-calibrated models results in a miscalibrated model\n\nWhat is sensitivity to hyperparameter R? \n\nMinor:\n\nComplementary, not complimentary\n\nIn related work: multiclasss", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2946/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2946/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improving Calibration through the Relationship with Adversarial Robustness", "authorids": ["~Yao_Qin1", "~Xuezhi_Wang3", "~Alex_Beutel1", "~Ed_Chi1"], "authors": ["Yao Qin", "Xuezhi Wang", "Alex Beutel", "Ed Chi"], "keywords": ["Calibration", "Uncertainty Estimates", "Adversarial Robustness"], "abstract": "Neural networks lack adversarial robustness -- they are vulnerable to adversarial examples that through small perturbations to inputs cause incorrect predictions. Further, trust is undermined when models give miscalibrated uncertainty estimates, i.e.  the predicted probability is not a good indicator of how much we should trust our model.  In this paper, we study the connection between adversarial robustness and calibration on four classification networks and datasets. We find that the inputs for which the model is sensitive to small perturbations (are easily attacked) are more likely to have poorly calibrated predictions. Based on this insight, we examine if calibration can be improved by addressing those adversarially unrobust inputs. To this end, we propose Adversarial Robustness based Adaptive Label Smoothing (AR-AdaLS) that integrates the correlations of adversarial robustness and uncertainty into training by adaptively softening labels for an example based on how easily it can be attacked by an adversary. We find that our method, taking the adversarial robustness of the in-distribution data into consideration, leads to better calibration over the model even under distributional shifts. In addition, AR-AdaLS can also be applied to an ensemble model to further improve model's calibration.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "qin|improving_calibration_through_the_relationship_with_adversarial_robustness", "supplementary_material": "/attachment/bbfd4615961f5c2e4041661265d1d2d5060bcfe2.zip", "pdf": "/pdf/cdcef30fc5bc532a48436f64878059dd115ecd96.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=wfki-EBdjD", "_bibtex": "@misc{\nqin2021improving,\ntitle={Improving Calibration through the Relationship with Adversarial Robustness},\nauthor={Yao Qin and Xuezhi Wang and Alex Beutel and Ed Chi},\nyear={2021},\nurl={https://openreview.net/forum?id=fTeb_adw5y4}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "fTeb_adw5y4", "replyto": "fTeb_adw5y4", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2946/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538085505, "tmdate": 1606915764445, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2946/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2946/-/Official_Review"}}}, {"id": "yJdruPs01-6", "original": null, "number": 6, "cdate": 1606193726716, "ddate": null, "tcdate": 1606193726716, "tmdate": 1606288578560, "tddate": null, "forum": "fTeb_adw5y4", "replyto": "141LpOkuuxm", "invitation": "ICLR.cc/2021/Conference/Paper2946/-/Official_Comment", "content": {"title": "Response to Reviewer1", "comment": "1.Missing baselines: Thanks for your reviews but we would like to emphasize that the main contribution of our work is to reveal the correlation between two different research areas: adversarial robustness and calibration, and further we show we can use this correlation to improve calibration. Our main focus is not to develop a single method or model with state-of-the-art calibration but to demonstrate the value of new research work at the intersection of these areas and inspire others in this direction. Therefore, we believe the most important baseline method to understand the value of this insight is label smoothing, which we built our method on top of; we show a significant improvement of our method over standard label smoothing. \n\nFurther, we also follow your suggestion to compare mixup and AR-AdaLS and find that AR-AdaLS is better than mixup on ImageNet and comparable to mixup on CIFAR datasets, as shown in Table 1. In addition, since our method focuses on changing the label space, we observe that our method is complementary to many other state-of-the-art methods, e.g, ensembles (shown in the paper) and the post-processing calibration methods, e.g., scaling-binning method proposed in [2], as shown in Table 1: AR-AdaLS + ScaleBin.\n\nTable 1: ECE score (%) on the clean CIFAR10 and CIFAR100 dataset with WideResNet 28-10.\n\n| Dataset  | AR-AdaLS| AR-AdaLS + ScaleBin[2] | Mixup [1] | Mix-n-Match [4] | CCAT [3]  |\n|---|:---:|:----:|:---:|:---:| :---:|\n|CIFAR10| 0.6 | 0.1| 0.8| 1.0| 2.4|\n|CIFAR100| 2.3 |1.5| 1.8|2.8| 4.2| \n\nThe result of Mix-n-Match [4] and CCAT [3] are also reported in Table 1. We want to emphasize the difference between CCAT [3] and ours. CCAT [3] focuses on improving a model\u2019s adversarial robustness but our work mainly focuses on improving a model\u2019s calibration performance. Second, CCAT [3] builds their method on top of adversarial training while our method AR-AdaLS trains the model only with the clean training data rather than training on the adversarial examples. Therefore, CCAT [3] has a much lower clean accuracy compared to our method, e.g., CCAT: 94.6% vs. AR-AdaLS: 95.6% on CIFAR-10 and CCAT 75.3% vs. AR-AdaLS 79.2% on CIFAR-100.\n\n[1] Thulasidasan, S., et al, On mixup training: Improved calibration and predictive uncertainty for deep neural networks. NeurIPS  2019.\n\n[2] Kumar, A., et al, Verified uncertainty calibration. NeurIPS 2019.\n\n[3] Stutz, D., et al, Confidence-calibrated adversarial training: Generalizing to unseen attacks. ICML 2020.\n\n[4] Zhang, J., et al, Mix-n-Match: Ensemble and Compositional Methods for Uncertainty Calibration in Deep Learning. ICML 2020.\n\n2.OOD experiments: \nFirst, improving calibration and understanding its relationship with adversarial robustness is important independent of OOD applications. Many existing work focusing on calibration, e.g,  Verified uncertainty calibration. NeurIPS 2019 and Mix-n-Match, ICML 2020, do not include OOD experiments.\n\nSecond, we agree with the reviewer that our work is inspired by the work of (Snoek et al, Neurips 2019), but the major contribution and novelty of our work is to reveal the correlation between adversarial robustness and calibration, which is not discussed in the work of Snoek et al. \n\nAlthough OOD is not our focus, we still take the reviewe suggestions to includes experiments on OOD. Specifically, we plot the histogram of predictive entropy on out-of-distribution data of Vanilla, Label Smoothing and our AR-AdaLS. Each model is trained on CIFAR-10 and tested on CIFAR-100. Experimental results show that AR-AdaLS significantly reduces the number of low-entropy prediction on OOD data (see detailed discussion in Section \"Improvements on Out-of-Distribution Data\" on page 8).\n\n3.ECE under domain shifts for CIFAR-100 has originally been included in Table 2. \n\nThe results of ensembles on ImageNet are originally included in the appendix due to the space limitation and we move them into the main text in Table 4 as the final version allows one extra page. A full bar plot of ensembles results on ImageNet is shown in Figure 8 in Appendix.\n\nUnfortunately, there is no existing corrupted dataset for SVHN, but we believe the results on three datasets demonstrate the effectiveness of our algorithms.\n\n4.Sensitivity analysis: We perform a sensitivity analysis of hyperparameter R and found that the performance is robust when R is chosen between 4 to 16. We add a detailed plot of ECE on CIFAR-10 and CIFAR-10-C of AR-AdaLS with varying number of adversarial robustness subset R in Figure 4. In addition, we also include a discussion about sensitivity analysis at the bottom of Page 8 in the main text.\n\n5.\u201cEnsembles of Vanilla\u201d are equivalent to \u201cdeep ensembles\u201d as illustrated in Section 5.1 Baselines.\n\n6.Overfitting: First, we do not include any early stopping while training the model. Second, we did not observe any overfitting problem of our algorithm. \n\n7.Availability of the code: We will open-source the code once the work is accepted.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2946/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2946/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improving Calibration through the Relationship with Adversarial Robustness", "authorids": ["~Yao_Qin1", "~Xuezhi_Wang3", "~Alex_Beutel1", "~Ed_Chi1"], "authors": ["Yao Qin", "Xuezhi Wang", "Alex Beutel", "Ed Chi"], "keywords": ["Calibration", "Uncertainty Estimates", "Adversarial Robustness"], "abstract": "Neural networks lack adversarial robustness -- they are vulnerable to adversarial examples that through small perturbations to inputs cause incorrect predictions. Further, trust is undermined when models give miscalibrated uncertainty estimates, i.e.  the predicted probability is not a good indicator of how much we should trust our model.  In this paper, we study the connection between adversarial robustness and calibration on four classification networks and datasets. We find that the inputs for which the model is sensitive to small perturbations (are easily attacked) are more likely to have poorly calibrated predictions. Based on this insight, we examine if calibration can be improved by addressing those adversarially unrobust inputs. To this end, we propose Adversarial Robustness based Adaptive Label Smoothing (AR-AdaLS) that integrates the correlations of adversarial robustness and uncertainty into training by adaptively softening labels for an example based on how easily it can be attacked by an adversary. We find that our method, taking the adversarial robustness of the in-distribution data into consideration, leads to better calibration over the model even under distributional shifts. In addition, AR-AdaLS can also be applied to an ensemble model to further improve model's calibration.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "qin|improving_calibration_through_the_relationship_with_adversarial_robustness", "supplementary_material": "/attachment/bbfd4615961f5c2e4041661265d1d2d5060bcfe2.zip", "pdf": "/pdf/cdcef30fc5bc532a48436f64878059dd115ecd96.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=wfki-EBdjD", "_bibtex": "@misc{\nqin2021improving,\ntitle={Improving Calibration through the Relationship with Adversarial Robustness},\nauthor={Yao Qin and Xuezhi Wang and Alex Beutel and Ed Chi},\nyear={2021},\nurl={https://openreview.net/forum?id=fTeb_adw5y4}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "fTeb_adw5y4", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2946/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2946/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2946/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2946/Authors|ICLR.cc/2021/Conference/Paper2946/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2946/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923842837, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2946/-/Official_Comment"}}}, {"id": "Kpv_WQZ-TZs", "original": null, "number": 5, "cdate": 1606191610768, "ddate": null, "tcdate": 1606191610768, "tmdate": 1606287707992, "tddate": null, "forum": "fTeb_adw5y4", "replyto": "h9-gyachAjU", "invitation": "ICLR.cc/2021/Conference/Paper2946/-/Official_Comment", "content": {"title": "To Reviewer2", "comment": "Thanks very much for your support of our work and we would love to address your concerns as follows:\n1. Q: Confusion of adversarial robustness-overconfidence connection. \nA: First, our AR-AdaLS is built on top of label smoothing. Therefore, the benefit of label smoothing introduced in Muller et al, that is to reduce the difference between logits of two classes, also holds true on AR-AdaLS. Second, the main difference between AR-AdaLS and label smoothing is that AR-AdaLS smooths the labels of the training data to **different** extents based on their adversarial robustness while standard label smoothing just smooths all the training labels to the **same** extent. Assigning different labels to different training data is highly motivated by the nice correlation we revealed between adversarial robustness and over-confident predictions. Specifically, we find that for those most unrobust data, their predictions are more likely to be overconfident; for those robust data, their predictions are more likely to be well-calibrated. Therefore, we need to assign a smaller confidence score of the labels to those unrobust training data.\n\n2. Q: Extra computation: \nA: As you point out, AR-AdaLS shares the same latency as the vanilla model or label smoothing at inference, which we believe is more important while deploying a model in practice. Second, the extra computation mainly comes from computing the adversarial robustness of each training and validation data, which involves constructing CW attacks. Generating CW attacks once for both training data and validation data takes around an extra half of the training time. We believe it is an interesting further research question if this adversarial robustness of examples can be approximated more efficiently via other adversarial attacks.\n\n3. Q: Discussion of Figure 5. A: Sorry for the confusion of Figure 5 (now Figure 7 in the new version) and we add more explanations in the main text. First, we analyze the effect of an ensemble model, which is to improve the accuracy and reduce the confidence resulting from the disagreement of the prediction of each single model in ensembles. This is validated by Figure 10 in Appendix, where we compare the accuracy and confidence of the predicted class between each single and the corresponding ensemble model for 5 different methods.  Therefore, the overall effect of deep ensembles is to drive a model to be less confident, demonstrated in Figure 7 that the lines of ensemble models move to the upper left. Based on this, when ensembles are applied to a well-calibrated method, the resultant model becomes under-confident and has a worse calibration performance. Further, that ensemble can hurt the calibration performance of data augmentation, e.g, mixup, is also observed in [1] and [2].\n\n4. Q: Sensitivity analysis of hyperparameter R. A: We perform a sensitivity analysis of hyperparameter R and found that the performance is robust when R is chosen between 4 to 16. We add a detailed plot of ECE on CIFAR-10 and CIFAR-10-C of AR-AdaLS\nwith varying number of adversarial robustness subset R in Figure 4. In addition, we also include a discussion about sensitivity analysis on the bottom of Page 8 in the main text.\n\n[1] Wen et al, 2020. Combining Ensembles and Data Augmentation can Harm your Calibration\n\n[2] Rahaman, R. and Thiery, A.H., 2020. Uncertainty Quantification and Deep Ensembles."}, "signatures": ["ICLR.cc/2021/Conference/Paper2946/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2946/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improving Calibration through the Relationship with Adversarial Robustness", "authorids": ["~Yao_Qin1", "~Xuezhi_Wang3", "~Alex_Beutel1", "~Ed_Chi1"], "authors": ["Yao Qin", "Xuezhi Wang", "Alex Beutel", "Ed Chi"], "keywords": ["Calibration", "Uncertainty Estimates", "Adversarial Robustness"], "abstract": "Neural networks lack adversarial robustness -- they are vulnerable to adversarial examples that through small perturbations to inputs cause incorrect predictions. Further, trust is undermined when models give miscalibrated uncertainty estimates, i.e.  the predicted probability is not a good indicator of how much we should trust our model.  In this paper, we study the connection between adversarial robustness and calibration on four classification networks and datasets. We find that the inputs for which the model is sensitive to small perturbations (are easily attacked) are more likely to have poorly calibrated predictions. Based on this insight, we examine if calibration can be improved by addressing those adversarially unrobust inputs. To this end, we propose Adversarial Robustness based Adaptive Label Smoothing (AR-AdaLS) that integrates the correlations of adversarial robustness and uncertainty into training by adaptively softening labels for an example based on how easily it can be attacked by an adversary. We find that our method, taking the adversarial robustness of the in-distribution data into consideration, leads to better calibration over the model even under distributional shifts. In addition, AR-AdaLS can also be applied to an ensemble model to further improve model's calibration.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "qin|improving_calibration_through_the_relationship_with_adversarial_robustness", "supplementary_material": "/attachment/bbfd4615961f5c2e4041661265d1d2d5060bcfe2.zip", "pdf": "/pdf/cdcef30fc5bc532a48436f64878059dd115ecd96.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=wfki-EBdjD", "_bibtex": "@misc{\nqin2021improving,\ntitle={Improving Calibration through the Relationship with Adversarial Robustness},\nauthor={Yao Qin and Xuezhi Wang and Alex Beutel and Ed Chi},\nyear={2021},\nurl={https://openreview.net/forum?id=fTeb_adw5y4}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "fTeb_adw5y4", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2946/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2946/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2946/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2946/Authors|ICLR.cc/2021/Conference/Paper2946/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2946/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923842837, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2946/-/Official_Comment"}}}, {"id": "hm9bMZC_qv1", "original": null, "number": 4, "cdate": 1606191449363, "ddate": null, "tcdate": 1606191449363, "tmdate": 1606287150532, "tddate": null, "forum": "fTeb_adw5y4", "replyto": "wfenQgdNXcc", "invitation": "ICLR.cc/2021/Conference/Paper2946/-/Official_Comment", "content": {"title": "To Reviewer4", "comment": "Thanks very much for your reviews and we answer your questions as follows:\n\n1.Q: Definition of adversarial robustness. \nA: We agree that calling models robust is more common, but measuring the robustness of examples has been discussed in a few recent works. As we mentioned in related work \u201cRecently, Carlini et al. (2019); Stock & Cisse\u00b4 (2018) define adversarial robustness as the minimum distance in the input domain required to change the model\u2019s output prediction.\u201d  In addition, we also emphasize it in the definition of adversarial robustness in Section 3. Please refer to [1] & [2], which inspired us to use \u201cadversarial robustness\u201d to represent the robustness of data.\n\n[1] Nicholas Carlini et al. Distribution density, tails, and outliers in machine learning: Metrics and applications. ArXiv 2019.\n\n[2] Pierre Stock & Moustapha Cisse. Convnets and imagenet beyond accuracy: Understanding mistakes and uncovering biases. \n ECCV 2018.\n\n2.Q: Why do we need to divide the data into subsets? \nA: Accuracy and ECE are both most meaningful to compute over sets of data \"for statistical significance\". For example, we can't use the accuracy of one data point since it'll be either 0 or 1, and it doesn't make sense to smooth the confidence to 0 if a data point is mis-classified. Therefore, we cannot present the relationship between $\\delta$ for each data point and its corresponding calibration score (ECE) (which takes accuracy as an input and thus cannot be computed for each data point).\nThe 10th set represents the most robust data as mentioned in the title of Figure 1.\n\n3.Q: Conclusion from the top row of Figure 1? \nA:  First, we want to point out that a model\u2019s calibration measures the alignment between a model\u2019s accuracy and confidence. A well-calibrated model means that the predicted confidence is aligned with the predicted accuracy. For example, if a model\u2019s accuracy is around 60%, then if the model\u2019s predicted confidence is 0.6, that means the model is well-calibrated; if the model\u2019s predicted confidence is greater than 0.6, that means the model is over-confident; if the model\u2019s predicted confidence is smaller than 0.6, that means the model is under-confident. As shown in the top row of Figure 1, we plot both the accuracy and confidence for each adversarial robustness subset. We can see that the gap between accuracy and confidence is not always the same for all the adversarial robustness subsets. For the test data with small adversarial robustness (with small adversarial robustness level), the confidence is significantly higher than the accuracy. However, for the robust test data (with high adversarial robustness level), confidence is well aligned with the accuracy. Therefore, we draw a conclusion at the end of the second paragraph in Section 3.1 Correlations, \u201cThis indicates that although vanilla classification models achieve the state-of-the-art accuracy, they tend to give over-confident predictions, especially for those unrobust data.\u201d\n\n4.Q: Difference between Calibration and OOD detection. \nA: As we mentioned above, a model\u2019s calibration performance measures the alignment between a model\u2019s accuracy and confidence. The predicted confidence of a well-calibrated model can tell us how much we should trust the model\u2019s prediction. Calibration can be tested both on clean test data or on the shifted test data (e.g., we test the model\u2019s calibration performance on the corrupted datasets, CIFAR10-C, CIFAR100-C and ImageNet-C. The corrupted datasets are constructed by applying different types of corruptions, e.g., noise, blur, weather and digital categories, to the clean test data). Instead, OOD detection is mainly focusing on detecting out-of-distribution data. A common way to test the performance of OOD detection is to train a model on one dataset, e.g., CIFAR-10,  and then test it on another dataset including different classes in the training dataset, e.g., CIFAR-100.  \n\nIn addition, we include OOD experiments in Figure 5 to show the effectiveness of AR-AdaLS even on fully OOD data by significantly reducing the number of low-entropy prediction on OOD data (see Section \"Improvements on Out-of-Distribution Data\" on page 8).\n\nThe most important evaluation metric for calibration is ECE (expected calibration error), which is used in our work. We would like to refer to the following two works as \u201ccanonical\u201d in calibration similar to our problem setting. The first one is \u201cOn calibration of modern neural networks. ICML 2017\u201d, and another one is \u201cCan you trust your model\u2019s uncertainty? evaluating predictive uncertainty under dataset shift. NeurIPS, 2019.\u201d The second paper is a benchmark paper which evaluates many calibration methods under data shift and concludes that deep ensembles have the best calibration performance. That is why we compare our method with ensembles and combine AR-AdaLS with ensembles to further improve the calibration performance.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2946/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2946/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improving Calibration through the Relationship with Adversarial Robustness", "authorids": ["~Yao_Qin1", "~Xuezhi_Wang3", "~Alex_Beutel1", "~Ed_Chi1"], "authors": ["Yao Qin", "Xuezhi Wang", "Alex Beutel", "Ed Chi"], "keywords": ["Calibration", "Uncertainty Estimates", "Adversarial Robustness"], "abstract": "Neural networks lack adversarial robustness -- they are vulnerable to adversarial examples that through small perturbations to inputs cause incorrect predictions. Further, trust is undermined when models give miscalibrated uncertainty estimates, i.e.  the predicted probability is not a good indicator of how much we should trust our model.  In this paper, we study the connection between adversarial robustness and calibration on four classification networks and datasets. We find that the inputs for which the model is sensitive to small perturbations (are easily attacked) are more likely to have poorly calibrated predictions. Based on this insight, we examine if calibration can be improved by addressing those adversarially unrobust inputs. To this end, we propose Adversarial Robustness based Adaptive Label Smoothing (AR-AdaLS) that integrates the correlations of adversarial robustness and uncertainty into training by adaptively softening labels for an example based on how easily it can be attacked by an adversary. We find that our method, taking the adversarial robustness of the in-distribution data into consideration, leads to better calibration over the model even under distributional shifts. In addition, AR-AdaLS can also be applied to an ensemble model to further improve model's calibration.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "qin|improving_calibration_through_the_relationship_with_adversarial_robustness", "supplementary_material": "/attachment/bbfd4615961f5c2e4041661265d1d2d5060bcfe2.zip", "pdf": "/pdf/cdcef30fc5bc532a48436f64878059dd115ecd96.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=wfki-EBdjD", "_bibtex": "@misc{\nqin2021improving,\ntitle={Improving Calibration through the Relationship with Adversarial Robustness},\nauthor={Yao Qin and Xuezhi Wang and Alex Beutel and Ed Chi},\nyear={2021},\nurl={https://openreview.net/forum?id=fTeb_adw5y4}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "fTeb_adw5y4", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2946/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2946/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2946/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2946/Authors|ICLR.cc/2021/Conference/Paper2946/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2946/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923842837, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2946/-/Official_Comment"}}}, {"id": "h9EMBVwMPl", "original": null, "number": 3, "cdate": 1606191203656, "ddate": null, "tcdate": 1606191203656, "tmdate": 1606286601855, "tddate": null, "forum": "fTeb_adw5y4", "replyto": "LV8EMmAsEY6", "invitation": "ICLR.cc/2021/Conference/Paper2946/-/Official_Comment", "content": {"title": "Response to Reviewer3", "comment": "We really appreciate that you noticed that our main contribution is revealing an interesting connection between two research areas and use this correlation to further improve calibration. We answer your questions as follows:\n\n1. Computational cost: We agree that generating adversarial examples on-the-fly would be more expensive, as we mention in the paper. But comparing standard label smoothing and AR-AdaLS (pre-computed), we have already seen the benefit of AR-AdaLS (pre-computed) over standard label smoothing, even on the large-scale dataset, e.g., Imagenet. In addition, we want to emphasize that AR-AdaLS does not have any latency in inference, which is more important when we deploy the model in practice.\n\n2. \u201cAR-AdaLS does not perform better than deep ensembles\u201d: First, deep ensembles would be very expensive in inference, which is one of the major limitations of deep ensembles. Second, if there are computational resources for deep ensembles, we show that AR-AdaLS is complementary to deep ensembles and can further improve the calibration performance of deep ensembles.\n\n3. Missing results on other datasets: We are sorry about this confusion but actually we included all the results on ImageNet in the appendix due to the space limitation, and we have moved the numerical result on ImageNet to the main text in Table 4 since one extra page is allowed. A full bar plot is shown in Figure 8 in appendix on CIFAR10 and ImageNet. Further, the result on CIFAR100 of ensemble is also included in Table 2. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2946/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2946/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improving Calibration through the Relationship with Adversarial Robustness", "authorids": ["~Yao_Qin1", "~Xuezhi_Wang3", "~Alex_Beutel1", "~Ed_Chi1"], "authors": ["Yao Qin", "Xuezhi Wang", "Alex Beutel", "Ed Chi"], "keywords": ["Calibration", "Uncertainty Estimates", "Adversarial Robustness"], "abstract": "Neural networks lack adversarial robustness -- they are vulnerable to adversarial examples that through small perturbations to inputs cause incorrect predictions. Further, trust is undermined when models give miscalibrated uncertainty estimates, i.e.  the predicted probability is not a good indicator of how much we should trust our model.  In this paper, we study the connection between adversarial robustness and calibration on four classification networks and datasets. We find that the inputs for which the model is sensitive to small perturbations (are easily attacked) are more likely to have poorly calibrated predictions. Based on this insight, we examine if calibration can be improved by addressing those adversarially unrobust inputs. To this end, we propose Adversarial Robustness based Adaptive Label Smoothing (AR-AdaLS) that integrates the correlations of adversarial robustness and uncertainty into training by adaptively softening labels for an example based on how easily it can be attacked by an adversary. We find that our method, taking the adversarial robustness of the in-distribution data into consideration, leads to better calibration over the model even under distributional shifts. In addition, AR-AdaLS can also be applied to an ensemble model to further improve model's calibration.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "qin|improving_calibration_through_the_relationship_with_adversarial_robustness", "supplementary_material": "/attachment/bbfd4615961f5c2e4041661265d1d2d5060bcfe2.zip", "pdf": "/pdf/cdcef30fc5bc532a48436f64878059dd115ecd96.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=wfki-EBdjD", "_bibtex": "@misc{\nqin2021improving,\ntitle={Improving Calibration through the Relationship with Adversarial Robustness},\nauthor={Yao Qin and Xuezhi Wang and Alex Beutel and Ed Chi},\nyear={2021},\nurl={https://openreview.net/forum?id=fTeb_adw5y4}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "fTeb_adw5y4", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2946/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2946/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2946/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2946/Authors|ICLR.cc/2021/Conference/Paper2946/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2946/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923842837, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2946/-/Official_Comment"}}}, {"id": "cxdtH_WrBi", "original": null, "number": 2, "cdate": 1606191085033, "ddate": null, "tcdate": 1606191085033, "tmdate": 1606286539804, "tddate": null, "forum": "fTeb_adw5y4", "replyto": "fTeb_adw5y4", "invitation": "ICLR.cc/2021/Conference/Paper2946/-/Official_Comment", "content": {"title": "General Response to All Reviewers", "comment": "1. The key contribution of this work is that we reveal the correlation between adversarial robustness and calibration, drawing a link between two different research areas. We believe this can spur further work at the intersection of these areas of research. Based on this correlation, we want to show that the idea of differentiating the training data based on their adversarial robustness is promising to improve model\u2019s calibration rather than pushing the results to be the best.\n\n2. Due to the space limitation, we did not include all the experiments (e.g., ensemble results on ImageNet) in the main text but instead including them in the appendix. Since one extra page is allowed for the final version, we have moved these results into the main text in Table 4. A full bar plot of ensemble result on ImageNet is in Figure 8 in Appendix. \n\n3. We will open-source all the code if the paper is accepted.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2946/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2946/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improving Calibration through the Relationship with Adversarial Robustness", "authorids": ["~Yao_Qin1", "~Xuezhi_Wang3", "~Alex_Beutel1", "~Ed_Chi1"], "authors": ["Yao Qin", "Xuezhi Wang", "Alex Beutel", "Ed Chi"], "keywords": ["Calibration", "Uncertainty Estimates", "Adversarial Robustness"], "abstract": "Neural networks lack adversarial robustness -- they are vulnerable to adversarial examples that through small perturbations to inputs cause incorrect predictions. Further, trust is undermined when models give miscalibrated uncertainty estimates, i.e.  the predicted probability is not a good indicator of how much we should trust our model.  In this paper, we study the connection between adversarial robustness and calibration on four classification networks and datasets. We find that the inputs for which the model is sensitive to small perturbations (are easily attacked) are more likely to have poorly calibrated predictions. Based on this insight, we examine if calibration can be improved by addressing those adversarially unrobust inputs. To this end, we propose Adversarial Robustness based Adaptive Label Smoothing (AR-AdaLS) that integrates the correlations of adversarial robustness and uncertainty into training by adaptively softening labels for an example based on how easily it can be attacked by an adversary. We find that our method, taking the adversarial robustness of the in-distribution data into consideration, leads to better calibration over the model even under distributional shifts. In addition, AR-AdaLS can also be applied to an ensemble model to further improve model's calibration.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "qin|improving_calibration_through_the_relationship_with_adversarial_robustness", "supplementary_material": "/attachment/bbfd4615961f5c2e4041661265d1d2d5060bcfe2.zip", "pdf": "/pdf/cdcef30fc5bc532a48436f64878059dd115ecd96.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=wfki-EBdjD", "_bibtex": "@misc{\nqin2021improving,\ntitle={Improving Calibration through the Relationship with Adversarial Robustness},\nauthor={Yao Qin and Xuezhi Wang and Alex Beutel and Ed Chi},\nyear={2021},\nurl={https://openreview.net/forum?id=fTeb_adw5y4}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "fTeb_adw5y4", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2946/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2946/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2946/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2946/Authors|ICLR.cc/2021/Conference/Paper2946/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2946/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923842837, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2946/-/Official_Comment"}}}, {"id": "wfenQgdNXcc", "original": null, "number": 2, "cdate": 1603713009983, "ddate": null, "tcdate": 1603713009983, "tmdate": 1605024099643, "tddate": null, "forum": "fTeb_adw5y4", "replyto": "fTeb_adw5y4", "invitation": "ICLR.cc/2021/Conference/Paper2946/-/Official_Review", "content": {"title": "Good motivation, some parts remain unclear", "review": "### Main contribution\n1. The author find that unrobust data may cause worse calibration.\n2. This work propose to encourage models to be adversarial robust to help calibration with a modified version of label smoothing: AR-AdaLS\n\n### Clarity\n1. The introduction of the backgrounds for uncertainty estimates is not very clear\n2. The proposal of AR-AdaLS is clear and reasonable\n3. The argument of \"that unrobust data may cause worse calibration\" is not fully demonstrated in Sec 3. \n\n### Questions\n1. In adversarial learning area, people usually refer \"robustness\" for models rather than data. We can say a model is robust or not. But it is a little bit confusing when you apply the term on data. Can the author provide references of other canonical works that also use this phrase?\n2. In Sec 3, the author stated they consider adversarial perturbation \u03b4 as a measure of robustness. Then why bother to divide the data into ten sets based on \u03b4 in your most import illustrations, which is Fig 1. What will happen if you directly present the relationship between \u03b4 and score? Does the 10th set represents the most robust data or does the 1st set do?\n3. What conclusion can we draw from the first row of Fig 1? How does it relate to the argument that \"the model is sensitive to small perturbations are more likely to have poorly calibrated predictions\"?\n4. I am familiar with the area of adversarial robustness and out-of-distribution detection, but not very confident about uncertainty estimates. I also did not find any specific introduction or helpful information in Sec 2. It seems to me that this area is not very popular and you can only compare your method with a few of other works, most of which are not even proposed for calibration in the first place (Label Smoothing/Mixup/Temperature Scaling). So can the author illustrate how the area of CALIBRATION differs from OOD detection? What is the most important evaluation metric for this problem? And maybe tell me which one is the most canonical work in this area so that I can refer to it as a comparison.\n\nI will raise my score if the author can fully address my questions.", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2946/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2946/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improving Calibration through the Relationship with Adversarial Robustness", "authorids": ["~Yao_Qin1", "~Xuezhi_Wang3", "~Alex_Beutel1", "~Ed_Chi1"], "authors": ["Yao Qin", "Xuezhi Wang", "Alex Beutel", "Ed Chi"], "keywords": ["Calibration", "Uncertainty Estimates", "Adversarial Robustness"], "abstract": "Neural networks lack adversarial robustness -- they are vulnerable to adversarial examples that through small perturbations to inputs cause incorrect predictions. Further, trust is undermined when models give miscalibrated uncertainty estimates, i.e.  the predicted probability is not a good indicator of how much we should trust our model.  In this paper, we study the connection between adversarial robustness and calibration on four classification networks and datasets. We find that the inputs for which the model is sensitive to small perturbations (are easily attacked) are more likely to have poorly calibrated predictions. Based on this insight, we examine if calibration can be improved by addressing those adversarially unrobust inputs. To this end, we propose Adversarial Robustness based Adaptive Label Smoothing (AR-AdaLS) that integrates the correlations of adversarial robustness and uncertainty into training by adaptively softening labels for an example based on how easily it can be attacked by an adversary. We find that our method, taking the adversarial robustness of the in-distribution data into consideration, leads to better calibration over the model even under distributional shifts. In addition, AR-AdaLS can also be applied to an ensemble model to further improve model's calibration.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "qin|improving_calibration_through_the_relationship_with_adversarial_robustness", "supplementary_material": "/attachment/bbfd4615961f5c2e4041661265d1d2d5060bcfe2.zip", "pdf": "/pdf/cdcef30fc5bc532a48436f64878059dd115ecd96.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=wfki-EBdjD", "_bibtex": "@misc{\nqin2021improving,\ntitle={Improving Calibration through the Relationship with Adversarial Robustness},\nauthor={Yao Qin and Xuezhi Wang and Alex Beutel and Ed Chi},\nyear={2021},\nurl={https://openreview.net/forum?id=fTeb_adw5y4}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "fTeb_adw5y4", "replyto": "fTeb_adw5y4", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2946/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538085505, "tmdate": 1606915764445, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2946/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2946/-/Official_Review"}}}], "count": 11}