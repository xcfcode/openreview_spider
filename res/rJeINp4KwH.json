{"notes": [{"id": "rJeINp4KwH", "original": "Hyx-Eb7vPB", "number": 488, "cdate": 1569439022497, "ddate": null, "tcdate": 1569439022497, "tmdate": 1583912025436, "tddate": null, "forum": "rJeINp4KwH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Population-Guided Parallel Policy Search for Reinforcement Learning", "authors": ["Whiyoung Jung", "Giseung Park", "Youngchul Sung"], "authorids": ["wy.jung@kaist.ac.kr", "gs.park@kaist.ac.kr", "ycsung@kaist.ac.kr"], "keywords": ["Reinforcement Learning", "Parallel Learning", "Population Based Learning"], "abstract": "In this paper, a new population-guided parallel learning scheme is proposed to enhance the performance of off-policy reinforcement learning (RL). In the proposed scheme, multiple identical learners with their own value-functions and policies share a common experience replay buffer, and search a good policy in collaboration with the guidance of the best policy information. The key point is that the information of the best policy  is fused in a soft manner by constructing an augmented loss function for policy update to enlarge the overall search region by the multiple learners. The guidance by the previous best policy and the enlarged  range enable faster and better policy search, and monotone improvement of the expected cumulative return by the proposed scheme is proved theoretically. Working algorithms are constructed by applying the proposed scheme to the twin delayed deep deterministic (TD3) policy gradient algorithm, and numerical results show that the constructed P3S-TD3 outperforms most of the current state-of-the-art RL algorithms, and the gain is significant in the case of sparse reward environment.", "pdf": "/pdf/17b46ff13242a110aae198cfeb220153596d2b1d.pdf", "paperhash": "jung|populationguided_parallel_policy_search_for_reinforcement_learning", "_bibtex": "@inproceedings{\nJung2020Population-Guided,\ntitle={Population-Guided Parallel Policy Search for Reinforcement Learning},\nauthor={Whiyoung Jung and Giseung Park and Youngchul Sung},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rJeINp4KwH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/c739eba4d6d269e6f0a0c23db7b72eb20a83ce2a.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "swMAvtPNLo", "original": null, "number": 1, "cdate": 1576798697938, "ddate": null, "tcdate": 1576798697938, "tmdate": 1576800937847, "tddate": null, "forum": "rJeINp4KwH", "replyto": "rJeINp4KwH", "invitation": "ICLR.cc/2020/Conference/Paper488/-/Decision", "content": {"decision": "Accept (Poster)", "comment": "The  paper proposes a new approach to multi-actor RL, which ensure diversity and performance of the population of actors, by distilling the policy of the best performing agent in a soft way and maintaining some distance between the agents. The authors show improved performance over several state-of-the-art mono-actor algorithms and over several other multi-actor RL algorithms.  Initially, reviewers were concerned with magnitude of the contribution/novelty, as well as some technical issues (e.g. the beta update), and relative lack of baseline comparisons.  However, after discussion the reviewers largely agree that their main concerns have been addressed.  Therefore, I recommend this paper for acceptance.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Population-Guided Parallel Policy Search for Reinforcement Learning", "authors": ["Whiyoung Jung", "Giseung Park", "Youngchul Sung"], "authorids": ["wy.jung@kaist.ac.kr", "gs.park@kaist.ac.kr", "ycsung@kaist.ac.kr"], "keywords": ["Reinforcement Learning", "Parallel Learning", "Population Based Learning"], "abstract": "In this paper, a new population-guided parallel learning scheme is proposed to enhance the performance of off-policy reinforcement learning (RL). In the proposed scheme, multiple identical learners with their own value-functions and policies share a common experience replay buffer, and search a good policy in collaboration with the guidance of the best policy information. The key point is that the information of the best policy  is fused in a soft manner by constructing an augmented loss function for policy update to enlarge the overall search region by the multiple learners. The guidance by the previous best policy and the enlarged  range enable faster and better policy search, and monotone improvement of the expected cumulative return by the proposed scheme is proved theoretically. Working algorithms are constructed by applying the proposed scheme to the twin delayed deep deterministic (TD3) policy gradient algorithm, and numerical results show that the constructed P3S-TD3 outperforms most of the current state-of-the-art RL algorithms, and the gain is significant in the case of sparse reward environment.", "pdf": "/pdf/17b46ff13242a110aae198cfeb220153596d2b1d.pdf", "paperhash": "jung|populationguided_parallel_policy_search_for_reinforcement_learning", "_bibtex": "@inproceedings{\nJung2020Population-Guided,\ntitle={Population-Guided Parallel Policy Search for Reinforcement Learning},\nauthor={Whiyoung Jung and Giseung Park and Youngchul Sung},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rJeINp4KwH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/c739eba4d6d269e6f0a0c23db7b72eb20a83ce2a.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "rJeINp4KwH", "replyto": "rJeINp4KwH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795711870, "tmdate": 1576800261143, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper488/-/Decision"}}}, {"id": "r1lL6hgdKH", "original": null, "number": 3, "cdate": 1571454142221, "ddate": null, "tcdate": 1571454142221, "tmdate": 1574302546660, "tddate": null, "forum": "rJeINp4KwH", "replyto": "rJeINp4KwH", "invitation": "ICLR.cc/2020/Conference/Paper488/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #3", "review": "The authors propose another method of doing population-based training of RL policies. During the training process, there are N workers running in N copies of the environment, each with different parameter settings for the policies and value networks. Each worker pushes data to a shared replay buffer of experience. The paper claims that a natural approach is to have a chief job periodically poll for the best worker, then replace the weights of each worker with the best one. Whenever this occurs, this reduces the diversity within the population.\n\nIn its place, the authors propose a soft-update in the chief. At every merging cycle, the chief queries which worker performs best. If that worker is worker B, it emits pi_B's parameters to each of the other workers. Instead of replacing the parameters exactly, worker i's loss is then augmented by beta * D(pi_i, pi_B), where D is some distance measure that is measured over states sampled from the replay buffer. The \"soft\" update encourages individual workers to match pi_B without directly replacing their parameters, which maintains diversity in the population. In this work, pi is always represented by a deterministic policy and D is the mean-squared-error in action space (this is argued as equivalent to the KL divergence between the two policies if the policies were represented by Gaussian with the same, constant standard deviation). The beta parameter is updated online using heuristics based on how D(pi_i, pi_B) compares to D(pi_i, old_pi_i). Using TD3 as a base algorithm, the population-based version performs better, and there are ablations for various parts of the population algorithm.\n\nI thought this paper was interesting, but thought it was strange that there were very few comparisons to other population / ensemble-based training methods. In particular they mention the copying problem as a downside of population-based training (PBT), but do not compare against PBT at all. Additionally, my understanding of PBT is that when they replace bad agents with the best agent, they only replace the worst performing agents (not all of them), and they additionally add some random perturbations to their hyperparameter settings. This goes counter to the claim that they collapse the population to a single point- by my reading the exploration step avoids this collapse.\n\nAn experiment I'd like to see is trying PBT, where different workers do in fact use different hyperparameters. My understanding is that in P3S-TD3 there is a single hyperparameter setting shared across all workers (plus some hyperparameters deciding the soft update).\n\nI'd also like to see ablations for the Resetting variant (Re-TD3), where only the bottom half or 2/3rds of the workers are reset. This would give empirical evidence for the \"population collapse\" intuition - we should expect to see some improvements if we avoid totally collapsing the population, while still copying enough to partially exploit the current best setting.\n\nMany inequalities in the paper are argued by compare the expectation of negative Q of one policy to the negative Q of another - I believe the derivations would be much easier to follow if the authors simply multiplied all sides by -1 and adjusted inequalities accordingly. It is much easier to think about Q-value-1 > Q-value-2 rather than -Q-value-1 < -Q-value-2 when trying to interpret what the equation is saying.\n\nFor related work, papers on evolutionary strategies and the various self-play-in-a-population papers seem relevant, since these often take the form of having each worker i do a different perturbation that is later merged by a chief.\n\nIn Figure 4 it feels weird that results are the regular Mujoco envs for 2 problems and the delayed envs for the other 2 problems. When looking at the appendix, it's rather clearly cherry picked to show the best results in favor of PS3-TD3. I would prefer the Delayed MuJoCo experiments be in a separate figure, or to include the TD3/SAC/ACKTR/PPO/etc. results for the delayed envs as well (these don't appear to be in the appendix)\n\nOn the theoretical results: the 1st assumption seems very strong. The first assumption argues that pi_B is always 1-step better than pi_old for every state. That assumption already takes you very far towards arguing \"updating pi_old to pi_B is good\". The 2nd assumption is more reasonable but I'm confused how rho and d play into the theoretical results. Do they play any role in how much the policy is expected to improve, or do the constants just need to exist?\n\nThe last comment on the theory side is that I still don't understand the intuition for why we want to learn beta such that \n\nKL(pi_new || pi_b) = max {rho * KL_max(pi_new || pi_old), d}\n\nIn the practical algorithm, beta is updated online to increase / decrease the importance of the \"match pi_B\" term if the ratio between the two strays too far from 1 (with the threshold set to [1/1.5, 1.5] in a manner similar to PPO's approach). But why should it be important for the two values to be close to one another? Let me write out the derivation continuing from Eqn (57) in the appendix.\n\nWith a substitution that doesn't use (c) to drop the beta * (KL - KL) term, we get\n\nE_{pi_b}[-Q_new] >= E_{pi_new}[-Q_new] + beta * (KL - KL)\n-->\nE_{pi_new}[Q_new] >= E_{pi_b}[Q_new] + beta * (KL - KL)\n\nThen, in Theorem 1, we recursively apply this inequality, accumulating a number of beta * (KL - KL) terms. In the end we get\n\nQ_new >= (discounted sum rewards from pi_b) + (discounted sum of beta * (KL - KL) with expectation over states from pi_b)\n= Q_pi_b + (sum of beta *(KL - KL) terms)\n\nBy my reading, shouldn't this mean we want KL(pi_new || pi_b) - max {rho * KL_max(pi_new || pi_old), d} to be as large as possible, rather than 0? The more positive this term is, the more improvement we get between Q_new and Q_pi_b.\n\n--------------------------\n\nOverall, this feels like a good paper, but I'm not too familiar with prior empirical results for population-based RL methods. The ablations suggested that pretty much any reasonable population-based method outperformed using a single worker, and because of this it seems especially important to have ablations to other population-based prior work, rather than just variants of its own method. \n\nI would be okay with this paper as-is despite some of its flaws, but think it could be better pending rebuttal.\n\nEdit: I have read the author reply and other reviews. I do not plan to change my rating but do think the paper is improved by the added baselines and better explanation of what the beta adaptation rule is doing. I would ask the authors to make sure this description is as clear as possible, as the argued improvement gap seems central to the work.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper488/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper488/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Population-Guided Parallel Policy Search for Reinforcement Learning", "authors": ["Whiyoung Jung", "Giseung Park", "Youngchul Sung"], "authorids": ["wy.jung@kaist.ac.kr", "gs.park@kaist.ac.kr", "ycsung@kaist.ac.kr"], "keywords": ["Reinforcement Learning", "Parallel Learning", "Population Based Learning"], "abstract": "In this paper, a new population-guided parallel learning scheme is proposed to enhance the performance of off-policy reinforcement learning (RL). In the proposed scheme, multiple identical learners with their own value-functions and policies share a common experience replay buffer, and search a good policy in collaboration with the guidance of the best policy information. The key point is that the information of the best policy  is fused in a soft manner by constructing an augmented loss function for policy update to enlarge the overall search region by the multiple learners. The guidance by the previous best policy and the enlarged  range enable faster and better policy search, and monotone improvement of the expected cumulative return by the proposed scheme is proved theoretically. Working algorithms are constructed by applying the proposed scheme to the twin delayed deep deterministic (TD3) policy gradient algorithm, and numerical results show that the constructed P3S-TD3 outperforms most of the current state-of-the-art RL algorithms, and the gain is significant in the case of sparse reward environment.", "pdf": "/pdf/17b46ff13242a110aae198cfeb220153596d2b1d.pdf", "paperhash": "jung|populationguided_parallel_policy_search_for_reinforcement_learning", "_bibtex": "@inproceedings{\nJung2020Population-Guided,\ntitle={Population-Guided Parallel Policy Search for Reinforcement Learning},\nauthor={Whiyoung Jung and Giseung Park and Youngchul Sung},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rJeINp4KwH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/c739eba4d6d269e6f0a0c23db7b72eb20a83ce2a.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rJeINp4KwH", "replyto": "rJeINp4KwH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper488/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper488/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575427275336, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper488/Reviewers"], "noninvitees": [], "tcdate": 1570237751404, "tmdate": 1575427275350, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper488/-/Official_Review"}}}, {"id": "B1e8Q9IiuH", "original": null, "number": 2, "cdate": 1570626077966, "ddate": null, "tcdate": 1570626077966, "tmdate": 1574251104766, "tddate": null, "forum": "rJeINp4KwH", "replyto": "rJeINp4KwH", "invitation": "ICLR.cc/2020/Conference/Paper488/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #1", "review": "The  paper proposes a new approach to multi-actor RL, which ensure diversity and performance of the population of actors, by distilling the policy of the best performing agent in a soft way and maintaining some distance between the agents. The authors show improved performance over several state-of-the-art mono-actor algorithms and over several other multi-actor RL algorithms.\n\nI'm in favor of accepting the paper despite a few serious weaknesses described below.\n\nA good point of the paper is the related work section which provides a good and concise survey of various multi-actor RL approaches: distributed RL, population-based training and Guided Policy Search (the last part about exploiting best information looks less relevant).\n\nHere a set of random  remarks:\n\nAbout  population-based training, there is quite a lot of repetition between the introduction and the related work  section.\n\nThe P3S approach described in the beginning of Section 3 and the end of Section 3.2 seems to rely on some arbitrary choices and a few hyperparameters whose impact is not much studied.\n\nIn the theoretical study (Section 3.1), a KL divergence is used as a distance between policies, which implies stochastic policies. But P3S is used on top of TD3, where policies are deterministic. This should definitely be discussed.\n\nThe role of \\beta in the theory is not put forward in a way to make the point of Section 3.2 clear. The theory should be clarified in this respect.\n\nThe top of p5 is made of a unique sentence over 7 lines which is completely obfuscating. This part must be rewritten and much clarified.\n\nI would be glad to see the performance on Swimmer, as this benchmark is known to suffer from  a deceiptive gradient.\n\nThe fact that a negative cost of action in Ant-v1 can result in no action in this environment is reminiscent of the same effect shown in the simpler Continuous Mountain Car environment in the Gep-PG paper (Colas, Oudeyer and Sigaud, ICML 2018). Actually, moving to simpler benchmarks would make it possible to provide more detailed empirical studies of the inner mechanisms of the P3S approach.\n\n p6: \"The policies used for evaluation are stochastic for PPO and ACKTR, and deterministic for the others.\" Do you mean you used a deterministic policy for SAC? This would be unusual, as SAC with a deterministic policy is very close to TD3.\n\n p6: \"In Fig.   3,  it is first observed that the performance of TD3 here is similar to that in the originalTD3 paper (Fujimoto et al. (2018)), and the performance of other baseline algorithms is also similar to that in the original papers (Schulman et al. (2017); Haarnoja et al. (2018))\". This sentence can be much compressed: \"In Fig.   3,  it is observed that all other baseline algorithms is also similar to that in the original papers (Fujimoto et al. (2018),Schulman et al. (2017); Haarnoja et al. (2018))\". \n\nWhy did you use the v1 versions of the benchmarks, and not the v2?\n\nI did not check the proofs in appendix.\n\ntypos:\n\np2: hyperparamters\np7: is the way how the best =>  is the way the best...\np7: other all parallel => all other parallel\np22: environmet", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper488/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper488/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Population-Guided Parallel Policy Search for Reinforcement Learning", "authors": ["Whiyoung Jung", "Giseung Park", "Youngchul Sung"], "authorids": ["wy.jung@kaist.ac.kr", "gs.park@kaist.ac.kr", "ycsung@kaist.ac.kr"], "keywords": ["Reinforcement Learning", "Parallel Learning", "Population Based Learning"], "abstract": "In this paper, a new population-guided parallel learning scheme is proposed to enhance the performance of off-policy reinforcement learning (RL). In the proposed scheme, multiple identical learners with their own value-functions and policies share a common experience replay buffer, and search a good policy in collaboration with the guidance of the best policy information. The key point is that the information of the best policy  is fused in a soft manner by constructing an augmented loss function for policy update to enlarge the overall search region by the multiple learners. The guidance by the previous best policy and the enlarged  range enable faster and better policy search, and monotone improvement of the expected cumulative return by the proposed scheme is proved theoretically. Working algorithms are constructed by applying the proposed scheme to the twin delayed deep deterministic (TD3) policy gradient algorithm, and numerical results show that the constructed P3S-TD3 outperforms most of the current state-of-the-art RL algorithms, and the gain is significant in the case of sparse reward environment.", "pdf": "/pdf/17b46ff13242a110aae198cfeb220153596d2b1d.pdf", "paperhash": "jung|populationguided_parallel_policy_search_for_reinforcement_learning", "_bibtex": "@inproceedings{\nJung2020Population-Guided,\ntitle={Population-Guided Parallel Policy Search for Reinforcement Learning},\nauthor={Whiyoung Jung and Giseung Park and Youngchul Sung},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rJeINp4KwH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/c739eba4d6d269e6f0a0c23db7b72eb20a83ce2a.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rJeINp4KwH", "replyto": "rJeINp4KwH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper488/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper488/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575427275336, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper488/Reviewers"], "noninvitees": [], "tcdate": 1570237751404, "tmdate": 1575427275350, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper488/-/Official_Review"}}}, {"id": "BJlQNc2joB", "original": null, "number": 5, "cdate": 1573796394882, "ddate": null, "tcdate": 1573796394882, "tmdate": 1573796394882, "tddate": null, "forum": "rJeINp4KwH", "replyto": "ryxMDGOKur", "invitation": "ICLR.cc/2020/Conference/Paper488/-/Official_Comment", "content": {"title": "Response to Reviewer 2", "comment": "[Comment]: Level of innovation \n\n[Response]: We agree with the reviewer on that the innovation of the proposed P3S scheme is modest in some sense. However, we also think that there exists definitely some new ingredients in the proposed P3S scheme. During the revision, we compared the P3S scheme with more other parallel learning schemes including latest algorithms based on evolutionary approaches and center policy like in Distral/Divide-and-Conquer. It is now observed that the prosed P3S scheme outperforms these up-to-date algorithms. This shows the effectiveness of the proposed P3S method. \nFurthermore, we provide theoretical guarantee for monotone average performance improvement for P3S, while many algorithms just proposed algorithms only. As indicated by the reviewer, certain portion of the proof is from other papers, but the proof has non-trivial and non-straightforward steps to derive the desired result and the required condition for monotone improvement, which is expressed as Assumption 2 and implemented practically based on the proposed beta update rule.  We believe this is some definite contribution.\n\n [Comment]: Comparing other distributed RL methods and a method using a center policy (average policy)\n\n[Response]: We could not compare the performance with PBT for finding hyperparameters and suggested other variants of the resetting method because of the time limit. Instead, we were able to compare P3S-TD3 with CEM-TD3 (Pourchot & Sigaud (2019)) which is a state-of-the-art evolutionary RL algorithm, and Center-TD3 which is based on Distral (Teh et al. (2017))/Divide-and-Conquer (Ghosh et al. (2018)). It is observed that the proposed P3S-TD3 algorithm outperforms CEM-TD3 and algorithm based on Distral/Divide-and-Conquer on all the considered MuJoCo and  delayed MuJoCo environments (Hopper-v1, Walker2d-v1, HalfCheetah-v1, Ant-v1 and their delayed versions) except delayed HalfCheetah-v1. Thus, this shows the effectiveness of the proposed P3S method. Please see Appendices D and E in the revised paper.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper488/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper488/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Population-Guided Parallel Policy Search for Reinforcement Learning", "authors": ["Whiyoung Jung", "Giseung Park", "Youngchul Sung"], "authorids": ["wy.jung@kaist.ac.kr", "gs.park@kaist.ac.kr", "ycsung@kaist.ac.kr"], "keywords": ["Reinforcement Learning", "Parallel Learning", "Population Based Learning"], "abstract": "In this paper, a new population-guided parallel learning scheme is proposed to enhance the performance of off-policy reinforcement learning (RL). In the proposed scheme, multiple identical learners with their own value-functions and policies share a common experience replay buffer, and search a good policy in collaboration with the guidance of the best policy information. The key point is that the information of the best policy  is fused in a soft manner by constructing an augmented loss function for policy update to enlarge the overall search region by the multiple learners. The guidance by the previous best policy and the enlarged  range enable faster and better policy search, and monotone improvement of the expected cumulative return by the proposed scheme is proved theoretically. Working algorithms are constructed by applying the proposed scheme to the twin delayed deep deterministic (TD3) policy gradient algorithm, and numerical results show that the constructed P3S-TD3 outperforms most of the current state-of-the-art RL algorithms, and the gain is significant in the case of sparse reward environment.", "pdf": "/pdf/17b46ff13242a110aae198cfeb220153596d2b1d.pdf", "paperhash": "jung|populationguided_parallel_policy_search_for_reinforcement_learning", "_bibtex": "@inproceedings{\nJung2020Population-Guided,\ntitle={Population-Guided Parallel Policy Search for Reinforcement Learning},\nauthor={Whiyoung Jung and Giseung Park and Youngchul Sung},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rJeINp4KwH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/c739eba4d6d269e6f0a0c23db7b72eb20a83ce2a.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJeINp4KwH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper488/Authors", "ICLR.cc/2020/Conference/Paper488/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper488/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper488/Reviewers", "ICLR.cc/2020/Conference/Paper488/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper488/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper488/Authors|ICLR.cc/2020/Conference/Paper488/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504170735, "tmdate": 1576860537986, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper488/Authors", "ICLR.cc/2020/Conference/Paper488/Reviewers", "ICLR.cc/2020/Conference/Paper488/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper488/-/Official_Comment"}}}, {"id": "HygG-q3iiS", "original": null, "number": 4, "cdate": 1573796345921, "ddate": null, "tcdate": 1573796345921, "tmdate": 1573796345921, "tddate": null, "forum": "rJeINp4KwH", "replyto": "B1e8Q9IiuH", "invitation": "ICLR.cc/2020/Conference/Paper488/-/Official_Comment", "content": {"title": "Response to Reviewer 1", "comment": "[Comment]: Rewriting words and sentences for more clarity\n\n[Response]: During the revision, we rewrote many parts of the paper for readability and clarity. \n\n[Comment]: Sensitivity on hyperparameter setting\n\n[Response]: The major hyperparameters of P3S is the best policy update interval M, rho and d_min. As mentioned in Section 4, we used M=250 and rho=2 for all the considered environments whether they are the original MuJoCo or delayed version. Furthermore, we also used d_min= 0.02 or 0.05 for all the considered environments in Section 4 and Appendices. So, it seems that we do not need to fine-tune the hyperparameters for each environment.\n\n[Comment]: Application of the KL divergence to deterministic policies\n\n[Response]: Note that the KL divergence between two Gaussian distributions with (mu_1, sigma_1) and (mu_2, sigma_2) is \n             log(sigma_2 / sigma_1) + (1/2) [sigma_1^2 + (mu_1 \u2013 mu_2)^2] / [ sigma_2^2] - 1/2. \nA deterministic policy is expressed as a function a(s), where s is the input state and a(s) is the output action. For two deterministic policies a_1(s) and a_2(s), we can construct two stochastic Gaussian policies \\pi_{i, Gaussian}(a| s), i=1,2 with the same standard deviation sigma=sigma_1=sigma_2,  mu_1=a_1(s), and mu_2 = a_2(s).  As sigma decreases, the Gaussian policy converges to the corresponding deterministic policy. For the two Gaussian policies with the same standard deviation sigma, the KL divergence above reduces to\n            (mu_1 \u2013 mu_2)^2 / sigma^2 = (a_1(s) \u2013 a_2(s))^2/ sigma^2.\nHence, the KL divergence is proportional to the mean square error of two actions.\n\n[Comment]: Adaptation of \\beta \n\n[Response]: Please see the response to common comments.\n\n[Comment]: Performance on Swimmer and Continuous Mountain Car\n\n[Response]: The authors in (Khadka & Tumer (2018), Pourchot & Sigaud (2019)) noticed that most deep RL methods suffer from a deceptive gradient problem on the Swimmer-v1 task, and most RL methods could not learn effectively on the Swimmer-v1 task. Unfortunately, we observed that the proposed P3S-TD3 algorithm could not solve the deceptive gradient problem in the Swimmer-v1 task either.  Please see Appendix F.\nDue to the limited time for revision, we could not simulate the task of continuous mountain car.\n\n[Comment]: Stochastic or deterministic policy in SAC for evaluation\n\n[Response]: There seems a confusion. We used stochastic policy to train the policy in SAC. However, as described in the original SAC paper (Haarnoja et al. (2018)), the performance of the policy using only the mean (i.e., deterministic version) has better performance than stochastic policy.  Hence, the original SAC paper uses stochastic policy for training and deterministic policy for evaluation. Therefore, to give more favor to SAC, we also followed the training and evaluation procedure described in the original SAC paper.\n\n[Comment]: About the v1 version of tasks.\n\n[Response]: V1 tasks are the same as V2 tasks in the aspects of the shape of robots, observation space, action space, and reward function. Only the internal MuJoCo file version is different. V1 is based on MuJoCo file 1.31 and V2 is based on MuJoCo file 1.50. Therefore, the performance difference between the two versions is less. Most of the considered baseline algorithms used V1. Hence, for consistency and reproducibility, we used V1.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper488/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper488/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Population-Guided Parallel Policy Search for Reinforcement Learning", "authors": ["Whiyoung Jung", "Giseung Park", "Youngchul Sung"], "authorids": ["wy.jung@kaist.ac.kr", "gs.park@kaist.ac.kr", "ycsung@kaist.ac.kr"], "keywords": ["Reinforcement Learning", "Parallel Learning", "Population Based Learning"], "abstract": "In this paper, a new population-guided parallel learning scheme is proposed to enhance the performance of off-policy reinforcement learning (RL). In the proposed scheme, multiple identical learners with their own value-functions and policies share a common experience replay buffer, and search a good policy in collaboration with the guidance of the best policy information. The key point is that the information of the best policy  is fused in a soft manner by constructing an augmented loss function for policy update to enlarge the overall search region by the multiple learners. The guidance by the previous best policy and the enlarged  range enable faster and better policy search, and monotone improvement of the expected cumulative return by the proposed scheme is proved theoretically. Working algorithms are constructed by applying the proposed scheme to the twin delayed deep deterministic (TD3) policy gradient algorithm, and numerical results show that the constructed P3S-TD3 outperforms most of the current state-of-the-art RL algorithms, and the gain is significant in the case of sparse reward environment.", "pdf": "/pdf/17b46ff13242a110aae198cfeb220153596d2b1d.pdf", "paperhash": "jung|populationguided_parallel_policy_search_for_reinforcement_learning", "_bibtex": "@inproceedings{\nJung2020Population-Guided,\ntitle={Population-Guided Parallel Policy Search for Reinforcement Learning},\nauthor={Whiyoung Jung and Giseung Park and Youngchul Sung},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rJeINp4KwH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/c739eba4d6d269e6f0a0c23db7b72eb20a83ce2a.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJeINp4KwH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper488/Authors", "ICLR.cc/2020/Conference/Paper488/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper488/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper488/Reviewers", "ICLR.cc/2020/Conference/Paper488/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper488/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper488/Authors|ICLR.cc/2020/Conference/Paper488/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504170735, "tmdate": 1576860537986, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper488/Authors", "ICLR.cc/2020/Conference/Paper488/Reviewers", "ICLR.cc/2020/Conference/Paper488/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper488/-/Official_Comment"}}}, {"id": "HJgwiY2osS", "original": null, "number": 3, "cdate": 1573796254535, "ddate": null, "tcdate": 1573796254535, "tmdate": 1573796254535, "tddate": null, "forum": "rJeINp4KwH", "replyto": "r1lL6hgdKH", "invitation": "ICLR.cc/2020/Conference/Paper488/-/Official_Comment", "content": {"title": "Response to Reviewer 3", "comment": "[Comment]: On Theorem 1, performance improvement gap, and the adaptation rule for \\beta\n\n[Response]: Please see the response to common comments.\n\n[Comment]: Comparison with other methods such as PBT, other variants of the resetting method, and evolutionary algorithm.\n\n[Response]: We could not compare the performance with PBT for finding hyperparameters and suggested other variants of the resetting method because of the time limit. Instead, we were able to compare P3S-TD3 with CEM-TD3 (Pourchot & Sigaud (2019)) which is a state-of-the-art evolutionary RL algorithm, and Center-TD3 which is based on Distral (Teh et al. (2017))/Divide-and-Conquer (Ghosh et al. (2018)). It is observed that the proposed P3S-TD3 algorithm outperforms CEM-TD3 and algorithm based on Distral/Divide-and-Conquer on all the considered MuJoCo and delayed MuJoCo environments (Hopper-v1, Walker2d-v1, HalfCheetah-v1, Ant-v1 and their delayed versions) except delayed HalfCheetah-v1. Thus, this proves the effectiveness of the proposed P3S method. Please see Appendices D and E in the revised paper.\n\n[Comment]: Results of the single-agent baselines on delayed MuJoCo environments\n\n[Response]: In the revised paper, we have included the performance of the single-agent baselines on delayed MuJoCo environments in Appendix C. It is observed that P3S-TD3 outperforms these single-agent baselines except the case of ACKTR in delayed Ant-v1.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper488/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper488/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Population-Guided Parallel Policy Search for Reinforcement Learning", "authors": ["Whiyoung Jung", "Giseung Park", "Youngchul Sung"], "authorids": ["wy.jung@kaist.ac.kr", "gs.park@kaist.ac.kr", "ycsung@kaist.ac.kr"], "keywords": ["Reinforcement Learning", "Parallel Learning", "Population Based Learning"], "abstract": "In this paper, a new population-guided parallel learning scheme is proposed to enhance the performance of off-policy reinforcement learning (RL). In the proposed scheme, multiple identical learners with their own value-functions and policies share a common experience replay buffer, and search a good policy in collaboration with the guidance of the best policy information. The key point is that the information of the best policy  is fused in a soft manner by constructing an augmented loss function for policy update to enlarge the overall search region by the multiple learners. The guidance by the previous best policy and the enlarged  range enable faster and better policy search, and monotone improvement of the expected cumulative return by the proposed scheme is proved theoretically. Working algorithms are constructed by applying the proposed scheme to the twin delayed deep deterministic (TD3) policy gradient algorithm, and numerical results show that the constructed P3S-TD3 outperforms most of the current state-of-the-art RL algorithms, and the gain is significant in the case of sparse reward environment.", "pdf": "/pdf/17b46ff13242a110aae198cfeb220153596d2b1d.pdf", "paperhash": "jung|populationguided_parallel_policy_search_for_reinforcement_learning", "_bibtex": "@inproceedings{\nJung2020Population-Guided,\ntitle={Population-Guided Parallel Policy Search for Reinforcement Learning},\nauthor={Whiyoung Jung and Giseung Park and Youngchul Sung},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rJeINp4KwH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/c739eba4d6d269e6f0a0c23db7b72eb20a83ce2a.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJeINp4KwH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper488/Authors", "ICLR.cc/2020/Conference/Paper488/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper488/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper488/Reviewers", "ICLR.cc/2020/Conference/Paper488/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper488/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper488/Authors|ICLR.cc/2020/Conference/Paper488/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504170735, "tmdate": 1576860537986, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper488/Authors", "ICLR.cc/2020/Conference/Paper488/Reviewers", "ICLR.cc/2020/Conference/Paper488/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper488/-/Official_Comment"}}}, {"id": "S1enLtnjsB", "original": null, "number": 2, "cdate": 1573796179561, "ddate": null, "tcdate": 1573796179561, "tmdate": 1573796179561, "tddate": null, "forum": "rJeINp4KwH", "replyto": "rJeINp4KwH", "invitation": "ICLR.cc/2020/Conference/Paper488/-/Official_Comment", "content": {"title": "Common Response", "comment": "We thank all reviewers for their valuable comments. During the rebuttal period, we revised the paper to incorporate the reviewers\u2019 comments. We hope that the revised paper and this response satisfy all the reviewers\u2019 concerns. The modified part in the revised paper is in blue.\n\n[Comment]: On the connection to the theoretical result Theorem 1 and the adaptation method for \\beta\n\n[Response]: As noticed by Reviewer 3, there exists a performance improvement gap between the updated policy in the current update period and the best policy in the previous update period in Theorem 1. In the revised paper, we considered this gap explicitly, and Theorem 1 and its proof were modified accordingly. Now, the meaning of the proposed adaptation method of \\beta is clear. The proposed adaptation method of \\beta intends to maximize the improvement gap. We rewrote the beta update part in Section 3.2 and have included Appendix B to explain the beta update rule. Please see Section 3.2 and Appendix B for the beta adaptation rule and Appendix A for the modified proof of Theorem 1. \n\n[Comment]: More results on comparison with other population-based or parallel algorithms\n\n[Response]: Reviewers 2 and 3 suggested more results on comparison with other population-based algorithms or parallel algorithms. In Section 4.3 of the first submission, we considered several population-based methods such as distributed RL, the experience-sharing only method, the resetting method, where the resetting method resembles PBT (Jaderberg et al. (2017)). We realized that obtaining distributed performance results such as wall clock time on a real distributed simulation environment is beyond the scope of an academic institute. Instead, following the comments on more comparison results, during the revision we considered and implemented several up-to-date parallel learning algorithms such as CEM-TD3 (Pourchot & Sigaud (2019)) and Distral (Teh et al. (2017))/Divide-and-Conquer (Ghosh et al. (2018)). Here, CEM-TD3 was chosen as the latest evolutionary algorithm and Distral (Teh et al. (2017))/Divide-and-Conquer (Ghosh et al. (2018)) was chosen as algorithms based on a center policy. It is observed that the proposed P3S-TD3 algorithm outperforms CEM-TD3 and algorithm based on Distral/Divide-and-Conquer on all the considered MuJoCo and  delayed MuJoCo environments (Hopper-v1, Walker2d-v1, HalfCheetah-v1, Ant-v1 and their delayed versions) except delayed HalfCheetah-v1. Thus, this shows the effectiveness of the proposed P3S method. Please see Appendices D and E in the revised paper.\n\n[Comment]: Rewriting words and sentences for more clarity\n\n[Response]: During the revision, we rewrote many parts of the paper for readability and clarity. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper488/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper488/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Population-Guided Parallel Policy Search for Reinforcement Learning", "authors": ["Whiyoung Jung", "Giseung Park", "Youngchul Sung"], "authorids": ["wy.jung@kaist.ac.kr", "gs.park@kaist.ac.kr", "ycsung@kaist.ac.kr"], "keywords": ["Reinforcement Learning", "Parallel Learning", "Population Based Learning"], "abstract": "In this paper, a new population-guided parallel learning scheme is proposed to enhance the performance of off-policy reinforcement learning (RL). In the proposed scheme, multiple identical learners with their own value-functions and policies share a common experience replay buffer, and search a good policy in collaboration with the guidance of the best policy information. The key point is that the information of the best policy  is fused in a soft manner by constructing an augmented loss function for policy update to enlarge the overall search region by the multiple learners. The guidance by the previous best policy and the enlarged  range enable faster and better policy search, and monotone improvement of the expected cumulative return by the proposed scheme is proved theoretically. Working algorithms are constructed by applying the proposed scheme to the twin delayed deep deterministic (TD3) policy gradient algorithm, and numerical results show that the constructed P3S-TD3 outperforms most of the current state-of-the-art RL algorithms, and the gain is significant in the case of sparse reward environment.", "pdf": "/pdf/17b46ff13242a110aae198cfeb220153596d2b1d.pdf", "paperhash": "jung|populationguided_parallel_policy_search_for_reinforcement_learning", "_bibtex": "@inproceedings{\nJung2020Population-Guided,\ntitle={Population-Guided Parallel Policy Search for Reinforcement Learning},\nauthor={Whiyoung Jung and Giseung Park and Youngchul Sung},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rJeINp4KwH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/c739eba4d6d269e6f0a0c23db7b72eb20a83ce2a.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJeINp4KwH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper488/Authors", "ICLR.cc/2020/Conference/Paper488/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper488/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper488/Reviewers", "ICLR.cc/2020/Conference/Paper488/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper488/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper488/Authors|ICLR.cc/2020/Conference/Paper488/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504170735, "tmdate": 1576860537986, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper488/Authors", "ICLR.cc/2020/Conference/Paper488/Reviewers", "ICLR.cc/2020/Conference/Paper488/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper488/-/Official_Comment"}}}, {"id": "ryxMDGOKur", "original": null, "number": 1, "cdate": 1570501210009, "ddate": null, "tcdate": 1570501210009, "tmdate": 1572972589195, "tddate": null, "forum": "rJeINp4KwH", "replyto": "rJeINp4KwH", "invitation": "ICLR.cc/2020/Conference/Paper488/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This work presents a distributed framework for off-policy RL consisting of multiple agents that are trained in parallel while also being regularized to be similar to the current best policy. Maintaining a population of agents can help mitigate issues due to convergences to local optima. The method is evaluated on standard continuous control tasks, and shows some performance improvements over methods that train just a single agent. Ablation experiments are also conducted to evaluate the effects of different design decisions.\n\nThe overall method is sensible and performance looks promising. But the technical innovation is fairly modest. As the authors pointed out, the proximal constraint has been used in previous distributed RL framework, and the main difference in this work is enforcing a trust region penalty against the best policy, as opposed to an average policy. The proof of guaranteed improvement largely follows the proof from other trust region methods, by replacing the old policy with the best policy from the previous iteration. The experiments did compare with a number of previous algorithms, but the comparisons are all to algorithms that train a single agent at a time, and no comparisons are made to other distributed RL algorithms. Including some comparisons to other distributed methods can help strengthen the claims in favour P3S. In particular, how does regularizing using the best policy compare to using an average policy like in Distral [The et al., 2017]? That being said, the performance improvements on most environments appear to be fairly modest, and it is not clear if the improvements are indeed significant. Including additional experiments on more challenging tasks could be helpful here. Overall, the contribution of this work is pretty incremental, and I think it is not quite at the standards for ICLR at this time. But with more thorough evaluation and polishing, I think this work can make for a strong submission.\n\nMore specific notes:\n\nThere are a fair bit of awkward phrasing and grammatical errors in the writing, which hurts the clarity of the exposition.\n\nOne of the advantages of a distributed framework is faster wall-clock time. The current learning curves only compare the sample count. It might also be informative to compare the wall-clock time of the different methods, as well as including comparisons to other distributed frameworks.\n\nSome experiments to show how performance scales with the number of learners can also be helpful. Since one of the possible factors that improve performance for P3S is having multiple agents, providing some insight on how performance varies with different numbers of agents can be valuable in this regard.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper488/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper488/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Population-Guided Parallel Policy Search for Reinforcement Learning", "authors": ["Whiyoung Jung", "Giseung Park", "Youngchul Sung"], "authorids": ["wy.jung@kaist.ac.kr", "gs.park@kaist.ac.kr", "ycsung@kaist.ac.kr"], "keywords": ["Reinforcement Learning", "Parallel Learning", "Population Based Learning"], "abstract": "In this paper, a new population-guided parallel learning scheme is proposed to enhance the performance of off-policy reinforcement learning (RL). In the proposed scheme, multiple identical learners with their own value-functions and policies share a common experience replay buffer, and search a good policy in collaboration with the guidance of the best policy information. The key point is that the information of the best policy  is fused in a soft manner by constructing an augmented loss function for policy update to enlarge the overall search region by the multiple learners. The guidance by the previous best policy and the enlarged  range enable faster and better policy search, and monotone improvement of the expected cumulative return by the proposed scheme is proved theoretically. Working algorithms are constructed by applying the proposed scheme to the twin delayed deep deterministic (TD3) policy gradient algorithm, and numerical results show that the constructed P3S-TD3 outperforms most of the current state-of-the-art RL algorithms, and the gain is significant in the case of sparse reward environment.", "pdf": "/pdf/17b46ff13242a110aae198cfeb220153596d2b1d.pdf", "paperhash": "jung|populationguided_parallel_policy_search_for_reinforcement_learning", "_bibtex": "@inproceedings{\nJung2020Population-Guided,\ntitle={Population-Guided Parallel Policy Search for Reinforcement Learning},\nauthor={Whiyoung Jung and Giseung Park and Youngchul Sung},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rJeINp4KwH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/c739eba4d6d269e6f0a0c23db7b72eb20a83ce2a.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rJeINp4KwH", "replyto": "rJeINp4KwH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper488/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper488/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575427275336, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper488/Reviewers"], "noninvitees": [], "tcdate": 1570237751404, "tmdate": 1575427275350, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper488/-/Official_Review"}}}], "count": 9}