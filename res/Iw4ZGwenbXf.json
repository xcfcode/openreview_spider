{"notes": [{"id": "Iw4ZGwenbXf", "original": "gllss8qHcg", "number": 2254, "cdate": 1601308248327, "ddate": null, "tcdate": 1601308248327, "tmdate": 1615848277288, "tddate": null, "forum": "Iw4ZGwenbXf", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "NOVAS: Non-convex Optimization via Adaptive Stochastic Search for End-to-end Learning and Control", "authorids": ["~Ioannis_Exarchos1", "~Marcus_Aloysius_Pereira1", "~Ziyi_Wang1", "~Evangelos_Theodorou1"], "authors": ["Ioannis Exarchos", "Marcus Aloysius Pereira", "Ziyi Wang", "Evangelos Theodorou"], "keywords": ["deep neural networks", "nested optimization", "stochastic control", "deep FBSDEs"], "abstract": "In this work we propose the use of adaptive stochastic search as a building block for general, non-convex optimization operations within deep neural network architectures. Specifically, for an objective function located at some layer in the network and parameterized by some network parameters, we employ adaptive stochastic search to perform optimization over its output. This operation is differentiable and does not obstruct the passing of gradients during backpropagation, thus enabling us to incorporate it as a component in end-to-end learning. We study the proposed optimization module's properties and benchmark it against two existing alternatives on a synthetic energy-based structured prediction task, and further showcase its use in stochastic optimal control applications.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "exarchos|novas_nonconvex_optimization_via_adaptive_stochastic_search_for_endtoend_learning_and_control", "supplementary_material": "/attachment/5344cbd3208a33aa57b3eee2edee22b6df97cd27.zip", "pdf": "/pdf/19f093001a03a82a092d19740971a45fff9f47a8.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nexarchos2021novas,\ntitle={{\\{}NOVAS{\\}}: Non-convex Optimization via Adaptive Stochastic Search for End-to-end Learning and Control},\nauthor={Ioannis Exarchos and Marcus Aloysius Pereira and Ziyi Wang and Evangelos Theodorou},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Iw4ZGwenbXf}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 11, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "KXrK3zyOY6a", "original": null, "number": 1, "cdate": 1610040421755, "ddate": null, "tcdate": 1610040421755, "tmdate": 1610474020519, "tddate": null, "forum": "Iw4ZGwenbXf", "replyto": "Iw4ZGwenbXf", "invitation": "ICLR.cc/2021/Conference/Paper2254/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Poster)", "comment": "The authors propose an intriguing alternative to IFT or unrolled GD as a method for optimizing through arg min layers in a neural net, by using a differentiable sampling-based optimization approach. I found the general idea in the paper to be intriguing and thought-provoking. The reviewers generally seem to have also appreciated the method, and many of the reviewers' concerns were addressed by the authors during the rebuttal. Although the paper does have a number of flaws -- in particular, the evaluation is a bit hard to appreciate, since improvement over prior work is either unclear, or no meaningful comparison is offered, -- I think in this case the benefits outweigh the downsides. The work is far from perfect, but the ideas that are presented are interested and valuable to the community, and I think that ICLR attendees will appreciate learning about this work. I would encourage the authors however to improve the paper, and especially the empirical evaluation, as much as possible for the camera-ready, and to take reviewer comments into account insofar as feasible. I'm also not sure how much I buy the \"overfitting to hyperparameters\" argument for unrolled GD, and a less charitable interpretation is that the authors present this issue largely to make up for the comparative lack of other benefits. That's not necessarily a bad thing, but I think making such a big deal of it is a bit strange. It's probably fair to say at this stage that the actual benefits of this approach are a bit modest (though improvements in runtime are a good thing...), but the idea is interesting, and may spur future research."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NOVAS: Non-convex Optimization via Adaptive Stochastic Search for End-to-end Learning and Control", "authorids": ["~Ioannis_Exarchos1", "~Marcus_Aloysius_Pereira1", "~Ziyi_Wang1", "~Evangelos_Theodorou1"], "authors": ["Ioannis Exarchos", "Marcus Aloysius Pereira", "Ziyi Wang", "Evangelos Theodorou"], "keywords": ["deep neural networks", "nested optimization", "stochastic control", "deep FBSDEs"], "abstract": "In this work we propose the use of adaptive stochastic search as a building block for general, non-convex optimization operations within deep neural network architectures. Specifically, for an objective function located at some layer in the network and parameterized by some network parameters, we employ adaptive stochastic search to perform optimization over its output. This operation is differentiable and does not obstruct the passing of gradients during backpropagation, thus enabling us to incorporate it as a component in end-to-end learning. We study the proposed optimization module's properties and benchmark it against two existing alternatives on a synthetic energy-based structured prediction task, and further showcase its use in stochastic optimal control applications.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "exarchos|novas_nonconvex_optimization_via_adaptive_stochastic_search_for_endtoend_learning_and_control", "supplementary_material": "/attachment/5344cbd3208a33aa57b3eee2edee22b6df97cd27.zip", "pdf": "/pdf/19f093001a03a82a092d19740971a45fff9f47a8.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nexarchos2021novas,\ntitle={{\\{}NOVAS{\\}}: Non-convex Optimization via Adaptive Stochastic Search for End-to-end Learning and Control},\nauthor={Ioannis Exarchos and Marcus Aloysius Pereira and Ziyi Wang and Evangelos Theodorou},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Iw4ZGwenbXf}\n}"}, "tags": [], "invitation": {"reply": {"forum": "Iw4ZGwenbXf", "replyto": "Iw4ZGwenbXf", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040421741, "tmdate": 1610474020502, "id": "ICLR.cc/2021/Conference/Paper2254/-/Decision"}}}, {"id": "grUp8I9Xtqt", "original": null, "number": 4, "cdate": 1604360631859, "ddate": null, "tcdate": 1604360631859, "tmdate": 1606813973267, "tddate": null, "forum": "Iw4ZGwenbXf", "replyto": "Iw4ZGwenbXf", "invitation": "ICLR.cc/2021/Conference/Paper2254/-/Official_Review", "content": {"title": "Official Blind Review #3", "review": "**Summary**\nThis paper aims to present a method that allows efficient learning in neural networks architecture that present optimization blocks. These blocks have the form of x_{i+1} = \\arg \\min_x F(x, x_i, \\theta), and can be thought of as a neural network layer. The addition of this block results in a complex optimization problem, since it presents a multi-level problem. The approach presented in this paper relies on adaptive stochastic search as a differentiable optimization procedure. The authors evaluate the proposed algorithm in a variety of applications, including structured prediction networks and control.\n\n**Assessment**\nWhile the paper is well written, its structure could be significantly improved and the problem statement could be made clearer -- since the topic of the paper is not common, I believe further explanation should be made (it could be by the means of a graph or a figure).\n\n*Pros:* The algorithm presented here it\u2019s simple and seems to lead to good results. It is benchmarked in a variety of fields (1) energy-based learning, (2) robotic control, and (3) portfolio management. In all the cases exceeding the presented baselines.\n\n\n*Cons:* First of all, the paper does not present or mention the practical implementation of their algorithm, which I believe is the only contribution of their work. As it is right now, there is no section of their technical contribution -- just mention of the existing adaptive sampling. Secondly, the baselines introduced, except section 4.1, are non-existent (section 4.2) or extremely naive (section 4.3). Finally, it lacks further experiments underpinning the claims made and ablations. \n\nHere I provide more detailed feedback of my \u201ccons\u201d points. Regarding my first point, since this paper presents a simple-idea/existing-idea-in-another-context, I would expect more details of the practical implementation of their algorithm and sensitivity to the different hyper-parameters. Both of those are lacking in the main paper. The experiment section, while it tackles three different domains, it does not provide enough evidence with respect to previous methods. Section 4.2, depicts a toy reinforcement learning/control problem and no baseline is provided. I would like to (1) incorporate the baseline in Pereira et al. 2019 and LQR with an analysis of the run-time for each algorithm and performance, and (2) test the algorithm to be tested on more complex domains and provide baselines that present the same assumptions, e.g., iLQR. One of the main reasons for the presented method is that it can tackle non-convex objectives; however, the environment presented is convex. Finally, the authors make a lot of emphasis on the difference between their optimization approach and the one that meta-learning does. It seems to me that at least U-NOVA could perform the same task as MAM, or even NOVA with some context of the task. I would like further explanations of this, and some experiments with (U-)NOVA on meta-learning. Finally, I would like further clarifications on why the graph decoupling to get a better initialization is justifiable with NOVA and not other procedures.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2021/Conference/Paper2254/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2254/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NOVAS: Non-convex Optimization via Adaptive Stochastic Search for End-to-end Learning and Control", "authorids": ["~Ioannis_Exarchos1", "~Marcus_Aloysius_Pereira1", "~Ziyi_Wang1", "~Evangelos_Theodorou1"], "authors": ["Ioannis Exarchos", "Marcus Aloysius Pereira", "Ziyi Wang", "Evangelos Theodorou"], "keywords": ["deep neural networks", "nested optimization", "stochastic control", "deep FBSDEs"], "abstract": "In this work we propose the use of adaptive stochastic search as a building block for general, non-convex optimization operations within deep neural network architectures. Specifically, for an objective function located at some layer in the network and parameterized by some network parameters, we employ adaptive stochastic search to perform optimization over its output. This operation is differentiable and does not obstruct the passing of gradients during backpropagation, thus enabling us to incorporate it as a component in end-to-end learning. We study the proposed optimization module's properties and benchmark it against two existing alternatives on a synthetic energy-based structured prediction task, and further showcase its use in stochastic optimal control applications.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "exarchos|novas_nonconvex_optimization_via_adaptive_stochastic_search_for_endtoend_learning_and_control", "supplementary_material": "/attachment/5344cbd3208a33aa57b3eee2edee22b6df97cd27.zip", "pdf": "/pdf/19f093001a03a82a092d19740971a45fff9f47a8.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nexarchos2021novas,\ntitle={{\\{}NOVAS{\\}}: Non-convex Optimization via Adaptive Stochastic Search for End-to-end Learning and Control},\nauthor={Ioannis Exarchos and Marcus Aloysius Pereira and Ziyi Wang and Evangelos Theodorou},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Iw4ZGwenbXf}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Iw4ZGwenbXf", "replyto": "Iw4ZGwenbXf", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2254/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538100561, "tmdate": 1606915778623, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2254/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2254/-/Official_Review"}}}, {"id": "gVM6LPu_Ol3", "original": null, "number": 3, "cdate": 1603896617109, "ddate": null, "tcdate": 1603896617109, "tmdate": 1606777260752, "tddate": null, "forum": "Iw4ZGwenbXf", "replyto": "Iw4ZGwenbXf", "invitation": "ICLR.cc/2021/Conference/Paper2254/-/Official_Review", "content": {"title": "Great work!", "review": "### Summary \n\nThe authors present the idea of adaptive stochastic search as a building block for neural networks, as an alternative to other \"inner loop\" optimization methods like gradient descent.\n\n### Reasons for score \n\nPresented works well against the baselines selected and the claims are supported by empirical evidence.\n\n### Pros \n\n1. Paper is clear and grounded in existing literature and context\n1. Claims are supported by empirical evidence.\n1. Proposed method is robust to changes in hyperparameters like the number of inner loop iterations during inference.\n1. Proposed method allows learning the intended functions (see Figure 1) leading to better generalization, the modularity is a bonus.\n\n### Cons \n\n1. There are some cases (although not in general) of meta-learning for adaptation, for example where the update can be described in an implicit fixed point way or when the method used in [FirstOrderMAML](https://lilianweng.github.io/lil-log/2018/11/30/meta-learning.html#first-order-maml) , which allows not needing to unroll (when backpropagating) the function to be optimized. The paper doesn't present this relevant information in page 4. (Edit: this is fixed now)\n1. In Figure 2 clarifying if the loss is train set or test set would be nice, plotting both would be even better. (Edit: this is fixed now)\n1. Not comparing to existing task-adaptation/meta-learning methods and benchmarks (for example First-Order MAML's gradient)\n\n### Suggestions\n\n1. In Figure 1 both (b) and (c) visibly look really similar, I would suggest plotting the difference between them, so a reader can understand better via visualizing the energy function the advantages/disadvantages of (not) unrolling.  (Edit: the authors considered it)", "rating": "6: Marginally above acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2021/Conference/Paper2254/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2254/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NOVAS: Non-convex Optimization via Adaptive Stochastic Search for End-to-end Learning and Control", "authorids": ["~Ioannis_Exarchos1", "~Marcus_Aloysius_Pereira1", "~Ziyi_Wang1", "~Evangelos_Theodorou1"], "authors": ["Ioannis Exarchos", "Marcus Aloysius Pereira", "Ziyi Wang", "Evangelos Theodorou"], "keywords": ["deep neural networks", "nested optimization", "stochastic control", "deep FBSDEs"], "abstract": "In this work we propose the use of adaptive stochastic search as a building block for general, non-convex optimization operations within deep neural network architectures. Specifically, for an objective function located at some layer in the network and parameterized by some network parameters, we employ adaptive stochastic search to perform optimization over its output. This operation is differentiable and does not obstruct the passing of gradients during backpropagation, thus enabling us to incorporate it as a component in end-to-end learning. We study the proposed optimization module's properties and benchmark it against two existing alternatives on a synthetic energy-based structured prediction task, and further showcase its use in stochastic optimal control applications.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "exarchos|novas_nonconvex_optimization_via_adaptive_stochastic_search_for_endtoend_learning_and_control", "supplementary_material": "/attachment/5344cbd3208a33aa57b3eee2edee22b6df97cd27.zip", "pdf": "/pdf/19f093001a03a82a092d19740971a45fff9f47a8.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nexarchos2021novas,\ntitle={{\\{}NOVAS{\\}}: Non-convex Optimization via Adaptive Stochastic Search for End-to-end Learning and Control},\nauthor={Ioannis Exarchos and Marcus Aloysius Pereira and Ziyi Wang and Evangelos Theodorou},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Iw4ZGwenbXf}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Iw4ZGwenbXf", "replyto": "Iw4ZGwenbXf", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2254/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538100561, "tmdate": 1606915778623, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2254/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2254/-/Official_Review"}}}, {"id": "duXZnhy8Kq3", "original": null, "number": 1, "cdate": 1603891612227, "ddate": null, "tcdate": 1603891612227, "tmdate": 1606486880218, "tddate": null, "forum": "Iw4ZGwenbXf", "replyto": "Iw4ZGwenbXf", "invitation": "ICLR.cc/2021/Conference/Paper2254/-/Official_Review", "content": {"title": "Can be improved by a more compelling analysis and principled reasoning for the approach", "review": "The authors suggest a way to backpropagate through neural network with an embedded optimization problem.\nThey propose to perform the estimation of the gradient of the optimum of the embedded problem with respect to its parameterization by the differentiation of one step of a stochastic search algorithm.\nTheir suggested NOVAS approach together with the deep FBSDE method allows for solving optimal control problems efficiently and with low memory due to not unrolling multiple optimizer steps during the backpropagation.\nThe authors demonstrate the feasibility of their approach via an example with a structured prediction energy network and two optimal control tasks (cart-pole swing-up and portfolio selection).\n\nI do not recommend to accept the paper.\n\nThe results are interesting but require a more compelling presentation and in depth analysis.\nAt the current state it is hard for me to understand how their approach for differentiation of an embedded optimization problem is principled and why it should be better than other approaches.\nIt is also hard to evaluate whether the presented approach really is better from the given computational experiments.\n\nFor a locally strictly convex optimization problem of the form\n\nx^star = argmin_x f(x, theta)\n\nthe computation of the Jacobian dx^star / dtheta analytically requires the inversion of the full Hessian matrix via the implicit function theorem:\n\n[partial x^star / partial theta] = -[partial (df/dx) / partial x]^{-1} [partial (df/dx) / partial theta]\n\nwhere [partial (df/dx) / partial x] is the Hessian of the optimization problem at the optimum.\n\nIf x \\in R^n then in the locally strictly convex case the Hessian has full rank n.\n\nBackpropagation through only one gradient step contains only information of rank 1.\nSo the backpropagation of one gradient step generally only contains a small part of the sensitivity of the optimum with respect to theta.\n\nWhen starting the optimization from different points and repeating throughout multiple outer iterations we potentially gather enough sensitivity information to properly represent the shape of the loss function around the optimum in expectation.\nBut the approach certainly adds variance compared to having an exact gradient or unrolling many optimization steps.\n\nAdditionally, the authors suggest a stochastic search method that due to using the log function trick has low sample efficiency compared to using actual gradient information.\nThe stochastic search method proposed by the authors gains information about the loss function shape by sampling locally in the input space.\nThese types of gradient estimators have much higher variance for truly high dimensional loss functions due to the curse of dimensionality.\n\nSee Ben Rechts criticism of the policy gradient / REINFORCE gradient estimator that has the same issue:\nhttps://www.argmin.net/2018/02/20/reinforce/\n\nSo backpropagation of one gradient step with a high variance gradient estimate compared to the true derivative which has rank n curvature information should be inefficient in theory for truly high-dimensional optimization problems.\n\nPerhaps the authors could add at least some analysis of the variance of their gradient estimator compared to other gradient estimators for embedded optimization problems for different examples and show how the variance behaves depending on the difficulty of the optimization task (dimensionality, curvature) taking the above perspective into account.\n\nSince the authors explicitly treat nonconvex optimization problems they could also make more explicit that a nonconvex optimization problem does not necessarily have a unique optimum.\nThe argmin is therefore a set-valued map and not differentiable in the classical sense (even when the optimum is unique almost everywhere in the parameter space it is possible that the optimal value is discontinuous in the parameter).\nFor non-global optimizers (such as gradient descent) starting in such a way that we do not end up in the wrong local optimum is crucial.\n\nIt seems problematic that the comparison against the backpropagation through unrolled gradient descent is with an example where the latter finds a different (wrong) local optimum then the suggested NOVAS method.\nI don't see how the comparison results with respect to convergence, computational time and inference can be considered conclusive without making the tuning effort for the baseline to perform the desired task properly.\n\nThe authors refer to computation time as \"inner-loop convergence rate\" (Page 6) but convergence rate cannot be inferred from computation time if e.g. sampling gradients using the likelihood ratio trick is much faster than computing a backpropagation gradient.\nReverse mode autodiff gradients are only faster than finite difference / forward mode / sampling gradients for a non-zero constant number threshold of input dimension (how many depends on the efficiency of the autodiff module but it can be e.g. 100-200 input parameters).\nThe computation time for truly high-dimensional problems can thus look very different than medium dimensional problems (n = 100).\n\nRegarding the violin plots for the portfolio optimization I am not sure how much they benefit the paper.\nThe goal of the paper should be to demonstrate the superiority of the suggested method over baselines.\nThe violin plots are relevant to people who are interested in the outcome of that specific optimal control task.\nSomeone not knowledgeable about the control problem will not benefit much from knowing the terminal wealth of the different strategies.\n\nMore analysis about the quality of the gradients obtained by the suggested method compared to other methods would improve the paper:\n- How many unrolling steps do what to the gradient variance?\n- What is the difference of using backprop through gradients of the loss vs stochastic search gradients (log likelihood trick) gradients for different dimensions of the optimization variable?\n\nAnother small suggestions:\nOn Page 4 \"desirable properties of optimization\" seems rather vague, perhaps it should be stated more specifically (most likely what is meant is smoothness of the resulting objective function).\n", "rating": "4: Ok but not good enough - rejection", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2021/Conference/Paper2254/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2254/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NOVAS: Non-convex Optimization via Adaptive Stochastic Search for End-to-end Learning and Control", "authorids": ["~Ioannis_Exarchos1", "~Marcus_Aloysius_Pereira1", "~Ziyi_Wang1", "~Evangelos_Theodorou1"], "authors": ["Ioannis Exarchos", "Marcus Aloysius Pereira", "Ziyi Wang", "Evangelos Theodorou"], "keywords": ["deep neural networks", "nested optimization", "stochastic control", "deep FBSDEs"], "abstract": "In this work we propose the use of adaptive stochastic search as a building block for general, non-convex optimization operations within deep neural network architectures. Specifically, for an objective function located at some layer in the network and parameterized by some network parameters, we employ adaptive stochastic search to perform optimization over its output. This operation is differentiable and does not obstruct the passing of gradients during backpropagation, thus enabling us to incorporate it as a component in end-to-end learning. We study the proposed optimization module's properties and benchmark it against two existing alternatives on a synthetic energy-based structured prediction task, and further showcase its use in stochastic optimal control applications.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "exarchos|novas_nonconvex_optimization_via_adaptive_stochastic_search_for_endtoend_learning_and_control", "supplementary_material": "/attachment/5344cbd3208a33aa57b3eee2edee22b6df97cd27.zip", "pdf": "/pdf/19f093001a03a82a092d19740971a45fff9f47a8.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nexarchos2021novas,\ntitle={{\\{}NOVAS{\\}}: Non-convex Optimization via Adaptive Stochastic Search for End-to-end Learning and Control},\nauthor={Ioannis Exarchos and Marcus Aloysius Pereira and Ziyi Wang and Evangelos Theodorou},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Iw4ZGwenbXf}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Iw4ZGwenbXf", "replyto": "Iw4ZGwenbXf", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2254/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538100561, "tmdate": 1606915778623, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2254/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2254/-/Official_Review"}}}, {"id": "XUxcZCTwT1", "original": null, "number": 10, "cdate": 1606161026480, "ddate": null, "tcdate": 1606161026480, "tmdate": 1606161225906, "tddate": null, "forum": "Iw4ZGwenbXf", "replyto": "k6hjEJ3myY", "invitation": "ICLR.cc/2021/Conference/Paper2254/-/Official_Comment", "content": {"title": "Author Response to AnonReviewer2 - Part II", "comment": "5). We agree that the violin plots of Figure 5(b) are informative only to people who are interested in the outcome of the specific optimal control task (performance compared to the wealth index). This plot was indeed provided for those curious about the actual application, and to show that the end result is reasonable (approximately 5% over the market average is a realistic performance gap). However, the violin plots of Figure 5(a) represent the total cost (i.e., the performance objective of the optimal control problem) of our method compared to the baselines. This does not require any knowledge about the particular problem and does indeed show superiority over the baselines. Since the outcome is highly stochastic and volatile (the noise being a dominant factor), rather than presenting the mean cost, we present the approximate full distributions. We updated the captions for clarity. Would the Reviewer prefer a different presentation? \n\n6). We agree that more investigation and analysis on hyper-parameters would be beneficial, and this on several different applications, including meta-learning as other reviewers suggested. We feel that such a broad-usage module as the one we propose would need to be evaluated separately in each application domain (deep-FBSDEs, SPENs, meta-learning, and many more), but it would be impossible to do so for all fields within a single publication. In this paper, our emphasis was on non-convex dynamic optimization for large-scale stochastic optimal control problems.  Problems on stochastic dynamic optimization using optimality principles to solve non-convex Hamiltonians are challenging and scalable algorithms for such problems are non-existent. Therein lies the value of our proposed method. \n\nWe would like to argue that even with the potential weaknesses mentioned by the Reviewer, we believe that this work serves a purpose: (A) it offers an alternative to unrolled GD and unrolled DCEM which researchers and practitioners can test within their own research field. As of now and to the best of our knowledge, only the aforementioned two are available for the type of optimization we consider. (B) compared to unrolled GD, preliminary results show potential decreased vulnerability to local optima, a correctly recovered energy landscape, and no overfitting to hyperparameters (seen in the SPEN example). (C) compared to unrolled DCEM, we offer a 5x speed increase, no need to solve additional convex problems to approximate the eliteness threshold, and higher inner-loop convergence rate. (D) we show that at least in the presented examples, unrolling the graph is not only unnecessary but also comes at a prohibitive memory cost, and (E) we present a new state of the art in deep FBSDE literature by solving fully nonlinear HJB PDEs with non-convex Hamiltonians, which was previously impossible. We believe that these results, despite not being immediately generalizable to every possible domain of application, nevertheless show a clear potential of this method to be useful and even preferable to the aforementioned alternatives. As such, we feel that a publication is justifiable. "}, "signatures": ["ICLR.cc/2021/Conference/Paper2254/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2254/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NOVAS: Non-convex Optimization via Adaptive Stochastic Search for End-to-end Learning and Control", "authorids": ["~Ioannis_Exarchos1", "~Marcus_Aloysius_Pereira1", "~Ziyi_Wang1", "~Evangelos_Theodorou1"], "authors": ["Ioannis Exarchos", "Marcus Aloysius Pereira", "Ziyi Wang", "Evangelos Theodorou"], "keywords": ["deep neural networks", "nested optimization", "stochastic control", "deep FBSDEs"], "abstract": "In this work we propose the use of adaptive stochastic search as a building block for general, non-convex optimization operations within deep neural network architectures. Specifically, for an objective function located at some layer in the network and parameterized by some network parameters, we employ adaptive stochastic search to perform optimization over its output. This operation is differentiable and does not obstruct the passing of gradients during backpropagation, thus enabling us to incorporate it as a component in end-to-end learning. We study the proposed optimization module's properties and benchmark it against two existing alternatives on a synthetic energy-based structured prediction task, and further showcase its use in stochastic optimal control applications.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "exarchos|novas_nonconvex_optimization_via_adaptive_stochastic_search_for_endtoend_learning_and_control", "supplementary_material": "/attachment/5344cbd3208a33aa57b3eee2edee22b6df97cd27.zip", "pdf": "/pdf/19f093001a03a82a092d19740971a45fff9f47a8.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nexarchos2021novas,\ntitle={{\\{}NOVAS{\\}}: Non-convex Optimization via Adaptive Stochastic Search for End-to-end Learning and Control},\nauthor={Ioannis Exarchos and Marcus Aloysius Pereira and Ziyi Wang and Evangelos Theodorou},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Iw4ZGwenbXf}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Iw4ZGwenbXf", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2254/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2254/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2254/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2254/Authors|ICLR.cc/2021/Conference/Paper2254/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2254/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923850544, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2254/-/Official_Comment"}}}, {"id": "k6hjEJ3myY", "original": null, "number": 9, "cdate": 1606160991547, "ddate": null, "tcdate": 1606160991547, "tmdate": 1606160991547, "tddate": null, "forum": "Iw4ZGwenbXf", "replyto": "duXZnhy8Kq3", "invitation": "ICLR.cc/2021/Conference/Paper2254/-/Official_Comment", "content": {"title": "Author Response to AnonReviewer2 - Part I", "comment": "We thank you for the thorough review and constructive criticism. We really appreciate the time invested to provide both. The main issues raised by the reviewer are: \n1). Back-propagating through only the last step provides incomplete information about the loss landscape, as the information contained is of rank 1, and information is localized at x*. \n2). The method we are using has low sample efficiency and, similar to policy gradient methods with which it shares some similarities, will not work for high-dimensional systems. \n3). Non-convex problems don\u2019t necessarily have a unique optimum, and thus non-global methods (including our own) do not offer guarantees against getting stuck in local optima. The same applies for GD, which the author offers as an explanation as to why it does not perform well in the SPEN example, and calls for further tuning effort in the GD case. \n4). Computation time is not the same as inner-loop convergence rate (page 6). \n5). The violin plots offered are not informative by themselves as a metric for comparison between the presented baselines. \n6). Improvement suggestions. \n\nIn a point-by-point response: \n\n1). Without wanting to question the argumentation, we have to admit that we struggle to understand why a single step x\u2019 \u2192 x* contains only information of rank 1 whereas a trajectory of several iterations x1 \u2192 x2 \u2192 \u2026 \u2192 x* is higher rank. This is most likely a confusion from our side, but wouldn\u2019t several iterations perform successive multiplications of the Hessian? How could that increase the rank? In any case, if one does not unroll the graph for GD, then the information at the last step would indeed seem to be localized around x*. But for sampling-based optimization, x* is by definition nothing more than a convex combination of the N sampled x\u2019s: x* = Sum_i w_i x_i, for non-negative w_i that sum up to 1 (in the more general case, there can also be momentum w.r.t. the value of the previous iteration). Doesn\u2019t it seem a bit odd that information is in this case still exclusively localized at a single value, x*?  \n\n2). This is a very interesting point. Again, without wanting to question the Reviewer\u2019s statements, we would like to argue that sampling-based optimization is a well-established field with lots of documented success. CEM, CMA-ES, Monte Carlo sampling and countless gradient-free variants have been used for many years, with notable results. Concerning policy gradient, we agree that its vanilla REINFORCE version is hardly a good option, but policy gradient algorithms such as TRPO and PPO have been dominating the field of robotics for the past 5 years and work very well in high-dimensional tasks. Furthermore, Adaptive Stochastic Search has been used as a trajectory optimization method (e.g. \u201cConstrained sampling-based trajectory optimization using stochastic approximation\u201d, Boutselis et al.) and so has CEM (see \u201cSample-efficient CEM for real-time planning\u201d, Pinneri et al.) in which a discretized (in time) control sequence is treated as the optimization variable and is optimized in its full dimensionality. The resulting control update law in the case of the former is a generalization of the Path Integral control law (see for example MPPI) as well as Information Theoretic MPC. In the controls field, all these algorithms have been used extensively and demonstrated solid performance. Rather than disagreeing with the Reviewer, we are merely surprised by this performance assessment. \n\n3). We updated the manuscript to include the Reviewer\u2019s insight on the argmin being potentially a set-valued map and not differentiable in the classical sense. We agree with the Reviewer concerning the vulnerability of both GD and sampling-based optimizers to local optima. There is some evidence that Gradient-based optimization is actually more prone to getting stuck in local optima, since sampling-based approaches evaluate the objective function over an extended area of the input space and, given enough sampling variability for exploration, have thus more chances of escaping a narrow local optimum. This seems to be the case in our SPEN example as well: all optimizers were initialized at the same x = 0; only unrolled GD failed to obtain a good solution. We did experiment with GD hyper-parameters but did not observe significant difference. We feel that changing the GD initialization would provide an unfair advantage not just to our method, but also to unrolled DCEM. Note that our work is not the first to report such failure of unrolled GD; the authors of DCEM made a similar observation. \n\n4). We totally agree. Our remark on inner-loop convergence is based on Fig. 2 (c), not (b) reporting computation time. 2(c) compares only DCEM with NOVAS, since nothing can be inferred about GD from that figure. "}, "signatures": ["ICLR.cc/2021/Conference/Paper2254/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2254/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NOVAS: Non-convex Optimization via Adaptive Stochastic Search for End-to-end Learning and Control", "authorids": ["~Ioannis_Exarchos1", "~Marcus_Aloysius_Pereira1", "~Ziyi_Wang1", "~Evangelos_Theodorou1"], "authors": ["Ioannis Exarchos", "Marcus Aloysius Pereira", "Ziyi Wang", "Evangelos Theodorou"], "keywords": ["deep neural networks", "nested optimization", "stochastic control", "deep FBSDEs"], "abstract": "In this work we propose the use of adaptive stochastic search as a building block for general, non-convex optimization operations within deep neural network architectures. Specifically, for an objective function located at some layer in the network and parameterized by some network parameters, we employ adaptive stochastic search to perform optimization over its output. This operation is differentiable and does not obstruct the passing of gradients during backpropagation, thus enabling us to incorporate it as a component in end-to-end learning. We study the proposed optimization module's properties and benchmark it against two existing alternatives on a synthetic energy-based structured prediction task, and further showcase its use in stochastic optimal control applications.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "exarchos|novas_nonconvex_optimization_via_adaptive_stochastic_search_for_endtoend_learning_and_control", "supplementary_material": "/attachment/5344cbd3208a33aa57b3eee2edee22b6df97cd27.zip", "pdf": "/pdf/19f093001a03a82a092d19740971a45fff9f47a8.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nexarchos2021novas,\ntitle={{\\{}NOVAS{\\}}: Non-convex Optimization via Adaptive Stochastic Search for End-to-end Learning and Control},\nauthor={Ioannis Exarchos and Marcus Aloysius Pereira and Ziyi Wang and Evangelos Theodorou},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Iw4ZGwenbXf}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Iw4ZGwenbXf", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2254/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2254/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2254/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2254/Authors|ICLR.cc/2021/Conference/Paper2254/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2254/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923850544, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2254/-/Official_Comment"}}}, {"id": "s3mUucmFLrR", "original": null, "number": 8, "cdate": 1606160762161, "ddate": null, "tcdate": 1606160762161, "tmdate": 1606160762161, "tddate": null, "forum": "Iw4ZGwenbXf", "replyto": "2J-HqSErnnN", "invitation": "ICLR.cc/2021/Conference/Paper2254/-/Official_Comment", "content": {"title": "Author Response to AnonReviewer4", "comment": "We would like to thank you for the positive reception of our paper. We absolutely agree that with respect to SPENs, the results presented are not enough to claim any advantage or superiority of our method specifically within the SPEN literature. The SPEN toy-example is used merely as a simple benchmark illustrating the various differences between unrolled GD, unrolled DCEM, and our method. More experiments are indeed needed to establish a proof of superior performance of our algorithm in the SPEN domain; this is left as a future research direction because of lack of space in the paper, as well as due to the fact that our emphasis in this paper was on non-convex dynamic optimization for large-scale stochastic optimal control problems.  We would have liked to compare the alternatives in the finance example, but their memory cost is prohibitive; we feel that this fact by itself advocates for the utility of our proposed approach. "}, "signatures": ["ICLR.cc/2021/Conference/Paper2254/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2254/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NOVAS: Non-convex Optimization via Adaptive Stochastic Search for End-to-end Learning and Control", "authorids": ["~Ioannis_Exarchos1", "~Marcus_Aloysius_Pereira1", "~Ziyi_Wang1", "~Evangelos_Theodorou1"], "authors": ["Ioannis Exarchos", "Marcus Aloysius Pereira", "Ziyi Wang", "Evangelos Theodorou"], "keywords": ["deep neural networks", "nested optimization", "stochastic control", "deep FBSDEs"], "abstract": "In this work we propose the use of adaptive stochastic search as a building block for general, non-convex optimization operations within deep neural network architectures. Specifically, for an objective function located at some layer in the network and parameterized by some network parameters, we employ adaptive stochastic search to perform optimization over its output. This operation is differentiable and does not obstruct the passing of gradients during backpropagation, thus enabling us to incorporate it as a component in end-to-end learning. We study the proposed optimization module's properties and benchmark it against two existing alternatives on a synthetic energy-based structured prediction task, and further showcase its use in stochastic optimal control applications.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "exarchos|novas_nonconvex_optimization_via_adaptive_stochastic_search_for_endtoend_learning_and_control", "supplementary_material": "/attachment/5344cbd3208a33aa57b3eee2edee22b6df97cd27.zip", "pdf": "/pdf/19f093001a03a82a092d19740971a45fff9f47a8.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nexarchos2021novas,\ntitle={{\\{}NOVAS{\\}}: Non-convex Optimization via Adaptive Stochastic Search for End-to-end Learning and Control},\nauthor={Ioannis Exarchos and Marcus Aloysius Pereira and Ziyi Wang and Evangelos Theodorou},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Iw4ZGwenbXf}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Iw4ZGwenbXf", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2254/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2254/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2254/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2254/Authors|ICLR.cc/2021/Conference/Paper2254/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2254/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923850544, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2254/-/Official_Comment"}}}, {"id": "cENL9Vlgcu", "original": null, "number": 7, "cdate": 1606160491790, "ddate": null, "tcdate": 1606160491790, "tmdate": 1606160564141, "tddate": null, "forum": "Iw4ZGwenbXf", "replyto": "gVM6LPu_Ol3", "invitation": "ICLR.cc/2021/Conference/Paper2254/-/Official_Comment", "content": {"title": "Author Response to AnonReviewer1", "comment": "Thank you very much for the positive reception of our paper. Concerning the Reviewers\u2019 feedback: (1) we were not aware of the results mentioned by the Reviewer in the meta-learning literature that do not involve unrolling of the graph. We included First-Order MAML as well as implicit MAML (which is what we think the Reviewer is also referring to) in the literature review. We thank the Reviewer for pointing out this omission. (2) Thank you for mentioning it: the results depicted in Fig. 2 (a) are losses on the test set. The caption has been updated. (3). We agree that testing NOVAS in meta-learning would be an exciting future research direction; we did not pursue this for lack of space in the paper, and because our emphasis in this paper was on non-convex dynamic optimization for large-scale stochastic optimal control problems. Concerning the suggestion for Fig 1 (b) and (c), we mainly wanted to show that NOVAS, in both its rolled and unrolled form, does indeed recover the correct energy landscape, as opposed to unrolled GD. If we were to plot their difference, this would distort the comparison to the ground-truth landscape. "}, "signatures": ["ICLR.cc/2021/Conference/Paper2254/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2254/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NOVAS: Non-convex Optimization via Adaptive Stochastic Search for End-to-end Learning and Control", "authorids": ["~Ioannis_Exarchos1", "~Marcus_Aloysius_Pereira1", "~Ziyi_Wang1", "~Evangelos_Theodorou1"], "authors": ["Ioannis Exarchos", "Marcus Aloysius Pereira", "Ziyi Wang", "Evangelos Theodorou"], "keywords": ["deep neural networks", "nested optimization", "stochastic control", "deep FBSDEs"], "abstract": "In this work we propose the use of adaptive stochastic search as a building block for general, non-convex optimization operations within deep neural network architectures. Specifically, for an objective function located at some layer in the network and parameterized by some network parameters, we employ adaptive stochastic search to perform optimization over its output. This operation is differentiable and does not obstruct the passing of gradients during backpropagation, thus enabling us to incorporate it as a component in end-to-end learning. We study the proposed optimization module's properties and benchmark it against two existing alternatives on a synthetic energy-based structured prediction task, and further showcase its use in stochastic optimal control applications.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "exarchos|novas_nonconvex_optimization_via_adaptive_stochastic_search_for_endtoend_learning_and_control", "supplementary_material": "/attachment/5344cbd3208a33aa57b3eee2edee22b6df97cd27.zip", "pdf": "/pdf/19f093001a03a82a092d19740971a45fff9f47a8.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nexarchos2021novas,\ntitle={{\\{}NOVAS{\\}}: Non-convex Optimization via Adaptive Stochastic Search for End-to-end Learning and Control},\nauthor={Ioannis Exarchos and Marcus Aloysius Pereira and Ziyi Wang and Evangelos Theodorou},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Iw4ZGwenbXf}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Iw4ZGwenbXf", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2254/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2254/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2254/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2254/Authors|ICLR.cc/2021/Conference/Paper2254/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2254/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923850544, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2254/-/Official_Comment"}}}, {"id": "1Y82HYHHCn", "original": null, "number": 6, "cdate": 1606160364388, "ddate": null, "tcdate": 1606160364388, "tmdate": 1606160552629, "tddate": null, "forum": "Iw4ZGwenbXf", "replyto": "grUp8I9Xtqt", "invitation": "ICLR.cc/2021/Conference/Paper2254/-/Official_Comment", "content": {"title": "Author Response to Official Blind Review #3", "comment": "Thank you for your feedback. Following your suggestion, we moved the NOVAS description given by  Alg. 1 from the Appendix to the main body of the paper, along with more details concerning its practical implementation. Several additional details, including hyperparameter sensitivity, are included in the Appendix (Section A1). Concerning the lack of a baseline for the cart-pole example of Section 4.2, we didn\u2019t include one because there is no reason to use NOVAS in such a problem; the Hamiltonian minimization problem treated by the NOVAS module in this particular case is not only convex but also quadratic, and thus allows for a closed-form solution, which is what Pereira et al. exploit to solve it using deep FBSDEs. We included this example merely as a \u201csanity check\u201d: the cart-pole problem is well-known and has an easily interpretable solution, and we wanted to show that combining NOVAS with deep FBSDEs would yield the usual, expected results. We expect the NOVAS-reliant deep FBSDE to be slower and possibly less accurate in this case than the deep FBSDE algorithm by Pereira et al. since it is numerically calculating a solution that has been \u201chard-coded\u201d in the latter, but the utility of NOVAS lies in problems that do not allow for such analytic solutions. In the finance example, the Hamiltonian minimization is non-convex and such a closed-form solution does not exist, therefore the framework by Pereira et al cannot be used. Nevertheless, the finance example is less well-known than the cart-pole and with perhaps less intuitive/anticipated solution. We therefore opted to present results for the cart-pole as well, mainly to increase the readers\u2019 confidence in the algorithm. Concerning the use of U-NOVAS in meta-learning, the Reviewer is absolutely right; U-NOVAS can indeed replace the typical unrolled gradient descent-based adaptation rule. However, for lack of space, and because our emphasis was on non-convex dynamic optimization for large-scale stochastic optimal control problems, we did not investigate this. Nevertheless, we did include this observation in the introduction of the updated manuscript. An important final issue is raised by the Reviewer, namely whether other methods such as DCEM are unable to avoid unrolling the graph. We do not make such a claim (in fact, we believe that one could indeed possibly avoid it, though still having to deal with the significant computational overhead of DCEM over NOVAS, as well as its slower convergence rate). However, we implemented DCEM exactly as it was proposed in the cited paper, and the implementation involves unrolling the graph (a fact we also confirmed by communicating with the DCEM lead author directly). The authors of DCEM intended it to be unrolled, and the discovery that unrolling can be unnecessary in such problems is part of the contributions of our paper. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2254/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2254/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NOVAS: Non-convex Optimization via Adaptive Stochastic Search for End-to-end Learning and Control", "authorids": ["~Ioannis_Exarchos1", "~Marcus_Aloysius_Pereira1", "~Ziyi_Wang1", "~Evangelos_Theodorou1"], "authors": ["Ioannis Exarchos", "Marcus Aloysius Pereira", "Ziyi Wang", "Evangelos Theodorou"], "keywords": ["deep neural networks", "nested optimization", "stochastic control", "deep FBSDEs"], "abstract": "In this work we propose the use of adaptive stochastic search as a building block for general, non-convex optimization operations within deep neural network architectures. Specifically, for an objective function located at some layer in the network and parameterized by some network parameters, we employ adaptive stochastic search to perform optimization over its output. This operation is differentiable and does not obstruct the passing of gradients during backpropagation, thus enabling us to incorporate it as a component in end-to-end learning. We study the proposed optimization module's properties and benchmark it against two existing alternatives on a synthetic energy-based structured prediction task, and further showcase its use in stochastic optimal control applications.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "exarchos|novas_nonconvex_optimization_via_adaptive_stochastic_search_for_endtoend_learning_and_control", "supplementary_material": "/attachment/5344cbd3208a33aa57b3eee2edee22b6df97cd27.zip", "pdf": "/pdf/19f093001a03a82a092d19740971a45fff9f47a8.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nexarchos2021novas,\ntitle={{\\{}NOVAS{\\}}: Non-convex Optimization via Adaptive Stochastic Search for End-to-end Learning and Control},\nauthor={Ioannis Exarchos and Marcus Aloysius Pereira and Ziyi Wang and Evangelos Theodorou},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Iw4ZGwenbXf}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Iw4ZGwenbXf", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2254/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2254/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2254/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2254/Authors|ICLR.cc/2021/Conference/Paper2254/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2254/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923850544, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2254/-/Official_Comment"}}}, {"id": "algEywQ-quT", "original": null, "number": 5, "cdate": 1606160228240, "ddate": null, "tcdate": 1606160228240, "tmdate": 1606160249451, "tddate": null, "forum": "Iw4ZGwenbXf", "replyto": "Iw4ZGwenbXf", "invitation": "ICLR.cc/2021/Conference/Paper2254/-/Official_Comment", "content": {"title": "First Revision Author Response", "comment": "We would like to thank all reviewers for their constructive feedback and suggestions. We updated the manuscript to reflect the Reviewers' improvement suggestions; changes in the manuscript are indicated by blue font for convenience."}, "signatures": ["ICLR.cc/2021/Conference/Paper2254/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2254/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NOVAS: Non-convex Optimization via Adaptive Stochastic Search for End-to-end Learning and Control", "authorids": ["~Ioannis_Exarchos1", "~Marcus_Aloysius_Pereira1", "~Ziyi_Wang1", "~Evangelos_Theodorou1"], "authors": ["Ioannis Exarchos", "Marcus Aloysius Pereira", "Ziyi Wang", "Evangelos Theodorou"], "keywords": ["deep neural networks", "nested optimization", "stochastic control", "deep FBSDEs"], "abstract": "In this work we propose the use of adaptive stochastic search as a building block for general, non-convex optimization operations within deep neural network architectures. Specifically, for an objective function located at some layer in the network and parameterized by some network parameters, we employ adaptive stochastic search to perform optimization over its output. This operation is differentiable and does not obstruct the passing of gradients during backpropagation, thus enabling us to incorporate it as a component in end-to-end learning. We study the proposed optimization module's properties and benchmark it against two existing alternatives on a synthetic energy-based structured prediction task, and further showcase its use in stochastic optimal control applications.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "exarchos|novas_nonconvex_optimization_via_adaptive_stochastic_search_for_endtoend_learning_and_control", "supplementary_material": "/attachment/5344cbd3208a33aa57b3eee2edee22b6df97cd27.zip", "pdf": "/pdf/19f093001a03a82a092d19740971a45fff9f47a8.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nexarchos2021novas,\ntitle={{\\{}NOVAS{\\}}: Non-convex Optimization via Adaptive Stochastic Search for End-to-end Learning and Control},\nauthor={Ioannis Exarchos and Marcus Aloysius Pereira and Ziyi Wang and Evangelos Theodorou},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Iw4ZGwenbXf}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Iw4ZGwenbXf", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2254/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2254/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2254/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2254/Authors|ICLR.cc/2021/Conference/Paper2254/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2254/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923850544, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2254/-/Official_Comment"}}}, {"id": "2J-HqSErnnN", "original": null, "number": 2, "cdate": 1603894694774, "ddate": null, "tcdate": 1603894694774, "tmdate": 1605024253959, "tddate": null, "forum": "Iw4ZGwenbXf", "replyto": "Iw4ZGwenbXf", "invitation": "ICLR.cc/2021/Conference/Paper2254/-/Official_Review", "content": {"title": "Efficient Inner Optimization module for networks", "review": "This paper proposes using adaptive stochastic search as an optimization module within deep neural networks to perform general non-convex optimization.  This is used as a block within deep FBSDEs,which in general do not have a closed form optimization solution, and use the resulting network to show state of the art results in solving high dimensional PDEs on a 101 dimensional portfolio optimization problem.\n\nPros : \n1. Using adaptive stochastic search allows the inner optimization module to take multiple iterations without unrolling the computation graph (unlike meta-learning methods), since the initial value used by the module is arbitrary, and not provided by the network. This greatly reduces memory requirements, and also speeds up learning and convergence. The authors empirically show a 5x speed-up in using their approach compared to unrolled differentiable cross entropy and better qualitative performance than unrolled gradient descent for training an energy function for a simple regression task.\n\n2. Authors confirm that their approach of using the optimization approach within a deep FBSDE works as expected by getting the optimal solution for cartpole, then use their method to beat random and constant strategies on a high dimensional portfolio optimization problem, for which it is not possible to use methods that unroll the computation graph (due to memory issues).\n\n\nCons:\n1. More experiments for Structured Energy Prediction Networks with more challenging functions would give a better indication of the limits of the proposed approach in comparison to prior work. It is also unclear if the proposed approach would show worse performance for non-toy datasets where a few iterations of unrolling gradient descent or differentiable cross entropy are sufficient. This is especially since the only non-toy experiment was performed in a domain where other methods (differentiable cross entropy and gradient descent) couldn't be run.", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2254/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2254/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NOVAS: Non-convex Optimization via Adaptive Stochastic Search for End-to-end Learning and Control", "authorids": ["~Ioannis_Exarchos1", "~Marcus_Aloysius_Pereira1", "~Ziyi_Wang1", "~Evangelos_Theodorou1"], "authors": ["Ioannis Exarchos", "Marcus Aloysius Pereira", "Ziyi Wang", "Evangelos Theodorou"], "keywords": ["deep neural networks", "nested optimization", "stochastic control", "deep FBSDEs"], "abstract": "In this work we propose the use of adaptive stochastic search as a building block for general, non-convex optimization operations within deep neural network architectures. Specifically, for an objective function located at some layer in the network and parameterized by some network parameters, we employ adaptive stochastic search to perform optimization over its output. This operation is differentiable and does not obstruct the passing of gradients during backpropagation, thus enabling us to incorporate it as a component in end-to-end learning. We study the proposed optimization module's properties and benchmark it against two existing alternatives on a synthetic energy-based structured prediction task, and further showcase its use in stochastic optimal control applications.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "exarchos|novas_nonconvex_optimization_via_adaptive_stochastic_search_for_endtoend_learning_and_control", "supplementary_material": "/attachment/5344cbd3208a33aa57b3eee2edee22b6df97cd27.zip", "pdf": "/pdf/19f093001a03a82a092d19740971a45fff9f47a8.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nexarchos2021novas,\ntitle={{\\{}NOVAS{\\}}: Non-convex Optimization via Adaptive Stochastic Search for End-to-end Learning and Control},\nauthor={Ioannis Exarchos and Marcus Aloysius Pereira and Ziyi Wang and Evangelos Theodorou},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Iw4ZGwenbXf}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Iw4ZGwenbXf", "replyto": "Iw4ZGwenbXf", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2254/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538100561, "tmdate": 1606915778623, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2254/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2254/-/Official_Review"}}}], "count": 12}