{"notes": [{"tddate": null, "ddate": null, "tmdate": 1502980669943, "tcdate": 1502980669943, "number": 11, "cdate": 1502980669943, "id": "B18BcXXO-", "invitation": "ICLR.cc/2017/conference/-/paper302/public/comment", "forum": "rJbbOLcex", "replyto": "ryxafJWO-", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "Is the length-K topic vector meant to represent topic proportions among K topics?", "comment": "In most of the paper the topic vector (whether the prior or the posterior) is modeled as a Gaussian, and as such its components can be negative. This seems consistent with NOT interpreting the topic vector as a mixture (or proportions) of topics.  However in Figure 2 the topic vector IS being interpreted as topic proportions, and all components are non-negative. I'm just bothered by this inconsistency. "}, "nonreaders": ["abd2141@columbia.edu", "chowang@microsoft.com", "jfgao@microsoft.com", "jpaisley@columbia.edu"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "TopicRNN: A Recurrent Neural Network with Long-Range Semantic Dependency", "abstract": "In this paper, we propose TopicRNN, a recurrent neural network (RNN)-based language model designed to directly capture the global semantic meaning relating words in a document via latent topics. Because of their sequential nature, RNNs are good at capturing the local structure of a word sequence \u2013 both semantic and syntactic \u2013 but might face difficulty remembering long-range dependencies. Intuitively, these long-range dependencies are of semantic nature. In contrast, latent topic models are able to capture the global underlying semantic structure of a document but do not account for word ordering. The proposed TopicRNN model integrates the merits of RNNs and latent topic models: it captures local (syntactic) dependencies using an RNN and global (semantic) dependencies using latent topics. Unlike previous work on contextual RNN language modeling, our model is learned end-to-end. Empirical results on word prediction show that TopicRNN outperforms existing contextual RNN baselines. In addition, TopicRNN can be used as an unsupervised feature extractor for documents. We do this for sentiment analysis on the IMDB movie review dataset and report an error rate of 6.28%. This is comparable to the state-of-the-art 5.91% resulting from a semi-supervised approach. Finally, TopicRNN also yields sensible topics, making it a useful alternative to document models such as latent Dirichlet allocation.", "pdf": "/pdf/180fcd5e6174cdcecfa7055f7f9afd5d3c326888.pdf", "paperhash": "dieng|topicrnn_a_recurrent_neural_network_with_longrange_semantic_dependency", "keywords": ["Natural language processing", "Deep learning"], "conflicts": ["microsoft.com", "columbia.edu"], "authors": ["Adji B. Dieng", "Chong Wang", "Jianfeng Gao", "John Paisley"], "authorids": ["abd2141@columbia.edu", "chowang@microsoft.com", "jfgao@microsoft.com", "jpaisley@columbia.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287630900, "id": "ICLR.cc/2017/conference/-/paper302/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rJbbOLcex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper302/reviewers", "ICLR.cc/2017/conference/paper302/areachairs"], "cdate": 1485287630900}}}, {"tddate": null, "ddate": null, "tmdate": 1502831287886, "tcdate": 1502831287886, "number": 10, "cdate": 1502831287886, "id": "ryxafJWO-", "invitation": "ICLR.cc/2017/conference/-/paper302/public/comment", "forum": "rJbbOLcex", "replyto": "rJbbOLcex", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "Distribution of the topic vector", "comment": "In the generative model in Sec 3 you're drawing a K-dimensional \"topic vector\" theta from a normal distribution N(0,I). If these are intended to be topic proportions, how is a normal distribution suited to this, since the components of theta can be negative? I would have thought maybe a logistic-normal distribution would be more suitable for this?"}, "nonreaders": ["abd2141@columbia.edu", "chowang@microsoft.com", "jfgao@microsoft.com", "jpaisley@columbia.edu"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "TopicRNN: A Recurrent Neural Network with Long-Range Semantic Dependency", "abstract": "In this paper, we propose TopicRNN, a recurrent neural network (RNN)-based language model designed to directly capture the global semantic meaning relating words in a document via latent topics. Because of their sequential nature, RNNs are good at capturing the local structure of a word sequence \u2013 both semantic and syntactic \u2013 but might face difficulty remembering long-range dependencies. Intuitively, these long-range dependencies are of semantic nature. In contrast, latent topic models are able to capture the global underlying semantic structure of a document but do not account for word ordering. The proposed TopicRNN model integrates the merits of RNNs and latent topic models: it captures local (syntactic) dependencies using an RNN and global (semantic) dependencies using latent topics. Unlike previous work on contextual RNN language modeling, our model is learned end-to-end. Empirical results on word prediction show that TopicRNN outperforms existing contextual RNN baselines. In addition, TopicRNN can be used as an unsupervised feature extractor for documents. We do this for sentiment analysis on the IMDB movie review dataset and report an error rate of 6.28%. This is comparable to the state-of-the-art 5.91% resulting from a semi-supervised approach. Finally, TopicRNN also yields sensible topics, making it a useful alternative to document models such as latent Dirichlet allocation.", "pdf": "/pdf/180fcd5e6174cdcecfa7055f7f9afd5d3c326888.pdf", "paperhash": "dieng|topicrnn_a_recurrent_neural_network_with_longrange_semantic_dependency", "keywords": ["Natural language processing", "Deep learning"], "conflicts": ["microsoft.com", "columbia.edu"], "authors": ["Adji B. Dieng", "Chong Wang", "Jianfeng Gao", "John Paisley"], "authorids": ["abd2141@columbia.edu", "chowang@microsoft.com", "jfgao@microsoft.com", "jpaisley@columbia.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287630900, "id": "ICLR.cc/2017/conference/-/paper302/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rJbbOLcex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper302/reviewers", "ICLR.cc/2017/conference/paper302/areachairs"], "cdate": 1485287630900}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1488155653156, "tcdate": 1478285305108, "number": 302, "id": "rJbbOLcex", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "rJbbOLcex", "signatures": ["~Adji_Bousso_Dieng1"], "readers": ["everyone"], "content": {"TL;DR": "", "title": "TopicRNN: A Recurrent Neural Network with Long-Range Semantic Dependency", "abstract": "In this paper, we propose TopicRNN, a recurrent neural network (RNN)-based language model designed to directly capture the global semantic meaning relating words in a document via latent topics. Because of their sequential nature, RNNs are good at capturing the local structure of a word sequence \u2013 both semantic and syntactic \u2013 but might face difficulty remembering long-range dependencies. Intuitively, these long-range dependencies are of semantic nature. In contrast, latent topic models are able to capture the global underlying semantic structure of a document but do not account for word ordering. The proposed TopicRNN model integrates the merits of RNNs and latent topic models: it captures local (syntactic) dependencies using an RNN and global (semantic) dependencies using latent topics. Unlike previous work on contextual RNN language modeling, our model is learned end-to-end. Empirical results on word prediction show that TopicRNN outperforms existing contextual RNN baselines. In addition, TopicRNN can be used as an unsupervised feature extractor for documents. We do this for sentiment analysis on the IMDB movie review dataset and report an error rate of 6.28%. This is comparable to the state-of-the-art 5.91% resulting from a semi-supervised approach. Finally, TopicRNN also yields sensible topics, making it a useful alternative to document models such as latent Dirichlet allocation.", "pdf": "/pdf/180fcd5e6174cdcecfa7055f7f9afd5d3c326888.pdf", "paperhash": "dieng|topicrnn_a_recurrent_neural_network_with_longrange_semantic_dependency", "keywords": ["Natural language processing", "Deep learning"], "conflicts": ["microsoft.com", "columbia.edu"], "authors": ["Adji B. Dieng", "Chong Wang", "Jianfeng Gao", "John Paisley"], "authorids": ["abd2141@columbia.edu", "chowang@microsoft.com", "jfgao@microsoft.com", "jpaisley@columbia.edu"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 18, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396492412, "tcdate": 1486396492412, "number": 1, "id": "HJVI3zIug", "invitation": "ICLR.cc/2017/conference/-/paper302/acceptance", "forum": "rJbbOLcex", "replyto": "rJbbOLcex", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"title": "ICLR committee final decision", "comment": "Though the have been attempts to incorporate both \"topic-like\" and \"sequence-like\" methods in the past (e.g, the work of Hanna Wallach, Amit Gruber and other), they were quite computationally expensive, especially when high-order ngrams are incorporated. This is a modern take on this challenge: using RNNs and the VAE / inference network framework. The results are quite convincing, and the paper is well written.\n \n Pros:\n -- clean and simple model\n -- sufficiently convincing experimentation\n \n Cons:\n -- other ways to model interaction between RNN and topic representation could be considered (see comments of R2 and R1)", "decision": "Accept (Poster)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "TopicRNN: A Recurrent Neural Network with Long-Range Semantic Dependency", "abstract": "In this paper, we propose TopicRNN, a recurrent neural network (RNN)-based language model designed to directly capture the global semantic meaning relating words in a document via latent topics. Because of their sequential nature, RNNs are good at capturing the local structure of a word sequence \u2013 both semantic and syntactic \u2013 but might face difficulty remembering long-range dependencies. Intuitively, these long-range dependencies are of semantic nature. In contrast, latent topic models are able to capture the global underlying semantic structure of a document but do not account for word ordering. The proposed TopicRNN model integrates the merits of RNNs and latent topic models: it captures local (syntactic) dependencies using an RNN and global (semantic) dependencies using latent topics. Unlike previous work on contextual RNN language modeling, our model is learned end-to-end. Empirical results on word prediction show that TopicRNN outperforms existing contextual RNN baselines. In addition, TopicRNN can be used as an unsupervised feature extractor for documents. We do this for sentiment analysis on the IMDB movie review dataset and report an error rate of 6.28%. This is comparable to the state-of-the-art 5.91% resulting from a semi-supervised approach. Finally, TopicRNN also yields sensible topics, making it a useful alternative to document models such as latent Dirichlet allocation.", "pdf": "/pdf/180fcd5e6174cdcecfa7055f7f9afd5d3c326888.pdf", "paperhash": "dieng|topicrnn_a_recurrent_neural_network_with_longrange_semantic_dependency", "keywords": ["Natural language processing", "Deep learning"], "conflicts": ["microsoft.com", "columbia.edu"], "authors": ["Adji B. Dieng", "Chong Wang", "Jianfeng Gao", "John Paisley"], "authorids": ["abd2141@columbia.edu", "chowang@microsoft.com", "jfgao@microsoft.com", "jpaisley@columbia.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396492916, "id": "ICLR.cc/2017/conference/-/paper302/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "rJbbOLcex", "replyto": "rJbbOLcex", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396492916}}}, {"tddate": null, "tmdate": 1484953204891, "tcdate": 1484953204891, "number": 9, "id": "HkadLMxvg", "invitation": "ICLR.cc/2017/conference/-/paper302/public/comment", "forum": "rJbbOLcex", "replyto": "SyoGQzeDx", "signatures": ["~Adji_Bousso_Dieng1"], "readers": ["everyone"], "writers": ["~Adji_Bousso_Dieng1"], "content": {"title": "Re: Thanks for the bug correction", "comment": "Thank you AnonReviewer1!"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "TopicRNN: A Recurrent Neural Network with Long-Range Semantic Dependency", "abstract": "In this paper, we propose TopicRNN, a recurrent neural network (RNN)-based language model designed to directly capture the global semantic meaning relating words in a document via latent topics. Because of their sequential nature, RNNs are good at capturing the local structure of a word sequence \u2013 both semantic and syntactic \u2013 but might face difficulty remembering long-range dependencies. Intuitively, these long-range dependencies are of semantic nature. In contrast, latent topic models are able to capture the global underlying semantic structure of a document but do not account for word ordering. The proposed TopicRNN model integrates the merits of RNNs and latent topic models: it captures local (syntactic) dependencies using an RNN and global (semantic) dependencies using latent topics. Unlike previous work on contextual RNN language modeling, our model is learned end-to-end. Empirical results on word prediction show that TopicRNN outperforms existing contextual RNN baselines. In addition, TopicRNN can be used as an unsupervised feature extractor for documents. We do this for sentiment analysis on the IMDB movie review dataset and report an error rate of 6.28%. This is comparable to the state-of-the-art 5.91% resulting from a semi-supervised approach. Finally, TopicRNN also yields sensible topics, making it a useful alternative to document models such as latent Dirichlet allocation.", "pdf": "/pdf/180fcd5e6174cdcecfa7055f7f9afd5d3c326888.pdf", "paperhash": "dieng|topicrnn_a_recurrent_neural_network_with_longrange_semantic_dependency", "keywords": ["Natural language processing", "Deep learning"], "conflicts": ["microsoft.com", "columbia.edu"], "authors": ["Adji B. Dieng", "Chong Wang", "Jianfeng Gao", "John Paisley"], "authorids": ["abd2141@columbia.edu", "chowang@microsoft.com", "jfgao@microsoft.com", "jpaisley@columbia.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287630900, "id": "ICLR.cc/2017/conference/-/paper302/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rJbbOLcex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper302/reviewers", "ICLR.cc/2017/conference/paper302/areachairs"], "cdate": 1485287630900}}}, {"tddate": null, "tmdate": 1484952338752, "tcdate": 1484952338752, "number": 3, "id": "SyoGQzeDx", "invitation": "ICLR.cc/2017/conference/-/paper302/official/comment", "forum": "rJbbOLcex", "replyto": "H1rG20tUg", "signatures": ["ICLR.cc/2017/conference/paper302/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper302/AnonReviewer1"], "content": {"title": "Thanks for the bug correction", "comment": "Thanks for the updated paper and the results with the bug corrected. I am keeping my rating."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "TopicRNN: A Recurrent Neural Network with Long-Range Semantic Dependency", "abstract": "In this paper, we propose TopicRNN, a recurrent neural network (RNN)-based language model designed to directly capture the global semantic meaning relating words in a document via latent topics. Because of their sequential nature, RNNs are good at capturing the local structure of a word sequence \u2013 both semantic and syntactic \u2013 but might face difficulty remembering long-range dependencies. Intuitively, these long-range dependencies are of semantic nature. In contrast, latent topic models are able to capture the global underlying semantic structure of a document but do not account for word ordering. The proposed TopicRNN model integrates the merits of RNNs and latent topic models: it captures local (syntactic) dependencies using an RNN and global (semantic) dependencies using latent topics. Unlike previous work on contextual RNN language modeling, our model is learned end-to-end. Empirical results on word prediction show that TopicRNN outperforms existing contextual RNN baselines. In addition, TopicRNN can be used as an unsupervised feature extractor for documents. We do this for sentiment analysis on the IMDB movie review dataset and report an error rate of 6.28%. This is comparable to the state-of-the-art 5.91% resulting from a semi-supervised approach. Finally, TopicRNN also yields sensible topics, making it a useful alternative to document models such as latent Dirichlet allocation.", "pdf": "/pdf/180fcd5e6174cdcecfa7055f7f9afd5d3c326888.pdf", "paperhash": "dieng|topicrnn_a_recurrent_neural_network_with_longrange_semantic_dependency", "keywords": ["Natural language processing", "Deep learning"], "conflicts": ["microsoft.com", "columbia.edu"], "authors": ["Adji B. Dieng", "Chong Wang", "Jianfeng Gao", "John Paisley"], "authorids": ["abd2141@columbia.edu", "chowang@microsoft.com", "jfgao@microsoft.com", "jpaisley@columbia.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287630740, "id": "ICLR.cc/2017/conference/-/paper302/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "rJbbOLcex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper302/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper302/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper302/reviewers", "ICLR.cc/2017/conference/paper302/areachairs"], "cdate": 1485287630740}}}, {"tddate": null, "tmdate": 1484876597258, "tcdate": 1484544283151, "number": 5, "id": "BkQQK0KIe", "invitation": "ICLR.cc/2017/conference/-/paper302/public/comment", "forum": "rJbbOLcex", "replyto": "rJbbOLcex", "signatures": ["~Adji_Bousso_Dieng1"], "readers": ["everyone"], "writers": ["~Adji_Bousso_Dieng1"], "content": {"title": "General Answer To Reviewers and ACs", "comment": "We thank the reviewers and the anonymous commenters for the helpful feedback and questions!\n\u00a0\nWe first summarize the main idea of this paper below:\n\u00a0\nNeural network-based language models have achieved state of the art results on many NLP tasks. One difficult problem is to capture long-range dependencies as motivated in the introduction of this paper. We propose to solve this by integrating latent topics as context and jointly training these contextual features with the parameters of an RNN language model. We provide a natural way of doing this integration by modeling stop words that are excluded by topic models but needed for sequential language models. This is done via binary classification where the probability of being a stop word is dictated by the hidden layer of the RNN. This modeling approach is possible when the contextual features as provided by the topics are passed directly to the softmax output layer of the RNN as additional bias. We illustrate the performance of this approach on two tasks and two datasets: word prediction on PTB and sentiment analysis on IMDB.\u00a0 We provide competitive perplexity scores on PTB showing more generalization capabilities (for example we only need a TopicGRU with 100 neurons to achieve a better perplexity than stacking 2 LSTMs with 200 neurons each ---112.4 vs 115.9). \"This method of jointly modeling topics and a language model seems effective and relatively easy to implement.\" quoted from AnonReviewer1.\n\u00a0\nWe have revised the paper and added the following changes:\n1- we added a line on the middle of page 7 to clarify even more how we compute the topic vector \\theta using a sliding window for word prediction.\n2- we added the test perplexity scores for TopicRNN, TopicLSTM, and TopicGRU as required by AnonReviewer3.\n3- we added the inferred distributions from some documents as required by AnonReviewer1.\n4- we added an explanation of why we passed the topics directly to the output layer at the bottom of page 4. \n\nWe answer each reviewer individually. See below."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "TopicRNN: A Recurrent Neural Network with Long-Range Semantic Dependency", "abstract": "In this paper, we propose TopicRNN, a recurrent neural network (RNN)-based language model designed to directly capture the global semantic meaning relating words in a document via latent topics. Because of their sequential nature, RNNs are good at capturing the local structure of a word sequence \u2013 both semantic and syntactic \u2013 but might face difficulty remembering long-range dependencies. Intuitively, these long-range dependencies are of semantic nature. In contrast, latent topic models are able to capture the global underlying semantic structure of a document but do not account for word ordering. The proposed TopicRNN model integrates the merits of RNNs and latent topic models: it captures local (syntactic) dependencies using an RNN and global (semantic) dependencies using latent topics. Unlike previous work on contextual RNN language modeling, our model is learned end-to-end. Empirical results on word prediction show that TopicRNN outperforms existing contextual RNN baselines. In addition, TopicRNN can be used as an unsupervised feature extractor for documents. We do this for sentiment analysis on the IMDB movie review dataset and report an error rate of 6.28%. This is comparable to the state-of-the-art 5.91% resulting from a semi-supervised approach. Finally, TopicRNN also yields sensible topics, making it a useful alternative to document models such as latent Dirichlet allocation.", "pdf": "/pdf/180fcd5e6174cdcecfa7055f7f9afd5d3c326888.pdf", "paperhash": "dieng|topicrnn_a_recurrent_neural_network_with_longrange_semantic_dependency", "keywords": ["Natural language processing", "Deep learning"], "conflicts": ["microsoft.com", "columbia.edu"], "authors": ["Adji B. Dieng", "Chong Wang", "Jianfeng Gao", "John Paisley"], "authorids": ["abd2141@columbia.edu", "chowang@microsoft.com", "jfgao@microsoft.com", "jpaisley@columbia.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287630900, "id": "ICLR.cc/2017/conference/-/paper302/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rJbbOLcex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper302/reviewers", "ICLR.cc/2017/conference/paper302/areachairs"], "cdate": 1485287630900}}}, {"tddate": null, "tmdate": 1484545037056, "tcdate": 1484545037056, "number": 8, "id": "H1rG20tUg", "invitation": "ICLR.cc/2017/conference/-/paper302/public/comment", "forum": "rJbbOLcex", "replyto": "Hy_fitzEx", "signatures": ["~Adji_Bousso_Dieng1"], "readers": ["everyone"], "writers": ["~Adji_Bousso_Dieng1"], "content": {"title": "Re: Nice work on feature extraction", "comment": "Thank you for your feedback and questions! \n\n- We added an explanation for our rationale of passing the topic vector directly to the output layer at the bottom of page 4 of the revised paper.\n\n- The RNN is trained using only the training data reviews. The labels were not used for training TopicRNN. The labels are used to train a simple classifier that uses the features extracted fromTopicRNN.\n\n- We believe the results from PTB are very encouraging. For example, with only 100 neurons we are able to achieve lower perplexity score with TopicGRU than a stack of 2 layers of LSTMS with 200 hidden units in each layer: 112.4 vs 115.9. See table 2.\n\n- Thank you for mentioning the Miyato et al. reference. We added it to the paper. We were not aware of it by the time we submitted the paper. However, note their approach is semi-supervised. They use the labels to train their network whereas we use an unsupervised approach. We added their SOTA score to table 4.\n\n- Regarding the stop word modeling, we added a note on the bottom of page 4. \n\n- TopicLSTM and TopicGRU actually perform better than TopicRNN when the number of hidden units is greater than 10. We corrected a bug on the computation of the ELBO and reported the new results on Table 2. Yes, we performed gradient clipping.\n\n-The stop word list is the one we provided in the link. As we mention in the discussion section, we leave as future work the dynamic discovery of the stop words. When the stop words are discovered dynamically a stop words list won\u2019t be needed.\n\n-We added inferred distributions from three different documents on page 6 in figure 2.  Thank you for suggesting this! As can be seen in that figure, some \u201ctopic\u201d components are a lot higher than others for different documents. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "TopicRNN: A Recurrent Neural Network with Long-Range Semantic Dependency", "abstract": "In this paper, we propose TopicRNN, a recurrent neural network (RNN)-based language model designed to directly capture the global semantic meaning relating words in a document via latent topics. Because of their sequential nature, RNNs are good at capturing the local structure of a word sequence \u2013 both semantic and syntactic \u2013 but might face difficulty remembering long-range dependencies. Intuitively, these long-range dependencies are of semantic nature. In contrast, latent topic models are able to capture the global underlying semantic structure of a document but do not account for word ordering. The proposed TopicRNN model integrates the merits of RNNs and latent topic models: it captures local (syntactic) dependencies using an RNN and global (semantic) dependencies using latent topics. Unlike previous work on contextual RNN language modeling, our model is learned end-to-end. Empirical results on word prediction show that TopicRNN outperforms existing contextual RNN baselines. In addition, TopicRNN can be used as an unsupervised feature extractor for documents. We do this for sentiment analysis on the IMDB movie review dataset and report an error rate of 6.28%. This is comparable to the state-of-the-art 5.91% resulting from a semi-supervised approach. Finally, TopicRNN also yields sensible topics, making it a useful alternative to document models such as latent Dirichlet allocation.", "pdf": "/pdf/180fcd5e6174cdcecfa7055f7f9afd5d3c326888.pdf", "paperhash": "dieng|topicrnn_a_recurrent_neural_network_with_longrange_semantic_dependency", "keywords": ["Natural language processing", "Deep learning"], "conflicts": ["microsoft.com", "columbia.edu"], "authors": ["Adji B. Dieng", "Chong Wang", "Jianfeng Gao", "John Paisley"], "authorids": ["abd2141@columbia.edu", "chowang@microsoft.com", "jfgao@microsoft.com", "jpaisley@columbia.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287630900, "id": "ICLR.cc/2017/conference/-/paper302/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rJbbOLcex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper302/reviewers", "ICLR.cc/2017/conference/paper302/areachairs"], "cdate": 1485287630900}}}, {"tddate": null, "tmdate": 1484544708339, "tcdate": 1484544708339, "number": 7, "id": "S1haqCt8g", "invitation": "ICLR.cc/2017/conference/-/paper302/public/comment", "forum": "rJbbOLcex", "replyto": "SJ1aaWHNx", "signatures": ["~Adji_Bousso_Dieng1"], "readers": ["everyone"], "writers": ["~Adji_Bousso_Dieng1"], "content": {"title": "Re: review", "comment": "Thanks for your questions and for suggesting we add results for TopicLSTM!\n\n- In Table 2, the first two lines were results reported in Mikolov et al 2012. They run LDA separately and extract features for words using the topic matrix.\n\n- We included results from TopicRNN, TopicLSTM, and TopicGRU. Contrary to what we mentioned earlier, TopicLSTM and TopicGRU actually perform very well. We corrected a bug on the computation of the ELBO. The new results are consistent with the story and are reported in table 2. (We are also running experiments with TopicGRU/TopicLSTM on IMDB data. However, it takes some time to finish. We will add them when they are available.)\n\n- Each text is generated using one example input document. The input for IMDB was a negative review. That sentiment is reflected in the generated text. Note one can sample from the prior for the topic vector \\theta and use that as bias on the trained model.\n\n-TopicRNN is a language model. As reported at the bottom of page 5, the complexity is dominated by the computation of the softmax output layer as is the case for language models. As such, all methods for dealing with the softmax layer are also applicable to TopicRNN. We reported the computation time in the experiments section to give an idea.\n\n- We followed the procedure in Paragraph\u00a0Vector. The main comparison here is against other unsupervised neural network based approaches (ex: Le and Mikolov 2014). Note it is also possible to train the classifier directly with TopicRNN. However, we wanted to highlight TopicRNN as unsupervised feature extractor.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "TopicRNN: A Recurrent Neural Network with Long-Range Semantic Dependency", "abstract": "In this paper, we propose TopicRNN, a recurrent neural network (RNN)-based language model designed to directly capture the global semantic meaning relating words in a document via latent topics. Because of their sequential nature, RNNs are good at capturing the local structure of a word sequence \u2013 both semantic and syntactic \u2013 but might face difficulty remembering long-range dependencies. Intuitively, these long-range dependencies are of semantic nature. In contrast, latent topic models are able to capture the global underlying semantic structure of a document but do not account for word ordering. The proposed TopicRNN model integrates the merits of RNNs and latent topic models: it captures local (syntactic) dependencies using an RNN and global (semantic) dependencies using latent topics. Unlike previous work on contextual RNN language modeling, our model is learned end-to-end. Empirical results on word prediction show that TopicRNN outperforms existing contextual RNN baselines. In addition, TopicRNN can be used as an unsupervised feature extractor for documents. We do this for sentiment analysis on the IMDB movie review dataset and report an error rate of 6.28%. This is comparable to the state-of-the-art 5.91% resulting from a semi-supervised approach. Finally, TopicRNN also yields sensible topics, making it a useful alternative to document models such as latent Dirichlet allocation.", "pdf": "/pdf/180fcd5e6174cdcecfa7055f7f9afd5d3c326888.pdf", "paperhash": "dieng|topicrnn_a_recurrent_neural_network_with_longrange_semantic_dependency", "keywords": ["Natural language processing", "Deep learning"], "conflicts": ["microsoft.com", "columbia.edu"], "authors": ["Adji B. Dieng", "Chong Wang", "Jianfeng Gao", "John Paisley"], "authorids": ["abd2141@columbia.edu", "chowang@microsoft.com", "jfgao@microsoft.com", "jpaisley@columbia.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287630900, "id": "ICLR.cc/2017/conference/-/paper302/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rJbbOLcex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper302/reviewers", "ICLR.cc/2017/conference/paper302/areachairs"], "cdate": 1485287630900}}}, {"tddate": null, "tmdate": 1484544450938, "tcdate": 1484544450938, "number": 6, "id": "ByipKCYUx", "invitation": "ICLR.cc/2017/conference/-/paper302/public/comment", "forum": "rJbbOLcex", "replyto": "S1mRre84l", "signatures": ["~Adji_Bousso_Dieng1"], "readers": ["everyone"], "writers": ["~Adji_Bousso_Dieng1"], "content": {"title": "We believe there was a misunderstanding of our proposed model.", "comment": "Thanks for your questions.\u00a0\n\n1. We believe there was a misunderstanding of our proposed model.\u00a0Unlike LDA, TopicRNN is a sequential model\u00a0(as expressed in the generative process in the middle of page 4) and as such does not make the exchangeability assumption.\u00a0\nThe inference network that produces the topic vector \\theta used as bias takes as input Xc which is a bag of words representation of the document.\u00a0Xc excludes stop words just as done in topic modeling. This is where exchangeability is needed and maybe where the confusion is coming from. But this is the inference network for \\theta, not the actual generative model.\n\u00a0\n2. There are three main reasons behind our choice of using the topic vector as bias instead of passing it into the hidden states of the RNN.\u00a0\n\na) First, this enables us to have a clear separation of the contributions of global semantics and those of local dynamics. The global semantics come from the topics which are meaningful when stop words are excluded.\u00a0However, these stop words are needed for the local dynamics of the language model. We hence achieve this separation of global vs local via a binary decision model for the stop words. It is unclear how to achieve this if we pass the topics to the hidden states of the RNN. This is because the hidden states of the RNN\u00a0will account for all words (including stop words) whereas topics exclude stop words.\u00a0Passing the topics through the hidden states of the RNN violates this.\n\nb) Second, we show empirical evidence that our approach does better than\u00a0previous ways of integrating topic models into RNNs.\u00a0This modeling choice also allows discovering interpretable topics within a single model.\u00a0\n\nc) Finally, this modeling choice allows easier end-to-end training of the model.\u00a0And we argue that although we do not have the topics directly\u00a0going into the hidden states, they will affect the whole trained model, including the hidden states\u00a0due to our end-to-end training approach (unlike the previous work that use pre-trained topic models).\n\nWe added this note on page 4 of the manuscript."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "TopicRNN: A Recurrent Neural Network with Long-Range Semantic Dependency", "abstract": "In this paper, we propose TopicRNN, a recurrent neural network (RNN)-based language model designed to directly capture the global semantic meaning relating words in a document via latent topics. Because of their sequential nature, RNNs are good at capturing the local structure of a word sequence \u2013 both semantic and syntactic \u2013 but might face difficulty remembering long-range dependencies. Intuitively, these long-range dependencies are of semantic nature. In contrast, latent topic models are able to capture the global underlying semantic structure of a document but do not account for word ordering. The proposed TopicRNN model integrates the merits of RNNs and latent topic models: it captures local (syntactic) dependencies using an RNN and global (semantic) dependencies using latent topics. Unlike previous work on contextual RNN language modeling, our model is learned end-to-end. Empirical results on word prediction show that TopicRNN outperforms existing contextual RNN baselines. In addition, TopicRNN can be used as an unsupervised feature extractor for documents. We do this for sentiment analysis on the IMDB movie review dataset and report an error rate of 6.28%. This is comparable to the state-of-the-art 5.91% resulting from a semi-supervised approach. Finally, TopicRNN also yields sensible topics, making it a useful alternative to document models such as latent Dirichlet allocation.", "pdf": "/pdf/180fcd5e6174cdcecfa7055f7f9afd5d3c326888.pdf", "paperhash": "dieng|topicrnn_a_recurrent_neural_network_with_longrange_semantic_dependency", "keywords": ["Natural language processing", "Deep learning"], "conflicts": ["microsoft.com", "columbia.edu"], "authors": ["Adji B. Dieng", "Chong Wang", "Jianfeng Gao", "John Paisley"], "authorids": ["abd2141@columbia.edu", "chowang@microsoft.com", "jfgao@microsoft.com", "jpaisley@columbia.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287630900, "id": "ICLR.cc/2017/conference/-/paper302/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rJbbOLcex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper302/reviewers", "ICLR.cc/2017/conference/paper302/areachairs"], "cdate": 1485287630900}}}, {"tddate": null, "tmdate": 1483116842971, "tcdate": 1483116842971, "number": 4, "id": "rymEZGNHx", "invitation": "ICLR.cc/2017/conference/-/paper302/public/comment", "forum": "rJbbOLcex", "replyto": "Sk2ycnmSx", "signatures": ["~Adji_Bousso_Dieng1"], "readers": ["everyone"], "writers": ["~Adji_Bousso_Dieng1"], "content": {"title": "Re: Is it unfair to use a global topic feature first and then do word prediction?", "comment": "Hi Anonymous. Thank you for your question.\n\nYes, it is unfair to use a global topic feature first and then do word prediction. That is why we don't do that for word prediction. We use a sliding window to compute \\theta as we go. The topic vector \\theta that is used from the current batch of words is estimated from the previous batch of words. We explain this in the middle of page 5 (section titled: \"Generating sequential text and computing perplexity\"). Apologies that it did not appear clear. Note this fair sliding window procedure was also used in Mikolov et al. 2012 which allows for fair comparison of test perplexities as reported in table 2.\n\nIn the sentiment analysis on IMDB we do not use a sliding window to compute \\theta since the ultimate goal in this task is classification and not word prediction."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "TopicRNN: A Recurrent Neural Network with Long-Range Semantic Dependency", "abstract": "In this paper, we propose TopicRNN, a recurrent neural network (RNN)-based language model designed to directly capture the global semantic meaning relating words in a document via latent topics. Because of their sequential nature, RNNs are good at capturing the local structure of a word sequence \u2013 both semantic and syntactic \u2013 but might face difficulty remembering long-range dependencies. Intuitively, these long-range dependencies are of semantic nature. In contrast, latent topic models are able to capture the global underlying semantic structure of a document but do not account for word ordering. The proposed TopicRNN model integrates the merits of RNNs and latent topic models: it captures local (syntactic) dependencies using an RNN and global (semantic) dependencies using latent topics. Unlike previous work on contextual RNN language modeling, our model is learned end-to-end. Empirical results on word prediction show that TopicRNN outperforms existing contextual RNN baselines. In addition, TopicRNN can be used as an unsupervised feature extractor for documents. We do this for sentiment analysis on the IMDB movie review dataset and report an error rate of 6.28%. This is comparable to the state-of-the-art 5.91% resulting from a semi-supervised approach. Finally, TopicRNN also yields sensible topics, making it a useful alternative to document models such as latent Dirichlet allocation.", "pdf": "/pdf/180fcd5e6174cdcecfa7055f7f9afd5d3c326888.pdf", "paperhash": "dieng|topicrnn_a_recurrent_neural_network_with_longrange_semantic_dependency", "keywords": ["Natural language processing", "Deep learning"], "conflicts": ["microsoft.com", "columbia.edu"], "authors": ["Adji B. Dieng", "Chong Wang", "Jianfeng Gao", "John Paisley"], "authorids": ["abd2141@columbia.edu", "chowang@microsoft.com", "jfgao@microsoft.com", "jpaisley@columbia.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287630900, "id": "ICLR.cc/2017/conference/-/paper302/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rJbbOLcex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper302/reviewers", "ICLR.cc/2017/conference/paper302/areachairs"], "cdate": 1485287630900}}}, {"tddate": null, "tmdate": 1483094499724, "tcdate": 1483094499724, "number": 3, "id": "Sk2ycnmSx", "invitation": "ICLR.cc/2017/conference/-/paper302/public/comment", "forum": "rJbbOLcex", "replyto": "rJbbOLcex", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "Is it unfair to use a global topic feature first and then do word prediction?", "comment": "I have a question regrading on the language modeling part. I believe it seems unfair to get a global word distribution(i.e. document topic) first and then use it to do word prediction. The RNN model would never do this and would perform not very good on the very beginning of this article. So does the ppl performance increase comes from this? \n\nWhat if the RNN model gets a global embedding first and then do the word prediction?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "TopicRNN: A Recurrent Neural Network with Long-Range Semantic Dependency", "abstract": "In this paper, we propose TopicRNN, a recurrent neural network (RNN)-based language model designed to directly capture the global semantic meaning relating words in a document via latent topics. Because of their sequential nature, RNNs are good at capturing the local structure of a word sequence \u2013 both semantic and syntactic \u2013 but might face difficulty remembering long-range dependencies. Intuitively, these long-range dependencies are of semantic nature. In contrast, latent topic models are able to capture the global underlying semantic structure of a document but do not account for word ordering. The proposed TopicRNN model integrates the merits of RNNs and latent topic models: it captures local (syntactic) dependencies using an RNN and global (semantic) dependencies using latent topics. Unlike previous work on contextual RNN language modeling, our model is learned end-to-end. Empirical results on word prediction show that TopicRNN outperforms existing contextual RNN baselines. In addition, TopicRNN can be used as an unsupervised feature extractor for documents. We do this for sentiment analysis on the IMDB movie review dataset and report an error rate of 6.28%. This is comparable to the state-of-the-art 5.91% resulting from a semi-supervised approach. Finally, TopicRNN also yields sensible topics, making it a useful alternative to document models such as latent Dirichlet allocation.", "pdf": "/pdf/180fcd5e6174cdcecfa7055f7f9afd5d3c326888.pdf", "paperhash": "dieng|topicrnn_a_recurrent_neural_network_with_longrange_semantic_dependency", "keywords": ["Natural language processing", "Deep learning"], "conflicts": ["microsoft.com", "columbia.edu"], "authors": ["Adji B. Dieng", "Chong Wang", "Jianfeng Gao", "John Paisley"], "authorids": ["abd2141@columbia.edu", "chowang@microsoft.com", "jfgao@microsoft.com", "jpaisley@columbia.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287630900, "id": "ICLR.cc/2017/conference/-/paper302/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rJbbOLcex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper302/reviewers", "ICLR.cc/2017/conference/paper302/areachairs"], "cdate": 1485287630900}}}, {"tddate": null, "tmdate": 1482192330936, "tcdate": 1482192330936, "number": 3, "id": "S1mRre84l", "invitation": "ICLR.cc/2017/conference/-/paper302/official/review", "forum": "rJbbOLcex", "replyto": "rJbbOLcex", "signatures": ["ICLR.cc/2017/conference/paper302/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper302/AnonReviewer2"], "content": {"title": "", "rating": "6: Marginally above acceptance threshold", "review": "This paper introduces a model that blends ideas from generative topic models with those from recurrent neural network language models. The authors evaluate the proposed approach on a document level classification benchmark as well as a language modeling benchmark and it seems to work well. There is also some analysis as to topics learned by the model and its ability to generate text. Overall the paper is clearly written and with the code promised by the authors others should be able to re-implement the approach. I have 2 potentially major questions I would ask the authors to address:\n\n1 - LDA topic models make an exchangability (bag of words) assumption. The discussion of the generative story for TopicRNN should explicitly discuss whether this assumption is also made. On the surface it appears it is since y_t is sampled using only the document topic vector and h_t but we know that in practice h_t comes from a recurrent model that observes y_t-1. Not clear how this clean exposition of the generative model relates to what is actually done. In the Generating sequential text section it\u2019s clear the topic model can\u2019t generate words without using y_1 - t-1 but this seems inconsistent with the generative model specification. This needs to be shown in the paper and made clear to have a complete paper.\n\n\n2 -  The topic model only allows for linear interactions of the topic vector theta. It seems like this might be required to keep the generative model tractable but seems like a very poor assumption. We would expect the topic representation to have rich interactions with a language model to create nonlinear adjustments to word probabilities for a document. Please add discussion as to why this modeling choice exists and if possible how future work could modify that assumption (or explain why it\u2019s not such a bad assumption as one might imagine)\n\n\n\n\nFigure 2 colors very difficult to distinguish. ", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "TopicRNN: A Recurrent Neural Network with Long-Range Semantic Dependency", "abstract": "In this paper, we propose TopicRNN, a recurrent neural network (RNN)-based language model designed to directly capture the global semantic meaning relating words in a document via latent topics. Because of their sequential nature, RNNs are good at capturing the local structure of a word sequence \u2013 both semantic and syntactic \u2013 but might face difficulty remembering long-range dependencies. Intuitively, these long-range dependencies are of semantic nature. In contrast, latent topic models are able to capture the global underlying semantic structure of a document but do not account for word ordering. The proposed TopicRNN model integrates the merits of RNNs and latent topic models: it captures local (syntactic) dependencies using an RNN and global (semantic) dependencies using latent topics. Unlike previous work on contextual RNN language modeling, our model is learned end-to-end. Empirical results on word prediction show that TopicRNN outperforms existing contextual RNN baselines. In addition, TopicRNN can be used as an unsupervised feature extractor for documents. We do this for sentiment analysis on the IMDB movie review dataset and report an error rate of 6.28%. This is comparable to the state-of-the-art 5.91% resulting from a semi-supervised approach. Finally, TopicRNN also yields sensible topics, making it a useful alternative to document models such as latent Dirichlet allocation.", "pdf": "/pdf/180fcd5e6174cdcecfa7055f7f9afd5d3c326888.pdf", "paperhash": "dieng|topicrnn_a_recurrent_neural_network_with_longrange_semantic_dependency", "keywords": ["Natural language processing", "Deep learning"], "conflicts": ["microsoft.com", "columbia.edu"], "authors": ["Adji B. Dieng", "Chong Wang", "Jianfeng Gao", "John Paisley"], "authorids": ["abd2141@columbia.edu", "chowang@microsoft.com", "jfgao@microsoft.com", "jpaisley@columbia.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512631304, "id": "ICLR.cc/2017/conference/-/paper302/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper302/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper302/AnonReviewer1", "ICLR.cc/2017/conference/paper302/AnonReviewer3", "ICLR.cc/2017/conference/paper302/AnonReviewer2"], "reply": {"forum": "rJbbOLcex", "replyto": "rJbbOLcex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper302/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper302/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512631304}}}, {"tddate": null, "tmdate": 1482132919225, "tcdate": 1482132919225, "number": 2, "id": "SJ1aaWHNx", "invitation": "ICLR.cc/2017/conference/-/paper302/official/review", "forum": "rJbbOLcex", "replyto": "rJbbOLcex", "signatures": ["ICLR.cc/2017/conference/paper302/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper302/AnonReviewer3"], "content": {"title": "review", "rating": "7: Good paper, accept", "review": "This paper presents TopicRNN, a combination of LDA and RNN that augments traditional RNN with latent topics by having a switching variable that includes/excludes additive effects from latent topics when generating a word. \nExperiments on two tasks are performed: language modeling on PTB, and sentiment analysis on IMBD. \nThe authors show that TopicRNN outperforms vanilla RNN on PTB and achieves SOTA result on IMDB.\n\nSome questions and comments:\n- In Table 2, how do you use LDA features for RNN (RNN LDA features)? \n- I would like to see results from LSTM included here, even though it is lower perplexity than TopicRNN. I think it's still useful to see how much adding latent topics close the gap between RNN and LSTM.\n- The generated text in Table 3 are not meaningful to me. What is this supposed to highlight? Is this generated text for topic \"trading\"? What about the IMDB one?\n- How scalable is the proposed method for large vocabulary size (>10K)?\n- What is the accuracy on IMDB if the extracted features is used directly to perform classification? (instead of being passed to a neural network with one hidden state). I think this is a fairer comparison to BoW, LDA, and SVM methods presented as baselines. ", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "TopicRNN: A Recurrent Neural Network with Long-Range Semantic Dependency", "abstract": "In this paper, we propose TopicRNN, a recurrent neural network (RNN)-based language model designed to directly capture the global semantic meaning relating words in a document via latent topics. Because of their sequential nature, RNNs are good at capturing the local structure of a word sequence \u2013 both semantic and syntactic \u2013 but might face difficulty remembering long-range dependencies. Intuitively, these long-range dependencies are of semantic nature. In contrast, latent topic models are able to capture the global underlying semantic structure of a document but do not account for word ordering. The proposed TopicRNN model integrates the merits of RNNs and latent topic models: it captures local (syntactic) dependencies using an RNN and global (semantic) dependencies using latent topics. Unlike previous work on contextual RNN language modeling, our model is learned end-to-end. Empirical results on word prediction show that TopicRNN outperforms existing contextual RNN baselines. In addition, TopicRNN can be used as an unsupervised feature extractor for documents. We do this for sentiment analysis on the IMDB movie review dataset and report an error rate of 6.28%. This is comparable to the state-of-the-art 5.91% resulting from a semi-supervised approach. Finally, TopicRNN also yields sensible topics, making it a useful alternative to document models such as latent Dirichlet allocation.", "pdf": "/pdf/180fcd5e6174cdcecfa7055f7f9afd5d3c326888.pdf", "paperhash": "dieng|topicrnn_a_recurrent_neural_network_with_longrange_semantic_dependency", "keywords": ["Natural language processing", "Deep learning"], "conflicts": ["microsoft.com", "columbia.edu"], "authors": ["Adji B. Dieng", "Chong Wang", "Jianfeng Gao", "John Paisley"], "authorids": ["abd2141@columbia.edu", "chowang@microsoft.com", "jfgao@microsoft.com", "jpaisley@columbia.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512631304, "id": "ICLR.cc/2017/conference/-/paper302/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper302/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper302/AnonReviewer1", "ICLR.cc/2017/conference/paper302/AnonReviewer3", "ICLR.cc/2017/conference/paper302/AnonReviewer2"], "reply": {"forum": "rJbbOLcex", "replyto": "rJbbOLcex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper302/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper302/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512631304}}}, {"tddate": null, "tmdate": 1481976649980, "tcdate": 1481968399631, "number": 1, "id": "Hy_fitzEx", "invitation": "ICLR.cc/2017/conference/-/paper302/official/review", "forum": "rJbbOLcex", "replyto": "rJbbOLcex", "signatures": ["ICLR.cc/2017/conference/paper302/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper302/AnonReviewer1"], "content": {"title": "Nice work on feature extraction", "rating": "8: Top 50% of accepted papers, clear accept", "review": "This work combines a LDA-type topic model with a RNN and models this by having an additive effect on the predictive distribution via the topic parameters. A variational auto-encoder is used to infer the topic distribution for a given piece of text and the RNN is trained as a RNNLM. The last hidden state of the RNNLM and the topic parameters are then concatenated to use as a feature representation.\n\nThe paper is well written and easy to understand. Using the topic as an additive effect on the vocabulary allows for easy inference but intuitively I would expect the topic to affect the dynamics too, e.g. the state of the RNN. The results on using this model as a feature extractor for IMDB are quite strong. Is the RNN fine-tuned on the labelled IMDB data? However, the results for PTB are weaker. From the original paper, an ensemble of 2 LSTMs is able to match the topicRNN score. This method of jointly modelling topics and a language model seems effective and relatively easy to implement.\n\nFinally, the IMDB result is no longer state of the art since this result appeared in May (Miyato et al., Adversarial Training Methods for Semi-Supervised Text Classification).\n\nSome questions:\nHow important is the stop word modelling? What do the results look like if l_t = 0.5 for all t?\n\nIt seems surprising that the RNN was more effective than the LSTM. Was gradient clipping tried in the topicLSTM case? Do GRUs also fail to work?\n\nIt is also unfortunate that the model requires a stop-word list. Is the link in footnote 4 the one that is used in the experiments?\n\nDoes factoring out the topics in this way allow the RNN to scale better with more neurons? How reasonable does the topic distribution look for individual documents? How peaked do they tend to be? Can you show some examples of the inferred distribution? The topics look odd for IMDB with the top word of two of the topics being the same: 'campbell'. It would be interesting to compare these topics with those inferred by LDA on the same datasets.\n\nMinor comments:\nBelow figure 2: GHz -> GB\n\\Gamma is not defined.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "TopicRNN: A Recurrent Neural Network with Long-Range Semantic Dependency", "abstract": "In this paper, we propose TopicRNN, a recurrent neural network (RNN)-based language model designed to directly capture the global semantic meaning relating words in a document via latent topics. Because of their sequential nature, RNNs are good at capturing the local structure of a word sequence \u2013 both semantic and syntactic \u2013 but might face difficulty remembering long-range dependencies. Intuitively, these long-range dependencies are of semantic nature. In contrast, latent topic models are able to capture the global underlying semantic structure of a document but do not account for word ordering. The proposed TopicRNN model integrates the merits of RNNs and latent topic models: it captures local (syntactic) dependencies using an RNN and global (semantic) dependencies using latent topics. Unlike previous work on contextual RNN language modeling, our model is learned end-to-end. Empirical results on word prediction show that TopicRNN outperforms existing contextual RNN baselines. In addition, TopicRNN can be used as an unsupervised feature extractor for documents. We do this for sentiment analysis on the IMDB movie review dataset and report an error rate of 6.28%. This is comparable to the state-of-the-art 5.91% resulting from a semi-supervised approach. Finally, TopicRNN also yields sensible topics, making it a useful alternative to document models such as latent Dirichlet allocation.", "pdf": "/pdf/180fcd5e6174cdcecfa7055f7f9afd5d3c326888.pdf", "paperhash": "dieng|topicrnn_a_recurrent_neural_network_with_longrange_semantic_dependency", "keywords": ["Natural language processing", "Deep learning"], "conflicts": ["microsoft.com", "columbia.edu"], "authors": ["Adji B. Dieng", "Chong Wang", "Jianfeng Gao", "John Paisley"], "authorids": ["abd2141@columbia.edu", "chowang@microsoft.com", "jfgao@microsoft.com", "jpaisley@columbia.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512631304, "id": "ICLR.cc/2017/conference/-/paper302/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper302/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper302/AnonReviewer1", "ICLR.cc/2017/conference/paper302/AnonReviewer3", "ICLR.cc/2017/conference/paper302/AnonReviewer2"], "reply": {"forum": "rJbbOLcex", "replyto": "rJbbOLcex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper302/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper302/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512631304}}}, {"tddate": null, "tmdate": 1480707044515, "tcdate": 1480707044511, "number": 2, "id": "HJ3y3H1Qx", "invitation": "ICLR.cc/2017/conference/-/paper302/public/comment", "forum": "rJbbOLcex", "replyto": "BJpisV1Qx", "signatures": ["~Adji_Bousso_Dieng1"], "readers": ["everyone"], "writers": ["~Adji_Bousso_Dieng1"], "content": {"title": "Re: TopicLSTM", "comment": "Thanks for your questions. We think it is fair to say that TopicRNN is better than RNN in terms of perplexity.  And we also show in the paper that TopicRNN is better than many variants of LSTM in terms of sentiment classification results in Figure 4. As far as we know, this is the best result on this task. For TopicLSTM, we haven\u2019t had a very definitive conclusion yet except that it is performing slightly worse than TopicRNN. We are working on a journal version of this paper to address these comparisons including TopicLSTM vs LSTM. \n\nGiven the fact that Mikolov et al. 2012 and Ghosh et al. (the LSTM version of Mikolov et al.) showed that adding contextual bias to an RNN (vanilla RNN or LSTM) yields better perplexity scores than not adding contextual bias, we hope we can find a better way to optimize TopicLSTM so that we can confirm this."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "TopicRNN: A Recurrent Neural Network with Long-Range Semantic Dependency", "abstract": "In this paper, we propose TopicRNN, a recurrent neural network (RNN)-based language model designed to directly capture the global semantic meaning relating words in a document via latent topics. Because of their sequential nature, RNNs are good at capturing the local structure of a word sequence \u2013 both semantic and syntactic \u2013 but might face difficulty remembering long-range dependencies. Intuitively, these long-range dependencies are of semantic nature. In contrast, latent topic models are able to capture the global underlying semantic structure of a document but do not account for word ordering. The proposed TopicRNN model integrates the merits of RNNs and latent topic models: it captures local (syntactic) dependencies using an RNN and global (semantic) dependencies using latent topics. Unlike previous work on contextual RNN language modeling, our model is learned end-to-end. Empirical results on word prediction show that TopicRNN outperforms existing contextual RNN baselines. In addition, TopicRNN can be used as an unsupervised feature extractor for documents. We do this for sentiment analysis on the IMDB movie review dataset and report an error rate of 6.28%. This is comparable to the state-of-the-art 5.91% resulting from a semi-supervised approach. Finally, TopicRNN also yields sensible topics, making it a useful alternative to document models such as latent Dirichlet allocation.", "pdf": "/pdf/180fcd5e6174cdcecfa7055f7f9afd5d3c326888.pdf", "paperhash": "dieng|topicrnn_a_recurrent_neural_network_with_longrange_semantic_dependency", "keywords": ["Natural language processing", "Deep learning"], "conflicts": ["microsoft.com", "columbia.edu"], "authors": ["Adji B. Dieng", "Chong Wang", "Jianfeng Gao", "John Paisley"], "authorids": ["abd2141@columbia.edu", "chowang@microsoft.com", "jfgao@microsoft.com", "jpaisley@columbia.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287630900, "id": "ICLR.cc/2017/conference/-/paper302/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rJbbOLcex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper302/reviewers", "ICLR.cc/2017/conference/paper302/areachairs"], "cdate": 1485287630900}}}, {"tddate": null, "tmdate": 1480702885412, "tcdate": 1480702885407, "number": 1, "id": "BJpisV1Qx", "invitation": "ICLR.cc/2017/conference/-/paper302/official/comment", "forum": "rJbbOLcex", "replyto": "rkwY3Zk7l", "signatures": ["ICLR.cc/2017/conference/paper302/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper302/AnonReviewer3"], "content": {"title": "TopicLSTM", "comment": "Thank you updating the paper. So would it be fair to say that TopicRNN > RNN but TopicLSTM < LSTM? What about Topic RNN ---- LSTM?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "TopicRNN: A Recurrent Neural Network with Long-Range Semantic Dependency", "abstract": "In this paper, we propose TopicRNN, a recurrent neural network (RNN)-based language model designed to directly capture the global semantic meaning relating words in a document via latent topics. Because of their sequential nature, RNNs are good at capturing the local structure of a word sequence \u2013 both semantic and syntactic \u2013 but might face difficulty remembering long-range dependencies. Intuitively, these long-range dependencies are of semantic nature. In contrast, latent topic models are able to capture the global underlying semantic structure of a document but do not account for word ordering. The proposed TopicRNN model integrates the merits of RNNs and latent topic models: it captures local (syntactic) dependencies using an RNN and global (semantic) dependencies using latent topics. Unlike previous work on contextual RNN language modeling, our model is learned end-to-end. Empirical results on word prediction show that TopicRNN outperforms existing contextual RNN baselines. In addition, TopicRNN can be used as an unsupervised feature extractor for documents. We do this for sentiment analysis on the IMDB movie review dataset and report an error rate of 6.28%. This is comparable to the state-of-the-art 5.91% resulting from a semi-supervised approach. Finally, TopicRNN also yields sensible topics, making it a useful alternative to document models such as latent Dirichlet allocation.", "pdf": "/pdf/180fcd5e6174cdcecfa7055f7f9afd5d3c326888.pdf", "paperhash": "dieng|topicrnn_a_recurrent_neural_network_with_longrange_semantic_dependency", "keywords": ["Natural language processing", "Deep learning"], "conflicts": ["microsoft.com", "columbia.edu"], "authors": ["Adji B. Dieng", "Chong Wang", "Jianfeng Gao", "John Paisley"], "authorids": ["abd2141@columbia.edu", "chowang@microsoft.com", "jfgao@microsoft.com", "jpaisley@columbia.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287630740, "id": "ICLR.cc/2017/conference/-/paper302/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "rJbbOLcex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper302/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper302/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper302/reviewers", "ICLR.cc/2017/conference/paper302/areachairs"], "cdate": 1485287630740}}}, {"tddate": null, "tmdate": 1480690815380, "tcdate": 1480690815373, "number": 1, "id": "rkwY3Zk7l", "invitation": "ICLR.cc/2017/conference/-/paper302/public/comment", "forum": "rJbbOLcex", "replyto": "rya03kRGg", "signatures": ["~Adji_Bousso_Dieng1"], "readers": ["everyone"], "writers": ["~Adji_Bousso_Dieng1"], "content": {"title": "Re: Prereview Questions", "comment": "Thank you for your comment. We added the answers to your first two questions in the main text and posted a revision. \n\nRegarding your third question:\nWe are making this speculation given that TopicLSTM gave worse performance in terms of perplexity on PTB. This could be because LSTM already tries to model long-term dependencies and that might interact negatively with the topic bias. This might cause the model to get stuck in a bad local mode. With TopicRNN we only need a few steps (note this prevents vanishing/exploding gradients issues during optimization). The topic bias provided can be seen as some sort of \"memory\" for the RNN."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "TopicRNN: A Recurrent Neural Network with Long-Range Semantic Dependency", "abstract": "In this paper, we propose TopicRNN, a recurrent neural network (RNN)-based language model designed to directly capture the global semantic meaning relating words in a document via latent topics. Because of their sequential nature, RNNs are good at capturing the local structure of a word sequence \u2013 both semantic and syntactic \u2013 but might face difficulty remembering long-range dependencies. Intuitively, these long-range dependencies are of semantic nature. In contrast, latent topic models are able to capture the global underlying semantic structure of a document but do not account for word ordering. The proposed TopicRNN model integrates the merits of RNNs and latent topic models: it captures local (syntactic) dependencies using an RNN and global (semantic) dependencies using latent topics. Unlike previous work on contextual RNN language modeling, our model is learned end-to-end. Empirical results on word prediction show that TopicRNN outperforms existing contextual RNN baselines. In addition, TopicRNN can be used as an unsupervised feature extractor for documents. We do this for sentiment analysis on the IMDB movie review dataset and report an error rate of 6.28%. This is comparable to the state-of-the-art 5.91% resulting from a semi-supervised approach. Finally, TopicRNN also yields sensible topics, making it a useful alternative to document models such as latent Dirichlet allocation.", "pdf": "/pdf/180fcd5e6174cdcecfa7055f7f9afd5d3c326888.pdf", "paperhash": "dieng|topicrnn_a_recurrent_neural_network_with_longrange_semantic_dependency", "keywords": ["Natural language processing", "Deep learning"], "conflicts": ["microsoft.com", "columbia.edu"], "authors": ["Adji B. Dieng", "Chong Wang", "Jianfeng Gao", "John Paisley"], "authorids": ["abd2141@columbia.edu", "chowang@microsoft.com", "jfgao@microsoft.com", "jpaisley@columbia.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287630900, "id": "ICLR.cc/2017/conference/-/paper302/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rJbbOLcex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper302/reviewers", "ICLR.cc/2017/conference/paper302/areachairs"], "cdate": 1485287630900}}}, {"tddate": null, "tmdate": 1480617172615, "tcdate": 1480617172610, "number": 1, "id": "rya03kRGg", "invitation": "ICLR.cc/2017/conference/-/paper302/pre-review/question", "forum": "rJbbOLcex", "replyto": "rJbbOLcex", "signatures": ["ICLR.cc/2017/conference/paper302/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper302/AnonReviewer3"], "content": {"title": "Prereview Questions", "question": "- How long did it take to train TopicRNN on the PTB and IMBD datasets?\n- How much worse are the TopicRNN results with LSTM cell compared to standard RNN cell? \n- Could you please elaborate a little bit more your speculation that \"topic models are more effective than LSTM at capturing global semantic information; standard RNNs and topic models are really complementary in this case.\"? Is this based on your observations on the performance of baseline contextual LSTM models?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "TopicRNN: A Recurrent Neural Network with Long-Range Semantic Dependency", "abstract": "In this paper, we propose TopicRNN, a recurrent neural network (RNN)-based language model designed to directly capture the global semantic meaning relating words in a document via latent topics. Because of their sequential nature, RNNs are good at capturing the local structure of a word sequence \u2013 both semantic and syntactic \u2013 but might face difficulty remembering long-range dependencies. Intuitively, these long-range dependencies are of semantic nature. In contrast, latent topic models are able to capture the global underlying semantic structure of a document but do not account for word ordering. The proposed TopicRNN model integrates the merits of RNNs and latent topic models: it captures local (syntactic) dependencies using an RNN and global (semantic) dependencies using latent topics. Unlike previous work on contextual RNN language modeling, our model is learned end-to-end. Empirical results on word prediction show that TopicRNN outperforms existing contextual RNN baselines. In addition, TopicRNN can be used as an unsupervised feature extractor for documents. We do this for sentiment analysis on the IMDB movie review dataset and report an error rate of 6.28%. This is comparable to the state-of-the-art 5.91% resulting from a semi-supervised approach. Finally, TopicRNN also yields sensible topics, making it a useful alternative to document models such as latent Dirichlet allocation.", "pdf": "/pdf/180fcd5e6174cdcecfa7055f7f9afd5d3c326888.pdf", "paperhash": "dieng|topicrnn_a_recurrent_neural_network_with_longrange_semantic_dependency", "keywords": ["Natural language processing", "Deep learning"], "conflicts": ["microsoft.com", "columbia.edu"], "authors": ["Adji B. Dieng", "Chong Wang", "Jianfeng Gao", "John Paisley"], "authorids": ["abd2141@columbia.edu", "chowang@microsoft.com", "jfgao@microsoft.com", "jpaisley@columbia.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959351115, "id": "ICLR.cc/2017/conference/-/paper302/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper302/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper302/AnonReviewer3"], "reply": {"forum": "rJbbOLcex", "replyto": "rJbbOLcex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper302/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper302/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959351115}}}], "count": 19}