{"notes": [{"id": "SyGjQ30qFX", "original": "r1gNFzCctm", "number": 1392, "cdate": 1538087971387, "ddate": null, "tcdate": 1538087971387, "tmdate": 1545355389638, "tddate": null, "forum": "SyGjQ30qFX", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "TopicGAN: Unsupervised Text Generation from Explainable Latent Topics", "abstract": "Learning discrete representations of data and then generating data from the discovered representations have been increasingly studied because the obtained discrete representations can benefit unsupervised learning. However, the performance of learning discrete representations of textual data with deep generative models has not been widely explored. In addition, although generative adversarial networks(GAN) have shown impressing results in many areas such as image generation, for text generation, it is notorious for extremely difficult to train. In this work, we propose TopicGAN, a two-step text generative model, which is able to solve those two important problems simultaneously. In the first step, it discovers the latent topics and produced bag-of-words according to the latent topics. In the second step, it generates text from the produced bag-of-words. In our experiments, we show our model can discover meaningful discrete latent topics of texts in an unsupervised fashion and generate high quality natural language from the discovered latent topics.", "keywords": ["unsupervised learning", "topic model", "text generation"], "authorids": ["king6101@gmail.com", "y.v.chen@ieee.org", "tlkagkb93901106@gmail.com"], "authors": ["Yau-Shian Wang", "Yun-Nung Chen", "Hung-Yi Lee"], "pdf": "/pdf/9cbfa19fa5a15f522cec935b087b28c27d57eb3b.pdf", "paperhash": "wang|topicgan_unsupervised_text_generation_from_explainable_latent_topics", "_bibtex": "@misc{\nwang2019topicgan,\ntitle={Topic{GAN}: Unsupervised Text Generation from Explainable Latent Topics},\nauthor={Yau-Shian Wang and Yun-Nung Chen and Hung-Yi Lee},\nyear={2019},\nurl={https://openreview.net/forum?id=SyGjQ30qFX},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "Byg_im0lxN", "original": null, "number": 1, "cdate": 1544770464267, "ddate": null, "tcdate": 1544770464267, "tmdate": 1545354521265, "tddate": null, "forum": "SyGjQ30qFX", "replyto": "SyGjQ30qFX", "invitation": "ICLR.cc/2019/Conference/-/Paper1392/Meta_Review", "content": {"metareview": "This paper proposes TopicGAN, a generative adversarial approach to topic modeling and text generation. TopicGAN operates in two steps: it first generates latent topics and produces bag-of-words corresponding to those latent topics. In the second step, the model generates text conditioning on those topic words.\n\nPros: \nIt combines the strength of topic models (interpretable topics that are learned unsupervised) with GAN for text generation.\n\nCons: \nThere are three major concerns raised by reviewers: (1) clarity, (2) relatively thin experimental results, and (3) novelty. Of these, the first two were the main concerns. In particular, R1 and R2 raised concerns about insufficient component-wise evaluation (e.g., text classification from topic models) and insufficient GAN-based baselines. Also, the topic model part of TopicGAN seems somewhat underdeveloped in that the model assumes a single topic per document, which is a relatively strong simplifying assumption compared to most other topic models (R1, R3). The technical novelty is not extremely strong in that the proposed model combines existing components together. But this alone would have not been a deal breaker if the empirical results were rigorous and strong.\n\nVerdict:\nReject. Many technical details require clarification and experiments lack sufficient comparisons against prior art.", "confidence": "5: The area chair is absolutely certain", "recommendation": "Reject", "title": "technical details require clarification and experiments lack sufficient comparisons "}, "signatures": ["ICLR.cc/2019/Conference/Paper1392/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper1392/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "TopicGAN: Unsupervised Text Generation from Explainable Latent Topics", "abstract": "Learning discrete representations of data and then generating data from the discovered representations have been increasingly studied because the obtained discrete representations can benefit unsupervised learning. However, the performance of learning discrete representations of textual data with deep generative models has not been widely explored. In addition, although generative adversarial networks(GAN) have shown impressing results in many areas such as image generation, for text generation, it is notorious for extremely difficult to train. In this work, we propose TopicGAN, a two-step text generative model, which is able to solve those two important problems simultaneously. In the first step, it discovers the latent topics and produced bag-of-words according to the latent topics. In the second step, it generates text from the produced bag-of-words. In our experiments, we show our model can discover meaningful discrete latent topics of texts in an unsupervised fashion and generate high quality natural language from the discovered latent topics.", "keywords": ["unsupervised learning", "topic model", "text generation"], "authorids": ["king6101@gmail.com", "y.v.chen@ieee.org", "tlkagkb93901106@gmail.com"], "authors": ["Yau-Shian Wang", "Yun-Nung Chen", "Hung-Yi Lee"], "pdf": "/pdf/9cbfa19fa5a15f522cec935b087b28c27d57eb3b.pdf", "paperhash": "wang|topicgan_unsupervised_text_generation_from_explainable_latent_topics", "_bibtex": "@misc{\nwang2019topicgan,\ntitle={Topic{GAN}: Unsupervised Text Generation from Explainable Latent Topics},\nauthor={Yau-Shian Wang and Yun-Nung Chen and Hung-Yi Lee},\nyear={2019},\nurl={https://openreview.net/forum?id=SyGjQ30qFX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1392/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545352854828, "tddate": null, "super": null, "final": null, "reply": {"forum": "SyGjQ30qFX", "replyto": "SyGjQ30qFX", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1392/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper1392/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1392/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545352854828}}}, {"id": "BkguegXjR7", "original": null, "number": 3, "cdate": 1543348208119, "ddate": null, "tcdate": 1543348208119, "tmdate": 1543348208119, "tddate": null, "forum": "SyGjQ30qFX", "replyto": "H1l8tzxq3Q", "invitation": "ICLR.cc/2019/Conference/-/Paper1392/Official_Comment", "content": {"title": "Thank you for your valuable review", "comment": "\n(1)More details and writing:\nIn the revised version of paper, we have provided more details and clearer explanations of our model in revised version Section 3.3. We have also rewritten many parts of the article to make the paper easier to understand.\n\n(2)Baseline:\nBecause we use GAN to do the same fine tuning method  of our proposed TopicGAN and our baseline VAE+WGAN. Therefore, we consider it a proper baseline. Please notice that the baseline and other related works can only generate text conditioned on noise, while our generation task is more difficult that we conditioned not only on noise but also on discovered latent topics.\n\n(3)Evaluation metric:\nWe used perplexity of generated text as our standard metrics. That is because we studied some previous generation papers(e.g.: https://arxiv.org/abs/1801.07736) and find some them using perplexity as their evaluation metric. To evaluate whether our topic model can discover meaningful topics, we also report topic coherence score in revised paper Table.3.\n\n(4)\nThe reason that we said our method can be combined with any other text generation method using GAN is that we can use any other GAN to jointly train our whole text generator G, where G is composed of BOW generator $G_{bow}$ and sequence generator $G_{seq}$. \n\n\nMinor comments:\nYou are right. It should be equation 1 instead. We have revised this mistake."}, "signatures": ["ICLR.cc/2019/Conference/Paper1392/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1392/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1392/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "TopicGAN: Unsupervised Text Generation from Explainable Latent Topics", "abstract": "Learning discrete representations of data and then generating data from the discovered representations have been increasingly studied because the obtained discrete representations can benefit unsupervised learning. However, the performance of learning discrete representations of textual data with deep generative models has not been widely explored. In addition, although generative adversarial networks(GAN) have shown impressing results in many areas such as image generation, for text generation, it is notorious for extremely difficult to train. In this work, we propose TopicGAN, a two-step text generative model, which is able to solve those two important problems simultaneously. In the first step, it discovers the latent topics and produced bag-of-words according to the latent topics. In the second step, it generates text from the produced bag-of-words. In our experiments, we show our model can discover meaningful discrete latent topics of texts in an unsupervised fashion and generate high quality natural language from the discovered latent topics.", "keywords": ["unsupervised learning", "topic model", "text generation"], "authorids": ["king6101@gmail.com", "y.v.chen@ieee.org", "tlkagkb93901106@gmail.com"], "authors": ["Yau-Shian Wang", "Yun-Nung Chen", "Hung-Yi Lee"], "pdf": "/pdf/9cbfa19fa5a15f522cec935b087b28c27d57eb3b.pdf", "paperhash": "wang|topicgan_unsupervised_text_generation_from_explainable_latent_topics", "_bibtex": "@misc{\nwang2019topicgan,\ntitle={Topic{GAN}: Unsupervised Text Generation from Explainable Latent Topics},\nauthor={Yau-Shian Wang and Yun-Nung Chen and Hung-Yi Lee},\nyear={2019},\nurl={https://openreview.net/forum?id=SyGjQ30qFX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1392/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621608770, "tddate": null, "super": null, "final": null, "reply": {"forum": "SyGjQ30qFX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1392/Authors", "ICLR.cc/2019/Conference/Paper1392/Reviewers", "ICLR.cc/2019/Conference/Paper1392/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1392/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1392/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1392/Authors|ICLR.cc/2019/Conference/Paper1392/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1392/Reviewers", "ICLR.cc/2019/Conference/Paper1392/Authors", "ICLR.cc/2019/Conference/Paper1392/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621608770}}}, {"id": "B1eCntlsRQ", "original": null, "number": 1, "cdate": 1543338421569, "ddate": null, "tcdate": 1543338421569, "tmdate": 1543343382241, "tddate": null, "forum": "SyGjQ30qFX", "replyto": "B1gtuhMg6m", "invitation": "ICLR.cc/2019/Conference/-/Paper1392/Official_Comment", "content": {"title": "Thank you for your valuable review", "comment": "\n(1)Writing:\nWe have rewritten many parts of the article to make the paper easier to understand. In addition, some not convincing explanations mentioned in the review are also revised.\n\n(2)Assuming documents are generated from one single main topic:\nIn our experiments, we conduct unsupervised document classification, in which the documents have only one single class. Therefore, for those unsupervised classification experiments, assuming each documents coming from a single main topic is a more appropriate assumption, which allows our model to learn more distinct topics. In addition, as the length of our training documents is short, it\u2019s hard to break the short text into several topics, which is one of the possible reason that makes LDA works not well on short text.\n\nHowever, we acknowledge that for long documents, it's more appropriate to assume they come from the mixture of topics. In fact, it's feasible for our method to generate documents from several topics because info-GAN allows us to decide the distribution of the predicted code. We are conducting experiments on using several topics to generate longer documents and the current result seems better than generating from one single main topic.\n\n(3)Novelty:\nThe novelty of our work is that (a) as far as we know, there is no previous work which tries to use GAN to achieve topic modeling, which is a worth exploring direction. (b) Some extra tricks for Info-GAN training (c)Two steps generation of text may also be a better and easier method for generating text.\n\n(4)Evaluation:\nWe have evaluated the topic coherence score and reported the score on revised paper Table 3. Our method outperformed baseline method on all datasets, which implies the effectiveness of our proposed topic model.\nWhen conducting human evaluation to evaluate the quality of sentences, we asked 17 annotators to compare 13 sets of sentences generated by different methods."}, "signatures": ["ICLR.cc/2019/Conference/Paper1392/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1392/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1392/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "TopicGAN: Unsupervised Text Generation from Explainable Latent Topics", "abstract": "Learning discrete representations of data and then generating data from the discovered representations have been increasingly studied because the obtained discrete representations can benefit unsupervised learning. However, the performance of learning discrete representations of textual data with deep generative models has not been widely explored. In addition, although generative adversarial networks(GAN) have shown impressing results in many areas such as image generation, for text generation, it is notorious for extremely difficult to train. In this work, we propose TopicGAN, a two-step text generative model, which is able to solve those two important problems simultaneously. In the first step, it discovers the latent topics and produced bag-of-words according to the latent topics. In the second step, it generates text from the produced bag-of-words. In our experiments, we show our model can discover meaningful discrete latent topics of texts in an unsupervised fashion and generate high quality natural language from the discovered latent topics.", "keywords": ["unsupervised learning", "topic model", "text generation"], "authorids": ["king6101@gmail.com", "y.v.chen@ieee.org", "tlkagkb93901106@gmail.com"], "authors": ["Yau-Shian Wang", "Yun-Nung Chen", "Hung-Yi Lee"], "pdf": "/pdf/9cbfa19fa5a15f522cec935b087b28c27d57eb3b.pdf", "paperhash": "wang|topicgan_unsupervised_text_generation_from_explainable_latent_topics", "_bibtex": "@misc{\nwang2019topicgan,\ntitle={Topic{GAN}: Unsupervised Text Generation from Explainable Latent Topics},\nauthor={Yau-Shian Wang and Yun-Nung Chen and Hung-Yi Lee},\nyear={2019},\nurl={https://openreview.net/forum?id=SyGjQ30qFX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1392/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621608770, "tddate": null, "super": null, "final": null, "reply": {"forum": "SyGjQ30qFX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1392/Authors", "ICLR.cc/2019/Conference/Paper1392/Reviewers", "ICLR.cc/2019/Conference/Paper1392/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1392/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1392/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1392/Authors|ICLR.cc/2019/Conference/Paper1392/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1392/Reviewers", "ICLR.cc/2019/Conference/Paper1392/Authors", "ICLR.cc/2019/Conference/Paper1392/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621608770}}}, {"id": "S1erKnbi0X", "original": null, "number": 2, "cdate": 1543343228918, "ddate": null, "tcdate": 1543343228918, "tmdate": 1543343228918, "tddate": null, "forum": "SyGjQ30qFX", "replyto": "SJlsDbO627", "invitation": "ICLR.cc/2019/Conference/-/Paper1392/Official_Comment", "content": {"title": "Thank you for your thorough review. ", "comment": "\nWriting:\nWe have rewritten many parts of the article to make the paper easier to understand. In addition, some not convincing explanations mentioned in the review are also revised.\n\n(1)How to select topic words:\nOur topic classification model is a  V*K matrix M, where V is the word number and K is the number of latent topics.\nFor each column of M is a topic distribution of words which is similar to conventional topic models such as LDA.\nThe value of M[i][j] represents the importance of i-th word to j-th topic. Therefore, we were able to select the top few words with highest weight within each topic as topic words. We have included those details in the revised version (Section4.2).\n\n(2)Why word sequence generator is included in the paper:\nThe goal of our work not only aims to train a high quality topic model, but also aims to generate high quality text using GAN by two steps generation. Using GAN for language generating is notorious for extremely difficult to train because it needs to (1) generate meaningful context with (2) correct grammar simultaneously. However, in our work, we try to separate this two core part of language generation and make the generation process easier. \n\n(3)Some detail:\nFor all experiments(including baseline models), we set the topic number same as the class number. For example, in 20 News Groups, the class number is 20, and thus we set the topic number to 20. We use online LDA with different hyperparameters adjusted to get the better result on each dataset.\n\n(4)Why BOW vocabulary size is smaller:\nThe size of the bag-of-words vocabulary is smaller because we hope during bag-of-words generation our model can focus on more important and general words. With smaller vocabulary size of bag-of-words, the result of unsupervised learning is better.\n\n(5)Cross-references:\nWe have rewritten some methodology part and make it clearer.\n\n(6)Typo:\nWe have revised this typo.\n\nPart2:\n(1)Assuming documents are generated from one single main topic:\nIn our experiments, we conduct unsupervised document classification, in which the documents have only one single class. Therefore, for those unsupervised classification experiments, assuming each documents coming from a single main topic is a more appropriate assumption, which allows our model to learn more distinct topics. In addition, as the length of our training documents is short, it\u2019s hard to break the short text into several topics, which is one of the possible reason that makes LDA works not well on short text.\n\n(2)Proposed model ignores the word counts:\nAlthough the model ignores the word counts, it still performs well in unsupervised document classification and topic coherence score.\n\n(3)Topic coherence score is reported:\nWe have evaluated the topic coherence score and reported the score on revised paper Table 3. Our method outperformed baseline method on all datasets, which implies the effectiveness of our proposed topic model. We believe LDA worked properly as the topic coherence scores and unsupervised classification accuracy were in reasonable range.\nWe think LDA is the most famous conventional topic model, could you list the state-of-the-art conventional topic model that should be compared?"}, "signatures": ["ICLR.cc/2019/Conference/Paper1392/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1392/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1392/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "TopicGAN: Unsupervised Text Generation from Explainable Latent Topics", "abstract": "Learning discrete representations of data and then generating data from the discovered representations have been increasingly studied because the obtained discrete representations can benefit unsupervised learning. However, the performance of learning discrete representations of textual data with deep generative models has not been widely explored. In addition, although generative adversarial networks(GAN) have shown impressing results in many areas such as image generation, for text generation, it is notorious for extremely difficult to train. In this work, we propose TopicGAN, a two-step text generative model, which is able to solve those two important problems simultaneously. In the first step, it discovers the latent topics and produced bag-of-words according to the latent topics. In the second step, it generates text from the produced bag-of-words. In our experiments, we show our model can discover meaningful discrete latent topics of texts in an unsupervised fashion and generate high quality natural language from the discovered latent topics.", "keywords": ["unsupervised learning", "topic model", "text generation"], "authorids": ["king6101@gmail.com", "y.v.chen@ieee.org", "tlkagkb93901106@gmail.com"], "authors": ["Yau-Shian Wang", "Yun-Nung Chen", "Hung-Yi Lee"], "pdf": "/pdf/9cbfa19fa5a15f522cec935b087b28c27d57eb3b.pdf", "paperhash": "wang|topicgan_unsupervised_text_generation_from_explainable_latent_topics", "_bibtex": "@misc{\nwang2019topicgan,\ntitle={Topic{GAN}: Unsupervised Text Generation from Explainable Latent Topics},\nauthor={Yau-Shian Wang and Yun-Nung Chen and Hung-Yi Lee},\nyear={2019},\nurl={https://openreview.net/forum?id=SyGjQ30qFX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1392/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621608770, "tddate": null, "super": null, "final": null, "reply": {"forum": "SyGjQ30qFX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1392/Authors", "ICLR.cc/2019/Conference/Paper1392/Reviewers", "ICLR.cc/2019/Conference/Paper1392/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1392/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1392/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1392/Authors|ICLR.cc/2019/Conference/Paper1392/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1392/Reviewers", "ICLR.cc/2019/Conference/Paper1392/Authors", "ICLR.cc/2019/Conference/Paper1392/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621608770}}}, {"id": "B1gtuhMg6m", "original": null, "number": 3, "cdate": 1541577840878, "ddate": null, "tcdate": 1541577840878, "tmdate": 1541577840878, "tddate": null, "forum": "SyGjQ30qFX", "replyto": "SyGjQ30qFX", "invitation": "ICLR.cc/2019/Conference/-/Paper1392/Official_Review", "content": {"title": "This paper proposes a generative adversarial approach to topic modeling. While the idea is fine, the paper has several limitations.", "review": "This paper proposes TopicGAN, a generative adversarial approach to topic modeling and text generation. The model basically combines two steps: first to generate words (bag-of-words) for a topic, then second to generate the sequence of the words.\n\nWhile the idea is interesting, there are several important limitations. First, the paper is difficult to understand, and some of the explanations are not convincing. For example, in section 4.1.1, it says \"... our method assumes that the documents are produced from a single topic ... Our assumption aligns well with human intuition that most documents are generated from a single main topic.\" This goes very much against the common assumption of a generative topic model, such as LDA, which the model compares against. I don't mean to argue either way, but if the paper presents a viewpoint which is quite different from the commonly accepted viewpoint (within the specific research field), then there needs to be a much deeper explanation, ideally with concrete evidence to support it. Another sentence from the same paragraph states that their \"model outperforms LDA because LDA is a statistical model, while our generator is a deep generative model.\" This argument also seems flawed and without concrete evidence. There are other parts in the paper where the logic seems strange and without evidence, and they make it difficult to understand and accept the major claims of the paper.\n\nSecond, the model does not offer much novelty. It seems that the two-stage model simply puts the two pieces, a GAN-style generator and an LSTM sequence model together. Perhaps I am not understanding the model, but the model description was also not clear nor easy to understand with respect to its novelty.\n\nThird, the evaluation is somewhat weak. There are two main evaluations tasks: text classification and text generation. For the first task, classification is not the main purpose of topic models, and while text classification _is_ used in many topic modeling papers, it is almost always accompanied by other evaluation metrics such as held-out perplexity and topic coherence. This is because the main purpose of topic modeling is to actually infer the topics (per-topic word distribution and per-document topic distribution) and model the corpus. Thus I feel it is not a fair evaluation to just compare the models using text classification tasks. The second evaluation task of text generation is not explained enough. For the human evaluation, who were the annotators, and how were they trained? How many people annotated each output, and what was the inter-rater agreement? How many sentences were evaluated, and how were they chosen? Without these details, it is difficult to judge whether this evaluation was valid.\n\nLastly, the results are mediocre. Besides the classification task, the others do not show significant improvements over the baseline models. Perplexity (table 3) shows similar results for DBPedia and worse results (than WGAN-gp) for Gigaword. Table 4 shows slightly better results for \"Preference\" for TopicGAN with joint training, but \"Accuracy\" is measured only for the proposed model and not the baseline model. ", "rating": "4: Ok but not good enough - rejection", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2019/Conference/Paper1392/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "TopicGAN: Unsupervised Text Generation from Explainable Latent Topics", "abstract": "Learning discrete representations of data and then generating data from the discovered representations have been increasingly studied because the obtained discrete representations can benefit unsupervised learning. However, the performance of learning discrete representations of textual data with deep generative models has not been widely explored. In addition, although generative adversarial networks(GAN) have shown impressing results in many areas such as image generation, for text generation, it is notorious for extremely difficult to train. In this work, we propose TopicGAN, a two-step text generative model, which is able to solve those two important problems simultaneously. In the first step, it discovers the latent topics and produced bag-of-words according to the latent topics. In the second step, it generates text from the produced bag-of-words. In our experiments, we show our model can discover meaningful discrete latent topics of texts in an unsupervised fashion and generate high quality natural language from the discovered latent topics.", "keywords": ["unsupervised learning", "topic model", "text generation"], "authorids": ["king6101@gmail.com", "y.v.chen@ieee.org", "tlkagkb93901106@gmail.com"], "authors": ["Yau-Shian Wang", "Yun-Nung Chen", "Hung-Yi Lee"], "pdf": "/pdf/9cbfa19fa5a15f522cec935b087b28c27d57eb3b.pdf", "paperhash": "wang|topicgan_unsupervised_text_generation_from_explainable_latent_topics", "_bibtex": "@misc{\nwang2019topicgan,\ntitle={Topic{GAN}: Unsupervised Text Generation from Explainable Latent Topics},\nauthor={Yau-Shian Wang and Yun-Nung Chen and Hung-Yi Lee},\nyear={2019},\nurl={https://openreview.net/forum?id=SyGjQ30qFX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1392/Official_Review", "cdate": 1542234239596, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "SyGjQ30qFX", "replyto": "SyGjQ30qFX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1392/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335937007, "tmdate": 1552335937007, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1392/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "SJlsDbO627", "original": null, "number": 2, "cdate": 1541402978590, "ddate": null, "tcdate": 1541402978590, "tmdate": 1541533171332, "tddate": null, "forum": "SyGjQ30qFX", "replyto": "SyGjQ30qFX", "invitation": "ICLR.cc/2019/Conference/-/Paper1392/Official_Review", "content": {"title": "This paper presents a topic model based on adversarial training.", "review": "This paper presents a topic model based on adversarial training. Specifically, the paper adopts the framework of InfoGAN to generates the bag-of-words of a document and the latent codes in InfoGAN correspond to the latent topics in topic modelling. In addition to the above framework, to make the model work better, several add-ons are also proposed, combining autoencoder, loss clipping, and a generative model to generate text sequences based on the bag-of-words.\n\nMy comments are as follows:\n\n1. There are several issues of this paper on clarity:\n\n(1) The first major one for me is that the authors did not give any details on how to interpret the latent code (i.e. the topics here) with the top words. In conventional topic models, usually a topic is a distribution of words, so that top words can be selected by their weights. But I did not see something similar in the proposed model.\n\n(2) Another major one is why the word sequence generator is introduced in the proposed model. I did not see the contribution of this part to the whole model as a topic model, although the joint training shows the marginal performance gain on text generation.\n\n(3) Some of the experiment settings are not provided, for example, the number of topics, the value of \\alpha and \\lambda in the proposed model, the hyperparameters of LDA, which are crucial for the results.\n\n(4) Why is the size of the bag-of-words vocabulary set to be 3K whereas that of the word generation vocabulary set to be 15K?\n\nMinor issues:\n\n(5) In the related work of InfoGAN, there are a lot of cross-references to the following sections, before they are properly introduced.\n\n(6) Typo of \"Accurcay\" in Table 4(a).\n\n2. Using adversarial training for topic models seems to be an interesting idea. There is not much work in this line and this paper proposes a model that seems to be working. But it seems to be that the proposed model has several issues as follows:\n\n(1) Each document seems to have only one topic, which can be an impractical setting for long documents.\n\n(2) The proposed model ignores the word counts, which can be important for topic modelling.\n\n(3) I did not see a major improvement of the proposed model over others, given that the only numerical result reported is classification accuracy and the state-of-the-art conventional topic models are not compared. This also leads to my concern about the experiments. I would expect more comparisons than classification accuracy, such as topic coherence and perplexity (for topic modelling) and with more advanced conventional models. From the low values of the accuracy on 20NG, I am wondering if LDA is working properly.  \n\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1392/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "TopicGAN: Unsupervised Text Generation from Explainable Latent Topics", "abstract": "Learning discrete representations of data and then generating data from the discovered representations have been increasingly studied because the obtained discrete representations can benefit unsupervised learning. However, the performance of learning discrete representations of textual data with deep generative models has not been widely explored. In addition, although generative adversarial networks(GAN) have shown impressing results in many areas such as image generation, for text generation, it is notorious for extremely difficult to train. In this work, we propose TopicGAN, a two-step text generative model, which is able to solve those two important problems simultaneously. In the first step, it discovers the latent topics and produced bag-of-words according to the latent topics. In the second step, it generates text from the produced bag-of-words. In our experiments, we show our model can discover meaningful discrete latent topics of texts in an unsupervised fashion and generate high quality natural language from the discovered latent topics.", "keywords": ["unsupervised learning", "topic model", "text generation"], "authorids": ["king6101@gmail.com", "y.v.chen@ieee.org", "tlkagkb93901106@gmail.com"], "authors": ["Yau-Shian Wang", "Yun-Nung Chen", "Hung-Yi Lee"], "pdf": "/pdf/9cbfa19fa5a15f522cec935b087b28c27d57eb3b.pdf", "paperhash": "wang|topicgan_unsupervised_text_generation_from_explainable_latent_topics", "_bibtex": "@misc{\nwang2019topicgan,\ntitle={Topic{GAN}: Unsupervised Text Generation from Explainable Latent Topics},\nauthor={Yau-Shian Wang and Yun-Nung Chen and Hung-Yi Lee},\nyear={2019},\nurl={https://openreview.net/forum?id=SyGjQ30qFX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1392/Official_Review", "cdate": 1542234239596, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "SyGjQ30qFX", "replyto": "SyGjQ30qFX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1392/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335937007, "tmdate": 1552335937007, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1392/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "H1l8tzxq3Q", "original": null, "number": 1, "cdate": 1541173886003, "ddate": null, "tcdate": 1541173886003, "tmdate": 1541533171128, "tddate": null, "forum": "SyGjQ30qFX", "replyto": "SyGjQ30qFX", "invitation": "ICLR.cc/2019/Conference/-/Paper1392/Official_Review", "content": {"title": "not convincing", "review": "This paper proposes a new framework for topic modeling, which consists of two main steps: generating bag of words for topics and then using RNN to decode a sequence text. \n\nPros:\nThe author draws lessons from the infoGAN and designed a creative object function with reconstruction loss and categorical loss. As a result, this paper achieved impressive outcome for topic modeling tasks.\n\nComments:\n1. High-level language is used to describe how to train two parts of the model, which is not technically clear. It would be better describe the algorithms in more details by listing steps for your algorithm in the section 3.3.\n\n2. For text generation experiments, why didn\u2019t you compare your model with any other related model such as SeqGAN or TextGAN? It is not so convincing to just use VAE+Wgan-gp as a baseline model.\n\n3. For qualitative analysis part, you just listed some of your generated sentences for proving the fluency and relevance. Why didn\u2019t you use some standard metrics for evaluating the quality of the text? I cannot judge the quality of your model through these randomly selected sentences.\n\n4. As you mentioned in this paper \u201cyour model can be easily combined with any current text generation models\u201d, have you done any experiments for demonstrating the original text generation model will get better performance after applying your framework? \n\nMinor comments:\n1. On page 2 and page 4, you mentioned \u201cthe third term in (2)\u201d. According to my understanding, this should be equation 1 instead. \n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1392/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "TopicGAN: Unsupervised Text Generation from Explainable Latent Topics", "abstract": "Learning discrete representations of data and then generating data from the discovered representations have been increasingly studied because the obtained discrete representations can benefit unsupervised learning. However, the performance of learning discrete representations of textual data with deep generative models has not been widely explored. In addition, although generative adversarial networks(GAN) have shown impressing results in many areas such as image generation, for text generation, it is notorious for extremely difficult to train. In this work, we propose TopicGAN, a two-step text generative model, which is able to solve those two important problems simultaneously. In the first step, it discovers the latent topics and produced bag-of-words according to the latent topics. In the second step, it generates text from the produced bag-of-words. In our experiments, we show our model can discover meaningful discrete latent topics of texts in an unsupervised fashion and generate high quality natural language from the discovered latent topics.", "keywords": ["unsupervised learning", "topic model", "text generation"], "authorids": ["king6101@gmail.com", "y.v.chen@ieee.org", "tlkagkb93901106@gmail.com"], "authors": ["Yau-Shian Wang", "Yun-Nung Chen", "Hung-Yi Lee"], "pdf": "/pdf/9cbfa19fa5a15f522cec935b087b28c27d57eb3b.pdf", "paperhash": "wang|topicgan_unsupervised_text_generation_from_explainable_latent_topics", "_bibtex": "@misc{\nwang2019topicgan,\ntitle={Topic{GAN}: Unsupervised Text Generation from Explainable Latent Topics},\nauthor={Yau-Shian Wang and Yun-Nung Chen and Hung-Yi Lee},\nyear={2019},\nurl={https://openreview.net/forum?id=SyGjQ30qFX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1392/Official_Review", "cdate": 1542234239596, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "SyGjQ30qFX", "replyto": "SyGjQ30qFX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1392/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335937007, "tmdate": 1552335937007, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1392/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 8}