{"notes": [{"id": "Skgb5h4KPH", "original": "rJePI-80LH", "number": 106, "cdate": 1569438857506, "ddate": null, "tcdate": 1569438857506, "tmdate": 1577168282378, "tddate": null, "forum": "Skgb5h4KPH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"abstract": "We study the training process of Deep Neural Networks (DNNs) from the Fourier analysis perspective. We demonstrate a very universal Frequency Principle (F-Principle) --- DNNs often fit target functions from low to high frequencies --- on high-dimensional benchmark datasets, such as MNIST/CIFAR10, and deep networks, such as VGG16. This F-Principle of DNNs is opposite to the learning behavior of most conventional iterative numerical schemes (e.g., Jacobi method), which exhibits faster convergence for higher frequencies, for various scientific computing problems. With a naive theory, we illustrate that this F-Principle results from the regularity of the commonly used activation functions. The F-Principle implies an implicit bias that DNNs tend to fit training data by a low-frequency function. This understanding provides an explanation of good generalization of DNNs on most real datasets and bad generalization of DNNs on parity function or randomized dataset.", "title": "Frequency Principle: Fourier Analysis Sheds Light on Deep Neural Networks", "keywords": ["deep learning", "training behavior", "Fourier analysis", "generalization"], "pdf": "/pdf/b38a23a244dfb59238f22a154229345291c1d402.pdf", "authors": ["Zhi-Qin John Xu", "Yaoyu Zhang", "Tao Luo", "Yanyang Xiao", "Zheng Ma"], "TL;DR": "In real problems, we found that DNNs often fit target functions from low to high frequencies during the training process.", "authorids": ["xuzhiqin@sjtu.edu.cn", "yaoyu@ias.edu", "luo196@purdue.edu", "xyy82148@gmail.com", "ma531@purdue.edu"], "paperhash": "xu|frequency_principle_fourier_analysis_sheds_light_on_deep_neural_networks", "original_pdf": "/attachment/57e0e726ad748bcdf888dc9d9b6796d3bcd03e9f.pdf", "_bibtex": "@misc{\nxu2020frequency,\ntitle={Frequency Principle: Fourier Analysis Sheds Light on Deep Neural Networks},\nauthor={Zhi-Qin John Xu and Yaoyu Zhang and Tao Luo and Yanyang Xiao and Zheng Ma},\nyear={2020},\nurl={https://openreview.net/forum?id=Skgb5h4KPH}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "Xx6IwfdRuX", "original": null, "number": 1, "cdate": 1576798687540, "ddate": null, "tcdate": 1576798687540, "tmdate": 1576800947565, "tddate": null, "forum": "Skgb5h4KPH", "replyto": "Skgb5h4KPH", "invitation": "ICLR.cc/2020/Conference/Paper106/-/Decision", "content": {"decision": "Reject", "comment": "Borderline decision.  The idea is nice, but the theory is not completely convincing.  That makes the results in this paper not be significant enough.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"abstract": "We study the training process of Deep Neural Networks (DNNs) from the Fourier analysis perspective. We demonstrate a very universal Frequency Principle (F-Principle) --- DNNs often fit target functions from low to high frequencies --- on high-dimensional benchmark datasets, such as MNIST/CIFAR10, and deep networks, such as VGG16. This F-Principle of DNNs is opposite to the learning behavior of most conventional iterative numerical schemes (e.g., Jacobi method), which exhibits faster convergence for higher frequencies, for various scientific computing problems. With a naive theory, we illustrate that this F-Principle results from the regularity of the commonly used activation functions. The F-Principle implies an implicit bias that DNNs tend to fit training data by a low-frequency function. This understanding provides an explanation of good generalization of DNNs on most real datasets and bad generalization of DNNs on parity function or randomized dataset.", "title": "Frequency Principle: Fourier Analysis Sheds Light on Deep Neural Networks", "keywords": ["deep learning", "training behavior", "Fourier analysis", "generalization"], "pdf": "/pdf/b38a23a244dfb59238f22a154229345291c1d402.pdf", "authors": ["Zhi-Qin John Xu", "Yaoyu Zhang", "Tao Luo", "Yanyang Xiao", "Zheng Ma"], "TL;DR": "In real problems, we found that DNNs often fit target functions from low to high frequencies during the training process.", "authorids": ["xuzhiqin@sjtu.edu.cn", "yaoyu@ias.edu", "luo196@purdue.edu", "xyy82148@gmail.com", "ma531@purdue.edu"], "paperhash": "xu|frequency_principle_fourier_analysis_sheds_light_on_deep_neural_networks", "original_pdf": "/attachment/57e0e726ad748bcdf888dc9d9b6796d3bcd03e9f.pdf", "_bibtex": "@misc{\nxu2020frequency,\ntitle={Frequency Principle: Fourier Analysis Sheds Light on Deep Neural Networks},\nauthor={Zhi-Qin John Xu and Yaoyu Zhang and Tao Luo and Yanyang Xiao and Zheng Ma},\nyear={2020},\nurl={https://openreview.net/forum?id=Skgb5h4KPH}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "Skgb5h4KPH", "replyto": "Skgb5h4KPH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795724167, "tmdate": 1576800275768, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper106/-/Decision"}}}, {"id": "r1g3UpbZoH", "original": null, "number": 4, "cdate": 1573096788044, "ddate": null, "tcdate": 1573096788044, "tmdate": 1573115352761, "tddate": null, "forum": "Skgb5h4KPH", "replyto": "r1eohdR3Kr", "invitation": "ICLR.cc/2020/Conference/Paper106/-/Official_Comment", "content": {"title": "To Reviewer 1", "comment": "\nWe thank you for very helpful and detailed comments. We respond to your concerns as follows.\n\n1 We had also looked at other directions, e.g., second or higher principle component, and found that F-Principle always holds. We will add more details about these results in our revision.\n\n2 In all our experiments, we consistently consider the response frequency of $\\{\\vec{y}_i\\}_{i=0}^{n-1}$ on training inputs $\\{\\vec{x}_i\\}_{i=0}^{n-1}$  by the standard nonuniform discrete Fourier transform (NUDFT). See more in Authors' comment [RFD]. For your convenience, we briefly respond to this concern as follows. In the projection method, each frequency k is computed by the NUDFT. Due to the space limit, we make these formulas inline. In the filtering method, we do not compute each frequency due to the high computational cost of high-dimensional Fourier transform. We use a Gaussian filter to decompose the frequency domain into a low-frequency part and a high-frequency part, and then examine the F-Principle in the radially averaged sense. We will make the response frequency clearer in the revision. \n\n3 We did experiments for various optimizers including these suggested by the reviewer and even non-gradient based methods. F-Principle can always be robustly observed. We will add discussions about these results in our revision.\n\nFinally, thank the reviewer for improving our writing. Please do not hesitate to tell us if you have more concerns."}, "signatures": ["ICLR.cc/2020/Conference/Paper106/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper106/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"abstract": "We study the training process of Deep Neural Networks (DNNs) from the Fourier analysis perspective. We demonstrate a very universal Frequency Principle (F-Principle) --- DNNs often fit target functions from low to high frequencies --- on high-dimensional benchmark datasets, such as MNIST/CIFAR10, and deep networks, such as VGG16. This F-Principle of DNNs is opposite to the learning behavior of most conventional iterative numerical schemes (e.g., Jacobi method), which exhibits faster convergence for higher frequencies, for various scientific computing problems. With a naive theory, we illustrate that this F-Principle results from the regularity of the commonly used activation functions. The F-Principle implies an implicit bias that DNNs tend to fit training data by a low-frequency function. This understanding provides an explanation of good generalization of DNNs on most real datasets and bad generalization of DNNs on parity function or randomized dataset.", "title": "Frequency Principle: Fourier Analysis Sheds Light on Deep Neural Networks", "keywords": ["deep learning", "training behavior", "Fourier analysis", "generalization"], "pdf": "/pdf/b38a23a244dfb59238f22a154229345291c1d402.pdf", "authors": ["Zhi-Qin John Xu", "Yaoyu Zhang", "Tao Luo", "Yanyang Xiao", "Zheng Ma"], "TL;DR": "In real problems, we found that DNNs often fit target functions from low to high frequencies during the training process.", "authorids": ["xuzhiqin@sjtu.edu.cn", "yaoyu@ias.edu", "luo196@purdue.edu", "xyy82148@gmail.com", "ma531@purdue.edu"], "paperhash": "xu|frequency_principle_fourier_analysis_sheds_light_on_deep_neural_networks", "original_pdf": "/attachment/57e0e726ad748bcdf888dc9d9b6796d3bcd03e9f.pdf", "_bibtex": "@misc{\nxu2020frequency,\ntitle={Frequency Principle: Fourier Analysis Sheds Light on Deep Neural Networks},\nauthor={Zhi-Qin John Xu and Yaoyu Zhang and Tao Luo and Yanyang Xiao and Zheng Ma},\nyear={2020},\nurl={https://openreview.net/forum?id=Skgb5h4KPH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Skgb5h4KPH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper106/Authors", "ICLR.cc/2020/Conference/Paper106/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper106/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper106/Reviewers", "ICLR.cc/2020/Conference/Paper106/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper106/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper106/Authors|ICLR.cc/2020/Conference/Paper106/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504176313, "tmdate": 1576860534527, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper106/Authors", "ICLR.cc/2020/Conference/Paper106/Reviewers", "ICLR.cc/2020/Conference/Paper106/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper106/-/Official_Comment"}}}, {"id": "Hyg-liWWiB", "original": null, "number": 2, "cdate": 1573096169178, "ddate": null, "tcdate": 1573096169178, "tmdate": 1573115334377, "tddate": null, "forum": "Skgb5h4KPH", "replyto": "BklpyIz0YB", "invitation": "ICLR.cc/2020/Conference/Paper106/-/Official_Comment", "content": {"title": "To Reviewer #2", "comment": "\nThank you for insightful comments. We response to your concerns as follows.\n\n(1) The rigorous definition of the F-Principle relies on the rigorous definitions of response frequency and converging speed. For your convenience,  we recapitulate detailed definition and calculation of response frequency in Authors' comment [RFD] based on non-uniform discrete Fourier transform (NUDFT), which is a standard technique for the evaluation of frequency for nonuniformly sampled data. And the converging speed is defined by the relative error (Sec. 3.1 and 4.1). Therefore, the definition of the F-Principle is defined rigorously. We will elaborate this definition clearer in the revisions.\n\n(2) Your concern can be solved by clarifying the definition of response frequency, which can be found in Authors' comment [RFD]. Here we also explains it in more details. First, in the real datasets, the frequencies CANNOT be directly calculated on a grid along the $1$-dimensional subspace. This is because we DO NOT know the ground truth. We only have some FIXED training data $\\{\\vec{x}_i,\\vec{y}_i\\}$, which are non-uniformly sampled. The response frequencies are calculated by the standard NUDFT on dataset $\\{\\vec{x}_i, \\vec{y}_i\\}$, which depend on both the input $\\{\\vec{x}_i\\}$ and output $\\{\\vec{y}_i\\}$. Second, although during the training process $\\{\\vec{x}_i\\}$ does not change, the output of the predictor function (here is DNN) keeps evolving. Therefore, it is not weird to compute the response frequencies. Third, to avoid confusion, in Section 2, we emphasize that the response frequency of $\\{\\vec{y}_i\\}$ w.r.t. the input $\\{\\vec{x}_i\\}$ is considered. \n\n(3)  All Sections 3, 4 and 6 contributes to the study of the response frequency. The only difference is that we apply NUDFT for experimental estimation of frequency on real datasets whereas we apply the original integral form of Fourier transform for our theorems. We would emphasize the response frequency more in the revision.\n\nFinally, we hope the clarification of NUDFT in Authors' comment [RFD] can help you make more sense of this paper. Please do not hesitate to tell us if you have more concerns."}, "signatures": ["ICLR.cc/2020/Conference/Paper106/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper106/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"abstract": "We study the training process of Deep Neural Networks (DNNs) from the Fourier analysis perspective. We demonstrate a very universal Frequency Principle (F-Principle) --- DNNs often fit target functions from low to high frequencies --- on high-dimensional benchmark datasets, such as MNIST/CIFAR10, and deep networks, such as VGG16. This F-Principle of DNNs is opposite to the learning behavior of most conventional iterative numerical schemes (e.g., Jacobi method), which exhibits faster convergence for higher frequencies, for various scientific computing problems. With a naive theory, we illustrate that this F-Principle results from the regularity of the commonly used activation functions. The F-Principle implies an implicit bias that DNNs tend to fit training data by a low-frequency function. This understanding provides an explanation of good generalization of DNNs on most real datasets and bad generalization of DNNs on parity function or randomized dataset.", "title": "Frequency Principle: Fourier Analysis Sheds Light on Deep Neural Networks", "keywords": ["deep learning", "training behavior", "Fourier analysis", "generalization"], "pdf": "/pdf/b38a23a244dfb59238f22a154229345291c1d402.pdf", "authors": ["Zhi-Qin John Xu", "Yaoyu Zhang", "Tao Luo", "Yanyang Xiao", "Zheng Ma"], "TL;DR": "In real problems, we found that DNNs often fit target functions from low to high frequencies during the training process.", "authorids": ["xuzhiqin@sjtu.edu.cn", "yaoyu@ias.edu", "luo196@purdue.edu", "xyy82148@gmail.com", "ma531@purdue.edu"], "paperhash": "xu|frequency_principle_fourier_analysis_sheds_light_on_deep_neural_networks", "original_pdf": "/attachment/57e0e726ad748bcdf888dc9d9b6796d3bcd03e9f.pdf", "_bibtex": "@misc{\nxu2020frequency,\ntitle={Frequency Principle: Fourier Analysis Sheds Light on Deep Neural Networks},\nauthor={Zhi-Qin John Xu and Yaoyu Zhang and Tao Luo and Yanyang Xiao and Zheng Ma},\nyear={2020},\nurl={https://openreview.net/forum?id=Skgb5h4KPH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Skgb5h4KPH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper106/Authors", "ICLR.cc/2020/Conference/Paper106/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper106/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper106/Reviewers", "ICLR.cc/2020/Conference/Paper106/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper106/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper106/Authors|ICLR.cc/2020/Conference/Paper106/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504176313, "tmdate": 1576860534527, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper106/Authors", "ICLR.cc/2020/Conference/Paper106/Reviewers", "ICLR.cc/2020/Conference/Paper106/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper106/-/Official_Comment"}}}, {"id": "SkedZT-ZsS", "original": null, "number": 3, "cdate": 1573096704257, "ddate": null, "tcdate": 1573096704257, "tmdate": 1573115294126, "tddate": null, "forum": "Skgb5h4KPH", "replyto": "rkeMvyPTFH", "invitation": "ICLR.cc/2020/Conference/Paper106/-/Official_Comment", "content": {"title": "To reviewer 3", "comment": "Thanks for helpful comments. We respond to your concerns as follows.\n\n(1) The filtering method is our important contribution  since high dimension is a key feature of real data.\n\nYour concerns in (2,3,4) can be clarified by the definition of response frequency based on the standard non-uniform discrete Fourier transform (NUDFT)  in Authors' comment [RFD]. For your convenience, we respond to each concern in detail.\n\n(2) A short answer is we CANNOT define the function. A long one is as follows.  (See wiki for more) When dealing with real data, we do not know the true function, which is defined on $\\mathbb{R}^d$. Therefore, we CANNOT define the function since we only have some FIXED training samples $\\{\\vec{x}_{i},\\vec{y}_{i}\\}$, where $\\vec{x}_i \\in R^d$. \"As a generalized approach for nonuniform sampling, the NUDFT allows one to obtain frequency domain information of a finite length signal at any frequency.\"\n\n(3) The Eq. (4) can be performed on all points in $R^d$, however, since we have only limited training data points in real applications, Eq. (4) is performed discretely on $\\{\\vec{x}_{i},\\vec{y}_{i}\\}$. \n\n(4) The $L^2$ norm (over $\\vec{x} \\in R^d$) is the total energy, including all frequencies (Parseval theorem). Next, we explain why Eq. (4) is a good approximation of the low-frequency part. Eq. (4) is a convolution. Fourier transform (FT) of convolution in the spatial domain is equivalent to the multiplication of Fourier transforms in the frequency domain (see convolution theorem in \\url{https://en.wikipedia.org/wiki/Convolution_theorem}). Therefore, the FT of Eq. (4) equals to the FT of $y$ multiplied by the FT of Gaussian function. Since, the FT of a Gaussian is still a Gaussian, which exponentially decays w.r.t. frequency, the FT of Eq. (4) almost loses all high-frequency information of y. This is why Gaussian is a low-frequency filter. As a simple example, when the variance of the Gaussian filter goes to infinity, Eq. (4) leads to that $\\vec{y}^{{\\rm low},\\delta}_{i}$ equals to the mean (zero frequency) of y for all $i$. When the variance goes to zero, the Gaussian tends to be a delta function, $ \\vec{y}^{{\\rm low},\\delta}_{i}=\\vec{y}_{i}$ for each $i$, i.e., keeping information of all frequencies. Therefore, by tuning the Gaussian variance, we can control how much low-frequency information is kept in ${ \\vec{y}^{{\\rm low},\\delta}_i}$. \n\n(5) We would refine our related work section to state our contribution clearer. In the following, we detail our contribution compared with previous works. \n\nEmpirical results:\n\nF-Principle was first discovered in [1] and [2] simultaneously through simple synthetic data and not very deep networks. (We use the name F-Principle following work [2]) Although in the revised version of [1], they also examine the F-Principle in the MNIST dataset. However, they add artificial noise to MNIST, which CONTAMINATES the labels and damages the structure of real data.  \n\nHowever, in deep learning, empirical phenomena VARY from one network structure to another, from one dataset to another. Most importantly, they may exhibit significant difference between synthetic data and high dimensional real data. In addition, networks in real applications are very deep while those in [1,2] are not very deep. Therefore, it is important to empirically verify F-Principle in more general and realistic settings.\n\nOur contribution in the empirical verification is as follows: (a) Our paper utilizes two methods to verify the F-Principle in high-dimensional dataset (MNIST and CIFAR10) from different perspectives. (b) We show F-Principle holds in very deep networks (VGG16). (c) We show F-Principle holds in both CNN and fully connected networks. (d) We show F-Principle holds not only for previously investigated MSE Loss, but also for cross-entropy loss in classification problems and variational loss function in solving PDE. Overall, this work provides a convincing empirical demonstration for the universality of the F-Principle that DNNs learn low-frequency patterns first.  \n\nTheoretical results:\n\nIn this paper, we prove the F-Principle in an ideal setting. This is important because (a) it provides key insights into the mechanism underlying the F-Principle; (b) it inspires a more rigorous proof in a following work, which is mentioned in the paper as mentioned by the reviewer. According to the public dates of the papers in arxiv, this paper is much earlier than that rigorous theoretical paper. The rigorous theoretical paper also clearly states that its idea FOLLOWS this under-reviewed paper. \nThe theoretical study of the gradient of $\\tanh(x)$ in the Fourier domain is adopted by [1], in which they generalize the analysis to ReLU and show similar results.\n\nPlease do not hesitate to tell us if you have more concerns.\n\n[1] Nasim Rahaman et al. On the spectral bias of deep neural networks.  arXiv:1806.08734v2 \n\n[2] Xu et al. Training behavior of deep neural network in frequency domain.  arXiv:1807.01251"}, "signatures": ["ICLR.cc/2020/Conference/Paper106/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper106/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"abstract": "We study the training process of Deep Neural Networks (DNNs) from the Fourier analysis perspective. We demonstrate a very universal Frequency Principle (F-Principle) --- DNNs often fit target functions from low to high frequencies --- on high-dimensional benchmark datasets, such as MNIST/CIFAR10, and deep networks, such as VGG16. This F-Principle of DNNs is opposite to the learning behavior of most conventional iterative numerical schemes (e.g., Jacobi method), which exhibits faster convergence for higher frequencies, for various scientific computing problems. With a naive theory, we illustrate that this F-Principle results from the regularity of the commonly used activation functions. The F-Principle implies an implicit bias that DNNs tend to fit training data by a low-frequency function. This understanding provides an explanation of good generalization of DNNs on most real datasets and bad generalization of DNNs on parity function or randomized dataset.", "title": "Frequency Principle: Fourier Analysis Sheds Light on Deep Neural Networks", "keywords": ["deep learning", "training behavior", "Fourier analysis", "generalization"], "pdf": "/pdf/b38a23a244dfb59238f22a154229345291c1d402.pdf", "authors": ["Zhi-Qin John Xu", "Yaoyu Zhang", "Tao Luo", "Yanyang Xiao", "Zheng Ma"], "TL;DR": "In real problems, we found that DNNs often fit target functions from low to high frequencies during the training process.", "authorids": ["xuzhiqin@sjtu.edu.cn", "yaoyu@ias.edu", "luo196@purdue.edu", "xyy82148@gmail.com", "ma531@purdue.edu"], "paperhash": "xu|frequency_principle_fourier_analysis_sheds_light_on_deep_neural_networks", "original_pdf": "/attachment/57e0e726ad748bcdf888dc9d9b6796d3bcd03e9f.pdf", "_bibtex": "@misc{\nxu2020frequency,\ntitle={Frequency Principle: Fourier Analysis Sheds Light on Deep Neural Networks},\nauthor={Zhi-Qin John Xu and Yaoyu Zhang and Tao Luo and Yanyang Xiao and Zheng Ma},\nyear={2020},\nurl={https://openreview.net/forum?id=Skgb5h4KPH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Skgb5h4KPH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper106/Authors", "ICLR.cc/2020/Conference/Paper106/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper106/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper106/Reviewers", "ICLR.cc/2020/Conference/Paper106/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper106/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper106/Authors|ICLR.cc/2020/Conference/Paper106/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504176313, "tmdate": 1576860534527, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper106/Authors", "ICLR.cc/2020/Conference/Paper106/Reviewers", "ICLR.cc/2020/Conference/Paper106/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper106/-/Official_Comment"}}}, {"id": "BylnTTZWsB", "original": null, "number": 5, "cdate": 1573096900457, "ddate": null, "tcdate": 1573096900457, "tmdate": 1573096900457, "tddate": null, "forum": "Skgb5h4KPH", "replyto": "BkglOAJn5B", "invitation": "ICLR.cc/2020/Conference/Paper106/-/Official_Comment", "content": {"title": "Done on small portion of ImageNet", "comment": "Thanks for your suggestion. We did some test on small portion of ImageNet. The F-Principle holds. However, we yet to perform a large-scale experiment."}, "signatures": ["ICLR.cc/2020/Conference/Paper106/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper106/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"abstract": "We study the training process of Deep Neural Networks (DNNs) from the Fourier analysis perspective. We demonstrate a very universal Frequency Principle (F-Principle) --- DNNs often fit target functions from low to high frequencies --- on high-dimensional benchmark datasets, such as MNIST/CIFAR10, and deep networks, such as VGG16. This F-Principle of DNNs is opposite to the learning behavior of most conventional iterative numerical schemes (e.g., Jacobi method), which exhibits faster convergence for higher frequencies, for various scientific computing problems. With a naive theory, we illustrate that this F-Principle results from the regularity of the commonly used activation functions. The F-Principle implies an implicit bias that DNNs tend to fit training data by a low-frequency function. This understanding provides an explanation of good generalization of DNNs on most real datasets and bad generalization of DNNs on parity function or randomized dataset.", "title": "Frequency Principle: Fourier Analysis Sheds Light on Deep Neural Networks", "keywords": ["deep learning", "training behavior", "Fourier analysis", "generalization"], "pdf": "/pdf/b38a23a244dfb59238f22a154229345291c1d402.pdf", "authors": ["Zhi-Qin John Xu", "Yaoyu Zhang", "Tao Luo", "Yanyang Xiao", "Zheng Ma"], "TL;DR": "In real problems, we found that DNNs often fit target functions from low to high frequencies during the training process.", "authorids": ["xuzhiqin@sjtu.edu.cn", "yaoyu@ias.edu", "luo196@purdue.edu", "xyy82148@gmail.com", "ma531@purdue.edu"], "paperhash": "xu|frequency_principle_fourier_analysis_sheds_light_on_deep_neural_networks", "original_pdf": "/attachment/57e0e726ad748bcdf888dc9d9b6796d3bcd03e9f.pdf", "_bibtex": "@misc{\nxu2020frequency,\ntitle={Frequency Principle: Fourier Analysis Sheds Light on Deep Neural Networks},\nauthor={Zhi-Qin John Xu and Yaoyu Zhang and Tao Luo and Yanyang Xiao and Zheng Ma},\nyear={2020},\nurl={https://openreview.net/forum?id=Skgb5h4KPH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Skgb5h4KPH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper106/Authors", "ICLR.cc/2020/Conference/Paper106/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper106/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper106/Reviewers", "ICLR.cc/2020/Conference/Paper106/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper106/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper106/Authors|ICLR.cc/2020/Conference/Paper106/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504176313, "tmdate": 1576860534527, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper106/Authors", "ICLR.cc/2020/Conference/Paper106/Reviewers", "ICLR.cc/2020/Conference/Paper106/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper106/-/Official_Comment"}}}, {"id": "BJg6WcZbsr", "original": null, "number": 1, "cdate": 1573095941322, "ddate": null, "tcdate": 1573095941322, "tmdate": 1573095941322, "tddate": null, "forum": "Skgb5h4KPH", "replyto": "Skgb5h4KPH", "invitation": "ICLR.cc/2020/Conference/Paper106/-/Official_Comment", "content": {"title": "[RFD] Response frequency definition", "comment": "Response frequency of training data $\\{\\vec{y}_i\\}_{i=0}^{n-1}$ on inputs $\\{\\vec{x}_i\\}_{i=0}^{n-1}$.\n\n\nIn all our experiments, we consistently consider the response frequency defined for the mapping function $g$ between inputs and outputs, say $\\mathbb{R}^d\\to\\mathbb{R}$ and any $\\vec{k}\\in\\mathbb{R}^d$ via the standard nonuniform discrete Fourier transform (NUDFT)\n$$\n\\hat{g}_{\\vec{k}}= \\frac{1}{n}\\sum_{i=0}^{n-1}{g(\\vec{x}_{i})\\mathrm{e}^{-\\mathrm{i} 2\\pi\\vec{k}\\cdot\\vec{x}_i}},\n$$\nwhich is a natural estimator of frequency composition of $g$. (More details can be found in \\url{https://en.wikipedia.org/wiki/Non-uniform_discrete_Fourier_transform}.) As $n\\to \\infty$, $\\hat{g}_{\\vec{k}}\\to\\int{g(\\vec{x})\\mathrm{e}^{-\\mathrm{i} 2\\pi\\vec{k}\\cdot\\vec{x}}\\nu(\\vec{x}){\\ \\mathrm{d}\\vec{x}}}$, where $\\nu(\\vec{x})$ is the data distribution.\nWe restrict all the evaluation of Fourier transform in our experiments to NUDFT of $\\{\\vec{y}_i\\}_{i=0}^{n-1}$  at $\\{\\vec{x}_i\\}_{i=0}^{n-1}$ for the following practical reasons.\n\n(i) The information of target function is only available at $\\{\\vec{x}_i\\}_{i=0}^{n-1}$ for training.\n\n(ii) It allows us to perform the convergence analysis. As $t\\to \\infty$, in general, $h(\\vec{x}_i,t)\\to \\vec{y}_{i}$ for any $i$ ($h(\\vec{x}_i,t)$ is the DNN output), leading to $\\hat{h}_{\\vec{k}}\\to \\hat{y}_{\\vec{k}}$ for any $\\vec{k}$. Therefore, we can analyze the convergence at different $\\vec{k}$ by evaluating $\\Delta_{F}(\\vec{k})=|\\hat{h}_{\\vec{k}}-\\hat{y}_{\\vec{k}}|/|\\hat{y}_{\\vec{k}}|$ during the training. If we use a different set of data points for frequency evaluation of DNN output, then $\\Delta_{F}(\\vec{k})$ may not converge to $0$ at the end of training. \n\n(iii) $\\hat{y}_{\\vec{k}}$ faithfully reflect the frequency structure of training data $\\{\\vec{x}_i,\\vec{y}_i\\}_{i=0}^{n-1}$. Intuitively, high frequencies of $\\hat{y}_{\\vec{k}}$ correspond to sharp changes of output for some nearby points in the training data. Then, by applying a Gaussian filter and evaluating still at $\\{\\vec{x}_i\\}_{i=0}^{n-1}$, we obtain the low frequency part of training data with these sharp changes (high frequencies) well suppressed.\n\nIn practice, it is impossible to evaluate and compare the convergence of all $\\vec{k}\\in\\mathbb{R}^d$ even with a proper cutoff frequency for a very large $d$ of $O(10^2)$ (MNIST) or $O(10^3)$ (CIFAR10) due to curse of dimensionality. Therefore, we propose the projection approach, i.e., fixing $\\vec{k}$ at a specific direction and the filtering approach as detailed in Section 3 and 4, respectively."}, "signatures": ["ICLR.cc/2020/Conference/Paper106/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper106/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"abstract": "We study the training process of Deep Neural Networks (DNNs) from the Fourier analysis perspective. We demonstrate a very universal Frequency Principle (F-Principle) --- DNNs often fit target functions from low to high frequencies --- on high-dimensional benchmark datasets, such as MNIST/CIFAR10, and deep networks, such as VGG16. This F-Principle of DNNs is opposite to the learning behavior of most conventional iterative numerical schemes (e.g., Jacobi method), which exhibits faster convergence for higher frequencies, for various scientific computing problems. With a naive theory, we illustrate that this F-Principle results from the regularity of the commonly used activation functions. The F-Principle implies an implicit bias that DNNs tend to fit training data by a low-frequency function. This understanding provides an explanation of good generalization of DNNs on most real datasets and bad generalization of DNNs on parity function or randomized dataset.", "title": "Frequency Principle: Fourier Analysis Sheds Light on Deep Neural Networks", "keywords": ["deep learning", "training behavior", "Fourier analysis", "generalization"], "pdf": "/pdf/b38a23a244dfb59238f22a154229345291c1d402.pdf", "authors": ["Zhi-Qin John Xu", "Yaoyu Zhang", "Tao Luo", "Yanyang Xiao", "Zheng Ma"], "TL;DR": "In real problems, we found that DNNs often fit target functions from low to high frequencies during the training process.", "authorids": ["xuzhiqin@sjtu.edu.cn", "yaoyu@ias.edu", "luo196@purdue.edu", "xyy82148@gmail.com", "ma531@purdue.edu"], "paperhash": "xu|frequency_principle_fourier_analysis_sheds_light_on_deep_neural_networks", "original_pdf": "/attachment/57e0e726ad748bcdf888dc9d9b6796d3bcd03e9f.pdf", "_bibtex": "@misc{\nxu2020frequency,\ntitle={Frequency Principle: Fourier Analysis Sheds Light on Deep Neural Networks},\nauthor={Zhi-Qin John Xu and Yaoyu Zhang and Tao Luo and Yanyang Xiao and Zheng Ma},\nyear={2020},\nurl={https://openreview.net/forum?id=Skgb5h4KPH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Skgb5h4KPH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper106/Authors", "ICLR.cc/2020/Conference/Paper106/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper106/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper106/Reviewers", "ICLR.cc/2020/Conference/Paper106/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper106/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper106/Authors|ICLR.cc/2020/Conference/Paper106/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504176313, "tmdate": 1576860534527, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper106/Authors", "ICLR.cc/2020/Conference/Paper106/Reviewers", "ICLR.cc/2020/Conference/Paper106/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper106/-/Official_Comment"}}}, {"id": "r1eohdR3Kr", "original": null, "number": 1, "cdate": 1571772595121, "ddate": null, "tcdate": 1571772595121, "tmdate": 1572972637992, "tddate": null, "forum": "Skgb5h4KPH", "replyto": "Skgb5h4KPH", "invitation": "ICLR.cc/2020/Conference/Paper106/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "This paper proposes to analyze the loss of neural networks in the Fourier domain. Since this is computationally expensive for larger-dimensional datasets, the analysis instead first projects the data onto the principal component of the data, and then using a Gaussian kernel estimation (which has nice properties in the Fourier domain). The analysis finds that DNNs tend to learn low-frequency components before high-frequency ones.\n\nOverall I quite like the analysis of this paper. I think it could be clearer and contain more experiments but it is otherwise rather convincing proof that DNNs learn low-frequency patterns first.\n\n\n- More experiments: in particular, analyzing this phenomenon over more than a single principal component or through non-linear transformations of the data.\n- It's not always clear how $\\mathbf{k}$ is calculated or where it comes from, whether it is implicit through the Gaussian metric or chosen randomly. The paper would benefit from always making this clear in the text and figure captions.\n- It's well known that different optimizers seem to learn differently both in terms of speed and features that end up being learned (thus generalization), repeating this analysis for Adam, RMSprop & friends would be great.\n\nThe paper is mostly easy to read, but there are a few mistakes here and there that slow down reading. Here are a few:\n- \"variation problems\" you mean \"variational\"?\n- \"This difference implicates\" -> \"implies\"\n- \"by the Parseval's theorem\" -> \"by Parseval's theorem\" (occurs multiple times)\n- \"difference of\" -> \"difference between\"\n- \"in previous section\" -> \"in the previous section\"\n- \"to verify F-Principle\" -> \"to verify the F-Principle\""}, "signatures": ["ICLR.cc/2020/Conference/Paper106/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper106/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"abstract": "We study the training process of Deep Neural Networks (DNNs) from the Fourier analysis perspective. We demonstrate a very universal Frequency Principle (F-Principle) --- DNNs often fit target functions from low to high frequencies --- on high-dimensional benchmark datasets, such as MNIST/CIFAR10, and deep networks, such as VGG16. This F-Principle of DNNs is opposite to the learning behavior of most conventional iterative numerical schemes (e.g., Jacobi method), which exhibits faster convergence for higher frequencies, for various scientific computing problems. With a naive theory, we illustrate that this F-Principle results from the regularity of the commonly used activation functions. The F-Principle implies an implicit bias that DNNs tend to fit training data by a low-frequency function. This understanding provides an explanation of good generalization of DNNs on most real datasets and bad generalization of DNNs on parity function or randomized dataset.", "title": "Frequency Principle: Fourier Analysis Sheds Light on Deep Neural Networks", "keywords": ["deep learning", "training behavior", "Fourier analysis", "generalization"], "pdf": "/pdf/b38a23a244dfb59238f22a154229345291c1d402.pdf", "authors": ["Zhi-Qin John Xu", "Yaoyu Zhang", "Tao Luo", "Yanyang Xiao", "Zheng Ma"], "TL;DR": "In real problems, we found that DNNs often fit target functions from low to high frequencies during the training process.", "authorids": ["xuzhiqin@sjtu.edu.cn", "yaoyu@ias.edu", "luo196@purdue.edu", "xyy82148@gmail.com", "ma531@purdue.edu"], "paperhash": "xu|frequency_principle_fourier_analysis_sheds_light_on_deep_neural_networks", "original_pdf": "/attachment/57e0e726ad748bcdf888dc9d9b6796d3bcd03e9f.pdf", "_bibtex": "@misc{\nxu2020frequency,\ntitle={Frequency Principle: Fourier Analysis Sheds Light on Deep Neural Networks},\nauthor={Zhi-Qin John Xu and Yaoyu Zhang and Tao Luo and Yanyang Xiao and Zheng Ma},\nyear={2020},\nurl={https://openreview.net/forum?id=Skgb5h4KPH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Skgb5h4KPH", "replyto": "Skgb5h4KPH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper106/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper106/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575667947502, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper106/Reviewers"], "noninvitees": [], "tcdate": 1570237756987, "tmdate": 1575667947516, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper106/-/Official_Review"}}}, {"id": "rkeMvyPTFH", "original": null, "number": 2, "cdate": 1571807066038, "ddate": null, "tcdate": 1571807066038, "tmdate": 1572972637939, "tddate": null, "forum": "Skgb5h4KPH", "replyto": "Skgb5h4KPH", "invitation": "ICLR.cc/2020/Conference/Paper106/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper studies the training process of NNs through the lens of Fourier analysis. The authors argue that during the training process, NNs will first learn low frequencies part of the function first and then the high frequency part. To verify this claim empirically, the author propose two methods: 1. examine the convergence of different frequencies in a pre-selected direction in the frequency space during training; 2. examine the convergence rate of the 2-norm of low v.s. high frequencies during training.  Through the experimental results of these two methods, the authors conclude that NNs learn the low  frequency components before the high frequency components. The authors also discuss a potential application of this observation to solving high dimensional PDEs: coupling DNNs training (good at learning low frequency components) with the Jacobi method (good at learning high frequency components). Finally, the authors also provide some theoretical intuition (Thm 1., 2.) why low frequency components are learned faster and an explanation why NNs could generalize well on images but perform poorly on tasks like learning parity functions. \n\n\nOther comments: \n1. It seems the filtering method is a better (might be a sufficient) way to justify the F-Principle than the projection method, given the projection method examines only one direction (also appointed out in the paper).  \n2. When talking about Fourier transform, would you specify what is the domain of the functions and how the functions are defined (section 3.1) The notation there is somewhat confusing (which makes the rest of the paper difficult to follow) since you are mentioning the Fourier transform of the set {(x_i, y_i)}. It will be helpful to define the function before defining its Fourier transform.  Please also mention what is the domain of the function, {x_i}_i or R^d? \n3. According to equation (4), it seems the domain of the functions is {x_i}_i, otherwise equation (4) should be a function of x\\in R^d, not x_i.  \n4. Could you elaborate why (4) is a good approximation of the low frequency energy rather than the L2 norm (over x\\in R^d) of (4) with x_i replaced by x\\in R^d. \n5. It might be useful to refine the related work section. It is not clear what are the previous contributions prior to this paper, and it seems [1] shares some similar results/observation with the this paper. \n\nOverall, I lean to a weak rejection. The key findings (and similar results, e.g. NNs learn simple functions first), i.e. F-Principle seems to have already appeared in previous works, e.g. [1] and the theoretical results of this paper are limited to an idealized setting (results of more general setting appear in another work, mentioned in the paper.)\n\n\n[1]Nasim Rahaman, Devansh Arpit, Aristide Baratin, Felix Draxler, Min Lin, Fred A Hamprecht,\nYoshua Bengio, and Aaron Courville. On the spectral bias of deep neural networks. arXiv preprint\narXiv:1806.08734, 2018. 1, 8, A \n\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper106/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper106/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"abstract": "We study the training process of Deep Neural Networks (DNNs) from the Fourier analysis perspective. We demonstrate a very universal Frequency Principle (F-Principle) --- DNNs often fit target functions from low to high frequencies --- on high-dimensional benchmark datasets, such as MNIST/CIFAR10, and deep networks, such as VGG16. This F-Principle of DNNs is opposite to the learning behavior of most conventional iterative numerical schemes (e.g., Jacobi method), which exhibits faster convergence for higher frequencies, for various scientific computing problems. With a naive theory, we illustrate that this F-Principle results from the regularity of the commonly used activation functions. The F-Principle implies an implicit bias that DNNs tend to fit training data by a low-frequency function. This understanding provides an explanation of good generalization of DNNs on most real datasets and bad generalization of DNNs on parity function or randomized dataset.", "title": "Frequency Principle: Fourier Analysis Sheds Light on Deep Neural Networks", "keywords": ["deep learning", "training behavior", "Fourier analysis", "generalization"], "pdf": "/pdf/b38a23a244dfb59238f22a154229345291c1d402.pdf", "authors": ["Zhi-Qin John Xu", "Yaoyu Zhang", "Tao Luo", "Yanyang Xiao", "Zheng Ma"], "TL;DR": "In real problems, we found that DNNs often fit target functions from low to high frequencies during the training process.", "authorids": ["xuzhiqin@sjtu.edu.cn", "yaoyu@ias.edu", "luo196@purdue.edu", "xyy82148@gmail.com", "ma531@purdue.edu"], "paperhash": "xu|frequency_principle_fourier_analysis_sheds_light_on_deep_neural_networks", "original_pdf": "/attachment/57e0e726ad748bcdf888dc9d9b6796d3bcd03e9f.pdf", "_bibtex": "@misc{\nxu2020frequency,\ntitle={Frequency Principle: Fourier Analysis Sheds Light on Deep Neural Networks},\nauthor={Zhi-Qin John Xu and Yaoyu Zhang and Tao Luo and Yanyang Xiao and Zheng Ma},\nyear={2020},\nurl={https://openreview.net/forum?id=Skgb5h4KPH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Skgb5h4KPH", "replyto": "Skgb5h4KPH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper106/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper106/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575667947502, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper106/Reviewers"], "noninvitees": [], "tcdate": 1570237756987, "tmdate": 1575667947516, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper106/-/Official_Review"}}}, {"id": "BklpyIz0YB", "original": null, "number": 3, "cdate": 1571853796721, "ddate": null, "tcdate": 1571853796721, "tmdate": 1572972637896, "tddate": null, "forum": "Skgb5h4KPH", "replyto": "Skgb5h4KPH", "invitation": "ICLR.cc/2020/Conference/Paper106/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper mainly focuses on experimental results on real data to verify the so-called Frequency Principle: DNNs often fit target functions from low to high frequencies during the training process. Some theoretical analyses are also provided to backup the empirical observation. \n\nThis paper is very well-written. The methods are explained very clearly, and the logic is easy to follow. However I think this paper also has some weak points, as listed below:\n\n(1) The frequency principle lacks a rigorous definition. In Section 2, the authors provide very inspiring explanations and examples, however no rigorous definitions are given. Is there a way to directly quantitatively define the response frequency?\n\n(2) In Section 3.1, it is not explained why the frequencies are calculated based on the samples. Probably I have missed something, but based on the description, can\u2019t the frequencies be directly calculated on a grid along the 1-dimensional subspace defined by the mean and first principle component of the data? In other words, even after training the predictor function with real data samples for several steps, the frequency of a predictor function should still be its own property and  should be independent of the distribution of data inputs. In fact, are the vectors $n^{-1/2} (cos( 2\\pi k x_{p_1,1}), cos( 2\\pi k x_{p_1,2}), \\ldots, cos( 2\\pi k x_{p_1,n}) )$ for different $k$ even orthonormal vectors? I think unless $x_{p_1,i}$ follows certain specific distributions, these vectors are not even close to orthonormal. Therefore using them to calculate the frequencies is very weird.\n\n(3) In fact, are Sections 3,4 and 6 studying the same kind of frequency? It is not very clear due to the vague definitions.\n\nBecause of these concerns, I think this paper is on the borderline. For now I tend to recommend a weak reject.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper106/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper106/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"abstract": "We study the training process of Deep Neural Networks (DNNs) from the Fourier analysis perspective. We demonstrate a very universal Frequency Principle (F-Principle) --- DNNs often fit target functions from low to high frequencies --- on high-dimensional benchmark datasets, such as MNIST/CIFAR10, and deep networks, such as VGG16. This F-Principle of DNNs is opposite to the learning behavior of most conventional iterative numerical schemes (e.g., Jacobi method), which exhibits faster convergence for higher frequencies, for various scientific computing problems. With a naive theory, we illustrate that this F-Principle results from the regularity of the commonly used activation functions. The F-Principle implies an implicit bias that DNNs tend to fit training data by a low-frequency function. This understanding provides an explanation of good generalization of DNNs on most real datasets and bad generalization of DNNs on parity function or randomized dataset.", "title": "Frequency Principle: Fourier Analysis Sheds Light on Deep Neural Networks", "keywords": ["deep learning", "training behavior", "Fourier analysis", "generalization"], "pdf": "/pdf/b38a23a244dfb59238f22a154229345291c1d402.pdf", "authors": ["Zhi-Qin John Xu", "Yaoyu Zhang", "Tao Luo", "Yanyang Xiao", "Zheng Ma"], "TL;DR": "In real problems, we found that DNNs often fit target functions from low to high frequencies during the training process.", "authorids": ["xuzhiqin@sjtu.edu.cn", "yaoyu@ias.edu", "luo196@purdue.edu", "xyy82148@gmail.com", "ma531@purdue.edu"], "paperhash": "xu|frequency_principle_fourier_analysis_sheds_light_on_deep_neural_networks", "original_pdf": "/attachment/57e0e726ad748bcdf888dc9d9b6796d3bcd03e9f.pdf", "_bibtex": "@misc{\nxu2020frequency,\ntitle={Frequency Principle: Fourier Analysis Sheds Light on Deep Neural Networks},\nauthor={Zhi-Qin John Xu and Yaoyu Zhang and Tao Luo and Yanyang Xiao and Zheng Ma},\nyear={2020},\nurl={https://openreview.net/forum?id=Skgb5h4KPH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Skgb5h4KPH", "replyto": "Skgb5h4KPH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper106/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper106/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575667947502, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper106/Reviewers"], "noninvitees": [], "tcdate": 1570237756987, "tmdate": 1575667947516, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper106/-/Official_Review"}}}, {"id": "BkglOAJn5B", "original": null, "number": 1, "cdate": 1572761192243, "ddate": null, "tcdate": 1572761192243, "tmdate": 1572761192243, "tddate": null, "forum": "Skgb5h4KPH", "replyto": "Skgb5h4KPH", "invitation": "ICLR.cc/2020/Conference/Paper106/-/Public_Comment", "content": {"title": "Results on ImageNet?", "comment": "As the title suggests, have you guys tried on your analysis on ImageNet?"}, "signatures": ["~Rui_Wang1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Rui_Wang1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"abstract": "We study the training process of Deep Neural Networks (DNNs) from the Fourier analysis perspective. We demonstrate a very universal Frequency Principle (F-Principle) --- DNNs often fit target functions from low to high frequencies --- on high-dimensional benchmark datasets, such as MNIST/CIFAR10, and deep networks, such as VGG16. This F-Principle of DNNs is opposite to the learning behavior of most conventional iterative numerical schemes (e.g., Jacobi method), which exhibits faster convergence for higher frequencies, for various scientific computing problems. With a naive theory, we illustrate that this F-Principle results from the regularity of the commonly used activation functions. The F-Principle implies an implicit bias that DNNs tend to fit training data by a low-frequency function. This understanding provides an explanation of good generalization of DNNs on most real datasets and bad generalization of DNNs on parity function or randomized dataset.", "title": "Frequency Principle: Fourier Analysis Sheds Light on Deep Neural Networks", "keywords": ["deep learning", "training behavior", "Fourier analysis", "generalization"], "pdf": "/pdf/b38a23a244dfb59238f22a154229345291c1d402.pdf", "authors": ["Zhi-Qin John Xu", "Yaoyu Zhang", "Tao Luo", "Yanyang Xiao", "Zheng Ma"], "TL;DR": "In real problems, we found that DNNs often fit target functions from low to high frequencies during the training process.", "authorids": ["xuzhiqin@sjtu.edu.cn", "yaoyu@ias.edu", "luo196@purdue.edu", "xyy82148@gmail.com", "ma531@purdue.edu"], "paperhash": "xu|frequency_principle_fourier_analysis_sheds_light_on_deep_neural_networks", "original_pdf": "/attachment/57e0e726ad748bcdf888dc9d9b6796d3bcd03e9f.pdf", "_bibtex": "@misc{\nxu2020frequency,\ntitle={Frequency Principle: Fourier Analysis Sheds Light on Deep Neural Networks},\nauthor={Zhi-Qin John Xu and Yaoyu Zhang and Tao Luo and Yanyang Xiao and Zheng Ma},\nyear={2020},\nurl={https://openreview.net/forum?id=Skgb5h4KPH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Skgb5h4KPH", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504213864, "tmdate": 1576860568185, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper106/Authors", "ICLR.cc/2020/Conference/Paper106/Reviewers", "ICLR.cc/2020/Conference/Paper106/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper106/-/Public_Comment"}}}], "count": 11}