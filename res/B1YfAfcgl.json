{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1488661642006, "tcdate": 1478270480891, "number": 185, "id": "B1YfAfcgl", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "B1YfAfcgl", "signatures": ["~Pratik_A_Chaudhari1"], "readers": ["everyone"], "content": {"title": "Entropy-SGD: Biasing Gradient Descent Into Wide Valleys", "abstract": "This paper proposes a new optimization algorithm called Entropy-SGD for training deep neural networks that is motivated by the local geometry of the energy landscape. Local extrema with low generalization error have a large proportion of almost-zero eigenvalues in the Hessian with very few positive or negative eigenvalues. We leverage upon this observation to construct a local-entropy-based objective function that favors well-generalizable solutions lying in large flat regions of the energy landscape, while avoiding poorly-generalizable solutions located in the sharp valleys. Conceptually, our algorithm resembles two nested loops of SGD where we use Langevin dynamics in the inner loop to compute the gradient of the local entropy before each update of the weights. We show that the new objective has a smoother energy landscape and show improved generalization over SGD using uniform stability, under certain assumptions. Our experiments on convolutional and recurrent neural networks demonstrate that Entropy-SGD compares favorably to state-of-the-art techniques in terms of generalization error and training time.", "pdf": "/pdf/2fb588bb07ffd17030e097202cf514bdfb5bcbca.pdf", "TL;DR": "This paper focuses on developing new optimization tools for deep learning that are tailored to exploit the local geometric properties of the objective function.", "paperhash": "chaudhari|entropysgd_biasing_gradient_descent_into_wide_valleys", "keywords": ["Deep learning", "Optimization"], "conflicts": ["ucla.edu", "cs.ucla.edu", "nyu.edu", "microsoft.com", "gwu.edu", "nyu.edu", "cims.nyu.edu", "columbia.edu", "facebook.com", "microsoft.com", "polito.it"], "authors": ["Pratik Chaudhari", "Anna Choromanska", "Stefano Soatto", "Yann LeCun", "Carlo Baldassi", "Christian Borgs", "Jennifer Chayes", "Levent Sagun", "Riccardo Zecchina"], "authorids": ["pratikac@ucla.edu", "achoroma@cims.nyu.edu", "soatto@cs.ucla.edu", "yann@cs.nyu.edu", "carlo.baldassi@polito.it", "borgs@microsoft.com", "jchayes@microsoft.com", "sagun@cims.nyu.edu", "riccardo.zecchina@polito.it"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 33, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396419101, "tcdate": 1486396419101, "number": 1, "id": "SJiZnfI_e", "invitation": "ICLR.cc/2017/conference/-/paper185/acceptance", "forum": "B1YfAfcgl", "replyto": "B1YfAfcgl", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"title": "ICLR committee final decision", "comment": "This paper presents both an analysis of neural net optimization landscapes, and an optimization algorithm that encourages movement in directions of high entropy. The motivation is based on intuitions from physics.\n \n Pros\n  - the main idea is well-motivated, from a non-standard perspective.\n  - There are lots of side-experiments supporting the claims for the motivation.\n Cons\n  - The propose method is very complicated, and it sounds like good performance depended on adding and annealing yet another hyperparameter, referred to as 'scoping'.\n  - The motivating intuition has been around for a long time in different forms. In particular, the proposed method is very closely related to stochastic variational inference, or MCMC methods. Appendix C makes it clear that the two methods aren't identical, but I wish the authors had simply run SVI with their proposed modification, instead of appearing to re-invent the idea of maximizing local volume from scratch. The intuition that good generalization comes from regions of high volume is also exactly what Bayes rule says.\n \n In summary, while there is improvement for the paper, the idea is well-motivated and the experimental results are sound.", "decision": "Accept (Poster)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Entropy-SGD: Biasing Gradient Descent Into Wide Valleys", "abstract": "This paper proposes a new optimization algorithm called Entropy-SGD for training deep neural networks that is motivated by the local geometry of the energy landscape. Local extrema with low generalization error have a large proportion of almost-zero eigenvalues in the Hessian with very few positive or negative eigenvalues. We leverage upon this observation to construct a local-entropy-based objective function that favors well-generalizable solutions lying in large flat regions of the energy landscape, while avoiding poorly-generalizable solutions located in the sharp valleys. Conceptually, our algorithm resembles two nested loops of SGD where we use Langevin dynamics in the inner loop to compute the gradient of the local entropy before each update of the weights. We show that the new objective has a smoother energy landscape and show improved generalization over SGD using uniform stability, under certain assumptions. Our experiments on convolutional and recurrent neural networks demonstrate that Entropy-SGD compares favorably to state-of-the-art techniques in terms of generalization error and training time.", "pdf": "/pdf/2fb588bb07ffd17030e097202cf514bdfb5bcbca.pdf", "TL;DR": "This paper focuses on developing new optimization tools for deep learning that are tailored to exploit the local geometric properties of the objective function.", "paperhash": "chaudhari|entropysgd_biasing_gradient_descent_into_wide_valleys", "keywords": ["Deep learning", "Optimization"], "conflicts": ["ucla.edu", "cs.ucla.edu", "nyu.edu", "microsoft.com", "gwu.edu", "nyu.edu", "cims.nyu.edu", "columbia.edu", "facebook.com", "microsoft.com", "polito.it"], "authors": ["Pratik Chaudhari", "Anna Choromanska", "Stefano Soatto", "Yann LeCun", "Carlo Baldassi", "Christian Borgs", "Jennifer Chayes", "Levent Sagun", "Riccardo Zecchina"], "authorids": ["pratikac@ucla.edu", "achoroma@cims.nyu.edu", "soatto@cs.ucla.edu", "yann@cs.nyu.edu", "carlo.baldassi@polito.it", "borgs@microsoft.com", "jchayes@microsoft.com", "sagun@cims.nyu.edu", "riccardo.zecchina@polito.it"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396419601, "id": "ICLR.cc/2017/conference/-/paper185/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "B1YfAfcgl", "replyto": "B1YfAfcgl", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396419601}}}, {"tddate": null, "tmdate": 1484963799070, "tcdate": 1484963799070, "number": 17, "id": "BJ1kgSgvx", "invitation": "ICLR.cc/2017/conference/-/paper185/public/comment", "forum": "B1YfAfcgl", "replyto": "Hy4SB2CUg", "signatures": ["~Pratik_A_Chaudhari1"], "readers": ["everyone"], "writers": ["~Pratik_A_Chaudhari1"], "content": {"title": "Re: short questions", "comment": "We plan to open-source the code and examples.\n\nWe could speculate on how our method relates to the experiments of [1] but we would rather leave this to further investigation.\n\n[2] is quite different from the replica theoretic motivations of Entropy-SGD, the latter creates one good model. While it is possible to create ensembles of networks with more local scoping, the benefits might be offset by significantly longer test times.\n\nThe metric in [3] could be considered as a proxy for local entropy. Entropy-SGD is a specialized algorithm to lead to flat minima, obtaining the same effect using parameters such as batch-size, dropout, batch-normalization, data augmentation etc. is quite hard."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Entropy-SGD: Biasing Gradient Descent Into Wide Valleys", "abstract": "This paper proposes a new optimization algorithm called Entropy-SGD for training deep neural networks that is motivated by the local geometry of the energy landscape. Local extrema with low generalization error have a large proportion of almost-zero eigenvalues in the Hessian with very few positive or negative eigenvalues. We leverage upon this observation to construct a local-entropy-based objective function that favors well-generalizable solutions lying in large flat regions of the energy landscape, while avoiding poorly-generalizable solutions located in the sharp valleys. Conceptually, our algorithm resembles two nested loops of SGD where we use Langevin dynamics in the inner loop to compute the gradient of the local entropy before each update of the weights. We show that the new objective has a smoother energy landscape and show improved generalization over SGD using uniform stability, under certain assumptions. Our experiments on convolutional and recurrent neural networks demonstrate that Entropy-SGD compares favorably to state-of-the-art techniques in terms of generalization error and training time.", "pdf": "/pdf/2fb588bb07ffd17030e097202cf514bdfb5bcbca.pdf", "TL;DR": "This paper focuses on developing new optimization tools for deep learning that are tailored to exploit the local geometric properties of the objective function.", "paperhash": "chaudhari|entropysgd_biasing_gradient_descent_into_wide_valleys", "keywords": ["Deep learning", "Optimization"], "conflicts": ["ucla.edu", "cs.ucla.edu", "nyu.edu", "microsoft.com", "gwu.edu", "nyu.edu", "cims.nyu.edu", "columbia.edu", "facebook.com", "microsoft.com", "polito.it"], "authors": ["Pratik Chaudhari", "Anna Choromanska", "Stefano Soatto", "Yann LeCun", "Carlo Baldassi", "Christian Borgs", "Jennifer Chayes", "Levent Sagun", "Riccardo Zecchina"], "authorids": ["pratikac@ucla.edu", "achoroma@cims.nyu.edu", "soatto@cs.ucla.edu", "yann@cs.nyu.edu", "carlo.baldassi@polito.it", "borgs@microsoft.com", "jchayes@microsoft.com", "sagun@cims.nyu.edu", "riccardo.zecchina@polito.it"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287695367, "id": "ICLR.cc/2017/conference/-/paper185/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "B1YfAfcgl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper185/reviewers", "ICLR.cc/2017/conference/paper185/areachairs"], "cdate": 1485287695367}}}, {"tddate": null, "tmdate": 1484888586413, "tcdate": 1484888586413, "number": 9, "id": "S1MMcMkPl", "invitation": "ICLR.cc/2017/conference/-/paper185/official/comment", "forum": "B1YfAfcgl", "replyto": "BkkrNQv8g", "signatures": ["ICLR.cc/2017/conference/paper185/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper185/AnonReviewer1"], "content": {"title": "Rating update to 7.", "comment": "Acknowledging the author responses to improve the paper w.r.t the reviewer comments, I changed the rating to 7."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Entropy-SGD: Biasing Gradient Descent Into Wide Valleys", "abstract": "This paper proposes a new optimization algorithm called Entropy-SGD for training deep neural networks that is motivated by the local geometry of the energy landscape. Local extrema with low generalization error have a large proportion of almost-zero eigenvalues in the Hessian with very few positive or negative eigenvalues. We leverage upon this observation to construct a local-entropy-based objective function that favors well-generalizable solutions lying in large flat regions of the energy landscape, while avoiding poorly-generalizable solutions located in the sharp valleys. Conceptually, our algorithm resembles two nested loops of SGD where we use Langevin dynamics in the inner loop to compute the gradient of the local entropy before each update of the weights. We show that the new objective has a smoother energy landscape and show improved generalization over SGD using uniform stability, under certain assumptions. Our experiments on convolutional and recurrent neural networks demonstrate that Entropy-SGD compares favorably to state-of-the-art techniques in terms of generalization error and training time.", "pdf": "/pdf/2fb588bb07ffd17030e097202cf514bdfb5bcbca.pdf", "TL;DR": "This paper focuses on developing new optimization tools for deep learning that are tailored to exploit the local geometric properties of the objective function.", "paperhash": "chaudhari|entropysgd_biasing_gradient_descent_into_wide_valleys", "keywords": ["Deep learning", "Optimization"], "conflicts": ["ucla.edu", "cs.ucla.edu", "nyu.edu", "microsoft.com", "gwu.edu", "nyu.edu", "cims.nyu.edu", "columbia.edu", "facebook.com", "microsoft.com", "polito.it"], "authors": ["Pratik Chaudhari", "Anna Choromanska", "Stefano Soatto", "Yann LeCun", "Carlo Baldassi", "Christian Borgs", "Jennifer Chayes", "Levent Sagun", "Riccardo Zecchina"], "authorids": ["pratikac@ucla.edu", "achoroma@cims.nyu.edu", "soatto@cs.ucla.edu", "yann@cs.nyu.edu", "carlo.baldassi@polito.it", "borgs@microsoft.com", "jchayes@microsoft.com", "sagun@cims.nyu.edu", "riccardo.zecchina@polito.it"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287695237, "id": "ICLR.cc/2017/conference/-/paper185/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "B1YfAfcgl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper185/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper185/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper185/reviewers", "ICLR.cc/2017/conference/paper185/areachairs"], "cdate": 1485287695237}}}, {"tddate": null, "tmdate": 1484888453214, "tcdate": 1482043658246, "number": 2, "id": "ByGf-37Nl", "invitation": "ICLR.cc/2017/conference/-/paper185/official/review", "forum": "B1YfAfcgl", "replyto": "B1YfAfcgl", "signatures": ["ICLR.cc/2017/conference/paper185/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper185/AnonReviewer1"], "content": {"title": "official review", "rating": "7: Good paper, accept", "review": "The paper introduces a new regularization term which encourages the optimizer \nto search for a flat local minimum of reasonably low loss instead of seeking a \nsharp region of a low loss. This is motivated by some empirical observations that\nlocal minima of good generalization performance tend to have flat shape. \nTo achieve this, a regularization term based on the free local energy is proposed\nand the gradient of this term, which do not have tractable closed-form solution, \nis obtained by performing Monte Carlo estimation using SGLD sampler. In the \nexperiments, the authors show some evidence of the flatness of good local \nminima, and also the performance of the proposed method in comparison to the\nAdam optimizer. \n\nThe paper is well and clearly written. I enjoyed reading the paper. The connection\nto the concept of free energy in optimization framework seems interesting. The \nmotivation of pursuing flatness is also well analyzed with a few experiments. I'm\nwondering if the first term in eqn. (8) is correct. I guess it should be f(x') not f(x)?\nAlso, I'm wondering why the authors did not add the experiment results on RNN in\nthe evaluation of the performance because char-lstm for text generation was \nalready used for the flatness experiments. I think adding more experiments on \nvarious models and applications of deep architectures (e.g., RNN, seq2seq, etc.) \nwill make the author's claim more persuasive. I also found the mixed usage of the\nterminology, e.g., free energy and free entropy, a bit confusing. \n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Entropy-SGD: Biasing Gradient Descent Into Wide Valleys", "abstract": "This paper proposes a new optimization algorithm called Entropy-SGD for training deep neural networks that is motivated by the local geometry of the energy landscape. Local extrema with low generalization error have a large proportion of almost-zero eigenvalues in the Hessian with very few positive or negative eigenvalues. We leverage upon this observation to construct a local-entropy-based objective function that favors well-generalizable solutions lying in large flat regions of the energy landscape, while avoiding poorly-generalizable solutions located in the sharp valleys. Conceptually, our algorithm resembles two nested loops of SGD where we use Langevin dynamics in the inner loop to compute the gradient of the local entropy before each update of the weights. We show that the new objective has a smoother energy landscape and show improved generalization over SGD using uniform stability, under certain assumptions. Our experiments on convolutional and recurrent neural networks demonstrate that Entropy-SGD compares favorably to state-of-the-art techniques in terms of generalization error and training time.", "pdf": "/pdf/2fb588bb07ffd17030e097202cf514bdfb5bcbca.pdf", "TL;DR": "This paper focuses on developing new optimization tools for deep learning that are tailored to exploit the local geometric properties of the objective function.", "paperhash": "chaudhari|entropysgd_biasing_gradient_descent_into_wide_valleys", "keywords": ["Deep learning", "Optimization"], "conflicts": ["ucla.edu", "cs.ucla.edu", "nyu.edu", "microsoft.com", "gwu.edu", "nyu.edu", "cims.nyu.edu", "columbia.edu", "facebook.com", "microsoft.com", "polito.it"], "authors": ["Pratik Chaudhari", "Anna Choromanska", "Stefano Soatto", "Yann LeCun", "Carlo Baldassi", "Christian Borgs", "Jennifer Chayes", "Levent Sagun", "Riccardo Zecchina"], "authorids": ["pratikac@ucla.edu", "achoroma@cims.nyu.edu", "soatto@cs.ucla.edu", "yann@cs.nyu.edu", "carlo.baldassi@polito.it", "borgs@microsoft.com", "jchayes@microsoft.com", "sagun@cims.nyu.edu", "riccardo.zecchina@polito.it"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1483384389082, "id": "ICLR.cc/2017/conference/-/paper185/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper185/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper185/AnonReviewer2", "ICLR.cc/2017/conference/paper185/AnonReviewer1", "ICLR.cc/2017/conference/paper185/AnonReviewer4"], "reply": {"forum": "B1YfAfcgl", "replyto": "B1YfAfcgl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper185/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper185/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1483384389082}}}, {"tddate": null, "tmdate": 1484862779822, "tcdate": 1484862779822, "number": 16, "id": "Hy4SB2CUg", "invitation": "ICLR.cc/2017/conference/-/paper185/public/comment", "forum": "B1YfAfcgl", "replyto": "HkmLkmwLx", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "short questions", "comment": "Congratulations to the authors for the high points on the updated PDF. I hope I am not very late in asking two short questions. Will you be opensourcing the code for the method & how does the method relate to the discussions in some other recent ICLR papers\n\n[1] Understanding deep learning requires rethinking generalization\n[2] Snapshot Ensembles: Train 1, Get M for Free\n[3] On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima\n\nThey are only tangentially related I know but I was curious how the motivation of the method fits into the generalization arguments of [1,3] and whether the ensembling trick of [2] can be used in conjunction with Entropy-SGD to reap benefits of both.\n\n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Entropy-SGD: Biasing Gradient Descent Into Wide Valleys", "abstract": "This paper proposes a new optimization algorithm called Entropy-SGD for training deep neural networks that is motivated by the local geometry of the energy landscape. Local extrema with low generalization error have a large proportion of almost-zero eigenvalues in the Hessian with very few positive or negative eigenvalues. We leverage upon this observation to construct a local-entropy-based objective function that favors well-generalizable solutions lying in large flat regions of the energy landscape, while avoiding poorly-generalizable solutions located in the sharp valleys. Conceptually, our algorithm resembles two nested loops of SGD where we use Langevin dynamics in the inner loop to compute the gradient of the local entropy before each update of the weights. We show that the new objective has a smoother energy landscape and show improved generalization over SGD using uniform stability, under certain assumptions. Our experiments on convolutional and recurrent neural networks demonstrate that Entropy-SGD compares favorably to state-of-the-art techniques in terms of generalization error and training time.", "pdf": "/pdf/2fb588bb07ffd17030e097202cf514bdfb5bcbca.pdf", "TL;DR": "This paper focuses on developing new optimization tools for deep learning that are tailored to exploit the local geometric properties of the objective function.", "paperhash": "chaudhari|entropysgd_biasing_gradient_descent_into_wide_valleys", "keywords": ["Deep learning", "Optimization"], "conflicts": ["ucla.edu", "cs.ucla.edu", "nyu.edu", "microsoft.com", "gwu.edu", "nyu.edu", "cims.nyu.edu", "columbia.edu", "facebook.com", "microsoft.com", "polito.it"], "authors": ["Pratik Chaudhari", "Anna Choromanska", "Stefano Soatto", "Yann LeCun", "Carlo Baldassi", "Christian Borgs", "Jennifer Chayes", "Levent Sagun", "Riccardo Zecchina"], "authorids": ["pratikac@ucla.edu", "achoroma@cims.nyu.edu", "soatto@cs.ucla.edu", "yann@cs.nyu.edu", "carlo.baldassi@polito.it", "borgs@microsoft.com", "jchayes@microsoft.com", "sagun@cims.nyu.edu", "riccardo.zecchina@polito.it"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287695367, "id": "ICLR.cc/2017/conference/-/paper185/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "B1YfAfcgl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper185/reviewers", "ICLR.cc/2017/conference/paper185/areachairs"], "cdate": 1485287695367}}}, {"tddate": null, "tmdate": 1484837968151, "tcdate": 1484837968151, "number": 8, "id": "ryuLVURUg", "invitation": "ICLR.cc/2017/conference/-/paper185/official/comment", "forum": "B1YfAfcgl", "replyto": "HkmLkmwLx", "signatures": ["ICLR.cc/2017/conference/paper185/AnonReviewer4"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper185/AnonReviewer4"], "content": {"title": "Authors have addressed concerns in review through major revisions, and added a useful improvement to algorithm", "comment": "The major revisions to the paper have answered each of the concerns I raise in the first review. The authors have also improved the technique by introducing an enhancement to the algorithm through the reverse-annealing of the scoping parameter. Hence, the evidence presented for the practical value of the technique is much improved, so that the paper has a clearer pragmatic as well as theoretical value. The ties of this work to previous studies is also much improved. These changes motivate the update in my review status to a clear accept."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Entropy-SGD: Biasing Gradient Descent Into Wide Valleys", "abstract": "This paper proposes a new optimization algorithm called Entropy-SGD for training deep neural networks that is motivated by the local geometry of the energy landscape. Local extrema with low generalization error have a large proportion of almost-zero eigenvalues in the Hessian with very few positive or negative eigenvalues. We leverage upon this observation to construct a local-entropy-based objective function that favors well-generalizable solutions lying in large flat regions of the energy landscape, while avoiding poorly-generalizable solutions located in the sharp valleys. Conceptually, our algorithm resembles two nested loops of SGD where we use Langevin dynamics in the inner loop to compute the gradient of the local entropy before each update of the weights. We show that the new objective has a smoother energy landscape and show improved generalization over SGD using uniform stability, under certain assumptions. Our experiments on convolutional and recurrent neural networks demonstrate that Entropy-SGD compares favorably to state-of-the-art techniques in terms of generalization error and training time.", "pdf": "/pdf/2fb588bb07ffd17030e097202cf514bdfb5bcbca.pdf", "TL;DR": "This paper focuses on developing new optimization tools for deep learning that are tailored to exploit the local geometric properties of the objective function.", "paperhash": "chaudhari|entropysgd_biasing_gradient_descent_into_wide_valleys", "keywords": ["Deep learning", "Optimization"], "conflicts": ["ucla.edu", "cs.ucla.edu", "nyu.edu", "microsoft.com", "gwu.edu", "nyu.edu", "cims.nyu.edu", "columbia.edu", "facebook.com", "microsoft.com", "polito.it"], "authors": ["Pratik Chaudhari", "Anna Choromanska", "Stefano Soatto", "Yann LeCun", "Carlo Baldassi", "Christian Borgs", "Jennifer Chayes", "Levent Sagun", "Riccardo Zecchina"], "authorids": ["pratikac@ucla.edu", "achoroma@cims.nyu.edu", "soatto@cs.ucla.edu", "yann@cs.nyu.edu", "carlo.baldassi@polito.it", "borgs@microsoft.com", "jchayes@microsoft.com", "sagun@cims.nyu.edu", "riccardo.zecchina@polito.it"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287695237, "id": "ICLR.cc/2017/conference/-/paper185/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "B1YfAfcgl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper185/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper185/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper185/reviewers", "ICLR.cc/2017/conference/paper185/areachairs"], "cdate": 1485287695237}}}, {"tddate": null, "tmdate": 1484836764981, "tcdate": 1483384388350, "number": 3, "id": "SknB8QuHg", "invitation": "ICLR.cc/2017/conference/-/paper185/official/review", "forum": "B1YfAfcgl", "replyto": "B1YfAfcgl", "signatures": ["ICLR.cc/2017/conference/paper185/AnonReviewer4"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper185/AnonReviewer4"], "content": {"title": "Well-written paper explores promising direction for training generalizable deep neural network, but empirical results are too preliminary to support some arguments made", "rating": "8: Top 50% of accepted papers, clear accept", "review": "Overview: \n\nThis paper introduces a biasing term for SGD that, in theoretical results and a toy example, yields solutions with an approximately equal or lower generalization error. This comes at a computational cost of estimating the gradient of the biasing term for each iteration through stochastic gradient Langevin dynamics, approximating an MCMC sample of the log partition function of a modified Gibbs distribution. The cost is equivalent to adding an inner for-loop to the standard SGD algorithm for each minibatch.\n\nPros:\n- Reviews and distills many results and theorems from past 2 decades that suggest a promising way forward for increasing the generalizability of deep neural networks\n- Generally very well written and well presented results, with interesting discussion of eigenvalues of Hessian as a way to characterize \u201cflat\u201d minima\n- Promising mathematical arguments suggest that E-SGD has generalization error bounded below by SGD, motivating further research in the area\n\nCons / points suggested for a rebuttal:\n(1) One claim of the paper given in the abstract is \u201dexperiments on competitive baselines demonstrate that Entropy-SGD leads to improved generalization and has the potential to accelerate training.\u201c This does not appear to be supported by the current set of experiments. As the authors comment in the discussion section, \u201cIn our experiments, Entropy-SGD results in a comparable generalization error as SGD, but always has a lower cross-entropy loss.\u201d It's not clear to me how to reconcile those two claims.\n\n(2) Similarly, the claim of accelerated training is not convincingly supported in the present version of the paper. Vanilla SGD requires a single forward pass through all M minibatches during one epoch for a parameter update, but the new method, E-SGD requires, L*M forward passes during one epoch where L is the number of Langevin updates, which require a minibatch sample each. This could in fact mean that E-SGD has worse computational complexity to reach the same point. In a remark on p.9, the authors note that a single epoch is defined to be \u201cthe number of parameter updates required to run through the dataset once.\u201d It\u2019s not clear to me how this answers the objection to a factor of L additional computations required for the inner-loop SGLD iterations. SGLD appears to introduces a potentially costly tradeoff that must be carefully managed by a user of E-SGD.\n\n(3) As the previous two points suggest, the paper could use some attention to the magnitude of the claims. For example, the introduction reads \u201cActively biasing towards wide valleys aids generalization, in fact, we can optimize solely the free energy term to obtain similar generalization error as SGD on the original loss function.\u201c According the the values reported on pp.9-10, only on MNIST is the generalization error, using only the free energy term (the log partition function of the modified Gibbs distribution), equivalent to using only the SGD loss function. This corresponds to setting rho to 0 in equation (6). On CIFAR-10, rho = 0.01 is used.\n\n(4) Another contribution of this paper, the characterization of the optimization landscape in terms of the eigenvalues of the Hessian and low generalization error being associated with flat local extrema, is helpful and interesting. I found the plots clear and useful. As another reviewer has already pointed out, there are high-level similarities to \u201cFlat Minima\u201d by Hochreiter and Schmidhuber (1997). The authors have responded already by adding a paragraph that helpfully explores some differences with H&S 1997. However, the similarities should also be carefully identified and mentioned. H&S 1997 includes detailed theoretical analysis that could be helpful for future work in this area, and has independently discovered a similar approach to training generalizable networks.\n\n(5) It's not clear how the assumption about the eigenvalues that were made in section 4.4 / Appendix B affect the application of this result to real-world problems. What magnitude of c>0 needs to be chosen? Does this correspond to a measurable characteristic of the dataset? It's a little mysterious in the current version of the paper.\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Entropy-SGD: Biasing Gradient Descent Into Wide Valleys", "abstract": "This paper proposes a new optimization algorithm called Entropy-SGD for training deep neural networks that is motivated by the local geometry of the energy landscape. Local extrema with low generalization error have a large proportion of almost-zero eigenvalues in the Hessian with very few positive or negative eigenvalues. We leverage upon this observation to construct a local-entropy-based objective function that favors well-generalizable solutions lying in large flat regions of the energy landscape, while avoiding poorly-generalizable solutions located in the sharp valleys. Conceptually, our algorithm resembles two nested loops of SGD where we use Langevin dynamics in the inner loop to compute the gradient of the local entropy before each update of the weights. We show that the new objective has a smoother energy landscape and show improved generalization over SGD using uniform stability, under certain assumptions. Our experiments on convolutional and recurrent neural networks demonstrate that Entropy-SGD compares favorably to state-of-the-art techniques in terms of generalization error and training time.", "pdf": "/pdf/2fb588bb07ffd17030e097202cf514bdfb5bcbca.pdf", "TL;DR": "This paper focuses on developing new optimization tools for deep learning that are tailored to exploit the local geometric properties of the objective function.", "paperhash": "chaudhari|entropysgd_biasing_gradient_descent_into_wide_valleys", "keywords": ["Deep learning", "Optimization"], "conflicts": ["ucla.edu", "cs.ucla.edu", "nyu.edu", "microsoft.com", "gwu.edu", "nyu.edu", "cims.nyu.edu", "columbia.edu", "facebook.com", "microsoft.com", "polito.it"], "authors": ["Pratik Chaudhari", "Anna Choromanska", "Stefano Soatto", "Yann LeCun", "Carlo Baldassi", "Christian Borgs", "Jennifer Chayes", "Levent Sagun", "Riccardo Zecchina"], "authorids": ["pratikac@ucla.edu", "achoroma@cims.nyu.edu", "soatto@cs.ucla.edu", "yann@cs.nyu.edu", "carlo.baldassi@polito.it", "borgs@microsoft.com", "jchayes@microsoft.com", "sagun@cims.nyu.edu", "riccardo.zecchina@polito.it"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1483384389082, "id": "ICLR.cc/2017/conference/-/paper185/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper185/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper185/AnonReviewer2", "ICLR.cc/2017/conference/paper185/AnonReviewer1", "ICLR.cc/2017/conference/paper185/AnonReviewer4"], "reply": {"forum": "B1YfAfcgl", "replyto": "B1YfAfcgl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper185/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper185/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1483384389082}}}, {"tddate": null, "tmdate": 1484595450654, "tcdate": 1484595258036, "number": 7, "id": "r1GHlj9Ue", "invitation": "ICLR.cc/2017/conference/-/paper185/official/comment", "forum": "B1YfAfcgl", "replyto": "HkmLkmwLx", "signatures": ["ICLR.cc/2017/conference/paper185/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper185/AnonReviewer2"], "content": {"title": "Paper in great shape, upgrading my rating", "comment": "The draft has undergone a major revision and my comments are handled very well in the new version. I think now the draft is in great shape, both from formulation viewpoint (connection to principles in statistical physics) and practical impact (Entropy-SGD is shown to beat SGD and SGLD in terms of wall-clock time). As a result of these improvements, I am raising my rating for this paper by one level. \n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Entropy-SGD: Biasing Gradient Descent Into Wide Valleys", "abstract": "This paper proposes a new optimization algorithm called Entropy-SGD for training deep neural networks that is motivated by the local geometry of the energy landscape. Local extrema with low generalization error have a large proportion of almost-zero eigenvalues in the Hessian with very few positive or negative eigenvalues. We leverage upon this observation to construct a local-entropy-based objective function that favors well-generalizable solutions lying in large flat regions of the energy landscape, while avoiding poorly-generalizable solutions located in the sharp valleys. Conceptually, our algorithm resembles two nested loops of SGD where we use Langevin dynamics in the inner loop to compute the gradient of the local entropy before each update of the weights. We show that the new objective has a smoother energy landscape and show improved generalization over SGD using uniform stability, under certain assumptions. Our experiments on convolutional and recurrent neural networks demonstrate that Entropy-SGD compares favorably to state-of-the-art techniques in terms of generalization error and training time.", "pdf": "/pdf/2fb588bb07ffd17030e097202cf514bdfb5bcbca.pdf", "TL;DR": "This paper focuses on developing new optimization tools for deep learning that are tailored to exploit the local geometric properties of the objective function.", "paperhash": "chaudhari|entropysgd_biasing_gradient_descent_into_wide_valleys", "keywords": ["Deep learning", "Optimization"], "conflicts": ["ucla.edu", "cs.ucla.edu", "nyu.edu", "microsoft.com", "gwu.edu", "nyu.edu", "cims.nyu.edu", "columbia.edu", "facebook.com", "microsoft.com", "polito.it"], "authors": ["Pratik Chaudhari", "Anna Choromanska", "Stefano Soatto", "Yann LeCun", "Carlo Baldassi", "Christian Borgs", "Jennifer Chayes", "Levent Sagun", "Riccardo Zecchina"], "authorids": ["pratikac@ucla.edu", "achoroma@cims.nyu.edu", "soatto@cs.ucla.edu", "yann@cs.nyu.edu", "carlo.baldassi@polito.it", "borgs@microsoft.com", "jchayes@microsoft.com", "sagun@cims.nyu.edu", "riccardo.zecchina@polito.it"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287695237, "id": "ICLR.cc/2017/conference/-/paper185/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "B1YfAfcgl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper185/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper185/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper185/reviewers", "ICLR.cc/2017/conference/paper185/areachairs"], "cdate": 1485287695237}}}, {"tddate": null, "tmdate": 1484594918305, "tcdate": 1481915788402, "number": 1, "id": "r1Vca3Z4x", "invitation": "ICLR.cc/2017/conference/-/paper185/official/review", "forum": "B1YfAfcgl", "replyto": "B1YfAfcgl", "signatures": ["ICLR.cc/2017/conference/paper185/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper185/AnonReviewer2"], "content": {"title": "Insightful Work", "rating": "9: Top 15% of accepted papers, strong accept", "review": "This paper presents a principled approach to finding flat minima. The motivation to seek such minima is due to their better generalization ability. The idea is to add to the original loss function a new term that exploits both width and depth of the objective function. In fact, the regularization term can be interpreted as Gaussian convolution of the exponentiated loss. Therefore, the introduced regularization term is essentially Gaussian smoothed version of the exponentiated loss. The smoothing obviously tends to suppress sharp minima.\n\nOverall, developing such regularization term based on thermodynamics concepts is very interesting. I have a couple of concerns that the authors may want to clarify in the rebuttal.\n\n1. When reporting the generalization performance, the experiments report the number of epochs; showing the proposed algorithm reaches better generalization in fewer epochs than plain SGD. Is this the number of epochs it takes by line 7 of your algorithm, or it is the total number of epochs (line 3 and 7 all combined)? If the former, it is not a fair comparison. If you multiply the number of epochs of SGD (line 7) by the number iterations it takes to approximate Langevin dynamics, it seems you obtain little gain against plain SGD.\n\n2. The proposed algorithm approximates the smoothed \"exponentiated\" loss (by smoothing I refer to convolution with the Gaussian). I am wondering how it compares against simpler idea of smoothing the original loss (dropping exponentiation)? Is the difference only in the motivation (e.g. thermodynamics interpretation) or it is deeper, e.g. the proposed scheme lends itself to more accurate approximation and/or achieves better generalization bound (in terms of the attained smoothness)? Smoothing the cost function without exponentiation allows simpler approximation (Monte Carlo integration instead of MCMC), e.g. see section 5.3 of https://arxiv.org/pdf/1601.04114\n\n3. Section 4.4. Thank you for revising the statements related to the eigenvalues of the Hessian. However, even in the revised version, there seems to be some discrepancy. You \"assume no eigenvalue of the Hessian lies in the set [\u22122\u03b3 \u2212c, c] for some small c > 0\". This essentially says the eigenvalues are far from zero. Such assumption seems to be in the opposite direction of the reality: the plots of eigenvalues (Figure 1) show most eigenvalues are indeed close to zero.\n\n4. Theorem 3 from Hardt 2015: The way you quote it differs from the original paper. Are you referring to the Theorem 3.12 of Hardt's paper? If so, why the difference, including elimination of dependency on constant c in the exponent of T?  \n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Entropy-SGD: Biasing Gradient Descent Into Wide Valleys", "abstract": "This paper proposes a new optimization algorithm called Entropy-SGD for training deep neural networks that is motivated by the local geometry of the energy landscape. Local extrema with low generalization error have a large proportion of almost-zero eigenvalues in the Hessian with very few positive or negative eigenvalues. We leverage upon this observation to construct a local-entropy-based objective function that favors well-generalizable solutions lying in large flat regions of the energy landscape, while avoiding poorly-generalizable solutions located in the sharp valleys. Conceptually, our algorithm resembles two nested loops of SGD where we use Langevin dynamics in the inner loop to compute the gradient of the local entropy before each update of the weights. We show that the new objective has a smoother energy landscape and show improved generalization over SGD using uniform stability, under certain assumptions. Our experiments on convolutional and recurrent neural networks demonstrate that Entropy-SGD compares favorably to state-of-the-art techniques in terms of generalization error and training time.", "pdf": "/pdf/2fb588bb07ffd17030e097202cf514bdfb5bcbca.pdf", "TL;DR": "This paper focuses on developing new optimization tools for deep learning that are tailored to exploit the local geometric properties of the objective function.", "paperhash": "chaudhari|entropysgd_biasing_gradient_descent_into_wide_valleys", "keywords": ["Deep learning", "Optimization"], "conflicts": ["ucla.edu", "cs.ucla.edu", "nyu.edu", "microsoft.com", "gwu.edu", "nyu.edu", "cims.nyu.edu", "columbia.edu", "facebook.com", "microsoft.com", "polito.it"], "authors": ["Pratik Chaudhari", "Anna Choromanska", "Stefano Soatto", "Yann LeCun", "Carlo Baldassi", "Christian Borgs", "Jennifer Chayes", "Levent Sagun", "Riccardo Zecchina"], "authorids": ["pratikac@ucla.edu", "achoroma@cims.nyu.edu", "soatto@cs.ucla.edu", "yann@cs.nyu.edu", "carlo.baldassi@polito.it", "borgs@microsoft.com", "jchayes@microsoft.com", "sagun@cims.nyu.edu", "riccardo.zecchina@polito.it"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1483384389082, "id": "ICLR.cc/2017/conference/-/paper185/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper185/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper185/AnonReviewer2", "ICLR.cc/2017/conference/paper185/AnonReviewer1", "ICLR.cc/2017/conference/paper185/AnonReviewer4"], "reply": {"forum": "B1YfAfcgl", "replyto": "B1YfAfcgl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper185/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper185/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1483384389082}}}, {"tddate": null, "tmdate": 1484367177008, "tcdate": 1484367177008, "number": 15, "id": "SJ-8BQDUe", "invitation": "ICLR.cc/2017/conference/-/paper185/public/comment", "forum": "B1YfAfcgl", "replyto": "ry03FJ2Sx", "signatures": ["~Pratik_A_Chaudhari1"], "readers": ["everyone"], "writers": ["~Pratik_A_Chaudhari1"], "content": {"title": "Re: Exact HVP is easy to compute", "comment": "The comment regarding [P94] above discusses this. Computing one exact HVP requires averaging over the entire dataset, i.e., ~4x the complexity of one epoch."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Entropy-SGD: Biasing Gradient Descent Into Wide Valleys", "abstract": "This paper proposes a new optimization algorithm called Entropy-SGD for training deep neural networks that is motivated by the local geometry of the energy landscape. Local extrema with low generalization error have a large proportion of almost-zero eigenvalues in the Hessian with very few positive or negative eigenvalues. We leverage upon this observation to construct a local-entropy-based objective function that favors well-generalizable solutions lying in large flat regions of the energy landscape, while avoiding poorly-generalizable solutions located in the sharp valleys. Conceptually, our algorithm resembles two nested loops of SGD where we use Langevin dynamics in the inner loop to compute the gradient of the local entropy before each update of the weights. We show that the new objective has a smoother energy landscape and show improved generalization over SGD using uniform stability, under certain assumptions. Our experiments on convolutional and recurrent neural networks demonstrate that Entropy-SGD compares favorably to state-of-the-art techniques in terms of generalization error and training time.", "pdf": "/pdf/2fb588bb07ffd17030e097202cf514bdfb5bcbca.pdf", "TL;DR": "This paper focuses on developing new optimization tools for deep learning that are tailored to exploit the local geometric properties of the objective function.", "paperhash": "chaudhari|entropysgd_biasing_gradient_descent_into_wide_valleys", "keywords": ["Deep learning", "Optimization"], "conflicts": ["ucla.edu", "cs.ucla.edu", "nyu.edu", "microsoft.com", "gwu.edu", "nyu.edu", "cims.nyu.edu", "columbia.edu", "facebook.com", "microsoft.com", "polito.it"], "authors": ["Pratik Chaudhari", "Anna Choromanska", "Stefano Soatto", "Yann LeCun", "Carlo Baldassi", "Christian Borgs", "Jennifer Chayes", "Levent Sagun", "Riccardo Zecchina"], "authorids": ["pratikac@ucla.edu", "achoroma@cims.nyu.edu", "soatto@cs.ucla.edu", "yann@cs.nyu.edu", "carlo.baldassi@polito.it", "borgs@microsoft.com", "jchayes@microsoft.com", "sagun@cims.nyu.edu", "riccardo.zecchina@polito.it"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287695367, "id": "ICLR.cc/2017/conference/-/paper185/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "B1YfAfcgl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper185/reviewers", "ICLR.cc/2017/conference/paper185/areachairs"], "cdate": 1485287695367}}}, {"tddate": null, "tmdate": 1484367103341, "tcdate": 1484367103341, "number": 14, "id": "r1v-S7vLl", "invitation": "ICLR.cc/2017/conference/-/paper185/public/comment", "forum": "B1YfAfcgl", "replyto": "BJwxn1nre", "signatures": ["~Pratik_A_Chaudhari1"], "readers": ["everyone"], "writers": ["~Pratik_A_Chaudhari1"], "content": {"title": "Re: You should make these connections clearer and more explicit", "comment": "We have updated the paper to include a detailed discussion about SVI and added experiments comparing with SGLD in Appendix C (also see the discussion in the last paragraph of Sec. 2). As already remarked in our previous comment, characterizing local entropy as \"SVI with a moving prior\" would be inaccurate as their relation is just conceptual and not rigorous.\n\n>> change x-axis for error curves\nWe have updated the experiments in the paper and now plot all error curves against the \"effective number of epochs\", i.e., the number of epochs for Entropy-SGD is multiplied by L (we set L=1 for SGD/Adam). Note that this is proportional to the number of back-props as suggested.\n\nPlease see the \"updates to the paper\" comment above for more details."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Entropy-SGD: Biasing Gradient Descent Into Wide Valleys", "abstract": "This paper proposes a new optimization algorithm called Entropy-SGD for training deep neural networks that is motivated by the local geometry of the energy landscape. Local extrema with low generalization error have a large proportion of almost-zero eigenvalues in the Hessian with very few positive or negative eigenvalues. We leverage upon this observation to construct a local-entropy-based objective function that favors well-generalizable solutions lying in large flat regions of the energy landscape, while avoiding poorly-generalizable solutions located in the sharp valleys. Conceptually, our algorithm resembles two nested loops of SGD where we use Langevin dynamics in the inner loop to compute the gradient of the local entropy before each update of the weights. We show that the new objective has a smoother energy landscape and show improved generalization over SGD using uniform stability, under certain assumptions. Our experiments on convolutional and recurrent neural networks demonstrate that Entropy-SGD compares favorably to state-of-the-art techniques in terms of generalization error and training time.", "pdf": "/pdf/2fb588bb07ffd17030e097202cf514bdfb5bcbca.pdf", "TL;DR": "This paper focuses on developing new optimization tools for deep learning that are tailored to exploit the local geometric properties of the objective function.", "paperhash": "chaudhari|entropysgd_biasing_gradient_descent_into_wide_valleys", "keywords": ["Deep learning", "Optimization"], "conflicts": ["ucla.edu", "cs.ucla.edu", "nyu.edu", "microsoft.com", "gwu.edu", "nyu.edu", "cims.nyu.edu", "columbia.edu", "facebook.com", "microsoft.com", "polito.it"], "authors": ["Pratik Chaudhari", "Anna Choromanska", "Stefano Soatto", "Yann LeCun", "Carlo Baldassi", "Christian Borgs", "Jennifer Chayes", "Levent Sagun", "Riccardo Zecchina"], "authorids": ["pratikac@ucla.edu", "achoroma@cims.nyu.edu", "soatto@cs.ucla.edu", "yann@cs.nyu.edu", "carlo.baldassi@polito.it", "borgs@microsoft.com", "jchayes@microsoft.com", "sagun@cims.nyu.edu", "riccardo.zecchina@polito.it"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287695367, "id": "ICLR.cc/2017/conference/-/paper185/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "B1YfAfcgl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper185/reviewers", "ICLR.cc/2017/conference/paper185/areachairs"], "cdate": 1485287695367}}}, {"tddate": null, "tmdate": 1484366953060, "tcdate": 1484366953060, "number": 13, "id": "Hk-ONXvIe", "invitation": "ICLR.cc/2017/conference/-/paper185/public/comment", "forum": "B1YfAfcgl", "replyto": "r1Vca3Z4x", "signatures": ["~Pratik_A_Chaudhari1"], "readers": ["everyone"], "writers": ["~Pratik_A_Chaudhari1"], "content": {"title": "Re: Insightful Work", "comment": ">> Thm. 3 does not have the constant c in Hardt et. al. '15\nWe set the constant c of Hardt et. al., to 1 in our statement to avoid confusion with our constant, with is also called c (our statement has the learning rate \\eta_t \\leq 1/t instead of their original \\eta_t \\leq c/t). This does not change the result qualitatively.\n\nPlease see the \"updates to the paper\" comment above for the response to the questions regarding (i) the eigenvalue assumption, (ii) thorough experimental results and, (iii) smoothing the original loss vs. local entropy."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Entropy-SGD: Biasing Gradient Descent Into Wide Valleys", "abstract": "This paper proposes a new optimization algorithm called Entropy-SGD for training deep neural networks that is motivated by the local geometry of the energy landscape. Local extrema with low generalization error have a large proportion of almost-zero eigenvalues in the Hessian with very few positive or negative eigenvalues. We leverage upon this observation to construct a local-entropy-based objective function that favors well-generalizable solutions lying in large flat regions of the energy landscape, while avoiding poorly-generalizable solutions located in the sharp valleys. Conceptually, our algorithm resembles two nested loops of SGD where we use Langevin dynamics in the inner loop to compute the gradient of the local entropy before each update of the weights. We show that the new objective has a smoother energy landscape and show improved generalization over SGD using uniform stability, under certain assumptions. Our experiments on convolutional and recurrent neural networks demonstrate that Entropy-SGD compares favorably to state-of-the-art techniques in terms of generalization error and training time.", "pdf": "/pdf/2fb588bb07ffd17030e097202cf514bdfb5bcbca.pdf", "TL;DR": "This paper focuses on developing new optimization tools for deep learning that are tailored to exploit the local geometric properties of the objective function.", "paperhash": "chaudhari|entropysgd_biasing_gradient_descent_into_wide_valleys", "keywords": ["Deep learning", "Optimization"], "conflicts": ["ucla.edu", "cs.ucla.edu", "nyu.edu", "microsoft.com", "gwu.edu", "nyu.edu", "cims.nyu.edu", "columbia.edu", "facebook.com", "microsoft.com", "polito.it"], "authors": ["Pratik Chaudhari", "Anna Choromanska", "Stefano Soatto", "Yann LeCun", "Carlo Baldassi", "Christian Borgs", "Jennifer Chayes", "Levent Sagun", "Riccardo Zecchina"], "authorids": ["pratikac@ucla.edu", "achoroma@cims.nyu.edu", "soatto@cs.ucla.edu", "yann@cs.nyu.edu", "carlo.baldassi@polito.it", "borgs@microsoft.com", "jchayes@microsoft.com", "sagun@cims.nyu.edu", "riccardo.zecchina@polito.it"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287695367, "id": "ICLR.cc/2017/conference/-/paper185/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "B1YfAfcgl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper185/reviewers", "ICLR.cc/2017/conference/paper185/areachairs"], "cdate": 1485287695367}}}, {"tddate": null, "tmdate": 1484366902978, "tcdate": 1484366902978, "number": 12, "id": "BkkrNQv8g", "invitation": "ICLR.cc/2017/conference/-/paper185/public/comment", "forum": "B1YfAfcgl", "replyto": "ByGf-37Nl", "signatures": ["~Pratik_A_Chaudhari1"], "readers": ["everyone"], "writers": ["~Pratik_A_Chaudhari1"], "content": {"title": "Re: official review", "comment": ">> Eqn. 8 should have f(x')\nThanks, it is fixed now.\n\n>> experiments on RNNs\nThanks for suggesting this. We have included results for word-level and character-level text prediction on PTB and char-LSTM with War and Peace. We not only obtain a better test perplexity than a competitive baseline with SGD, but also train in about half as much wall-clock time.\n\nPlease see the \"updates to the paper\" comment above for more details."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Entropy-SGD: Biasing Gradient Descent Into Wide Valleys", "abstract": "This paper proposes a new optimization algorithm called Entropy-SGD for training deep neural networks that is motivated by the local geometry of the energy landscape. Local extrema with low generalization error have a large proportion of almost-zero eigenvalues in the Hessian with very few positive or negative eigenvalues. We leverage upon this observation to construct a local-entropy-based objective function that favors well-generalizable solutions lying in large flat regions of the energy landscape, while avoiding poorly-generalizable solutions located in the sharp valleys. Conceptually, our algorithm resembles two nested loops of SGD where we use Langevin dynamics in the inner loop to compute the gradient of the local entropy before each update of the weights. We show that the new objective has a smoother energy landscape and show improved generalization over SGD using uniform stability, under certain assumptions. Our experiments on convolutional and recurrent neural networks demonstrate that Entropy-SGD compares favorably to state-of-the-art techniques in terms of generalization error and training time.", "pdf": "/pdf/2fb588bb07ffd17030e097202cf514bdfb5bcbca.pdf", "TL;DR": "This paper focuses on developing new optimization tools for deep learning that are tailored to exploit the local geometric properties of the objective function.", "paperhash": "chaudhari|entropysgd_biasing_gradient_descent_into_wide_valleys", "keywords": ["Deep learning", "Optimization"], "conflicts": ["ucla.edu", "cs.ucla.edu", "nyu.edu", "microsoft.com", "gwu.edu", "nyu.edu", "cims.nyu.edu", "columbia.edu", "facebook.com", "microsoft.com", "polito.it"], "authors": ["Pratik Chaudhari", "Anna Choromanska", "Stefano Soatto", "Yann LeCun", "Carlo Baldassi", "Christian Borgs", "Jennifer Chayes", "Levent Sagun", "Riccardo Zecchina"], "authorids": ["pratikac@ucla.edu", "achoroma@cims.nyu.edu", "soatto@cs.ucla.edu", "yann@cs.nyu.edu", "carlo.baldassi@polito.it", "borgs@microsoft.com", "jchayes@microsoft.com", "sagun@cims.nyu.edu", "riccardo.zecchina@polito.it"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287695367, "id": "ICLR.cc/2017/conference/-/paper185/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "B1YfAfcgl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper185/reviewers", "ICLR.cc/2017/conference/paper185/areachairs"], "cdate": 1485287695367}}}, {"tddate": null, "tmdate": 1484366834109, "tcdate": 1484366834109, "number": 11, "id": "By9xNQvUl", "invitation": "ICLR.cc/2017/conference/-/paper185/public/comment", "forum": "B1YfAfcgl", "replyto": "B1p44ASBe", "signatures": ["~Pratik_A_Chaudhari1"], "readers": ["everyone"], "writers": ["~Pratik_A_Chaudhari1"], "content": {"title": "Re: high level idea may be good, details are not entirely convincing", "comment": "Please see the \"updates to the paper\" comment above for the response to the questions regarding (i) the eigenvalue assumption, (ii) thorough experimental results and, (iii) smoothing the loss vs. local entropy.\n\n>> discussion of Baldassi et. al. '16 in related work\nWe have expanded the discussion of local entropy previously introduced by Baldassi et. al. in Sec. 2 and included due citations in Sec. 1 and 3 as well. This paper builds upon their work and generalizes it to models with continuous variables, deep networks in particular. After posting the first version of this paper, we have been collaborating with Baldassi et. al. for experiments on scoping techniques and they have co-authored this version of the paper."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Entropy-SGD: Biasing Gradient Descent Into Wide Valleys", "abstract": "This paper proposes a new optimization algorithm called Entropy-SGD for training deep neural networks that is motivated by the local geometry of the energy landscape. Local extrema with low generalization error have a large proportion of almost-zero eigenvalues in the Hessian with very few positive or negative eigenvalues. We leverage upon this observation to construct a local-entropy-based objective function that favors well-generalizable solutions lying in large flat regions of the energy landscape, while avoiding poorly-generalizable solutions located in the sharp valleys. Conceptually, our algorithm resembles two nested loops of SGD where we use Langevin dynamics in the inner loop to compute the gradient of the local entropy before each update of the weights. We show that the new objective has a smoother energy landscape and show improved generalization over SGD using uniform stability, under certain assumptions. Our experiments on convolutional and recurrent neural networks demonstrate that Entropy-SGD compares favorably to state-of-the-art techniques in terms of generalization error and training time.", "pdf": "/pdf/2fb588bb07ffd17030e097202cf514bdfb5bcbca.pdf", "TL;DR": "This paper focuses on developing new optimization tools for deep learning that are tailored to exploit the local geometric properties of the objective function.", "paperhash": "chaudhari|entropysgd_biasing_gradient_descent_into_wide_valleys", "keywords": ["Deep learning", "Optimization"], "conflicts": ["ucla.edu", "cs.ucla.edu", "nyu.edu", "microsoft.com", "gwu.edu", "nyu.edu", "cims.nyu.edu", "columbia.edu", "facebook.com", "microsoft.com", "polito.it"], "authors": ["Pratik Chaudhari", "Anna Choromanska", "Stefano Soatto", "Yann LeCun", "Carlo Baldassi", "Christian Borgs", "Jennifer Chayes", "Levent Sagun", "Riccardo Zecchina"], "authorids": ["pratikac@ucla.edu", "achoroma@cims.nyu.edu", "soatto@cs.ucla.edu", "yann@cs.nyu.edu", "carlo.baldassi@polito.it", "borgs@microsoft.com", "jchayes@microsoft.com", "sagun@cims.nyu.edu", "riccardo.zecchina@polito.it"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287695367, "id": "ICLR.cc/2017/conference/-/paper185/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "B1YfAfcgl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper185/reviewers", "ICLR.cc/2017/conference/paper185/areachairs"], "cdate": 1485287695367}}}, {"tddate": null, "tmdate": 1484366696718, "tcdate": 1484366552687, "number": 10, "id": "Hyb1mXPLe", "invitation": "ICLR.cc/2017/conference/-/paper185/public/comment", "forum": "B1YfAfcgl", "replyto": "SknB8QuHg", "signatures": ["~Pratik_A_Chaudhari1"], "readers": ["everyone"], "writers": ["~Pratik_A_Chaudhari1"], "content": {"title": "Re: empirical results are too preliminary to support some arguments made", "comment": "Please see the \"updates to the paper\" comment above for the response to the questions regarding (i) the eigenvalue assumption and, (ii) thorough experimental results.\n\n>> how to reconcile \"Entropy-SGD obtains better generalization but results in lower cross-entropy loss\"\nThe confusion is probably caused by our omission, it should read \"... lower cross-entropy loss on the training set\" (fixed now). This experimental observation suggests that there exist wide valleys that are deeper in the energy landscape that also generalize well; Entropy-SGD manages to find them while SGD gets stuck at a higher loss.\n\n>> \\rho = 0.01 on CIFAR-10 but \\rho = 0 on MNIST\nWe have modified the algorithm to only train with the local entropy objective, i.e., \\rho = 0 always (cf. Eqn. 6). Using scoping we can obtain better results on all our networks both in terms of generalization error and wall-clock time.\n\n>> discuss similarities to Hochreiter and Schmidhuber '97 [HS97]\nWe have expanded our discussion of [HS97] in Sec. 2 to include this discussion: While the motivations are exactly the same, similarities with their exact formulation are only conceptual in nature, e.g., in a flat minimum, the local entropy is a direct measure of the width of the valley which is similar to their usage of Hessian. The Gibbs variant to averaging in weight space in their analysis (Eqn. 33, pg. 22 of [HS97]) is similar to the averaging in Eqn. 7 of our paper. \n\nWe agree with the reviewer that the elaborate analysis of generalization of [HS97] using the Gibbs formalism is a promising direction. In our case, we benefit from similar elaborate and technical results introduced for uniform stability [BE02].\n\n>> experiments are on a toy example\nPlease note that our results on CIFAR-10 are without any data augmentation, the best result for this is 6.55% error using ELU units by [CUH15]. To our knowledge, our baseline on CIFAR-10 (7.71 \\pm 0.19% error with SGD), is the best reported result for the popular All-CNN-C network [S14], which is a medium-sized model with about 1.6 million weights. The largest model we have experimented with is an LSTM on the PTB dataset with 66 million weights and we achieve better test perplexity than the original authors [ZSV14] in half as much wall-clock time.\n\n[HS97] Hochreiter, S. and Schmidhuber, J. (1997), Flat Minima.\n[BE02] Olivier Bousquet and Andre Elisseeff. Stability and generalization. JMLR, 2002.\n[CUH15] Clevert, Djork-Arn\u00e9, Thomas Unterthiner, and Sepp Hochreiter. \"Fast and accurate deep network learning by exponential linear units (elus).\" arXiv:1511.07289 (2015).\n[S14] Springenberg, Jost Tobias, et al. \"Striving for simplicity: The all convolutional net.\" arXiv:1412.6806 (2014).\n[ZSV14] Zaremba, Wojciech, Ilya Sutskever, and Oriol Vinyals. \"Recurrent neural network regularization.\" arXiv:1409.2329 (2014)."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Entropy-SGD: Biasing Gradient Descent Into Wide Valleys", "abstract": "This paper proposes a new optimization algorithm called Entropy-SGD for training deep neural networks that is motivated by the local geometry of the energy landscape. Local extrema with low generalization error have a large proportion of almost-zero eigenvalues in the Hessian with very few positive or negative eigenvalues. We leverage upon this observation to construct a local-entropy-based objective function that favors well-generalizable solutions lying in large flat regions of the energy landscape, while avoiding poorly-generalizable solutions located in the sharp valleys. Conceptually, our algorithm resembles two nested loops of SGD where we use Langevin dynamics in the inner loop to compute the gradient of the local entropy before each update of the weights. We show that the new objective has a smoother energy landscape and show improved generalization over SGD using uniform stability, under certain assumptions. Our experiments on convolutional and recurrent neural networks demonstrate that Entropy-SGD compares favorably to state-of-the-art techniques in terms of generalization error and training time.", "pdf": "/pdf/2fb588bb07ffd17030e097202cf514bdfb5bcbca.pdf", "TL;DR": "This paper focuses on developing new optimization tools for deep learning that are tailored to exploit the local geometric properties of the objective function.", "paperhash": "chaudhari|entropysgd_biasing_gradient_descent_into_wide_valleys", "keywords": ["Deep learning", "Optimization"], "conflicts": ["ucla.edu", "cs.ucla.edu", "nyu.edu", "microsoft.com", "gwu.edu", "nyu.edu", "cims.nyu.edu", "columbia.edu", "facebook.com", "microsoft.com", "polito.it"], "authors": ["Pratik Chaudhari", "Anna Choromanska", "Stefano Soatto", "Yann LeCun", "Carlo Baldassi", "Christian Borgs", "Jennifer Chayes", "Levent Sagun", "Riccardo Zecchina"], "authorids": ["pratikac@ucla.edu", "achoroma@cims.nyu.edu", "soatto@cs.ucla.edu", "yann@cs.nyu.edu", "carlo.baldassi@polito.it", "borgs@microsoft.com", "jchayes@microsoft.com", "sagun@cims.nyu.edu", "riccardo.zecchina@polito.it"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287695367, "id": "ICLR.cc/2017/conference/-/paper185/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "B1YfAfcgl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper185/reviewers", "ICLR.cc/2017/conference/paper185/areachairs"], "cdate": 1485287695367}}}, {"tddate": null, "tmdate": 1484365642894, "tcdate": 1484365642894, "number": 9, "id": "HkmLkmwLx", "invitation": "ICLR.cc/2017/conference/-/paper185/public/comment", "forum": "B1YfAfcgl", "replyto": "B1YfAfcgl", "signatures": ["~Pratik_A_Chaudhari1"], "readers": ["everyone"], "writers": ["~Pratik_A_Chaudhari1"], "content": {"title": "Updates to the paper", "comment": "We thank the reviewers and the area chair for their insightful feedback. We have incorporated all comments into our current draft. We first discuss the updates to the paper and address common questions raised by the reviewers. We have also posted individual comments to the reviewers to address specific questions.\n\nUpdates\n=====\n\na) We have updated the experimental section of the paper with new experiments on MNIST, CIFAR-10 and two datasets on RNNs (PTB and char-LSTM).\n\nb) We have modified the algorithm to introduce a technique called \"scoping\". This increases the scope parameter \\gamma as training progresses instead of fixing it and has the effect of exploring the parameter space in the beginning of training (Sec. 4.3). As a result of this, we can now train all our networks with only the local entropy loss instead of treating it as a regularizer (Eqn. 6).\n\nc) For a fair comparison of the training time, we now plot the error curves in Figs. 4, 5 and 6 against the \"effective number of epochs\", i.e., the number of epochs of Entropy-SGD is multiplied by the number of Langevin iterations L (we set L=1 for SGD/Adam). Thus the x-axis is a direct measure of the wall-clock time agnostic to the underlying implementation and is proportional to the number of back-props as suggested.\n\nWe obtain significant speed-ups with respect to our earlier results due to scoping and the wall-clock training time for all our networks with Entropy-SGD is now comparable to SGD/Adam. In fact, Entropy-SGD is almost twice as fast as SGD on our experiments on RNNs and also obtains a better generalization error (cf. Fig. 6). The acceleration for CNNs on MNIST and CIFAR-10 is about 20%.\n\nTable 1 (page 11) summarizes the experimental section of the paper.\n\nd) Improved exposition of the algorithm in Sec. 4.2 that includes intuition for hyper-parameter tuning. We have expanded the discussion of experiments in Sec. 5.3, 5.4 to provide more details and insights that relate to the energy landscape of deep networks.\n\ne) Appendix C discusses the possible connections to variational inference (this is the same material as the discussion with the AC below). Sec. C.1 presents an experimental comparison of local entropy vs. SGLD. We note here that our results using Entropy-SGD for both CNNs and RNNs are much better than vanilla SGLD in significantly smaller (~3-5x) wall-clock times.\n\nResponse to the reviewers:\n================\n\n>> smoothing of the original loss vs. local entropy\nWe discuss this in detail in the related work in Sec. 2 and Appendix C. While smoothing the original loss function using convolutions or averaging the gradient over perturbations of weights reduces the ruggedness of the energy landscape, it does not help with sharp, narrow valleys. Local entropy introduces a measure that focuses on wide local minima in the energy landscape (cf. Fig. 2 which has a \"global\" minimum at a wide valley); this is the primary reason for its efficacy. Smoothing the loss function can also, for instance, generate an artificial local minimum between two close by sharp valleys, which is detrimental to generalization.\n\n>> unrealistic eigenvalue assumption in Sec. 4.3\nWe have clarified this point in Remark 4. Our analysis employs an assumption that the Hessian \\nabla^2 f(x) does not have eigenvalues in the set [-2\\gamma-c, c] for some c > 0. This is admittedly unrealistic, for instance, the eigenspectrum of the Hessian in Fig. 1 has a large fraction of its eigenvalues almost zero. Let us note though that Fig. 1 is plotted at a local minimum, from our experiments, the eigenspectrum is less sparse in the beginning of training.\n\nWe would like to remark that the bound on uniform stability in Thm. 3 by Hardt et al. assumes global conditions on the smoothness of the loss function; one imagines that Eqn. 9 remains qualitatively the same (in particular, with respect to the number of training iterations) even if this assumption is violated to an extent before convergence happens. Obtaining a rigorous generalization bound without this assumption requires a dynamical analysis of SGD and seems out of reach currently."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Entropy-SGD: Biasing Gradient Descent Into Wide Valleys", "abstract": "This paper proposes a new optimization algorithm called Entropy-SGD for training deep neural networks that is motivated by the local geometry of the energy landscape. Local extrema with low generalization error have a large proportion of almost-zero eigenvalues in the Hessian with very few positive or negative eigenvalues. We leverage upon this observation to construct a local-entropy-based objective function that favors well-generalizable solutions lying in large flat regions of the energy landscape, while avoiding poorly-generalizable solutions located in the sharp valleys. Conceptually, our algorithm resembles two nested loops of SGD where we use Langevin dynamics in the inner loop to compute the gradient of the local entropy before each update of the weights. We show that the new objective has a smoother energy landscape and show improved generalization over SGD using uniform stability, under certain assumptions. Our experiments on convolutional and recurrent neural networks demonstrate that Entropy-SGD compares favorably to state-of-the-art techniques in terms of generalization error and training time.", "pdf": "/pdf/2fb588bb07ffd17030e097202cf514bdfb5bcbca.pdf", "TL;DR": "This paper focuses on developing new optimization tools for deep learning that are tailored to exploit the local geometric properties of the objective function.", "paperhash": "chaudhari|entropysgd_biasing_gradient_descent_into_wide_valleys", "keywords": ["Deep learning", "Optimization"], "conflicts": ["ucla.edu", "cs.ucla.edu", "nyu.edu", "microsoft.com", "gwu.edu", "nyu.edu", "cims.nyu.edu", "columbia.edu", "facebook.com", "microsoft.com", "polito.it"], "authors": ["Pratik Chaudhari", "Anna Choromanska", "Stefano Soatto", "Yann LeCun", "Carlo Baldassi", "Christian Borgs", "Jennifer Chayes", "Levent Sagun", "Riccardo Zecchina"], "authorids": ["pratikac@ucla.edu", "achoroma@cims.nyu.edu", "soatto@cs.ucla.edu", "yann@cs.nyu.edu", "carlo.baldassi@polito.it", "borgs@microsoft.com", "jchayes@microsoft.com", "sagun@cims.nyu.edu", "riccardo.zecchina@polito.it"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287695367, "id": "ICLR.cc/2017/conference/-/paper185/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "B1YfAfcgl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper185/reviewers", "ICLR.cc/2017/conference/paper185/areachairs"], "cdate": 1485287695367}}}, {"tddate": null, "tmdate": 1483631598758, "tcdate": 1483631598758, "number": 6, "id": "BJwxn1nre", "invitation": "ICLR.cc/2017/conference/-/paper185/official/comment", "forum": "B1YfAfcgl", "replyto": "BJo0-8dHg", "signatures": ["ICLR.cc/2017/conference/paper185/areachair1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper185/areachair1"], "content": {"title": "You should make these connections clearer and more explicit", "comment": "Ok, great, you're not doing exactly the same thing as stochastic variational inference.  But the point is that you're proposing a method whose motivations are almost identical to those of Bayesian methods, and which looks very similar to SVI in practice, and which using Bayesian methods in the inner loop.\n\nSo it's kind of obnoxious to not make a clear and explicit comparison in the paper between your new method and the existing, standard methods which are extremely similar.  I think the physics notation obfuscates what's really going on.  I would suggest adding the above discussion to the paper.\n\nWould it be fair to say that your algorithm is SVI but with a moving prior?  Because that would be a nice connection to make, and it would inform the design of inference algorithms more generally.\n\nRegarding experiments, I would also strongly recommend that you use time or gradient evaluations on the x-axis.  As it stands it's not clear whether or not your new algorithm gets better performance in the same amount of time.  Also, I still think it's strange that you don't compare against SVI."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Entropy-SGD: Biasing Gradient Descent Into Wide Valleys", "abstract": "This paper proposes a new optimization algorithm called Entropy-SGD for training deep neural networks that is motivated by the local geometry of the energy landscape. Local extrema with low generalization error have a large proportion of almost-zero eigenvalues in the Hessian with very few positive or negative eigenvalues. We leverage upon this observation to construct a local-entropy-based objective function that favors well-generalizable solutions lying in large flat regions of the energy landscape, while avoiding poorly-generalizable solutions located in the sharp valleys. Conceptually, our algorithm resembles two nested loops of SGD where we use Langevin dynamics in the inner loop to compute the gradient of the local entropy before each update of the weights. We show that the new objective has a smoother energy landscape and show improved generalization over SGD using uniform stability, under certain assumptions. Our experiments on convolutional and recurrent neural networks demonstrate that Entropy-SGD compares favorably to state-of-the-art techniques in terms of generalization error and training time.", "pdf": "/pdf/2fb588bb07ffd17030e097202cf514bdfb5bcbca.pdf", "TL;DR": "This paper focuses on developing new optimization tools for deep learning that are tailored to exploit the local geometric properties of the objective function.", "paperhash": "chaudhari|entropysgd_biasing_gradient_descent_into_wide_valleys", "keywords": ["Deep learning", "Optimization"], "conflicts": ["ucla.edu", "cs.ucla.edu", "nyu.edu", "microsoft.com", "gwu.edu", "nyu.edu", "cims.nyu.edu", "columbia.edu", "facebook.com", "microsoft.com", "polito.it"], "authors": ["Pratik Chaudhari", "Anna Choromanska", "Stefano Soatto", "Yann LeCun", "Carlo Baldassi", "Christian Borgs", "Jennifer Chayes", "Levent Sagun", "Riccardo Zecchina"], "authorids": ["pratikac@ucla.edu", "achoroma@cims.nyu.edu", "soatto@cs.ucla.edu", "yann@cs.nyu.edu", "carlo.baldassi@polito.it", "borgs@microsoft.com", "jchayes@microsoft.com", "sagun@cims.nyu.edu", "riccardo.zecchina@polito.it"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287695237, "id": "ICLR.cc/2017/conference/-/paper185/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "B1YfAfcgl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper185/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper185/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper185/reviewers", "ICLR.cc/2017/conference/paper185/areachairs"], "cdate": 1485287695237}}}, {"tddate": null, "tmdate": 1483631030272, "tcdate": 1483631030272, "number": 5, "id": "ry03FJ2Sx", "invitation": "ICLR.cc/2017/conference/-/paper185/official/comment", "forum": "B1YfAfcgl", "replyto": "SkUoB8_rx", "signatures": ["ICLR.cc/2017/conference/paper185/areachair1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper185/areachair1"], "content": {"title": "Exact Hessian-vector product is easy to compute", "comment": "There's no need to use finite differences to compute the HVP - it can be computed exactly for 4 times the cost of the original function evaluation, using reverse-mode autodiff on a closure that computes the gradient of the original function times a vector.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Entropy-SGD: Biasing Gradient Descent Into Wide Valleys", "abstract": "This paper proposes a new optimization algorithm called Entropy-SGD for training deep neural networks that is motivated by the local geometry of the energy landscape. Local extrema with low generalization error have a large proportion of almost-zero eigenvalues in the Hessian with very few positive or negative eigenvalues. We leverage upon this observation to construct a local-entropy-based objective function that favors well-generalizable solutions lying in large flat regions of the energy landscape, while avoiding poorly-generalizable solutions located in the sharp valleys. Conceptually, our algorithm resembles two nested loops of SGD where we use Langevin dynamics in the inner loop to compute the gradient of the local entropy before each update of the weights. We show that the new objective has a smoother energy landscape and show improved generalization over SGD using uniform stability, under certain assumptions. Our experiments on convolutional and recurrent neural networks demonstrate that Entropy-SGD compares favorably to state-of-the-art techniques in terms of generalization error and training time.", "pdf": "/pdf/2fb588bb07ffd17030e097202cf514bdfb5bcbca.pdf", "TL;DR": "This paper focuses on developing new optimization tools for deep learning that are tailored to exploit the local geometric properties of the objective function.", "paperhash": "chaudhari|entropysgd_biasing_gradient_descent_into_wide_valleys", "keywords": ["Deep learning", "Optimization"], "conflicts": ["ucla.edu", "cs.ucla.edu", "nyu.edu", "microsoft.com", "gwu.edu", "nyu.edu", "cims.nyu.edu", "columbia.edu", "facebook.com", "microsoft.com", "polito.it"], "authors": ["Pratik Chaudhari", "Anna Choromanska", "Stefano Soatto", "Yann LeCun", "Carlo Baldassi", "Christian Borgs", "Jennifer Chayes", "Levent Sagun", "Riccardo Zecchina"], "authorids": ["pratikac@ucla.edu", "achoroma@cims.nyu.edu", "soatto@cs.ucla.edu", "yann@cs.nyu.edu", "carlo.baldassi@polito.it", "borgs@microsoft.com", "jchayes@microsoft.com", "sagun@cims.nyu.edu", "riccardo.zecchina@polito.it"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287695237, "id": "ICLR.cc/2017/conference/-/paper185/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "B1YfAfcgl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper185/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper185/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper185/reviewers", "ICLR.cc/2017/conference/paper185/areachairs"], "cdate": 1485287695237}}}, {"tddate": null, "tmdate": 1483398204462, "tcdate": 1483396509938, "number": 8, "id": "SkUoB8_rx", "invitation": "ICLR.cc/2017/conference/-/paper185/public/comment", "forum": "B1YfAfcgl", "replyto": "rkQD9K-Hg", "signatures": ["~Pratik_A_Chaudhari1"], "readers": ["everyone"], "writers": ["~Pratik_A_Chaudhari1"], "content": {"title": "Re: Is the Hessian-vector product actually expensive?", "comment": "Comment 3) above is for HVP of the loss function of a general deep network, not local entropy. Indeed, HVPs can be computed using forward differencing with two back-props. Such an approximation is susceptible to numerical errors --- especially in high dimensions [P94, M10]. One typically averages the HVP over many samples in the dataset which is expensive, for instance, the authors in [LSP93] average over a few hundred samples, which roughly translates to 5-10x the time required for one iteration of vanilla SGD. More accurate algorithms like that of [P94] also require this averaging over samples.\n\nOne could argue that accuracy in the approximation of HVP does not matter in practice for purposes of training; what matters is only the local curvature at a scale commensurate with the typical weight updates. If so, indeed, approximate computation of HVP can be considered cheap. The perturbation vector for computing HVP using back-props however needs to be chosen carefully.\n\n[LSP93] LeCun, Yann, Patrice Y. Simard, and Barak Pearlmutter. \"Automatic learning rate maximization by on-line estimation of the Hessian\u2019s eigenvectors.\" NIPS (1993).\n[P94] Pearlmutter, Barak A. \"Fast exact multiplication by the Hessian.\" Neural computation 6.1 (1994): 147-160.\n[M10] Martens, James. \"Deep learning via Hessian-free optimization.\" ICML (2010)."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Entropy-SGD: Biasing Gradient Descent Into Wide Valleys", "abstract": "This paper proposes a new optimization algorithm called Entropy-SGD for training deep neural networks that is motivated by the local geometry of the energy landscape. Local extrema with low generalization error have a large proportion of almost-zero eigenvalues in the Hessian with very few positive or negative eigenvalues. We leverage upon this observation to construct a local-entropy-based objective function that favors well-generalizable solutions lying in large flat regions of the energy landscape, while avoiding poorly-generalizable solutions located in the sharp valleys. Conceptually, our algorithm resembles two nested loops of SGD where we use Langevin dynamics in the inner loop to compute the gradient of the local entropy before each update of the weights. We show that the new objective has a smoother energy landscape and show improved generalization over SGD using uniform stability, under certain assumptions. Our experiments on convolutional and recurrent neural networks demonstrate that Entropy-SGD compares favorably to state-of-the-art techniques in terms of generalization error and training time.", "pdf": "/pdf/2fb588bb07ffd17030e097202cf514bdfb5bcbca.pdf", "TL;DR": "This paper focuses on developing new optimization tools for deep learning that are tailored to exploit the local geometric properties of the objective function.", "paperhash": "chaudhari|entropysgd_biasing_gradient_descent_into_wide_valleys", "keywords": ["Deep learning", "Optimization"], "conflicts": ["ucla.edu", "cs.ucla.edu", "nyu.edu", "microsoft.com", "gwu.edu", "nyu.edu", "cims.nyu.edu", "columbia.edu", "facebook.com", "microsoft.com", "polito.it"], "authors": ["Pratik Chaudhari", "Anna Choromanska", "Stefano Soatto", "Yann LeCun", "Carlo Baldassi", "Christian Borgs", "Jennifer Chayes", "Levent Sagun", "Riccardo Zecchina"], "authorids": ["pratikac@ucla.edu", "achoroma@cims.nyu.edu", "soatto@cs.ucla.edu", "yann@cs.nyu.edu", "carlo.baldassi@polito.it", "borgs@microsoft.com", "jchayes@microsoft.com", "sagun@cims.nyu.edu", "riccardo.zecchina@polito.it"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287695367, "id": "ICLR.cc/2017/conference/-/paper185/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "B1YfAfcgl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper185/reviewers", "ICLR.cc/2017/conference/paper185/areachairs"], "cdate": 1485287695367}}}, {"tddate": null, "tmdate": 1483395915196, "tcdate": 1483395539271, "number": 7, "id": "BJo0-8dHg", "invitation": "ICLR.cc/2017/conference/-/paper185/public/comment", "forum": "B1YfAfcgl", "replyto": "BkVgcr_4e", "signatures": ["~Pratik_A_Chaudhari1"], "readers": ["everyone"], "writers": ["~Pratik_A_Chaudhari1"], "content": {"title": "SVI does not resemble Entropy-SGD without a \"moving prior\", a feature of our scheme", "comment": "To address the objections 1) and 3), let \\Xi denote the dataset, z denote the weights and x be the parameters of a variational distribution q_x(z). The ELBO can then be written as\n(i) \\log p(\\Xi)\n        \\geq E_{z \\sim q_x(z)} [\\log p(\\Xi | z)] - KL(q_x(z) || p(z))\nand maximized with respect to x. The distribution p(z) is a fixed (parameter-free) prior, which one has to postulate.\n\nOn the other hand, Eqn. 4 in the paper can be used to write the log of local entropy as:\n(ii) \\log F(x,\\gamma)\n        = \\log \\int_{z \\in Z} e^{-f(z; \\Xi) - \\gamma/2 |x-z|^2} dz\n        \\geq \\int_{z \\in Z} [-f(z; \\Xi) - \\gamma/2 |x-z|^2] dz;\nwhere f(z) = -\\log p(\\Xi | z). We are unaware of a general way to choose a prior p(z) and a variational family q_x(z) that that makes (i) resemble (ii), and therefore interpret our method as \u201cintegrating out [\u2026] the posterior over neural network parameters.\u201d The only way we can do so is to pick a specific \u201cprior\" that depends on the parameter x (hence, not really a prior). For instance, one could choose a uniform variational family (say, q_x(z) \\propto constant for |x-z| \\leq C and zero otherwise) and a Gaussian \u201cprior\" with mean x (\\log p(z) = -\\gamma/2 |x-z|^2) to make (ii) resemble ELBO. In this case p(z) would not be fixed, but it would \u201cmove\u201d along with the current iterate x.\n\nThis \"moving prior\" is a crucial feature of our proposed algorithm. The gradient of local entropy (Eqn. 7 in the paper) clarifies this further:\n    dF  = -\\gamma (x - E_{z \\sim r(z; x)} [z]);\nwhere the distribution r(z; x) is\n    r(z; x) \\propto p(\\Xi | z) \\exp(-\\gamma/2 |x-z|^2);\ni.e. it contains a data likelihood term with a prior that \"moves\" along with the current iterate x.\n\nConcerning the relation to SGLD, consider Belief Propagation (BP). Our proposed algorithm relates to the \"focusing-Belief Propagation\" variant (fBP), rather than the standard one [BBC16]. The difference between BP and fBP is analogous to that between SGLD and Entropy-SGD: the latter operates on a transformation of the energy landscape of the former, exploiting local entropic effects. This difference is crucial and indeed related to the \"moving prior\" of the previous discussion; plain SGLD (or BP) can only trade energy for entropy via the temperature parameter which does not allow for direct use of any geometric information of the landscape and does not help with narrow minima.\n\nIn view of your comment 2), we also implemented SGLD for LeNet on MNIST and All-CNN-BN on CIFAR and will add the following results to our paper: After a hyper-parameter search, the best we obtained were a test error of LeNet on 0.63 \\pm 0.1% on MNIST after 300 epochs and 9.89 \\pm 0.11% on All-CNN-BN after 500 epochs. Disregarding the slow convergence of SGLD, its generalization error is slightly worse than the results in our paper, viz. 0.48% with Entropy-SGD on LeNet (0.51% with SGD) and 8.65% with Entropy-SGD on All-CNN-BN (8.3% with SGD). For comparison, the authors in [CCF15] report an error of 0.71% with SGLD on MNIST with a slightly larger network (0.47% with Santa), there are no results in the literature where MCMC methods perform comparably to SGD on larger networks.\n\n[BBC16] Baldassi, Carlo, et al. \"Unreasonable effectiveness of learning neural networks: From accessible states and robust ensembles to basic algorithmic schemes.\" PNAS (2016).\n[CCF15] Chen, Changyou, et al. \"Bridging the gap between stochastic gradient MCMC and stochastic optimization.\" arXiv:1512.07962 (2015)."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Entropy-SGD: Biasing Gradient Descent Into Wide Valleys", "abstract": "This paper proposes a new optimization algorithm called Entropy-SGD for training deep neural networks that is motivated by the local geometry of the energy landscape. Local extrema with low generalization error have a large proportion of almost-zero eigenvalues in the Hessian with very few positive or negative eigenvalues. We leverage upon this observation to construct a local-entropy-based objective function that favors well-generalizable solutions lying in large flat regions of the energy landscape, while avoiding poorly-generalizable solutions located in the sharp valleys. Conceptually, our algorithm resembles two nested loops of SGD where we use Langevin dynamics in the inner loop to compute the gradient of the local entropy before each update of the weights. We show that the new objective has a smoother energy landscape and show improved generalization over SGD using uniform stability, under certain assumptions. Our experiments on convolutional and recurrent neural networks demonstrate that Entropy-SGD compares favorably to state-of-the-art techniques in terms of generalization error and training time.", "pdf": "/pdf/2fb588bb07ffd17030e097202cf514bdfb5bcbca.pdf", "TL;DR": "This paper focuses on developing new optimization tools for deep learning that are tailored to exploit the local geometric properties of the objective function.", "paperhash": "chaudhari|entropysgd_biasing_gradient_descent_into_wide_valleys", "keywords": ["Deep learning", "Optimization"], "conflicts": ["ucla.edu", "cs.ucla.edu", "nyu.edu", "microsoft.com", "gwu.edu", "nyu.edu", "cims.nyu.edu", "columbia.edu", "facebook.com", "microsoft.com", "polito.it"], "authors": ["Pratik Chaudhari", "Anna Choromanska", "Stefano Soatto", "Yann LeCun", "Carlo Baldassi", "Christian Borgs", "Jennifer Chayes", "Levent Sagun", "Riccardo Zecchina"], "authorids": ["pratikac@ucla.edu", "achoroma@cims.nyu.edu", "soatto@cs.ucla.edu", "yann@cs.nyu.edu", "carlo.baldassi@polito.it", "borgs@microsoft.com", "jchayes@microsoft.com", "sagun@cims.nyu.edu", "riccardo.zecchina@polito.it"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287695367, "id": "ICLR.cc/2017/conference/-/paper185/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "B1YfAfcgl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper185/reviewers", "ICLR.cc/2017/conference/paper185/areachairs"], "cdate": 1485287695367}}}, {"tddate": null, "tmdate": 1483294022767, "tcdate": 1483294022767, "number": 6, "id": "H118Sa8re", "invitation": "ICLR.cc/2017/conference/-/paper185/public/comment", "forum": "B1YfAfcgl", "replyto": "HksUIdIrl", "signatures": ["~Csaba_Szepesvari1"], "readers": ["everyone"], "writers": ["~Csaba_Szepesvari1"], "content": {"title": "oops", "comment": "Oops, sorry, I did not notice the box there. I edited my rating now. Though it is a bit unclear to me where exactly the acceptance threshold will/should be, so my rating is just an educated guess. I think the flaws can be addressed and the paper has some interesting aspects so it may be worthwhile to publish it."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Entropy-SGD: Biasing Gradient Descent Into Wide Valleys", "abstract": "This paper proposes a new optimization algorithm called Entropy-SGD for training deep neural networks that is motivated by the local geometry of the energy landscape. Local extrema with low generalization error have a large proportion of almost-zero eigenvalues in the Hessian with very few positive or negative eigenvalues. We leverage upon this observation to construct a local-entropy-based objective function that favors well-generalizable solutions lying in large flat regions of the energy landscape, while avoiding poorly-generalizable solutions located in the sharp valleys. Conceptually, our algorithm resembles two nested loops of SGD where we use Langevin dynamics in the inner loop to compute the gradient of the local entropy before each update of the weights. We show that the new objective has a smoother energy landscape and show improved generalization over SGD using uniform stability, under certain assumptions. Our experiments on convolutional and recurrent neural networks demonstrate that Entropy-SGD compares favorably to state-of-the-art techniques in terms of generalization error and training time.", "pdf": "/pdf/2fb588bb07ffd17030e097202cf514bdfb5bcbca.pdf", "TL;DR": "This paper focuses on developing new optimization tools for deep learning that are tailored to exploit the local geometric properties of the objective function.", "paperhash": "chaudhari|entropysgd_biasing_gradient_descent_into_wide_valleys", "keywords": ["Deep learning", "Optimization"], "conflicts": ["ucla.edu", "cs.ucla.edu", "nyu.edu", "microsoft.com", "gwu.edu", "nyu.edu", "cims.nyu.edu", "columbia.edu", "facebook.com", "microsoft.com", "polito.it"], "authors": ["Pratik Chaudhari", "Anna Choromanska", "Stefano Soatto", "Yann LeCun", "Carlo Baldassi", "Christian Borgs", "Jennifer Chayes", "Levent Sagun", "Riccardo Zecchina"], "authorids": ["pratikac@ucla.edu", "achoroma@cims.nyu.edu", "soatto@cs.ucla.edu", "yann@cs.nyu.edu", "carlo.baldassi@polito.it", "borgs@microsoft.com", "jchayes@microsoft.com", "sagun@cims.nyu.edu", "riccardo.zecchina@polito.it"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287695367, "id": "ICLR.cc/2017/conference/-/paper185/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "B1YfAfcgl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper185/reviewers", "ICLR.cc/2017/conference/paper185/areachairs"], "cdate": 1485287695367}}}, {"tddate": null, "tmdate": 1483293641422, "tcdate": 1483232308764, "number": 1, "id": "B1p44ASBe", "invitation": "ICLR.cc/2017/conference/-/paper185/public/review", "forum": "B1YfAfcgl", "replyto": "B1YfAfcgl", "signatures": ["~Csaba_Szepesvari1"], "readers": ["everyone"], "writers": ["~Csaba_Szepesvari1"], "content": {"title": "Smoothing the error surface to improve generalization; high level idea may be good, details are not entirely convincing.", "rating": "6: Marginally above acceptance threshold", "review": "__Note__: An earlier version of the review (almost identical to the present one) for an earlier version of the paper (available on arXiV) can be found here: http://www.shortscience.org/paper?bibtexKey=journals/corr/1611.01838#csaba\nThe only change concerns relation to previous work.\n\n__Problem__: The problem considered is to derive an improved version of SGD for training neural networks (or minimize empirical loss) by modifying the loss optimized to that the solution found is more likely to end up in the vicinity of a minimum where the loss changes slowly (\"flat minima\" as in the paper of Hochreiter and Schmidhuber from 1997). \n\n__Motivation__: It is hypothetised that flat minima \"generalize\" better. \n\n__Algorithmic approach__: Let $f$ be the (empirical) loss to be minimized. Modify this to $$\\overline{f}(x) = \\rho f(x) - \\log \\int \\exp(-\\frac{\\gamma}{2}\\||z\\||^2-f(x+z))\\,dz$$ with some $\\rho,\\gamma>0$ tunable parameters. For $\\||z\\||^2\\gg \\frac{1}{\\gamma}$, the term $\\exp(-\\frac{\\gamma}{2}\\||z\\||^2)$ becomes very small, so effectively the second term is close to a constant times the integral of $f$ over a ball centered at $x$ and having a radius of $\\propto \\gamma^{-1/2}$. This is a smoothened version of $f$, hence one expects that by making this term more important then the first term, a procedure minimizing $\\bar f$ will be more likely to end up at a flat minima of $f$. Since the gradient is somewhat complicated, an MCMC algorithm is proposed (\"stochastic gradient Langevin dynamics\" from Welling and Teh, 2011).\n\n__Results__: There is a theoretical result that quantifies the increased smoothness of $\\overline{f}$, which is connected to stability and ultimately to generalization through citing a result of Hardt et al. (2015). Empirical results show better validation error on two datasets: MNIST and CIFAR-10 (the respective networks are LeNet and All-CNN-C). The improvement is in terms of reaching the same validation error as with an \"original SGD\" but with fewer \"epochs\".\n\n__Soundness, significance__: The proof of the __theoretical result__ relies on an arbitrary assumption that there exists some $c>0$ such that no eigenvalue of the hessian of $f$ lies in the set $[-2\\gamma-c,c]$ (the reason for the assumption is because otherwise a uniform improvement cannot be shown). For $\\rho=0$ the improvement of the smoothness (first and second order) is a factor of $1/(1+c/\\gamma)$. The proof uses Laplace's method and is more a sketch than a rigorous proof (error terms are dropped; it would be good to make this clear in the statement of the result).\n\nIn the experiments the modified procedure did not consistently reach a smaller validation error. The authors did not present running times, hence it is unclear whether the procedure's increased computation cost is offset by the faster convergence. \n\n__Evaluation__: It is puzzling why a simpler smoothing, e.g., $$\\overline{f}(x) = \\int f(x+z) g(z) dz$$ (with $g$ being the density of a centered probability distribution) is not considered. The authors note that something \"like this\" may be infeasible in \"deep neural networks\" (the note is somewhat vague). However, under mild conditions, $\\frac{\\partial}{\\partial x} \\overline{f}(x) = \\int \\frac{\\partial}{\\partial x} f(x+z) g(z) dz$, hence, for $Z\\sim g(\\cdot)$, $\\frac{\\partial}{\\partial x} f(x+Z)$ is an unbiased estimate of $\\frac{\\partial}{\\partial x} \\overline{f}(x)$, whose calculation is as cheap as that of vanilla SGD. Also, how much the smoothness of $f$ changes when using this approach is quite well understood.\n\n__Related work__: It is also strange that the specific modification that appears in this paper was proposed by others (Baldassi et al.), whom this paper also cites, but without giving these authors credit for introducing local entropy as a smoothing technique. ", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Entropy-SGD: Biasing Gradient Descent Into Wide Valleys", "abstract": "This paper proposes a new optimization algorithm called Entropy-SGD for training deep neural networks that is motivated by the local geometry of the energy landscape. Local extrema with low generalization error have a large proportion of almost-zero eigenvalues in the Hessian with very few positive or negative eigenvalues. We leverage upon this observation to construct a local-entropy-based objective function that favors well-generalizable solutions lying in large flat regions of the energy landscape, while avoiding poorly-generalizable solutions located in the sharp valleys. Conceptually, our algorithm resembles two nested loops of SGD where we use Langevin dynamics in the inner loop to compute the gradient of the local entropy before each update of the weights. We show that the new objective has a smoother energy landscape and show improved generalization over SGD using uniform stability, under certain assumptions. Our experiments on convolutional and recurrent neural networks demonstrate that Entropy-SGD compares favorably to state-of-the-art techniques in terms of generalization error and training time.", "pdf": "/pdf/2fb588bb07ffd17030e097202cf514bdfb5bcbca.pdf", "TL;DR": "This paper focuses on developing new optimization tools for deep learning that are tailored to exploit the local geometric properties of the objective function.", "paperhash": "chaudhari|entropysgd_biasing_gradient_descent_into_wide_valleys", "keywords": ["Deep learning", "Optimization"], "conflicts": ["ucla.edu", "cs.ucla.edu", "nyu.edu", "microsoft.com", "gwu.edu", "nyu.edu", "cims.nyu.edu", "columbia.edu", "facebook.com", "microsoft.com", "polito.it"], "authors": ["Pratik Chaudhari", "Anna Choromanska", "Stefano Soatto", "Yann LeCun", "Carlo Baldassi", "Christian Borgs", "Jennifer Chayes", "Levent Sagun", "Riccardo Zecchina"], "authorids": ["pratikac@ucla.edu", "achoroma@cims.nyu.edu", "soatto@cs.ucla.edu", "yann@cs.nyu.edu", "carlo.baldassi@polito.it", "borgs@microsoft.com", "jchayes@microsoft.com", "sagun@cims.nyu.edu", "riccardo.zecchina@polito.it"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1483232309355, "id": "ICLR.cc/2017/conference/-/paper185/public/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "B1YfAfcgl", "replyto": "B1YfAfcgl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "noninvitees": ["pratikac@ucla.edu", "achoroma@cims.nyu.edu", "soatto@cs.ucla.edu", "yann@cs.nyu.edu", "ICLR.cc/2017/conference/paper185/reviewers", "ICLR.cc/2017/conference/paper185/areachairs", "~Csaba_Szepesvari1"], "cdate": 1483232309355}}}, {"tddate": null, "tmdate": 1483273810928, "tcdate": 1483273810928, "number": 4, "id": "HksUIdIrl", "invitation": "ICLR.cc/2017/conference/-/paper185/official/comment", "forum": "B1YfAfcgl", "replyto": "B1p44ASBe", "signatures": ["ICLR.cc/2017/conference/paper185/areachair1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper185/areachair1"], "content": {"title": "Rating doesn't match review", "comment": "Many thanks for the detailed review.  However, I'm confused by your rating - While your review seems to point out many flaws with the paper, you rated it a 10/10.  Was this deliberate?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Entropy-SGD: Biasing Gradient Descent Into Wide Valleys", "abstract": "This paper proposes a new optimization algorithm called Entropy-SGD for training deep neural networks that is motivated by the local geometry of the energy landscape. Local extrema with low generalization error have a large proportion of almost-zero eigenvalues in the Hessian with very few positive or negative eigenvalues. We leverage upon this observation to construct a local-entropy-based objective function that favors well-generalizable solutions lying in large flat regions of the energy landscape, while avoiding poorly-generalizable solutions located in the sharp valleys. Conceptually, our algorithm resembles two nested loops of SGD where we use Langevin dynamics in the inner loop to compute the gradient of the local entropy before each update of the weights. We show that the new objective has a smoother energy landscape and show improved generalization over SGD using uniform stability, under certain assumptions. Our experiments on convolutional and recurrent neural networks demonstrate that Entropy-SGD compares favorably to state-of-the-art techniques in terms of generalization error and training time.", "pdf": "/pdf/2fb588bb07ffd17030e097202cf514bdfb5bcbca.pdf", "TL;DR": "This paper focuses on developing new optimization tools for deep learning that are tailored to exploit the local geometric properties of the objective function.", "paperhash": "chaudhari|entropysgd_biasing_gradient_descent_into_wide_valleys", "keywords": ["Deep learning", "Optimization"], "conflicts": ["ucla.edu", "cs.ucla.edu", "nyu.edu", "microsoft.com", "gwu.edu", "nyu.edu", "cims.nyu.edu", "columbia.edu", "facebook.com", "microsoft.com", "polito.it"], "authors": ["Pratik Chaudhari", "Anna Choromanska", "Stefano Soatto", "Yann LeCun", "Carlo Baldassi", "Christian Borgs", "Jennifer Chayes", "Levent Sagun", "Riccardo Zecchina"], "authorids": ["pratikac@ucla.edu", "achoroma@cims.nyu.edu", "soatto@cs.ucla.edu", "yann@cs.nyu.edu", "carlo.baldassi@polito.it", "borgs@microsoft.com", "jchayes@microsoft.com", "sagun@cims.nyu.edu", "riccardo.zecchina@polito.it"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287695237, "id": "ICLR.cc/2017/conference/-/paper185/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "B1YfAfcgl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper185/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper185/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper185/reviewers", "ICLR.cc/2017/conference/paper185/areachairs"], "cdate": 1485287695237}}}, {"tddate": null, "tmdate": 1482951259439, "tcdate": 1482951259439, "number": 3, "id": "rkQD9K-Hg", "invitation": "ICLR.cc/2017/conference/-/paper185/official/comment", "forum": "B1YfAfcgl", "replyto": "Bkv-PNb-x", "signatures": ["ICLR.cc/2017/conference/paper185/areachair1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper185/areachair1"], "content": {"title": "Is the Hessian-vector product actually expensive?", "comment": "You claim that \"For modern deep networks with a few million weights, computing the Hessian-vector product as done in HS97 is prohibitive\".  But in general the HVP can be computed for only twice the time cost as a gradient evaluation.  Can you elaborate on why this is expensive in your case?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Entropy-SGD: Biasing Gradient Descent Into Wide Valleys", "abstract": "This paper proposes a new optimization algorithm called Entropy-SGD for training deep neural networks that is motivated by the local geometry of the energy landscape. Local extrema with low generalization error have a large proportion of almost-zero eigenvalues in the Hessian with very few positive or negative eigenvalues. We leverage upon this observation to construct a local-entropy-based objective function that favors well-generalizable solutions lying in large flat regions of the energy landscape, while avoiding poorly-generalizable solutions located in the sharp valleys. Conceptually, our algorithm resembles two nested loops of SGD where we use Langevin dynamics in the inner loop to compute the gradient of the local entropy before each update of the weights. We show that the new objective has a smoother energy landscape and show improved generalization over SGD using uniform stability, under certain assumptions. Our experiments on convolutional and recurrent neural networks demonstrate that Entropy-SGD compares favorably to state-of-the-art techniques in terms of generalization error and training time.", "pdf": "/pdf/2fb588bb07ffd17030e097202cf514bdfb5bcbca.pdf", "TL;DR": "This paper focuses on developing new optimization tools for deep learning that are tailored to exploit the local geometric properties of the objective function.", "paperhash": "chaudhari|entropysgd_biasing_gradient_descent_into_wide_valleys", "keywords": ["Deep learning", "Optimization"], "conflicts": ["ucla.edu", "cs.ucla.edu", "nyu.edu", "microsoft.com", "gwu.edu", "nyu.edu", "cims.nyu.edu", "columbia.edu", "facebook.com", "microsoft.com", "polito.it"], "authors": ["Pratik Chaudhari", "Anna Choromanska", "Stefano Soatto", "Yann LeCun", "Carlo Baldassi", "Christian Borgs", "Jennifer Chayes", "Levent Sagun", "Riccardo Zecchina"], "authorids": ["pratikac@ucla.edu", "achoroma@cims.nyu.edu", "soatto@cs.ucla.edu", "yann@cs.nyu.edu", "carlo.baldassi@polito.it", "borgs@microsoft.com", "jchayes@microsoft.com", "sagun@cims.nyu.edu", "riccardo.zecchina@polito.it"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287695237, "id": "ICLR.cc/2017/conference/-/paper185/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "B1YfAfcgl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper185/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper185/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper185/reviewers", "ICLR.cc/2017/conference/paper185/areachairs"], "cdate": 1485287695237}}}, {"tddate": null, "tmdate": 1482344940124, "tcdate": 1482344940124, "number": 2, "id": "BkVgcr_4e", "invitation": "ICLR.cc/2017/conference/-/paper185/official/comment", "forum": "B1YfAfcgl", "replyto": "rkoLqqP4g", "signatures": ["ICLR.cc/2017/conference/paper185/areachair1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper185/areachair1"], "content": {"title": "You can do SVI on the network weights; there don't have to be extra latent variables", "comment": "Thanks for the detailed reply.  However, I feel like we're talking past each other on a few points:\n\n1) You explain that SVI wouldn't be appropriate here because there aren't latent variables.  But I'm suggesting that you're effectively approximately integrating out, or sampling from, the posterior over neural network parameters.  That is, you only have one set of latent variables, which are the parameters of your network.\n\n2) You assert it wouldn't be appropriate to compare against SGLD because it's a sampler and you're introducing an optimizer.  However, the main motivation for your method seems to be better generalization error, not a better optimization of the original objective.  Better generalization is the reason why people advocate using approximate inference such as SGLD to choose weights instead of maximum likelihood.\n\nI realize you claim to not be doing approximate inference, but since SGLD is a strictly simpler method than the one you propose which also has the same motivations, you should compare against it.\n\n3) Thanks for spelling out the free energy in terms of F, U and \\beta - however I would find it much more illuminating if you were to write these quantities in terms of integrals of log-probabilities.\n\nAgain, the entire motivation for this method seems to be the same as for approximate inference methods, to the point where I suspect you might be re-inventing existing methods.  So a clearer conceptual and experimental comparison would be helpful."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Entropy-SGD: Biasing Gradient Descent Into Wide Valleys", "abstract": "This paper proposes a new optimization algorithm called Entropy-SGD for training deep neural networks that is motivated by the local geometry of the energy landscape. Local extrema with low generalization error have a large proportion of almost-zero eigenvalues in the Hessian with very few positive or negative eigenvalues. We leverage upon this observation to construct a local-entropy-based objective function that favors well-generalizable solutions lying in large flat regions of the energy landscape, while avoiding poorly-generalizable solutions located in the sharp valleys. Conceptually, our algorithm resembles two nested loops of SGD where we use Langevin dynamics in the inner loop to compute the gradient of the local entropy before each update of the weights. We show that the new objective has a smoother energy landscape and show improved generalization over SGD using uniform stability, under certain assumptions. Our experiments on convolutional and recurrent neural networks demonstrate that Entropy-SGD compares favorably to state-of-the-art techniques in terms of generalization error and training time.", "pdf": "/pdf/2fb588bb07ffd17030e097202cf514bdfb5bcbca.pdf", "TL;DR": "This paper focuses on developing new optimization tools for deep learning that are tailored to exploit the local geometric properties of the objective function.", "paperhash": "chaudhari|entropysgd_biasing_gradient_descent_into_wide_valleys", "keywords": ["Deep learning", "Optimization"], "conflicts": ["ucla.edu", "cs.ucla.edu", "nyu.edu", "microsoft.com", "gwu.edu", "nyu.edu", "cims.nyu.edu", "columbia.edu", "facebook.com", "microsoft.com", "polito.it"], "authors": ["Pratik Chaudhari", "Anna Choromanska", "Stefano Soatto", "Yann LeCun", "Carlo Baldassi", "Christian Borgs", "Jennifer Chayes", "Levent Sagun", "Riccardo Zecchina"], "authorids": ["pratikac@ucla.edu", "achoroma@cims.nyu.edu", "soatto@cs.ucla.edu", "yann@cs.nyu.edu", "carlo.baldassi@polito.it", "borgs@microsoft.com", "jchayes@microsoft.com", "sagun@cims.nyu.edu", "riccardo.zecchina@polito.it"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287695237, "id": "ICLR.cc/2017/conference/-/paper185/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "B1YfAfcgl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper185/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper185/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper185/reviewers", "ICLR.cc/2017/conference/paper185/areachairs"], "cdate": 1485287695237}}}, {"tddate": null, "tmdate": 1482299986642, "tcdate": 1482299986642, "number": 5, "id": "rkoLqqP4g", "invitation": "ICLR.cc/2017/conference/-/paper185/public/comment", "forum": "B1YfAfcgl", "replyto": "rkiKns-Ne", "signatures": ["~Pratik_A_Chaudhari1"], "readers": ["everyone"], "writers": ["~Pratik_A_Chaudhari1"], "content": {"title": " ", "comment": "We are happy to provide clarifications to the questions posed. We enclose them below.\n\n\"mostly re-inventing stochastic variational inference\"\nSVI computes the posterior distribution of hidden variables given data by maximizing the ELBO. Please note that we do not have hidden variables in our formulation; \"x\" are the variables of our objective f(x), not data. We are interested in maximizing the free energy F(x,\\gamma) as defined in Eqn. 4. This is the *free energy of the energy landscape* (defined via a Gibbs distribution). This is thus unrelated to marginal likelihood or variational ELBO where one can impose entropic constraints on the posterior distribution of the *hidden variables* via a prior on them.\n\n\"I suspect it'd become clear that you are maximizing the marginal likelihood\"\nPlease note that we are not maximizing the marginal likelihood, as discussed above.\n\n\"how is this better than SGLD\", \"bizarre that you compare against Adam instead of SGLD\"\nSGLD is an MCMC algorithm that draws samples from a given distribution. If the step-size goes to zero slowly enough, akin to all MCMC algorithms, it converges to the maximum of the likelihood in exponentially long time-scales (see algorithms like SGHMC [CFG14], Santa [CCG15] etc. that train Bayesian neural networks using MCMC algorithms). Note that SGLD does not optimize the marginal likelihood because there is no notion of hidden variables in vanilla SGLD. We provide a brief review of SGLD in Appendix A of our paper.\n\nWe do not know of any results in the literature that train large deep networks such as the one used for CIFAR to get competitive error rates using SGLD. We would like to emphasize that Entropy-SGD simply uses MCMC sampling to estimate Eqn. 7 but it is unrelated to SGLD otherwise.\n\nOn the other hand, Adam is an algorithm for computing the maximum of a the likelihood of data given parameters or (equivalently, minimize the loss function). Entropy-SGD is an algorithm designed for minimizing the loss function f(x), in particular, it is not an MCMC algorithm that draws samples from the likelihood. It however does not explicitly do so and instead maximizes the local entropy. We therefore compare Entropy-SGD with state-of-the-art algorithms for training deep networks like Adam and SGD.\n\n\"frustrating that you discuss free energy and entropy without precise definitions\"\nLocal entropy (local free energy) is formally defined in Def. 1 (Eqn. 4) but it is already introduced on pg. 2 in the Introduction. The discussion towards the end of Sec. 3 (pg. 5, first line) defines the classical entropy of a Gibbs distribution. The beginning of Sec. 3 based on Fig. 2 is intended to explain things to the reader at an intuitive level before proceeding to the formal definitions which are nevertheless present.\n\nFree energy and classical entropy are related by the informal description \"free energy = internal energy - temperature x entropy\". Formally, the relation is\n    F(\\beta) = U(\\beta) - \\frac{1}{\\beta} S(\\beta)\nwhere the log-partition function (free energy) is defined as F(\\beta) = -\\beta^{-1} \\log Z(\\beta), the internal energy is U(\\beta) = \\partial_\\beta (\\beta F(\\beta)) and S(\\beta) = \\beta^2 \\partial_\\beta (F(\\beta)) is the classical entropy.\n\nNote that the above equation is about the entire Gibbs distribution, in our work we define a \"local free energy\" F(x, \\beta) via the modified Gibbs distribution in Eqn. 3. We can add this discussion to the paper.\n\n[CFG14] Chen, Tianqi, Emily B. Fox, and Carlos Guestrin. \"Stochastic Gradient Hamiltonian Monte Carlo.\" ICML. 2014.\n[CCF15] Chen, Changyou, et al. \"Bridging the gap between stochastic gradient mcmc and stochastic optimization.\" arXiv:1512.07962 (2015)."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Entropy-SGD: Biasing Gradient Descent Into Wide Valleys", "abstract": "This paper proposes a new optimization algorithm called Entropy-SGD for training deep neural networks that is motivated by the local geometry of the energy landscape. Local extrema with low generalization error have a large proportion of almost-zero eigenvalues in the Hessian with very few positive or negative eigenvalues. We leverage upon this observation to construct a local-entropy-based objective function that favors well-generalizable solutions lying in large flat regions of the energy landscape, while avoiding poorly-generalizable solutions located in the sharp valleys. Conceptually, our algorithm resembles two nested loops of SGD where we use Langevin dynamics in the inner loop to compute the gradient of the local entropy before each update of the weights. We show that the new objective has a smoother energy landscape and show improved generalization over SGD using uniform stability, under certain assumptions. Our experiments on convolutional and recurrent neural networks demonstrate that Entropy-SGD compares favorably to state-of-the-art techniques in terms of generalization error and training time.", "pdf": "/pdf/2fb588bb07ffd17030e097202cf514bdfb5bcbca.pdf", "TL;DR": "This paper focuses on developing new optimization tools for deep learning that are tailored to exploit the local geometric properties of the objective function.", "paperhash": "chaudhari|entropysgd_biasing_gradient_descent_into_wide_valleys", "keywords": ["Deep learning", "Optimization"], "conflicts": ["ucla.edu", "cs.ucla.edu", "nyu.edu", "microsoft.com", "gwu.edu", "nyu.edu", "cims.nyu.edu", "columbia.edu", "facebook.com", "microsoft.com", "polito.it"], "authors": ["Pratik Chaudhari", "Anna Choromanska", "Stefano Soatto", "Yann LeCun", "Carlo Baldassi", "Christian Borgs", "Jennifer Chayes", "Levent Sagun", "Riccardo Zecchina"], "authorids": ["pratikac@ucla.edu", "achoroma@cims.nyu.edu", "soatto@cs.ucla.edu", "yann@cs.nyu.edu", "carlo.baldassi@polito.it", "borgs@microsoft.com", "jchayes@microsoft.com", "sagun@cims.nyu.edu", "riccardo.zecchina@polito.it"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287695367, "id": "ICLR.cc/2017/conference/-/paper185/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "B1YfAfcgl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper185/reviewers", "ICLR.cc/2017/conference/paper185/areachairs"], "cdate": 1485287695367}}}, {"tddate": null, "tmdate": 1482227543042, "tcdate": 1482227543042, "number": 2, "content": {"title": "-", "question": "-"}, "id": "B1ywyFL4x", "invitation": "ICLR.cc/2017/conference/-/paper185/pre-review/question", "forum": "B1YfAfcgl", "replyto": "B1YfAfcgl", "signatures": ["ICLR.cc/2017/conference/paper185/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper185/AnonReviewer1"], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Entropy-SGD: Biasing Gradient Descent Into Wide Valleys", "abstract": "This paper proposes a new optimization algorithm called Entropy-SGD for training deep neural networks that is motivated by the local geometry of the energy landscape. Local extrema with low generalization error have a large proportion of almost-zero eigenvalues in the Hessian with very few positive or negative eigenvalues. We leverage upon this observation to construct a local-entropy-based objective function that favors well-generalizable solutions lying in large flat regions of the energy landscape, while avoiding poorly-generalizable solutions located in the sharp valleys. Conceptually, our algorithm resembles two nested loops of SGD where we use Langevin dynamics in the inner loop to compute the gradient of the local entropy before each update of the weights. We show that the new objective has a smoother energy landscape and show improved generalization over SGD using uniform stability, under certain assumptions. Our experiments on convolutional and recurrent neural networks demonstrate that Entropy-SGD compares favorably to state-of-the-art techniques in terms of generalization error and training time.", "pdf": "/pdf/2fb588bb07ffd17030e097202cf514bdfb5bcbca.pdf", "TL;DR": "This paper focuses on developing new optimization tools for deep learning that are tailored to exploit the local geometric properties of the objective function.", "paperhash": "chaudhari|entropysgd_biasing_gradient_descent_into_wide_valleys", "keywords": ["Deep learning", "Optimization"], "conflicts": ["ucla.edu", "cs.ucla.edu", "nyu.edu", "microsoft.com", "gwu.edu", "nyu.edu", "cims.nyu.edu", "columbia.edu", "facebook.com", "microsoft.com", "polito.it"], "authors": ["Pratik Chaudhari", "Anna Choromanska", "Stefano Soatto", "Yann LeCun", "Carlo Baldassi", "Christian Borgs", "Jennifer Chayes", "Levent Sagun", "Riccardo Zecchina"], "authorids": ["pratikac@ucla.edu", "achoroma@cims.nyu.edu", "soatto@cs.ucla.edu", "yann@cs.nyu.edu", "carlo.baldassi@polito.it", "borgs@microsoft.com", "jchayes@microsoft.com", "sagun@cims.nyu.edu", "riccardo.zecchina@polito.it"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1482227543697, "id": "ICLR.cc/2017/conference/-/paper185/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper185/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper185/AnonReviewer2", "ICLR.cc/2017/conference/paper185/AnonReviewer1"], "reply": {"forum": "B1YfAfcgl", "replyto": "B1YfAfcgl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper185/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper185/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1482227543697}}}, {"tddate": null, "tmdate": 1481911427290, "tcdate": 1481911427290, "number": 1, "id": "rkiKns-Ne", "invitation": "ICLR.cc/2017/conference/-/paper185/official/comment", "forum": "B1YfAfcgl", "replyto": "B1YfAfcgl", "signatures": ["ICLR.cc/2017/conference/paper185/areachair1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper185/areachair1"], "content": {"title": "How is this better than stochastic-gradient Langevin dynamics, or stochastic variational inference?", "comment": "I agree with the motivations of the method, but this paper seems to be mostly re-inventing stochastic variational inference.  The SVI ELBO estimate also encourages the approximate posterior mean to head in directions that have high likelihood times volume.\n\nMore broadly, the marginal likelihood is also trying to achieve the same tradeoffs, which is the motivation for MCMC methods such as SGLD in the first place.  So It seems bizarre that you would compare Adam against a complicated method with SGLD in the inner loop, without comparing against the much simpler SGLD as well.\n\nFinally, it's frustrating that you discuss free energy and entropy at length in words without giving their precise definitions.  If you did so, I suspect it would become clear that you're proposing maximizing the marginal likelihood, which is definitely a great idea but already well-known.  How does the entropy you optimize relate to the marginal likelihood and the variational ELBO?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Entropy-SGD: Biasing Gradient Descent Into Wide Valleys", "abstract": "This paper proposes a new optimization algorithm called Entropy-SGD for training deep neural networks that is motivated by the local geometry of the energy landscape. Local extrema with low generalization error have a large proportion of almost-zero eigenvalues in the Hessian with very few positive or negative eigenvalues. We leverage upon this observation to construct a local-entropy-based objective function that favors well-generalizable solutions lying in large flat regions of the energy landscape, while avoiding poorly-generalizable solutions located in the sharp valleys. Conceptually, our algorithm resembles two nested loops of SGD where we use Langevin dynamics in the inner loop to compute the gradient of the local entropy before each update of the weights. We show that the new objective has a smoother energy landscape and show improved generalization over SGD using uniform stability, under certain assumptions. Our experiments on convolutional and recurrent neural networks demonstrate that Entropy-SGD compares favorably to state-of-the-art techniques in terms of generalization error and training time.", "pdf": "/pdf/2fb588bb07ffd17030e097202cf514bdfb5bcbca.pdf", "TL;DR": "This paper focuses on developing new optimization tools for deep learning that are tailored to exploit the local geometric properties of the objective function.", "paperhash": "chaudhari|entropysgd_biasing_gradient_descent_into_wide_valleys", "keywords": ["Deep learning", "Optimization"], "conflicts": ["ucla.edu", "cs.ucla.edu", "nyu.edu", "microsoft.com", "gwu.edu", "nyu.edu", "cims.nyu.edu", "columbia.edu", "facebook.com", "microsoft.com", "polito.it"], "authors": ["Pratik Chaudhari", "Anna Choromanska", "Stefano Soatto", "Yann LeCun", "Carlo Baldassi", "Christian Borgs", "Jennifer Chayes", "Levent Sagun", "Riccardo Zecchina"], "authorids": ["pratikac@ucla.edu", "achoroma@cims.nyu.edu", "soatto@cs.ucla.edu", "yann@cs.nyu.edu", "carlo.baldassi@polito.it", "borgs@microsoft.com", "jchayes@microsoft.com", "sagun@cims.nyu.edu", "riccardo.zecchina@polito.it"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287695237, "id": "ICLR.cc/2017/conference/-/paper185/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "B1YfAfcgl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper185/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper185/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper185/reviewers", "ICLR.cc/2017/conference/paper185/areachairs"], "cdate": 1485287695237}}}, {"tddate": null, "tmdate": 1480950692623, "tcdate": 1480950692616, "number": 4, "id": "HyaimZ7Ql", "invitation": "ICLR.cc/2017/conference/-/paper185/public/comment", "forum": "B1YfAfcgl", "replyto": "Bku-eYk7g", "signatures": ["~Pratik_A_Chaudhari1"], "readers": ["everyone"], "writers": ["~Pratik_A_Chaudhari1"], "content": {"title": " works for mixed-sign eigenvalues", "comment": "The proof works for both positive and negative eigenvalues. The assumption (in Sec. 4.4) should read \"assume |\\lambda (\\nabla^2 f(x))| \\geq 2 \\gamma + c for some small c > 0\". This can be made slightly stricter to read \"assume that \\nabla^2 f(x) does not have any eigenvalue between [-2\\gamma-c,c]\". We have rectified this in the paper."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Entropy-SGD: Biasing Gradient Descent Into Wide Valleys", "abstract": "This paper proposes a new optimization algorithm called Entropy-SGD for training deep neural networks that is motivated by the local geometry of the energy landscape. Local extrema with low generalization error have a large proportion of almost-zero eigenvalues in the Hessian with very few positive or negative eigenvalues. We leverage upon this observation to construct a local-entropy-based objective function that favors well-generalizable solutions lying in large flat regions of the energy landscape, while avoiding poorly-generalizable solutions located in the sharp valleys. Conceptually, our algorithm resembles two nested loops of SGD where we use Langevin dynamics in the inner loop to compute the gradient of the local entropy before each update of the weights. We show that the new objective has a smoother energy landscape and show improved generalization over SGD using uniform stability, under certain assumptions. Our experiments on convolutional and recurrent neural networks demonstrate that Entropy-SGD compares favorably to state-of-the-art techniques in terms of generalization error and training time.", "pdf": "/pdf/2fb588bb07ffd17030e097202cf514bdfb5bcbca.pdf", "TL;DR": "This paper focuses on developing new optimization tools for deep learning that are tailored to exploit the local geometric properties of the objective function.", "paperhash": "chaudhari|entropysgd_biasing_gradient_descent_into_wide_valleys", "keywords": ["Deep learning", "Optimization"], "conflicts": ["ucla.edu", "cs.ucla.edu", "nyu.edu", "microsoft.com", "gwu.edu", "nyu.edu", "cims.nyu.edu", "columbia.edu", "facebook.com", "microsoft.com", "polito.it"], "authors": ["Pratik Chaudhari", "Anna Choromanska", "Stefano Soatto", "Yann LeCun", "Carlo Baldassi", "Christian Borgs", "Jennifer Chayes", "Levent Sagun", "Riccardo Zecchina"], "authorids": ["pratikac@ucla.edu", "achoroma@cims.nyu.edu", "soatto@cs.ucla.edu", "yann@cs.nyu.edu", "carlo.baldassi@polito.it", "borgs@microsoft.com", "jchayes@microsoft.com", "sagun@cims.nyu.edu", "riccardo.zecchina@polito.it"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287695367, "id": "ICLR.cc/2017/conference/-/paper185/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "B1YfAfcgl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper185/reviewers", "ICLR.cc/2017/conference/paper185/areachairs"], "cdate": 1485287695367}}}, {"tddate": null, "tmdate": 1480720384157, "tcdate": 1480720384153, "number": 1, "id": "Bku-eYk7g", "invitation": "ICLR.cc/2017/conference/-/paper185/pre-review/question", "forum": "B1YfAfcgl", "replyto": "B1YfAfcgl", "signatures": ["ICLR.cc/2017/conference/paper185/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper185/AnonReviewer2"], "content": {"title": "Allowing eigenvalues with mixed signs?", "question": "Section 4.4: The original loss function is denoted by f(x) and right before lemma (2), related to the eigenvalues of hessian of f(x), it says either lambda_min>=c (with c>=0) or lambda_max<=-2*gamma-c (with c>=0 and gamma>=0), which respectively imply that at each point x, either all eigenvalues are nonnoegative, or all are nonpositive. If I understand this correctly, then the analysis does not consider points which have mixed-sign eigenvalues. Did I misunderstand something?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Entropy-SGD: Biasing Gradient Descent Into Wide Valleys", "abstract": "This paper proposes a new optimization algorithm called Entropy-SGD for training deep neural networks that is motivated by the local geometry of the energy landscape. Local extrema with low generalization error have a large proportion of almost-zero eigenvalues in the Hessian with very few positive or negative eigenvalues. We leverage upon this observation to construct a local-entropy-based objective function that favors well-generalizable solutions lying in large flat regions of the energy landscape, while avoiding poorly-generalizable solutions located in the sharp valleys. Conceptually, our algorithm resembles two nested loops of SGD where we use Langevin dynamics in the inner loop to compute the gradient of the local entropy before each update of the weights. We show that the new objective has a smoother energy landscape and show improved generalization over SGD using uniform stability, under certain assumptions. Our experiments on convolutional and recurrent neural networks demonstrate that Entropy-SGD compares favorably to state-of-the-art techniques in terms of generalization error and training time.", "pdf": "/pdf/2fb588bb07ffd17030e097202cf514bdfb5bcbca.pdf", "TL;DR": "This paper focuses on developing new optimization tools for deep learning that are tailored to exploit the local geometric properties of the objective function.", "paperhash": "chaudhari|entropysgd_biasing_gradient_descent_into_wide_valleys", "keywords": ["Deep learning", "Optimization"], "conflicts": ["ucla.edu", "cs.ucla.edu", "nyu.edu", "microsoft.com", "gwu.edu", "nyu.edu", "cims.nyu.edu", "columbia.edu", "facebook.com", "microsoft.com", "polito.it"], "authors": ["Pratik Chaudhari", "Anna Choromanska", "Stefano Soatto", "Yann LeCun", "Carlo Baldassi", "Christian Borgs", "Jennifer Chayes", "Levent Sagun", "Riccardo Zecchina"], "authorids": ["pratikac@ucla.edu", "achoroma@cims.nyu.edu", "soatto@cs.ucla.edu", "yann@cs.nyu.edu", "carlo.baldassi@polito.it", "borgs@microsoft.com", "jchayes@microsoft.com", "sagun@cims.nyu.edu", "riccardo.zecchina@polito.it"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1482227543697, "id": "ICLR.cc/2017/conference/-/paper185/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper185/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper185/AnonReviewer2", "ICLR.cc/2017/conference/paper185/AnonReviewer1"], "reply": {"forum": "B1YfAfcgl", "replyto": "B1YfAfcgl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper185/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper185/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1482227543697}}}, {"tddate": null, "tmdate": 1479080682781, "tcdate": 1479080682777, "number": 3, "id": "Bk7xou8-x", "invitation": "ICLR.cc/2017/conference/-/paper185/public/comment", "forum": "B1YfAfcgl", "replyto": "B1YfAfcgl", "signatures": ["~Pratik_A_Chaudhari1"], "readers": ["everyone"], "writers": ["~Pratik_A_Chaudhari1"], "content": {"title": "updates to the paper", "comment": "We have updated the \"Related work\" section of the paper with a discussion of [HS97] and [KS16].\n\n[HS97] Hochreiter, Sepp, and J\u00fcrgen Schmidhuber. \"Flat minima.\" Neural Computation 9.1 (1997): 1-42.\n[KS16] Keskar, Nitish Shirish, et al. \"On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima.\" arXiv:1609.04836 (2016)."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Entropy-SGD: Biasing Gradient Descent Into Wide Valleys", "abstract": "This paper proposes a new optimization algorithm called Entropy-SGD for training deep neural networks that is motivated by the local geometry of the energy landscape. Local extrema with low generalization error have a large proportion of almost-zero eigenvalues in the Hessian with very few positive or negative eigenvalues. We leverage upon this observation to construct a local-entropy-based objective function that favors well-generalizable solutions lying in large flat regions of the energy landscape, while avoiding poorly-generalizable solutions located in the sharp valleys. Conceptually, our algorithm resembles two nested loops of SGD where we use Langevin dynamics in the inner loop to compute the gradient of the local entropy before each update of the weights. We show that the new objective has a smoother energy landscape and show improved generalization over SGD using uniform stability, under certain assumptions. Our experiments on convolutional and recurrent neural networks demonstrate that Entropy-SGD compares favorably to state-of-the-art techniques in terms of generalization error and training time.", "pdf": "/pdf/2fb588bb07ffd17030e097202cf514bdfb5bcbca.pdf", "TL;DR": "This paper focuses on developing new optimization tools for deep learning that are tailored to exploit the local geometric properties of the objective function.", "paperhash": "chaudhari|entropysgd_biasing_gradient_descent_into_wide_valleys", "keywords": ["Deep learning", "Optimization"], "conflicts": ["ucla.edu", "cs.ucla.edu", "nyu.edu", "microsoft.com", "gwu.edu", "nyu.edu", "cims.nyu.edu", "columbia.edu", "facebook.com", "microsoft.com", "polito.it"], "authors": ["Pratik Chaudhari", "Anna Choromanska", "Stefano Soatto", "Yann LeCun", "Carlo Baldassi", "Christian Borgs", "Jennifer Chayes", "Levent Sagun", "Riccardo Zecchina"], "authorids": ["pratikac@ucla.edu", "achoroma@cims.nyu.edu", "soatto@cs.ucla.edu", "yann@cs.nyu.edu", "carlo.baldassi@polito.it", "borgs@microsoft.com", "jchayes@microsoft.com", "sagun@cims.nyu.edu", "riccardo.zecchina@polito.it"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287695367, "id": "ICLR.cc/2017/conference/-/paper185/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "B1YfAfcgl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper185/reviewers", "ICLR.cc/2017/conference/paper185/areachairs"], "cdate": 1485287695367}}}, {"tddate": null, "tmdate": 1478735614790, "tcdate": 1478735614764, "number": 2, "id": "Bkv-PNb-x", "invitation": "ICLR.cc/2017/conference/-/paper185/public/comment", "forum": "B1YfAfcgl", "replyto": "H1nvb9k-e", "signatures": ["~Pratik_A_Chaudhari1"], "readers": ["everyone"], "writers": ["~Pratik_A_Chaudhari1"], "content": {"title": "reply", "comment": "Thank you for pointing us to the \"Flat Minima Search\" paper, we will add a reference to this work in our paper. Entropy-SGD is a more natural and scalable approach that is based on the same idea that flat regions in the energy landscape generalize better. Please read below for details.\n\nHS97 constructs a loss function given by E + \\lambda B where E is the error on the training set and B is the log-volume of a flat region. Flatness condition 1 in the paper is akin to usual SGD while flatness condition 2 in the paper explicitly penalizes the variance of the network's output near the current weights. In this context, Entropy-SGD is very similar to FMS: we maximize the free energy F(x, \\gamma), i.e., a sum of internal energy (E in HS97) and negative entropy (B in HS97). Our formulation based on local entropy however provides a few benefits:\n\n1. The authors mention in Sec. 7 and elaborate in their experiments in Sec. 5 that obtaining a good value of \\epsilon (maximum perturbation of weights) and E_{tol} (upper bound on training error) is difficult. The parameter \\lambda in their loss function is simply the Gibbs temperature in our case (cf. Sec. 3 in our paper), while -\\lambda B is easily seen as the classical entropy under the implicit assumption in HS97 that the Gibbs distribution is flat at the minima (end of Sec. 3 in our paper). Roughly, in the formulation of HS97, \\epsilon is hidden inside \\lambda (larger the penalty on flatness, smaller the \\epsilon, larger the \\lambda) while E_{tol} is a function of \\epsilon itself, since local minima that minimize training loss might not always be flatter than \\epsilon. Our formulation completely avoids such confusing choices of parameters, we rely on \\gamma for enforcing smoothness and on \\rho for fine-tuning more complex networks.\n\n2. There are a few delicate points in the formulation of HS97, viz., axis-aligned boxes are essential to enforce flatness, connection to MDL only exists for flat regions, for instance, local minima with multiple modes of symmetry (pg. 23 in HS97) have low entropy which is captured in our formulation but not via MDL.\n\n3. For modern deep networks with a few million weights, computing the Hessian-vector product as done in HS97 is prohibitive, more so, when averaging the Hessian over a mini-batch (Eq. 41 in HS97). Langevin dynamics affords a much more viable alternative; our experiments show that Entropy-SD scales well, it trains on CIFAR-10 within 15 epochs with 20 Langevin iterations.\n\n4. Our analysis that local entropy results in a smoother energy landscape is novel while we use uniform stability BE02 to obtain a precise characterization of how \\gamma affects generalization (cf. Theorem 3). HS97 use the Gibbs formalism HO91 to analyze the generalization performance which necessitates a few unrealistic assumptions (Sec. A1). The connection of Gibbs variant to averaging in the weight space (Eq. 33, pg. 22) is very similar to the Gibbs average in the gradient of our local entropy formulation.\n\n[HO91] Haussler, D and Opper M., Mutual information, metric entropy and cumulative relative entropy risk, https://projecteuclid.org/download/pdf_1/euclid.aos/1030741081\n\n[BE02] Olivier Bousquet and Andre Elisseeff. Stability and generalization. JMLR, 2002."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Entropy-SGD: Biasing Gradient Descent Into Wide Valleys", "abstract": "This paper proposes a new optimization algorithm called Entropy-SGD for training deep neural networks that is motivated by the local geometry of the energy landscape. Local extrema with low generalization error have a large proportion of almost-zero eigenvalues in the Hessian with very few positive or negative eigenvalues. We leverage upon this observation to construct a local-entropy-based objective function that favors well-generalizable solutions lying in large flat regions of the energy landscape, while avoiding poorly-generalizable solutions located in the sharp valleys. Conceptually, our algorithm resembles two nested loops of SGD where we use Langevin dynamics in the inner loop to compute the gradient of the local entropy before each update of the weights. We show that the new objective has a smoother energy landscape and show improved generalization over SGD using uniform stability, under certain assumptions. Our experiments on convolutional and recurrent neural networks demonstrate that Entropy-SGD compares favorably to state-of-the-art techniques in terms of generalization error and training time.", "pdf": "/pdf/2fb588bb07ffd17030e097202cf514bdfb5bcbca.pdf", "TL;DR": "This paper focuses on developing new optimization tools for deep learning that are tailored to exploit the local geometric properties of the objective function.", "paperhash": "chaudhari|entropysgd_biasing_gradient_descent_into_wide_valleys", "keywords": ["Deep learning", "Optimization"], "conflicts": ["ucla.edu", "cs.ucla.edu", "nyu.edu", "microsoft.com", "gwu.edu", "nyu.edu", "cims.nyu.edu", "columbia.edu", "facebook.com", "microsoft.com", "polito.it"], "authors": ["Pratik Chaudhari", "Anna Choromanska", "Stefano Soatto", "Yann LeCun", "Carlo Baldassi", "Christian Borgs", "Jennifer Chayes", "Levent Sagun", "Riccardo Zecchina"], "authorids": ["pratikac@ucla.edu", "achoroma@cims.nyu.edu", "soatto@cs.ucla.edu", "yann@cs.nyu.edu", "carlo.baldassi@polito.it", "borgs@microsoft.com", "jchayes@microsoft.com", "sagun@cims.nyu.edu", "riccardo.zecchina@polito.it"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287695367, "id": "ICLR.cc/2017/conference/-/paper185/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "B1YfAfcgl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper185/reviewers", "ICLR.cc/2017/conference/paper185/areachairs"], "cdate": 1485287695367}}}, {"tddate": null, "tmdate": 1478627683903, "tcdate": 1478627683898, "number": 1, "id": "H1nvb9k-e", "invitation": "ICLR.cc/2017/conference/-/paper185/public/comment", "forum": "B1YfAfcgl", "replyto": "B1YfAfcgl", "signatures": ["~Martin_Heusel1"], "readers": ["everyone"], "writers": ["~Martin_Heusel1"], "content": {"title": "Relation to \"Flat Minimum Search\"", "comment": "Very interesting paper. The authors propose an algorithm which moves the parameters of a neural network towards flat landscapes of the error surface while decreasing the training error. I'm wondering what are the advantages over the related method \"Flat Minima Search\" (FMS) HS97. In HS97 the authors have shown a connection between flat minima and good generalization via MDL. They suggest the FMS algorithm to find such flat regions.\n    \n\nReference:\n\n[HS97] Hochreiter, S. and Schmidhuber, J. (1997), Flat Minima.\n       http://bioinf.jku.at/publications/older/3304.pdf\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Entropy-SGD: Biasing Gradient Descent Into Wide Valleys", "abstract": "This paper proposes a new optimization algorithm called Entropy-SGD for training deep neural networks that is motivated by the local geometry of the energy landscape. Local extrema with low generalization error have a large proportion of almost-zero eigenvalues in the Hessian with very few positive or negative eigenvalues. We leverage upon this observation to construct a local-entropy-based objective function that favors well-generalizable solutions lying in large flat regions of the energy landscape, while avoiding poorly-generalizable solutions located in the sharp valleys. Conceptually, our algorithm resembles two nested loops of SGD where we use Langevin dynamics in the inner loop to compute the gradient of the local entropy before each update of the weights. We show that the new objective has a smoother energy landscape and show improved generalization over SGD using uniform stability, under certain assumptions. Our experiments on convolutional and recurrent neural networks demonstrate that Entropy-SGD compares favorably to state-of-the-art techniques in terms of generalization error and training time.", "pdf": "/pdf/2fb588bb07ffd17030e097202cf514bdfb5bcbca.pdf", "TL;DR": "This paper focuses on developing new optimization tools for deep learning that are tailored to exploit the local geometric properties of the objective function.", "paperhash": "chaudhari|entropysgd_biasing_gradient_descent_into_wide_valleys", "keywords": ["Deep learning", "Optimization"], "conflicts": ["ucla.edu", "cs.ucla.edu", "nyu.edu", "microsoft.com", "gwu.edu", "nyu.edu", "cims.nyu.edu", "columbia.edu", "facebook.com", "microsoft.com", "polito.it"], "authors": ["Pratik Chaudhari", "Anna Choromanska", "Stefano Soatto", "Yann LeCun", "Carlo Baldassi", "Christian Borgs", "Jennifer Chayes", "Levent Sagun", "Riccardo Zecchina"], "authorids": ["pratikac@ucla.edu", "achoroma@cims.nyu.edu", "soatto@cs.ucla.edu", "yann@cs.nyu.edu", "carlo.baldassi@polito.it", "borgs@microsoft.com", "jchayes@microsoft.com", "sagun@cims.nyu.edu", "riccardo.zecchina@polito.it"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287695367, "id": "ICLR.cc/2017/conference/-/paper185/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "B1YfAfcgl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper185/reviewers", "ICLR.cc/2017/conference/paper185/areachairs"], "cdate": 1485287695367}}}], "count": 34}