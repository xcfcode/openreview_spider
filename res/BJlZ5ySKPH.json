{"notes": [{"id": "BJlZ5ySKPH", "original": "HyxlFd0dvr", "number": 1867, "cdate": 1569439624964, "ddate": null, "tcdate": 1569439624964, "tmdate": 1588003314157, "tddate": null, "forum": "BJlZ5ySKPH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "U-GAT-IT: Unsupervised Generative Attentional Networks with Adaptive Layer-Instance Normalization for Image-to-Image Translation", "authors": ["Junho Kim", "Minjae Kim", "Hyeonwoo Kang", "Kwang Hee Lee"], "authorids": ["takis0112@gmail.com", "minjaekim@ncsoft.com", "hwkang0131@ncsoft.com", "lkwanghee@gmail.com"], "keywords": ["Image-to-Image Translation", "Generative Attentional Networks", "Adaptive Layer-Instance Normalization"], "abstract": "We propose a novel method for unsupervised image-to-image translation, which incorporates a new attention module and a new learnable normalization function in an end-to-end manner. The attention module guides our model to focus on more important regions distinguishing between source and target domains based on the attention map obtained by the auxiliary classifier. Unlike previous attention-based method which cannot handle the geometric changes between domains, our model can translate both images requiring holistic changes and images requiring large shape changes. Moreover, our new AdaLIN (Adaptive Layer-Instance Normalization) function helps our attention-guided model to flexibly control the amount of change in shape and texture by learned parameters depending on datasets. Experimental results show the superiority of the proposed method compared to the existing state-of-the-art models with a fixed network architecture and hyper-parameters. ", "pdf": "/pdf/c710441de8d037b36560f5bf335dcba8d3c0911f.pdf", "code": "https://github.com/taki0112/UGATIT", "paperhash": "kim|ugatit_unsupervised_generative_attentional_networks_with_adaptive_layerinstance_normalization_for_imagetoimage_translation", "_bibtex": "@inproceedings{\nKim2020U-GAT-IT:,\ntitle={U-GAT-IT: Unsupervised Generative Attentional Networks with Adaptive Layer-Instance Normalization for Image-to-Image Translation},\nauthor={Junho Kim and Minjae Kim and Hyeonwoo Kang and Kwang Hee Lee},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BJlZ5ySKPH}\n}", "original_pdf": "/attachment/0300e43650f08cc5f6409bfa4dd2d7591c651000.pdf"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "qr0-XhhOa", "original": null, "number": 1, "cdate": 1576798734507, "ddate": null, "tcdate": 1576798734507, "tmdate": 1576800901900, "tddate": null, "forum": "BJlZ5ySKPH", "replyto": "BJlZ5ySKPH", "invitation": "ICLR.cc/2020/Conference/Paper1867/-/Decision", "content": {"decision": "Accept (Poster)", "comment": "The paper proposes a new architecture for unsupervised image2image translation.\nFollowing the revision/discussion, all reviewers agree that the proposed ideas are reasonable, well described, convincingly validated, and of clear though limited novelty. Accept.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "U-GAT-IT: Unsupervised Generative Attentional Networks with Adaptive Layer-Instance Normalization for Image-to-Image Translation", "authors": ["Junho Kim", "Minjae Kim", "Hyeonwoo Kang", "Kwang Hee Lee"], "authorids": ["takis0112@gmail.com", "minjaekim@ncsoft.com", "hwkang0131@ncsoft.com", "lkwanghee@gmail.com"], "keywords": ["Image-to-Image Translation", "Generative Attentional Networks", "Adaptive Layer-Instance Normalization"], "abstract": "We propose a novel method for unsupervised image-to-image translation, which incorporates a new attention module and a new learnable normalization function in an end-to-end manner. The attention module guides our model to focus on more important regions distinguishing between source and target domains based on the attention map obtained by the auxiliary classifier. Unlike previous attention-based method which cannot handle the geometric changes between domains, our model can translate both images requiring holistic changes and images requiring large shape changes. Moreover, our new AdaLIN (Adaptive Layer-Instance Normalization) function helps our attention-guided model to flexibly control the amount of change in shape and texture by learned parameters depending on datasets. Experimental results show the superiority of the proposed method compared to the existing state-of-the-art models with a fixed network architecture and hyper-parameters. ", "pdf": "/pdf/c710441de8d037b36560f5bf335dcba8d3c0911f.pdf", "code": "https://github.com/taki0112/UGATIT", "paperhash": "kim|ugatit_unsupervised_generative_attentional_networks_with_adaptive_layerinstance_normalization_for_imagetoimage_translation", "_bibtex": "@inproceedings{\nKim2020U-GAT-IT:,\ntitle={U-GAT-IT: Unsupervised Generative Attentional Networks with Adaptive Layer-Instance Normalization for Image-to-Image Translation},\nauthor={Junho Kim and Minjae Kim and Hyeonwoo Kang and Kwang Hee Lee},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BJlZ5ySKPH}\n}", "original_pdf": "/attachment/0300e43650f08cc5f6409bfa4dd2d7591c651000.pdf"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "BJlZ5ySKPH", "replyto": "BJlZ5ySKPH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795723938, "tmdate": 1576800275501, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1867/-/Decision"}}}, {"id": "rJeTLRLCFr", "original": null, "number": 2, "cdate": 1571872340736, "ddate": null, "tcdate": 1571872340736, "tmdate": 1574358987896, "tddate": null, "forum": "BJlZ5ySKPH", "replyto": "BJlZ5ySKPH", "invitation": "ICLR.cc/2020/Conference/Paper1867/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "8: Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #1", "review": "I have read the authors' rebuttal and satisfied with their response. Novelty is a little on the lower side, but thorough writing, results, and insightful comparisons make up for this in my opinion. I have updated my score to 8: Accept.\n\n=====\n\nThis paper proposes an approach to perform image translation called U-GAT-IT. In image translation, the goal is to learn a mapping from images in a source domain to corresponding images in a target domain. Contemporary image translation approaches are able to transfer local texture but struggle to handle shape transfer. To address this concern, the authors introduce an attention mechanism based on CAM [1] and an adaptive normalization layer into a GAN-based image translation framework. Results indicate favorable quantitative and qualitative performance relative to a number of baselines.\n\nSpecific contributions include:\n* Introduction of a normalization layer called AdaLIN that can interpolate between instance normalization and layer normalization based on the input.\n* Introduction of an attention mechanism based on CAM [1] that allows the model to focus on specific parts of the image when either generating or discriminating.\n* Collection and release of a selfie-to-anime dataset.\n* Release of U-GAT-IT code.\n  \nIn my opinion this paper is borderline, leaning towards weak accept. The experiments are thorough and the paper is well-written. I have concerns about the novelty and significance of the work, but overall the paper feels very close to being a finished piece of work in spite of its (relatively minor) flaws.\n  \nStrong points of this work include the writing and experiments. The paper is clearly organized and feels polished. It cites many relevant works, giving the reader a sense of the contemporary approaches for image translation. There is a thorough description of model architecture, dataset and tuning parameters in the appendix. In addition, code and the selfie-to-anime dataset have been released by the authors. In terms of experiments, the authors provide many qualitative visualizations comparing the proposed model to baselines on various datasets. Quantitative evaluation includes KID and a perceptual evaluation on human subjects.\n\nWeak points include novelty and significance. The proposed approach combines two ideas already applied to image translation (adaptive normalization [3] and attention [4]).  It therefore synthesizes these ideas into an effective algorithm rather than directly adding something new. It is unclear to me how others can build on top of this work to further advance state-of-the-art in image translation. Are more sophisticated normalization and attention mechanisms truly the key to improving image translation in the future?\n\nSpecific comments:\n* The formulation of AdaLIN in Equation (1) is vague. The text states \"parameters are dynamically computed by a fully connected layer from the attention map\", but it's not clear what those parameters are in the equation. Explicitly writing \\gamma and \\beta as functions of the fully-connected layer and \\mu_I, \\sigma_I, \\mu_L, \\sigma_L as the corresponding mean and standard deviation expressions would make things more clear.\n* The motivation for using layer normalization was discussed in 2.1.1 but I still do not understand why it is beneficial.\n* The term \"importance weights\" has a specific meaning in the context of Monte Carlo methods. I would suggest choosing a different term here.\n\nQuestions for the authors:\n* How does U-GAT-IT compare to TransGaGa [2]? One of the stated goals of U-GAT-IT is to better handle shape when performing image translation. TransGaGa has a similar motivation and so I would have liked to see an experimental comparison or at the very least a description of how U-GAT-IT differs. What sorts of shape transfer could U-GAT-IT handle that TransGaGa couldn't and vice versa?\n* What are the shortcomings of the model and how could they possibly be addressed? \n  \n[1] Zhou, B., Khosla, A., Lapedriza, A., Oliva, A. and Torralba, A., 2016. Learning deep features for discriminative localization. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 2921-2929).\n[2] Wu, W., Cao, K., Li, C., Qian, C. and Loy, C.C., 2019. Transgaga: Geometry-aware unsupervised image-to-image translation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 8012-8021).\n[3] Huang, X. and Belongie, S., 2017. Arbitrary style transfer in real-time with adaptive instance normalization. In Proceedings of the IEEE International Conference on Computer Vision (pp. 1501-1510).\n[4] Mejjati, Y.A., Richardt, C., Tompkin, J., Cosker, D. and Kim, K.I., 2018. Unsupervised attention-guided image-to-image translation. In Advances in Neural Information Processing Systems (pp. 3693-3703).", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper1867/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1867/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "U-GAT-IT: Unsupervised Generative Attentional Networks with Adaptive Layer-Instance Normalization for Image-to-Image Translation", "authors": ["Junho Kim", "Minjae Kim", "Hyeonwoo Kang", "Kwang Hee Lee"], "authorids": ["takis0112@gmail.com", "minjaekim@ncsoft.com", "hwkang0131@ncsoft.com", "lkwanghee@gmail.com"], "keywords": ["Image-to-Image Translation", "Generative Attentional Networks", "Adaptive Layer-Instance Normalization"], "abstract": "We propose a novel method for unsupervised image-to-image translation, which incorporates a new attention module and a new learnable normalization function in an end-to-end manner. The attention module guides our model to focus on more important regions distinguishing between source and target domains based on the attention map obtained by the auxiliary classifier. Unlike previous attention-based method which cannot handle the geometric changes between domains, our model can translate both images requiring holistic changes and images requiring large shape changes. Moreover, our new AdaLIN (Adaptive Layer-Instance Normalization) function helps our attention-guided model to flexibly control the amount of change in shape and texture by learned parameters depending on datasets. Experimental results show the superiority of the proposed method compared to the existing state-of-the-art models with a fixed network architecture and hyper-parameters. ", "pdf": "/pdf/c710441de8d037b36560f5bf335dcba8d3c0911f.pdf", "code": "https://github.com/taki0112/UGATIT", "paperhash": "kim|ugatit_unsupervised_generative_attentional_networks_with_adaptive_layerinstance_normalization_for_imagetoimage_translation", "_bibtex": "@inproceedings{\nKim2020U-GAT-IT:,\ntitle={U-GAT-IT: Unsupervised Generative Attentional Networks with Adaptive Layer-Instance Normalization for Image-to-Image Translation},\nauthor={Junho Kim and Minjae Kim and Hyeonwoo Kang and Kwang Hee Lee},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BJlZ5ySKPH}\n}", "original_pdf": "/attachment/0300e43650f08cc5f6409bfa4dd2d7591c651000.pdf"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BJlZ5ySKPH", "replyto": "BJlZ5ySKPH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1867/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1867/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575657945283, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1867/Reviewers"], "noninvitees": [], "tcdate": 1570237731157, "tmdate": 1575657945302, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1867/-/Official_Review"}}}, {"id": "SJl2YNBMjB", "original": null, "number": 3, "cdate": 1573176451859, "ddate": null, "tcdate": 1573176451859, "tmdate": 1573216981119, "tddate": null, "forum": "BJlZ5ySKPH", "replyto": "B1g6gslaFr", "invitation": "ICLR.cc/2020/Conference/Paper1867/-/Official_Comment", "content": {"title": "Response to Reviewer #3", "comment": "Thank you for the valuable comments and constructive feedback.  In the revised draft, we mark our major revisions by \u201cviolet\u201d , and would like to answer the reviewer\u2019s questions as follows:\n\n1. Description for CAM\n\nAs you suggested, in the revised draft, we add the related works including the description for CAM in Appendix A.\n\n2. The local and global discriminators\n\nAs you suggested, in the revised draft, we add the description for the multi-scale discriminator in Sec. 2.1.2.\n\n3. Result without CAM (Figure2(f))\n\nTo make sure that the effect of the CAM, we have all set the same hyper-parameters and retrained the model.\n\n4. The generator model architecture  (Figure 1)\n\nYou're right. We will modify the figure to make it appear that the encoder feature maps are fed into the adaptive residual blocks.\n\n\n5. Why not using GN? \n\nWe thought GN was theoretically an intermediate version of IN and LN. Therefore, the GN can be properly expressed by the /rho value. The selection was based on whether the result would be closer to the target domain or more biased toward the source domain rather than the naturality of the results. In Figure 3 (f), you can see more textures from the background of the source domain.\n\n6. Ablation Study\n\nAs shown in Table 1, it includes the ablation study for generator and discriminator separately with KID. If space is allowed, we will add the image results.\n\n7. Discussion on the attention mechanism compared with other related works\n\nOur experiment results already are including the result of AGGAN[1] and discussed about that. \n[1] Unsupervised-Attention-guided-Image-to-Image-Translation. NIPS\u201918"}, "signatures": ["ICLR.cc/2020/Conference/Paper1867/Authors"], "readers": ["everyone", "ICLR.cc/2020/Conference/Paper1867/Reviewers/Submitted"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1867/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "U-GAT-IT: Unsupervised Generative Attentional Networks with Adaptive Layer-Instance Normalization for Image-to-Image Translation", "authors": ["Junho Kim", "Minjae Kim", "Hyeonwoo Kang", "Kwang Hee Lee"], "authorids": ["takis0112@gmail.com", "minjaekim@ncsoft.com", "hwkang0131@ncsoft.com", "lkwanghee@gmail.com"], "keywords": ["Image-to-Image Translation", "Generative Attentional Networks", "Adaptive Layer-Instance Normalization"], "abstract": "We propose a novel method for unsupervised image-to-image translation, which incorporates a new attention module and a new learnable normalization function in an end-to-end manner. The attention module guides our model to focus on more important regions distinguishing between source and target domains based on the attention map obtained by the auxiliary classifier. Unlike previous attention-based method which cannot handle the geometric changes between domains, our model can translate both images requiring holistic changes and images requiring large shape changes. Moreover, our new AdaLIN (Adaptive Layer-Instance Normalization) function helps our attention-guided model to flexibly control the amount of change in shape and texture by learned parameters depending on datasets. Experimental results show the superiority of the proposed method compared to the existing state-of-the-art models with a fixed network architecture and hyper-parameters. ", "pdf": "/pdf/c710441de8d037b36560f5bf335dcba8d3c0911f.pdf", "code": "https://github.com/taki0112/UGATIT", "paperhash": "kim|ugatit_unsupervised_generative_attentional_networks_with_adaptive_layerinstance_normalization_for_imagetoimage_translation", "_bibtex": "@inproceedings{\nKim2020U-GAT-IT:,\ntitle={U-GAT-IT: Unsupervised Generative Attentional Networks with Adaptive Layer-Instance Normalization for Image-to-Image Translation},\nauthor={Junho Kim and Minjae Kim and Hyeonwoo Kang and Kwang Hee Lee},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BJlZ5ySKPH}\n}", "original_pdf": "/attachment/0300e43650f08cc5f6409bfa4dd2d7591c651000.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJlZ5ySKPH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1867/Authors", "ICLR.cc/2020/Conference/Paper1867/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1867/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1867/Reviewers", "ICLR.cc/2020/Conference/Paper1867/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1867/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1867/Authors|ICLR.cc/2020/Conference/Paper1867/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504149736, "tmdate": 1576860540673, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1867/Authors", "ICLR.cc/2020/Conference/Paper1867/Reviewers", "ICLR.cc/2020/Conference/Paper1867/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1867/-/Official_Comment"}}}, {"id": "BklxmksZjB", "original": null, "number": 2, "cdate": 1573134104286, "ddate": null, "tcdate": 1573134104286, "tmdate": 1573216375547, "tddate": null, "forum": "BJlZ5ySKPH", "replyto": "rJeTLRLCFr", "invitation": "ICLR.cc/2020/Conference/Paper1867/-/Official_Comment", "content": {"title": "Response to Reviewer #1", "comment": "We thank the reviewer for the valuable comments and constructive feedback.  In the revised draft, we mark our major revisions by \u201cviolet\u201d , and would like to answer the reviewer\u2019s questions as follows: \n \n*About Novelty:\n\n1. Attention mechanism \n\nAlthough we proposed similar attention concept, the goal and how to generate are different. Previous attention-based work [1] do not allow to transform the shape of the instance because of attaching the background to the (translated) cropped instances. Unlike these works, we assumed that our model will guide to focus on more important regions and ignore minor regions by distinguishing between the target and not-target domains based on the importance map obtained by the auxiliary classifier. These attention maps are embedded into the generator and discriminator to focus on semantically important areas, thus facilitating the shape transformation. The attention map in the generator induces focus on areas that specifically distinguish between the two domains. The attention map in discriminator helps fine-tuning by focusing on the difference between real image and fake image in target domain.\n\n2. Normalization \n\nAdaLIN tells the model how much it should transform. Instance Norm (IN) is capable of preserving the characteristics of source image. Layer Norm (LN) which uses layer-wise feature statistics is better at transforming to target domain. We found that combining advantages of both IN and LN is beneficial to image-to-image translation task in various datasets by controlling the amount of transform. Though its idea borrows from previous work [2], the proposed method is the first attempt to combine IN and LN in image-to-image translation task as far as we investigated.\n\n[1] Y. Alami Mejjati, C. Richardt, J. Tompkin, D. Cosker, and K. I. Kim. Unsupervised attention-guided image-to-image translation. In NIPS. 2018. \n[2] H. Nam and H.-E. Kim. Batch-instance normalization for adaptively style-invariant neural networks. In NIPS, 2018. \n\n*Specific Comments:\n\n1. The formulation of AdaLIN in Equation (1)\n\nI agree with the comment from the Reviewer. In the revised draft, I modify like  \"parameters, /gamma and /beta are dynamically computed by a fully connected layer from the attention map\" in sec 2.1.1.\n\n2.  The motivation for using layer normalization \n\nWe assumed that optimal stylization method was \"whitening and Coloring Transform\". However, the computational cost is high due to the calculation of the covariance matrix and matrix inverse. To compensate between the computational cost and the quality of the result, we borrowed two sub-optimal normalization methods, AdaIN and Layer Normalization. During stylization, while the AdaIN has the characteristics keeping more contents information, the Layer Normalization tends to make stylization more obvious instead keeping content information less.\n\n3. The term \"important weights\"\n\nIn the revised draft, we modify it to \"the weight of the k-th feature map for the source domain\"  in sec 2.1.1.\n\n*Questions for the authors:\n\n1. U-GAT-IT vs TransGaGa\n\nThe goal of U-GAT-IT is to change the shape of the foreground while maintaining the content of the background. TransGaGa deal with the geometry and appearance separately and fully converts the appearance of the source domain into that of the target domain without considering the foreground and background. Therefore, as can be seen from the experimental results, the background of the source image is not maintained at all. However, U-GAT-IT can maintain or change the contents of the source domain adaptively through attention. In addition, U-GAT-IT can achieve good results in style transfer as well as shape change through AdaLIN. Therefore, we think U-GAT-IT is a more generalized version than TransGaGa.\n\n2. What are the shortcomings of the model and how could they possibly be addressed?\n\nThe shortcomings for our model is \"one-to-one mapping\". But we will design UGATIT with our future work to be multi-modal and multi-domain together."}, "signatures": ["ICLR.cc/2020/Conference/Paper1867/Authors"], "readers": ["everyone", "ICLR.cc/2020/Conference/Paper1867/Reviewers/Submitted"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1867/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "U-GAT-IT: Unsupervised Generative Attentional Networks with Adaptive Layer-Instance Normalization for Image-to-Image Translation", "authors": ["Junho Kim", "Minjae Kim", "Hyeonwoo Kang", "Kwang Hee Lee"], "authorids": ["takis0112@gmail.com", "minjaekim@ncsoft.com", "hwkang0131@ncsoft.com", "lkwanghee@gmail.com"], "keywords": ["Image-to-Image Translation", "Generative Attentional Networks", "Adaptive Layer-Instance Normalization"], "abstract": "We propose a novel method for unsupervised image-to-image translation, which incorporates a new attention module and a new learnable normalization function in an end-to-end manner. The attention module guides our model to focus on more important regions distinguishing between source and target domains based on the attention map obtained by the auxiliary classifier. Unlike previous attention-based method which cannot handle the geometric changes between domains, our model can translate both images requiring holistic changes and images requiring large shape changes. Moreover, our new AdaLIN (Adaptive Layer-Instance Normalization) function helps our attention-guided model to flexibly control the amount of change in shape and texture by learned parameters depending on datasets. Experimental results show the superiority of the proposed method compared to the existing state-of-the-art models with a fixed network architecture and hyper-parameters. ", "pdf": "/pdf/c710441de8d037b36560f5bf335dcba8d3c0911f.pdf", "code": "https://github.com/taki0112/UGATIT", "paperhash": "kim|ugatit_unsupervised_generative_attentional_networks_with_adaptive_layerinstance_normalization_for_imagetoimage_translation", "_bibtex": "@inproceedings{\nKim2020U-GAT-IT:,\ntitle={U-GAT-IT: Unsupervised Generative Attentional Networks with Adaptive Layer-Instance Normalization for Image-to-Image Translation},\nauthor={Junho Kim and Minjae Kim and Hyeonwoo Kang and Kwang Hee Lee},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BJlZ5ySKPH}\n}", "original_pdf": "/attachment/0300e43650f08cc5f6409bfa4dd2d7591c651000.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJlZ5ySKPH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1867/Authors", "ICLR.cc/2020/Conference/Paper1867/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1867/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1867/Reviewers", "ICLR.cc/2020/Conference/Paper1867/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1867/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1867/Authors|ICLR.cc/2020/Conference/Paper1867/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504149736, "tmdate": 1576860540673, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1867/Authors", "ICLR.cc/2020/Conference/Paper1867/Reviewers", "ICLR.cc/2020/Conference/Paper1867/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1867/-/Official_Comment"}}}, {"id": "SkgVvj-ZiS", "original": null, "number": 1, "cdate": 1573096284377, "ddate": null, "tcdate": 1573096284377, "tmdate": 1573176493970, "tddate": null, "forum": "BJlZ5ySKPH", "replyto": "Syg1daPM9B", "invitation": "ICLR.cc/2020/Conference/Paper1867/-/Official_Comment", "content": {"title": "Response to Reviewer #2", "comment": "We thank the reviewer for the valuable comments and constructive feedback , and would like to answer the reviewer\u2019s questions as follows:\nThe perceptual study was conducted on 153 participants of an AI community, which includes a mix of experts and non-specialists."}, "signatures": ["ICLR.cc/2020/Conference/Paper1867/Authors"], "readers": ["everyone", "ICLR.cc/2020/Conference/Paper1867/Reviewers/Submitted"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1867/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "U-GAT-IT: Unsupervised Generative Attentional Networks with Adaptive Layer-Instance Normalization for Image-to-Image Translation", "authors": ["Junho Kim", "Minjae Kim", "Hyeonwoo Kang", "Kwang Hee Lee"], "authorids": ["takis0112@gmail.com", "minjaekim@ncsoft.com", "hwkang0131@ncsoft.com", "lkwanghee@gmail.com"], "keywords": ["Image-to-Image Translation", "Generative Attentional Networks", "Adaptive Layer-Instance Normalization"], "abstract": "We propose a novel method for unsupervised image-to-image translation, which incorporates a new attention module and a new learnable normalization function in an end-to-end manner. The attention module guides our model to focus on more important regions distinguishing between source and target domains based on the attention map obtained by the auxiliary classifier. Unlike previous attention-based method which cannot handle the geometric changes between domains, our model can translate both images requiring holistic changes and images requiring large shape changes. Moreover, our new AdaLIN (Adaptive Layer-Instance Normalization) function helps our attention-guided model to flexibly control the amount of change in shape and texture by learned parameters depending on datasets. Experimental results show the superiority of the proposed method compared to the existing state-of-the-art models with a fixed network architecture and hyper-parameters. ", "pdf": "/pdf/c710441de8d037b36560f5bf335dcba8d3c0911f.pdf", "code": "https://github.com/taki0112/UGATIT", "paperhash": "kim|ugatit_unsupervised_generative_attentional_networks_with_adaptive_layerinstance_normalization_for_imagetoimage_translation", "_bibtex": "@inproceedings{\nKim2020U-GAT-IT:,\ntitle={U-GAT-IT: Unsupervised Generative Attentional Networks with Adaptive Layer-Instance Normalization for Image-to-Image Translation},\nauthor={Junho Kim and Minjae Kim and Hyeonwoo Kang and Kwang Hee Lee},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BJlZ5ySKPH}\n}", "original_pdf": "/attachment/0300e43650f08cc5f6409bfa4dd2d7591c651000.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJlZ5ySKPH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1867/Authors", "ICLR.cc/2020/Conference/Paper1867/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1867/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1867/Reviewers", "ICLR.cc/2020/Conference/Paper1867/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1867/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1867/Authors|ICLR.cc/2020/Conference/Paper1867/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504149736, "tmdate": 1576860540673, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1867/Authors", "ICLR.cc/2020/Conference/Paper1867/Reviewers", "ICLR.cc/2020/Conference/Paper1867/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1867/-/Official_Comment"}}}, {"id": "B1g6gslaFr", "original": null, "number": 1, "cdate": 1571781365017, "ddate": null, "tcdate": 1571781365017, "tmdate": 1572972413309, "tddate": null, "forum": "BJlZ5ySKPH", "replyto": "BJlZ5ySKPH", "invitation": "ICLR.cc/2020/Conference/Paper1867/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a new attention mechanism for unsupervised image-to-image translation task. The proposed attention mechanism consists of an attention module and a learnable normalization function. Sufficient experiments and analysis are done on five datasets.  \n\nPros:\n1. The proposed method seems to generalize well to the different datasets with the same network architecture and hyper-parameters compared to previous works. This could benefit other researchers who want to apply the method to other data or tasks.\n2. The translated results seem more semantic consistent with the source image compared to other methods, although the sores are not the top on photo2portrait and photo2vangogh. The results also look more pleasing.\n\nCons:\n1. The CAM loss is one of the key components in the proposed method. However, there is only the reference and no detailed description in the paper. More intuitive descriptions are necessary for easy understanding.\n2. The local and global discriminators are not explained until the result analysis. It\u2019s a bit confusing when I see the local and global attention maps visualization results. It\u2019s better to mention it in the method section.\n3. I wonder why some translations are not done at all in the results without CAM in Figure 2(f). Because without CAM, the framework would be somehow similar to MUNIT or DRIT. I suppose the hyper-parameters are not suitable for this setting.\n4. The generator model architecture in Figure 1 is confusing. The adaptive residual blocks only receive the gamma and beta parameters. I suppose that the encoder feature maps are also fed into the adaptive residual blocks.\n5. In Figure 3, the comparison of the results using each normalization function is reported. While in my view, the results only using GN in decoder with CAM looks more natural. I wonder why the proposed method only consists of instance norm and layer norm? I suppose the group norm might help with the predefined group.\n6. In the ablation study, the CAM is evaluated for generator and discriminator together. I would recommend doing this ablation study for generator and discriminator separately to see if it\u2019s necessary for generator or discriminator.\n7. It would be good to see some discussion on the attention mechanism compared with other related works. For example,  [a,b] predict the attention masks for unsupervised I2I, but applies them on the pixel/feature spatial level to keep the semantic consistency.\n[a] Unsupervised-Attention-guided-Image-to-Image-Translation. NIPS\u201918\n[a] Exemplar guided unsupervised image-to-image translation with semantic consistency. ICLR\u201919\n\nMy initial rating is above boardline."}, "signatures": ["ICLR.cc/2020/Conference/Paper1867/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1867/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "U-GAT-IT: Unsupervised Generative Attentional Networks with Adaptive Layer-Instance Normalization for Image-to-Image Translation", "authors": ["Junho Kim", "Minjae Kim", "Hyeonwoo Kang", "Kwang Hee Lee"], "authorids": ["takis0112@gmail.com", "minjaekim@ncsoft.com", "hwkang0131@ncsoft.com", "lkwanghee@gmail.com"], "keywords": ["Image-to-Image Translation", "Generative Attentional Networks", "Adaptive Layer-Instance Normalization"], "abstract": "We propose a novel method for unsupervised image-to-image translation, which incorporates a new attention module and a new learnable normalization function in an end-to-end manner. The attention module guides our model to focus on more important regions distinguishing between source and target domains based on the attention map obtained by the auxiliary classifier. Unlike previous attention-based method which cannot handle the geometric changes between domains, our model can translate both images requiring holistic changes and images requiring large shape changes. Moreover, our new AdaLIN (Adaptive Layer-Instance Normalization) function helps our attention-guided model to flexibly control the amount of change in shape and texture by learned parameters depending on datasets. Experimental results show the superiority of the proposed method compared to the existing state-of-the-art models with a fixed network architecture and hyper-parameters. ", "pdf": "/pdf/c710441de8d037b36560f5bf335dcba8d3c0911f.pdf", "code": "https://github.com/taki0112/UGATIT", "paperhash": "kim|ugatit_unsupervised_generative_attentional_networks_with_adaptive_layerinstance_normalization_for_imagetoimage_translation", "_bibtex": "@inproceedings{\nKim2020U-GAT-IT:,\ntitle={U-GAT-IT: Unsupervised Generative Attentional Networks with Adaptive Layer-Instance Normalization for Image-to-Image Translation},\nauthor={Junho Kim and Minjae Kim and Hyeonwoo Kang and Kwang Hee Lee},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BJlZ5ySKPH}\n}", "original_pdf": "/attachment/0300e43650f08cc5f6409bfa4dd2d7591c651000.pdf"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BJlZ5ySKPH", "replyto": "BJlZ5ySKPH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1867/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1867/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575657945283, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1867/Reviewers"], "noninvitees": [], "tcdate": 1570237731157, "tmdate": 1575657945302, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1867/-/Official_Review"}}}, {"id": "Syg1daPM9B", "original": null, "number": 3, "cdate": 1572138343289, "ddate": null, "tcdate": 1572138343289, "tmdate": 1572972413265, "tddate": null, "forum": "BJlZ5ySKPH", "replyto": "BJlZ5ySKPH", "invitation": "ICLR.cc/2020/Conference/Paper1867/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summarize what the paper claims to do/contribute.\n* The paper proposes a new image-to-image GAN-based translator that uses attention and a new normalization that learns a proper ratio between instance and layer normalization. Experiments benchmark the new method against multiple prior ones, and on a number of dataset pairs.\n\nClearly state your decision (accept or reject) with one or two key reasons for this choice.\nWeak Accept\n\n* The paper was well-written and the method and contributions are clearly explained.\n* There is clear novelty in this paper, even if slightly limited. However, the newly proposed normalization seems to work quite well.\n* The results look good, however it is hard to compare methods quantitatively with only few samples. (Nothing that the authors could have done: there are many samples in the supplementary material and results seem consistent.) Qualitative measures like FID and KID should be taken with a grain of salt also. It is a big plus that a user study was conducted! (However, details of how these subjects were selected would be useful)"}, "signatures": ["ICLR.cc/2020/Conference/Paper1867/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1867/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "U-GAT-IT: Unsupervised Generative Attentional Networks with Adaptive Layer-Instance Normalization for Image-to-Image Translation", "authors": ["Junho Kim", "Minjae Kim", "Hyeonwoo Kang", "Kwang Hee Lee"], "authorids": ["takis0112@gmail.com", "minjaekim@ncsoft.com", "hwkang0131@ncsoft.com", "lkwanghee@gmail.com"], "keywords": ["Image-to-Image Translation", "Generative Attentional Networks", "Adaptive Layer-Instance Normalization"], "abstract": "We propose a novel method for unsupervised image-to-image translation, which incorporates a new attention module and a new learnable normalization function in an end-to-end manner. The attention module guides our model to focus on more important regions distinguishing between source and target domains based on the attention map obtained by the auxiliary classifier. Unlike previous attention-based method which cannot handle the geometric changes between domains, our model can translate both images requiring holistic changes and images requiring large shape changes. Moreover, our new AdaLIN (Adaptive Layer-Instance Normalization) function helps our attention-guided model to flexibly control the amount of change in shape and texture by learned parameters depending on datasets. Experimental results show the superiority of the proposed method compared to the existing state-of-the-art models with a fixed network architecture and hyper-parameters. ", "pdf": "/pdf/c710441de8d037b36560f5bf335dcba8d3c0911f.pdf", "code": "https://github.com/taki0112/UGATIT", "paperhash": "kim|ugatit_unsupervised_generative_attentional_networks_with_adaptive_layerinstance_normalization_for_imagetoimage_translation", "_bibtex": "@inproceedings{\nKim2020U-GAT-IT:,\ntitle={U-GAT-IT: Unsupervised Generative Attentional Networks with Adaptive Layer-Instance Normalization for Image-to-Image Translation},\nauthor={Junho Kim and Minjae Kim and Hyeonwoo Kang and Kwang Hee Lee},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BJlZ5ySKPH}\n}", "original_pdf": "/attachment/0300e43650f08cc5f6409bfa4dd2d7591c651000.pdf"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BJlZ5ySKPH", "replyto": "BJlZ5ySKPH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1867/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1867/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575657945283, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1867/Reviewers"], "noninvitees": [], "tcdate": 1570237731157, "tmdate": 1575657945302, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1867/-/Official_Review"}}}, {"id": "HJeuDHzfYr", "original": null, "number": 1, "cdate": 1571067232388, "ddate": null, "tcdate": 1571067232388, "tmdate": 1571069509397, "tddate": null, "forum": "BJlZ5ySKPH", "replyto": "BJlZ5ySKPH", "invitation": "ICLR.cc/2020/Conference/Paper1867/-/Public_Comment", "content": {"comment": "[NOTE BY PROGRAM CHAIR: the urls were redacted to preserve the anonymity. this submission does *not* violate the ICLR's policy on double blind reviewing. see https://iclr.cc/Conferences/2020/CallForPapers which states \"... papers that have appeared on non-peered reviewed websites (like arXiv) or that have been presented at workshops (i.e., venues that do not have a publication proceedings) do not violate the policy. The policy is enforced during the whole reviewing process period. Submission of the paper to archival repositories such as arXiv are allowed.\"]\n\nIt seems like double-blind review is violated. This paper was published a long time ago (25 July 2019).\n\n[URL REDACTED]\n\nThere are all author names at the top of the paper, their affiliation, etc.\nFurthermore, there is an official implementation of this model on GitHub for both Tensorflow (4k stars) and Pytorch (1.3k stars) frameworks of the first author of the paper [REDACTED] and his co-author [REDACTED].\n\n[URL REDACTED] \n[URL REDACTED]", "title": "[REDACTED BY PROGRAM CHAIR] Double-blind review is violated"}, "signatures": ["~Kyunghyun_Cho1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Kyunghyun_Cho1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "U-GAT-IT: Unsupervised Generative Attentional Networks with Adaptive Layer-Instance Normalization for Image-to-Image Translation", "authors": ["Junho Kim", "Minjae Kim", "Hyeonwoo Kang", "Kwang Hee Lee"], "authorids": ["takis0112@gmail.com", "minjaekim@ncsoft.com", "hwkang0131@ncsoft.com", "lkwanghee@gmail.com"], "keywords": ["Image-to-Image Translation", "Generative Attentional Networks", "Adaptive Layer-Instance Normalization"], "abstract": "We propose a novel method for unsupervised image-to-image translation, which incorporates a new attention module and a new learnable normalization function in an end-to-end manner. The attention module guides our model to focus on more important regions distinguishing between source and target domains based on the attention map obtained by the auxiliary classifier. Unlike previous attention-based method which cannot handle the geometric changes between domains, our model can translate both images requiring holistic changes and images requiring large shape changes. Moreover, our new AdaLIN (Adaptive Layer-Instance Normalization) function helps our attention-guided model to flexibly control the amount of change in shape and texture by learned parameters depending on datasets. Experimental results show the superiority of the proposed method compared to the existing state-of-the-art models with a fixed network architecture and hyper-parameters. ", "pdf": "/pdf/c710441de8d037b36560f5bf335dcba8d3c0911f.pdf", "code": "https://github.com/taki0112/UGATIT", "paperhash": "kim|ugatit_unsupervised_generative_attentional_networks_with_adaptive_layerinstance_normalization_for_imagetoimage_translation", "_bibtex": "@inproceedings{\nKim2020U-GAT-IT:,\ntitle={U-GAT-IT: Unsupervised Generative Attentional Networks with Adaptive Layer-Instance Normalization for Image-to-Image Translation},\nauthor={Junho Kim and Minjae Kim and Hyeonwoo Kang and Kwang Hee Lee},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BJlZ5ySKPH}\n}", "original_pdf": "/attachment/0300e43650f08cc5f6409bfa4dd2d7591c651000.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJlZ5ySKPH", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504188657, "tmdate": 1576860574187, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper1867/Authors", "ICLR.cc/2020/Conference/Paper1867/Reviewers", "ICLR.cc/2020/Conference/Paper1867/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1867/-/Public_Comment"}}}], "count": 9}