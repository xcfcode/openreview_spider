{"notes": [{"tddate": null, "replyto": null, "nonreaders": null, "ddate": null, "tmdate": 1511031923563, "tcdate": 1487340372472, "number": 88, "id": "H1hLmF4Fx", "invitation": "ICLR.cc/2017/workshop/-/submission", "forum": "H1hLmF4Fx", "signatures": ["~Takeru_Miyato1"], "readers": ["everyone"], "content": {"TL;DR": "", "title": "Synthetic Gradient Methods with Virtual Forward-Backward Networks", "abstract": "The concept of synthetic gradient introduced by Jaderberg et al. (2016) provides an avant-garde framework for asynchronous learning of neural network.\nTheir model, however, has a weakness in its construction, because the structure of their synthetic gradient has little relation to the objective function of the target task.\nIn this paper we introduce virtual forward-backward networks (VFBN). \nVFBN is a model that produces synthetic gradient whose structure is analogous to the actual gradient of the objective function.\nVFBN is the first of its kind that succeeds in decoupling deep networks like ResNet-110 (He et al., 2016) without compromising its performance.", "pdf": "/pdf/6a1d60d2690cf164e98aedd28eb5e74ff7931d9c.pdf", "paperhash": "miyato|synthetic_gradient_methods_with_virtual_forwardbackward_networks", "conflicts": ["preferred.jp", "atr.jp", "kyoto-u.ac.jp", "ritsumei.ac.jp"], "keywords": ["Deep learning", "Optimization"], "authors": ["Takeru Miyato", "Daisuke Okanohara", "Shin-ichi Maeda", "Masanori Koyama"], "authorids": ["miyato@preferred.jp", "hillbig@preferred.jp", "ichi@sys.i.kyoto-u.ac.jp", "mkoyama@fc.ritsumei.ac.jp"]}, "writers": [], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1487690420000, "tmdate": 1484242559574, "id": "ICLR.cc/2017/workshop/-/submission", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1495466420000, "cdate": 1484242559574}}}, {"tddate": null, "ddate": null, "cdate": null, "original": null, "tmdate": 1490028591089, "tcdate": 1490028591089, "number": 1, "id": "r1PEdKaie", "invitation": "ICLR.cc/2017/workshop/-/paper88/acceptance", "forum": "H1hLmF4Fx", "replyto": "H1hLmF4Fx", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Accept", "title": "ICLR committee final decision"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Synthetic Gradient Methods with Virtual Forward-Backward Networks", "abstract": "The concept of synthetic gradient introduced by Jaderberg et al. (2016) provides an avant-garde framework for asynchronous learning of neural network.\nTheir model, however, has a weakness in its construction, because the structure of their synthetic gradient has little relation to the objective function of the target task.\nIn this paper we introduce virtual forward-backward networks (VFBN). \nVFBN is a model that produces synthetic gradient whose structure is analogous to the actual gradient of the objective function.\nVFBN is the first of its kind that succeeds in decoupling deep networks like ResNet-110 (He et al., 2016) without compromising its performance.", "pdf": "/pdf/6a1d60d2690cf164e98aedd28eb5e74ff7931d9c.pdf", "paperhash": "miyato|synthetic_gradient_methods_with_virtual_forwardbackward_networks", "conflicts": ["preferred.jp", "atr.jp", "kyoto-u.ac.jp", "ritsumei.ac.jp"], "keywords": ["Deep learning", "Optimization"], "authors": ["Takeru Miyato", "Daisuke Okanohara", "Shin-ichi Maeda", "Masanori Koyama"], "authorids": ["miyato@preferred.jp", "hillbig@preferred.jp", "ichi@sys.i.kyoto-u.ac.jp", "mkoyama@fc.ritsumei.ac.jp"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1490028591621, "id": "ICLR.cc/2017/workshop/-/paper88/acceptance", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "H1hLmF4Fx", "replyto": "H1hLmF4Fx", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept", "Reject"]}}}, "nonreaders": [], "cdate": 1490028591621}}}, {"tddate": null, "nonreaders": null, "tmdate": 1489248646548, "tcdate": 1489208477786, "number": 1, "id": "HJ8sVb-ox", "invitation": "ICLR.cc/2017/workshop/-/paper88/public/comment", "forum": "H1hLmF4Fx", "replyto": "H1hLmF4Fx", "signatures": ["~Takeru_Miyato1"], "readers": ["everyone"], "writers": ["~Takeru_Miyato1"], "content": {"title": "Response to reviewers", "comment": "We thank the reviewers for reading our manuscript and for comments and suggestions. \nAs for the quality of the writing, we proofread the manuscript again and corrected grammatical mistakes, and uploaded the revised manuscript. Also, the details of the architecture of the proposed model and the baseline can be found in Appendix section.\n\nMeanwhile, we completely agree with both reviewers on the point that we need to more quantitatively evaluate the computation times of the algorithms.  As of now, we are performing additional experiments to clock the runtime of the algorithms, and we plan to present the result of this comparative study at the workshop. \n\nStill yet, we would like to emphasize that the structure of VFBN is not so complicated despite the appearance of its formulation.  \nOur VFBN and Jaderberg\u2019s model differs only in the construction of $\\delta$, which is a function of $h$ and $y$. Jaderberg\u2019s model computes $\\delta$ with a function that does not impose any specific relation between hidden variable $h$ and the output $y$.  Our VFBN, on the other hand, respects the relation that $h$ is an intermediary input and $y$ is the output.    \n\nThe computation time largely depends on how we define the VFBN, and the choice of its  structure is up to the user. Needless to say, however, in order to enjoy the merit of decoupling, we need to make VFBN very simple relative to the target network we try to optimize.  For our experiment with 110 layer-Resnet, we used 5 layer-virtual forward network and its corresponding backward network. We would like to note in advance that the computational cost of training these mini networks are negligible when compared to the training of the full network. \n\nWe also agree with the reviewer that we need to experiment with decouplings at multiple locations as well. Unfortunately, however, we could not conduct this additional experiment in the given time frame, and we plan to conduct additional experiments in the future study.  "}, "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Synthetic Gradient Methods with Virtual Forward-Backward Networks", "abstract": "The concept of synthetic gradient introduced by Jaderberg et al. (2016) provides an avant-garde framework for asynchronous learning of neural network.\nTheir model, however, has a weakness in its construction, because the structure of their synthetic gradient has little relation to the objective function of the target task.\nIn this paper we introduce virtual forward-backward networks (VFBN). \nVFBN is a model that produces synthetic gradient whose structure is analogous to the actual gradient of the objective function.\nVFBN is the first of its kind that succeeds in decoupling deep networks like ResNet-110 (He et al., 2016) without compromising its performance.", "pdf": "/pdf/6a1d60d2690cf164e98aedd28eb5e74ff7931d9c.pdf", "paperhash": "miyato|synthetic_gradient_methods_with_virtual_forwardbackward_networks", "conflicts": ["preferred.jp", "atr.jp", "kyoto-u.ac.jp", "ritsumei.ac.jp"], "keywords": ["Deep learning", "Optimization"], "authors": ["Takeru Miyato", "Daisuke Okanohara", "Shin-ichi Maeda", "Masanori Koyama"], "authorids": ["miyato@preferred.jp", "hillbig@preferred.jp", "ichi@sys.i.kyoto-u.ac.jp", "mkoyama@fc.ritsumei.ac.jp"]}, "tags": [], "invitation": {"tddate": null, "tmdate": 1487340373030, "tcdate": 1487340373030, "id": "ICLR.cc/2017/workshop/-/paper88/public/comment", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2017/workshop/paper88/reviewers"], "reply": {"forum": "H1hLmF4Fx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/workshop/reviewers", "ICLR.cc/2017/pcs"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1487340373030}}}, {"tddate": null, "tmdate": 1489183188309, "tcdate": 1489183188309, "number": 2, "id": "BJhCWigoe", "invitation": "ICLR.cc/2017/workshop/-/paper88/official/review", "forum": "H1hLmF4Fx", "replyto": "H1hLmF4Fx", "signatures": ["ICLR.cc/2017/workshop/paper88/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper88/AnonReviewer1"], "content": {"title": "Review", "rating": "7: Good paper, accept", "review": "Summary:\n\nThis work extends the work of Jaderberg et al (2016) on synthetic gradients,\nby replacing their gradient regressors with a mini-neural networks (called\nVirtual Forward Backward Networks or, VFBN), which predict the final label\n(instead of the gradients), and are trained by minimizing the l2-loss between\ngradients backpropagated to the layer from these mini-networks and the\nactual gradients from the full model.\n\nThese VFBNS can be seen as implementing \"auxiliary losses\", but are notably not\ntrained by miniming the label-prediction loss.\n\nCons:\n\n1. The overall presentation is clear, however there are a few typos.\n2. The exact details of how the various components were updated are missing.\n3. Only one layer was decoupled. More experiments on decoupling the whole \n   network would be insightful.\n4. These \"mini (or virtual) networks incur additional costs; experiments \n   which control for computational costs should be insightful.\n\nPros:\n\n1. Potentially interesting insight into training with synthetic gradients, but\n   requires further investigation.\n2. Impressive performance on CIFAR-10.\n\nThe work aligns well with Workshop Track's objective to \"stimulate discussion of new ideas and directions\".\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Synthetic Gradient Methods with Virtual Forward-Backward Networks", "abstract": "The concept of synthetic gradient introduced by Jaderberg et al. (2016) provides an avant-garde framework for asynchronous learning of neural network.\nTheir model, however, has a weakness in its construction, because the structure of their synthetic gradient has little relation to the objective function of the target task.\nIn this paper we introduce virtual forward-backward networks (VFBN). \nVFBN is a model that produces synthetic gradient whose structure is analogous to the actual gradient of the objective function.\nVFBN is the first of its kind that succeeds in decoupling deep networks like ResNet-110 (He et al., 2016) without compromising its performance.", "pdf": "/pdf/6a1d60d2690cf164e98aedd28eb5e74ff7931d9c.pdf", "paperhash": "miyato|synthetic_gradient_methods_with_virtual_forwardbackward_networks", "conflicts": ["preferred.jp", "atr.jp", "kyoto-u.ac.jp", "ritsumei.ac.jp"], "keywords": ["Deep learning", "Optimization"], "authors": ["Takeru Miyato", "Daisuke Okanohara", "Shin-ichi Maeda", "Masanori Koyama"], "authorids": ["miyato@preferred.jp", "hillbig@preferred.jp", "ichi@sys.i.kyoto-u.ac.jp", "mkoyama@fc.ritsumei.ac.jp"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489183188923, "id": "ICLR.cc/2017/workshop/-/paper88/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper88/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper88/AnonReviewer2", "ICLR.cc/2017/workshop/paper88/AnonReviewer1"], "reply": {"forum": "H1hLmF4Fx", "replyto": "H1hLmF4Fx", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper88/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper88/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489183188923}}}, {"tddate": null, "tmdate": 1489076705420, "tcdate": 1489076705420, "number": 1, "id": "BJtyz-kse", "invitation": "ICLR.cc/2017/workshop/-/paper88/official/review", "forum": "H1hLmF4Fx", "replyto": "H1hLmF4Fx", "signatures": ["ICLR.cc/2017/workshop/paper88/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper88/AnonReviewer2"], "content": {"title": "More details should be described, ", "rating": "5: Marginally below acceptance threshold", "review": "This paper is well-motivated, which improve Jaderberg's by exploiting the structure of gradients. The experimental results are promising. However, the cons of the paper are:\n1. The English of the paper is not good. \n2. The paper should provides more details of the experiments, such as the configuration of the proposed model and the baseline. Moreover, the paper should also provide the time used to train the model, since Jaderberg's model is more simple. ", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Synthetic Gradient Methods with Virtual Forward-Backward Networks", "abstract": "The concept of synthetic gradient introduced by Jaderberg et al. (2016) provides an avant-garde framework for asynchronous learning of neural network.\nTheir model, however, has a weakness in its construction, because the structure of their synthetic gradient has little relation to the objective function of the target task.\nIn this paper we introduce virtual forward-backward networks (VFBN). \nVFBN is a model that produces synthetic gradient whose structure is analogous to the actual gradient of the objective function.\nVFBN is the first of its kind that succeeds in decoupling deep networks like ResNet-110 (He et al., 2016) without compromising its performance.", "pdf": "/pdf/6a1d60d2690cf164e98aedd28eb5e74ff7931d9c.pdf", "paperhash": "miyato|synthetic_gradient_methods_with_virtual_forwardbackward_networks", "conflicts": ["preferred.jp", "atr.jp", "kyoto-u.ac.jp", "ritsumei.ac.jp"], "keywords": ["Deep learning", "Optimization"], "authors": ["Takeru Miyato", "Daisuke Okanohara", "Shin-ichi Maeda", "Masanori Koyama"], "authorids": ["miyato@preferred.jp", "hillbig@preferred.jp", "ichi@sys.i.kyoto-u.ac.jp", "mkoyama@fc.ritsumei.ac.jp"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489183188923, "id": "ICLR.cc/2017/workshop/-/paper88/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper88/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper88/AnonReviewer2", "ICLR.cc/2017/workshop/paper88/AnonReviewer1"], "reply": {"forum": "H1hLmF4Fx", "replyto": "H1hLmF4Fx", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper88/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper88/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489183188923}}}], "count": 5}