{"notes": [{"id": "EKb4Z0aSNf", "original": "rglNG24F0C", "number": 1642, "cdate": 1601308182001, "ddate": null, "tcdate": 1601308182001, "tmdate": 1614985716764, "tddate": null, "forum": "EKb4Z0aSNf", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "CLOPS: Continual Learning of Physiological Signals", "authorids": ["~Dani_Kiyasseh1", "tingting.zhu@eng.ox.ac.uk", "~David_A._Clifton1"], "authors": ["Dani Kiyasseh", "Tingting Zhu", "David A. Clifton"], "keywords": ["Continual learning", "physiological signals", "healthcare"], "abstract": "Deep learning algorithms are known to experience destructive interference when instances violate the assumption of being independent and identically distributed (i.i.d). This violation, however, is ubiquitous in clinical settings where data are streamed temporally and from a multitude of physiological sensors. To overcome this obstacle, we propose CLOPS, a replay-based continual learning strategy. In three continual learning scenarios based on three publically-available datasets, we show that CLOPS can outperform the state-of-the-art methods, GEM and MIR. Moreover, we propose end-to-end trainable parameters, which we term task-instance parameters, that can be used to quantify task difficulty and similarity. This quantification yields insights into both network interpretability and clinical applications, where task difficulty is poorly quantified.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kiyasseh|clops_continual_learning_of_physiological_signals", "supplementary_material": "/attachment/49796ec1806fe872109be2b790f1265b2ddc2452.zip", "pdf": "/pdf/abbad498238f1d6a353d71e52a4bf1703fc29065.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=1RqymILGmH", "_bibtex": "@misc{\nkiyasseh2021clops,\ntitle={{\\{}CLOPS{\\}}: Continual Learning of Physiological Signals},\nauthor={Dani Kiyasseh and Tingting Zhu and David A. Clifton},\nyear={2021},\nurl={https://openreview.net/forum?id=EKb4Z0aSNf}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 17, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "iVnmyp74DXo", "original": null, "number": 1, "cdate": 1610040425844, "ddate": null, "tcdate": 1610040425844, "tmdate": 1610474025234, "tddate": null, "forum": "EKb4Z0aSNf", "replyto": "EKb4Z0aSNf", "invitation": "ICLR.cc/2021/Conference/Paper1642/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "Reviewers could not reach consensus here and important concerns from one reviewer on empirical results could not be convincingly addressed. The authors have provided a comprehensive response to the reviews, yet failed to convince them.\n"}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "CLOPS: Continual Learning of Physiological Signals", "authorids": ["~Dani_Kiyasseh1", "tingting.zhu@eng.ox.ac.uk", "~David_A._Clifton1"], "authors": ["Dani Kiyasseh", "Tingting Zhu", "David A. Clifton"], "keywords": ["Continual learning", "physiological signals", "healthcare"], "abstract": "Deep learning algorithms are known to experience destructive interference when instances violate the assumption of being independent and identically distributed (i.i.d). This violation, however, is ubiquitous in clinical settings where data are streamed temporally and from a multitude of physiological sensors. To overcome this obstacle, we propose CLOPS, a replay-based continual learning strategy. In three continual learning scenarios based on three publically-available datasets, we show that CLOPS can outperform the state-of-the-art methods, GEM and MIR. Moreover, we propose end-to-end trainable parameters, which we term task-instance parameters, that can be used to quantify task difficulty and similarity. This quantification yields insights into both network interpretability and clinical applications, where task difficulty is poorly quantified.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kiyasseh|clops_continual_learning_of_physiological_signals", "supplementary_material": "/attachment/49796ec1806fe872109be2b790f1265b2ddc2452.zip", "pdf": "/pdf/abbad498238f1d6a353d71e52a4bf1703fc29065.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=1RqymILGmH", "_bibtex": "@misc{\nkiyasseh2021clops,\ntitle={{\\{}CLOPS{\\}}: Continual Learning of Physiological Signals},\nauthor={Dani Kiyasseh and Tingting Zhu and David A. Clifton},\nyear={2021},\nurl={https://openreview.net/forum?id=EKb4Z0aSNf}\n}"}, "tags": [], "invitation": {"reply": {"forum": "EKb4Z0aSNf", "replyto": "EKb4Z0aSNf", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040425829, "tmdate": 1610474025217, "id": "ICLR.cc/2021/Conference/Paper1642/-/Decision"}}}, {"id": "DKBbtKSwthi", "original": null, "number": 1, "cdate": 1603814093193, "ddate": null, "tcdate": 1603814093193, "tmdate": 1607390855559, "tddate": null, "forum": "EKb4Z0aSNf", "replyto": "EKb4Z0aSNf", "invitation": "ICLR.cc/2021/Conference/Paper1642/-/Official_Review", "content": {"title": "Clearly described approach; not sure about evaluation quality", "review": "**Update**\nI think the manuscript has been further improved now and I improve my score to 7. Also since I think results on public datasets in new domains other than images may be helpful for the wider research community. Keep in mind, as written below, that other reviewers more familiar with this research field may be more  able than me to judge the evaluation quality and whether the evaluation contains too substantial flaws to allow publication. \n\n**Disclaimer**\nUpon reviewing the manuscript, I found I am unfortunately not well-acquainted enough with the research fields (active learning/continual learning) to be able to evaluate very well what this manuscript contributes to these fields. I hope my review can still be useful.\n\n**Summary**\nThe manuscript describes and evaluates a continual learning algorithm that learns per-instance scalars beta that weight the loss of each instance. These betas are optimized together with all the other parameters, using an additional regularization term that prevents all betas to be optimized to zero. The betas averaged over time are used to decide which instances get into the replay buffer (easier ones are used). Additionally, to decide which instances  from the replay buffer to train on, Monte Carlo Dropout is used to estimate the uncertainties of the network on each instance and more uncertain instances are then more likely to be sampled for training. The approach is evaluated on three healthcare datasets, in either a setting where sequentially different tasks are learned by the network, the same task is learned with data sequentially used across time (affected by seasonal changes), or the same task is learned while sequentially shifting the domain/input distribution in a controlled way (different projections of same ECG signal). The proposed approach outperforms two baselines with regard to performance on previously-trained-on tasks. Ablation studies show the role of the storage strategy and the acquisition strategy while validating the interpretation of the betas as instance difficulties.\n\n**Main Impression**\nThe approach is clearly described, it was fairly easy to understand for me, the writing is nice. The evaluation seems interesting; it would be more convincing to me if it was also evaluated on some dataset where published results of other works are available. Ablation studies are nice, the results of them could be discussed more.\n\n**Major Points**\n\nAs written above, I would really like to see an evaluation of this approach where one can directly compare to published results by others. Reimplemented baselines always have a risk of being not tuned well enough etc. And as far as I could understand, there does not seem to be anything so healthcare-specific in the algorithm itself that comparison in other domains would not make sense, in case that is the problem.\n\nI like the ablation studies and would have enjoyed more discussion of their results. \n- It seems for b=0.1 and b=0.25, random storage always outperforms CLOPS? Why? b=0.5 also mostly outperforms. \n- Similar, for a=0.5 and a=1.0, it seems that random acquisition mostly outperforms CLOPS, for a=0.25 it is still close, why?\n- What happens for random acquisition and random storage? Was this tested?\n\n\n **Minor Points**\n\n*Some parts of introduction I didn\u2019t agree or find too broad*\n\n\u201cDeep learning algorithms operate under the assumption that instances are independent and identically-distributed (i.i.d.). \u201c\n-> not really specific to deep learning at all to me. Obviously one can apply deep neural networks also under different assumptions. Would either write: \u201cMany machine learning and deep learning algorithms ...\u201d, Or simply \u201cMany deep learning algorithms\u2026\u201d\n\n\u201cGiven the potential impact of designing such an algorithm and the machine learning community\u2019s efforts towards achieving artificial general intelligence, research on continual learning has increased (Parisi et al., 2019).\u201d\n-> a bit too broad stuff to me, could be just removed (AGI etc.)\n\n*Confusing beta zero part*\n\n\u201cIn this setup, we discovered \u03b2_iT quickly decay to zero, and as a coefficient of the loss, prevents the network from learning the task\u201d\nThis made me a bit confused: As per formula, if you set all betas to 0, the loss would become zero no? So it should be a direct consequence of the formula that yes, gradient descent or any optimization should set all betas to zero without any regularization/constraints? The \u201cdiscovered\u201d confused me here. Or am I misunderstanding something here?\n\n*Figure improvements*\n\nFigure 2: thicker curves? Hard to see\n\nFigure 5 in appendix: legend would be helpful\n\n\n\nAs written above, I don\u2019t feel well-acquainted enough with the literature on continual/active learning to judge the contribution of this manuscript to these fields with confidence; hope other reviewers can do this :)\n", "rating": "7: Good paper, accept", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2021/Conference/Paper1642/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1642/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "CLOPS: Continual Learning of Physiological Signals", "authorids": ["~Dani_Kiyasseh1", "tingting.zhu@eng.ox.ac.uk", "~David_A._Clifton1"], "authors": ["Dani Kiyasseh", "Tingting Zhu", "David A. Clifton"], "keywords": ["Continual learning", "physiological signals", "healthcare"], "abstract": "Deep learning algorithms are known to experience destructive interference when instances violate the assumption of being independent and identically distributed (i.i.d). This violation, however, is ubiquitous in clinical settings where data are streamed temporally and from a multitude of physiological sensors. To overcome this obstacle, we propose CLOPS, a replay-based continual learning strategy. In three continual learning scenarios based on three publically-available datasets, we show that CLOPS can outperform the state-of-the-art methods, GEM and MIR. Moreover, we propose end-to-end trainable parameters, which we term task-instance parameters, that can be used to quantify task difficulty and similarity. This quantification yields insights into both network interpretability and clinical applications, where task difficulty is poorly quantified.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kiyasseh|clops_continual_learning_of_physiological_signals", "supplementary_material": "/attachment/49796ec1806fe872109be2b790f1265b2ddc2452.zip", "pdf": "/pdf/abbad498238f1d6a353d71e52a4bf1703fc29065.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=1RqymILGmH", "_bibtex": "@misc{\nkiyasseh2021clops,\ntitle={{\\{}CLOPS{\\}}: Continual Learning of Physiological Signals},\nauthor={Dani Kiyasseh and Tingting Zhu and David A. Clifton},\nyear={2021},\nurl={https://openreview.net/forum?id=EKb4Z0aSNf}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "EKb4Z0aSNf", "replyto": "EKb4Z0aSNf", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1642/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538114017, "tmdate": 1606915779742, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1642/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1642/-/Official_Review"}}}, {"id": "Vqg_phL9tYC", "original": null, "number": 3, "cdate": 1604590938447, "ddate": null, "tcdate": 1604590938447, "tmdate": 1606720663463, "tddate": null, "forum": "EKb4Z0aSNf", "replyto": "EKb4Z0aSNf", "invitation": "ICLR.cc/2021/Conference/Paper1642/-/Official_Review", "content": {"title": "No theoretical basis, slack empirical validation", "review": "## Summary\nThis paper proposes a replay-based continual learning method and applies it to physiological data.\nThe proposed method consists of two heuristics to manage the replay memory:\n- Learn a weight parameter (task-instance parameter) for each data point by minimizing a special loss function, and store the data with large weight parameters.\n- Periodically compute epistemic uncertainty for all data in the replay memory, and replay the data with large uncertainty.\n\n---\n\n## Pros\n\nThis paper points out the importance of continual learning in the medical field.\n\n---\n\n## Cons\n\n### Weak contribution\n\nSince this paper deals with the continual learning of a specific type of data, I think there are two directions in which this study can be meaningful:\n- Test various CL algorithms in the new domain, determine which method performs better than others, and analyze the reason.\n- Use domain knowledge to design a new algorithm that performs particularly better on the domain.\n\nHowever, I think this paper belongs to neither of the two. There is no domain-specific component in the algorithm, and only two baselines are compared.\n\nIf the method is general enough to be used in other domains, I think the authors should have also tested standard CL scenarios such as Split-CIFAR10 or Split-CIFAR100.\n\n### Lack of theoretical justification\n\nThe proposed method is mostly heuristic and does not have a theoretical basis. Especially eq. (2) seems too arbitrary. Since this paper's empirical evidence is weak, I think a theoretical analysis is necessary to support the algorithm's efficacy.\n\n### Vague and improper definition of time incremental learning\n\nThe authors seem to propose time incremental learning for the first time, but the definition is too vague:\n> Time Incremental Learning (Task-IL) - the same dataset and prediction problem are used for each task, however the time of year at which the data were collected differs from one task to another. Such seasonality is most common in healthcare applications.\n\nThe authors should specify what changes over time. However, I think there is even a bigger problem:\n- If the input distribution changes, it is not different from domain incremental learning.\n- If the output distribution changes for the same input, it is not continual learning. The model *should* forget old tasks and adapt to new tasks.\n- If both the input and output distributions change, it is class incremental learning.\n\nTherefore, I think time incremental learning is not a novel branch of continual learning. The corresponding experiments should better be reformulated as domain incremental learning or class incremental learning.\n\n---\n\n## Overall evaluation\n\nI do not think this paper proposes a novel idea with either a solid theoretical basis or strong empirical results. Therefore, I recommend rejection.\n\n---\n\n## Post rebuttal\nDuring the rebuttal, the authors failed to handle the issues that I raised. Especially, the authors did not respond to my criticisms about the strange experiments. Therefore, I stick to my initial rating.", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1642/AnonReviewer5"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1642/AnonReviewer5"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "CLOPS: Continual Learning of Physiological Signals", "authorids": ["~Dani_Kiyasseh1", "tingting.zhu@eng.ox.ac.uk", "~David_A._Clifton1"], "authors": ["Dani Kiyasseh", "Tingting Zhu", "David A. Clifton"], "keywords": ["Continual learning", "physiological signals", "healthcare"], "abstract": "Deep learning algorithms are known to experience destructive interference when instances violate the assumption of being independent and identically distributed (i.i.d). This violation, however, is ubiquitous in clinical settings where data are streamed temporally and from a multitude of physiological sensors. To overcome this obstacle, we propose CLOPS, a replay-based continual learning strategy. In three continual learning scenarios based on three publically-available datasets, we show that CLOPS can outperform the state-of-the-art methods, GEM and MIR. Moreover, we propose end-to-end trainable parameters, which we term task-instance parameters, that can be used to quantify task difficulty and similarity. This quantification yields insights into both network interpretability and clinical applications, where task difficulty is poorly quantified.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kiyasseh|clops_continual_learning_of_physiological_signals", "supplementary_material": "/attachment/49796ec1806fe872109be2b790f1265b2ddc2452.zip", "pdf": "/pdf/abbad498238f1d6a353d71e52a4bf1703fc29065.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=1RqymILGmH", "_bibtex": "@misc{\nkiyasseh2021clops,\ntitle={{\\{}CLOPS{\\}}: Continual Learning of Physiological Signals},\nauthor={Dani Kiyasseh and Tingting Zhu and David A. Clifton},\nyear={2021},\nurl={https://openreview.net/forum?id=EKb4Z0aSNf}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "EKb4Z0aSNf", "replyto": "EKb4Z0aSNf", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1642/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538114017, "tmdate": 1606915779742, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1642/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1642/-/Official_Review"}}}, {"id": "GpPLN95--Xg", "original": null, "number": 14, "cdate": 1606171937050, "ddate": null, "tcdate": 1606171937050, "tmdate": 1606171962951, "tddate": null, "forum": "EKb4Z0aSNf", "replyto": "EKb4Z0aSNf", "invitation": "ICLR.cc/2021/Conference/Paper1642/-/Official_Comment", "content": {"title": "Final Version of Manuscript", "comment": "We thank all the reviewers for taking the time and effort to read the manuscript and for providing us with valuable feedback. \n\n**FINAL UPLOAD OF MANUSCRIPT**\nWe have now modified the manuscript, as per your comments, and uploaded the main manuscript and the supplementary material. Here are the high-level changes that we have made to the manuscript:\n\n1) We have included Fig. 1 in **Sec. 3.1** to clarify the various continual learning scenarios that we are experimenting under.  \n2) We have modified **Sec. 4** significantly to improve clarity as it pertains to the explanation of the loss-weighting mechanism and the motivation behind the storage function (Eq. 6).\n3) We have explicitly mentioned the storage and acquisition fractions in **Sec. 4**. \n4) We have modified the figures containing learning curves (e.g., Fig. 3) to improve clarity and illustrate when the network is in a 'training' phase. This is indicated by the coloured-blocks. \n5) We have modified **Sec. 6.2** to extend the training on task Term 3 until convergence. We find that CLOPS not only exhibits forward weight transfer (FWT) but also mitigates catastrophic forgetting relative to the fine-tuning setup. More details can be found in Sec. 6.2. \n6) We conducted Random Storage and Acquisition experiments in **Sec. 6.4** in order to better illustrate the useful components of CLOPS. These results can be seen in Fig. 5. \n7) We have annotated the ECG recordings in Fig. 6 in order to better illustrate our point regarding ST-elevation (a medical condition). \n8) We have modified **Sec. 6.5** to better illustrate how we go about quantifying task similarity and leveraging that to generate a curriculum. \n\nWe hope our responses to your reviews in addition to the modified version of the manuscript have addressed your concerns. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1642/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1642/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "CLOPS: Continual Learning of Physiological Signals", "authorids": ["~Dani_Kiyasseh1", "tingting.zhu@eng.ox.ac.uk", "~David_A._Clifton1"], "authors": ["Dani Kiyasseh", "Tingting Zhu", "David A. Clifton"], "keywords": ["Continual learning", "physiological signals", "healthcare"], "abstract": "Deep learning algorithms are known to experience destructive interference when instances violate the assumption of being independent and identically distributed (i.i.d). This violation, however, is ubiquitous in clinical settings where data are streamed temporally and from a multitude of physiological sensors. To overcome this obstacle, we propose CLOPS, a replay-based continual learning strategy. In three continual learning scenarios based on three publically-available datasets, we show that CLOPS can outperform the state-of-the-art methods, GEM and MIR. Moreover, we propose end-to-end trainable parameters, which we term task-instance parameters, that can be used to quantify task difficulty and similarity. This quantification yields insights into both network interpretability and clinical applications, where task difficulty is poorly quantified.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kiyasseh|clops_continual_learning_of_physiological_signals", "supplementary_material": "/attachment/49796ec1806fe872109be2b790f1265b2ddc2452.zip", "pdf": "/pdf/abbad498238f1d6a353d71e52a4bf1703fc29065.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=1RqymILGmH", "_bibtex": "@misc{\nkiyasseh2021clops,\ntitle={{\\{}CLOPS{\\}}: Continual Learning of Physiological Signals},\nauthor={Dani Kiyasseh and Tingting Zhu and David A. Clifton},\nyear={2021},\nurl={https://openreview.net/forum?id=EKb4Z0aSNf}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "EKb4Z0aSNf", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1642/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1642/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1642/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1642/Authors|ICLR.cc/2021/Conference/Paper1642/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1642/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923857388, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1642/-/Official_Comment"}}}, {"id": "gYF1JCKv3Fq", "original": null, "number": 13, "cdate": 1606087732833, "ddate": null, "tcdate": 1606087732833, "tmdate": 1606087732833, "tddate": null, "forum": "EKb4Z0aSNf", "replyto": "YpLe9a-zcws", "invitation": "ICLR.cc/2021/Conference/Paper1642/-/Official_Comment", "content": {"title": "New Section 4.2 is nice while other issue remains. ", "comment": "Let me further explain my concerns at each point as the following. \n\nTASK DESCRIPTION\n\nI am still confused about the Class-IL scenario. From Fig.1 in the revised manuscript, it seems you are using one predictor \u201cf_w\u201d for both binary classification tasks [0-1] and [2-3]. It does not make sense to me. Why we would like to adopt a classifier previously trained to classify cat versus dog to a new task about a car versus plane?\n\nSTORAGE AND ACQUISITION FRACTIONS\n\nThe new section 4 looks better. \n\nFINE-TUNING VS MULTI-TASK LEARNING\n\nThe author is arguing that \u201cthe final set of parameters obtained by the continual learning methods (i.e., at epoch 120 in the Class-IL scenario) achieve stronger generalization performance than the MTL set of parameters. \u201d Current result in Tab.1 is not enough to support the argument. We need the training error of \u201cFine-tuning\u201d and \u201cMTL\u201d to compare the generalization. I would like to know whether \u201cMTL\u201d has a similar training error as the \u201cFine-tuning\u201d. If \u201cMTL\u201d has a worse training error than \u201cFine-tuning\u201d, it is likely that the \u201cMTL\u201d is underfitting due to potential reasons like tasks have interference. It is a standard issue of \u201cMTL\u201d. One could consider many tricks like PCGrad [1] to mitigate it. \n\nBy the way, it related to R5\u2019s concern \u201cI think this strongly suggests that the baselines are poorly tuned\u201d. At a high-level, I agree with R5. It seems the author still needs some effort here. \n\n[1] Yu, Tianhe, et al. \"Gradient surgery for multi-task learning.\" arXiv preprint arXiv:2001.06782 (2020).\n\nEXPLANATION OF TIME-IL RESULTS\n\nI understand that from Fig.4, it looks that \u201cFine-tuning\u201d takes 20 epochs to reach the performance of \u201cCLOPS\u201d at epoch 1. However, my main concern is that learning is not finished yet. The curve of \u201cFine-tuning\u201d has a much large slope comparing to \u201cCLOPS\u201d. The trend continuing persists for like 100 epochs, the \u201cFine-tuning\u201d will be a clear winner against \u201cCLOPS\u201d.  So I suggest the author train the model longer and draw conclusions after the model converged. \n\nHOW TO USE TASK SIMILARITY\n\nFrom the current result presented in Tab 3, I cannot agree that curriculum learning or anti curriculum learning is performing better than \u201cRandom\u201d.  Clearly, \u201cRandom\u201d has a higher AUC. So I am not sure about the benefit of doing such task-similarity-based curriculum learning.\n\nIn summary, since most of my concerns are still there, my score remains unchanged. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1642/AnonReviewer1"], "readers": ["everyone", "ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1642/Area_Chairs", "ICLR.cc/2021/Conference/Paper1642/Reviewers", "ICLR.cc/2021/Conference/Paper1642/AnonReviewer1", "ICLR.cc/2021/Conference/Paper1642/Authors"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1642/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "CLOPS: Continual Learning of Physiological Signals", "authorids": ["~Dani_Kiyasseh1", "tingting.zhu@eng.ox.ac.uk", "~David_A._Clifton1"], "authors": ["Dani Kiyasseh", "Tingting Zhu", "David A. Clifton"], "keywords": ["Continual learning", "physiological signals", "healthcare"], "abstract": "Deep learning algorithms are known to experience destructive interference when instances violate the assumption of being independent and identically distributed (i.i.d). This violation, however, is ubiquitous in clinical settings where data are streamed temporally and from a multitude of physiological sensors. To overcome this obstacle, we propose CLOPS, a replay-based continual learning strategy. In three continual learning scenarios based on three publically-available datasets, we show that CLOPS can outperform the state-of-the-art methods, GEM and MIR. Moreover, we propose end-to-end trainable parameters, which we term task-instance parameters, that can be used to quantify task difficulty and similarity. This quantification yields insights into both network interpretability and clinical applications, where task difficulty is poorly quantified.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kiyasseh|clops_continual_learning_of_physiological_signals", "supplementary_material": "/attachment/49796ec1806fe872109be2b790f1265b2ddc2452.zip", "pdf": "/pdf/abbad498238f1d6a353d71e52a4bf1703fc29065.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=1RqymILGmH", "_bibtex": "@misc{\nkiyasseh2021clops,\ntitle={{\\{}CLOPS{\\}}: Continual Learning of Physiological Signals},\nauthor={Dani Kiyasseh and Tingting Zhu and David A. Clifton},\nyear={2021},\nurl={https://openreview.net/forum?id=EKb4Z0aSNf}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "EKb4Z0aSNf", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1642/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1642/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1642/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1642/Authors|ICLR.cc/2021/Conference/Paper1642/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1642/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923857388, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1642/-/Official_Comment"}}}, {"id": "e_AnZD2nL2m", "original": null, "number": 12, "cdate": 1605857202726, "ddate": null, "tcdate": 1605857202726, "tmdate": 1605857202726, "tddate": null, "forum": "EKb4Z0aSNf", "replyto": "6hUIq71uoW", "invitation": "ICLR.cc/2021/Conference/Paper1642/-/Official_Comment", "content": {"title": "Answers to R4", "comment": "Although the evaluation is a critical issue, I put more weight on the weak contribution of this paper. Here are my answers to R4's questions.\n\n1. For me, providing code is not a critical factor for the decision. It definitely helps to verify the results, but in this case, the result itself is very confusing.\n2. In my opinion, evaluating on common benchmarks is not enough. If the proposed algorithm performs well on standard benchmarks (although I highly doubt it), then there is no reason to focus only on physiological data, and it should better be introduced as a general CL algorithm. Since the main novelty of this paper is the application of CL to a new domain, I did not ask the authors to put additional effort into testing the standard benchmarks."}, "signatures": ["ICLR.cc/2021/Conference/Paper1642/AnonReviewer5"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1642/AnonReviewer5"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "CLOPS: Continual Learning of Physiological Signals", "authorids": ["~Dani_Kiyasseh1", "tingting.zhu@eng.ox.ac.uk", "~David_A._Clifton1"], "authors": ["Dani Kiyasseh", "Tingting Zhu", "David A. Clifton"], "keywords": ["Continual learning", "physiological signals", "healthcare"], "abstract": "Deep learning algorithms are known to experience destructive interference when instances violate the assumption of being independent and identically distributed (i.i.d). This violation, however, is ubiquitous in clinical settings where data are streamed temporally and from a multitude of physiological sensors. To overcome this obstacle, we propose CLOPS, a replay-based continual learning strategy. In three continual learning scenarios based on three publically-available datasets, we show that CLOPS can outperform the state-of-the-art methods, GEM and MIR. Moreover, we propose end-to-end trainable parameters, which we term task-instance parameters, that can be used to quantify task difficulty and similarity. This quantification yields insights into both network interpretability and clinical applications, where task difficulty is poorly quantified.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kiyasseh|clops_continual_learning_of_physiological_signals", "supplementary_material": "/attachment/49796ec1806fe872109be2b790f1265b2ddc2452.zip", "pdf": "/pdf/abbad498238f1d6a353d71e52a4bf1703fc29065.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=1RqymILGmH", "_bibtex": "@misc{\nkiyasseh2021clops,\ntitle={{\\{}CLOPS{\\}}: Continual Learning of Physiological Signals},\nauthor={Dani Kiyasseh and Tingting Zhu and David A. Clifton},\nyear={2021},\nurl={https://openreview.net/forum?id=EKb4Z0aSNf}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "EKb4Z0aSNf", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1642/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1642/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1642/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1642/Authors|ICLR.cc/2021/Conference/Paper1642/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1642/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923857388, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1642/-/Official_Comment"}}}, {"id": "6hUIq71uoW", "original": null, "number": 11, "cdate": 1605831746208, "ddate": null, "tcdate": 1605831746208, "tmdate": 1605831746208, "tddate": null, "forum": "EKb4Z0aSNf", "replyto": "RgZyFKeI_7j", "invitation": "ICLR.cc/2021/Conference/Paper1642/-/Official_Comment", "content": {"title": "Do public code/data help and could further experiments rescue the manuscript?", "comment": "The evaluation seems to be the core issue overall, so, as a reviewer with less experience in the continual learning field, I wanted to add my questions here as well, actually more for R5:\n\n1. I agree it is not perfect to have a new algorithm evaluated only on new datasets, but do you consider since the manuscript uses public datasets and public code it might still be valuable enough to publish it so others can run further comparisons?\n2. Do you think evaluating the proposed algorithm on common benchmarks would be enough to endorse the manuscript? Or would it depend on the result as well?"}, "signatures": ["ICLR.cc/2021/Conference/Paper1642/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1642/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "CLOPS: Continual Learning of Physiological Signals", "authorids": ["~Dani_Kiyasseh1", "tingting.zhu@eng.ox.ac.uk", "~David_A._Clifton1"], "authors": ["Dani Kiyasseh", "Tingting Zhu", "David A. Clifton"], "keywords": ["Continual learning", "physiological signals", "healthcare"], "abstract": "Deep learning algorithms are known to experience destructive interference when instances violate the assumption of being independent and identically distributed (i.i.d). This violation, however, is ubiquitous in clinical settings where data are streamed temporally and from a multitude of physiological sensors. To overcome this obstacle, we propose CLOPS, a replay-based continual learning strategy. In three continual learning scenarios based on three publically-available datasets, we show that CLOPS can outperform the state-of-the-art methods, GEM and MIR. Moreover, we propose end-to-end trainable parameters, which we term task-instance parameters, that can be used to quantify task difficulty and similarity. This quantification yields insights into both network interpretability and clinical applications, where task difficulty is poorly quantified.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kiyasseh|clops_continual_learning_of_physiological_signals", "supplementary_material": "/attachment/49796ec1806fe872109be2b790f1265b2ddc2452.zip", "pdf": "/pdf/abbad498238f1d6a353d71e52a4bf1703fc29065.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=1RqymILGmH", "_bibtex": "@misc{\nkiyasseh2021clops,\ntitle={{\\{}CLOPS{\\}}: Continual Learning of Physiological Signals},\nauthor={Dani Kiyasseh and Tingting Zhu and David A. Clifton},\nyear={2021},\nurl={https://openreview.net/forum?id=EKb4Z0aSNf}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "EKb4Z0aSNf", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1642/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1642/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1642/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1642/Authors|ICLR.cc/2021/Conference/Paper1642/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1642/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923857388, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1642/-/Official_Comment"}}}, {"id": "D6u5PEAbQBw", "original": null, "number": 10, "cdate": 1605831635450, "ddate": null, "tcdate": 1605831635450, "tmdate": 1605831635450, "tddate": null, "forum": "EKb4Z0aSNf", "replyto": "URWOrG-oAU0", "invitation": "ICLR.cc/2021/Conference/Paper1642/-/Official_Comment", "content": {"title": "Many issues resolved, remains the empirical comparison to published results issue", "comment": "EXPLANATION OF ABLATION RESULTS and NEW RANDOM STORAGE AND ACQUISITION EXPERIMENT\nThank you for the additional discussion and experiment, I think it adds to the value of the manuscript.\n\nBROAD SENTENCES \nThank you, reads better to me now\n\nEXPLANATION OF LOSS-WEIGHTING MECHANISM\nBetter than before, I would maybe even prefer writing in words like \u201cbeta will go to zero\u201d and no need to add the confirmed empirically, but these are minor details.\n\nFIGURE IMPROVEMENTS\nThanks, to me it is much more easily understandable now. You may consider whether the figures, esp. Figure 3, are understandable for persons with colorblindness. I am not colorblind myself so cannot judge, just noticed there seems to be green and something reddish used in Fig.3? \n\nOverall, the main unaddressed issue remaining is evaluation on published data, but I will write in the thread with Reviewer 5 to not split up discussion further.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1642/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1642/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "CLOPS: Continual Learning of Physiological Signals", "authorids": ["~Dani_Kiyasseh1", "tingting.zhu@eng.ox.ac.uk", "~David_A._Clifton1"], "authors": ["Dani Kiyasseh", "Tingting Zhu", "David A. Clifton"], "keywords": ["Continual learning", "physiological signals", "healthcare"], "abstract": "Deep learning algorithms are known to experience destructive interference when instances violate the assumption of being independent and identically distributed (i.i.d). This violation, however, is ubiquitous in clinical settings where data are streamed temporally and from a multitude of physiological sensors. To overcome this obstacle, we propose CLOPS, a replay-based continual learning strategy. In three continual learning scenarios based on three publically-available datasets, we show that CLOPS can outperform the state-of-the-art methods, GEM and MIR. Moreover, we propose end-to-end trainable parameters, which we term task-instance parameters, that can be used to quantify task difficulty and similarity. This quantification yields insights into both network interpretability and clinical applications, where task difficulty is poorly quantified.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kiyasseh|clops_continual_learning_of_physiological_signals", "supplementary_material": "/attachment/49796ec1806fe872109be2b790f1265b2ddc2452.zip", "pdf": "/pdf/abbad498238f1d6a353d71e52a4bf1703fc29065.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=1RqymILGmH", "_bibtex": "@misc{\nkiyasseh2021clops,\ntitle={{\\{}CLOPS{\\}}: Continual Learning of Physiological Signals},\nauthor={Dani Kiyasseh and Tingting Zhu and David A. Clifton},\nyear={2021},\nurl={https://openreview.net/forum?id=EKb4Z0aSNf}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "EKb4Z0aSNf", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1642/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1642/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1642/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1642/Authors|ICLR.cc/2021/Conference/Paper1642/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1642/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923857388, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1642/-/Official_Comment"}}}, {"id": "RgZyFKeI_7j", "original": null, "number": 9, "cdate": 1605790894905, "ddate": null, "tcdate": 1605790894905, "tmdate": 1605790894905, "tddate": null, "forum": "EKb4Z0aSNf", "replyto": "OZp8SGGrlX", "invitation": "ICLR.cc/2021/Conference/Paper1642/-/Official_Comment", "content": {"title": "Re: Response to Reviewer 5 - Round 2", "comment": "**Contributions**\n\nPlease note that the practicality of the problem has never been my concern from the beginning. Nonetheless, I do like the argument about the privacy issue.\n\nMy point is, I could not get useful insight from the method or experiments. I think a good paper should provide more than just a method and its scores. If the authors designed a method for physiological data, then the authors should justify their design philosophy, i.e., why they chose a specific architecture for that type of data. Currently, there are an algorithm and scores, but not why.\n\n---\n\n**Empirical Evidence**\n\nIt is hard to understand the authors' response:\n> As a result, in the MTL setting, this problem is a 12-way multi-class classification problem, which is arguably quite difficult to solve. In contrast, the remaining methods involve solving a relatively easier binary classification setting (Class 0 vs. Class 1, Class 2 vs. Class 3, etc.).\n\nIn section 3 of the paper, the authors stated:\n> In all of the following cases, task identities are absent during both training and testing and neural architectures are single-headed.\n\nThese two clearly contradict each other. If we call a model single-headed, it should classify all 12 classes, not just two for each task. If the architecture is multi-headed, and the task IDs are provided at test time, the authors should test MTL with the same binary classification setting. However, I think single-headed experiments are much more meaningful.\n\nMoreover, the authors did not explain why fine-tuning works better than GEM and MIR, which also suggests insufficient hyperparameter tuning."}, "signatures": ["ICLR.cc/2021/Conference/Paper1642/AnonReviewer5"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1642/AnonReviewer5"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "CLOPS: Continual Learning of Physiological Signals", "authorids": ["~Dani_Kiyasseh1", "tingting.zhu@eng.ox.ac.uk", "~David_A._Clifton1"], "authors": ["Dani Kiyasseh", "Tingting Zhu", "David A. Clifton"], "keywords": ["Continual learning", "physiological signals", "healthcare"], "abstract": "Deep learning algorithms are known to experience destructive interference when instances violate the assumption of being independent and identically distributed (i.i.d). This violation, however, is ubiquitous in clinical settings where data are streamed temporally and from a multitude of physiological sensors. To overcome this obstacle, we propose CLOPS, a replay-based continual learning strategy. In three continual learning scenarios based on three publically-available datasets, we show that CLOPS can outperform the state-of-the-art methods, GEM and MIR. Moreover, we propose end-to-end trainable parameters, which we term task-instance parameters, that can be used to quantify task difficulty and similarity. This quantification yields insights into both network interpretability and clinical applications, where task difficulty is poorly quantified.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kiyasseh|clops_continual_learning_of_physiological_signals", "supplementary_material": "/attachment/49796ec1806fe872109be2b790f1265b2ddc2452.zip", "pdf": "/pdf/abbad498238f1d6a353d71e52a4bf1703fc29065.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=1RqymILGmH", "_bibtex": "@misc{\nkiyasseh2021clops,\ntitle={{\\{}CLOPS{\\}}: Continual Learning of Physiological Signals},\nauthor={Dani Kiyasseh and Tingting Zhu and David A. Clifton},\nyear={2021},\nurl={https://openreview.net/forum?id=EKb4Z0aSNf}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "EKb4Z0aSNf", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1642/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1642/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1642/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1642/Authors|ICLR.cc/2021/Conference/Paper1642/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1642/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923857388, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1642/-/Official_Comment"}}}, {"id": "OZp8SGGrlX", "original": null, "number": 8, "cdate": 1605783260980, "ddate": null, "tcdate": 1605783260980, "tmdate": 1605783260980, "tddate": null, "forum": "EKb4Z0aSNf", "replyto": "l5jbM2NkiO", "invitation": "ICLR.cc/2021/Conference/Paper1642/-/Official_Comment", "content": {"title": "Response to Reviewer 5 - Round 2", "comment": "**CONTRIBUTIONS**\nOur proposed problems had been motivated by real-world clinical workflows and designed based on discussions with medical practitioners in hospital settings. As an exemplar, we provide you with a concrete example of such a clinical workflow. Let us assume we have a patient connected to multiple physiological recording devices (e.g., an ECG machine). Note that this patient can either be in a hospital bed or ambulatory (i.e., moving around while wearing a smartwatch with physiological recording capabilities). Data from such devices are streaming in an online manner as they measure the physiological activity of the patient. In the case of cardiac arrhythmia (abnormalities of the heart), this patient may experience various forms (classes) of such abnormalities while wearing the smartwatch for a certain duration of time. In this context, we need a system that is capable of identifying potentially new classes of arrhythmia that are introduced as time progresses (Class-IL), or to adapt to shifted input distributions as a result of the patient hiking near Mt. Fuji (Domain-IL). A system capable of dynamically adapting to such situations is critical as it ensures we have robust cardiac arrhythmia classification algorithms. This brings us to the connection to the problem domain that you had mentioned. Beyond the proposed method, we believe the connection of our paper to the problem domain lies in our formulation of the three continual learning scenarios themselves (Class-IL, Domain-IL, Domain-IL with temporal component).\n\nAs for the size of the problems, at the time of writing, we had used the two largest publicly-available 12-lead ECG datasets. Of course, validation on much larger privately-held datasets tucked away at clinical institutions would add significant value, but as you can imagine, accessing such data is nontrivial. Having said that, we believe that the size of the dataset is not the only factor one should consider when it comes to deciding between a CL method and full re-training. Other factors include the relatively higher computational cost of full re-training, both in terms of physical resources but also the time taken to do so. A healthcare-specific factor is that of patient privacy. For example, certain regulations stipulate that some data can only be stored on servers at healthcare institutions for a certain number of months before it must be discarded. As a result, full re-training on such data is not possible. Nonetheless, we believe our formulation opens the door for healthcare institutions with access to large-scale datasets to exploit CL. \n\n**EMPIRICAL EVIDENCE**\nTypically, MTL does perform best and fine-tuning does perform worst. Indeed we confirm these intuitions in our experiments (apart from the Class-IL scenario). In the Class-IL scenario, we would like to clarify the differences between MTL and the remaining methods (e.g., fine-tuning, CLOPS, etc.) in attempt to further explain potential differences between the results of these methods. The cardiology dataset, which is used for the Class-IL scenario, consists of 12 cardiac arrhythmia classes. As a result, in the MTL setting, this problem is a 12-way multi-class classification problem, which is arguably quite difficult to solve. In contrast, the remaining methods involve solving a relatively easier binary classification setting (Class 0 vs. Class 1, Class 2 vs. Class 3, etc.). This could help explain why CLOPS and fine-tuning perform better than MTL. As for GEM, this algorithm involves solving a quadratic programming problem, which simply did not perform well in our healthcare time-series setting. \n\nWe are happy to run additional experiments that may bolster your confidence in the way we have conducted the Class-IL scenario experiments. **Do you have any suggestions on that front?** We had also made the code publicly-available and so you are welcome to experiment with the MTL method, if you so wish.\n\nWe hope the above responses have addressed your concerns.  "}, "signatures": ["ICLR.cc/2021/Conference/Paper1642/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1642/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "CLOPS: Continual Learning of Physiological Signals", "authorids": ["~Dani_Kiyasseh1", "tingting.zhu@eng.ox.ac.uk", "~David_A._Clifton1"], "authors": ["Dani Kiyasseh", "Tingting Zhu", "David A. Clifton"], "keywords": ["Continual learning", "physiological signals", "healthcare"], "abstract": "Deep learning algorithms are known to experience destructive interference when instances violate the assumption of being independent and identically distributed (i.i.d). This violation, however, is ubiquitous in clinical settings where data are streamed temporally and from a multitude of physiological sensors. To overcome this obstacle, we propose CLOPS, a replay-based continual learning strategy. In three continual learning scenarios based on three publically-available datasets, we show that CLOPS can outperform the state-of-the-art methods, GEM and MIR. Moreover, we propose end-to-end trainable parameters, which we term task-instance parameters, that can be used to quantify task difficulty and similarity. This quantification yields insights into both network interpretability and clinical applications, where task difficulty is poorly quantified.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kiyasseh|clops_continual_learning_of_physiological_signals", "supplementary_material": "/attachment/49796ec1806fe872109be2b790f1265b2ddc2452.zip", "pdf": "/pdf/abbad498238f1d6a353d71e52a4bf1703fc29065.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=1RqymILGmH", "_bibtex": "@misc{\nkiyasseh2021clops,\ntitle={{\\{}CLOPS{\\}}: Continual Learning of Physiological Signals},\nauthor={Dani Kiyasseh and Tingting Zhu and David A. Clifton},\nyear={2021},\nurl={https://openreview.net/forum?id=EKb4Z0aSNf}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "EKb4Z0aSNf", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1642/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1642/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1642/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1642/Authors|ICLR.cc/2021/Conference/Paper1642/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1642/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923857388, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1642/-/Official_Comment"}}}, {"id": "l5jbM2NkiO", "original": null, "number": 7, "cdate": 1605763337736, "ddate": null, "tcdate": 1605763337736, "tmdate": 1605763337736, "tddate": null, "forum": "EKb4Z0aSNf", "replyto": "g5zIh9_4HB_", "invitation": "ICLR.cc/2021/Conference/Paper1642/-/Official_Comment", "content": {"title": "Re: Response to Reviewer 5 - Round 1", "comment": "**Contributions**\n\nI could not agree more that applying continual learning to realistic scenarios is important. However, it is hard to agree that the proposed CL problems are *realistic*. Although the type of data is different, the scale of the scenario seems to be not that different from the existing ones (correct me if I am wrong). The youtube video mentioned by the authors points out that many CL problems do not match real-world application needs. Then, do the proposed problems match real-world application needs? Considering the size of the problems, will the practitioners actually use a CL method, rather than full re-training?\n\nSince I think designing a truly realistic CL scenario itself is a significant challenge, I do not want to problematize how realistic the scenario is. Instead, I asked for novel insights from the combination of CL and a new type of domain. That is why I thought the paper should be fitted into one of the two directions that I mentioned in the initial review. Although the proposed method is something new, I could not find any connection to the problem domain.\n\n---\n\n**Empirical Evidence**\nFirst, I apologize for not explaining why I thought the empirical validation is weak. I think I mistakenly deleted a whole paragraph about that while changing the ordering of some sections.\n\nThe main reason I thought the experiments are possibly problematic is that the relative performances of baselines are strange. In every CL paper that I know, fine-tuning is always the worst and MTL is always the best. However, In table 1, fine-tuning is better than GEM, MIR, and even MTL. In table 2, fine-tuning is better than GEM and CLOPS is better than MTL. I think this strongly suggests that the baselines are poorly tuned.\n\nIn the response to Reviewer 1, the authors claim that this is the effect of curriculum learning. In curriculum learning, however, the data distribution of the final training step is the same as the whole distribution. This is why catastrophic forgetting does not happen in curriculum learning. Then, there are two ways to interpret the strange result:\n- The baselines are poorly tuned.\n- The training scenario is far from continual learning.\n\nEither way, I think it is a critical problem.\n\n---\n\n**Definition of Continual Learning Scenarios**\n\nI think Time-IL should be introduced as a sub-branch of Domain-IL."}, "signatures": ["ICLR.cc/2021/Conference/Paper1642/AnonReviewer5"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1642/AnonReviewer5"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "CLOPS: Continual Learning of Physiological Signals", "authorids": ["~Dani_Kiyasseh1", "tingting.zhu@eng.ox.ac.uk", "~David_A._Clifton1"], "authors": ["Dani Kiyasseh", "Tingting Zhu", "David A. Clifton"], "keywords": ["Continual learning", "physiological signals", "healthcare"], "abstract": "Deep learning algorithms are known to experience destructive interference when instances violate the assumption of being independent and identically distributed (i.i.d). This violation, however, is ubiquitous in clinical settings where data are streamed temporally and from a multitude of physiological sensors. To overcome this obstacle, we propose CLOPS, a replay-based continual learning strategy. In three continual learning scenarios based on three publically-available datasets, we show that CLOPS can outperform the state-of-the-art methods, GEM and MIR. Moreover, we propose end-to-end trainable parameters, which we term task-instance parameters, that can be used to quantify task difficulty and similarity. This quantification yields insights into both network interpretability and clinical applications, where task difficulty is poorly quantified.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kiyasseh|clops_continual_learning_of_physiological_signals", "supplementary_material": "/attachment/49796ec1806fe872109be2b790f1265b2ddc2452.zip", "pdf": "/pdf/abbad498238f1d6a353d71e52a4bf1703fc29065.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=1RqymILGmH", "_bibtex": "@misc{\nkiyasseh2021clops,\ntitle={{\\{}CLOPS{\\}}: Continual Learning of Physiological Signals},\nauthor={Dani Kiyasseh and Tingting Zhu and David A. Clifton},\nyear={2021},\nurl={https://openreview.net/forum?id=EKb4Z0aSNf}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "EKb4Z0aSNf", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1642/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1642/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1642/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1642/Authors|ICLR.cc/2021/Conference/Paper1642/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1642/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923857388, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1642/-/Official_Comment"}}}, {"id": "g5zIh9_4HB_", "original": null, "number": 6, "cdate": 1605719238256, "ddate": null, "tcdate": 1605719238256, "tmdate": 1605719238256, "tddate": null, "forum": "EKb4Z0aSNf", "replyto": "Vqg_phL9tYC", "invitation": "ICLR.cc/2021/Conference/Paper1642/-/Official_Comment", "content": {"title": "Response to Reviewer 5 - Round 1", "comment": "We thank the reviewer for taking the time and effort to review the manuscript and for providing us with valuable feedback. We address your comments below.\n\n**CONTRIBUTIONS**\nExisting continual learning algorithms are predominantly tested on the two datasets you had suggested we experiment with: Split-CIFAR10 or Split-CIFAR100. In addition, the tasks they concoct are quite unrealistic and several researchers in this domain have begun asking for realistic scenarios in which continual learning algorithms can be evaluated. You can refer to https://www.youtube.com/watch?v=N7XJ-QTEoHI at 28:00 for more details. We believe the continual learning setting is naturally found in the medical domain and such an observation motivates our entire paper. In addition to the potential clinical impact of our proposed approach, we believe it exhibits novelty in two distinct ways. First, we introduce an importance-guided buffer storage mechanism and an uncertainty-based acquisition mechanism. To the best of our knowledge, this has not been implemented before in the literature. Beyond novelty from the perspective of methodology, we also implement various state-of-the-art replayed based CL algorithms and illustrate that CLOPS outperforms those methods in three realistic CL scenarios. \n\n**EMPIRICAL EVIDENCE**\nWe would like to respectfully disagree that the empirical evidence is weak. We are happy to hear your thoughts on which components of the empirical results are considered weak from your perspective. We have illustrated that CLOPS (our method) outperforms state-of-the-art replay-based CL methods (GEM and MIR) in three realistic continual learning scenarios involving three large-scale ECG datasets. Moreover, not only do we obtain strong generalization performance (i.e., Average AUC), we also manage to mitigate catastrophic forgetting (i.e., higher backward weight transfer) relative to these state-of-the-art methods. \n\n**DEFINITION OF CONTINUAL LEARNING SCENARIOS**\nWe would also like to respectfully disagree that our definition of Time-IL is improper. To clarify any misunderstandings about the CL scenarios, we introduce Fig. 1 in the modified version of the manuscript. In a nutshell, and in our context Time-IL reflects a realistic scenario that is experienced in the medical domain and is defined by a multi-class classification problem in response to data collected at different times of the year (e.g., winter, summer). To use the scenario breakdown you have provided, Time-IL consists of different inputs and same outputs. If you feel this is better suited to Domain-IL, then we can introduce Time-IL as a sub-branch of Domain-IL. It just depends on how you introduce the domain shift. \n\nWe refer the reviewer to the modified version of the manuscript in order to get a better understanding and appreciation of the contributions of our proposed methodology and its potential impact on the clinical workflow. \n\nWe hope the above responses and the modified version of the manuscript have addressed your concerns. "}, "signatures": ["ICLR.cc/2021/Conference/Paper1642/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1642/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "CLOPS: Continual Learning of Physiological Signals", "authorids": ["~Dani_Kiyasseh1", "tingting.zhu@eng.ox.ac.uk", "~David_A._Clifton1"], "authors": ["Dani Kiyasseh", "Tingting Zhu", "David A. Clifton"], "keywords": ["Continual learning", "physiological signals", "healthcare"], "abstract": "Deep learning algorithms are known to experience destructive interference when instances violate the assumption of being independent and identically distributed (i.i.d). This violation, however, is ubiquitous in clinical settings where data are streamed temporally and from a multitude of physiological sensors. To overcome this obstacle, we propose CLOPS, a replay-based continual learning strategy. In three continual learning scenarios based on three publically-available datasets, we show that CLOPS can outperform the state-of-the-art methods, GEM and MIR. Moreover, we propose end-to-end trainable parameters, which we term task-instance parameters, that can be used to quantify task difficulty and similarity. This quantification yields insights into both network interpretability and clinical applications, where task difficulty is poorly quantified.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kiyasseh|clops_continual_learning_of_physiological_signals", "supplementary_material": "/attachment/49796ec1806fe872109be2b790f1265b2ddc2452.zip", "pdf": "/pdf/abbad498238f1d6a353d71e52a4bf1703fc29065.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=1RqymILGmH", "_bibtex": "@misc{\nkiyasseh2021clops,\ntitle={{\\{}CLOPS{\\}}: Continual Learning of Physiological Signals},\nauthor={Dani Kiyasseh and Tingting Zhu and David A. Clifton},\nyear={2021},\nurl={https://openreview.net/forum?id=EKb4Z0aSNf}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "EKb4Z0aSNf", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1642/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1642/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1642/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1642/Authors|ICLR.cc/2021/Conference/Paper1642/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1642/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923857388, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1642/-/Official_Comment"}}}, {"id": "URWOrG-oAU0", "original": null, "number": 5, "cdate": 1605717899481, "ddate": null, "tcdate": 1605717899481, "tmdate": 1605717899481, "tddate": null, "forum": "EKb4Z0aSNf", "replyto": "DKBbtKSwthi", "invitation": "ICLR.cc/2021/Conference/Paper1642/-/Official_Comment", "content": {"title": "Response to Reviewer 4 - Round 1", "comment": "We thank the reviewer for taking the time and effort to review the manuscript and for providing us with valuable feedback. We address your comments below.\n\n**EXPLANATION OF ABLATION RESULTS**\nAt b = 0.1, we find that the Random Storage approach outperforms CLOPS. This implies that a strategy, with all else being equal, that randomly chooses 10% of the instances to store into the buffer is better than one that leverages the storage function (Eq. 6) to store instances. We offer a potential explanation for this observation. Recall that our storage mechanism stores the relatively \u2018easiest-to-classify\u2019 instances in the buffer. If these \u2018easiest-to-classify\u2019 instances happen to belong predominantly to only one of the two classes from the previous task, then the buffer ends up containing data that is heavily biased towards a small subset of classes, and thus exhibits poor coverage of the data distribution from previous tasks. Therefore, downstream acquisition of these instances from the buffer might hinder performance. \n\nTo explain the relative improvement in performance of Random Acquisition over CLOPS at a=0.5, we offer a similar argument based on data coverage. Recall that our acquisition strategy is based on how uncertain a network is about a particular instance. Therefore, if the network happens to be most uncertain about instances from a subset of classes, then the instances that are acquired from the buffer will be a poor reflection of the data distribution of the previous tasks. This, in turn, provides an imbalanced supervisory signal upon training and might hinder performance. \n\n**NEW RANDOM STORAGE AND ACQUISITION EXPERIMENT**\nIn the modified version of the manuscript, we illustrate the results for the random storage and acquisition experiments which we just recently conducted (Sec. 6.4). We provide an explanation for our results that is focused on the utility of task-instance parameters as loss-weighting mechanisms and the utility of leveraging an acquisition function to acquire instances from the buffer. \n\n**BROAD SENTENCES**\nWe have modified sentences that consist of broad generalizations. \n\n**EXPLANATION OF LOSS-WEIGHTING MECHANISM**\nWe modify Sec. 4.1.1 to improve clarity. In the process, we have attempted to better explain the role of task-instance parameters as a loss-weighting mechanism and the need for a regularization term to avoid a vanishing loss term. Please inform us if this section remains unclear. \n\n**FIGURE IMPROVEMENTS**\nWe have added coloured-blocks to each of the figures containing validation AUC curves to illustrate when a task is currently being trained on. This should improve the clarity of the figures. We have also annotated what used to be Fig. 5 (and is now Fig. 2) to improve clarity and help explain the intuition behind our storage function design. \n\nWe hope the above responses and the modified version of the manuscript have addressed your concerns. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1642/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1642/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "CLOPS: Continual Learning of Physiological Signals", "authorids": ["~Dani_Kiyasseh1", "tingting.zhu@eng.ox.ac.uk", "~David_A._Clifton1"], "authors": ["Dani Kiyasseh", "Tingting Zhu", "David A. Clifton"], "keywords": ["Continual learning", "physiological signals", "healthcare"], "abstract": "Deep learning algorithms are known to experience destructive interference when instances violate the assumption of being independent and identically distributed (i.i.d). This violation, however, is ubiquitous in clinical settings where data are streamed temporally and from a multitude of physiological sensors. To overcome this obstacle, we propose CLOPS, a replay-based continual learning strategy. In three continual learning scenarios based on three publically-available datasets, we show that CLOPS can outperform the state-of-the-art methods, GEM and MIR. Moreover, we propose end-to-end trainable parameters, which we term task-instance parameters, that can be used to quantify task difficulty and similarity. This quantification yields insights into both network interpretability and clinical applications, where task difficulty is poorly quantified.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kiyasseh|clops_continual_learning_of_physiological_signals", "supplementary_material": "/attachment/49796ec1806fe872109be2b790f1265b2ddc2452.zip", "pdf": "/pdf/abbad498238f1d6a353d71e52a4bf1703fc29065.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=1RqymILGmH", "_bibtex": "@misc{\nkiyasseh2021clops,\ntitle={{\\{}CLOPS{\\}}: Continual Learning of Physiological Signals},\nauthor={Dani Kiyasseh and Tingting Zhu and David A. Clifton},\nyear={2021},\nurl={https://openreview.net/forum?id=EKb4Z0aSNf}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "EKb4Z0aSNf", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1642/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1642/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1642/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1642/Authors|ICLR.cc/2021/Conference/Paper1642/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1642/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923857388, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1642/-/Official_Comment"}}}, {"id": "xnhTErR--7", "original": null, "number": 4, "cdate": 1605717242491, "ddate": null, "tcdate": 1605717242491, "tmdate": 1605717297163, "tddate": null, "forum": "EKb4Z0aSNf", "replyto": "M2-p8cWciVL", "invitation": "ICLR.cc/2021/Conference/Paper1642/-/Official_Comment", "content": {"title": "Response to Reviewer 2 - Round 1", "comment": "We thank the reviewer for taking the time and effort to review the manuscript and for providing us with valuable feedback. We address your comments below.\n\n**CLARIFYING EQUATION 5**\nWe modify Sec. 4.1.1 to improve clarity and avoid any potential confusion that may arise based on previous and current task terminology. \n\n**FIGURE FOR SECTION 6.3**\nAlthough we do have this figure at hand, we had opted to relegate it to the Appendix (Appendix E.2). Its results are consistent with those shown elsewhere in the main manuscript and would have served to reaffirm our findings. In the interest of including more description of the ablation studies and validation of the task-instance parameters, we opted to keep this figure in Appendix E.2. \n\n**EXPLANATION OF REGULARIZATION TERM**\nAs mentioned above, we modified Sec. 4.1.1 in order to improve clarity and address potential confusions that had arisen in that section. We include an additional equation (Eq. 2) to help illustrate the issue we are trying to circumvent by introducing the regularization term. In a nutshell, Eq. 2 suggests that instances with a larger loss value (hard-to-classify) will exhibit lower values of Bit. As presented, however, the derivative Bit = Lit > 0 implies that Bit -> 0 as training progresses, which we confirmed empirically. This implies that L -> 0 and the network is unable to learn from the data. To avoid this behaviour, we initialize Bit as 1 (as with a normal loss function) and introduce a regularization term to penalize extreme deviations from 1. Please refer to Sec. 4.1.1 for a more complete picture. \n\n**MOTIVATION FOR STORAGE FUNCTION**\nWe provide additional motivation for the choice of our storage function in Sec. 4.1.2. This explanation comes alongside Fig. 2 (which was in the Appendix) and is now annotated to better complement the explanation. We also implement a variant of our storage function (Eq. 14, Appendix H), as recommended by the reviewer, and illustrate its inferior performance relative to the one we had chosen to use in the main manuscript. These results can be found in Appendix H. \n\n**NUMBER OF SEEDS**\nEach of our experiments are conducted over 5 seeds in attempt to avoid any one-off positive or negative outlier experiments and obtain a reasonable estimate of the performance of the methods. This number of seeds is quite typical in the machine learning domain and is partially motivated by the computational cost of experimentation\n\n**MEDICAL JARGON**\nWe explain the importance of the ST-elevation illustrated in Fig. 6 and annotate that figure to help machine learning practitioners better understand the implications of it in this context. \n\nWe hope the above responses and the modified version of the manuscript have addressed your concerns. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1642/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1642/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "CLOPS: Continual Learning of Physiological Signals", "authorids": ["~Dani_Kiyasseh1", "tingting.zhu@eng.ox.ac.uk", "~David_A._Clifton1"], "authors": ["Dani Kiyasseh", "Tingting Zhu", "David A. Clifton"], "keywords": ["Continual learning", "physiological signals", "healthcare"], "abstract": "Deep learning algorithms are known to experience destructive interference when instances violate the assumption of being independent and identically distributed (i.i.d). This violation, however, is ubiquitous in clinical settings where data are streamed temporally and from a multitude of physiological sensors. To overcome this obstacle, we propose CLOPS, a replay-based continual learning strategy. In three continual learning scenarios based on three publically-available datasets, we show that CLOPS can outperform the state-of-the-art methods, GEM and MIR. Moreover, we propose end-to-end trainable parameters, which we term task-instance parameters, that can be used to quantify task difficulty and similarity. This quantification yields insights into both network interpretability and clinical applications, where task difficulty is poorly quantified.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kiyasseh|clops_continual_learning_of_physiological_signals", "supplementary_material": "/attachment/49796ec1806fe872109be2b790f1265b2ddc2452.zip", "pdf": "/pdf/abbad498238f1d6a353d71e52a4bf1703fc29065.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=1RqymILGmH", "_bibtex": "@misc{\nkiyasseh2021clops,\ntitle={{\\{}CLOPS{\\}}: Continual Learning of Physiological Signals},\nauthor={Dani Kiyasseh and Tingting Zhu and David A. Clifton},\nyear={2021},\nurl={https://openreview.net/forum?id=EKb4Z0aSNf}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "EKb4Z0aSNf", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1642/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1642/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1642/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1642/Authors|ICLR.cc/2021/Conference/Paper1642/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1642/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923857388, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1642/-/Official_Comment"}}}, {"id": "YpLe9a-zcws", "original": null, "number": 3, "cdate": 1605716320433, "ddate": null, "tcdate": 1605716320433, "tmdate": 1605716320433, "tddate": null, "forum": "EKb4Z0aSNf", "replyto": "8h1vP7FyXS3", "invitation": "ICLR.cc/2021/Conference/Paper1642/-/Official_Comment", "content": {"title": "Response to Reviewer 1 - Round 1 ", "comment": "We thank the reviewer for taking the time and effort to review the manuscript and for providing us with valuable feedback. We address your comments below.\n\n**TASK DESCRIPTION**\nIn our manuscript, the physiological data are predominantly electrocardiogram (ECG) signals. These are time-series signals that are associated with cardiac arrhythmia labels. As outlined in Appendix B, and as is typical in time-series analysis, we split time-series signals into frames of shorter duration. In our case, each frame was 2500 samples in length which roughly approximates to 5 seconds worth of ECG data (depending on the sampling rate of the dataset). Each of these frames was then associated with a cardiac arrhythmia label. \n\nThe Class-IL scenario consists of binary cardiac arrhythmia classification. For example, in Fig. 2 (Sec. 6.1), the first task, [0-1], refers to a binary classification of the two cardiac arrhythmia classes labelled 0 and 1. The second task, [2-3], refers to a binary classification of the two cardiac arrhythmia classes labelled 2 and 3, and so on. Our reference to task [0-2] in the second paragraph of Sec. 6.1 is a typo and should instead state \u2018task [2-3]\u2019. To more explicitly address the potential confusion surrounding the specific task at hand (cardiac arrhythmia classification) in addition to the continual learning scenarios, we have included a figure (Fig. 1) to summarize the three continual learning scenarios that we implement. \n\n**STORAGE AND ACQUISITION FRACTIONS**\nWe discuss the storage and acquisition fractions in more detail in the methods section to reflect their importance on replay-based continual learning methods. More specifically, we talk about the implications of the storage fraction in Sec. 4.1.2 (last paragraph) and the acquisition fraction in Sec. 4.2. \n\n**FINE-TUNING VS MULTI-TASK LEARNING**\nTypically, one does expect multi-task learning (MTL) to perform better than its continual learning counterpart (see Table 2 Sec 6.3 and Table 8 Appendix E.1). As you mentioned, this is primarily due to the presence of a strong supervisory signal during training. Occasionally, however, a continual learning method and even a na\u00efve fine-tuning one can outperform the MTL setup. This implies that the final set of parameters obtained by the continual learning methods (i.e., at epoch 120 in the Class-IL scenario) achieve stronger generalization performance than the MTL set of parameters. We hypothesize that this improvement in generalization performance can be attributed to positive weight transfer. Such positive weight transfer can be due to the existence of a curriculum (as with curriculum learning) where network generalization improves as a result of presenting tasks of different levels of difficulty to a network in a sequential manner. In fact, we present a set of curriculum learning experiments in Sec. 6.5 to further explore this notion. \n\nAs for the effect of task order on the performance of continual learning methods, we hypothesize that one potential explanation lies in the relative difficulty of tasks that are presented sequentially to a network. To support our statement, we direct your attention to Table. 3 (Sec. 6.5) where we order the tasks that are presented to a network based either on a curriculum (easy-to-hard) or an anti-curriculum (hard-to-easy) and observe a discrepancy in the Average AUC (0.744 vs. 0.783).\n\n**EXPLANATION OF TIME-IL RESULTS**\nAs for Sec. 6.2 (Time-IL scenario), we claim that CLOPS is better than fine-tuning as it pertains to training efficiency. To substantiate this claim, we direct the reviewer to Fig. 4 (Sec 6.2). At epoch 40, the Fine-tuning method and CLOPS achieve an AUC of 0.50 and 0.62, respectively. Even if we were to consider the lower end of the CLOPS performance, due to the relatively higher variance, CLOPS still achieves an AUC of 0.60, which is higher than its Fine-tuning counterpart. This discrepancy indicates that CLOPS has exhibited positive forward weight transfer. To make an assessment of the training efficiency of CLOPS, we ask the question, \u201chow many epochs of training does it take for the Fine-tuning method to achieve the same AUC (i.e., 0.62) as CLOPS had achieved in a single training epoch?\u201d We find that it takes the Fine-tuning method 20 training epochs (from epoch 40 to epoch 60) to arrive at that AUC value.\n\n**HOW TO USE TASK SIMILARITY**\nAs for the purpose of quantifying task-similarity, we direct the reviewer to Sec. 6.5 (paragraph 3). In a nutshell, we exploit task-similarity to order the tasks that are presented to a network and, in turn, help design the curriculum learning procedure. To address this potential confusion more explicitly, we have included in Sec. 6.5 (a) our definition of task-similarity and (b) a figure (Fig. 7) illustrating how we exploited task-similarity to design the task order. \n\nWe hope the above responses and the modified version of the manuscript have addressed your concerns. "}, "signatures": ["ICLR.cc/2021/Conference/Paper1642/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1642/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "CLOPS: Continual Learning of Physiological Signals", "authorids": ["~Dani_Kiyasseh1", "tingting.zhu@eng.ox.ac.uk", "~David_A._Clifton1"], "authors": ["Dani Kiyasseh", "Tingting Zhu", "David A. Clifton"], "keywords": ["Continual learning", "physiological signals", "healthcare"], "abstract": "Deep learning algorithms are known to experience destructive interference when instances violate the assumption of being independent and identically distributed (i.i.d). This violation, however, is ubiquitous in clinical settings where data are streamed temporally and from a multitude of physiological sensors. To overcome this obstacle, we propose CLOPS, a replay-based continual learning strategy. In three continual learning scenarios based on three publically-available datasets, we show that CLOPS can outperform the state-of-the-art methods, GEM and MIR. Moreover, we propose end-to-end trainable parameters, which we term task-instance parameters, that can be used to quantify task difficulty and similarity. This quantification yields insights into both network interpretability and clinical applications, where task difficulty is poorly quantified.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kiyasseh|clops_continual_learning_of_physiological_signals", "supplementary_material": "/attachment/49796ec1806fe872109be2b790f1265b2ddc2452.zip", "pdf": "/pdf/abbad498238f1d6a353d71e52a4bf1703fc29065.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=1RqymILGmH", "_bibtex": "@misc{\nkiyasseh2021clops,\ntitle={{\\{}CLOPS{\\}}: Continual Learning of Physiological Signals},\nauthor={Dani Kiyasseh and Tingting Zhu and David A. Clifton},\nyear={2021},\nurl={https://openreview.net/forum?id=EKb4Z0aSNf}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "EKb4Z0aSNf", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1642/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1642/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1642/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1642/Authors|ICLR.cc/2021/Conference/Paper1642/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1642/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923857388, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1642/-/Official_Comment"}}}, {"id": "M2-p8cWciVL", "original": null, "number": 2, "cdate": 1604098281709, "ddate": null, "tcdate": 1604098281709, "tmdate": 1605024393686, "tddate": null, "forum": "EKb4Z0aSNf", "replyto": "EKb4Z0aSNf", "invitation": "ICLR.cc/2021/Conference/Paper1642/-/Official_Review", "content": {"title": "a unique combination of applying Active Learning to physiological signals for CL", "review": "The authors propose a learning methodology designed to offset detriments to algorithm performance that arise when instances are not i.i.d (independent and identically distributed), focusing on cases in continual learning (CL) given physiological signals. They designed a replay-based learning method that handles an instance buffer using Importance-guided Storage and Uncertainty-based Acquisition strategies. They apply their method on Class, Time and Domain types of CL, and they introduce t-Step Backward Weight Transfer and  Lambda Backward Weight Transfer methods by which to evaluate their method. They conclude with two ablation studies to explore an explanation for their method\u2019s performance and attempt to validate their hypotheses based on these studies.\n\nSuggestions: \n* Make more explicit, in 4.1.2 (regarding equation 5), which type of task is being referred to, when comparing between prior and present tasks.\n* If time permits, a figure for Experimental Results section 6.3 (to correspond to figures provided in 6.1 and 6.2) would be nice.\n* A brief explanation should be included for the -1 in the regularization term in equation (2)\n* In section 4.1.2, the need for a storage function was well motivated; however, the particular solution that the authors chose needs more motivation (and comparison with possible alternatives such as \\beta^2. Not suggesting that this be empirically verified.)\n* In Figure 2, SDs were calculated over 5 seeds. Was this low number due to the computational cost of experimentation?\n* For a lay ML practitioner to appreciate the assertion made in section 6.5, \"ST-elevation\" needs to be defined more concretely.\n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1642/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1642/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "CLOPS: Continual Learning of Physiological Signals", "authorids": ["~Dani_Kiyasseh1", "tingting.zhu@eng.ox.ac.uk", "~David_A._Clifton1"], "authors": ["Dani Kiyasseh", "Tingting Zhu", "David A. Clifton"], "keywords": ["Continual learning", "physiological signals", "healthcare"], "abstract": "Deep learning algorithms are known to experience destructive interference when instances violate the assumption of being independent and identically distributed (i.i.d). This violation, however, is ubiquitous in clinical settings where data are streamed temporally and from a multitude of physiological sensors. To overcome this obstacle, we propose CLOPS, a replay-based continual learning strategy. In three continual learning scenarios based on three publically-available datasets, we show that CLOPS can outperform the state-of-the-art methods, GEM and MIR. Moreover, we propose end-to-end trainable parameters, which we term task-instance parameters, that can be used to quantify task difficulty and similarity. This quantification yields insights into both network interpretability and clinical applications, where task difficulty is poorly quantified.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kiyasseh|clops_continual_learning_of_physiological_signals", "supplementary_material": "/attachment/49796ec1806fe872109be2b790f1265b2ddc2452.zip", "pdf": "/pdf/abbad498238f1d6a353d71e52a4bf1703fc29065.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=1RqymILGmH", "_bibtex": "@misc{\nkiyasseh2021clops,\ntitle={{\\{}CLOPS{\\}}: Continual Learning of Physiological Signals},\nauthor={Dani Kiyasseh and Tingting Zhu and David A. Clifton},\nyear={2021},\nurl={https://openreview.net/forum?id=EKb4Z0aSNf}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "EKb4Z0aSNf", "replyto": "EKb4Z0aSNf", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1642/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538114017, "tmdate": 1606915779742, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1642/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1642/-/Official_Review"}}}, {"id": "8h1vP7FyXS3", "original": null, "number": 5, "cdate": 1604657451032, "ddate": null, "tcdate": 1604657451032, "tmdate": 1605024393547, "tddate": null, "forum": "EKb4Z0aSNf", "replyto": "EKb4Z0aSNf", "invitation": "ICLR.cc/2021/Conference/Paper1642/-/Official_Review", "content": {"title": "interesting problem; unclear task description;", "review": "The paper presents, CLOPS, a new replay-based continual learning (CL) strategy for physiological signals.\n\nThe paper has the following merits. It is well-organized. I can understand its method, different settings of continual learning, and more ablation study on the model parameters. It did through experiments on the 3 datasets and get good results showing CLOPS beats naive baselines like multi-tasks learning, Fine-tuning as well as other CL methods like GEM, MIR.  \n\nOn the other side, I have the following concerns about the submission.\n\n1. what is the task exactly?\n\nIs cardiac arrhythmia label also time series? Does it have the same sampling frequency as physiological data?\nIn the Class-IL setting, are you doing multi-class classification or binary class? The notation is confusing. Is task [0,1], task [2,3] meaning binary classifications of class 0 v.s  class 1 and class 2 v.s. class 3? Then what does task [0-2] mean? It appears in the second paragraph of section 6.1.\n\n2. not enough highlight for important parameters (storage and acquisition fractions, b, and a).\n\nStorage and acquisition fractions, \"b\" and \"a\" are important parameters of replay-based CL algorithms. However currently, the definition of them is hidden in section 4 and figure captions. They are important parameters. The author even did an ablation study in section 6.4 on it. However, when I first read the paper, after reading the method section, I had no impression of \"a\" and \"b\".\n\n3. not well explained experimental result. \n\nIn Table 1, it shows Fine-tuning works better than Multi-task learning. It is a bit counter-intuitive to me since MTL has stronger supervision at all the time. Do there exist differences between task difficulty? It seems the task order influences the performance of fine-tuning according to the result in Table 1 and Table 9 in appendix G. Can the author provide any explanation?\nIn section 6.2, the author argues that CLOPS achieves an AUC after one epoch while fine-tuning requires 20 epochs to achieve that. First, I am not sure whether it is true since it seems the performance of the fine-tuning is not converged yet, its performance may still increasing. Second, clearly, CLOPS has a much larger performance variance than fine-tuning.  So I think the current result does not support that CLOPS is better than fine-tuning. \n\n4. unclear usage of task similarity.\n\nIn the appendix, the author shows the task similarity produced by the model. However, I am not sure what we can do with such task similarity. In the abstract, the author claim \"this quantification yields insights into both network interpretability and clinical applications\". However, in the paper, I can not see any arguments about what insight of network interpretability and clinical applications we can get.", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1642/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1642/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "CLOPS: Continual Learning of Physiological Signals", "authorids": ["~Dani_Kiyasseh1", "tingting.zhu@eng.ox.ac.uk", "~David_A._Clifton1"], "authors": ["Dani Kiyasseh", "Tingting Zhu", "David A. Clifton"], "keywords": ["Continual learning", "physiological signals", "healthcare"], "abstract": "Deep learning algorithms are known to experience destructive interference when instances violate the assumption of being independent and identically distributed (i.i.d). This violation, however, is ubiquitous in clinical settings where data are streamed temporally and from a multitude of physiological sensors. To overcome this obstacle, we propose CLOPS, a replay-based continual learning strategy. In three continual learning scenarios based on three publically-available datasets, we show that CLOPS can outperform the state-of-the-art methods, GEM and MIR. Moreover, we propose end-to-end trainable parameters, which we term task-instance parameters, that can be used to quantify task difficulty and similarity. This quantification yields insights into both network interpretability and clinical applications, where task difficulty is poorly quantified.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kiyasseh|clops_continual_learning_of_physiological_signals", "supplementary_material": "/attachment/49796ec1806fe872109be2b790f1265b2ddc2452.zip", "pdf": "/pdf/abbad498238f1d6a353d71e52a4bf1703fc29065.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=1RqymILGmH", "_bibtex": "@misc{\nkiyasseh2021clops,\ntitle={{\\{}CLOPS{\\}}: Continual Learning of Physiological Signals},\nauthor={Dani Kiyasseh and Tingting Zhu and David A. Clifton},\nyear={2021},\nurl={https://openreview.net/forum?id=EKb4Z0aSNf}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "EKb4Z0aSNf", "replyto": "EKb4Z0aSNf", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1642/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538114017, "tmdate": 1606915779742, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1642/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1642/-/Official_Review"}}}], "count": 18}