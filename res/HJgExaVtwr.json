{"notes": [{"id": "kAZY0BtPWc-", "original": null, "number": 6, "cdate": 1598348056064, "ddate": null, "tcdate": 1598348056064, "tmdate": 1598348056064, "tddate": null, "forum": "HJgExaVtwr", "replyto": "NgsFwZ5fAxE", "invitation": "ICLR.cc/2020/Conference/Paper335/-/Public_Comment", "content": {"title": "About warm-up, co-training", "comment": "Dear authors, \n\nI hope all of you are very well. Many thanks for your kind reply. \nI totally understand that differnt persons have different appetite. Presumably,  it is simple in the context of co-training and warm-up training. \n\nVery personally, I am not in favor of warm-up training, co-training due to those questions \n(those questions are for warm-up training and co-training, not specifically for this work, i.e. DivideMix ): \n1. How many iterations should we train in warm-up stage? How to determine this?  Will  warm-up stage overfit? \n2. Why two networks since by SGD (sometimes Dropout), one network can be interpreted as an ensemble model.  Or why not three, or even many more networks?    \n\nSpecifically for this work,  in every epoch, DivideMix divides the whole dataset into two subsets (labelled and unlabelled subsets). Therefore, it seems to me that DivideMix is complex, and not scalable to very large datasets.\n\nFinally, please kindly let me know if I am wrong.  \n\nMany thanks. "}, "signatures": ["~Xinshao_Wang1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Xinshao_Wang1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DivideMix: Learning with Noisy Labels as Semi-supervised Learning", "authors": ["Junnan Li", "Richard Socher", "Steven C.H. Hoi"], "authorids": ["junnan.li@salesforce.com", "rsocher@salesforce.com", "shoi@salesforce.com"], "keywords": ["label noise", "semi-supervised learning"], "TL;DR": "We propose a novel semi-supervised learning approach with SOTA performance on combating learning with noisy labels.", "abstract": "Deep neural networks are known to be annotation-hungry. Numerous efforts have been devoted to reducing the annotation cost when learning with deep networks. Two prominent directions include learning with noisy labels and semi-supervised learning by exploiting unlabeled data. In this work, we propose DivideMix, a novel framework for learning with noisy labels by leveraging semi-supervised learning techniques. In particular, DivideMix models the per-sample loss distribution with a mixture model to dynamically divide the training data into a labeled set with clean samples and an unlabeled set with noisy samples, and trains the model on both the labeled and unlabeled data in a semi-supervised manner. To avoid confirmation bias, we simultaneously train two diverged networks where each network uses the dataset division from the other network. During the semi-supervised training phase, we improve the MixMatch strategy by performing label co-refinement and label co-guessing on labeled and unlabeled samples, respectively. Experiments on multiple benchmark datasets demonstrate substantial improvements over state-of-the-art methods. Code is available at https://github.com/LiJunnan1992/DivideMix .", "pdf": "/pdf/b3cf972104156ac812aba28a8c140879fe430148.pdf", "paperhash": "li|dividemix_learning_with_noisy_labels_as_semisupervised_learning", "_bibtex": "@inproceedings{\nLi2020DivideMix:,\ntitle={DivideMix: Learning with Noisy Labels as Semi-supervised Learning},\nauthor={Junnan Li and Richard Socher and Steven C.H. Hoi},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJgExaVtwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/e4a6ce14c28738ac3972e56acaeea772fdcc0d70.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJgExaVtwr", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504210554, "tmdate": 1576860578820, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper335/Authors", "ICLR.cc/2020/Conference/Paper335/Reviewers", "ICLR.cc/2020/Conference/Paper335/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper335/-/Public_Comment"}}}, {"id": "NgsFwZ5fAxE", "original": null, "number": 13, "cdate": 1598317590304, "ddate": null, "tcdate": 1598317590304, "tmdate": 1598317590304, "tddate": null, "forum": "HJgExaVtwr", "replyto": "keqS67sTCbi", "invitation": "ICLR.cc/2020/Conference/Paper335/-/Official_Comment", "content": {"title": "Thanks but DivideMix is a simple method.", "comment": "Hi Xinshao,\n\nThanks for your comments and sharing of your papers. We would like to note that the techniques used in DivideMix (i.e. warm-up, co-training, mixup) are commonly adopted in many previous works. Rather than being complex, DivideMix is a simple method that is easy to use."}, "signatures": ["ICLR.cc/2020/Conference/Paper335/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper335/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DivideMix: Learning with Noisy Labels as Semi-supervised Learning", "authors": ["Junnan Li", "Richard Socher", "Steven C.H. Hoi"], "authorids": ["junnan.li@salesforce.com", "rsocher@salesforce.com", "shoi@salesforce.com"], "keywords": ["label noise", "semi-supervised learning"], "TL;DR": "We propose a novel semi-supervised learning approach with SOTA performance on combating learning with noisy labels.", "abstract": "Deep neural networks are known to be annotation-hungry. Numerous efforts have been devoted to reducing the annotation cost when learning with deep networks. Two prominent directions include learning with noisy labels and semi-supervised learning by exploiting unlabeled data. In this work, we propose DivideMix, a novel framework for learning with noisy labels by leveraging semi-supervised learning techniques. In particular, DivideMix models the per-sample loss distribution with a mixture model to dynamically divide the training data into a labeled set with clean samples and an unlabeled set with noisy samples, and trains the model on both the labeled and unlabeled data in a semi-supervised manner. To avoid confirmation bias, we simultaneously train two diverged networks where each network uses the dataset division from the other network. During the semi-supervised training phase, we improve the MixMatch strategy by performing label co-refinement and label co-guessing on labeled and unlabeled samples, respectively. Experiments on multiple benchmark datasets demonstrate substantial improvements over state-of-the-art methods. Code is available at https://github.com/LiJunnan1992/DivideMix .", "pdf": "/pdf/b3cf972104156ac812aba28a8c140879fe430148.pdf", "paperhash": "li|dividemix_learning_with_noisy_labels_as_semisupervised_learning", "_bibtex": "@inproceedings{\nLi2020DivideMix:,\ntitle={DivideMix: Learning with Noisy Labels as Semi-supervised Learning},\nauthor={Junnan Li and Richard Socher and Steven C.H. Hoi},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJgExaVtwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/e4a6ce14c28738ac3972e56acaeea772fdcc0d70.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJgExaVtwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper335/Authors", "ICLR.cc/2020/Conference/Paper335/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper335/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper335/Reviewers", "ICLR.cc/2020/Conference/Paper335/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper335/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper335/Authors|ICLR.cc/2020/Conference/Paper335/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504172954, "tmdate": 1576860545458, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper335/Authors", "ICLR.cc/2020/Conference/Paper335/Reviewers", "ICLR.cc/2020/Conference/Paper335/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper335/-/Official_Comment"}}}, {"id": "keqS67sTCbi", "original": null, "number": 5, "cdate": 1598301817870, "ddate": null, "tcdate": 1598301817870, "tmdate": 1598302385584, "tddate": null, "forum": "HJgExaVtwr", "replyto": "HJgExaVtwr", "invitation": "ICLR.cc/2020/Conference/Paper335/-/Public_Comment", "content": {"title": "Algorithm 1 seems very complex. ", "comment": "I have read this work carefully. However, it seems to me that *Algorithm 1 is extremely complex*: \n\n1. Two networks are required to train; \n2. A warm-up stage (standard training) is required to train each network. How many iterations should we train? How to determine this? \n3. In every epoch, we need to divide the whole dataset into two subsets (labelled and unlabelled subsets). Therefore, every epoch will *take much more time than standard training.* \n  3.1 Two Gaussian Mixture Models are trained;\n  3.2 Two networks: num_iters \\times 2;\n  3.3 Mixup data augmentation; \n\nConsequently, here, I would like to share some much simpler methods: \n1.  IMAE for Noise-Robust Learning: Mean Absolute Error Does Not Treat Examples Equally and Gradient Magnitude's Variance Matters \nhttps://arxiv.org/abs/1903.12141\n2. Derivative Manipulation for General Example Weighting \nhttps://arxiv.org/abs/1905.11233\n3. Progressive Self Label Correction (ProSelfLC) for Training Robust Deep Neural Networks \nhttps://xinshaoamoswang.github.io/blogs/2020-06-07-Progressive-self-label-correction/\n\nhttps://xinshaoamoswang.github.io/blogs/2020-06-14-Robust-Deep-LearningviaDerivativeManipulationIMAE/"}, "signatures": ["~Xinshao_Wang1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Xinshao_Wang1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DivideMix: Learning with Noisy Labels as Semi-supervised Learning", "authors": ["Junnan Li", "Richard Socher", "Steven C.H. Hoi"], "authorids": ["junnan.li@salesforce.com", "rsocher@salesforce.com", "shoi@salesforce.com"], "keywords": ["label noise", "semi-supervised learning"], "TL;DR": "We propose a novel semi-supervised learning approach with SOTA performance on combating learning with noisy labels.", "abstract": "Deep neural networks are known to be annotation-hungry. Numerous efforts have been devoted to reducing the annotation cost when learning with deep networks. Two prominent directions include learning with noisy labels and semi-supervised learning by exploiting unlabeled data. In this work, we propose DivideMix, a novel framework for learning with noisy labels by leveraging semi-supervised learning techniques. In particular, DivideMix models the per-sample loss distribution with a mixture model to dynamically divide the training data into a labeled set with clean samples and an unlabeled set with noisy samples, and trains the model on both the labeled and unlabeled data in a semi-supervised manner. To avoid confirmation bias, we simultaneously train two diverged networks where each network uses the dataset division from the other network. During the semi-supervised training phase, we improve the MixMatch strategy by performing label co-refinement and label co-guessing on labeled and unlabeled samples, respectively. Experiments on multiple benchmark datasets demonstrate substantial improvements over state-of-the-art methods. Code is available at https://github.com/LiJunnan1992/DivideMix .", "pdf": "/pdf/b3cf972104156ac812aba28a8c140879fe430148.pdf", "paperhash": "li|dividemix_learning_with_noisy_labels_as_semisupervised_learning", "_bibtex": "@inproceedings{\nLi2020DivideMix:,\ntitle={DivideMix: Learning with Noisy Labels as Semi-supervised Learning},\nauthor={Junnan Li and Richard Socher and Steven C.H. Hoi},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJgExaVtwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/e4a6ce14c28738ac3972e56acaeea772fdcc0d70.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJgExaVtwr", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504210554, "tmdate": 1576860578820, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper335/Authors", "ICLR.cc/2020/Conference/Paper335/Reviewers", "ICLR.cc/2020/Conference/Paper335/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper335/-/Public_Comment"}}}, {"id": "HJgExaVtwr", "original": "SkxhErJIvB", "number": 335, "cdate": 1569438956171, "ddate": null, "tcdate": 1569438956171, "tmdate": 1583912036793, "tddate": null, "forum": "HJgExaVtwr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "DivideMix: Learning with Noisy Labels as Semi-supervised Learning", "authors": ["Junnan Li", "Richard Socher", "Steven C.H. Hoi"], "authorids": ["junnan.li@salesforce.com", "rsocher@salesforce.com", "shoi@salesforce.com"], "keywords": ["label noise", "semi-supervised learning"], "TL;DR": "We propose a novel semi-supervised learning approach with SOTA performance on combating learning with noisy labels.", "abstract": "Deep neural networks are known to be annotation-hungry. Numerous efforts have been devoted to reducing the annotation cost when learning with deep networks. Two prominent directions include learning with noisy labels and semi-supervised learning by exploiting unlabeled data. In this work, we propose DivideMix, a novel framework for learning with noisy labels by leveraging semi-supervised learning techniques. In particular, DivideMix models the per-sample loss distribution with a mixture model to dynamically divide the training data into a labeled set with clean samples and an unlabeled set with noisy samples, and trains the model on both the labeled and unlabeled data in a semi-supervised manner. To avoid confirmation bias, we simultaneously train two diverged networks where each network uses the dataset division from the other network. During the semi-supervised training phase, we improve the MixMatch strategy by performing label co-refinement and label co-guessing on labeled and unlabeled samples, respectively. Experiments on multiple benchmark datasets demonstrate substantial improvements over state-of-the-art methods. Code is available at https://github.com/LiJunnan1992/DivideMix .", "pdf": "/pdf/b3cf972104156ac812aba28a8c140879fe430148.pdf", "paperhash": "li|dividemix_learning_with_noisy_labels_as_semisupervised_learning", "_bibtex": "@inproceedings{\nLi2020DivideMix:,\ntitle={DivideMix: Learning with Noisy Labels as Semi-supervised Learning},\nauthor={Junnan Li and Richard Socher and Steven C.H. Hoi},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJgExaVtwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/e4a6ce14c28738ac3972e56acaeea772fdcc0d70.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 14, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "fDUHvhjYl6", "original": null, "number": 2, "cdate": 1577739973292, "ddate": null, "tcdate": 1577739973292, "tmdate": 1577739973292, "tddate": null, "forum": "HJgExaVtwr", "replyto": "HJgExaVtwr", "invitation": "ICLR.cc/2020/Conference/Paper335/-/Public_Comment", "content": {"title": "Relevant paper", "comment": "Hi,\n\nIt is quite interesting paper!\n\nSince your work is closely based on MixMatch,  I would like to point out to our previous work ICT, which is a precursor of MixMatch.\n\nhttps://www.ijcai.org/proceedings/2019/0504.pdf"}, "signatures": ["~Vikas_Verma1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Vikas_Verma1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DivideMix: Learning with Noisy Labels as Semi-supervised Learning", "authors": ["Junnan Li", "Richard Socher", "Steven C.H. Hoi"], "authorids": ["junnan.li@salesforce.com", "rsocher@salesforce.com", "shoi@salesforce.com"], "keywords": ["label noise", "semi-supervised learning"], "TL;DR": "We propose a novel semi-supervised learning approach with SOTA performance on combating learning with noisy labels.", "abstract": "Deep neural networks are known to be annotation-hungry. Numerous efforts have been devoted to reducing the annotation cost when learning with deep networks. Two prominent directions include learning with noisy labels and semi-supervised learning by exploiting unlabeled data. In this work, we propose DivideMix, a novel framework for learning with noisy labels by leveraging semi-supervised learning techniques. In particular, DivideMix models the per-sample loss distribution with a mixture model to dynamically divide the training data into a labeled set with clean samples and an unlabeled set with noisy samples, and trains the model on both the labeled and unlabeled data in a semi-supervised manner. To avoid confirmation bias, we simultaneously train two diverged networks where each network uses the dataset division from the other network. During the semi-supervised training phase, we improve the MixMatch strategy by performing label co-refinement and label co-guessing on labeled and unlabeled samples, respectively. Experiments on multiple benchmark datasets demonstrate substantial improvements over state-of-the-art methods. Code is available at https://github.com/LiJunnan1992/DivideMix .", "pdf": "/pdf/b3cf972104156ac812aba28a8c140879fe430148.pdf", "paperhash": "li|dividemix_learning_with_noisy_labels_as_semisupervised_learning", "_bibtex": "@inproceedings{\nLi2020DivideMix:,\ntitle={DivideMix: Learning with Noisy Labels as Semi-supervised Learning},\nauthor={Junnan Li and Richard Socher and Steven C.H. Hoi},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJgExaVtwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/e4a6ce14c28738ac3972e56acaeea772fdcc0d70.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJgExaVtwr", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504210554, "tmdate": 1576860578820, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper335/Authors", "ICLR.cc/2020/Conference/Paper335/Reviewers", "ICLR.cc/2020/Conference/Paper335/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper335/-/Public_Comment"}}}, {"id": "Va_IHM9Xhv", "original": null, "number": 1, "cdate": 1576798693558, "ddate": null, "tcdate": 1576798693558, "tmdate": 1576800941904, "tddate": null, "forum": "HJgExaVtwr", "replyto": "HJgExaVtwr", "invitation": "ICLR.cc/2020/Conference/Paper335/-/Decision", "content": {"decision": "Accept (Poster)", "comment": "This paper proposes an algorithm for noisy labels by adopting an idea in the recent semi-supervised learning algorithm.\n\nAs two problems of training noisy labels and semi-supervised ones are closely related, it is not surprising to expect such results as pointed out by reviewers. However, reported thorough experimental results are strong and I think this paper can be useful for practitioners and following works. \n\nHence, I recommend acceptance.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DivideMix: Learning with Noisy Labels as Semi-supervised Learning", "authors": ["Junnan Li", "Richard Socher", "Steven C.H. Hoi"], "authorids": ["junnan.li@salesforce.com", "rsocher@salesforce.com", "shoi@salesforce.com"], "keywords": ["label noise", "semi-supervised learning"], "TL;DR": "We propose a novel semi-supervised learning approach with SOTA performance on combating learning with noisy labels.", "abstract": "Deep neural networks are known to be annotation-hungry. Numerous efforts have been devoted to reducing the annotation cost when learning with deep networks. Two prominent directions include learning with noisy labels and semi-supervised learning by exploiting unlabeled data. In this work, we propose DivideMix, a novel framework for learning with noisy labels by leveraging semi-supervised learning techniques. In particular, DivideMix models the per-sample loss distribution with a mixture model to dynamically divide the training data into a labeled set with clean samples and an unlabeled set with noisy samples, and trains the model on both the labeled and unlabeled data in a semi-supervised manner. To avoid confirmation bias, we simultaneously train two diverged networks where each network uses the dataset division from the other network. During the semi-supervised training phase, we improve the MixMatch strategy by performing label co-refinement and label co-guessing on labeled and unlabeled samples, respectively. Experiments on multiple benchmark datasets demonstrate substantial improvements over state-of-the-art methods. Code is available at https://github.com/LiJunnan1992/DivideMix .", "pdf": "/pdf/b3cf972104156ac812aba28a8c140879fe430148.pdf", "paperhash": "li|dividemix_learning_with_noisy_labels_as_semisupervised_learning", "_bibtex": "@inproceedings{\nLi2020DivideMix:,\ntitle={DivideMix: Learning with Noisy Labels as Semi-supervised Learning},\nauthor={Junnan Li and Richard Socher and Steven C.H. Hoi},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJgExaVtwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/e4a6ce14c28738ac3972e56acaeea772fdcc0d70.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "HJgExaVtwr", "replyto": "HJgExaVtwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795710940, "tmdate": 1576800260031, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper335/-/Decision"}}}, {"id": "ByxNpGZ2jB", "original": null, "number": 6, "cdate": 1573814971771, "ddate": null, "tcdate": 1573814971771, "tmdate": 1573814971771, "tddate": null, "forum": "HJgExaVtwr", "replyto": "rJe2zKAosB", "invitation": "ICLR.cc/2020/Conference/Paper335/-/Official_Comment", "content": {"title": "Thank you", "comment": "The authors have responded to my questions, and I have no other comments to make."}, "signatures": ["ICLR.cc/2020/Conference/Paper335/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper335/AnonReviewer3", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DivideMix: Learning with Noisy Labels as Semi-supervised Learning", "authors": ["Junnan Li", "Richard Socher", "Steven C.H. Hoi"], "authorids": ["junnan.li@salesforce.com", "rsocher@salesforce.com", "shoi@salesforce.com"], "keywords": ["label noise", "semi-supervised learning"], "TL;DR": "We propose a novel semi-supervised learning approach with SOTA performance on combating learning with noisy labels.", "abstract": "Deep neural networks are known to be annotation-hungry. Numerous efforts have been devoted to reducing the annotation cost when learning with deep networks. Two prominent directions include learning with noisy labels and semi-supervised learning by exploiting unlabeled data. In this work, we propose DivideMix, a novel framework for learning with noisy labels by leveraging semi-supervised learning techniques. In particular, DivideMix models the per-sample loss distribution with a mixture model to dynamically divide the training data into a labeled set with clean samples and an unlabeled set with noisy samples, and trains the model on both the labeled and unlabeled data in a semi-supervised manner. To avoid confirmation bias, we simultaneously train two diverged networks where each network uses the dataset division from the other network. During the semi-supervised training phase, we improve the MixMatch strategy by performing label co-refinement and label co-guessing on labeled and unlabeled samples, respectively. Experiments on multiple benchmark datasets demonstrate substantial improvements over state-of-the-art methods. Code is available at https://github.com/LiJunnan1992/DivideMix .", "pdf": "/pdf/b3cf972104156ac812aba28a8c140879fe430148.pdf", "paperhash": "li|dividemix_learning_with_noisy_labels_as_semisupervised_learning", "_bibtex": "@inproceedings{\nLi2020DivideMix:,\ntitle={DivideMix: Learning with Noisy Labels as Semi-supervised Learning},\nauthor={Junnan Li and Richard Socher and Steven C.H. Hoi},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJgExaVtwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/e4a6ce14c28738ac3972e56acaeea772fdcc0d70.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJgExaVtwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper335/Authors", "ICLR.cc/2020/Conference/Paper335/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper335/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper335/Reviewers", "ICLR.cc/2020/Conference/Paper335/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper335/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper335/Authors|ICLR.cc/2020/Conference/Paper335/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504172954, "tmdate": 1576860545458, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper335/Authors", "ICLR.cc/2020/Conference/Paper335/Reviewers", "ICLR.cc/2020/Conference/Paper335/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper335/-/Official_Comment"}}}, {"id": "ryxKwK0jiH", "original": null, "number": 5, "cdate": 1573804385270, "ddate": null, "tcdate": 1573804385270, "tmdate": 1573804385270, "tddate": null, "forum": "HJgExaVtwr", "replyto": "S1gKsV9EYS", "invitation": "ICLR.cc/2020/Conference/Paper335/-/Official_Comment", "content": {"title": "Response to Reviewer #2", "comment": "We thank Reviewer 2 for the insightful comments. We believe that our work can inspire new research ideas to explore the intersection between the areas of learning with noisy labels and semi-supervised learning."}, "signatures": ["ICLR.cc/2020/Conference/Paper335/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper335/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DivideMix: Learning with Noisy Labels as Semi-supervised Learning", "authors": ["Junnan Li", "Richard Socher", "Steven C.H. Hoi"], "authorids": ["junnan.li@salesforce.com", "rsocher@salesforce.com", "shoi@salesforce.com"], "keywords": ["label noise", "semi-supervised learning"], "TL;DR": "We propose a novel semi-supervised learning approach with SOTA performance on combating learning with noisy labels.", "abstract": "Deep neural networks are known to be annotation-hungry. Numerous efforts have been devoted to reducing the annotation cost when learning with deep networks. Two prominent directions include learning with noisy labels and semi-supervised learning by exploiting unlabeled data. In this work, we propose DivideMix, a novel framework for learning with noisy labels by leveraging semi-supervised learning techniques. In particular, DivideMix models the per-sample loss distribution with a mixture model to dynamically divide the training data into a labeled set with clean samples and an unlabeled set with noisy samples, and trains the model on both the labeled and unlabeled data in a semi-supervised manner. To avoid confirmation bias, we simultaneously train two diverged networks where each network uses the dataset division from the other network. During the semi-supervised training phase, we improve the MixMatch strategy by performing label co-refinement and label co-guessing on labeled and unlabeled samples, respectively. Experiments on multiple benchmark datasets demonstrate substantial improvements over state-of-the-art methods. Code is available at https://github.com/LiJunnan1992/DivideMix .", "pdf": "/pdf/b3cf972104156ac812aba28a8c140879fe430148.pdf", "paperhash": "li|dividemix_learning_with_noisy_labels_as_semisupervised_learning", "_bibtex": "@inproceedings{\nLi2020DivideMix:,\ntitle={DivideMix: Learning with Noisy Labels as Semi-supervised Learning},\nauthor={Junnan Li and Richard Socher and Steven C.H. Hoi},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJgExaVtwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/e4a6ce14c28738ac3972e56acaeea772fdcc0d70.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJgExaVtwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper335/Authors", "ICLR.cc/2020/Conference/Paper335/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper335/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper335/Reviewers", "ICLR.cc/2020/Conference/Paper335/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper335/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper335/Authors|ICLR.cc/2020/Conference/Paper335/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504172954, "tmdate": 1576860545458, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper335/Authors", "ICLR.cc/2020/Conference/Paper335/Reviewers", "ICLR.cc/2020/Conference/Paper335/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper335/-/Official_Comment"}}}, {"id": "HyxSBF0ssB", "original": null, "number": 4, "cdate": 1573804348629, "ddate": null, "tcdate": 1573804348629, "tmdate": 1573804348629, "tddate": null, "forum": "HJgExaVtwr", "replyto": "HkeBKNcaKr", "invitation": "ICLR.cc/2020/Conference/Paper335/-/Official_Comment", "content": {"title": "Response to Reviewer #1", "comment": "We appreciate Reviewer 1 for the very helpful and constructive suggestions.  Following the suggestions, we have improved the paper to (1) emphasize the difference between the two types of loss corrections in Section 2.1, (2) include [1-2] into the related work section, and (3) add a training time analysis in Appendix D. Next we provide the computation time analysis.\n \nIn terms of inference time, our method does not introduce extra computation if a single model (e.g. theta1) is used for test, whereas the inference time is doubled if we use the ensemble of both models.\nIn terms of training time, Appendix D shows a detailed analysis. We first compare the total training time of DivideMix to several SOTA methods, using a single Nvidia V100 GPU. DivideMix (5.2 h) is slower than Co-teaching+ (4.3 h) but faster than P-correction (6.0 h) and Meta-Learning (8.6 h) which involve multiple training iterations. We also break down the computation time per-epoch for each operation in DivideMix, Co-Divide (Alg. 1, line 4-8) takes 17.2 seconds, MixMatch of data (Alg. 1, line 12-24) takes 16.0 seconds, whereas the model\u2019s forward-backward computation (Alg. 1, line 25-27) takes 12.5 seconds."}, "signatures": ["ICLR.cc/2020/Conference/Paper335/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper335/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DivideMix: Learning with Noisy Labels as Semi-supervised Learning", "authors": ["Junnan Li", "Richard Socher", "Steven C.H. Hoi"], "authorids": ["junnan.li@salesforce.com", "rsocher@salesforce.com", "shoi@salesforce.com"], "keywords": ["label noise", "semi-supervised learning"], "TL;DR": "We propose a novel semi-supervised learning approach with SOTA performance on combating learning with noisy labels.", "abstract": "Deep neural networks are known to be annotation-hungry. Numerous efforts have been devoted to reducing the annotation cost when learning with deep networks. Two prominent directions include learning with noisy labels and semi-supervised learning by exploiting unlabeled data. In this work, we propose DivideMix, a novel framework for learning with noisy labels by leveraging semi-supervised learning techniques. In particular, DivideMix models the per-sample loss distribution with a mixture model to dynamically divide the training data into a labeled set with clean samples and an unlabeled set with noisy samples, and trains the model on both the labeled and unlabeled data in a semi-supervised manner. To avoid confirmation bias, we simultaneously train two diverged networks where each network uses the dataset division from the other network. During the semi-supervised training phase, we improve the MixMatch strategy by performing label co-refinement and label co-guessing on labeled and unlabeled samples, respectively. Experiments on multiple benchmark datasets demonstrate substantial improvements over state-of-the-art methods. Code is available at https://github.com/LiJunnan1992/DivideMix .", "pdf": "/pdf/b3cf972104156ac812aba28a8c140879fe430148.pdf", "paperhash": "li|dividemix_learning_with_noisy_labels_as_semisupervised_learning", "_bibtex": "@inproceedings{\nLi2020DivideMix:,\ntitle={DivideMix: Learning with Noisy Labels as Semi-supervised Learning},\nauthor={Junnan Li and Richard Socher and Steven C.H. Hoi},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJgExaVtwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/e4a6ce14c28738ac3972e56acaeea772fdcc0d70.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJgExaVtwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper335/Authors", "ICLR.cc/2020/Conference/Paper335/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper335/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper335/Reviewers", "ICLR.cc/2020/Conference/Paper335/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper335/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper335/Authors|ICLR.cc/2020/Conference/Paper335/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504172954, "tmdate": 1576860545458, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper335/Authors", "ICLR.cc/2020/Conference/Paper335/Reviewers", "ICLR.cc/2020/Conference/Paper335/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper335/-/Official_Comment"}}}, {"id": "rJe2zKAosB", "original": null, "number": 3, "cdate": 1573804308063, "ddate": null, "tcdate": 1573804308063, "tmdate": 1573804308063, "tddate": null, "forum": "HJgExaVtwr", "replyto": "SyxNO3Ne5S", "invitation": "ICLR.cc/2020/Conference/Paper335/-/Official_Comment", "content": {"title": "Response to Reviewer #3", "comment": "We appreciate Reviewer 3 for the recognition of this paper and the valuable comments. Next we response to the questions raised by the reviewer. \n \nQuestion 1: What is the value of \\tau in other experiments?\nResponse: As explained in Appendix B, we set the value of \\tau as 0.5 for other experiments.\n \nQuestion 2: I\u2019m also wondering if \\tau needs some decay during training, since deep NNs are gradually fitting noisy features during training.\nResponse: We find that the proposed DivideMix can prevent the network from fitting to label noise and keep higher loss for noisy samples. This is supported by Figure 3 in Appendix, which shows that the GMM improves in distinguishing clean and noisy samples as training proceeds. Therefore, we can keep a constant \\tau during training.\n \nQuestion 3: I\u2019m a bit concerned about efficiency. So how about the computation time?\nResponse: In terms of inference time, our method does not introduce extra computation if a single model (e.g. theta1) is used for test, whereas the inference time is doubled if we use the ensemble of both models.\nIn terms of training time, we have added a training time analysis in Appendix D. We first compare the total training time of DivideMix to several SOTA methods, using a single Nvidia V100 GPU. DivideMix (5.2 h) is slower than Co-teaching+ (4.3 h) but faster than P-correction (6.0 h) and Meta-Learning (8.6 h) which involve multiple training iterations. We also break down the computation time per-epoch for each operation in DivideMix, Co-Divide (Alg. 1, line 4-8) takes 17.2 seconds, MixMatch of data (Alg. 1, line 12-24) takes 16.0 seconds, whereas the model\u2019s forward-backward computation (Alg. 1, line 25-27) takes 12.5 seconds.\n \nQuestion 4: More details on experimental protocol may be needed: What kind of hyperparameter tuning was done? How many repeated runs? \nResponse: As explained in Section 4.1 and Appendix B, we keep most hyperparameters fixed across experiments and do light tuning on \\lambda_u (weight for unsupervised loss) on a per-experiment basis using a small validation set. We report results on single runs due to time concerns with the large number of experiments that we have conducted. Since all methods use the same training data, we believe that the results are fair for comparison. We thank the reviewer for this suggestion and would perform repeated runs in the future."}, "signatures": ["ICLR.cc/2020/Conference/Paper335/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper335/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DivideMix: Learning with Noisy Labels as Semi-supervised Learning", "authors": ["Junnan Li", "Richard Socher", "Steven C.H. Hoi"], "authorids": ["junnan.li@salesforce.com", "rsocher@salesforce.com", "shoi@salesforce.com"], "keywords": ["label noise", "semi-supervised learning"], "TL;DR": "We propose a novel semi-supervised learning approach with SOTA performance on combating learning with noisy labels.", "abstract": "Deep neural networks are known to be annotation-hungry. Numerous efforts have been devoted to reducing the annotation cost when learning with deep networks. Two prominent directions include learning with noisy labels and semi-supervised learning by exploiting unlabeled data. In this work, we propose DivideMix, a novel framework for learning with noisy labels by leveraging semi-supervised learning techniques. In particular, DivideMix models the per-sample loss distribution with a mixture model to dynamically divide the training data into a labeled set with clean samples and an unlabeled set with noisy samples, and trains the model on both the labeled and unlabeled data in a semi-supervised manner. To avoid confirmation bias, we simultaneously train two diverged networks where each network uses the dataset division from the other network. During the semi-supervised training phase, we improve the MixMatch strategy by performing label co-refinement and label co-guessing on labeled and unlabeled samples, respectively. Experiments on multiple benchmark datasets demonstrate substantial improvements over state-of-the-art methods. Code is available at https://github.com/LiJunnan1992/DivideMix .", "pdf": "/pdf/b3cf972104156ac812aba28a8c140879fe430148.pdf", "paperhash": "li|dividemix_learning_with_noisy_labels_as_semisupervised_learning", "_bibtex": "@inproceedings{\nLi2020DivideMix:,\ntitle={DivideMix: Learning with Noisy Labels as Semi-supervised Learning},\nauthor={Junnan Li and Richard Socher and Steven C.H. Hoi},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJgExaVtwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/e4a6ce14c28738ac3972e56acaeea772fdcc0d70.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJgExaVtwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper335/Authors", "ICLR.cc/2020/Conference/Paper335/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper335/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper335/Reviewers", "ICLR.cc/2020/Conference/Paper335/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper335/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper335/Authors|ICLR.cc/2020/Conference/Paper335/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504172954, "tmdate": 1576860545458, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper335/Authors", "ICLR.cc/2020/Conference/Paper335/Reviewers", "ICLR.cc/2020/Conference/Paper335/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper335/-/Official_Comment"}}}, {"id": "S1gKsV9EYS", "original": null, "number": 1, "cdate": 1571230880917, "ddate": null, "tcdate": 1571230880917, "tmdate": 1572972608224, "tddate": null, "forum": "HJgExaVtwr", "replyto": "HJgExaVtwr", "invitation": "ICLR.cc/2020/Conference/Paper335/-/Official_Review", "content": {"rating": "6: Weak Accept", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper proposed a method named DivideMix for learning with noisy labels, on top of the recent semi-supervised learning method MixMatch from Google. The idea is to model the per-sample loss distribution with a mixture model to dynamically DIVIDE the training data into (a labeled set with clean samples) and (an unlabeled set with noisy samples) and trains the model on both the labeled and unlabeled data in a semi-supervised manner.\n\nThe novelty is borderline. In the area of learning with noisy labels, it is known that SSL can work under this problem setting for several years, for example, the famous method \"virtual adversarial training\" from ICLR 2016 and a recent method \"smooth neighbors on teacher graphs\" from CVPR 2018. As a consequence, it is not surprising that the latest MixMatch can work as well, since MixMatch comes from mixup, virtual adversarial training and entropy minimization. This makes the novelty borderline. However, the significance may still be high according to the reported experimental results, and thus we may accept it to let more deep learning practitioners see the promising results."}, "signatures": ["ICLR.cc/2020/Conference/Paper335/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper335/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DivideMix: Learning with Noisy Labels as Semi-supervised Learning", "authors": ["Junnan Li", "Richard Socher", "Steven C.H. Hoi"], "authorids": ["junnan.li@salesforce.com", "rsocher@salesforce.com", "shoi@salesforce.com"], "keywords": ["label noise", "semi-supervised learning"], "TL;DR": "We propose a novel semi-supervised learning approach with SOTA performance on combating learning with noisy labels.", "abstract": "Deep neural networks are known to be annotation-hungry. Numerous efforts have been devoted to reducing the annotation cost when learning with deep networks. Two prominent directions include learning with noisy labels and semi-supervised learning by exploiting unlabeled data. In this work, we propose DivideMix, a novel framework for learning with noisy labels by leveraging semi-supervised learning techniques. In particular, DivideMix models the per-sample loss distribution with a mixture model to dynamically divide the training data into a labeled set with clean samples and an unlabeled set with noisy samples, and trains the model on both the labeled and unlabeled data in a semi-supervised manner. To avoid confirmation bias, we simultaneously train two diverged networks where each network uses the dataset division from the other network. During the semi-supervised training phase, we improve the MixMatch strategy by performing label co-refinement and label co-guessing on labeled and unlabeled samples, respectively. Experiments on multiple benchmark datasets demonstrate substantial improvements over state-of-the-art methods. Code is available at https://github.com/LiJunnan1992/DivideMix .", "pdf": "/pdf/b3cf972104156ac812aba28a8c140879fe430148.pdf", "paperhash": "li|dividemix_learning_with_noisy_labels_as_semisupervised_learning", "_bibtex": "@inproceedings{\nLi2020DivideMix:,\ntitle={DivideMix: Learning with Noisy Labels as Semi-supervised Learning},\nauthor={Junnan Li and Richard Socher and Steven C.H. Hoi},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJgExaVtwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/e4a6ce14c28738ac3972e56acaeea772fdcc0d70.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJgExaVtwr", "replyto": "HJgExaVtwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper335/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper335/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575474103107, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper335/Reviewers"], "noninvitees": [], "tcdate": 1570237753629, "tmdate": 1575474103122, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper335/-/Official_Review"}}}, {"id": "HkeBKNcaKr", "original": null, "number": 2, "cdate": 1571820668991, "ddate": null, "tcdate": 1571820668991, "tmdate": 1572972608189, "tddate": null, "forum": "HJgExaVtwr", "replyto": "HJgExaVtwr", "invitation": "ICLR.cc/2020/Conference/Paper335/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes an algorithm that learns with noisy labels that achieves state-of-the-art results. Their algorithm tries to exploit the noisy samples by assigning a \u2018correct\u2019 label through MixMatch. It borrows the idea from both semi-supervised learning and learning with label noise. \n\nSuggestions:\n\n1. When the author talked about \u201ccorrect the loss function\u201d, there are in fact two types of corrections: the first type tries to correct the loss function by equally treating all the samples, e.g., classical Huber loss, or F-correction. The second type tries to either re-weight samples or separate clean and noisy samples explicitly, which also results in correcting the loss function. It would be more clear if Section 2.1 can emphasize the difference between the two. \n\n2. Also, there are other papers providing different but insightful ideas to label noise problem. The authors addressed the idea of using co-learning to avoid confirmation bias. However, it should be noted that this is not the only way to avoid confirmation bias, and their are other methods without using two networks [1-2], both providing some theoretical insights to the problem. It would be good to include them in the related work as well. \n\n3. I would like to see a comparison of running time other than the accuracy, understanding the efficiency of each algorithm is important from a practical perspective. \n\n[1] Learning with Bad Training Data via Iterative Trimmed Loss Minimization, Yanyao Shen, Sujay Sanghavi, ICML 2019.\n[2] Robust Learning from Untrusted Sources,Nikola Konstantinov, Christoph H. Lampert, ICML 2019\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper335/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper335/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DivideMix: Learning with Noisy Labels as Semi-supervised Learning", "authors": ["Junnan Li", "Richard Socher", "Steven C.H. Hoi"], "authorids": ["junnan.li@salesforce.com", "rsocher@salesforce.com", "shoi@salesforce.com"], "keywords": ["label noise", "semi-supervised learning"], "TL;DR": "We propose a novel semi-supervised learning approach with SOTA performance on combating learning with noisy labels.", "abstract": "Deep neural networks are known to be annotation-hungry. Numerous efforts have been devoted to reducing the annotation cost when learning with deep networks. Two prominent directions include learning with noisy labels and semi-supervised learning by exploiting unlabeled data. In this work, we propose DivideMix, a novel framework for learning with noisy labels by leveraging semi-supervised learning techniques. In particular, DivideMix models the per-sample loss distribution with a mixture model to dynamically divide the training data into a labeled set with clean samples and an unlabeled set with noisy samples, and trains the model on both the labeled and unlabeled data in a semi-supervised manner. To avoid confirmation bias, we simultaneously train two diverged networks where each network uses the dataset division from the other network. During the semi-supervised training phase, we improve the MixMatch strategy by performing label co-refinement and label co-guessing on labeled and unlabeled samples, respectively. Experiments on multiple benchmark datasets demonstrate substantial improvements over state-of-the-art methods. Code is available at https://github.com/LiJunnan1992/DivideMix .", "pdf": "/pdf/b3cf972104156ac812aba28a8c140879fe430148.pdf", "paperhash": "li|dividemix_learning_with_noisy_labels_as_semisupervised_learning", "_bibtex": "@inproceedings{\nLi2020DivideMix:,\ntitle={DivideMix: Learning with Noisy Labels as Semi-supervised Learning},\nauthor={Junnan Li and Richard Socher and Steven C.H. Hoi},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJgExaVtwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/e4a6ce14c28738ac3972e56acaeea772fdcc0d70.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJgExaVtwr", "replyto": "HJgExaVtwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper335/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper335/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575474103107, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper335/Reviewers"], "noninvitees": [], "tcdate": 1570237753629, "tmdate": 1575474103122, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper335/-/Official_Review"}}}, {"id": "SyxNO3Ne5S", "original": null, "number": 3, "cdate": 1571994732227, "ddate": null, "tcdate": 1571994732227, "tmdate": 1572972608146, "tddate": null, "forum": "HJgExaVtwr", "replyto": "HJgExaVtwr", "invitation": "ICLR.cc/2020/Conference/Paper335/-/Official_Review", "content": {"rating": "6: Weak Accept", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper proposes a DivideMix framework for learning with noisy labels, where they first Co-Divide the training data into a labeled clean set and an unlabeled noisy set by modeling the per-sample loss distribution with GMM and using the small loss trick, then they exploit MixMatch to train the model on those labeled and unlabeled data in a semi-supervised manner. Experiments and comparisons with SOTA are provided, together with an ablation study.\n\nPros:\n-The paper bridges the area of learning with noisy labels with semi-supervised learning and proposes an interesting method to learn with noisy labels in a semi-supervised manner. The treatment proceeds from analyzing good unsupervised loss functions, improving the MixMatch and implementing thorough experiments.\n\n-The paper is clear and flows smoothly. The treatment is thorough, proceeding from designing algorithms, implementing experiments and an ablation study.\n\n-The impact of the method is a clear asset. Most existing label noise works focus on certain noise models, but this paper is more general and proposes an interesting method to learn with noisy labels in a semi-supervised manner, and the experimental results are promising.\n\n-The effort made on designing the confidence penalty for asymmetric noise is interesting.\n\nRemarks:\n-Sec 3.1: it seems that \\tau is an important hyperparameter for dividing the noisy data into labeled and unlabeled sets, but in Sec 4 I only see that \\tao is set to be 0.5 or 0.6 for CIFAR, what about other experiments? I\u2019m also wondering if \\tao needs some decay during training? For example, it may be 0.5 at early training epochs, but may be smaller at last, since deep NNs are gradually fitting noisy features during training.\n\n-Algorithm: the proposed method needs to train two networks simultaneously. During each epoch, it firstly divides the noisy data by modeling the per-sample loss distribution with GMM, and then do MixMatch with label co-refinement and co-guessing. I\u2019m a bit concerned about efficiency. So how about the computation time?\n\n-Experiments: more details on experimental protocol may be needed: what kind of hyperparameter tuning was done? How many repeated runs? It would be helpful to report the means and standard deviations based on repeated samplings.\n\nOverall take: this paper proposes a thorough treatment of learning with noisy labels in a semi-supervised manner, designing the algorithm and testing it empirically, which is an interesting and important contribution. My only concern is about the novelty since the small loss trick in label noise and the MixMatch approach in SSL are already explored by many recent studies, but to the best of my knowledge, this paper is the first to unify them to solve label noise problems."}, "signatures": ["ICLR.cc/2020/Conference/Paper335/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper335/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DivideMix: Learning with Noisy Labels as Semi-supervised Learning", "authors": ["Junnan Li", "Richard Socher", "Steven C.H. Hoi"], "authorids": ["junnan.li@salesforce.com", "rsocher@salesforce.com", "shoi@salesforce.com"], "keywords": ["label noise", "semi-supervised learning"], "TL;DR": "We propose a novel semi-supervised learning approach with SOTA performance on combating learning with noisy labels.", "abstract": "Deep neural networks are known to be annotation-hungry. Numerous efforts have been devoted to reducing the annotation cost when learning with deep networks. Two prominent directions include learning with noisy labels and semi-supervised learning by exploiting unlabeled data. In this work, we propose DivideMix, a novel framework for learning with noisy labels by leveraging semi-supervised learning techniques. In particular, DivideMix models the per-sample loss distribution with a mixture model to dynamically divide the training data into a labeled set with clean samples and an unlabeled set with noisy samples, and trains the model on both the labeled and unlabeled data in a semi-supervised manner. To avoid confirmation bias, we simultaneously train two diverged networks where each network uses the dataset division from the other network. During the semi-supervised training phase, we improve the MixMatch strategy by performing label co-refinement and label co-guessing on labeled and unlabeled samples, respectively. Experiments on multiple benchmark datasets demonstrate substantial improvements over state-of-the-art methods. Code is available at https://github.com/LiJunnan1992/DivideMix .", "pdf": "/pdf/b3cf972104156ac812aba28a8c140879fe430148.pdf", "paperhash": "li|dividemix_learning_with_noisy_labels_as_semisupervised_learning", "_bibtex": "@inproceedings{\nLi2020DivideMix:,\ntitle={DivideMix: Learning with Noisy Labels as Semi-supervised Learning},\nauthor={Junnan Li and Richard Socher and Steven C.H. Hoi},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJgExaVtwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/e4a6ce14c28738ac3972e56acaeea772fdcc0d70.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJgExaVtwr", "replyto": "HJgExaVtwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper335/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper335/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575474103107, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper335/Reviewers"], "noninvitees": [], "tcdate": 1570237753629, "tmdate": 1575474103122, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper335/-/Official_Review"}}}, {"id": "B1l5KLT-uS", "original": null, "number": 1, "cdate": 1569998465608, "ddate": null, "tcdate": 1569998465608, "tmdate": 1569998465608, "tddate": null, "forum": "HJgExaVtwr", "replyto": "SJgK7RakdS", "invitation": "ICLR.cc/2020/Conference/Paper335/-/Official_Comment", "content": {"comment": "Thanks for your comments! \nWe have indeed analyzed the divergence of the two networks using the networks' prediction discrepancy (on the same images) as an indicator. Our result shows that the two networks can stay sufficiently diverged because of multiple factors: different parameter initialization, different training data division, different mini-batch sequence, and different training targets. The iou of the labeled set is one of the causes, rather than a direct indicator of the network divergence. The iou would get smaller as training proceeds because both networks would get better at dividing clean and noisy samples.", "title": "Divergence analysis"}, "signatures": ["ICLR.cc/2020/Conference/Paper335/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper335/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DivideMix: Learning with Noisy Labels as Semi-supervised Learning", "authors": ["Junnan Li", "Richard Socher", "Steven C.H. Hoi"], "authorids": ["junnan.li@salesforce.com", "rsocher@salesforce.com", "shoi@salesforce.com"], "keywords": ["label noise", "semi-supervised learning"], "TL;DR": "We propose a novel semi-supervised learning approach with SOTA performance on combating learning with noisy labels.", "abstract": "Deep neural networks are known to be annotation-hungry. Numerous efforts have been devoted to reducing the annotation cost when learning with deep networks. Two prominent directions include learning with noisy labels and semi-supervised learning by exploiting unlabeled data. In this work, we propose DivideMix, a novel framework for learning with noisy labels by leveraging semi-supervised learning techniques. In particular, DivideMix models the per-sample loss distribution with a mixture model to dynamically divide the training data into a labeled set with clean samples and an unlabeled set with noisy samples, and trains the model on both the labeled and unlabeled data in a semi-supervised manner. To avoid confirmation bias, we simultaneously train two diverged networks where each network uses the dataset division from the other network. During the semi-supervised training phase, we improve the MixMatch strategy by performing label co-refinement and label co-guessing on labeled and unlabeled samples, respectively. Experiments on multiple benchmark datasets demonstrate substantial improvements over state-of-the-art methods. Code is available at https://github.com/LiJunnan1992/DivideMix .", "pdf": "/pdf/b3cf972104156ac812aba28a8c140879fe430148.pdf", "paperhash": "li|dividemix_learning_with_noisy_labels_as_semisupervised_learning", "_bibtex": "@inproceedings{\nLi2020DivideMix:,\ntitle={DivideMix: Learning with Noisy Labels as Semi-supervised Learning},\nauthor={Junnan Li and Richard Socher and Steven C.H. Hoi},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJgExaVtwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/e4a6ce14c28738ac3972e56acaeea772fdcc0d70.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJgExaVtwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper335/Authors", "ICLR.cc/2020/Conference/Paper335/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper335/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper335/Reviewers", "ICLR.cc/2020/Conference/Paper335/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper335/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper335/Authors|ICLR.cc/2020/Conference/Paper335/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504172954, "tmdate": 1576860545458, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper335/Authors", "ICLR.cc/2020/Conference/Paper335/Reviewers", "ICLR.cc/2020/Conference/Paper335/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper335/-/Official_Comment"}}}, {"id": "SJgK7RakdS", "original": null, "number": 1, "cdate": 1569869345197, "ddate": null, "tcdate": 1569869345197, "tmdate": 1569869345197, "tddate": null, "forum": "HJgExaVtwr", "replyto": "HJgExaVtwr", "invitation": "ICLR.cc/2020/Conference/Paper335/-/Public_Comment", "content": {"comment": "Thank you for your well written and thorough study. The joint training was specially interesting to me. Have you looked at the intersection over union of the samples in one set (labeled) during training of the two networks? \n\nThis could be a divergence indicator of the two networks. If iou is high, maybe one can enforce (guarantees even) divergence with aux losses. But if analyzing the iou plot shows they are already sufficiently diverged there would be no point in it. \n\nThanks", "title": "IOU of labeled set during training"}, "signatures": ["~Sara_Sabour1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Sara_Sabour1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DivideMix: Learning with Noisy Labels as Semi-supervised Learning", "authors": ["Junnan Li", "Richard Socher", "Steven C.H. Hoi"], "authorids": ["junnan.li@salesforce.com", "rsocher@salesforce.com", "shoi@salesforce.com"], "keywords": ["label noise", "semi-supervised learning"], "TL;DR": "We propose a novel semi-supervised learning approach with SOTA performance on combating learning with noisy labels.", "abstract": "Deep neural networks are known to be annotation-hungry. Numerous efforts have been devoted to reducing the annotation cost when learning with deep networks. Two prominent directions include learning with noisy labels and semi-supervised learning by exploiting unlabeled data. In this work, we propose DivideMix, a novel framework for learning with noisy labels by leveraging semi-supervised learning techniques. In particular, DivideMix models the per-sample loss distribution with a mixture model to dynamically divide the training data into a labeled set with clean samples and an unlabeled set with noisy samples, and trains the model on both the labeled and unlabeled data in a semi-supervised manner. To avoid confirmation bias, we simultaneously train two diverged networks where each network uses the dataset division from the other network. During the semi-supervised training phase, we improve the MixMatch strategy by performing label co-refinement and label co-guessing on labeled and unlabeled samples, respectively. Experiments on multiple benchmark datasets demonstrate substantial improvements over state-of-the-art methods. Code is available at https://github.com/LiJunnan1992/DivideMix .", "pdf": "/pdf/b3cf972104156ac812aba28a8c140879fe430148.pdf", "paperhash": "li|dividemix_learning_with_noisy_labels_as_semisupervised_learning", "_bibtex": "@inproceedings{\nLi2020DivideMix:,\ntitle={DivideMix: Learning with Noisy Labels as Semi-supervised Learning},\nauthor={Junnan Li and Richard Socher and Steven C.H. Hoi},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJgExaVtwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/e4a6ce14c28738ac3972e56acaeea772fdcc0d70.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJgExaVtwr", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504210554, "tmdate": 1576860578820, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper335/Authors", "ICLR.cc/2020/Conference/Paper335/Reviewers", "ICLR.cc/2020/Conference/Paper335/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper335/-/Public_Comment"}}}], "count": 15}