{"notes": [{"id": "rkgyS0VFvr", "original": "HkxCSOIdvS", "number": 1097, "cdate": 1569439287328, "ddate": null, "tcdate": 1569439287328, "tmdate": 1588473399683, "tddate": null, "forum": "rkgyS0VFvr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["chulinxie@zju.edu.cn", "nick_cooper@sjtu.edu.cn", "pin-yu.chen@ibm.com", "lbo@illinois.edu"], "title": "DBA: Distributed Backdoor Attacks against Federated Learning", "authors": ["Chulin Xie", "Keli Huang", "Pin-Yu Chen", "Bo Li"], "pdf": "/pdf/61dc789b9f12be96506a23ddb7670ac132a51d6d.pdf", "TL;DR": "We proposed a novel distributed backdoor attack on federated learning and show that it is not only more effective compared with standard centralized attacks, but also harder to be defended by existing robust FL methods", "abstract": "Backdoor attacks aim to manipulate a subset of training data by injecting adversarial triggers such that machine learning models trained on the tampered dataset will make arbitrarily (targeted) incorrect prediction on the testset with the same trigger embedded. While federated learning (FL) is capable of aggregating information provided by different parties for training a better model, its distributed learning methodology and inherently heterogeneous data distribution across parties may bring new vulnerabilities. In addition to recent centralized backdoor attacks on FL where each party embeds the same global trigger during training, we propose the distributed backdoor attack (DBA) --- a novel threat assessment framework developed by fully exploiting the distributed nature of FL. DBA decomposes a global trigger pattern into separate local patterns and embed them into the training set of different adversarial parties respectively. Compared to standard centralized backdoors, we show that DBA is substantially more persistent and stealthy against FL on diverse datasets such as finance and image data. We conduct extensive experiments to show that the attack success rate of DBA is significantly higher than centralized backdoors under different settings. Moreover, we find that distributed attacks are indeed more insidious, as DBA can evade two state-of-the-art robust FL algorithms against centralized backdoors. We also provide explanations for the effectiveness of DBA via feature visual interpretation and feature importance ranking.\nTo further explore the properties of DBA, we test the attack performance by varying different trigger factors, including local trigger variations (size, gap, and location), scaling factor in FL, data distribution, and poison ratio and interval. Our proposed DBA and thorough evaluation results shed lights on characterizing the robustness of FL.", "keywords": ["distributed backdoor attack", "federated learning"], "paperhash": "xie|dba_distributed_backdoor_attacks_against_federated_learning", "_bibtex": "@inproceedings{\nXie2020DBA:,\ntitle={DBA: Distributed Backdoor Attacks against Federated Learning},\nauthor={Chulin Xie and Keli Huang and Pin-Yu Chen and Bo Li},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rkgyS0VFvr}\n}", "original_pdf": "/attachment/dc0f552db4cfc9d3d21e1f5c47f4c99b9521dca0.pdf"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "r1gNsdHnjH", "original": null, "number": 3, "cdate": 1573832859761, "ddate": null, "tcdate": 1573832859761, "tmdate": 1580708518126, "tddate": null, "forum": "rkgyS0VFvr", "replyto": "rkeucgq15r", "invitation": "ICLR.cc/2020/Conference/Paper1097/-/Official_Comment", "content": {"title": "Reply to Reviewer 1", "comment": "Thank you for your appreciation of our work!"}, "signatures": ["ICLR.cc/2020/Conference/Paper1097/Authors"], "readers": ["ICLR.cc/2020/Conference/Paper1097/Authors", "ICLR.cc/2020/Conference/Paper1097/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1097/Reviewers", "ICLR.cc/2020/Conference/Paper1097/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs", "everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1097/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["chulinxie@zju.edu.cn", "nick_cooper@sjtu.edu.cn", "pin-yu.chen@ibm.com", "lbo@illinois.edu"], "title": "DBA: Distributed Backdoor Attacks against Federated Learning", "authors": ["Chulin Xie", "Keli Huang", "Pin-Yu Chen", "Bo Li"], "pdf": "/pdf/61dc789b9f12be96506a23ddb7670ac132a51d6d.pdf", "TL;DR": "We proposed a novel distributed backdoor attack on federated learning and show that it is not only more effective compared with standard centralized attacks, but also harder to be defended by existing robust FL methods", "abstract": "Backdoor attacks aim to manipulate a subset of training data by injecting adversarial triggers such that machine learning models trained on the tampered dataset will make arbitrarily (targeted) incorrect prediction on the testset with the same trigger embedded. While federated learning (FL) is capable of aggregating information provided by different parties for training a better model, its distributed learning methodology and inherently heterogeneous data distribution across parties may bring new vulnerabilities. In addition to recent centralized backdoor attacks on FL where each party embeds the same global trigger during training, we propose the distributed backdoor attack (DBA) --- a novel threat assessment framework developed by fully exploiting the distributed nature of FL. DBA decomposes a global trigger pattern into separate local patterns and embed them into the training set of different adversarial parties respectively. Compared to standard centralized backdoors, we show that DBA is substantially more persistent and stealthy against FL on diverse datasets such as finance and image data. We conduct extensive experiments to show that the attack success rate of DBA is significantly higher than centralized backdoors under different settings. Moreover, we find that distributed attacks are indeed more insidious, as DBA can evade two state-of-the-art robust FL algorithms against centralized backdoors. We also provide explanations for the effectiveness of DBA via feature visual interpretation and feature importance ranking.\nTo further explore the properties of DBA, we test the attack performance by varying different trigger factors, including local trigger variations (size, gap, and location), scaling factor in FL, data distribution, and poison ratio and interval. Our proposed DBA and thorough evaluation results shed lights on characterizing the robustness of FL.", "keywords": ["distributed backdoor attack", "federated learning"], "paperhash": "xie|dba_distributed_backdoor_attacks_against_federated_learning", "_bibtex": "@inproceedings{\nXie2020DBA:,\ntitle={DBA: Distributed Backdoor Attacks against Federated Learning},\nauthor={Chulin Xie and Keli Huang and Pin-Yu Chen and Bo Li},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rkgyS0VFvr}\n}", "original_pdf": "/attachment/dc0f552db4cfc9d3d21e1f5c47f4c99b9521dca0.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rkgyS0VFvr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1097/Authors", "ICLR.cc/2020/Conference/Paper1097/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1097/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1097/Reviewers", "ICLR.cc/2020/Conference/Paper1097/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1097/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1097/Authors|ICLR.cc/2020/Conference/Paper1097/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504161302, "tmdate": 1576860530196, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1097/Authors", "ICLR.cc/2020/Conference/Paper1097/Reviewers", "ICLR.cc/2020/Conference/Paper1097/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1097/-/Official_Comment"}}}, {"id": "rkgZSdB3oH", "original": null, "number": 2, "cdate": 1573832760895, "ddate": null, "tcdate": 1573832760895, "tmdate": 1580708500728, "tddate": null, "forum": "rkgyS0VFvr", "replyto": "SJe4yM7lcr", "invitation": "ICLR.cc/2020/Conference/Paper1097/-/Official_Comment", "content": {"title": "Reply to Reviewer 3", "comment": "Thanks so much for your valuable review comments! \n\nFollowing your suggestion, we evaluated the Byzantine settings Multi-Krum (Blanchard et al 2017) and Bulyan (El Mhamdi et al 2018 ICML). For both DBA and centralized attack we use the aggregation rule that can tolerate f Byzantine workers among n workers (Blanchard et al 2017). For centralized attack there is 1 attacker and n-1 non-Byzantine workers. For DBA there are f distributed attackers and n-f non-Byzantine workers.  The total number of poisoned pixel amounts are kept the same.\n\n1. Multi-Krum\n- To meet the assumption that 2f + 2 < n, we set  (n=10, f=3) for loan and (n=12, f=4) for image datasets. The Multi-Krum parameter m is set to m=n-f. For Tiny-imagenet we decrease the poison ratio to 5/64 for both attacks. Other parameters are the same as described in the paper.\n- For CIFAR and Tiny-imagenet, we find that DBA is more effective.  \n- For LOAN and MNIST, both attacks don\u2019t behave well. We believe the reason can be explained by the fact that Loan and MNIST are simpler tasks and benign clients quickly agree on the correct gradient direction, so malicious updates are more difficult to succeed. \n\n2. Bulyan\n- We use Bulyan based on the Byzantine\u2013resilient aggregation rule Krum. To meet the assumption that 4f + 3 <= n, we set  (n=15, f=3) for loan and (n=20, f=4) for image datasets. \n- For CIFAR, DBA is more effective.  \n- For other datasets, both attacks fail. However, we note that our distributed and centralized backdoor attacks are not optimized for Byzantine setting. We believe it\u2019s worthwhile to explore the distributed version of other new attack algorithms, e.g. A Little Is Enough (Baruch et al 2019) that manipulates its update to mitigate Krum and Bulyan defenses. \n\nIn summary, Multi-Krum and Bulyan have stricter assumptions on the proportion of attackers than RFA and FoolsGold. In addition, while RFA and FoolsGold still assign potential outliers with extreme low weights, Krum (Multi-Krum, Krum-based Bulyan) directly removes them, making it impossible to inject backdoors if the malicious updates are obviously far from the benign updates. The centralized attack for four datasets totally fails under Multi-Krum and Bulyan while DBA can still succeed in some cases. We have included these results in Appendix A.6 of the revised version.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1097/Authors"], "readers": ["ICLR.cc/2020/Conference/Paper1097/Authors", "ICLR.cc/2020/Conference/Paper1097/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1097/Reviewers", "ICLR.cc/2020/Conference/Paper1097/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs", "everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1097/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["chulinxie@zju.edu.cn", "nick_cooper@sjtu.edu.cn", "pin-yu.chen@ibm.com", "lbo@illinois.edu"], "title": "DBA: Distributed Backdoor Attacks against Federated Learning", "authors": ["Chulin Xie", "Keli Huang", "Pin-Yu Chen", "Bo Li"], "pdf": "/pdf/61dc789b9f12be96506a23ddb7670ac132a51d6d.pdf", "TL;DR": "We proposed a novel distributed backdoor attack on federated learning and show that it is not only more effective compared with standard centralized attacks, but also harder to be defended by existing robust FL methods", "abstract": "Backdoor attacks aim to manipulate a subset of training data by injecting adversarial triggers such that machine learning models trained on the tampered dataset will make arbitrarily (targeted) incorrect prediction on the testset with the same trigger embedded. While federated learning (FL) is capable of aggregating information provided by different parties for training a better model, its distributed learning methodology and inherently heterogeneous data distribution across parties may bring new vulnerabilities. In addition to recent centralized backdoor attacks on FL where each party embeds the same global trigger during training, we propose the distributed backdoor attack (DBA) --- a novel threat assessment framework developed by fully exploiting the distributed nature of FL. DBA decomposes a global trigger pattern into separate local patterns and embed them into the training set of different adversarial parties respectively. Compared to standard centralized backdoors, we show that DBA is substantially more persistent and stealthy against FL on diverse datasets such as finance and image data. We conduct extensive experiments to show that the attack success rate of DBA is significantly higher than centralized backdoors under different settings. Moreover, we find that distributed attacks are indeed more insidious, as DBA can evade two state-of-the-art robust FL algorithms against centralized backdoors. We also provide explanations for the effectiveness of DBA via feature visual interpretation and feature importance ranking.\nTo further explore the properties of DBA, we test the attack performance by varying different trigger factors, including local trigger variations (size, gap, and location), scaling factor in FL, data distribution, and poison ratio and interval. Our proposed DBA and thorough evaluation results shed lights on characterizing the robustness of FL.", "keywords": ["distributed backdoor attack", "federated learning"], "paperhash": "xie|dba_distributed_backdoor_attacks_against_federated_learning", "_bibtex": "@inproceedings{\nXie2020DBA:,\ntitle={DBA: Distributed Backdoor Attacks against Federated Learning},\nauthor={Chulin Xie and Keli Huang and Pin-Yu Chen and Bo Li},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rkgyS0VFvr}\n}", "original_pdf": "/attachment/dc0f552db4cfc9d3d21e1f5c47f4c99b9521dca0.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rkgyS0VFvr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1097/Authors", "ICLR.cc/2020/Conference/Paper1097/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1097/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1097/Reviewers", "ICLR.cc/2020/Conference/Paper1097/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1097/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1097/Authors|ICLR.cc/2020/Conference/Paper1097/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504161302, "tmdate": 1576860530196, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1097/Authors", "ICLR.cc/2020/Conference/Paper1097/Reviewers", "ICLR.cc/2020/Conference/Paper1097/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1097/-/Official_Comment"}}}, {"id": "r1gTFFSnor", "original": null, "number": 4, "cdate": 1573833093355, "ddate": null, "tcdate": 1573833093355, "tmdate": 1580708488563, "tddate": null, "forum": "rkgyS0VFvr", "replyto": "BJl5fZS6YS", "invitation": "ICLR.cc/2020/Conference/Paper1097/-/Official_Comment", "content": {"title": "Reply to Reviewer 2", "comment": "We thank the reviewer for the valuable review comments and suggestions! Please find our point-by-point response as follows.\n\nQ1: Is there only 1 adversarial party?\nA1: There is only one adversarial party in centralized attack. But we make sure that the total injected triggers (e.g., modified pixels) of DBA attackers is close to and even less than that of the centralized attacker.  We stressed this setup in Section 3.2. That is, the ratio of the global trigger of DBA pixels to the centralized is 0.992 for LOAN, 0.964 for MNIST, 0.990 for CIFAR and 0.991 for Tiny-imagenet.\n\nQ2: What\u2019s the result for centralized attacks with the same number of scaling times as DBA, but each update includes 1/4 number of poisoning samples?\nA2: Following your suggestion, we conducted two sets of new experiments.\n1. Change the poison ratio into 1/4: We decrease the fraction of backdoored samples added per training batch into 1/4.\n2. Change the data size into 1/4: We divide the local dataset into 4 parts and use 1/4 dataset for each update and keep the poison ratio unchanged.\nWe have included the results and discussion in Appendix A.4 of the revised version.\n\nQ3: If the decomposition is also useful for trigger patterns that are not necessarily regular shapes?\nA3: It\u2019s also useful for irregular shape triggers. \n1. We study the irregular pixel logo \u2018ICLR\u2019 for three image datasets. Specifically, we use \u2018ICLR\u2019 as the global trigger pattern and decompose it into \u2018I\u2019, \u2018C\u2019, \u2018L\u2019, \u2018R\u2019 for local triggers. \n2. We also use the physical trigger glasses (Chen et al.,2017) on Tiny-imagenet and decomposed the pattern into four parts. \nThe results are in Appendix A.3 of our revised version. DBA is also more effective and this conclusion is consistent in different colors of glasses.\n\nQ4: Can the authors show concrete examples on how the attacks are generated? The details are especially unclear on LOAN.\nA4: We note that we have mentioned our attack formulation and algorithm in Section 2.2;\nWe have also provided more details about LOAN dataset and how we attack in Appendix A.1. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1097/Authors"], "readers": ["ICLR.cc/2020/Conference/Paper1097/Authors", "ICLR.cc/2020/Conference/Paper1097/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1097/Reviewers", "ICLR.cc/2020/Conference/Paper1097/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs", "everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1097/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["chulinxie@zju.edu.cn", "nick_cooper@sjtu.edu.cn", "pin-yu.chen@ibm.com", "lbo@illinois.edu"], "title": "DBA: Distributed Backdoor Attacks against Federated Learning", "authors": ["Chulin Xie", "Keli Huang", "Pin-Yu Chen", "Bo Li"], "pdf": "/pdf/61dc789b9f12be96506a23ddb7670ac132a51d6d.pdf", "TL;DR": "We proposed a novel distributed backdoor attack on federated learning and show that it is not only more effective compared with standard centralized attacks, but also harder to be defended by existing robust FL methods", "abstract": "Backdoor attacks aim to manipulate a subset of training data by injecting adversarial triggers such that machine learning models trained on the tampered dataset will make arbitrarily (targeted) incorrect prediction on the testset with the same trigger embedded. While federated learning (FL) is capable of aggregating information provided by different parties for training a better model, its distributed learning methodology and inherently heterogeneous data distribution across parties may bring new vulnerabilities. In addition to recent centralized backdoor attacks on FL where each party embeds the same global trigger during training, we propose the distributed backdoor attack (DBA) --- a novel threat assessment framework developed by fully exploiting the distributed nature of FL. DBA decomposes a global trigger pattern into separate local patterns and embed them into the training set of different adversarial parties respectively. Compared to standard centralized backdoors, we show that DBA is substantially more persistent and stealthy against FL on diverse datasets such as finance and image data. We conduct extensive experiments to show that the attack success rate of DBA is significantly higher than centralized backdoors under different settings. Moreover, we find that distributed attacks are indeed more insidious, as DBA can evade two state-of-the-art robust FL algorithms against centralized backdoors. We also provide explanations for the effectiveness of DBA via feature visual interpretation and feature importance ranking.\nTo further explore the properties of DBA, we test the attack performance by varying different trigger factors, including local trigger variations (size, gap, and location), scaling factor in FL, data distribution, and poison ratio and interval. Our proposed DBA and thorough evaluation results shed lights on characterizing the robustness of FL.", "keywords": ["distributed backdoor attack", "federated learning"], "paperhash": "xie|dba_distributed_backdoor_attacks_against_federated_learning", "_bibtex": "@inproceedings{\nXie2020DBA:,\ntitle={DBA: Distributed Backdoor Attacks against Federated Learning},\nauthor={Chulin Xie and Keli Huang and Pin-Yu Chen and Bo Li},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rkgyS0VFvr}\n}", "original_pdf": "/attachment/dc0f552db4cfc9d3d21e1f5c47f4c99b9521dca0.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rkgyS0VFvr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1097/Authors", "ICLR.cc/2020/Conference/Paper1097/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1097/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1097/Reviewers", "ICLR.cc/2020/Conference/Paper1097/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1097/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1097/Authors|ICLR.cc/2020/Conference/Paper1097/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504161302, "tmdate": 1576860530196, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1097/Authors", "ICLR.cc/2020/Conference/Paper1097/Reviewers", "ICLR.cc/2020/Conference/Paper1097/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1097/-/Official_Comment"}}}, {"id": "9bFX0tdWI5", "original": null, "number": 1, "cdate": 1576798714407, "ddate": null, "tcdate": 1576798714407, "tmdate": 1576800922090, "tddate": null, "forum": "rkgyS0VFvr", "replyto": "rkgyS0VFvr", "invitation": "ICLR.cc/2020/Conference/Paper1097/-/Decision", "content": {"decision": "Accept (Poster)", "comment": "Thanks for the discussion, all. This paper proposes an attack strategy against federated learning. Reviewers put this in the top tier, and the authors responded appropriately to their criticisms. ", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["chulinxie@zju.edu.cn", "nick_cooper@sjtu.edu.cn", "pin-yu.chen@ibm.com", "lbo@illinois.edu"], "title": "DBA: Distributed Backdoor Attacks against Federated Learning", "authors": ["Chulin Xie", "Keli Huang", "Pin-Yu Chen", "Bo Li"], "pdf": "/pdf/61dc789b9f12be96506a23ddb7670ac132a51d6d.pdf", "TL;DR": "We proposed a novel distributed backdoor attack on federated learning and show that it is not only more effective compared with standard centralized attacks, but also harder to be defended by existing robust FL methods", "abstract": "Backdoor attacks aim to manipulate a subset of training data by injecting adversarial triggers such that machine learning models trained on the tampered dataset will make arbitrarily (targeted) incorrect prediction on the testset with the same trigger embedded. While federated learning (FL) is capable of aggregating information provided by different parties for training a better model, its distributed learning methodology and inherently heterogeneous data distribution across parties may bring new vulnerabilities. In addition to recent centralized backdoor attacks on FL where each party embeds the same global trigger during training, we propose the distributed backdoor attack (DBA) --- a novel threat assessment framework developed by fully exploiting the distributed nature of FL. DBA decomposes a global trigger pattern into separate local patterns and embed them into the training set of different adversarial parties respectively. Compared to standard centralized backdoors, we show that DBA is substantially more persistent and stealthy against FL on diverse datasets such as finance and image data. We conduct extensive experiments to show that the attack success rate of DBA is significantly higher than centralized backdoors under different settings. Moreover, we find that distributed attacks are indeed more insidious, as DBA can evade two state-of-the-art robust FL algorithms against centralized backdoors. We also provide explanations for the effectiveness of DBA via feature visual interpretation and feature importance ranking.\nTo further explore the properties of DBA, we test the attack performance by varying different trigger factors, including local trigger variations (size, gap, and location), scaling factor in FL, data distribution, and poison ratio and interval. Our proposed DBA and thorough evaluation results shed lights on characterizing the robustness of FL.", "keywords": ["distributed backdoor attack", "federated learning"], "paperhash": "xie|dba_distributed_backdoor_attacks_against_federated_learning", "_bibtex": "@inproceedings{\nXie2020DBA:,\ntitle={DBA: Distributed Backdoor Attacks against Federated Learning},\nauthor={Chulin Xie and Keli Huang and Pin-Yu Chen and Bo Li},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rkgyS0VFvr}\n}", "original_pdf": "/attachment/dc0f552db4cfc9d3d21e1f5c47f4c99b9521dca0.pdf"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "rkgyS0VFvr", "replyto": "rkgyS0VFvr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795707244, "tmdate": 1576800255432, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1097/-/Decision"}}}, {"id": "BJl5fZS6YS", "original": null, "number": 1, "cdate": 1571799314365, "ddate": null, "tcdate": 1571799314365, "tmdate": 1574306917565, "tddate": null, "forum": "rkgyS0VFvr", "replyto": "rkgyS0VFvr", "invitation": "ICLR.cc/2020/Conference/Paper1097/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #2", "review": "This paper studies backdoor attacks under federated learning setting. To inject a certain backdoor pattern, existing work generate poisoning samples by blending the same pattern with different input samples. Even for federated learning where the adversary can control multiple parties, such as [1], all parties still use the same global backdoor pattern to generate poisoning samples locally. On the contrary, in this work, they decompose the global pattern into several small local patterns, and each adversarial party only uses a local pattern to generate poisoning samples. In their evaluation, they show that the backdoor attacks generated in this way are more effective, resilient to benign model parameter updates, and also survive better against existing defense algorithms against attacks in federated learning settings.\n\nI think the topic studied in this paper is very important and meaningful, and I am convinced that by decomposing a global pattern into several smaller local pieces, the model parameter updates computed by each party should be more similar to benign updates and thus can better bypass the defense algorithms. Meanwhile, the evaluation is pretty comprehensive and it is good to see that the conducted backdoor attacks are effective. However, when there is no defense deployed in the training process, it is not intuitive to see why the proposed attack is more effective and persistent than the centralized attack, given that a smaller trigger usually results in a worse attack performance. Thus, I would like to see more possible explanation on it. Specifically, I have the following questions for clarification:\n\n1. For the evaluation of DBA, I assume that there are 4 adversarial parties, controlling each of the 4 local triggers. When using centralized attacks, are there still 4 adversarial parties, although they share the same global trigger, or if there is only 1 adversarial party?\n\n2. To evaluate A-S setting, I understand that it may be tricky to enable a fair comparison between the centralized attack and DBA. However, one explanation of why DBA is more persistent in this case is because the adversarial parameter updates happen 4x times compared to the centralized attack. Therefore, another baseline to check is to conduct centralized attacks with the same number of times as DBA, but each update includes 1/4 number of poisoning samples, so that the total number of poisoning samples included to compute the gradient update still stays the same.\n\n3. Can the authors show if the decomposition is also useful for trigger patterns that are not necessarily regular shapes? For backdoor attacks, a line of work studies physical triggers, e.g., glasses in [2]. It is not natural to decompose such kind of patterns into several smaller pieces, unless the performance is significantly boosted.\n\n4. Can the authors show concrete examples on how the attacks are generated? The details are especially unclear on LOAN. Specifically, which features are perturbed, what are the values assigned as the trigger, and what is the corresponding target label?\n\n[1]  Bagdasaryan et al., How to backdoor federated learning.\n[2] Chen et al., Targeted Backdoor Attacks on Deep Learning Systems Using Data Poisoning.\n\n------------\nPost-rebuttal comments\n\nI appreciate the authors' great effort to address my concerns! I think the evaluation in the current version of the paper is pretty comprehensive and provides a valuable study, and I am happy to raise my score accordingly.\n-------------", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper1097/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1097/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["chulinxie@zju.edu.cn", "nick_cooper@sjtu.edu.cn", "pin-yu.chen@ibm.com", "lbo@illinois.edu"], "title": "DBA: Distributed Backdoor Attacks against Federated Learning", "authors": ["Chulin Xie", "Keli Huang", "Pin-Yu Chen", "Bo Li"], "pdf": "/pdf/61dc789b9f12be96506a23ddb7670ac132a51d6d.pdf", "TL;DR": "We proposed a novel distributed backdoor attack on federated learning and show that it is not only more effective compared with standard centralized attacks, but also harder to be defended by existing robust FL methods", "abstract": "Backdoor attacks aim to manipulate a subset of training data by injecting adversarial triggers such that machine learning models trained on the tampered dataset will make arbitrarily (targeted) incorrect prediction on the testset with the same trigger embedded. While federated learning (FL) is capable of aggregating information provided by different parties for training a better model, its distributed learning methodology and inherently heterogeneous data distribution across parties may bring new vulnerabilities. In addition to recent centralized backdoor attacks on FL where each party embeds the same global trigger during training, we propose the distributed backdoor attack (DBA) --- a novel threat assessment framework developed by fully exploiting the distributed nature of FL. DBA decomposes a global trigger pattern into separate local patterns and embed them into the training set of different adversarial parties respectively. Compared to standard centralized backdoors, we show that DBA is substantially more persistent and stealthy against FL on diverse datasets such as finance and image data. We conduct extensive experiments to show that the attack success rate of DBA is significantly higher than centralized backdoors under different settings. Moreover, we find that distributed attacks are indeed more insidious, as DBA can evade two state-of-the-art robust FL algorithms against centralized backdoors. We also provide explanations for the effectiveness of DBA via feature visual interpretation and feature importance ranking.\nTo further explore the properties of DBA, we test the attack performance by varying different trigger factors, including local trigger variations (size, gap, and location), scaling factor in FL, data distribution, and poison ratio and interval. Our proposed DBA and thorough evaluation results shed lights on characterizing the robustness of FL.", "keywords": ["distributed backdoor attack", "federated learning"], "paperhash": "xie|dba_distributed_backdoor_attacks_against_federated_learning", "_bibtex": "@inproceedings{\nXie2020DBA:,\ntitle={DBA: Distributed Backdoor Attacks against Federated Learning},\nauthor={Chulin Xie and Keli Huang and Pin-Yu Chen and Bo Li},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rkgyS0VFvr}\n}", "original_pdf": "/attachment/dc0f552db4cfc9d3d21e1f5c47f4c99b9521dca0.pdf"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rkgyS0VFvr", "replyto": "rkgyS0VFvr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1097/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1097/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575547107677, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1097/Reviewers"], "noninvitees": [], "tcdate": 1570237742420, "tmdate": 1575547107692, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1097/-/Official_Review"}}}, {"id": "SJe4yM7lcr", "original": null, "number": 3, "cdate": 1571987932104, "ddate": null, "tcdate": 1571987932104, "tmdate": 1573902831595, "tddate": null, "forum": "rkgyS0VFvr", "replyto": "rkgyS0VFvr", "invitation": "ICLR.cc/2020/Conference/Paper1097/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #3", "review": "This paper proposes a distributed backdoor attack strategy, framed differently from the previous two main approches (1) the centralised backdoor approach and (2) the (less discussed in the paper) distributed fault tolerance approach (often named \"Byzantine\").\n\nThe authors show through experiments how their attack is more persistent than centralised backdoor attack.\nThe authors also compare two aggregation rules for federated learning schemes, (Fung et al 2018 & Pillutla et al 2019), suggesting that both rules are bypassed by the proposed distributed backdoor attack.\n\nStrength:\n\nwhat I found most interesting in the paper is Section 3.4, presenting an appreciable attempt to \"interpret\" poisoning. Together with Section 4. \nThis kind of fine-grained analysis of poisoning is highly needed.\n\nWeakness: \n\nin section 3.3, the authors compare against RFA and take what is claimed in Pillulata et al as granted (that RFA detects more nuanced outliers than the wort-case of the Byzantine setting (Blanchard et al 2017) ). In fact, there is more to the Byzantine setting than that, see e.g. Draco (Chen et al 2018 SysML), Bulyan (El Mhamdi et al 2018 ICML) and SignSGD (Bernstein et al 2019 ICLR) which have proposed more sophisticated approches to distributed robustness.\nSince this paper is about distributed robustness and distributed attacks, it would be very informative to the community to illustrate DBA attack on these methods to have a more compelling message.\n\npost rebuttal: thank your for your detailed reply, I acknowledge your new comparisons with the distributed robustness mechanisms of Krum and Bulyan, too bad time was short to compare with the other measures such as Draco and SignSGD.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper1097/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1097/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["chulinxie@zju.edu.cn", "nick_cooper@sjtu.edu.cn", "pin-yu.chen@ibm.com", "lbo@illinois.edu"], "title": "DBA: Distributed Backdoor Attacks against Federated Learning", "authors": ["Chulin Xie", "Keli Huang", "Pin-Yu Chen", "Bo Li"], "pdf": "/pdf/61dc789b9f12be96506a23ddb7670ac132a51d6d.pdf", "TL;DR": "We proposed a novel distributed backdoor attack on federated learning and show that it is not only more effective compared with standard centralized attacks, but also harder to be defended by existing robust FL methods", "abstract": "Backdoor attacks aim to manipulate a subset of training data by injecting adversarial triggers such that machine learning models trained on the tampered dataset will make arbitrarily (targeted) incorrect prediction on the testset with the same trigger embedded. While federated learning (FL) is capable of aggregating information provided by different parties for training a better model, its distributed learning methodology and inherently heterogeneous data distribution across parties may bring new vulnerabilities. In addition to recent centralized backdoor attacks on FL where each party embeds the same global trigger during training, we propose the distributed backdoor attack (DBA) --- a novel threat assessment framework developed by fully exploiting the distributed nature of FL. DBA decomposes a global trigger pattern into separate local patterns and embed them into the training set of different adversarial parties respectively. Compared to standard centralized backdoors, we show that DBA is substantially more persistent and stealthy against FL on diverse datasets such as finance and image data. We conduct extensive experiments to show that the attack success rate of DBA is significantly higher than centralized backdoors under different settings. Moreover, we find that distributed attacks are indeed more insidious, as DBA can evade two state-of-the-art robust FL algorithms against centralized backdoors. We also provide explanations for the effectiveness of DBA via feature visual interpretation and feature importance ranking.\nTo further explore the properties of DBA, we test the attack performance by varying different trigger factors, including local trigger variations (size, gap, and location), scaling factor in FL, data distribution, and poison ratio and interval. Our proposed DBA and thorough evaluation results shed lights on characterizing the robustness of FL.", "keywords": ["distributed backdoor attack", "federated learning"], "paperhash": "xie|dba_distributed_backdoor_attacks_against_federated_learning", "_bibtex": "@inproceedings{\nXie2020DBA:,\ntitle={DBA: Distributed Backdoor Attacks against Federated Learning},\nauthor={Chulin Xie and Keli Huang and Pin-Yu Chen and Bo Li},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rkgyS0VFvr}\n}", "original_pdf": "/attachment/dc0f552db4cfc9d3d21e1f5c47f4c99b9521dca0.pdf"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rkgyS0VFvr", "replyto": "rkgyS0VFvr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1097/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1097/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575547107677, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1097/Reviewers"], "noninvitees": [], "tcdate": 1570237742420, "tmdate": 1575547107692, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1097/-/Official_Review"}}}, {"id": "rkeucgq15r", "original": null, "number": 2, "cdate": 1571950736280, "ddate": null, "tcdate": 1571950736280, "tmdate": 1572972513241, "tddate": null, "forum": "rkgyS0VFvr", "replyto": "rkgyS0VFvr", "invitation": "ICLR.cc/2020/Conference/Paper1097/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The authors introduce the idea of distributed backdoor attacks in the FL framework, in which the dishonest participants in FL add local triggers to their training data to influence the global model to classify triggered images in a desired way. They show empirically that the learned models then are more likely to be successfully forced to misclassified images in which all the local triggers are present at test time, than are models learned using centralized backdoor attacks, where all attackers use the same trigger pattern (one of the same size as the concatenation of the local triggers, to be fair in the comparison). They then demonstrate that because the local triggers cause smaller corruptions in the model coefficients, these distributed attacks survive robust FL training algorithms (namely FoolsGold, and a recent robust regression based method) more often than centralized attacks. Similar experiments are conducted on the Loan text dataset, using appropriate analogs of local triggers, with similar results.\n\nThe paper contributes a novel model for conducting backdoor attacks in the FL setup, and shows that this model is more successful at attacking when training using robust FL algorithms than the standard centralized backdoor attack model. I lean towards accept, as this is a realistic attack model, and as such can further stimulate research into the robustification of FL model aggregation algorithms."}, "signatures": ["ICLR.cc/2020/Conference/Paper1097/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1097/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["chulinxie@zju.edu.cn", "nick_cooper@sjtu.edu.cn", "pin-yu.chen@ibm.com", "lbo@illinois.edu"], "title": "DBA: Distributed Backdoor Attacks against Federated Learning", "authors": ["Chulin Xie", "Keli Huang", "Pin-Yu Chen", "Bo Li"], "pdf": "/pdf/61dc789b9f12be96506a23ddb7670ac132a51d6d.pdf", "TL;DR": "We proposed a novel distributed backdoor attack on federated learning and show that it is not only more effective compared with standard centralized attacks, but also harder to be defended by existing robust FL methods", "abstract": "Backdoor attacks aim to manipulate a subset of training data by injecting adversarial triggers such that machine learning models trained on the tampered dataset will make arbitrarily (targeted) incorrect prediction on the testset with the same trigger embedded. While federated learning (FL) is capable of aggregating information provided by different parties for training a better model, its distributed learning methodology and inherently heterogeneous data distribution across parties may bring new vulnerabilities. In addition to recent centralized backdoor attacks on FL where each party embeds the same global trigger during training, we propose the distributed backdoor attack (DBA) --- a novel threat assessment framework developed by fully exploiting the distributed nature of FL. DBA decomposes a global trigger pattern into separate local patterns and embed them into the training set of different adversarial parties respectively. Compared to standard centralized backdoors, we show that DBA is substantially more persistent and stealthy against FL on diverse datasets such as finance and image data. We conduct extensive experiments to show that the attack success rate of DBA is significantly higher than centralized backdoors under different settings. Moreover, we find that distributed attacks are indeed more insidious, as DBA can evade two state-of-the-art robust FL algorithms against centralized backdoors. We also provide explanations for the effectiveness of DBA via feature visual interpretation and feature importance ranking.\nTo further explore the properties of DBA, we test the attack performance by varying different trigger factors, including local trigger variations (size, gap, and location), scaling factor in FL, data distribution, and poison ratio and interval. Our proposed DBA and thorough evaluation results shed lights on characterizing the robustness of FL.", "keywords": ["distributed backdoor attack", "federated learning"], "paperhash": "xie|dba_distributed_backdoor_attacks_against_federated_learning", "_bibtex": "@inproceedings{\nXie2020DBA:,\ntitle={DBA: Distributed Backdoor Attacks against Federated Learning},\nauthor={Chulin Xie and Keli Huang and Pin-Yu Chen and Bo Li},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rkgyS0VFvr}\n}", "original_pdf": "/attachment/dc0f552db4cfc9d3d21e1f5c47f4c99b9521dca0.pdf"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rkgyS0VFvr", "replyto": "rkgyS0VFvr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1097/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1097/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575547107677, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1097/Reviewers"], "noninvitees": [], "tcdate": 1570237742420, "tmdate": 1575547107692, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1097/-/Official_Review"}}}], "count": 8}