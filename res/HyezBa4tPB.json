{"notes": [{"id": "HyezBa4tPB", "original": "rJeZFqrDvS", "number": 514, "cdate": 1569439033725, "ddate": null, "tcdate": 1569439033725, "tmdate": 1577168215691, "tddate": null, "forum": "HyezBa4tPB", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Dirichlet Wrapper to Quantify Classification Uncertainty in Black-Box Systems", "authors": ["Jos\u00e9 Mena Rold\u00e1n", "Oriol Pujol Vila", "Jordi Vitri\u00e0 Marca"], "authorids": ["jmenarol7@alumnes.ub.edu", "oriol_pujol@ub.edu", "jordi.vitria@ub.edu"], "keywords": ["uncertainty", "black-box classifiers", "rejection", "deep learning", "NLP", "CV"], "TL;DR": "A Dirichlet Deep Learning wrapper to quantify uncertainty in black-box systems applied to a rejection system to improve the quality of predictions", "abstract": "Nowadays, machine learning models are becoming a utility in many sectors. AI companies deliver pre-trained encapsulated models as application programming interfaces (APIs) that developers can combine with third party components, their models, and proprietary data, to create complex data products. This complexity and the lack of control and knowledge of the internals of these external components might cause unavoidable effects, such as lack of transparency, difficulty in auditability, and the emergence of uncontrolled potential risks. These issues are especially critical when practitioners use these components as black-boxes in new datasets. In order to provide actionable insights in this type of scenarios, in this work we propose the use of a wrapping deep learning model to enrich the output of a classification black-box with a measure of uncertainty. Given a black-box classifier, we propose a probabilistic neural network that works in parallel to the black-box and uses a Dirichlet layer as the fusion layer with the black-box.  This Dirichlet layer yields a distribution on top of the multinomial output parameters of the classifier and enables the estimation of aleatoric uncertainty for any data sample.  \nBased on the resulting uncertainty measure, we advocate for a rejection system that selects the more confident predictions, discarding those more uncertain, leading to an improvement in the trustability of the resulting system. We showcase the proposed technique and methodology in two practical scenarios, one for NLP and another for computer vision, where a simulated API based is applied to different domains. Results demonstrate the effectiveness of the uncertainty computed by the wrapper and its high correlation to wrong predictions and misclassifications.", "pdf": "/pdf/a1b53ac7a08397d12ffe2b7555c0efa583f1dc56.pdf", "code": "https://colab.research.google.com/drive/1YsJucBwBSW3Fy6ECAhab21Nzrib3K_t2", "paperhash": "rold\u00e1n|dirichlet_wrapper_to_quantify_classification_uncertainty_in_blackbox_systems", "original_pdf": "/attachment/a1b53ac7a08397d12ffe2b7555c0efa583f1dc56.pdf", "_bibtex": "@misc{\nrold{\\'a}n2020dirichlet,\ntitle={Dirichlet Wrapper to Quantify Classification Uncertainty in Black-Box Systems},\nauthor={Jos{\\'e} Mena Rold{\\'a}n and Oriol Pujol Vila and Jordi Vitri{\\`a} Marca},\nyear={2020},\nurl={https://openreview.net/forum?id=HyezBa4tPB}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 5, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "WknBMp7_lr", "original": null, "number": 1, "cdate": 1576798698612, "ddate": null, "tcdate": 1576798698612, "tmdate": 1576800937186, "tddate": null, "forum": "HyezBa4tPB", "replyto": "HyezBa4tPB", "invitation": "ICLR.cc/2020/Conference/Paper514/-/Decision", "content": {"decision": "Reject", "comment": "This paper proposes adding a Dirichlet distribution as a wrapper on top of a black box classifier in order to better capture uncertainty in the predictions.  This paper received four reviews in total with scores (1,1,1,6).  The reviewer who gave the weak accept found the paper well written, easy to follow and intuitive.  The other reviewers, however, were primarily concerned about the empirical evaluation of the method.  They found the baselines too weak and weren't convinced that the method would work well in practice.  The reviewers also cited a lack of comparison to existing literature for their scores.  One reviewer noted that while the method addresses aleatoric uncertainty, it doesn't provide any mechanism for epistemic uncertainty, which would be necessary for the applications motivating the work.  \n\nThe authors did not provide a response and thus there was no discussion. ", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dirichlet Wrapper to Quantify Classification Uncertainty in Black-Box Systems", "authors": ["Jos\u00e9 Mena Rold\u00e1n", "Oriol Pujol Vila", "Jordi Vitri\u00e0 Marca"], "authorids": ["jmenarol7@alumnes.ub.edu", "oriol_pujol@ub.edu", "jordi.vitria@ub.edu"], "keywords": ["uncertainty", "black-box classifiers", "rejection", "deep learning", "NLP", "CV"], "TL;DR": "A Dirichlet Deep Learning wrapper to quantify uncertainty in black-box systems applied to a rejection system to improve the quality of predictions", "abstract": "Nowadays, machine learning models are becoming a utility in many sectors. AI companies deliver pre-trained encapsulated models as application programming interfaces (APIs) that developers can combine with third party components, their models, and proprietary data, to create complex data products. This complexity and the lack of control and knowledge of the internals of these external components might cause unavoidable effects, such as lack of transparency, difficulty in auditability, and the emergence of uncontrolled potential risks. These issues are especially critical when practitioners use these components as black-boxes in new datasets. In order to provide actionable insights in this type of scenarios, in this work we propose the use of a wrapping deep learning model to enrich the output of a classification black-box with a measure of uncertainty. Given a black-box classifier, we propose a probabilistic neural network that works in parallel to the black-box and uses a Dirichlet layer as the fusion layer with the black-box.  This Dirichlet layer yields a distribution on top of the multinomial output parameters of the classifier and enables the estimation of aleatoric uncertainty for any data sample.  \nBased on the resulting uncertainty measure, we advocate for a rejection system that selects the more confident predictions, discarding those more uncertain, leading to an improvement in the trustability of the resulting system. We showcase the proposed technique and methodology in two practical scenarios, one for NLP and another for computer vision, where a simulated API based is applied to different domains. Results demonstrate the effectiveness of the uncertainty computed by the wrapper and its high correlation to wrong predictions and misclassifications.", "pdf": "/pdf/a1b53ac7a08397d12ffe2b7555c0efa583f1dc56.pdf", "code": "https://colab.research.google.com/drive/1YsJucBwBSW3Fy6ECAhab21Nzrib3K_t2", "paperhash": "rold\u00e1n|dirichlet_wrapper_to_quantify_classification_uncertainty_in_blackbox_systems", "original_pdf": "/attachment/a1b53ac7a08397d12ffe2b7555c0efa583f1dc56.pdf", "_bibtex": "@misc{\nrold{\\'a}n2020dirichlet,\ntitle={Dirichlet Wrapper to Quantify Classification Uncertainty in Black-Box Systems},\nauthor={Jos{\\'e} Mena Rold{\\'a}n and Oriol Pujol Vila and Jordi Vitri{\\`a} Marca},\nyear={2020},\nurl={https://openreview.net/forum?id=HyezBa4tPB}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "HyezBa4tPB", "replyto": "HyezBa4tPB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795729360, "tmdate": 1576800281938, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper514/-/Decision"}}}, {"id": "BJgRMcbStS", "original": null, "number": 1, "cdate": 1571260949873, "ddate": null, "tcdate": 1571260949873, "tmdate": 1572972585911, "tddate": null, "forum": "HyezBa4tPB", "replyto": "HyezBa4tPB", "invitation": "ICLR.cc/2020/Conference/Paper514/-/Official_Review", "content": {"rating": "6: Weak Accept", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "In this paper, the authors propose to use a dirichlet prior over the multinomial distribution outputted by blackbox DL models, to quantify uncertainty in predictions. The main contribution is to learn the parameters of the prior and use it as a wrapper over the black box, to adjudicate whether to retain or reject a particular prediction made by the model. The dirichlet parameters are learnt in conjunction with the model parameters as a fine tuning step in transfer learning tasks. Experiments on NLP and CV domains and multiple datasets demonstrate the efficacy of the method. \n\nThe paper is well written, and easy to understand. The main motivation of the paper seems to be to learn what samples to drop, but the authors do not address what can be done about the dropped samples (i.e what happens if we end up having to drop 80% of the samples?) . Adding the dirichlet prior to quantify uncertainty has also been studied in the context of VAEs before, (and LDA back in the day) so conceptually there's limited novelty. Nonetheless, the method seems to provide impressive results on multiple datasets, and I think this is interesting enough to warrant an accept. \n\nMINOR:\n1. Figure 1: the values of \\beta is the same in all subfigures\n2. Sec 2.2 line 1: SImilarly --> Similar\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper514/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper514/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dirichlet Wrapper to Quantify Classification Uncertainty in Black-Box Systems", "authors": ["Jos\u00e9 Mena Rold\u00e1n", "Oriol Pujol Vila", "Jordi Vitri\u00e0 Marca"], "authorids": ["jmenarol7@alumnes.ub.edu", "oriol_pujol@ub.edu", "jordi.vitria@ub.edu"], "keywords": ["uncertainty", "black-box classifiers", "rejection", "deep learning", "NLP", "CV"], "TL;DR": "A Dirichlet Deep Learning wrapper to quantify uncertainty in black-box systems applied to a rejection system to improve the quality of predictions", "abstract": "Nowadays, machine learning models are becoming a utility in many sectors. AI companies deliver pre-trained encapsulated models as application programming interfaces (APIs) that developers can combine with third party components, their models, and proprietary data, to create complex data products. This complexity and the lack of control and knowledge of the internals of these external components might cause unavoidable effects, such as lack of transparency, difficulty in auditability, and the emergence of uncontrolled potential risks. These issues are especially critical when practitioners use these components as black-boxes in new datasets. In order to provide actionable insights in this type of scenarios, in this work we propose the use of a wrapping deep learning model to enrich the output of a classification black-box with a measure of uncertainty. Given a black-box classifier, we propose a probabilistic neural network that works in parallel to the black-box and uses a Dirichlet layer as the fusion layer with the black-box.  This Dirichlet layer yields a distribution on top of the multinomial output parameters of the classifier and enables the estimation of aleatoric uncertainty for any data sample.  \nBased on the resulting uncertainty measure, we advocate for a rejection system that selects the more confident predictions, discarding those more uncertain, leading to an improvement in the trustability of the resulting system. We showcase the proposed technique and methodology in two practical scenarios, one for NLP and another for computer vision, where a simulated API based is applied to different domains. Results demonstrate the effectiveness of the uncertainty computed by the wrapper and its high correlation to wrong predictions and misclassifications.", "pdf": "/pdf/a1b53ac7a08397d12ffe2b7555c0efa583f1dc56.pdf", "code": "https://colab.research.google.com/drive/1YsJucBwBSW3Fy6ECAhab21Nzrib3K_t2", "paperhash": "rold\u00e1n|dirichlet_wrapper_to_quantify_classification_uncertainty_in_blackbox_systems", "original_pdf": "/attachment/a1b53ac7a08397d12ffe2b7555c0efa583f1dc56.pdf", "_bibtex": "@misc{\nrold{\\'a}n2020dirichlet,\ntitle={Dirichlet Wrapper to Quantify Classification Uncertainty in Black-Box Systems},\nauthor={Jos{\\'e} Mena Rold{\\'a}n and Oriol Pujol Vila and Jordi Vitri{\\`a} Marca},\nyear={2020},\nurl={https://openreview.net/forum?id=HyezBa4tPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HyezBa4tPB", "replyto": "HyezBa4tPB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper514/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper514/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575323049603, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper514/Reviewers"], "noninvitees": [], "tcdate": 1570237751038, "tmdate": 1575323049615, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper514/-/Official_Review"}}}, {"id": "rkekhHcaYB", "original": null, "number": 2, "cdate": 1571820967364, "ddate": null, "tcdate": 1571820967364, "tmdate": 1572972585879, "tddate": null, "forum": "HyezBa4tPB", "replyto": "HyezBa4tPB", "invitation": "ICLR.cc/2020/Conference/Paper514/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper presents a method for learning a \u201cwrapper\u201d model which endows a multiclass predictor with an estimate of model uncertainty. The base model is treated as a black box which emits a categorical distribution, while the wrapper model estimates the parameters of a Dirichlet distribution. The wrapper is trained against silver labels from the base model, and the sampled predictive entropy is used to threshold predictions so as to withhold a decision on uncertain examples.\n\nI believe this paper should not be accepted, as the main contribution is not particularly novel and some experimental details (Section 3) are not well-motivated. \n\nIn particular, the idea of learning a Dirichlet prior is very similar to that proposed in (Malinin and Gales, NeurIPS 2018). The main contribution in this work seems to be the application to an existing black-box model, but this seems like a straightforward application of knowledge distillation, another well-established technique (e.g. Hinton et al. 2015, though oddly, this paper doesn\u2019t mention this).\n\nThe details of the modeling set-up (section 2.2 and 3) are also not entirely clear. It seems from the preceding discussion that the main goal of the wrapper model is to estimate \\beta, but it is not clear how the sampling procedure (2.2) allows for this. It seems that the value of the \u201csampled predictive entropy\u201d (no equation number, but see end of Section 3) is just that of the mean predicted distribution, and it\u2019s not clear why this should be different (assuming the wrapper model converges) than the predictive entropy of the base model.\n\nThis paper could be improved by a more thorough comparison to the literature, and by a clearer motivation for the training procedure used. Additionally, it would help to present a summary table of results, and to frame the metrics in terms of standard classification metrics such as precision and recall where applicable.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper514/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper514/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dirichlet Wrapper to Quantify Classification Uncertainty in Black-Box Systems", "authors": ["Jos\u00e9 Mena Rold\u00e1n", "Oriol Pujol Vila", "Jordi Vitri\u00e0 Marca"], "authorids": ["jmenarol7@alumnes.ub.edu", "oriol_pujol@ub.edu", "jordi.vitria@ub.edu"], "keywords": ["uncertainty", "black-box classifiers", "rejection", "deep learning", "NLP", "CV"], "TL;DR": "A Dirichlet Deep Learning wrapper to quantify uncertainty in black-box systems applied to a rejection system to improve the quality of predictions", "abstract": "Nowadays, machine learning models are becoming a utility in many sectors. AI companies deliver pre-trained encapsulated models as application programming interfaces (APIs) that developers can combine with third party components, their models, and proprietary data, to create complex data products. This complexity and the lack of control and knowledge of the internals of these external components might cause unavoidable effects, such as lack of transparency, difficulty in auditability, and the emergence of uncontrolled potential risks. These issues are especially critical when practitioners use these components as black-boxes in new datasets. In order to provide actionable insights in this type of scenarios, in this work we propose the use of a wrapping deep learning model to enrich the output of a classification black-box with a measure of uncertainty. Given a black-box classifier, we propose a probabilistic neural network that works in parallel to the black-box and uses a Dirichlet layer as the fusion layer with the black-box.  This Dirichlet layer yields a distribution on top of the multinomial output parameters of the classifier and enables the estimation of aleatoric uncertainty for any data sample.  \nBased on the resulting uncertainty measure, we advocate for a rejection system that selects the more confident predictions, discarding those more uncertain, leading to an improvement in the trustability of the resulting system. We showcase the proposed technique and methodology in two practical scenarios, one for NLP and another for computer vision, where a simulated API based is applied to different domains. Results demonstrate the effectiveness of the uncertainty computed by the wrapper and its high correlation to wrong predictions and misclassifications.", "pdf": "/pdf/a1b53ac7a08397d12ffe2b7555c0efa583f1dc56.pdf", "code": "https://colab.research.google.com/drive/1YsJucBwBSW3Fy6ECAhab21Nzrib3K_t2", "paperhash": "rold\u00e1n|dirichlet_wrapper_to_quantify_classification_uncertainty_in_blackbox_systems", "original_pdf": "/attachment/a1b53ac7a08397d12ffe2b7555c0efa583f1dc56.pdf", "_bibtex": "@misc{\nrold{\\'a}n2020dirichlet,\ntitle={Dirichlet Wrapper to Quantify Classification Uncertainty in Black-Box Systems},\nauthor={Jos{\\'e} Mena Rold{\\'a}n and Oriol Pujol Vila and Jordi Vitri{\\`a} Marca},\nyear={2020},\nurl={https://openreview.net/forum?id=HyezBa4tPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HyezBa4tPB", "replyto": "HyezBa4tPB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper514/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper514/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575323049603, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper514/Reviewers"], "noninvitees": [], "tcdate": 1570237751038, "tmdate": 1575323049615, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper514/-/Official_Review"}}}, {"id": "SkgWFpePqH", "original": null, "number": 3, "cdate": 1572437368642, "ddate": null, "tcdate": 1572437368642, "tmdate": 1572972585837, "tddate": null, "forum": "HyezBa4tPB", "replyto": "HyezBa4tPB", "invitation": "ICLR.cc/2020/Conference/Paper514/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This manuscript proposes to train a wrapper to assess the confidence of a black box classifier decision on new samples. The resulting uncertainty prediction is used to reject decision with low confidence, such that the accuracy of the prediction on retained samples remains high. The idea, although a bit incremental, is potentially useful to practitioners, as argued in the introduction, and some empirical result tend to suggest that the method can be useful. However, I am not convinced the approach and its implementation are appropriate. \nMain comments:\nWriting: the model seems strongly based on references such as Kendall & Gal (2017), however lack of introduction of notations and explanations prevent the paper to be self contained. For example: \n(1)\tHere is a list of quantities for which I could not find a definition: y_m, \\omega, W. \n(2)\t\\hat{y} appears with several indexing styles (up to three indices), sometime bold and sometimes not: the meaning of the indexing (which I could not find) is difficult to infer from the text\nObjective:\nI was unable to understand the rational behind the objective to be minimized, introduced in page 4. This is introduced as a cross-entropy loss, however, taking the expression of the Dirichlet distribution, it is not obvious to me how to reach this very simple expression. Is it possible that the authors simply mimic the expression of Kendall and Gal (2017, eq. (5)), that was designed for the Gaussian case, for which the cross-entropy expression is correct?\nAs a consequence, I do not see how this objective is supposed to learn properly the correct beta parameter.\n\nResults:\nFigures 4-8 shows convincing evidence that the procedure improves with respect to a baseline that consist (as far as I understood) in ranking decision based on the entropy of the output probability vector of the classifier. However, given that I am unsure about what the proposed optimization does, it remains unclear to me whether these results reflect a true achievement. For example, one can argue that the chosen baseline is unlikely to be a good estimate of the entropy of the decision due to the fluctuations of the output probability for the unlikely classes. Those low probability values are the classical source of variance and bias in entropy estimates, and a classifier is not designed to get these low probabilities right (as they are low anyways). As a consequence, an already better baseline might be achieved for a given reference class by cropping the probability vector to keep only classes that non-negligible probability over the training set (here they can be many alternative approaches to test). \nA trivial explanation for the proposed approached to work better than the currently chosen baseline is that the noise introduced by the sampling from the Dirichlet distribution leads to larger probabilities for the cases where the probability given by the classifier is small, which would reduce the variance of the entropy estimator based on the formula of page 5 (top). Overall, extensively checking many simpler baselines (which do not require training!) is a first step to see if the achieved result is not easy to get.   \nMinor comments:\n(1)\tPlease check for typos. \n(2)\tPlease avoid remove the multiple parenthesis for successive citations (use a single \u201c\\citep\u201d).\n(3)\tIn Fig. 1, subtitles are inconsistent.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper514/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper514/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dirichlet Wrapper to Quantify Classification Uncertainty in Black-Box Systems", "authors": ["Jos\u00e9 Mena Rold\u00e1n", "Oriol Pujol Vila", "Jordi Vitri\u00e0 Marca"], "authorids": ["jmenarol7@alumnes.ub.edu", "oriol_pujol@ub.edu", "jordi.vitria@ub.edu"], "keywords": ["uncertainty", "black-box classifiers", "rejection", "deep learning", "NLP", "CV"], "TL;DR": "A Dirichlet Deep Learning wrapper to quantify uncertainty in black-box systems applied to a rejection system to improve the quality of predictions", "abstract": "Nowadays, machine learning models are becoming a utility in many sectors. AI companies deliver pre-trained encapsulated models as application programming interfaces (APIs) that developers can combine with third party components, their models, and proprietary data, to create complex data products. This complexity and the lack of control and knowledge of the internals of these external components might cause unavoidable effects, such as lack of transparency, difficulty in auditability, and the emergence of uncontrolled potential risks. These issues are especially critical when practitioners use these components as black-boxes in new datasets. In order to provide actionable insights in this type of scenarios, in this work we propose the use of a wrapping deep learning model to enrich the output of a classification black-box with a measure of uncertainty. Given a black-box classifier, we propose a probabilistic neural network that works in parallel to the black-box and uses a Dirichlet layer as the fusion layer with the black-box.  This Dirichlet layer yields a distribution on top of the multinomial output parameters of the classifier and enables the estimation of aleatoric uncertainty for any data sample.  \nBased on the resulting uncertainty measure, we advocate for a rejection system that selects the more confident predictions, discarding those more uncertain, leading to an improvement in the trustability of the resulting system. We showcase the proposed technique and methodology in two practical scenarios, one for NLP and another for computer vision, where a simulated API based is applied to different domains. Results demonstrate the effectiveness of the uncertainty computed by the wrapper and its high correlation to wrong predictions and misclassifications.", "pdf": "/pdf/a1b53ac7a08397d12ffe2b7555c0efa583f1dc56.pdf", "code": "https://colab.research.google.com/drive/1YsJucBwBSW3Fy6ECAhab21Nzrib3K_t2", "paperhash": "rold\u00e1n|dirichlet_wrapper_to_quantify_classification_uncertainty_in_blackbox_systems", "original_pdf": "/attachment/a1b53ac7a08397d12ffe2b7555c0efa583f1dc56.pdf", "_bibtex": "@misc{\nrold{\\'a}n2020dirichlet,\ntitle={Dirichlet Wrapper to Quantify Classification Uncertainty in Black-Box Systems},\nauthor={Jos{\\'e} Mena Rold{\\'a}n and Oriol Pujol Vila and Jordi Vitri{\\`a} Marca},\nyear={2020},\nurl={https://openreview.net/forum?id=HyezBa4tPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HyezBa4tPB", "replyto": "HyezBa4tPB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper514/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper514/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575323049603, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper514/Reviewers"], "noninvitees": [], "tcdate": 1570237751038, "tmdate": 1575323049615, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper514/-/Official_Review"}}}, {"id": "Byev-9Zwcr", "original": null, "number": 4, "cdate": 1572440575335, "ddate": null, "tcdate": 1572440575335, "tmdate": 1572972585795, "tddate": null, "forum": "HyezBa4tPB", "replyto": "HyezBa4tPB", "invitation": "ICLR.cc/2020/Conference/Paper514/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Motivated by real-world challenges in applying pre-trained models, the authors propose a model for selective prediction (prediction with an option for abstention) that wraps an existing black-box classification model. The resulting model output is a Dirichlet distribution with mean equal to the categorical distribution produced by the black-box and concentration parameter specified by a separate auxiliary model. This additional model is trained to minimize negative log-likelihood of observations under categorical distributions sampled from the aforementioned Dirichlet along with an L1 regularization term on the concentration parameter. To infer the model\u2019s level of uncertainty, the authors propose computing the entropy of the average of sampled categorical distributions.\n\nThe authors evaluated this model on several pairs of sentiment-analysis NLP tasks and one pair of image datasets where a base model is trained on the source dataset, and the auxiliary model is trained on the second, target dataset. Using metrics proposed in (Condessa et al. 2015), the results show positive results at nearly all thresholds compared to a simple entropy baseline.\n\nThe paper addresses an important practical challenge in machine learning, but a confusing problem-framing and lack of robust baselines make me skeptical that it is suitable for publication at ICLR 2020.\n\nA primary concern with this work is its framing of the problem as one of measuring aleatoric (irreducible) uncertainty, but the motivation in transfer learning and interdependence in production ML systems requires models that can characterize epistemic (reducible) uncertainty. A black box model that yields distributions over classes expresses aleatoric uncertainty via that distribution, and uncertainty due to a shifted data distribution is epistemic as additional data from the new domain would reduce it.\n\nMore problematic is the study\u2019s lack of robust baselines. The authors only present the predictive entropy baseline, but numerous methods exist for out-of-distribution detection and selective classification. Though the specific case of selective classification from a blackbox base model is perhaps more niche, other methods from related problems can either be adapted accordingly or used as upper / lower bounds on what we can expect for this problem. \n\nSome simple baselines I would expect to see include:\n* Training a new classification model entirely on the new domain.\n* Training an auxiliary classifier to predict if the base model will be correct (similar to SelectiveNet but without a shared network body). Ideally this model should also have access to the base-model\u2019s prediction as input.\n* \u201cConfidence score\u201d (i.e. the probability assigned to the base-model\u2019s predicted class) -- this is a common baseline for OOD detection.\n\nAdditional questions / concerns:\n* Do I understand correctly that when drawing samples for the entropy calculation, $E[\\hat{y}]$ will equal the black-box model\u2019s prediction in the limit of sample size? If so, this looks like an inefficient and complicated proxy for measuring the concentration of categorical entropies produced by the Dirichlet.\n* Since the paper is focused on scenarios where one is taking advantage of a pre-trained model, one might wonder if the wrapping scheme is indeed less expensive than a new model trained from scratch on the new domain (e.g. with comparable capacity to the proposed wrapping model). Authors should include experiments / baselines to assess this.\n* In section 2, the paper asserts that assuming access to logits breaks the blackbox assumption, but these are computable from softmax values (up to constant factors).\n* Is the beta regularization term is theoretically required to prevent unbounded growth or is it simply an empirically practical necessity?\n* In the image transfer experiments, STL-10 has relatively very few labeled examples (hundreds per class), so why was this only used as the source domain? Realistic transfer learning generally entails an expensive model trained on a source domain with plentiful data transferred to a target domain with scarce data.\n* Authors should mention that STL-10 images represent a distribution shift from CIFAR-10 as the two were not generated identically. The paper currently only highlights differences in dataset size.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper514/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper514/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dirichlet Wrapper to Quantify Classification Uncertainty in Black-Box Systems", "authors": ["Jos\u00e9 Mena Rold\u00e1n", "Oriol Pujol Vila", "Jordi Vitri\u00e0 Marca"], "authorids": ["jmenarol7@alumnes.ub.edu", "oriol_pujol@ub.edu", "jordi.vitria@ub.edu"], "keywords": ["uncertainty", "black-box classifiers", "rejection", "deep learning", "NLP", "CV"], "TL;DR": "A Dirichlet Deep Learning wrapper to quantify uncertainty in black-box systems applied to a rejection system to improve the quality of predictions", "abstract": "Nowadays, machine learning models are becoming a utility in many sectors. AI companies deliver pre-trained encapsulated models as application programming interfaces (APIs) that developers can combine with third party components, their models, and proprietary data, to create complex data products. This complexity and the lack of control and knowledge of the internals of these external components might cause unavoidable effects, such as lack of transparency, difficulty in auditability, and the emergence of uncontrolled potential risks. These issues are especially critical when practitioners use these components as black-boxes in new datasets. In order to provide actionable insights in this type of scenarios, in this work we propose the use of a wrapping deep learning model to enrich the output of a classification black-box with a measure of uncertainty. Given a black-box classifier, we propose a probabilistic neural network that works in parallel to the black-box and uses a Dirichlet layer as the fusion layer with the black-box.  This Dirichlet layer yields a distribution on top of the multinomial output parameters of the classifier and enables the estimation of aleatoric uncertainty for any data sample.  \nBased on the resulting uncertainty measure, we advocate for a rejection system that selects the more confident predictions, discarding those more uncertain, leading to an improvement in the trustability of the resulting system. We showcase the proposed technique and methodology in two practical scenarios, one for NLP and another for computer vision, where a simulated API based is applied to different domains. Results demonstrate the effectiveness of the uncertainty computed by the wrapper and its high correlation to wrong predictions and misclassifications.", "pdf": "/pdf/a1b53ac7a08397d12ffe2b7555c0efa583f1dc56.pdf", "code": "https://colab.research.google.com/drive/1YsJucBwBSW3Fy6ECAhab21Nzrib3K_t2", "paperhash": "rold\u00e1n|dirichlet_wrapper_to_quantify_classification_uncertainty_in_blackbox_systems", "original_pdf": "/attachment/a1b53ac7a08397d12ffe2b7555c0efa583f1dc56.pdf", "_bibtex": "@misc{\nrold{\\'a}n2020dirichlet,\ntitle={Dirichlet Wrapper to Quantify Classification Uncertainty in Black-Box Systems},\nauthor={Jos{\\'e} Mena Rold{\\'a}n and Oriol Pujol Vila and Jordi Vitri{\\`a} Marca},\nyear={2020},\nurl={https://openreview.net/forum?id=HyezBa4tPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HyezBa4tPB", "replyto": "HyezBa4tPB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper514/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper514/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575323049603, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper514/Reviewers"], "noninvitees": [], "tcdate": 1570237751038, "tmdate": 1575323049615, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper514/-/Official_Review"}}}], "count": 6}