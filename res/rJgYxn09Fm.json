{"notes": [{"id": "rJgYxn09Fm", "original": "ryxvKh3qFQ", "number": 1098, "cdate": 1538087921362, "ddate": null, "tcdate": 1538087921362, "tmdate": 1551478419721, "tddate": null, "forum": "rJgYxn09Fm", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Learning Implicitly Recurrent CNNs Through Parameter Sharing", "abstract": "We introduce a parameter sharing scheme, in which different layers of a convolutional neural network (CNN) are defined by a learned linear combination of parameter tensors from a global bank of templates.  Restricting the number of templates yields a flexible hybridization of traditional CNNs and recurrent networks.  Compared to traditional CNNs, we demonstrate substantial parameter savings on standard image classification tasks, while maintaining accuracy.\nOur simple parameter sharing scheme, though defined via soft weights, in practice often yields trained networks with near strict recurrent structure; with negligible side effects, they convert into networks with actual loops. Training these networks thus implicitly involves discovery of suitable recurrent architectures. Though considering only the aspect of recurrent links, our trained networks achieve accuracy competitive with those built using state-of-the-art neural architecture search (NAS) procedures.\nOur hybridization of recurrent and convolutional networks may also represent a beneficial architectural bias.  Specifically, on synthetic tasks which are algorithmic in nature, our hybrid networks both train faster and extrapolate better to test examples outside the span of the training set.", "keywords": ["deep learning", "architecture search", "computer vision"], "authorids": ["savarese@ttic.edu", "mmaire@uchicago.edu"], "authors": ["Pedro Savarese", "Michael Maire"], "TL;DR": "We propose a method that enables CNN folding to create recurrent connections", "pdf": "/pdf/c0ca6381a4b14e3fd45586e92ef0e243d5f1fd0e.pdf", "paperhash": "savarese|learning_implicitly_recurrent_cnns_through_parameter_sharing", "_bibtex": "@inproceedings{\nsavarese2018learning,\ntitle={Learning Implicitly Recurrent {CNN}s Through Parameter Sharing},\nauthor={Pedro Savarese and Michael Maire},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJgYxn09Fm},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 11, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "B1eEebvBlE", "original": null, "number": 1, "cdate": 1545068779550, "ddate": null, "tcdate": 1545068779550, "tmdate": 1545354481946, "tddate": null, "forum": "rJgYxn09Fm", "replyto": "rJgYxn09Fm", "invitation": "ICLR.cc/2019/Conference/-/Paper1098/Meta_Review", "content": {"metareview": "This paper proposed an interesting approach to weight sharing among CNN layers via shared weight templates to save parameters. It's well written with convincing results. Reviewers have a consensus on accept.", "confidence": "5: The area chair is absolutely certain", "recommendation": "Accept (Poster)", "title": "a promising idea"}, "signatures": ["ICLR.cc/2019/Conference/Paper1098/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper1098/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Implicitly Recurrent CNNs Through Parameter Sharing", "abstract": "We introduce a parameter sharing scheme, in which different layers of a convolutional neural network (CNN) are defined by a learned linear combination of parameter tensors from a global bank of templates.  Restricting the number of templates yields a flexible hybridization of traditional CNNs and recurrent networks.  Compared to traditional CNNs, we demonstrate substantial parameter savings on standard image classification tasks, while maintaining accuracy.\nOur simple parameter sharing scheme, though defined via soft weights, in practice often yields trained networks with near strict recurrent structure; with negligible side effects, they convert into networks with actual loops. Training these networks thus implicitly involves discovery of suitable recurrent architectures. Though considering only the aspect of recurrent links, our trained networks achieve accuracy competitive with those built using state-of-the-art neural architecture search (NAS) procedures.\nOur hybridization of recurrent and convolutional networks may also represent a beneficial architectural bias.  Specifically, on synthetic tasks which are algorithmic in nature, our hybrid networks both train faster and extrapolate better to test examples outside the span of the training set.", "keywords": ["deep learning", "architecture search", "computer vision"], "authorids": ["savarese@ttic.edu", "mmaire@uchicago.edu"], "authors": ["Pedro Savarese", "Michael Maire"], "TL;DR": "We propose a method that enables CNN folding to create recurrent connections", "pdf": "/pdf/c0ca6381a4b14e3fd45586e92ef0e243d5f1fd0e.pdf", "paperhash": "savarese|learning_implicitly_recurrent_cnns_through_parameter_sharing", "_bibtex": "@inproceedings{\nsavarese2018learning,\ntitle={Learning Implicitly Recurrent {CNN}s Through Parameter Sharing},\nauthor={Pedro Savarese and Michael Maire},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJgYxn09Fm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1098/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545352966743, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJgYxn09Fm", "replyto": "rJgYxn09Fm", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1098/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper1098/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1098/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545352966743}}}, {"id": "H1lWxZDEAm", "original": null, "number": 7, "cdate": 1542906089074, "ddate": null, "tcdate": 1542906089074, "tmdate": 1542906089074, "tddate": null, "forum": "rJgYxn09Fm", "replyto": "BygqPHo70Q", "invitation": "ICLR.cc/2019/Conference/-/Paper1098/Official_Comment", "content": {"title": "Thank you for the detailed and satisfactory reply.", "comment": "Thank you for addressing all my comments in detail, your replies satisfactorily cover all the issues I raised. I look forward to reading the final version of the manuscript."}, "signatures": ["ICLR.cc/2019/Conference/Paper1098/AnonReviewer3"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1098/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1098/AnonReviewer3", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Implicitly Recurrent CNNs Through Parameter Sharing", "abstract": "We introduce a parameter sharing scheme, in which different layers of a convolutional neural network (CNN) are defined by a learned linear combination of parameter tensors from a global bank of templates.  Restricting the number of templates yields a flexible hybridization of traditional CNNs and recurrent networks.  Compared to traditional CNNs, we demonstrate substantial parameter savings on standard image classification tasks, while maintaining accuracy.\nOur simple parameter sharing scheme, though defined via soft weights, in practice often yields trained networks with near strict recurrent structure; with negligible side effects, they convert into networks with actual loops. Training these networks thus implicitly involves discovery of suitable recurrent architectures. Though considering only the aspect of recurrent links, our trained networks achieve accuracy competitive with those built using state-of-the-art neural architecture search (NAS) procedures.\nOur hybridization of recurrent and convolutional networks may also represent a beneficial architectural bias.  Specifically, on synthetic tasks which are algorithmic in nature, our hybrid networks both train faster and extrapolate better to test examples outside the span of the training set.", "keywords": ["deep learning", "architecture search", "computer vision"], "authorids": ["savarese@ttic.edu", "mmaire@uchicago.edu"], "authors": ["Pedro Savarese", "Michael Maire"], "TL;DR": "We propose a method that enables CNN folding to create recurrent connections", "pdf": "/pdf/c0ca6381a4b14e3fd45586e92ef0e243d5f1fd0e.pdf", "paperhash": "savarese|learning_implicitly_recurrent_cnns_through_parameter_sharing", "_bibtex": "@inproceedings{\nsavarese2018learning,\ntitle={Learning Implicitly Recurrent {CNN}s Through Parameter Sharing},\nauthor={Pedro Savarese and Michael Maire},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJgYxn09Fm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1098/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621616164, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJgYxn09Fm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1098/Authors", "ICLR.cc/2019/Conference/Paper1098/Reviewers", "ICLR.cc/2019/Conference/Paper1098/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1098/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1098/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1098/Authors|ICLR.cc/2019/Conference/Paper1098/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1098/Reviewers", "ICLR.cc/2019/Conference/Paper1098/Authors", "ICLR.cc/2019/Conference/Paper1098/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621616164}}}, {"id": "Sye4qrs7Am", "original": null, "number": 4, "cdate": 1542858124428, "ddate": null, "tcdate": 1542858124428, "tmdate": 1542858418738, "tddate": null, "forum": "rJgYxn09Fm", "replyto": "BygndrCp3m", "invitation": "ICLR.cc/2019/Conference/-/Paper1098/Official_Comment", "content": {"title": "[3/4] Thank you. We appreciate the detailed feedback and questions. ", "comment": "8 - Sec 4.4, it is unclear to me what can be the contribution of the 1x1 initial convolution, since it will see no context and all the information at the pixel level can be represented by a binary bit. Also, are the 3x3 convolutions \u201csame\u201d convolutions? If not, how are the last feature maps upscaled to be able to predict at the original resolution?\n\nWe wanted all 3x3 convolutions to participate in parameter sharing, and to do so they need to have the same number of input and output channels. Without the initial 1x1 convolution, the next convolution (the first 3x3 one) would have 2 input channels instead of 32, like all other 3x3 convolutions. The 1x1 kernel size was chosen since the only purpose of this convolution is to change the number of channels.\n\nAll 3x3 convolutions are \u201csame\u201d, with outputs having the same spatial resolution as the input.\n\n\n9 - At the end of sec 4.4 the authors claim that the SCNN is \u201calso advantaged over a more RNN-like model\u201d. I fail to understand how to process this sentence, but I have a feeling that it\u2019s incorrect to make any claims to the performance of \u201cRNN-like models\u201d as such a model was not used as a baseline in the experiments in any way. Similarly, in the conclusions I find it a bit stretched to claim that you can \u201cgain a more flexible form of behavior typically attributed to RNNs\u201d. While it\u2019s true that the proposed network can in theory learn to reuse the same combination of templates, which can be mapped to a network with recursion, the results in this direction don\u2019t seem strong enough to draw any conclusion and a more in-depth comparison against RNN performance would be in order before making any claim in this direction.\n\nWe will rephrase our claims to make them more precise. By \u2018RNN-like model\u2019 we meant the SCNN with lambda_R = 0.01 (recurrence regularizer), where all 20 layers become maximally similar in the first 10 epochs (all elements in the LSM get very close to 1), meaning that the filters used by all 20 convolutions are (roughly) the same (up to scaling). While in practice this can be seen as a RNN, we agree that our wording should be more specific. We added this observation (on the LSM of the SCNN with the recurrence regularizer) to the manuscript."}, "signatures": ["ICLR.cc/2019/Conference/Paper1098/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1098/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1098/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Implicitly Recurrent CNNs Through Parameter Sharing", "abstract": "We introduce a parameter sharing scheme, in which different layers of a convolutional neural network (CNN) are defined by a learned linear combination of parameter tensors from a global bank of templates.  Restricting the number of templates yields a flexible hybridization of traditional CNNs and recurrent networks.  Compared to traditional CNNs, we demonstrate substantial parameter savings on standard image classification tasks, while maintaining accuracy.\nOur simple parameter sharing scheme, though defined via soft weights, in practice often yields trained networks with near strict recurrent structure; with negligible side effects, they convert into networks with actual loops. Training these networks thus implicitly involves discovery of suitable recurrent architectures. Though considering only the aspect of recurrent links, our trained networks achieve accuracy competitive with those built using state-of-the-art neural architecture search (NAS) procedures.\nOur hybridization of recurrent and convolutional networks may also represent a beneficial architectural bias.  Specifically, on synthetic tasks which are algorithmic in nature, our hybrid networks both train faster and extrapolate better to test examples outside the span of the training set.", "keywords": ["deep learning", "architecture search", "computer vision"], "authorids": ["savarese@ttic.edu", "mmaire@uchicago.edu"], "authors": ["Pedro Savarese", "Michael Maire"], "TL;DR": "We propose a method that enables CNN folding to create recurrent connections", "pdf": "/pdf/c0ca6381a4b14e3fd45586e92ef0e243d5f1fd0e.pdf", "paperhash": "savarese|learning_implicitly_recurrent_cnns_through_parameter_sharing", "_bibtex": "@inproceedings{\nsavarese2018learning,\ntitle={Learning Implicitly Recurrent {CNN}s Through Parameter Sharing},\nauthor={Pedro Savarese and Michael Maire},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJgYxn09Fm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1098/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621616164, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJgYxn09Fm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1098/Authors", "ICLR.cc/2019/Conference/Paper1098/Reviewers", "ICLR.cc/2019/Conference/Paper1098/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1098/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1098/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1098/Authors|ICLR.cc/2019/Conference/Paper1098/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1098/Reviewers", "ICLR.cc/2019/Conference/Paper1098/Authors", "ICLR.cc/2019/Conference/Paper1098/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621616164}}}, {"id": "HJxuxUsQA7", "original": null, "number": 5, "cdate": 1542858223618, "ddate": null, "tcdate": 1542858223618, "tmdate": 1542858400014, "tddate": null, "forum": "rJgYxn09Fm", "replyto": "BygndrCp3m", "invitation": "ICLR.cc/2019/Conference/-/Paper1098/Official_Comment", "content": {"title": "[2/4] Thank you. We appreciate the detailed feedback and questions.", "comment": "4 - While the number of learned parameters is indeed reduced when the templates are shared among layers - which could lead to better sample efficiency - I am not sure whether the memory footprint on GPU would change (i.e., I believe that current frameworks would allocate the same kernel n-times if the same template was shared by n layers, but I am not certain). Although the potential reduction of the number of trainable parameters is an important result by itself, I wonder if what you propose would also allow to run bigger models on the same device or not, without heavy modifications of the inner machineries of pyTorch or Tensorflow. Can you comment on this? Also note that the soft sharing regularization scheme that you propose can be of great interest for FPGA hardware implementations, that benefit a lot from module reuse. You could mention that in the paper.\n\nThanks for pointing out the potential for FPGA implementations.\nThere are at least two settings where our method can lead to infrastructural advantages:\n\n(i) During training PyTorch and Tensorflow both store the generated weights for each layer (to compute gradients in the backward pass), but at test time this is not the case: once the weights have been generated and used for the convolution, the memory is freed, and the only parameters that have to be stored are the alphas and the templates (which are only allocated once, regardless of how many layers use them).  Returning to the question of training, we believe that dedicated implementations could lead to significant memory savings.  For example, instead of storing the weights generated for each layer, training could recompute them in the backward pass (such recomputation is a cheap operation).\n\n(ii) In the distributed training setting with multiple machines, gradients have to be communicated at each optimization round. In the case where our method offers roughly 50% parameter reduction (Tables 1 and 2), communication cost would also be decreased by 50%.\n\n\n5 - Sec 4.1, the number of layers in one group is defined as (L-4)/3. It\u2019s unclear to me where the 4 comes from. Also on page 6, k = (L-2)/3 - 2 is said to set one template per layer. I thought the two formulas would be the same in that case. What am I missing? Is it possible that one of the two formulas contain a typo (I believe that at the very least it should be either (L-2) or (L-4) in both cases)?\n\nThe 4 is specific for Wide ResNets, where we have a first convolution with 3 input channels (RBG of the input image) and 3 convolutions with 1x1 kernels in skip-connections (one for every first skip-connection of each stage). These layers do not participate in sharing since no other layers in the network have the same parameter shapes.\nThe k = (L-2)/3 - 2 from page 6 is indeed a typo which we fixed -- thanks for pointing it out.\n\n\n6 - Sec 4.1, I find the notation SWRN-L-w-k and SWRN-L-w confusing. My suggestion is to set k to be the total number of templates (as opposed to the number of templates *per group of layers*), which makes it much easier to relate to, and most importantly allows for an immediate comparison with the capacity of the vanilla model. As a side effect, it also makes it very easy to spot the configuration with one template per layer (SWRN-L-w-L) thus eliminating the need for an ad-hoc notation to distinguish it.\n\nWe agree with your suggestion and will revise the notation in a forthcoming update of the paper. However, note that the configuration with one template per layer will not be denoted by SWRN-L-w-L since some layers do not participate in parameter sharing (see above).\n\n\n7 - The authors inspect how similar the template combination weights alphas are among layers. It would also be interesting to look into what is learned in the templates. CNN layers are known to learn peculiar and somewhat interpretable template matching filters. It would be really interesting to compare the filters learned by a vanilla network and its template sharing alternative. Also, I would welcome an analysis of which templates gets chosen the most at each layer in the hierarchy. It would be compelling if some kind of pattern of reuse emerged from learning. \n\nWe agree that inspecting the learned behavior of our shared networks and contrasting with standard CNNs in a worthwhile effort.  Given that visualizing and understanding CNNs is itself an area of active research, performing a thorough analysis in this respect is probably beyond the scope of the current paper; it would require substantial additional experiments.\n\nAs for which templates get chosen the most, we have plots of the alphas themselves, which indicate which templates are most important to each layer. We will add them to the final version.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1098/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1098/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1098/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Implicitly Recurrent CNNs Through Parameter Sharing", "abstract": "We introduce a parameter sharing scheme, in which different layers of a convolutional neural network (CNN) are defined by a learned linear combination of parameter tensors from a global bank of templates.  Restricting the number of templates yields a flexible hybridization of traditional CNNs and recurrent networks.  Compared to traditional CNNs, we demonstrate substantial parameter savings on standard image classification tasks, while maintaining accuracy.\nOur simple parameter sharing scheme, though defined via soft weights, in practice often yields trained networks with near strict recurrent structure; with negligible side effects, they convert into networks with actual loops. Training these networks thus implicitly involves discovery of suitable recurrent architectures. Though considering only the aspect of recurrent links, our trained networks achieve accuracy competitive with those built using state-of-the-art neural architecture search (NAS) procedures.\nOur hybridization of recurrent and convolutional networks may also represent a beneficial architectural bias.  Specifically, on synthetic tasks which are algorithmic in nature, our hybrid networks both train faster and extrapolate better to test examples outside the span of the training set.", "keywords": ["deep learning", "architecture search", "computer vision"], "authorids": ["savarese@ttic.edu", "mmaire@uchicago.edu"], "authors": ["Pedro Savarese", "Michael Maire"], "TL;DR": "We propose a method that enables CNN folding to create recurrent connections", "pdf": "/pdf/c0ca6381a4b14e3fd45586e92ef0e243d5f1fd0e.pdf", "paperhash": "savarese|learning_implicitly_recurrent_cnns_through_parameter_sharing", "_bibtex": "@inproceedings{\nsavarese2018learning,\ntitle={Learning Implicitly Recurrent {CNN}s Through Parameter Sharing},\nauthor={Pedro Savarese and Michael Maire},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJgYxn09Fm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1098/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621616164, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJgYxn09Fm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1098/Authors", "ICLR.cc/2019/Conference/Paper1098/Reviewers", "ICLR.cc/2019/Conference/Paper1098/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1098/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1098/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1098/Authors|ICLR.cc/2019/Conference/Paper1098/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1098/Reviewers", "ICLR.cc/2019/Conference/Paper1098/Authors", "ICLR.cc/2019/Conference/Paper1098/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621616164}}}, {"id": "SklIGLjQAm", "original": null, "number": 6, "cdate": 1542858253584, "ddate": null, "tcdate": 1542858253584, "tmdate": 1542858347116, "tddate": null, "forum": "rJgYxn09Fm", "replyto": "BygndrCp3m", "invitation": "ICLR.cc/2019/Conference/-/Paper1098/Official_Comment", "content": {"title": "[1/4] Thank you. We appreciate the detailed feedback and questions. ", "comment": "1 - It would be interesting to explore how often the layer parameters converge to similar weights and how similar. To this end I suggest to plot a 2d heatmap representing the similarity matrices between every pair of layers.\n\nThanks for the suggestion; we agree that this is a worthwhile experiment. The LSM matrices (Figure 4 & 6) already capture the similarity between every pair of layer for each stage: S_i,j is the absolute cosine similarity between the alpha parameters of layers i and j, so S_i,j ~ 1 (white) indicate that layers i and j have learned similar alphas (up to scaling).  However, exploring how often this alignment occurs is definitely relevant for the work. We will aim to include such experimental results in the final version of the paper.\n\n\n2 -  Figure 1 is not of immediate interpretability. Especially for the middle figure, what does the dotted box represent? What is the difference between weights and templates? Also it\u2019s unclear which of the three options corresponds to the proposed method. I would have thought the middle one, but the text seems to indicate it is the rightmost one instead.\n\nThe middle figure attempts to represent the case where filters W are generated from coefficients alpha and templates T through some function W = f(alpha, T). The dotted box represents that, unlike in the left figure (illustrating a network without parameter sharing), the weights are no longer parameters, but are generated from the coefficients alpha and the templates.  The distinction between weights and templates is that each has a different purpose: the weights are used as filters for the convolutional layers, while templates are used to generate weights (and are shared among many layers).\n\nOur method is illustrated by both the middle and the right figures: for the mapping W = f(alpha, T), whenever f(.) is a linear function, the middle and right figures become equivalent due to associativity (explained in more detail in section 3.1). They offer two perspectives for our technique.  We added a more detailed explanation to the caption of Figure 1. We updated the caption to make this more explicit.\n\n\n3 - How are the alphas initialized? How fast are their transitions? Do they change smoothly over training? Do they evolve rapidly and plateau to a fixed value or keep changing during training? It would be really interesting to plot their value and discuss their evolution.\n\nIn our ablation experiments, we observed that typical initializations (normal, uniform) are not good choices for the alphas. On the other hand, sparse and orthogonal initializations work better; we used orthogonal in the reported experiments. We added a discussion on initialization to the appendix.\n\nThe transitions are fast and happen mostly in the first 20 epochs of training, forming the patterns present in the Layer Similarity Matrices. After that point, typically similar layers become more similar and dissimilar layers become less similar, in a smooth fashion. We will add an example of evolution of a LSM during training in the appendix for the final version."}, "signatures": ["ICLR.cc/2019/Conference/Paper1098/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1098/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1098/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Implicitly Recurrent CNNs Through Parameter Sharing", "abstract": "We introduce a parameter sharing scheme, in which different layers of a convolutional neural network (CNN) are defined by a learned linear combination of parameter tensors from a global bank of templates.  Restricting the number of templates yields a flexible hybridization of traditional CNNs and recurrent networks.  Compared to traditional CNNs, we demonstrate substantial parameter savings on standard image classification tasks, while maintaining accuracy.\nOur simple parameter sharing scheme, though defined via soft weights, in practice often yields trained networks with near strict recurrent structure; with negligible side effects, they convert into networks with actual loops. Training these networks thus implicitly involves discovery of suitable recurrent architectures. Though considering only the aspect of recurrent links, our trained networks achieve accuracy competitive with those built using state-of-the-art neural architecture search (NAS) procedures.\nOur hybridization of recurrent and convolutional networks may also represent a beneficial architectural bias.  Specifically, on synthetic tasks which are algorithmic in nature, our hybrid networks both train faster and extrapolate better to test examples outside the span of the training set.", "keywords": ["deep learning", "architecture search", "computer vision"], "authorids": ["savarese@ttic.edu", "mmaire@uchicago.edu"], "authors": ["Pedro Savarese", "Michael Maire"], "TL;DR": "We propose a method that enables CNN folding to create recurrent connections", "pdf": "/pdf/c0ca6381a4b14e3fd45586e92ef0e243d5f1fd0e.pdf", "paperhash": "savarese|learning_implicitly_recurrent_cnns_through_parameter_sharing", "_bibtex": "@inproceedings{\nsavarese2018learning,\ntitle={Learning Implicitly Recurrent {CNN}s Through Parameter Sharing},\nauthor={Pedro Savarese and Michael Maire},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJgYxn09Fm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1098/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621616164, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJgYxn09Fm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1098/Authors", "ICLR.cc/2019/Conference/Paper1098/Reviewers", "ICLR.cc/2019/Conference/Paper1098/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1098/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1098/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1098/Authors|ICLR.cc/2019/Conference/Paper1098/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1098/Reviewers", "ICLR.cc/2019/Conference/Paper1098/Authors", "ICLR.cc/2019/Conference/Paper1098/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621616164}}}, {"id": "BygqPHo70Q", "original": null, "number": 3, "cdate": 1542858082051, "ddate": null, "tcdate": 1542858082051, "tmdate": 1542858327568, "tddate": null, "forum": "rJgYxn09Fm", "replyto": "BygndrCp3m", "invitation": "ICLR.cc/2019/Conference/-/Paper1098/Official_Comment", "content": {"title": "[4/4] Thank you.  We appreciate the detailed feedback and questions.", "comment": "MINOR\n- Sec3: I wouldn\u2019t say the parameters are shared among layers in LSTMs, but rather among time unrolls.\n\nWe changed to \u201cshared among all time steps\u201d in the manuscript.\n\n- One drawback of the proposed method is that the layers are constrained to have the same shape. This is not a major disadvantage, but is still a constraint that would be good to make more explicit in the description of the model.\n\nWe added an explicit mention of this constraint in Section 3 (paragraph following Equation 1).\n\n- Sec3, end of page 3: does the network reach the same accuracy as the vanilla model when k=L? Also, does the network use all the templates? How is the distribution of the alpha weights across layers in this case?\n\nYes, in all our experiments having one template per layer resulted in better accuracy than the vanilla model (for example, refer to Table 1, models WRN 28-10 and SWRN 28-10). In all our experiments, each template is used by at least one layer, however some layers do not use all templates (having one component of the coefficient vector alpha very close to zero). As mentioned previously, we will add plots of the coefficients to the final version.\n\n- Sec3.1, the V notation makes the narrative unnecessarily heavy. I suggest to drop it and refer directly to the templates T. Also the second part of the section, with examples of templates, doesn\u2019t add much in my opinion and would be better depicted with a figure.\n- Sec3.1, the e^(i) notation can be confused with an exp. I suggest to replace it with the much more common 1_{i=j}.\n\nWe removed the V notation and refer to templates directly. We have also removed the first example (\\alpha^(i) = e^(i)) for simplicity (and as the second one is more relevant for our work).\n\n- Figure 2 depicts the relation between the LSM matrix and the topology of the network. This should be declared more clearly in the caption, in place of the ambiguous \u201ccapturing implicit recurrencies\u201d. Also, the caption should explain what black/white stand for as well, and possibly quickly describe what the LSM matrix is. Also, it would be more clear that the network in the middle is equivalent to that on the right if the two were somehow connected in the figure. To this end they could, e.g., share a single LSM matrix among them. Finally, if possible try and put the LSM matrices on top of the related network, so that it\u2019s clear which network they refer to. Sec 3.2 should also refer to Fig2 I believe.\n\nWe updated Figure 2 and its caption (due to space constraints we could not place the LSM matrices on top of the networks, so we added dashed lines to make it clear which network each LSM matrix corresponds to).\n\n- Table 1: I suggest to leave the comment on the results out of the caption, since it\u2019s already in the main text.\n\nWe shortened the discussion on the results both in the caption of Table 1 and in Section 4.1.\n\n- Table 2: rather than using blue, I suggest to underline the overall best results, so that it\u2019s visible even if the paper is printed in B&W.\n\nWe changed from blue to underline to indicate best results.\n\n- Fig 3, I would specify that it\u2019s better viewed in color\n\nAdded to the caption.\n\n- Discussion: I feel the discussion of Table 1 is a bit difficult to follow. It could be made easier by reporting the difference in test error against the corresponding vanilla model (e.g., \u201cimproves the error rate on CIFAR10 by 0.26%\u201d, rather than reporting the performance of both models)\n\nWe updated the manuscript: instead of reporting the errors of both models, we report the relative error decrease (we believe the absolute error decrease might not be meaningful since the scale of the errors is no longer reported in the discussion).\n\n- Fig 4, are all the stages the same and is the network in the left one such stages? If so, update the caption to make it clear please.\n\nThe diagram on the left represents the architecture of each stage of the network, and all 3 stages have the same topology. We updated the caption to mention this explicitly.\n\n- Fig 4, which lambda has been used? Is it the same for all stages?\n\nThe recurrence regularizer has not been applied to any experiment except for the last one (Section 4.4): the patterns observed in the LSM, which enabled folding, have emerged naturally during training.\n\n- Fig 5, specify that the one on the right is the target grid. Also, I believe that merging the two figures would make it easier to understand (e.g., some of the structure in the target comes from how the obstacles are placed, which requires to move back and forth from input to target several times to understand)\n\nWe have merged the two figures together.\n\n- Sec 4.4, space permitting, I would like to see at least one sample of what kind of shortest path prediction the network can come up with.\n\nWe will aim to add at least one example in the final version of the manuscript."}, "signatures": ["ICLR.cc/2019/Conference/Paper1098/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1098/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1098/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Implicitly Recurrent CNNs Through Parameter Sharing", "abstract": "We introduce a parameter sharing scheme, in which different layers of a convolutional neural network (CNN) are defined by a learned linear combination of parameter tensors from a global bank of templates.  Restricting the number of templates yields a flexible hybridization of traditional CNNs and recurrent networks.  Compared to traditional CNNs, we demonstrate substantial parameter savings on standard image classification tasks, while maintaining accuracy.\nOur simple parameter sharing scheme, though defined via soft weights, in practice often yields trained networks with near strict recurrent structure; with negligible side effects, they convert into networks with actual loops. Training these networks thus implicitly involves discovery of suitable recurrent architectures. Though considering only the aspect of recurrent links, our trained networks achieve accuracy competitive with those built using state-of-the-art neural architecture search (NAS) procedures.\nOur hybridization of recurrent and convolutional networks may also represent a beneficial architectural bias.  Specifically, on synthetic tasks which are algorithmic in nature, our hybrid networks both train faster and extrapolate better to test examples outside the span of the training set.", "keywords": ["deep learning", "architecture search", "computer vision"], "authorids": ["savarese@ttic.edu", "mmaire@uchicago.edu"], "authors": ["Pedro Savarese", "Michael Maire"], "TL;DR": "We propose a method that enables CNN folding to create recurrent connections", "pdf": "/pdf/c0ca6381a4b14e3fd45586e92ef0e243d5f1fd0e.pdf", "paperhash": "savarese|learning_implicitly_recurrent_cnns_through_parameter_sharing", "_bibtex": "@inproceedings{\nsavarese2018learning,\ntitle={Learning Implicitly Recurrent {CNN}s Through Parameter Sharing},\nauthor={Pedro Savarese and Michael Maire},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJgYxn09Fm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1098/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621616164, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJgYxn09Fm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1098/Authors", "ICLR.cc/2019/Conference/Paper1098/Reviewers", "ICLR.cc/2019/Conference/Paper1098/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1098/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1098/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1098/Authors|ICLR.cc/2019/Conference/Paper1098/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1098/Reviewers", "ICLR.cc/2019/Conference/Paper1098/Authors", "ICLR.cc/2019/Conference/Paper1098/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621616164}}}, {"id": "HyxiNEjQCQ", "original": null, "number": 2, "cdate": 1542857779201, "ddate": null, "tcdate": 1542857779201, "tmdate": 1542857779201, "tddate": null, "forum": "rJgYxn09Fm", "replyto": "S1gMfAgqh7", "invitation": "ICLR.cc/2019/Conference/-/Paper1098/Official_Comment", "content": {"title": "We thank you for the feedback, and address specific points below.", "comment": "1 - The way of parameter sharing is similar to the filter prediction method proposed in Rebuff et al\u2019s work, where they model a convolutional layer\u2019s parameters as a linear combination of a bank of filters and use that to address difference among multiple domains.\n\nThank you for pointing out the work of Rebuffi et al.  There are similarities in some technical aspects, as both our work and theirs involve learning a bank of filters and mixing coefficients.  However, the overall goal of our work is entirely different from that of Rebuffi et al., as are the additional technical tools (e.g. layer similarity matrix, network folding) that we develop.\n\nRebuffi et al. focus on domain-adaptability and transfer-learning, while we focus on parameter reduction and architecture discovery. Because of this, we believe the findings of both papers (ours and Rebuffi et al.) are extremely complementary, as together they show the potential of \u2018template learning\u2019 both for the multi-domain setting (as it offers better domain-adaptation) and for single-domain (as it offers better performance, parameter reduction, and potentially simpler models). Note that we introduce novel tools for manipulating single-domain networks, yielding the ability to fold them into recurrent forms, that have no parallel in Rebuffi et al. We will add a discussion of the work of Rebuffi et al. in the final version of our manuscript.\n\n2 - However, they only experiment with one or two templates and advantage on accuracy and model size over other methods is not very clear.\n\nActually, our experiments include models with the number of templates ranging from 1 to 20 (per sharing group). More specifically, the CIFAR experiments (refer to Tables 1, 2 and Figure 3) consist of models with between 1 and 6 templates: the SWRN-L-w-k models where k is omitted (e.g. SWRN 28-10) have one template per sharing layer, meaning k=6 for 28-layered models.  As we believe that omitting k to represent one template per layer can lead to confusion, we will revise this notation in a forthcoming update of the paper, \n\nFor these same experiments, we focused on the regime with few templates since our goal is to decrease parameter redundancy: we believe one of our most important findings is that networks with k=1 or k=2 manage to outperform the same models with larger k, as the latter have significantly more capacity.\n\nIn our last experiments (in Section 4.4), the two SCNN models have a total of 20 templates shared among 20 convolutional layers. In this setting, the SCNN significantly outperforms the CNN, and adapts faster to curriculum changes (Figure 5b, compare blue and orange curves).\n\nAs for advantage on accuracy and model size, Table 1 shows that we can achieve both performance increase and parameter reduction: the SWRN 28-18-2 model not only outperforms SWRN 28-18, but also has less than half of its parameters. It also outperforms the ResNeXt model, which, even though it has bottleneck layers, still has more parameters than SWRN 28-18-2. We would also like to point out that all results are average of 5 runs (except the curves in Figure 5b)."}, "signatures": ["ICLR.cc/2019/Conference/Paper1098/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1098/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1098/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Implicitly Recurrent CNNs Through Parameter Sharing", "abstract": "We introduce a parameter sharing scheme, in which different layers of a convolutional neural network (CNN) are defined by a learned linear combination of parameter tensors from a global bank of templates.  Restricting the number of templates yields a flexible hybridization of traditional CNNs and recurrent networks.  Compared to traditional CNNs, we demonstrate substantial parameter savings on standard image classification tasks, while maintaining accuracy.\nOur simple parameter sharing scheme, though defined via soft weights, in practice often yields trained networks with near strict recurrent structure; with negligible side effects, they convert into networks with actual loops. Training these networks thus implicitly involves discovery of suitable recurrent architectures. Though considering only the aspect of recurrent links, our trained networks achieve accuracy competitive with those built using state-of-the-art neural architecture search (NAS) procedures.\nOur hybridization of recurrent and convolutional networks may also represent a beneficial architectural bias.  Specifically, on synthetic tasks which are algorithmic in nature, our hybrid networks both train faster and extrapolate better to test examples outside the span of the training set.", "keywords": ["deep learning", "architecture search", "computer vision"], "authorids": ["savarese@ttic.edu", "mmaire@uchicago.edu"], "authors": ["Pedro Savarese", "Michael Maire"], "TL;DR": "We propose a method that enables CNN folding to create recurrent connections", "pdf": "/pdf/c0ca6381a4b14e3fd45586e92ef0e243d5f1fd0e.pdf", "paperhash": "savarese|learning_implicitly_recurrent_cnns_through_parameter_sharing", "_bibtex": "@inproceedings{\nsavarese2018learning,\ntitle={Learning Implicitly Recurrent {CNN}s Through Parameter Sharing},\nauthor={Pedro Savarese and Michael Maire},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJgYxn09Fm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1098/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621616164, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJgYxn09Fm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1098/Authors", "ICLR.cc/2019/Conference/Paper1098/Reviewers", "ICLR.cc/2019/Conference/Paper1098/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1098/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1098/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1098/Authors|ICLR.cc/2019/Conference/Paper1098/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1098/Reviewers", "ICLR.cc/2019/Conference/Paper1098/Authors", "ICLR.cc/2019/Conference/Paper1098/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621616164}}}, {"id": "S1eeeNoXCX", "original": null, "number": 1, "cdate": 1542857704062, "ddate": null, "tcdate": 1542857704062, "tmdate": 1542857704062, "tddate": null, "forum": "rJgYxn09Fm", "replyto": "Hke2lCb92Q", "invitation": "ICLR.cc/2019/Conference/-/Paper1098/Official_Comment", "content": {"title": "Thank you for the feedback and specific questions, which we address in detail below.", "comment": "With respect to novelty, we do not believe there is any existing work that utilizes a parameter sharing scheme toward the objective we accomplish here: training a deep network and then folding it into a recurrent form.  Please also see our detailed reply to AnonReviewer2.\n\n1- Regarding the coefficient alpha, I'm not sure how cosine similarity is computed. I have the impression that each layer has its own alpha, which is a scalar. How is cosine similarity computed on scalars?\n\nEach layer i has its own alpha parameter, denoted by alpha^(i) in the manuscript (refer to equation 1 on page 3), but each alpha is a k-dimensional vector, where k is the number of templates to which that layer has access. For the SWRN-L-w-k models we use in the experiments, the same k denotes the number of templates each layer can use, so the dimensionality of each alpha ranges from 1 (in this case it\u2019s just a scalar) to 6 in our experiments. \n\nWe made alpha bold in the current version of the manuscript to clarify that it is a vector (except when k=1).\n\n2 - In the experiments, there's no mentioning of the regularization terms for alpha, which makes me think it is perhaps not important? What is the generic setup?\n\nWe tried applying L2 regularization to the alpha parameters in our initial experiments, but observed a performance drop, so all reported experiments have no regularization on the alphas.\n\nAs for the recurrence regularizer described in the end of Section 3.2, where we regularize the Layer Similarity Matrix, it was only used for the experiments in Section 4.4 -- more specifically, the \u201cSCNN, lambda_R = 0.01\u201d model depicted in Figure 5. It was not used for any other experiments, meaning that the observed patterns (e.g. the Layer Similarity Matrices in Figure 4) emerge naturally during optimization, where neither the alphas nor the LSMs had any regularization."}, "signatures": ["ICLR.cc/2019/Conference/Paper1098/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1098/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1098/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Implicitly Recurrent CNNs Through Parameter Sharing", "abstract": "We introduce a parameter sharing scheme, in which different layers of a convolutional neural network (CNN) are defined by a learned linear combination of parameter tensors from a global bank of templates.  Restricting the number of templates yields a flexible hybridization of traditional CNNs and recurrent networks.  Compared to traditional CNNs, we demonstrate substantial parameter savings on standard image classification tasks, while maintaining accuracy.\nOur simple parameter sharing scheme, though defined via soft weights, in practice often yields trained networks with near strict recurrent structure; with negligible side effects, they convert into networks with actual loops. Training these networks thus implicitly involves discovery of suitable recurrent architectures. Though considering only the aspect of recurrent links, our trained networks achieve accuracy competitive with those built using state-of-the-art neural architecture search (NAS) procedures.\nOur hybridization of recurrent and convolutional networks may also represent a beneficial architectural bias.  Specifically, on synthetic tasks which are algorithmic in nature, our hybrid networks both train faster and extrapolate better to test examples outside the span of the training set.", "keywords": ["deep learning", "architecture search", "computer vision"], "authorids": ["savarese@ttic.edu", "mmaire@uchicago.edu"], "authors": ["Pedro Savarese", "Michael Maire"], "TL;DR": "We propose a method that enables CNN folding to create recurrent connections", "pdf": "/pdf/c0ca6381a4b14e3fd45586e92ef0e243d5f1fd0e.pdf", "paperhash": "savarese|learning_implicitly_recurrent_cnns_through_parameter_sharing", "_bibtex": "@inproceedings{\nsavarese2018learning,\ntitle={Learning Implicitly Recurrent {CNN}s Through Parameter Sharing},\nauthor={Pedro Savarese and Michael Maire},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJgYxn09Fm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1098/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621616164, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJgYxn09Fm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1098/Authors", "ICLR.cc/2019/Conference/Paper1098/Reviewers", "ICLR.cc/2019/Conference/Paper1098/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1098/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1098/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1098/Authors|ICLR.cc/2019/Conference/Paper1098/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1098/Reviewers", "ICLR.cc/2019/Conference/Paper1098/Authors", "ICLR.cc/2019/Conference/Paper1098/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621616164}}}, {"id": "BygndrCp3m", "original": null, "number": 3, "cdate": 1541428595807, "ddate": null, "tcdate": 1541428595807, "tmdate": 1541533425158, "tddate": null, "forum": "rJgYxn09Fm", "replyto": "rJgYxn09Fm", "invitation": "ICLR.cc/2019/Conference/-/Paper1098/Official_Review", "content": {"title": "Interesting approach to weight sharing among CNN layers via shared weight templates, well written, convincing results.", "review": "The manuscript introduces a novel and interesting approach to weight sharing among CNNs layers, by learning linear combinations of shared weight templates. This allows parameter reduction, better sample efficiency. Furthermore, the authors propose a very simple way to inspect which layers choose similar combinations of template, as well as to push the network toward using similar combinations at each layer. This regularization term has a clear potential for computation reuse on dedicated hardware. The paper is well written, the method is interesting, the results are convincing and thoroughly conducted. I recommend acceptance.\n\n1) It would be interesting to explore how often the layer parameters converge to similar weights and how similar. To this end I suggest to plot a 2d heatmap representing the similarity matrices between every pair of layers.\n\n2) Figure 1 is not of immediate interpretability. Especially for the middle figure, what does the dotted box represent? What is the difference between weights and templates? Also it\u2019s unclear which of the three options corresponds to the proposed method. I would have thought the middle one, but the text seems to indicate it is the rightmost one instead.\n\n3) How are the alphas initialized? How fast are their transitions? Do they change smoothly over training? Do they evolve rapidly and plateau to a fixed value or keep changing during training? It would be really interesting to plot their value and discuss their evolution.\n\n4) While the number of learned parameters is indeed reduced when the templates are shared among layers - which could lead to better sample efficiency - I am not sure whether the memory footprint on GPU would change (i.e., I believe that current frameworks would allocate the same kernel n-times if the same template was shared by n layers, but I am not certain). Although the potential reduction of the number of trainable parameters is an important result by itself, I wonder if what you propose would also allow to run bigger models on the same device or not, without heavy modifications of the inner machineries of pyTorch or Tensorflow.  Can you comment on this? Also note that the soft sharing regularization scheme that you propose can be of great interest for FPGA hardware implementations, that benefit a lot from module reuse. You could mention that in the paper.\n\n5) Sec 4.1, the number of layers in one group is defined as (L-4)/3. It\u2019s unclear to me where the 4 comes from. Also on page 6, k = (L-2)/3 - 2 is said to set one template per layer. I thought the two formulas would be the same in that case. What am I missing? Is it possible that one of the two formulas contain a typo (I believe that at the very least it should be either (L-2) or (L-4) in both cases)?\n\n6) Sec 4.1, I find the notation SWRN-L-w-k and SWRN-L-w confusing. My suggestion is to set k to be the total number of templates (as opposed to the number of templates *per group of layers*), which makes it much easier to relate to, and most importantly allows for an immediate comparison with the capacity of the vanilla model. As a side effect, it also makes it very easy to spot the configuration with one template per layer (SWRN-L-w-L) thus eliminating the need for an ad-hoc notation to distinguish it.\n\n7) The authors inspect how similar the template combination weights alphas are among layers. It would also be interesting to look into what is learned in the templates. CNN layers are known to learn peculiar and somewhat interpretable template matching filters. It would be really interesting to compare the filters learned by a vanilla network and its template sharing alternative. Also, I would welcome an analysis of which templates gets chosen the most at each layer in the hierarchy. It would be compelling if some kind of pattern of reuse emerged from learning.\n\n8) Sec 4.4, it is unclear to me what can be the contribution of the 1x1 initial convolution, since it will see no context and all the information at the pixel level can be represented by a binary bit. Also, are the 3x3 convolutions \u201csame\u201d convolutions? If not, how are the last feature maps upscaled to be able to predict at the original resolution?\n\n9) At the end of sec 4.4 the authors claim that the SCNN is \u201calso advantaged over a more RNN-like model\u201d. I fail to understand how to process this sentence, but I have a feeling that it\u2019s incorrect to make any claims to the performance of \u201cRNN-like models\u201d as such a model was not used as a baseline in the experiments in any way. Similarly, in the conclusions I find it a bit stretched to claim that you can \u201cgain a more flexible form of behavior typically attributed to RNNs\u201d. While it\u2019s true that the proposed network can in theory learn to reuse the same combination of templates, which can be mapped to a network with recursion, the results in this direction don\u2019t seem strong enough to draw any conclusion and a more in-depth comparison against RNN performance would be in order before making any claim in this direction.\n\n\nMINOR\n- Sec3: I wouldn\u2019t say the parameters are shared among layers in LSTMs, but rather among time unrolls.\n- One drawback of the proposed method is that the layers are constrained to have the same shape. This is not a major disadvantage, but is still a constraint that would be good to make more explicit in the description of the model.\n- Sec3, end of page 3: does the network reach the same accuracy as the vanilla model when k=L? Also, does the network use all the templates? How is the distribution of the alpha weights across layers in this case?\n- Sec3.1, the V notation makes the narrative unnecessarily heavy. I suggest to drop it and refer directly to the templates T. Also the second part of the section, with examples of templates, doesn\u2019t add much in my opinion and would be better depicted with a figure.\n- Sec3.1, the e^(i) notation can be confused with an exp. I suggest to replace it with the much more common 1_{i=j}.\n- Figure 2 depicts the relation between the LSM matrix and the topology of the network. This should be declared more clearly in the caption, in place of the ambiguous \u201ccapturing implicit recurrencies\u201d. Also, the caption should explain what black/white stand for as well, and possibly quickly describe what the LSM matrix is. Also, it would be more clear that the network in the middle is equivalent to that on the right if the two were somehow connected in the figure. To this end they could, e.g., share a single LSM matrix among them. Finally, if possible try and put the LSM matrices on top of the related network, so that it\u2019s clear which network they refer to. Sec 3.2 should also refer to Fig2 I believe.\n- Table 1: I suggest to leave the comment on the results out of the caption, since it\u2019s already in the main text.\n- Table 2: rather than using blue, I suggest to underline the overall best results, so that it\u2019s visible even if the paper is printed in B&W.\n- Fig 3, I would specify that it\u2019s better viewed in color\n- Discussion: I feel the discussion of Table 1 is a bit difficult to follow. It could be made easier by reporting the difference in test error against the corresponding vanilla model (e.g., \u201cimproves the error rate on CIFAR10 by 0.26%\u201d, rather than reporting the performance of both models)\n- Fig 4, are all the stages the same and is the network in the left one such stages? If so, update the caption to make it clear please.\n- Fig 4, which lambda has been used? Is it the same for all stages?\n- Fig 5, specify that the one on the right is the target grid. Also, I believe that merging the two figures would make it easier to understand (e.g., some of the structure in the target comes from how the obstacles are placed, which requires to move back and forth from input to target several times to understand)\n- Sec 4.4, space permitting, I would like to see at least one sample of what kind of shortest path prediction the network can come up with.\n\n\n\nA few typos:\n    * End of 3.2: the closer elements -> the closer the elements\n    * Parameter efficiency: the period before re-parametrizing should probably be a comma?\n    * Fig 4, illustration of stages -> illustration of the stages\n    * End of pag7, an syntetic -> a syntetic", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1098/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Implicitly Recurrent CNNs Through Parameter Sharing", "abstract": "We introduce a parameter sharing scheme, in which different layers of a convolutional neural network (CNN) are defined by a learned linear combination of parameter tensors from a global bank of templates.  Restricting the number of templates yields a flexible hybridization of traditional CNNs and recurrent networks.  Compared to traditional CNNs, we demonstrate substantial parameter savings on standard image classification tasks, while maintaining accuracy.\nOur simple parameter sharing scheme, though defined via soft weights, in practice often yields trained networks with near strict recurrent structure; with negligible side effects, they convert into networks with actual loops. Training these networks thus implicitly involves discovery of suitable recurrent architectures. Though considering only the aspect of recurrent links, our trained networks achieve accuracy competitive with those built using state-of-the-art neural architecture search (NAS) procedures.\nOur hybridization of recurrent and convolutional networks may also represent a beneficial architectural bias.  Specifically, on synthetic tasks which are algorithmic in nature, our hybrid networks both train faster and extrapolate better to test examples outside the span of the training set.", "keywords": ["deep learning", "architecture search", "computer vision"], "authorids": ["savarese@ttic.edu", "mmaire@uchicago.edu"], "authors": ["Pedro Savarese", "Michael Maire"], "TL;DR": "We propose a method that enables CNN folding to create recurrent connections", "pdf": "/pdf/c0ca6381a4b14e3fd45586e92ef0e243d5f1fd0e.pdf", "paperhash": "savarese|learning_implicitly_recurrent_cnns_through_parameter_sharing", "_bibtex": "@inproceedings{\nsavarese2018learning,\ntitle={Learning Implicitly Recurrent {CNN}s Through Parameter Sharing},\nauthor={Pedro Savarese and Michael Maire},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJgYxn09Fm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1098/Official_Review", "cdate": 1542234306967, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "rJgYxn09Fm", "replyto": "rJgYxn09Fm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1098/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335872063, "tmdate": 1552335872063, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1098/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "Hke2lCb92Q", "original": null, "number": 2, "cdate": 1541180916310, "ddate": null, "tcdate": 1541180916310, "tmdate": 1541533424919, "tddate": null, "forum": "rJgYxn09Fm", "replyto": "rJgYxn09Fm", "invitation": "ICLR.cc/2019/Conference/-/Paper1098/Official_Review", "content": {"title": "a promising proposal that exploits the over-parameterization nature of neural nets to reduce the model size", "review": "This work is motivated by the widely recognized issue of over-parameterization in modern neural nets, and proposes a clever template sharing design to reduce the model size. The design is sound, and the experiments are valid and thorough. The writing is clear and fluent. \n\nThe reviewer is not entirely sure of the originality of this work. According to the sparse 'related work' section, the contribution is novel, but I will leave it to the consensus of others who are more versed in this regard.\n\nThe part that I find most interesting is the fact that template sharing helps with the optimization without even reducing the number of parameters, as illustrated in CIFAR from Table 1. The trade-off of accuracy and parameter-efficiency is overall well-studied in CIFAR and ImageNet, although results on ImageNet is not as impressive. \n\nRegarding the coefficient alpha, I'm not sure how cosine similarity is computed. I have the impression that each layer has its own alpha, which is a scalar. How is cosine similarity computed on scalars?\n\nIn the experiments, there's no mentioning of the regularization terms for alpha, which makes me think it is perhaps not important? What is the generic setup?\n\nIn summary, I find this work interesting, and with sufficient experiments to backup its claim. On the other hand, I'm not entirely sure of its novelty/originality, leaving this part open to others.", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1098/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Implicitly Recurrent CNNs Through Parameter Sharing", "abstract": "We introduce a parameter sharing scheme, in which different layers of a convolutional neural network (CNN) are defined by a learned linear combination of parameter tensors from a global bank of templates.  Restricting the number of templates yields a flexible hybridization of traditional CNNs and recurrent networks.  Compared to traditional CNNs, we demonstrate substantial parameter savings on standard image classification tasks, while maintaining accuracy.\nOur simple parameter sharing scheme, though defined via soft weights, in practice often yields trained networks with near strict recurrent structure; with negligible side effects, they convert into networks with actual loops. Training these networks thus implicitly involves discovery of suitable recurrent architectures. Though considering only the aspect of recurrent links, our trained networks achieve accuracy competitive with those built using state-of-the-art neural architecture search (NAS) procedures.\nOur hybridization of recurrent and convolutional networks may also represent a beneficial architectural bias.  Specifically, on synthetic tasks which are algorithmic in nature, our hybrid networks both train faster and extrapolate better to test examples outside the span of the training set.", "keywords": ["deep learning", "architecture search", "computer vision"], "authorids": ["savarese@ttic.edu", "mmaire@uchicago.edu"], "authors": ["Pedro Savarese", "Michael Maire"], "TL;DR": "We propose a method that enables CNN folding to create recurrent connections", "pdf": "/pdf/c0ca6381a4b14e3fd45586e92ef0e243d5f1fd0e.pdf", "paperhash": "savarese|learning_implicitly_recurrent_cnns_through_parameter_sharing", "_bibtex": "@inproceedings{\nsavarese2018learning,\ntitle={Learning Implicitly Recurrent {CNN}s Through Parameter Sharing},\nauthor={Pedro Savarese and Michael Maire},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJgYxn09Fm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1098/Official_Review", "cdate": 1542234306967, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "rJgYxn09Fm", "replyto": "rJgYxn09Fm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1098/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335872063, "tmdate": 1552335872063, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1098/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "S1gMfAgqh7", "original": null, "number": 1, "cdate": 1541176841697, "ddate": null, "tcdate": 1541176841697, "tmdate": 1541533424705, "tddate": null, "forum": "rJgYxn09Fm", "replyto": "rJgYxn09Fm", "invitation": "ICLR.cc/2019/Conference/-/Paper1098/Official_Review", "content": {"title": "Interesting idea of using parameter sharing scheme to explore network structure; experiments can be stronger ", "review": "Authors propose a parameter sharing scheme by allowing parameters to be reused across layers. It further makes connection between traditional CNNs with RNNs by adding additional regularization and using hard sharing scheme.\n\nThe way of parameter sharing is similar to the filter prediction method proposed in Rebuff et al\u2019s work, where they model a convolutional layer\u2019s parameters as a linear combination of a bank of filters and use that to address difference among multiple domains.\n\nSylvestre-Alvise Rebuffi, Hakan Bilen, Andrea Vedaldi, Learning multiple visual domains with residual adapters, NIPS 2017.\n\nThe discussion on the connection between coefficients for different layers and a network\u2019s structure and visualization of layer similarity matrix is interesting. Additional regularization can further encourage a recurrent neural network to be learned. \n\nHowever, they only experiment with one or two templates and advantage on accuracy and model size  over other methods is not very clear.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1098/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Implicitly Recurrent CNNs Through Parameter Sharing", "abstract": "We introduce a parameter sharing scheme, in which different layers of a convolutional neural network (CNN) are defined by a learned linear combination of parameter tensors from a global bank of templates.  Restricting the number of templates yields a flexible hybridization of traditional CNNs and recurrent networks.  Compared to traditional CNNs, we demonstrate substantial parameter savings on standard image classification tasks, while maintaining accuracy.\nOur simple parameter sharing scheme, though defined via soft weights, in practice often yields trained networks with near strict recurrent structure; with negligible side effects, they convert into networks with actual loops. Training these networks thus implicitly involves discovery of suitable recurrent architectures. Though considering only the aspect of recurrent links, our trained networks achieve accuracy competitive with those built using state-of-the-art neural architecture search (NAS) procedures.\nOur hybridization of recurrent and convolutional networks may also represent a beneficial architectural bias.  Specifically, on synthetic tasks which are algorithmic in nature, our hybrid networks both train faster and extrapolate better to test examples outside the span of the training set.", "keywords": ["deep learning", "architecture search", "computer vision"], "authorids": ["savarese@ttic.edu", "mmaire@uchicago.edu"], "authors": ["Pedro Savarese", "Michael Maire"], "TL;DR": "We propose a method that enables CNN folding to create recurrent connections", "pdf": "/pdf/c0ca6381a4b14e3fd45586e92ef0e243d5f1fd0e.pdf", "paperhash": "savarese|learning_implicitly_recurrent_cnns_through_parameter_sharing", "_bibtex": "@inproceedings{\nsavarese2018learning,\ntitle={Learning Implicitly Recurrent {CNN}s Through Parameter Sharing},\nauthor={Pedro Savarese and Michael Maire},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJgYxn09Fm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1098/Official_Review", "cdate": 1542234306967, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "rJgYxn09Fm", "replyto": "rJgYxn09Fm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1098/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335872063, "tmdate": 1552335872063, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1098/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 12}