{"notes": [{"id": "412_KkkGjJ4", "original": "RxyN5i8_D6L", "number": 1478, "cdate": 1601308164536, "ddate": null, "tcdate": 1601308164536, "tmdate": 1614985719218, "tddate": null, "forum": "412_KkkGjJ4", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Weakly Supervised Scene Graph Grounding", "authorids": ["~Yizhou_Zhang3", "~Zhaoheng_Zheng1", "~Yan_Liu1"], "authors": ["Yizhou Zhang", "Zhaoheng Zheng", "Yan Liu"], "keywords": ["Weakly Supervised Learning", "Scene Graph Grounding", "Visual Relation", "Computer Vision"], "abstract": " Recent researches have achieved substantial advances in learning structured representations from images. However, current methods rely heavily on the annotated mapping between the nodes of scene graphs and object bounding boxes inside images. Here, we explore the problem of learning the mapping between scene graph nodes and visual objects under weak supervision. Our proposed method learns a metric among visual objects and scene graph nodes by incorporating information from both object features and relational features. Extensive experiments on Visual Genome (VG) and Visual Relation Detection (VRD) datasets verify that our model post an improvement on scene graph grounding task over current state-of-the-art approaches. Further experiments on scene graph parsing task verify the grounding found by our model can reinforce the performance of the existing method. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|weakly_supervised_scene_graph_grounding", "one-sentence_summary": "We propose the task of weakly supervised scene graph grounding and provide a state-of-the-art solution.", "pdf": "/pdf/2d8af16be0d561548efc890051ec2794d15602a6.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=xZ82dD_7Ys", "_bibtex": "@misc{\nzhang2021weakly,\ntitle={Weakly Supervised Scene Graph Grounding},\nauthor={Yizhou Zhang and Zhaoheng Zheng and Yan Liu},\nyear={2021},\nurl={https://openreview.net/forum?id=412_KkkGjJ4}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "LHrdykpZ9_q", "original": null, "number": 1, "cdate": 1610040422712, "ddate": null, "tcdate": 1610040422712, "tmdate": 1610474021595, "tddate": null, "forum": "412_KkkGjJ4", "replyto": "412_KkkGjJ4", "invitation": "ICLR.cc/2021/Conference/Paper1478/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "This paper presents work on scene graph grounding under weak supervision.  The reviewers appreciated the consideration of this task and formulation of a solution for it.  However, concerns were raised over the importance of this weakly-supervised grounding task, how it addresses challenges in previous methods, the empirical evaluation, insights obtained, motivation, and clarity of exposition.  After reading the authors' response, the subsequent discussion and reconsideration resulted in a sense that while the task is new, the overall contribution and remaining questions over empirical evaluation mean the paper is not yet ready for publication at ICLR."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Weakly Supervised Scene Graph Grounding", "authorids": ["~Yizhou_Zhang3", "~Zhaoheng_Zheng1", "~Yan_Liu1"], "authors": ["Yizhou Zhang", "Zhaoheng Zheng", "Yan Liu"], "keywords": ["Weakly Supervised Learning", "Scene Graph Grounding", "Visual Relation", "Computer Vision"], "abstract": " Recent researches have achieved substantial advances in learning structured representations from images. However, current methods rely heavily on the annotated mapping between the nodes of scene graphs and object bounding boxes inside images. Here, we explore the problem of learning the mapping between scene graph nodes and visual objects under weak supervision. Our proposed method learns a metric among visual objects and scene graph nodes by incorporating information from both object features and relational features. Extensive experiments on Visual Genome (VG) and Visual Relation Detection (VRD) datasets verify that our model post an improvement on scene graph grounding task over current state-of-the-art approaches. Further experiments on scene graph parsing task verify the grounding found by our model can reinforce the performance of the existing method. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|weakly_supervised_scene_graph_grounding", "one-sentence_summary": "We propose the task of weakly supervised scene graph grounding and provide a state-of-the-art solution.", "pdf": "/pdf/2d8af16be0d561548efc890051ec2794d15602a6.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=xZ82dD_7Ys", "_bibtex": "@misc{\nzhang2021weakly,\ntitle={Weakly Supervised Scene Graph Grounding},\nauthor={Yizhou Zhang and Zhaoheng Zheng and Yan Liu},\nyear={2021},\nurl={https://openreview.net/forum?id=412_KkkGjJ4}\n}"}, "tags": [], "invitation": {"reply": {"forum": "412_KkkGjJ4", "replyto": "412_KkkGjJ4", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040422697, "tmdate": 1610474021579, "id": "ICLR.cc/2021/Conference/Paper1478/-/Decision"}}}, {"id": "H6N0GOpP2z-", "original": null, "number": 3, "cdate": 1603748840045, "ddate": null, "tcdate": 1603748840045, "tmdate": 1606754389817, "tddate": null, "forum": "412_KkkGjJ4", "replyto": "412_KkkGjJ4", "invitation": "ICLR.cc/2021/Conference/Paper1478/-/Official_Review", "content": {"title": "New problem, interesting findings, valuable insights.", "review": "This paper introduces a new problem called weakly supervised scene graph grounding, which is potentially very useful but barely studied in the literature. Given a dataset of images labeled with scene graphs, but without bounding box annotation, the task is to learn to align each node of a given scene graph to the corresponding object in the image. This is useful for weakly supervised training of scene graph generators, as well as other applications such as multimodal grounding. The authors clearly define the problem and propose an evaluation setting for it. They also propose a new method for this task and compare to existing baselines. Their findings are interesting, novel, and potentially influential for future work.\n\nNevertheless, there are some confusions/weaknesses that can be improved in the rebuttal:\n\n1. There is insufficient insight about the proposed method and whether/why it works. Particularly, the authors first train a weakly supervised object recognition model using a multiple instance learning framework (Eq. 3), then use that to align ground truth nodes to objects (Eq. 9), and then use this alignment as a ground truth for training the graph neural networks (Eq. 10). However, the ground truth alignment produced by the object recognition model only takes into account object classes and not their relations, while the goal of the graph neural networks is to incorporate relations to disambiguate between instance of the same class. Hence, the ground truth is inadequate to learn the task and the model may just learn to ignore relations and use objects alone to do the alignment. Experiment results (table 2) confirm this concern to some extent, as the ablation without relational metric (no graph neural net) achieves very close performance to the proposed model. Therefore, it is essential for the authors to justify their proposed model e.g. using extra experiments/analyses/visualizations, to prove that the graph neural networks indeed does learn to take relations into account for grounding objects.\n\n2. The writing of the paper can be improved. It is not very clear, except from Algorithm 2, that the training framework has 2 phases: training the object recognition model, and then training the graph comparison model based on that. The authors may add an overview to explain this. There are also notation issues and confusions in the equations. For instance, the definition of U, N, and Y below Eq 3 is confusing. Also it is not clear what is the input set to \\sigma (pooling). The definition of \\pi in Eq. 8 is also not precise enough. And in Eq. 10, the second equation is confusing, maybe the \\lambda inside the sum should be removed. Furthermore, Section 6 should be elaborated more. For instance, it should be made clear that this makes the training a 3-step process: learning object recognition, then learning the graph metric, and then using that to learn the scene graph generation model. \n\n3. Experiments provide new findings and useful insights, but they can be improved. For instance, it is not clear why, even though the proposed alignment method is far better than the VSPNet baseline in table 1, its effect on the scene graph generation performance is small (table 3). Moreover, the SGCls and PredCls evaluation metrics are both unrealistic metrics that assume ground truth bounding boxes are available during inference. The authors are encouraged to report SGDet, to show how effective the proposed method is in real-world settings.\n\nDespite the suggested improvements in experimentation, writing, and justification of the method, this paper carries a significant value for the community, as it calls attentions to a new task, that is barely studied, but has potential applications. The empirical findings of the paper provide new insights that do not exist in the literature, and can directly guide future work. Hence, my recommendation is to accept this paper. \n\nThe authors are strongly encouraged to address the concerns mentioned above. I also recommend resolving grammar mistakes throughout the paper, although that should not affect the final decision. Here are some more suggestions to improve the paper:\n\n1. The authors are encouraged to more extensively justify why this task is important and difficult. They did provide an example of a person riding a horse while another person is riding a bike, but that single example may not be convincing enough for others to pursue further research in this direction. The authors may provide some statistical analysis of how many difficult grounding cases exist in the dataset, what makes these cases difficult, and how improving the grounding can improve downstream tasks such as scene graph generation.\n\n2. There are some recent papers on visual grounding by incorporating relationships between objects, such as [1]. I recommend including a comprehensive review of such papers and discussing their connections and differences with this work.\n\n[1] He, Chuanzi, Haidong Zhu, Jiyang Gao, Kan Chen, and Ram Nevatia. \"CPARR: Category-based Proposal Analysis for Referring Relationships.\" In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pp. 948-949. 2020.\n\n\n######## Post-Rebuttal Updates:\n\nThe authors have addressed most of my concerns, and I appreciate their effort. I am not convinced that early-stopping alone can help the model learn to utilize relation information, when the target ground truth does not require the model to do so. However, I recommend accepting this paper as it introduces a new task and presents strong results. \n\n", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper1478/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1478/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Weakly Supervised Scene Graph Grounding", "authorids": ["~Yizhou_Zhang3", "~Zhaoheng_Zheng1", "~Yan_Liu1"], "authors": ["Yizhou Zhang", "Zhaoheng Zheng", "Yan Liu"], "keywords": ["Weakly Supervised Learning", "Scene Graph Grounding", "Visual Relation", "Computer Vision"], "abstract": " Recent researches have achieved substantial advances in learning structured representations from images. However, current methods rely heavily on the annotated mapping between the nodes of scene graphs and object bounding boxes inside images. Here, we explore the problem of learning the mapping between scene graph nodes and visual objects under weak supervision. Our proposed method learns a metric among visual objects and scene graph nodes by incorporating information from both object features and relational features. Extensive experiments on Visual Genome (VG) and Visual Relation Detection (VRD) datasets verify that our model post an improvement on scene graph grounding task over current state-of-the-art approaches. Further experiments on scene graph parsing task verify the grounding found by our model can reinforce the performance of the existing method. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|weakly_supervised_scene_graph_grounding", "one-sentence_summary": "We propose the task of weakly supervised scene graph grounding and provide a state-of-the-art solution.", "pdf": "/pdf/2d8af16be0d561548efc890051ec2794d15602a6.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=xZ82dD_7Ys", "_bibtex": "@misc{\nzhang2021weakly,\ntitle={Weakly Supervised Scene Graph Grounding},\nauthor={Yizhou Zhang and Zhaoheng Zheng and Yan Liu},\nyear={2021},\nurl={https://openreview.net/forum?id=412_KkkGjJ4}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "412_KkkGjJ4", "replyto": "412_KkkGjJ4", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1478/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538117720, "tmdate": 1606915778877, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1478/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1478/-/Official_Review"}}}, {"id": "8yluW74i4gB", "original": null, "number": 3, "cdate": 1606172828270, "ddate": null, "tcdate": 1606172828270, "tmdate": 1606180572790, "tddate": null, "forum": "412_KkkGjJ4", "replyto": "H6N0GOpP2z-", "invitation": "ICLR.cc/2021/Conference/Paper1478/-/Official_Comment", "content": {"title": "Response for AnonReviewer4", "comment": "Thank you for the questions and insightful suggestions! For the suggestions about writing, we have addressed them and updated the draft. Below are our responses to other questions and concerns about the proposed method and experiment results:\n> \u201cHowever, the ground truth alignment produced by the object recognition model only takes into account object classes and not their relations, while the goal of the graph neural networks is to incorporate relations to disambiguate between instances of the same class. Hence, the ground truth is inadequate to learn the task and the model may just learn to ignore relations and use objects alone to do the alignment.\u201d\n\n Thank you for your questions. Intuitively, if the GNN has gotten overfitted on the output from object-class based metric, it will ignore all relations and only apply the object. To avoid overfitting, we apply early-stopping via a small validation set. We conduct an experiment that shows our model can use relational information to distinguish the nodes with the same object class. The new results are included in Table 3 in the revised version of the paper. The reason why \u201cthe ablation without relational metric (no graph neural net) achieves very close performance to the proposed model\u201d is that relational information is crucial for a fraction of the nodes rather than all of them. For the nodes without ambiguous nodes (nodes of the same object class), object class information is adequate for the grounding. From Table 3, we can see that the relational metric significantly increases the grounding accuracy of the ambiguous nodes with relations (by 5%). Also, we visualized some cases in Figure 6 of the updated draft, showing that the ambiguous nodes can be distinguished by our model.\n\n\n\n\n \n>Experiments provide new findings and useful insights, but they can be improved. For instance, it is not clear why, even though the proposed alignment method is far better than the VSPNet baseline in Table 1, its effect on the scene graph generation performance is small (table 3). Moreover, the SGCls and PredCls evaluation metrics are both unrealistic metrics that assume ground truth bounding boxes are available during inference. The authors are encouraged to report SGDet, to show how effective the proposed method is in real-world settings.\n \nThe improvement of our grounding methods on scene graph generation for VSPNet is small because the scene graph parsing is a challenging task itself even with ground truth labels. For instance, the Recall@50 of VSPNet FS (in Table 3 of the original paper, Table 4 in the revised version) only lead the VSPNet WS with 1 percent, while the VSPNet FS is trained with the ground truth grounding (in other words, with 100% accuracy). Meanwhile, our grounding model, although significantly outperforming the baselines, is with 50.8% accuracy (less than 10 percent leading). We can also note that on other metrics, like PredCLS, our grounding brings more significant improvement. As for the SGDet metric, we really appreciate your suggestion and we have uploaded the results in the Table 5 rebuttal version of the draft. Our grounding model also improves the performance of SGDet (we use the name of \"SGGen\" to keep consistent with VSPNet).\n\n>The authors are encouraged to more extensively justify why this task is important and difficult. They did provide an example of a person riding a horse while another person is riding a bike, but that single example may not be convincing enough for others to pursue further research in this direction. The authors may provide some statistical analysis of how many difficult grounding cases exist in the dataset, what makes these cases difficult, and how improving the grounding can improve downstream tasks such as scene graph generation.\n\nThank you for your suggestions! We have included dataset analysis of the ambiguous nodes difficult for (grounding) in Table 3 of the updated draft. \n\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1478/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1478/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Weakly Supervised Scene Graph Grounding", "authorids": ["~Yizhou_Zhang3", "~Zhaoheng_Zheng1", "~Yan_Liu1"], "authors": ["Yizhou Zhang", "Zhaoheng Zheng", "Yan Liu"], "keywords": ["Weakly Supervised Learning", "Scene Graph Grounding", "Visual Relation", "Computer Vision"], "abstract": " Recent researches have achieved substantial advances in learning structured representations from images. However, current methods rely heavily on the annotated mapping between the nodes of scene graphs and object bounding boxes inside images. Here, we explore the problem of learning the mapping between scene graph nodes and visual objects under weak supervision. Our proposed method learns a metric among visual objects and scene graph nodes by incorporating information from both object features and relational features. Extensive experiments on Visual Genome (VG) and Visual Relation Detection (VRD) datasets verify that our model post an improvement on scene graph grounding task over current state-of-the-art approaches. Further experiments on scene graph parsing task verify the grounding found by our model can reinforce the performance of the existing method. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|weakly_supervised_scene_graph_grounding", "one-sentence_summary": "We propose the task of weakly supervised scene graph grounding and provide a state-of-the-art solution.", "pdf": "/pdf/2d8af16be0d561548efc890051ec2794d15602a6.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=xZ82dD_7Ys", "_bibtex": "@misc{\nzhang2021weakly,\ntitle={Weakly Supervised Scene Graph Grounding},\nauthor={Yizhou Zhang and Zhaoheng Zheng and Yan Liu},\nyear={2021},\nurl={https://openreview.net/forum?id=412_KkkGjJ4}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "412_KkkGjJ4", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1478/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1478/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1478/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1478/Authors|ICLR.cc/2021/Conference/Paper1478/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1478/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923859244, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1478/-/Official_Comment"}}}, {"id": "LztEfoix3jL", "original": null, "number": 5, "cdate": 1606173822379, "ddate": null, "tcdate": 1606173822379, "tmdate": 1606173822379, "tddate": null, "forum": "412_KkkGjJ4", "replyto": "lb5LUZd4Rq-", "invitation": "ICLR.cc/2021/Conference/Paper1478/-/Official_Comment", "content": {"title": "Responses for AnonReviewer3", "comment": "Thank you for your insightful review. Below are our response to your questions regarding the experiments setting:\n\n> The novelty of the idea for weakly-supervised scene graph grounding is limited. It regards this grounding task as a bipartite graph matching problem, which is very similar to the existing work for weakly-supervised scene graph parsing (ie, VSPNet).\n\nOne key difference between our model and VSPNet is that while VSPNet computes matches over two generated graphs, we further process those graphs into embedding vectors. Furthermore, VSPNet does not consider weakly scene graph grounding separately, while its design limits the ability to learn an individual grounding system. Yet our experiments show that, with our proposed explicit scene graph grounding system, we can further achieve better grounding results as well as parsing results. \n\n> Although this paper is the first work to explore weakly-supervised scene graph grounding tasks, the task is very similar to the existing weakly-supervised phrase grounding task.\n\nThough our proposed task is close to phrase grounding, the inputs of those two tasks are different. In our setting, we expect the input scene graph to contain objects and pairwise relations, while phrase grounding needs a detailed description for example, \u201c a man in red\u201d. In phrase grounding, the number of relationships associated with one noun is usually not large. For example, a phrase may only mention \u201ca man in house\u201d. But in scene graphs, one noun may be connected to many other nouns. It is hard and unnatural to adapt existing weakly supervised grounding models like GroundR on this task. Thus, we need a model that can aggregate multiple relationships.\n\n> The comparisons between the proposed method and baselines are not fair. For example, in Table 1, the baseline WS-VRD [Baldassarre et al., 2020] model, it only utilizes predicates as input for weakly-supervised learning, while the proposed model uses the whole scene graph as input.\n \nThe design of the WS-VRD model does not support taking the whole graph as input. That\u2019s one of the reasons why we consider designing a grounding model with the ability to read a whole graph as input since it carries more information. This is also a reason why we also include VSPNet as a baseline. VSPNet can take the whole graph as input in the graph alignment process. Our leading performance compared with both VSPNet and WS-VRD indicates that (1) enabling the model to read the whole graph as input is necessary and (2) our model works better in grounding with the whole graph input.\n \n> The details of some experiments are not clear. For example, for the compared baseline VSPNet [Zareian et al., 2020a], the authors use two variants of models (VSPNet-v1 and VSPNet-v2), but the details of these two variants are not clear, ie, how to change a scene graph parsing model (VSP) for the scene graph grounding task.\n \n\n\nVSPNet has a graph alignment module that solves graph matching in an iterative manner. In each iteration, the module can alternatively select starting from mapping edges or mapping nodes. We denote the mode starting from mapping edges as VSPNet-v1 and the one starting from mapping nodes as VSPNet-v2. This can be found in the Baseline paragraph in the Experiment section. As for the detailed process of how to apply VSPNet on scene graph grounding, we discussed the details in the fourth paragraph of the Introduction Sections. Although VSPNet is a model for VSP, it has a grounding module that can map the nodes to visual regions."}, "signatures": ["ICLR.cc/2021/Conference/Paper1478/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1478/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Weakly Supervised Scene Graph Grounding", "authorids": ["~Yizhou_Zhang3", "~Zhaoheng_Zheng1", "~Yan_Liu1"], "authors": ["Yizhou Zhang", "Zhaoheng Zheng", "Yan Liu"], "keywords": ["Weakly Supervised Learning", "Scene Graph Grounding", "Visual Relation", "Computer Vision"], "abstract": " Recent researches have achieved substantial advances in learning structured representations from images. However, current methods rely heavily on the annotated mapping between the nodes of scene graphs and object bounding boxes inside images. Here, we explore the problem of learning the mapping between scene graph nodes and visual objects under weak supervision. Our proposed method learns a metric among visual objects and scene graph nodes by incorporating information from both object features and relational features. Extensive experiments on Visual Genome (VG) and Visual Relation Detection (VRD) datasets verify that our model post an improvement on scene graph grounding task over current state-of-the-art approaches. Further experiments on scene graph parsing task verify the grounding found by our model can reinforce the performance of the existing method. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|weakly_supervised_scene_graph_grounding", "one-sentence_summary": "We propose the task of weakly supervised scene graph grounding and provide a state-of-the-art solution.", "pdf": "/pdf/2d8af16be0d561548efc890051ec2794d15602a6.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=xZ82dD_7Ys", "_bibtex": "@misc{\nzhang2021weakly,\ntitle={Weakly Supervised Scene Graph Grounding},\nauthor={Yizhou Zhang and Zhaoheng Zheng and Yan Liu},\nyear={2021},\nurl={https://openreview.net/forum?id=412_KkkGjJ4}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "412_KkkGjJ4", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1478/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1478/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1478/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1478/Authors|ICLR.cc/2021/Conference/Paper1478/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1478/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923859244, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1478/-/Official_Comment"}}}, {"id": "767kUZdtu5s", "original": null, "number": 4, "cdate": 1606173352857, "ddate": null, "tcdate": 1606173352857, "tmdate": 1606173531421, "tddate": null, "forum": "412_KkkGjJ4", "replyto": "HCEFU9T2JQ7", "invitation": "ICLR.cc/2021/Conference/Paper1478/-/Official_Comment", "content": {"title": "Response for AnonReviewer1", "comment": "> The motivation is not clearly justified in the Introduction. For example, for readers not familiar with this task, the authors may want to provide a clear illustrative example of this task, to show why scene graph grounding is challenging and significant. Moreover, compared to (Zareian et al., 2020b), whom the authors are targeting, in fact, I see no obvious weakness in their method and how you resolve it.\n\nVSPNet aims at weakly supervised scene graph parsing and does not consider weakly supervised scene graph grounding as an individual task. Yet in our paper, we argue that weakly supervised scene graph grounding is worth being considered independently. We show that, by grounding scene graph nodes prior to parsing, the performance on scene graph parsing can be improved, regardless of whether proposals come from RPN or ground truth.  \n\n> I am not objecting to use the simple EMD to solve weakly supervised matching problems, as it is straightforward. However, I feel that the authors are just using the right tool to do the right job, without convincing the readers why this tool is sufficient and necessary, especially compared to many other possible tools, such as using GNN, (Zareian et al., 2020b), MIL (Zhang et al. 2017b), and graphical models (Justin et al, Image Retrieval with Scene Graphs, CVPR'15).\n\nTo our knowledge, GNN is more practical than Graphical Model because it has much more trainable parameters. Besides, MIL only focuses on relations between adjacent nodes, or in other words, local relations. So it misses the overview of the whole network. That is why we adopt GCN and embedding distance matching as our approach. Actually, in the experiment sections, we discussed the performance comparison between the MIL-based model and our model on weakly-supervised scene graph parsing in Section 6 (Table 4 and 5). We did not apply the implementation in Zhang et al. 2017b because their model does not support external proposals. As for graphical models, the authors did not release the code.\n\n> The overall pipeline is not clearly illustrated. For example, after the EDM matching, how to obtain the final scene graph alignment?\n\nAfter the EDM matching, each scene graph node is associated with one proposal of the highest similarity score. And thus the correspondence between graph nodes and object proposals has already been established. \n\n>Experiments are not comprehensive. First, I'd like to see visual qualitative examples of the success and failures of grounding, especially for improving weakly supervised SGG. Second, ablations such as using different metrics for the matrices are missing.\n\nWe have visualized the success and failure grounding in the updated Appendix (Section E). The successful cases, together with Table 3 in the revised draft, verify that our model can learn to distinguish nodes of the same object class based on relational information. The failure cases also show some limitations of only considering object class and relations. Sometimes the model may simultaneously wrongly ground both the subject and object of a triplet. In this situation, it is hard for the model to correct the grounding with relational information. For example, in the second failure case, the model wrongly ground the \u201cplate\u201d and the \u201cvegetable\u201d on it. Thus, in future work, we may consider introducing attribute information. As for the discussion on the two different types of metrics (object-based and relational metric), we included them in Table 2 and Table 3 in the revised draft. As we can see, the object-based metric is not adequate to distinguish the nodes of the same class. By adding the relational metric, we can distinguish them better. A detailed discussion can be found in Sec 5.3. As for the different metrics, we show the results of different types of metrics (with or without relational information) in Table 2, and our discussion can be found in Sec 5.3. Our designing of the relational metric significantly improves the grounding accuracy, especially on the nodes with ambiguous nodes (nodes with the same object class) in the same graph.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1478/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1478/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Weakly Supervised Scene Graph Grounding", "authorids": ["~Yizhou_Zhang3", "~Zhaoheng_Zheng1", "~Yan_Liu1"], "authors": ["Yizhou Zhang", "Zhaoheng Zheng", "Yan Liu"], "keywords": ["Weakly Supervised Learning", "Scene Graph Grounding", "Visual Relation", "Computer Vision"], "abstract": " Recent researches have achieved substantial advances in learning structured representations from images. However, current methods rely heavily on the annotated mapping between the nodes of scene graphs and object bounding boxes inside images. Here, we explore the problem of learning the mapping between scene graph nodes and visual objects under weak supervision. Our proposed method learns a metric among visual objects and scene graph nodes by incorporating information from both object features and relational features. Extensive experiments on Visual Genome (VG) and Visual Relation Detection (VRD) datasets verify that our model post an improvement on scene graph grounding task over current state-of-the-art approaches. Further experiments on scene graph parsing task verify the grounding found by our model can reinforce the performance of the existing method. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|weakly_supervised_scene_graph_grounding", "one-sentence_summary": "We propose the task of weakly supervised scene graph grounding and provide a state-of-the-art solution.", "pdf": "/pdf/2d8af16be0d561548efc890051ec2794d15602a6.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=xZ82dD_7Ys", "_bibtex": "@misc{\nzhang2021weakly,\ntitle={Weakly Supervised Scene Graph Grounding},\nauthor={Yizhou Zhang and Zhaoheng Zheng and Yan Liu},\nyear={2021},\nurl={https://openreview.net/forum?id=412_KkkGjJ4}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "412_KkkGjJ4", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1478/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1478/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1478/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1478/Authors|ICLR.cc/2021/Conference/Paper1478/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1478/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923859244, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1478/-/Official_Comment"}}}, {"id": "P83sdrFKbnw", "original": null, "number": 2, "cdate": 1606172218425, "ddate": null, "tcdate": 1606172218425, "tmdate": 1606172872622, "tddate": null, "forum": "412_KkkGjJ4", "replyto": "4wPdxiv2KV", "invitation": "ICLR.cc/2021/Conference/Paper1478/-/Official_Comment", "content": {"title": "Response for AnonReviewer2", "comment": "Thank you for your suggestions on improving the experiment section and the writing of this paper. We have updated the draft and fixed the typos. Belows are our responses to your questions on the experiments setting:\nCons:\n> What's the design of pool-based multiple instance learning? Can you provide more details?\n\nWe provided a more detailed description in the appendix of the revised draft. It is in the A section of the Appendix.\n \n> In fig. 3, the authors mentioned that the boxes are ground-truth, while they also mentioned that the visual objects are extracted by off-the-shelf Faster-RCNN. \n\nIn our framework, the object features are extracted via an off-the-shelf Faster-RCNN, while the object bounding boxes can be either ground truth or proposals generated by off-the-shelf Faster-RCNN. We reported the numerical grounding results of both settings in the experiment section and provided the visualization of the ground truth box setting.\n \n> For VG and VRD, in my opinion, the objects have been semantically aligned with scene graph nodes since the objects are detected by the pre-trained detector (OpenImage). Why not use a scene graph generated from captions as the input.\n\nThe object detector is pre-trained on OpenImage dataset, whose label space is different from the VG and VRD dataset. This is the reason why we need to learn the object class based metric to align the object category semantic\n \n> Why the \u03bb for VG and VRD is different, how to get those values \n\nWe conduct hyper-parameter search on the validation split. From Table 1 and 2 we can see that VRD is a relatively \u201csimpler\u201d dataset (most of the baselines and variant models achieve higher scores on VRD dataset) and the object-class based metric has already achieved a high performance (66.7). Thus, the relationship based metric is not so important and can be assigned with a smaller weight.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1478/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1478/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Weakly Supervised Scene Graph Grounding", "authorids": ["~Yizhou_Zhang3", "~Zhaoheng_Zheng1", "~Yan_Liu1"], "authors": ["Yizhou Zhang", "Zhaoheng Zheng", "Yan Liu"], "keywords": ["Weakly Supervised Learning", "Scene Graph Grounding", "Visual Relation", "Computer Vision"], "abstract": " Recent researches have achieved substantial advances in learning structured representations from images. However, current methods rely heavily on the annotated mapping between the nodes of scene graphs and object bounding boxes inside images. Here, we explore the problem of learning the mapping between scene graph nodes and visual objects under weak supervision. Our proposed method learns a metric among visual objects and scene graph nodes by incorporating information from both object features and relational features. Extensive experiments on Visual Genome (VG) and Visual Relation Detection (VRD) datasets verify that our model post an improvement on scene graph grounding task over current state-of-the-art approaches. Further experiments on scene graph parsing task verify the grounding found by our model can reinforce the performance of the existing method. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|weakly_supervised_scene_graph_grounding", "one-sentence_summary": "We propose the task of weakly supervised scene graph grounding and provide a state-of-the-art solution.", "pdf": "/pdf/2d8af16be0d561548efc890051ec2794d15602a6.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=xZ82dD_7Ys", "_bibtex": "@misc{\nzhang2021weakly,\ntitle={Weakly Supervised Scene Graph Grounding},\nauthor={Yizhou Zhang and Zhaoheng Zheng and Yan Liu},\nyear={2021},\nurl={https://openreview.net/forum?id=412_KkkGjJ4}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "412_KkkGjJ4", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1478/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1478/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1478/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1478/Authors|ICLR.cc/2021/Conference/Paper1478/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1478/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923859244, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1478/-/Official_Comment"}}}, {"id": "4wPdxiv2KV", "original": null, "number": 4, "cdate": 1603863377773, "ddate": null, "tcdate": 1603863377773, "tmdate": 1605034780514, "tddate": null, "forum": "412_KkkGjJ4", "replyto": "412_KkkGjJ4", "invitation": "ICLR.cc/2021/Conference/Paper1478/-/Official_Review", "content": {"title": "Problem is new, but the experiment is weak", "review": "**Summary**:\nThis paper proposes a weakly supervised scene graph grounding method. The setting of this paper is new and interesting, and I think this task would be beneficial to downstream tasks. The main contribution of this method is the mapping between visual objects and nodes. To bridge the gap between them, the authors propose two metrics and learn their model on a weakly-supervised dataset. The ablation study shows the effectiveness of each contribution, and they further apply the method to weakly supervised scene graph parsing.\n\n**Pros**:\n- This paper proposes the weakly supervised scene graph grounding task and formulates the mapping between scene graph and objects as a minimum match problem on a bipartite graph instead of graph alignment. \n- They train their model with two metrics (object-class based and relation-based) and search the minimum match in an iterative manner.\n- The visualization shows that nodes and objects can be well aligned.\n\n**Cons**:\n- What's the design of pool-based multiple instance learning? Can you provide more details?\n- The updating algorithm of the scene graph is not new; it is close to the graph encoding process; it updates the representations according to its neighbor. \n- In fig. 3, the authors mentioned that the boxes are ground-truth, while they also mentioned that the visual objects are extracted by off-the-shelf Faster-RCNN.\n- For VG and VRD, in my opinion, the objects have been semantically aligned with scene graph nodes since the objects are detected by the pre-trained detector (OpenImage). Why not use a scene graph generated from captions as the input.\n- Why the $\\lambda$ for VG and VRD is different, how to get those values?\n- As for visualization, the authors only show the scene graphs with the same objects. They should also show the images with objects, not in the scene graph.\n- The writing needs to be improved; for example, Eq. 11 should be exp(-d(u,v)).\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1478/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1478/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Weakly Supervised Scene Graph Grounding", "authorids": ["~Yizhou_Zhang3", "~Zhaoheng_Zheng1", "~Yan_Liu1"], "authors": ["Yizhou Zhang", "Zhaoheng Zheng", "Yan Liu"], "keywords": ["Weakly Supervised Learning", "Scene Graph Grounding", "Visual Relation", "Computer Vision"], "abstract": " Recent researches have achieved substantial advances in learning structured representations from images. However, current methods rely heavily on the annotated mapping between the nodes of scene graphs and object bounding boxes inside images. Here, we explore the problem of learning the mapping between scene graph nodes and visual objects under weak supervision. Our proposed method learns a metric among visual objects and scene graph nodes by incorporating information from both object features and relational features. Extensive experiments on Visual Genome (VG) and Visual Relation Detection (VRD) datasets verify that our model post an improvement on scene graph grounding task over current state-of-the-art approaches. Further experiments on scene graph parsing task verify the grounding found by our model can reinforce the performance of the existing method. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|weakly_supervised_scene_graph_grounding", "one-sentence_summary": "We propose the task of weakly supervised scene graph grounding and provide a state-of-the-art solution.", "pdf": "/pdf/2d8af16be0d561548efc890051ec2794d15602a6.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=xZ82dD_7Ys", "_bibtex": "@misc{\nzhang2021weakly,\ntitle={Weakly Supervised Scene Graph Grounding},\nauthor={Yizhou Zhang and Zhaoheng Zheng and Yan Liu},\nyear={2021},\nurl={https://openreview.net/forum?id=412_KkkGjJ4}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "412_KkkGjJ4", "replyto": "412_KkkGjJ4", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1478/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538117720, "tmdate": 1606915778877, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1478/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1478/-/Official_Review"}}}, {"id": "lb5LUZd4Rq-", "original": null, "number": 1, "cdate": 1602605892926, "ddate": null, "tcdate": 1602605892926, "tmdate": 1605024433496, "tddate": null, "forum": "412_KkkGjJ4", "replyto": "412_KkkGjJ4", "invitation": "ICLR.cc/2021/Conference/Paper1478/-/Official_Review", "content": {"title": "An interesting and crucial task, but the novelty of idea is limited.", "review": "###############################\nSummary:\nThis paper focuses on a new scene understanding task: Weakly-Supervised Scene Graph Grounding. Given the image and a paired scene graph, this task needs to link each scene graph node to an object in the image. Specifically, it regards this problem as a bipartite graph matching problem, and designs a cost function considering both object-class similarity and relation-based similarity. Experiments on two datasets (Visual Genome and Visual Relation Detection) have demonstrated the effectiveness.\n\n################################\nAdvantages:\n+ The paper is the first work to explore the task: Weakly-Supervised Scene Graph Grounding. This task can serve as a crucial preprocessing step for weakly-supervised scene graph generation: After solving this grounding task, the weakly-supervised scene graph generation task can be easily transferred to fully-supervised scene graph generation by regarding the grounding results as ground-truth annotations.\n\n+ The whole paper is well-written and all notations are quite clear.\n\n#################################\nWeaknesses:\n\n- The novelty of the idea for weakly-supervised scene graph grounding is limited. It regards this grounding task as a bipartite graph matching problem, which is very similar to the existing work for weakly-supervised scene graph parsing (ie, VSPNet). \n\n- Although this paper is the first work to explore weakly-supervised scene graph grounding task, the task is very similar to the existing weakly-supervised phrase grounding task.\n\n- The comparisons between the proposed method and baselines are not fair. For example, in Table 1, for the baseline WS-VRD [Baldassarre et al., 2020] model, it only utilizes predicates as input for weakly-supervised learning, while the proposed model uses the whole scene graph as input.\n\n- The details of some experiments are not clear. For example, for the compared baseline VSPNet [Zareian et al., 2020a], the authors use two variants of models (VSPNet-v1 and VSPNet-v2), but the details of these two variants are not clear, ie, how to change an scene graph parsing model (VSP) for the scene graph grounding task.\n", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper1478/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1478/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Weakly Supervised Scene Graph Grounding", "authorids": ["~Yizhou_Zhang3", "~Zhaoheng_Zheng1", "~Yan_Liu1"], "authors": ["Yizhou Zhang", "Zhaoheng Zheng", "Yan Liu"], "keywords": ["Weakly Supervised Learning", "Scene Graph Grounding", "Visual Relation", "Computer Vision"], "abstract": " Recent researches have achieved substantial advances in learning structured representations from images. However, current methods rely heavily on the annotated mapping between the nodes of scene graphs and object bounding boxes inside images. Here, we explore the problem of learning the mapping between scene graph nodes and visual objects under weak supervision. Our proposed method learns a metric among visual objects and scene graph nodes by incorporating information from both object features and relational features. Extensive experiments on Visual Genome (VG) and Visual Relation Detection (VRD) datasets verify that our model post an improvement on scene graph grounding task over current state-of-the-art approaches. Further experiments on scene graph parsing task verify the grounding found by our model can reinforce the performance of the existing method. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|weakly_supervised_scene_graph_grounding", "one-sentence_summary": "We propose the task of weakly supervised scene graph grounding and provide a state-of-the-art solution.", "pdf": "/pdf/2d8af16be0d561548efc890051ec2794d15602a6.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=xZ82dD_7Ys", "_bibtex": "@misc{\nzhang2021weakly,\ntitle={Weakly Supervised Scene Graph Grounding},\nauthor={Yizhou Zhang and Zhaoheng Zheng and Yan Liu},\nyear={2021},\nurl={https://openreview.net/forum?id=412_KkkGjJ4}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "412_KkkGjJ4", "replyto": "412_KkkGjJ4", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1478/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538117720, "tmdate": 1606915778877, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1478/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1478/-/Official_Review"}}}, {"id": "HCEFU9T2JQ7", "original": null, "number": 2, "cdate": 1603592392375, "ddate": null, "tcdate": 1603592392375, "tmdate": 1605024433431, "tddate": null, "forum": "412_KkkGjJ4", "replyto": "412_KkkGjJ4", "invitation": "ICLR.cc/2021/Conference/Paper1478/-/Official_Review", "content": {"title": "see comments", "review": "Summary:\nThis paper presents a method on the task of Scene Graph Grounding, where the object and (object, object) relationship ground-truth labels are only at the image-level but not localized, i.e., instance-level. The core of the proposed method is the Wasserstein distance minimizer (aka, Earth Mover's Distance) for two object pair sets: one is from visual detection, and the other is from the textual nouns. The initial cost for each set is initialized by MIL probabilities. Experiments demonstrate a considerable performance boost over a SOTA (Zareian et al., 2020b)\n\nStrength:\n1. Strong performance in Table 1 (I do not mean that the experiments are comprehensive, see weakness below)\n\n\nWeakness:\n1. The motivation is not clearly justified in the Introduction. For example, for readers not familiar with this task, the authors may want to provide a clear illustrative example of this task, to show why scene graph grounding is challenging and significant. Moreover, compared to (Zareian et al., 2020b), whom the authors are targeting, in fact, I see no obvious weakness in their method and how you resolve it. \n\n2. I am not objecting to use the simple EMD to solve weakly supervised matching problems, as it is straightforward. However, I feel that the authors are just using the right tool to do the right job, without convincing the readers why this tool is sufficient and necessary, especially compared to many other possible tools, such as using GNN, (Zareian et al., 2020b), MIL (Zhang et al. 2017b), and graphical models (Justin et al, Image Retrieval with Scene Graphs, CVPR'15). \n\n3. The overall pipeline is not clearly illustrated. For example, after the EDM matching, how to obtain the final scene graph alignment?\n\n4. Experiments are not comprehensive. First, I'd like to see visual qualitative examples of the success and failures of grounding, especially for improving weakly supervised SGG. Second, ablations such as using different metrics for the matrices are missing.", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper1478/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1478/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Weakly Supervised Scene Graph Grounding", "authorids": ["~Yizhou_Zhang3", "~Zhaoheng_Zheng1", "~Yan_Liu1"], "authors": ["Yizhou Zhang", "Zhaoheng Zheng", "Yan Liu"], "keywords": ["Weakly Supervised Learning", "Scene Graph Grounding", "Visual Relation", "Computer Vision"], "abstract": " Recent researches have achieved substantial advances in learning structured representations from images. However, current methods rely heavily on the annotated mapping between the nodes of scene graphs and object bounding boxes inside images. Here, we explore the problem of learning the mapping between scene graph nodes and visual objects under weak supervision. Our proposed method learns a metric among visual objects and scene graph nodes by incorporating information from both object features and relational features. Extensive experiments on Visual Genome (VG) and Visual Relation Detection (VRD) datasets verify that our model post an improvement on scene graph grounding task over current state-of-the-art approaches. Further experiments on scene graph parsing task verify the grounding found by our model can reinforce the performance of the existing method. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|weakly_supervised_scene_graph_grounding", "one-sentence_summary": "We propose the task of weakly supervised scene graph grounding and provide a state-of-the-art solution.", "pdf": "/pdf/2d8af16be0d561548efc890051ec2794d15602a6.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=xZ82dD_7Ys", "_bibtex": "@misc{\nzhang2021weakly,\ntitle={Weakly Supervised Scene Graph Grounding},\nauthor={Yizhou Zhang and Zhaoheng Zheng and Yan Liu},\nyear={2021},\nurl={https://openreview.net/forum?id=412_KkkGjJ4}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "412_KkkGjJ4", "replyto": "412_KkkGjJ4", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1478/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538117720, "tmdate": 1606915778877, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1478/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1478/-/Official_Review"}}}], "count": 10}