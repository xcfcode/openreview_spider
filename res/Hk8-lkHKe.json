{"notes": [{"tddate": null, "ddate": null, "cdate": null, "original": null, "tmdate": 1490028617944, "tcdate": 1490028617944, "number": 1, "id": "Hkf8utpix", "invitation": "ICLR.cc/2017/workshop/-/paper130/acceptance", "forum": "Hk8-lkHKe", "replyto": "Hk8-lkHKe", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Accept", "title": "ICLR committee final decision"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Changing Model Behavior at Test-time Using Reinforcement Learning", "abstract": "Machine learning models are often used at test-time subject to constraints and trade-offs not present\nat training-time. For example, a computer vision model operating on an embedded device may need to perform\nreal-time inference, or a translation model operating on a cell phone may wish to bound its average\ncompute time in order to be power-efficient. In this work we describe a mixture-of-experts model and show how to change its  test-time resource-usage on a per-input basis using reinforcement learning. We test our method on a small MNIST-based example.", "pdf": "/pdf/3a1058172431aad2e63d66386ac5defbda063fd8.pdf", "TL;DR": "We describe a method to change model behavior at test-time on a per-input basis using reinforcement learning in a mixture-of-experts framework.", "paperhash": "odena|changing_model_behavior_at_testtime_using_reinforcement_learning", "conflicts": ["google.com"], "keywords": ["Reinforcement Learning", "Deep learning"], "authors": ["Augustus Odena", "Dieterich Lawson", "Christopher Olah"], "authorids": ["augustusodena@google.com", "dieterichl@google.com", "colah@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1490028618493, "id": "ICLR.cc/2017/workshop/-/paper130/acceptance", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "Hk8-lkHKe", "replyto": "Hk8-lkHKe", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept", "Reject"]}}}, "nonreaders": [], "cdate": 1490028618493}}}, {"tddate": null, "tmdate": 1489606032869, "tcdate": 1489606032869, "number": 1, "id": "HkK5Hfwie", "invitation": "ICLR.cc/2017/workshop/-/paper130/public/comment", "forum": "Hk8-lkHKe", "replyto": "SJ_uVm1jx", "signatures": ["~Dieterich_Lawson1"], "readers": ["everyone"], "writers": ["~Dieterich_Lawson1"], "content": {"title": "Figure 2 helps answer this question.", "comment": "Thank you for your review!\n\nI think your question is answered by figure 2. You can interpret the left and right edges of the green curve as a situation where you are training for the worst case or best case, respectively. At the left end, the green curve represents always using the most resource-constrained model (worst case) and at the right end it represents always using the most resource-hungry model (best case). It seems that if you know ahead of time that you need to operate exclusively in the worst case setting then it is better to train a model specifically for that task, as indicated by the green line crossing over the blue. The result is similar for the best case scenario although the performance gain is not as clear in that case. The main benefit of our model comes from the in-between cases when the model can choose per-instance how much of its 'budget' to expend."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Changing Model Behavior at Test-time Using Reinforcement Learning", "abstract": "Machine learning models are often used at test-time subject to constraints and trade-offs not present\nat training-time. For example, a computer vision model operating on an embedded device may need to perform\nreal-time inference, or a translation model operating on a cell phone may wish to bound its average\ncompute time in order to be power-efficient. In this work we describe a mixture-of-experts model and show how to change its  test-time resource-usage on a per-input basis using reinforcement learning. We test our method on a small MNIST-based example.", "pdf": "/pdf/3a1058172431aad2e63d66386ac5defbda063fd8.pdf", "TL;DR": "We describe a method to change model behavior at test-time on a per-input basis using reinforcement learning in a mixture-of-experts framework.", "paperhash": "odena|changing_model_behavior_at_testtime_using_reinforcement_learning", "conflicts": ["google.com"], "keywords": ["Reinforcement Learning", "Deep learning"], "authors": ["Augustus Odena", "Dieterich Lawson", "Christopher Olah"], "authorids": ["augustusodena@google.com", "dieterichl@google.com", "colah@google.com"]}, "tags": [], "invitation": {"tddate": null, "tmdate": 1487364094275, "tcdate": 1487364094275, "id": "ICLR.cc/2017/workshop/-/paper130/public/comment", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2017/workshop/paper130/reviewers"], "reply": {"forum": "Hk8-lkHKe", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/workshop/reviewers", "ICLR.cc/2017/pcs"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1487364094275}}}, {"tddate": null, "tmdate": 1489085552290, "tcdate": 1489085552290, "number": 2, "id": "SJ_uVm1jx", "invitation": "ICLR.cc/2017/workshop/-/paper130/official/review", "forum": "Hk8-lkHKe", "replyto": "Hk8-lkHKe", "signatures": ["ICLR.cc/2017/workshop/paper130/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper130/AnonReviewer1"], "content": {"title": "Moving computation tradeoffs to test-time", "rating": "8: Top 50% of accepted papers, clear accept", "review": "A cool improvement on previous partial computation works that allows to change amount of computation at test time rather than during training. \n\nOne downside is that empirical comparisons with previous approaches are lacking. Is it better to train for the worst case (previous methods) or to train for the general case with a test-time precision knob? (knob that controls computational resource allocation).", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Changing Model Behavior at Test-time Using Reinforcement Learning", "abstract": "Machine learning models are often used at test-time subject to constraints and trade-offs not present\nat training-time. For example, a computer vision model operating on an embedded device may need to perform\nreal-time inference, or a translation model operating on a cell phone may wish to bound its average\ncompute time in order to be power-efficient. In this work we describe a mixture-of-experts model and show how to change its  test-time resource-usage on a per-input basis using reinforcement learning. We test our method on a small MNIST-based example.", "pdf": "/pdf/3a1058172431aad2e63d66386ac5defbda063fd8.pdf", "TL;DR": "We describe a method to change model behavior at test-time on a per-input basis using reinforcement learning in a mixture-of-experts framework.", "paperhash": "odena|changing_model_behavior_at_testtime_using_reinforcement_learning", "conflicts": ["google.com"], "keywords": ["Reinforcement Learning", "Deep learning"], "authors": ["Augustus Odena", "Dieterich Lawson", "Christopher Olah"], "authorids": ["augustusodena@google.com", "dieterichl@google.com", "colah@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489085553123, "id": "ICLR.cc/2017/workshop/-/paper130/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper130/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper130/AnonReviewer2", "ICLR.cc/2017/workshop/paper130/AnonReviewer1"], "reply": {"forum": "Hk8-lkHKe", "replyto": "Hk8-lkHKe", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper130/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper130/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489085553123}}}, {"tddate": null, "tmdate": 1489074718886, "tcdate": 1489074718886, "number": 1, "id": "HyPXqgJjx", "invitation": "ICLR.cc/2017/workshop/-/paper130/official/review", "forum": "Hk8-lkHKe", "replyto": "Hk8-lkHKe", "signatures": ["ICLR.cc/2017/workshop/paper130/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper130/AnonReviewer2"], "content": {"title": "An interesting paper ", "rating": "7: Good paper, accept", "review": "The idea of the paper is to use RL to determine the resources used at testing phase. This paper is well-motivated and interesting, and the experimental results is promising. I think it is a good paper to be in the workshop track of ICLR.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Changing Model Behavior at Test-time Using Reinforcement Learning", "abstract": "Machine learning models are often used at test-time subject to constraints and trade-offs not present\nat training-time. For example, a computer vision model operating on an embedded device may need to perform\nreal-time inference, or a translation model operating on a cell phone may wish to bound its average\ncompute time in order to be power-efficient. In this work we describe a mixture-of-experts model and show how to change its  test-time resource-usage on a per-input basis using reinforcement learning. We test our method on a small MNIST-based example.", "pdf": "/pdf/3a1058172431aad2e63d66386ac5defbda063fd8.pdf", "TL;DR": "We describe a method to change model behavior at test-time on a per-input basis using reinforcement learning in a mixture-of-experts framework.", "paperhash": "odena|changing_model_behavior_at_testtime_using_reinforcement_learning", "conflicts": ["google.com"], "keywords": ["Reinforcement Learning", "Deep learning"], "authors": ["Augustus Odena", "Dieterich Lawson", "Christopher Olah"], "authorids": ["augustusodena@google.com", "dieterichl@google.com", "colah@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489085553123, "id": "ICLR.cc/2017/workshop/-/paper130/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper130/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper130/AnonReviewer2", "ICLR.cc/2017/workshop/paper130/AnonReviewer1"], "reply": {"forum": "Hk8-lkHKe", "replyto": "Hk8-lkHKe", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper130/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper130/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489085553123}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1487364093734, "tcdate": 1487364093734, "number": 130, "id": "Hk8-lkHKe", "invitation": "ICLR.cc/2017/workshop/-/submission", "forum": "Hk8-lkHKe", "signatures": ["~Augustus_Odena1"], "readers": ["everyone"], "content": {"title": "Changing Model Behavior at Test-time Using Reinforcement Learning", "abstract": "Machine learning models are often used at test-time subject to constraints and trade-offs not present\nat training-time. For example, a computer vision model operating on an embedded device may need to perform\nreal-time inference, or a translation model operating on a cell phone may wish to bound its average\ncompute time in order to be power-efficient. In this work we describe a mixture-of-experts model and show how to change its  test-time resource-usage on a per-input basis using reinforcement learning. We test our method on a small MNIST-based example.", "pdf": "/pdf/3a1058172431aad2e63d66386ac5defbda063fd8.pdf", "TL;DR": "We describe a method to change model behavior at test-time on a per-input basis using reinforcement learning in a mixture-of-experts framework.", "paperhash": "odena|changing_model_behavior_at_testtime_using_reinforcement_learning", "conflicts": ["google.com"], "keywords": ["Reinforcement Learning", "Deep learning"], "authors": ["Augustus Odena", "Dieterich Lawson", "Christopher Olah"], "authorids": ["augustusodena@google.com", "dieterichl@google.com", "colah@google.com"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1487690420000, "tmdate": 1484242559574, "id": "ICLR.cc/2017/workshop/-/submission", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1495466420000, "cdate": 1484242559574}}}], "count": 5}