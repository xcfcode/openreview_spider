{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1528124454389, "tcdate": 1518468737354, "number": 261, "cdate": 1518468737354, "id": "S1Yt0d1vG", "invitation": "ICLR.cc/2018/Workshop/-/Submission", "forum": "S1Yt0d1vG", "signatures": ["~Peter_Jin1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop"], "content": {"title": "Spatially Parallel Convolutions", "abstract": "The training of convolutional neural networks with large inputs on GPUs is limited by the available GPU memory capacity. In this work, we describe spatially parallel convolutions, which sidestep the memory capacity limit of a single GPU by partitioning tensors along their spatial axes across multiple GPUs. On modern multi-GPU systems, we demonstrate that spatially parallel convolutions attain excellent scaling when applied to input tensors with large spatial dimensions.", "paperhash": "jin|spatially_parallel_convolutions", "keywords": ["deep learning", "convolution", "parallelism"], "_bibtex": "@misc{\n  jin2018spatially,\n  title={Spatially Parallel Convolutions},\n  author={Peter Jin and Boris Ginsburg and Kurt Keutzer},\n  year={2018},\n  url={https://openreview.net/forum?id=S1Yt0d1vG}\n}", "authorids": ["phj@eecs.berkeley.edu", "bginsburg@nvidia.com", "keutzer@berkeley.edu"], "authors": ["Peter Jin", "Boris Ginsburg", "Kurt Keutzer"], "TL;DR": "Spatially parallel convolutions reduce per-GPU memory usage and scale excellently to multiple GPUs.", "pdf": "/pdf/ac84b20d2e9cdee66fb73982bbbb02c965dd7beb.pdf"}, "nonreaders": [], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1518472800000, "tmdate": 1518474081690, "id": "ICLR.cc/2018/Workshop/-/Submission", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Workshop"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2018/Workshop", "description": "Your authorized identity to be associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 9, "value-regex": "upload", "description": "Upload a PDF file that ends with .pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 8, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names. Please provide real names; identities will be anonymized."}, "keywords": {"order": 6, "values-regex": "(^$)|[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of keywords."}, "TL;DR": {"required": false, "order": 7, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,500}"}, "authorids": {"required": true, "order": 3, "values-regex": "([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,},){0,}([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,})", "description": "Comma separated list of author email addresses, lowercased, in the same order as above. For authors with existing OpenReview accounts, please make sure that the provided email address(es) match those listed in the author's profile. Please provide real emails; identities will be anonymized."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1526248800000, "cdate": 1518474081690}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582792105, "tcdate": 1520628426807, "number": 1, "cdate": 1520628426807, "id": "BymAGOgtG", "invitation": "ICLR.cc/2018/Workshop/-/Paper261/Official_Review", "forum": "S1Yt0d1vG", "replyto": "S1Yt0d1vG", "signatures": ["ICLR.cc/2018/Workshop/Paper261/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper261/AnonReviewer1"], "content": {"title": "memory efficient parallel convolutions", "rating": "7: Good paper, accept", "review": "This paper describes spatially parallel convolutions to spatially partition tensors across multiple GPUs. The spatial parallelism provides superlinear scaling with the number of GPUs. \n\nThe speedup results are impressive. The halo \"trick\" reduces additional communication for getting neighboring items. The evaluation is over NVLink. Since all-reduce can be expensive, do you need to heavily rely on this technology for the scaling? An additional question is how varying the halo sizes affects performance, can you add this in Table 1? Overall, the paper presents a clever idea and provides impressive results in the evaluation.\n\n", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Spatially Parallel Convolutions", "abstract": "The training of convolutional neural networks with large inputs on GPUs is limited by the available GPU memory capacity. In this work, we describe spatially parallel convolutions, which sidestep the memory capacity limit of a single GPU by partitioning tensors along their spatial axes across multiple GPUs. On modern multi-GPU systems, we demonstrate that spatially parallel convolutions attain excellent scaling when applied to input tensors with large spatial dimensions.", "paperhash": "jin|spatially_parallel_convolutions", "keywords": ["deep learning", "convolution", "parallelism"], "_bibtex": "@misc{\n  jin2018spatially,\n  title={Spatially Parallel Convolutions},\n  author={Peter Jin and Boris Ginsburg and Kurt Keutzer},\n  year={2018},\n  url={https://openreview.net/forum?id=S1Yt0d1vG}\n}", "authorids": ["phj@eecs.berkeley.edu", "bginsburg@nvidia.com", "keutzer@berkeley.edu"], "authors": ["Peter Jin", "Boris Ginsburg", "Kurt Keutzer"], "TL;DR": "Spatially parallel convolutions reduce per-GPU memory usage and scale excellently to multiple GPUs.", "pdf": "/pdf/ac84b20d2e9cdee66fb73982bbbb02c965dd7beb.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582791913, "id": "ICLR.cc/2018/Workshop/-/Paper261/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper261/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper261/AnonReviewer1", "ICLR.cc/2018/Workshop/Paper261/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper261/AnonReviewer2"], "reply": {"forum": "S1Yt0d1vG", "replyto": "S1Yt0d1vG", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper261/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper261/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582791913}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582703340, "tcdate": 1520697448758, "number": 2, "cdate": 1520697448758, "id": "Sk-OeFbYM", "invitation": "ICLR.cc/2018/Workshop/-/Paper261/Official_Review", "forum": "S1Yt0d1vG", "replyto": "S1Yt0d1vG", "signatures": ["ICLR.cc/2018/Workshop/Paper261/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper261/AnonReviewer3"], "content": {"title": "Good attempt, insufficient experiment", "rating": "6: Marginally above acceptance threshold", "review": "This paper engineers the cuda kernel for spatial parallelization of the convolution operator. \nSince this is more of an engineering paper, there is no very innovative idea in this paper, thus it is easy to understand.\nThis work targets the application scenario where the neural network is very large, and can not fit a single GPU.\nPro:\nThis work accomplished what it intend to solve, namely parallelize the convolution kernel spatially with reasonable speed up.\nThis work may be useful in the future if the memory of GPU is not further increasing and yet researchers are working on larger images/videos.\n \nCon:\nThe compared batch size in this paper is still 32, one could argue that in this case we can simply do data parallelization. Doing spatial parallelization will need to communicate both out of bounds data and gradient of weights, which has at most equal if not more communication compared to data parallelization. Comparison should be made in order to show the advantage of spatial than data parallelization. More experiments with large spatial size and smaller batch size are expected.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Spatially Parallel Convolutions", "abstract": "The training of convolutional neural networks with large inputs on GPUs is limited by the available GPU memory capacity. In this work, we describe spatially parallel convolutions, which sidestep the memory capacity limit of a single GPU by partitioning tensors along their spatial axes across multiple GPUs. On modern multi-GPU systems, we demonstrate that spatially parallel convolutions attain excellent scaling when applied to input tensors with large spatial dimensions.", "paperhash": "jin|spatially_parallel_convolutions", "keywords": ["deep learning", "convolution", "parallelism"], "_bibtex": "@misc{\n  jin2018spatially,\n  title={Spatially Parallel Convolutions},\n  author={Peter Jin and Boris Ginsburg and Kurt Keutzer},\n  year={2018},\n  url={https://openreview.net/forum?id=S1Yt0d1vG}\n}", "authorids": ["phj@eecs.berkeley.edu", "bginsburg@nvidia.com", "keutzer@berkeley.edu"], "authors": ["Peter Jin", "Boris Ginsburg", "Kurt Keutzer"], "TL;DR": "Spatially parallel convolutions reduce per-GPU memory usage and scale excellently to multiple GPUs.", "pdf": "/pdf/ac84b20d2e9cdee66fb73982bbbb02c965dd7beb.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582791913, "id": "ICLR.cc/2018/Workshop/-/Paper261/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper261/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper261/AnonReviewer1", "ICLR.cc/2018/Workshop/Paper261/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper261/AnonReviewer2"], "reply": {"forum": "S1Yt0d1vG", "replyto": "S1Yt0d1vG", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper261/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper261/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582791913}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582693992, "tcdate": 1520713046496, "number": 3, "cdate": 1520713046496, "id": "HJAIpnbYf", "invitation": "ICLR.cc/2018/Workshop/-/Paper261/Official_Review", "forum": "S1Yt0d1vG", "replyto": "S1Yt0d1vG", "signatures": ["ICLR.cc/2018/Workshop/Paper261/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper261/AnonReviewer2"], "content": {"title": "Encouraging result", "rating": "7: Good paper, accept", "review": "The authors address the challenge of using large inputs, e.g., high resolution images while training ConvNets on GPUs with limited memory. The proposed solution involves parallelizing convolutions across the spatial dimension, so that it can use multiple GPUs. The results show that such an operation can be efficient in practice and scales well with the number of GPUs.\n\nMinor comments\n- It would be helpful if the authors provided the baseline ResNet-18 number in the introduction\n- This is just a suggestion, and please feel free to ignore - If you could also provide the final numbers for the models trained on 512x512 images, it would be nice.", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Spatially Parallel Convolutions", "abstract": "The training of convolutional neural networks with large inputs on GPUs is limited by the available GPU memory capacity. In this work, we describe spatially parallel convolutions, which sidestep the memory capacity limit of a single GPU by partitioning tensors along their spatial axes across multiple GPUs. On modern multi-GPU systems, we demonstrate that spatially parallel convolutions attain excellent scaling when applied to input tensors with large spatial dimensions.", "paperhash": "jin|spatially_parallel_convolutions", "keywords": ["deep learning", "convolution", "parallelism"], "_bibtex": "@misc{\n  jin2018spatially,\n  title={Spatially Parallel Convolutions},\n  author={Peter Jin and Boris Ginsburg and Kurt Keutzer},\n  year={2018},\n  url={https://openreview.net/forum?id=S1Yt0d1vG}\n}", "authorids": ["phj@eecs.berkeley.edu", "bginsburg@nvidia.com", "keutzer@berkeley.edu"], "authors": ["Peter Jin", "Boris Ginsburg", "Kurt Keutzer"], "TL;DR": "Spatially parallel convolutions reduce per-GPU memory usage and scale excellently to multiple GPUs.", "pdf": "/pdf/ac84b20d2e9cdee66fb73982bbbb02c965dd7beb.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582791913, "id": "ICLR.cc/2018/Workshop/-/Paper261/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper261/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper261/AnonReviewer1", "ICLR.cc/2018/Workshop/Paper261/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper261/AnonReviewer2"], "reply": {"forum": "S1Yt0d1vG", "replyto": "S1Yt0d1vG", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper261/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper261/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582791913}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521573557413, "tcdate": 1521573557413, "number": 63, "cdate": 1521573557080, "id": "BkanAAAFz", "invitation": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "forum": "S1Yt0d1vG", "replyto": "S1Yt0d1vG", "signatures": ["ICLR.cc/2018/Workshop/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Program_Chairs"], "content": {"decision": "Accept", "title": "ICLR 2018 Workshop Acceptance Decision", "comment": "Congratulations, your paper was accepted to the ICLR workshop."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Spatially Parallel Convolutions", "abstract": "The training of convolutional neural networks with large inputs on GPUs is limited by the available GPU memory capacity. In this work, we describe spatially parallel convolutions, which sidestep the memory capacity limit of a single GPU by partitioning tensors along their spatial axes across multiple GPUs. On modern multi-GPU systems, we demonstrate that spatially parallel convolutions attain excellent scaling when applied to input tensors with large spatial dimensions.", "paperhash": "jin|spatially_parallel_convolutions", "keywords": ["deep learning", "convolution", "parallelism"], "_bibtex": "@misc{\n  jin2018spatially,\n  title={Spatially Parallel Convolutions},\n  author={Peter Jin and Boris Ginsburg and Kurt Keutzer},\n  year={2018},\n  url={https://openreview.net/forum?id=S1Yt0d1vG}\n}", "authorids": ["phj@eecs.berkeley.edu", "bginsburg@nvidia.com", "keutzer@berkeley.edu"], "authors": ["Peter Jin", "Boris Ginsburg", "Kurt Keutzer"], "TL;DR": "Spatially parallel convolutions reduce per-GPU memory usage and scale excellently to multiple GPUs.", "pdf": "/pdf/ac84b20d2e9cdee66fb73982bbbb02c965dd7beb.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518629844880, "id": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Program_Chairs"], "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Workshop/-/Submission", "writers": {"values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Workshop/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Workshop Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept", "Reject"]}}}, "nonreaders": [], "noninvitees": [], "cdate": 1518629844880}}}], "count": 5}