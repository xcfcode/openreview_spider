{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1528124428839, "tcdate": 1518462706235, "number": 215, "cdate": 1518462706235, "id": "Hy9xDwyPM", "invitation": "ICLR.cc/2018/Workshop/-/Submission", "forum": "Hy9xDwyPM", "signatures": ["~Trieu_Hoang_Trinh1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop"], "content": {"title": "Learning Longer-term Dependencies in RNNs with Auxiliary Losses", "abstract": "We present a simple method to improve learning long-term dependencies in recurrent neural networks (RNNs) by introducing unsupervised auxiliary losses. These auxiliary losses force RNNs to either remember distant past or predict future, enabling truncated backpropagation through time (BPTT) to work on very long sequences. We experimented on sequences up to 16000 tokens long and report faster training, more resource efficiency and better test performance than full BPTT baselines such as Long Short Term Memory (LSTM) networks or Transformer.", "paperhash": "trinh|learning_longerterm_dependencies_in_rnns_with_auxiliary_losses", "keywords": ["Deep Learning", "Semi-supervised Learning", "Unsupervised Learning", "Long-term dependencies", "Recurrent Neural Networks", "Auxiliary Losses"], "_bibtex": "@misc{\n  trinh2018learning,\n  title={Learning Longer-term Dependencies in RNNs with Auxiliary Losses},\n  author={Trieu H. Trinh and Andrew M. Dai and Minh-Thang Luong and Quoc V. Le},\n  year={2018},\n  url={https://openreview.net/forum?id=Hy9xDwyPM}\n}", "authorids": ["thtrieu@google.com", "adai@google.com", "thangluong@google.com", "qvl@google.com"], "authors": ["Trieu H. Trinh", "Andrew M. Dai", "Minh-Thang Luong", "Quoc V. Le"], "TL;DR": "Combining auxiliary losses and truncated backpropagation through time in RNNs improves resource efficiency, training speed and generalization in learning long term dependencies.", "pdf": "/pdf/16f1d7a80d68b8b2f87a51052d9193fc39a523aa.pdf"}, "nonreaders": [], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1518472800000, "tmdate": 1518474081690, "id": "ICLR.cc/2018/Workshop/-/Submission", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Workshop"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2018/Workshop", "description": "Your authorized identity to be associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 9, "value-regex": "upload", "description": "Upload a PDF file that ends with .pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 8, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names. Please provide real names; identities will be anonymized."}, "keywords": {"order": 6, "values-regex": "(^$)|[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of keywords."}, "TL;DR": {"required": false, "order": 7, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,500}"}, "authorids": {"required": true, "order": 3, "values-regex": "([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,},){0,}([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,})", "description": "Comma separated list of author email addresses, lowercased, in the same order as above. For authors with existing OpenReview accounts, please make sure that the provided email address(es) match those listed in the author's profile. Please provide real emails; identities will be anonymized."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1526248800000, "cdate": 1518474081690}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582873534, "tcdate": 1520548762208, "number": 1, "cdate": 1520548762208, "id": "Hkfso4ytf", "invitation": "ICLR.cc/2018/Workshop/-/Paper215/Official_Review", "forum": "Hy9xDwyPM", "replyto": "Hy9xDwyPM", "signatures": ["ICLR.cc/2018/Workshop/Paper215/AnonReviewer4"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper215/AnonReviewer4"], "content": {"title": "The paper is hard to read and seems to be of limited value compared to Twin Networks which is easy to understand and seems easier to implement, train and tune. A comparison with their results is necessary, benchmark LM results would greatly strengthen the paper.", "rating": "5: Marginally below acceptance threshold", "review": "The paper suggests to train an RNN where each state can both reconstruct its future and past inputs. In the case of sequence classification, this two reconstruction objectives acts as regularizer. In the case of language modeling (LM), the first reconstruction objective is the task (?) and the past reconstruction acts as a regularizer. \n\nI feel the contribution is limited to the reconstruction of the past since regularizing LSTM sequence classifier with a side LM task (or LM pre-training) has been done before. In the case of regularization by reconstruction of the past, I feel it would be necessary to cite and compare with Twin Networks: Matching the Future for Sequence Generation (Dmitriy Serdyuk et al, August 17) which introduce a stronger regularizer in the same spirit. There, the LSTM states have to match those of a backward language model. I would also suggest to report experiments on strong LM benchmarks such as PTB, 1billion word, wiki103.\n\nOn presentation, I feel that the paper is extremely unclear. There is no equation or even a name given for the auxiliary losses. I assumed that you maximized the likelihood of the future/previous symbols given the state, this should have to be guessed by the reader. Also, it is not clear why only a subset of points are anchors, as opposed to having auxiliary losses for all time steps.\n\nOverall, I feel that the paper is hard to read and seems to be of limited value compared to Twin Networks which is easy to understand and seems easier to implement, train and tune. A comparison with their results is necessary, benchmark LM results would greatly strengthen the paper.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Longer-term Dependencies in RNNs with Auxiliary Losses", "abstract": "We present a simple method to improve learning long-term dependencies in recurrent neural networks (RNNs) by introducing unsupervised auxiliary losses. These auxiliary losses force RNNs to either remember distant past or predict future, enabling truncated backpropagation through time (BPTT) to work on very long sequences. We experimented on sequences up to 16000 tokens long and report faster training, more resource efficiency and better test performance than full BPTT baselines such as Long Short Term Memory (LSTM) networks or Transformer.", "paperhash": "trinh|learning_longerterm_dependencies_in_rnns_with_auxiliary_losses", "keywords": ["Deep Learning", "Semi-supervised Learning", "Unsupervised Learning", "Long-term dependencies", "Recurrent Neural Networks", "Auxiliary Losses"], "_bibtex": "@misc{\n  trinh2018learning,\n  title={Learning Longer-term Dependencies in RNNs with Auxiliary Losses},\n  author={Trieu H. Trinh and Andrew M. Dai and Minh-Thang Luong and Quoc V. Le},\n  year={2018},\n  url={https://openreview.net/forum?id=Hy9xDwyPM}\n}", "authorids": ["thtrieu@google.com", "adai@google.com", "thangluong@google.com", "qvl@google.com"], "authors": ["Trieu H. Trinh", "Andrew M. Dai", "Minh-Thang Luong", "Quoc V. Le"], "TL;DR": "Combining auxiliary losses and truncated backpropagation through time in RNNs improves resource efficiency, training speed and generalization in learning long term dependencies.", "pdf": "/pdf/16f1d7a80d68b8b2f87a51052d9193fc39a523aa.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582873313, "id": "ICLR.cc/2018/Workshop/-/Paper215/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper215/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper215/AnonReviewer4", "ICLR.cc/2018/Workshop/Paper215/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper215/AnonReviewer1"], "reply": {"forum": "Hy9xDwyPM", "replyto": "Hy9xDwyPM", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper215/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper215/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582873313}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582820005, "tcdate": 1520612096274, "number": 2, "cdate": 1520612096274, "id": "rydWXVltG", "invitation": "ICLR.cc/2018/Workshop/-/Paper215/Official_Review", "forum": "Hy9xDwyPM", "replyto": "Hy9xDwyPM", "signatures": ["ICLR.cc/2018/Workshop/Paper215/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper215/AnonReviewer2"], "content": {"title": "An interesting idea yielding good results, but the presentation could be imporoved", "rating": "8: Top 50% of accepted papers, clear accept", "review": "The authors present a method to train recurrent neural networks for sequence classification tasks without having to back-propagate the loss signal through the whole sequence. To that end, they introduce two auxiliary objectives which encourage the model to keep track of important information: the hidden state at each time step is required to hold enough information to predict the next few time steps (p task) or to recall previously read symbols (r task). The authors show that if a model is trained on these tasks and on the task of interest with very limited back-propagation through time, it can match or improve on the performance of BPTT at a significantly reduced cost. The authors also compare and give some insights into the regularization properties of both auxiliary objectives.\n\nThe main issue of this paper is clarity: the entirety of the method is described in the third paragraph and Figure 1, and the lack of any kind of implementation details makes understanding what is going on a little difficult (for example, I assume that both \"prediction\" and \"reconstruction\" use a log-likelihood loss, but cannot find actual confirmation in the text). Even given the limited length of a workshop submission, adding a sentence or two there could be most helpful. Still, this is certainly a helpful method, which may hopefully be developed to work with larger label sets.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Longer-term Dependencies in RNNs with Auxiliary Losses", "abstract": "We present a simple method to improve learning long-term dependencies in recurrent neural networks (RNNs) by introducing unsupervised auxiliary losses. These auxiliary losses force RNNs to either remember distant past or predict future, enabling truncated backpropagation through time (BPTT) to work on very long sequences. We experimented on sequences up to 16000 tokens long and report faster training, more resource efficiency and better test performance than full BPTT baselines such as Long Short Term Memory (LSTM) networks or Transformer.", "paperhash": "trinh|learning_longerterm_dependencies_in_rnns_with_auxiliary_losses", "keywords": ["Deep Learning", "Semi-supervised Learning", "Unsupervised Learning", "Long-term dependencies", "Recurrent Neural Networks", "Auxiliary Losses"], "_bibtex": "@misc{\n  trinh2018learning,\n  title={Learning Longer-term Dependencies in RNNs with Auxiliary Losses},\n  author={Trieu H. Trinh and Andrew M. Dai and Minh-Thang Luong and Quoc V. Le},\n  year={2018},\n  url={https://openreview.net/forum?id=Hy9xDwyPM}\n}", "authorids": ["thtrieu@google.com", "adai@google.com", "thangluong@google.com", "qvl@google.com"], "authors": ["Trieu H. Trinh", "Andrew M. Dai", "Minh-Thang Luong", "Quoc V. Le"], "TL;DR": "Combining auxiliary losses and truncated backpropagation through time in RNNs improves resource efficiency, training speed and generalization in learning long term dependencies.", "pdf": "/pdf/16f1d7a80d68b8b2f87a51052d9193fc39a523aa.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582873313, "id": "ICLR.cc/2018/Workshop/-/Paper215/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper215/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper215/AnonReviewer4", "ICLR.cc/2018/Workshop/Paper215/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper215/AnonReviewer1"], "reply": {"forum": "Hy9xDwyPM", "replyto": "Hy9xDwyPM", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper215/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper215/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582873313}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582676732, "tcdate": 1520734779264, "number": 3, "cdate": 1520734779264, "id": "BkXBMzzYM", "invitation": "ICLR.cc/2018/Workshop/-/Paper215/Official_Review", "forum": "Hy9xDwyPM", "replyto": "Hy9xDwyPM", "signatures": ["ICLR.cc/2018/Workshop/Paper215/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper215/AnonReviewer1"], "content": {"title": "review", "rating": "6: Marginally above acceptance threshold", "review": "In this paper, the authors propose a new method to learn recurrent networks with long-term dependencies. One difficulty of \ntraining such models is that performing back-propagation is memory expensive (as it cannot be truncated). A few techniques \nhave been proposed to tackle this issue, such as the synthetic gradients (Jaderberg et al., 2016). This paper propose a new \nalternative, based on auxiliary losses which are added for each sub-sequence of the BPTT algorithm. The authors consider two \nunsupervised losses: a reconstruction loss (predicting previously seen tokens) and a prediction loss (predicting future tokens). \nThey show experimentally that these auxiliary losses make it possible to train RNNs with long dependencies on classical tasks \nsuch a permuted MNIST or CIFAR10.\n\nOverall, I think this paper introduces an interesting method, which is simple and efficient (based on results from the paper). \nThe experimental results from the papers seems strong (although I don't know the SOTA in that area). My main concerns with this paper are the following:\n- First, I believe that it is a bit hard to understand the method, and more technical details would be welcomed (e.g. where are \nthe auxiliary losses added? at the beginning or end of BPTT sub-sequences?)\n- I think it would also be interesting to have (more) experimental comparisons with previous work, such as synthetic gradients.\n\nPros/Cons:\n+ simple yet efficient method\n+ strong experimental results\n- paper is hard to follow\n- a bit incremental compared to pre-training (e.g. with \"LM\" loss)", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Longer-term Dependencies in RNNs with Auxiliary Losses", "abstract": "We present a simple method to improve learning long-term dependencies in recurrent neural networks (RNNs) by introducing unsupervised auxiliary losses. These auxiliary losses force RNNs to either remember distant past or predict future, enabling truncated backpropagation through time (BPTT) to work on very long sequences. We experimented on sequences up to 16000 tokens long and report faster training, more resource efficiency and better test performance than full BPTT baselines such as Long Short Term Memory (LSTM) networks or Transformer.", "paperhash": "trinh|learning_longerterm_dependencies_in_rnns_with_auxiliary_losses", "keywords": ["Deep Learning", "Semi-supervised Learning", "Unsupervised Learning", "Long-term dependencies", "Recurrent Neural Networks", "Auxiliary Losses"], "_bibtex": "@misc{\n  trinh2018learning,\n  title={Learning Longer-term Dependencies in RNNs with Auxiliary Losses},\n  author={Trieu H. Trinh and Andrew M. Dai and Minh-Thang Luong and Quoc V. Le},\n  year={2018},\n  url={https://openreview.net/forum?id=Hy9xDwyPM}\n}", "authorids": ["thtrieu@google.com", "adai@google.com", "thangluong@google.com", "qvl@google.com"], "authors": ["Trieu H. Trinh", "Andrew M. Dai", "Minh-Thang Luong", "Quoc V. Le"], "TL;DR": "Combining auxiliary losses and truncated backpropagation through time in RNNs improves resource efficiency, training speed and generalization in learning long term dependencies.", "pdf": "/pdf/16f1d7a80d68b8b2f87a51052d9193fc39a523aa.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582873313, "id": "ICLR.cc/2018/Workshop/-/Paper215/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper215/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper215/AnonReviewer4", "ICLR.cc/2018/Workshop/Paper215/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper215/AnonReviewer1"], "reply": {"forum": "Hy9xDwyPM", "replyto": "Hy9xDwyPM", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper215/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper215/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582873313}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521573565327, "tcdate": 1521573565327, "number": 98, "cdate": 1521573564989, "id": "BJSpRCAFM", "invitation": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "forum": "Hy9xDwyPM", "replyto": "Hy9xDwyPM", "signatures": ["ICLR.cc/2018/Workshop/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Program_Chairs"], "content": {"decision": "Accept", "title": "ICLR 2018 Workshop Acceptance Decision", "comment": "Congratulations, your paper was accepted to the ICLR workshop."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Longer-term Dependencies in RNNs with Auxiliary Losses", "abstract": "We present a simple method to improve learning long-term dependencies in recurrent neural networks (RNNs) by introducing unsupervised auxiliary losses. These auxiliary losses force RNNs to either remember distant past or predict future, enabling truncated backpropagation through time (BPTT) to work on very long sequences. We experimented on sequences up to 16000 tokens long and report faster training, more resource efficiency and better test performance than full BPTT baselines such as Long Short Term Memory (LSTM) networks or Transformer.", "paperhash": "trinh|learning_longerterm_dependencies_in_rnns_with_auxiliary_losses", "keywords": ["Deep Learning", "Semi-supervised Learning", "Unsupervised Learning", "Long-term dependencies", "Recurrent Neural Networks", "Auxiliary Losses"], "_bibtex": "@misc{\n  trinh2018learning,\n  title={Learning Longer-term Dependencies in RNNs with Auxiliary Losses},\n  author={Trieu H. Trinh and Andrew M. Dai and Minh-Thang Luong and Quoc V. Le},\n  year={2018},\n  url={https://openreview.net/forum?id=Hy9xDwyPM}\n}", "authorids": ["thtrieu@google.com", "adai@google.com", "thangluong@google.com", "qvl@google.com"], "authors": ["Trieu H. Trinh", "Andrew M. Dai", "Minh-Thang Luong", "Quoc V. Le"], "TL;DR": "Combining auxiliary losses and truncated backpropagation through time in RNNs improves resource efficiency, training speed and generalization in learning long term dependencies.", "pdf": "/pdf/16f1d7a80d68b8b2f87a51052d9193fc39a523aa.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518629844880, "id": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Program_Chairs"], "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Workshop/-/Submission", "writers": {"values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Workshop/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Workshop Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept", "Reject"]}}}, "nonreaders": [], "noninvitees": [], "cdate": 1518629844880}}}, {"tddate": null, "ddate": null, "tmdate": 1518982382352, "tcdate": 1518982382352, "number": 3, "cdate": 1518982382352, "id": "rJLlSUvDf", "invitation": "ICLR.cc/2018/Workshop/-/Paper215/Public_Comment", "forum": "Hy9xDwyPM", "replyto": "Hy9xDwyPM", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "Is the unsupervised loss applicable to Transformer as well?", "comment": "Transformer's cache cost can be upper-bounded if we modify the self-attention. It was discussed in the original paper as \"restricted self-attention,\" and recently OpenAI announced that this kind of modification to be an important open problem, not to mention that there's no guarantee that it will work. So, I'm curious to know an answer to my question in the title."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Longer-term Dependencies in RNNs with Auxiliary Losses", "abstract": "We present a simple method to improve learning long-term dependencies in recurrent neural networks (RNNs) by introducing unsupervised auxiliary losses. These auxiliary losses force RNNs to either remember distant past or predict future, enabling truncated backpropagation through time (BPTT) to work on very long sequences. We experimented on sequences up to 16000 tokens long and report faster training, more resource efficiency and better test performance than full BPTT baselines such as Long Short Term Memory (LSTM) networks or Transformer.", "paperhash": "trinh|learning_longerterm_dependencies_in_rnns_with_auxiliary_losses", "keywords": ["Deep Learning", "Semi-supervised Learning", "Unsupervised Learning", "Long-term dependencies", "Recurrent Neural Networks", "Auxiliary Losses"], "_bibtex": "@misc{\n  trinh2018learning,\n  title={Learning Longer-term Dependencies in RNNs with Auxiliary Losses},\n  author={Trieu H. Trinh and Andrew M. Dai and Minh-Thang Luong and Quoc V. Le},\n  year={2018},\n  url={https://openreview.net/forum?id=Hy9xDwyPM}\n}", "authorids": ["thtrieu@google.com", "adai@google.com", "thangluong@google.com", "qvl@google.com"], "authors": ["Trieu H. Trinh", "Andrew M. Dai", "Minh-Thang Luong", "Quoc V. Le"], "TL;DR": "Combining auxiliary losses and truncated backpropagation through time in RNNs improves resource efficiency, training speed and generalization in learning long term dependencies.", "pdf": "/pdf/16f1d7a80d68b8b2f87a51052d9193fc39a523aa.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518712624786, "id": "ICLR.cc/2018/Workshop/-/Paper215/Public_Comment", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper215/Reviewers"], "reply": {"replyto": null, "forum": "Hy9xDwyPM", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1518712624786}}}, {"tddate": null, "ddate": null, "tmdate": 1518894290549, "tcdate": 1518894290549, "number": 2, "cdate": 1518894290549, "id": "S1sA2eLvM", "invitation": "ICLR.cc/2018/Workshop/-/Paper215/Public_Comment", "forum": "Hy9xDwyPM", "replyto": "Bk-pfoBwM", "signatures": ["~Trieu_Hoang_Trinh1"], "readers": ["everyone"], "writers": ["~Trieu_Hoang_Trinh1"], "content": {"title": "Fixed version submitted", "comment": "Thank you for your request, we submitted the fixed length version to iclr2018.programchairs@gmail.com today (February 17th, 2018)."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Longer-term Dependencies in RNNs with Auxiliary Losses", "abstract": "We present a simple method to improve learning long-term dependencies in recurrent neural networks (RNNs) by introducing unsupervised auxiliary losses. These auxiliary losses force RNNs to either remember distant past or predict future, enabling truncated backpropagation through time (BPTT) to work on very long sequences. We experimented on sequences up to 16000 tokens long and report faster training, more resource efficiency and better test performance than full BPTT baselines such as Long Short Term Memory (LSTM) networks or Transformer.", "paperhash": "trinh|learning_longerterm_dependencies_in_rnns_with_auxiliary_losses", "keywords": ["Deep Learning", "Semi-supervised Learning", "Unsupervised Learning", "Long-term dependencies", "Recurrent Neural Networks", "Auxiliary Losses"], "_bibtex": "@misc{\n  trinh2018learning,\n  title={Learning Longer-term Dependencies in RNNs with Auxiliary Losses},\n  author={Trieu H. Trinh and Andrew M. Dai and Minh-Thang Luong and Quoc V. Le},\n  year={2018},\n  url={https://openreview.net/forum?id=Hy9xDwyPM}\n}", "authorids": ["thtrieu@google.com", "adai@google.com", "thangluong@google.com", "qvl@google.com"], "authors": ["Trieu H. Trinh", "Andrew M. Dai", "Minh-Thang Luong", "Quoc V. Le"], "TL;DR": "Combining auxiliary losses and truncated backpropagation through time in RNNs improves resource efficiency, training speed and generalization in learning long term dependencies.", "pdf": "/pdf/16f1d7a80d68b8b2f87a51052d9193fc39a523aa.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518712624786, "id": "ICLR.cc/2018/Workshop/-/Paper215/Public_Comment", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper215/Reviewers"], "reply": {"replyto": null, "forum": "Hy9xDwyPM", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1518712624786}}}, {"tddate": null, "ddate": null, "tmdate": 1518871251764, "tcdate": 1518871225630, "number": 1, "cdate": 1518871225630, "id": "Bk-pfoBwM", "invitation": "ICLR.cc/2018/Workshop/-/Paper215/Public_Comment", "forum": "Hy9xDwyPM", "replyto": "Hy9xDwyPM", "signatures": ["~Oriol_Vinyals1"], "readers": ["everyone"], "writers": ["~Oriol_Vinyals1"], "content": {"title": "Please Fix Length", "comment": "Your paper violates by a few lines the 3 page limit (see https://iclr.cc/Conferences/2018/CallForWorkshops). Please send us a fixed version of your PDF at iclr2018.programchairs@gmail.com by the end of Monday, February 19th, or else we will reject your paper.\n\nThanks,\nICLR2018 Program Chairs"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Longer-term Dependencies in RNNs with Auxiliary Losses", "abstract": "We present a simple method to improve learning long-term dependencies in recurrent neural networks (RNNs) by introducing unsupervised auxiliary losses. These auxiliary losses force RNNs to either remember distant past or predict future, enabling truncated backpropagation through time (BPTT) to work on very long sequences. We experimented on sequences up to 16000 tokens long and report faster training, more resource efficiency and better test performance than full BPTT baselines such as Long Short Term Memory (LSTM) networks or Transformer.", "paperhash": "trinh|learning_longerterm_dependencies_in_rnns_with_auxiliary_losses", "keywords": ["Deep Learning", "Semi-supervised Learning", "Unsupervised Learning", "Long-term dependencies", "Recurrent Neural Networks", "Auxiliary Losses"], "_bibtex": "@misc{\n  trinh2018learning,\n  title={Learning Longer-term Dependencies in RNNs with Auxiliary Losses},\n  author={Trieu H. Trinh and Andrew M. Dai and Minh-Thang Luong and Quoc V. Le},\n  year={2018},\n  url={https://openreview.net/forum?id=Hy9xDwyPM}\n}", "authorids": ["thtrieu@google.com", "adai@google.com", "thangluong@google.com", "qvl@google.com"], "authors": ["Trieu H. Trinh", "Andrew M. Dai", "Minh-Thang Luong", "Quoc V. Le"], "TL;DR": "Combining auxiliary losses and truncated backpropagation through time in RNNs improves resource efficiency, training speed and generalization in learning long term dependencies.", "pdf": "/pdf/16f1d7a80d68b8b2f87a51052d9193fc39a523aa.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518712624786, "id": "ICLR.cc/2018/Workshop/-/Paper215/Public_Comment", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper215/Reviewers"], "reply": {"replyto": null, "forum": "Hy9xDwyPM", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1518712624786}}}], "count": 8}