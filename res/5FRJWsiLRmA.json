{"notes": [{"id": "5FRJWsiLRmA", "original": "4eBspEQlq3q", "number": 399, "cdate": 1601308051939, "ddate": null, "tcdate": 1601308051939, "tmdate": 1614985711937, "tddate": null, "forum": "5FRJWsiLRmA", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Reservoir Transformers", "authorids": ["sheng.s@berkeley.edu", "~Alexei_Baevski1", "~Ari_S._Morcos1", "~Kurt_Keutzer1", "~Michael_Auli1", "~Douwe_Kiela1"], "authors": ["Sheng Shen", "Alexei Baevski", "Ari S. Morcos", "Kurt Keutzer", "Michael Auli", "Douwe Kiela"], "keywords": [], "abstract": "We demonstrate that transformers obtain impressive performance even when some of the layers are randomly initialized and never updated. Inspired by old and well-established ideas in machine learning, we explore a variety of non-linear reservoir layers interspersed with regular transformer layers, and show improvements in wall-clock compute time until convergence, as well as overall performance, on various machine translation and (masked) language modelling tasks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shen|reservoir_transformers", "pdf": "/pdf/03dfd303778baf8efa60c572dea9de9dd9edaf41.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=7LipKY3ocg", "_bibtex": "@misc{\nshen2021reservoir,\ntitle={Reservoir Transformers},\nauthor={Sheng Shen and Alexei Baevski and Ari S. Morcos and Kurt Keutzer and Michael Auli and Douwe Kiela},\nyear={2021},\nurl={https://openreview.net/forum?id=5FRJWsiLRmA}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "A4nh2hdEfp", "original": null, "number": 1, "cdate": 1610040430970, "ddate": null, "tcdate": 1610040430970, "tmdate": 1610474030950, "tddate": null, "forum": "5FRJWsiLRmA", "replyto": "5FRJWsiLRmA", "invitation": "ICLR.cc/2021/Conference/Paper399/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "The reviewers were split between accept (7) and borderline reject (two 5's). All three reviewers acknowledged that the proposed approach is simple and intuitive (but this paper follows, for the most part, the concept of reservoir operation and apply it to transformers). The main criticisms were insufficient experiments (R5) and the lack of a clear conclusion (R2). I found these concerns to be valid and did not find strong reasons to overturn their recommendations. More comprehensive experiments (especially on WMT) and clear conclusions (accuracy or efficiency) would make this paper much stronger."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Reservoir Transformers", "authorids": ["sheng.s@berkeley.edu", "~Alexei_Baevski1", "~Ari_S._Morcos1", "~Kurt_Keutzer1", "~Michael_Auli1", "~Douwe_Kiela1"], "authors": ["Sheng Shen", "Alexei Baevski", "Ari S. Morcos", "Kurt Keutzer", "Michael Auli", "Douwe Kiela"], "keywords": [], "abstract": "We demonstrate that transformers obtain impressive performance even when some of the layers are randomly initialized and never updated. Inspired by old and well-established ideas in machine learning, we explore a variety of non-linear reservoir layers interspersed with regular transformer layers, and show improvements in wall-clock compute time until convergence, as well as overall performance, on various machine translation and (masked) language modelling tasks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shen|reservoir_transformers", "pdf": "/pdf/03dfd303778baf8efa60c572dea9de9dd9edaf41.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=7LipKY3ocg", "_bibtex": "@misc{\nshen2021reservoir,\ntitle={Reservoir Transformers},\nauthor={Sheng Shen and Alexei Baevski and Ari S. Morcos and Kurt Keutzer and Michael Auli and Douwe Kiela},\nyear={2021},\nurl={https://openreview.net/forum?id=5FRJWsiLRmA}\n}"}, "tags": [], "invitation": {"reply": {"forum": "5FRJWsiLRmA", "replyto": "5FRJWsiLRmA", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040430954, "tmdate": 1610474030930, "id": "ICLR.cc/2021/Conference/Paper399/-/Decision"}}}, {"id": "meerRhluas6", "original": null, "number": 1, "cdate": 1603841461349, "ddate": null, "tcdate": 1603841461349, "tmdate": 1606792572104, "tddate": null, "forum": "5FRJWsiLRmA", "replyto": "5FRJWsiLRmA", "invitation": "ICLR.cc/2021/Conference/Paper399/-/Official_Review", "content": {"title": "Interesting and simple idea but the experiments need to be clearer", "review": "### Summary\n\nThe paper studies how transformers train when some layers are kept fixed at the randomly initialized parameters. The authors observe that transformers can be effectively trained with a significant percentage of their layers \"frozen\". It is also argued that as a result transformers can be trained more efficiently as for the frozen layers gradients with respect to the parameters need not be computed.\n\n### Strengths\n\n- The proposed idea is very simple and easy to understand and implement.\n- The authors perform thorough experiments on several datasets and tasks.\n\n### Weaknesses\n\n1. The AUCC metric is not measuring the efficiency independent of the time budget as it is claimed. For instance given a high enough T_hat the best performing model will always have the highest AUCC. Contrary to that, given a low enough T_hat the fastest to converge model will always be better. I agree with the authors that measuring the efficiency of a neural network is a hard problem but I don't think that the AUCC metric is a good solution. Time to X% of best score is probably much more informative. In addition, time to best score is also not very informative as it depends heavily on the learning rate schedule and the random oscillations of training.\n2. From the provided comparison with respect to the number of updateable layers we cannot deduce information regarding the efficiency. For instance in Fig. 2 for WMT how many \"frozen\" layers are used for the reservoir transformers? What is the wall-clock time per epoch for each model?\n3. From the comparison in the supplementary material with respect to all the layers in the model we observe that the performance is on par with standard transformers but what is the improvement in efficiency? Namely, how much faster are the reservoir transformers for a given number of layers?\n\n### Reasons for recommendation\n\nThe paper proposes a very simple idea and then evaluates it experimentally. Therefore, the experimental section should be thorough and should lead to clear conclusions. Due to the the newly proposed metric as well as the lack of comparison between training time and achieved performance lead me to propose rejection.\n\n### Miscellaneous\n\n- Figure 14 bottom right should probably read \"Validation PPL\".\n\n### Post-rebuttal update\n\nI would like to thank the authors for their additions to the paper. I believe that the extra metrics improve the reader's ability to extract conclusions from the experiments significantly.\n\nHaving said that, I believe that the extra experiments and numbers do not paint a clearer picture. For instance, in IWSLT and WMT indeed there is fairly consistent evidence that FFN Reservoir is more efficient to train than fully trainable transformers. However, for enwik8 the T Reservoir outperforms everything significantly when looking at figure 15 but judging from figure 13 we see that the best case scenario has been selected for T Reservoir (namely 32 or 48 layers). Even more importantly perhaps, the story is completely different when looking at the test set evaluation where FFN Reservoirs perform better and actually the T Reservoir performs the worst among all methods. Similarly on RoBERTa pretraining, fully trainable transformers seem to achieve the lowest validation perplexity and with the highest efficiency.\n\nTo summarize, I believe that the reservoir transformers could be a useful tool for improving either the efficiency or the generalization of transformers in low-data regimes or both. However, a distillation of the experiments and conclusions is required in order for this to be shown from the paper. Namely, even with all those numbers I still cannot judge which reservoir layer will be better, in what sense it will be better (accuracy or efficiency) and why is it going to be better.\n\nDue to the above, I tend to keep my score but because the additions of the authors provide significantly more information (and in my opinion value) to the paper, I will increase my score to 5.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper399/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper399/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Reservoir Transformers", "authorids": ["sheng.s@berkeley.edu", "~Alexei_Baevski1", "~Ari_S._Morcos1", "~Kurt_Keutzer1", "~Michael_Auli1", "~Douwe_Kiela1"], "authors": ["Sheng Shen", "Alexei Baevski", "Ari S. Morcos", "Kurt Keutzer", "Michael Auli", "Douwe Kiela"], "keywords": [], "abstract": "We demonstrate that transformers obtain impressive performance even when some of the layers are randomly initialized and never updated. Inspired by old and well-established ideas in machine learning, we explore a variety of non-linear reservoir layers interspersed with regular transformer layers, and show improvements in wall-clock compute time until convergence, as well as overall performance, on various machine translation and (masked) language modelling tasks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shen|reservoir_transformers", "pdf": "/pdf/03dfd303778baf8efa60c572dea9de9dd9edaf41.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=7LipKY3ocg", "_bibtex": "@misc{\nshen2021reservoir,\ntitle={Reservoir Transformers},\nauthor={Sheng Shen and Alexei Baevski and Ari S. Morcos and Kurt Keutzer and Michael Auli and Douwe Kiela},\nyear={2021},\nurl={https://openreview.net/forum?id=5FRJWsiLRmA}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "5FRJWsiLRmA", "replyto": "5FRJWsiLRmA", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper399/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538144017, "tmdate": 1606915781185, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper399/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper399/-/Official_Review"}}}, {"id": "7Ic7sYcpjmC", "original": null, "number": 2, "cdate": 1603975383556, "ddate": null, "tcdate": 1603975383556, "tmdate": 1606759575079, "tddate": null, "forum": "5FRJWsiLRmA", "replyto": "5FRJWsiLRmA", "invitation": "ICLR.cc/2021/Conference/Paper399/-/Official_Review", "content": {"title": "randomisation in transformer networks", "review": "The paper explores the concept of randomization in Transformer architectures. Essentially, some of the layers in the encoder network are replaced by fixed layers, which makes the computation faster straightforwardly in the prediction phase. The paper also illustrates a method, called backskipping, to reduce the cost of backpropagating gradients through the fixed layers. The proposal is well supported by a number of experiments in the area of Machine Translation.\nPROs:\n- I find the paper very clear and well written & readable. I enjoyed reading it.\n- The research proposed in the paper seems novel enough to me, and goes towards a fruitful research direction, that one of randomized neural algorithms (although this is nowadays established in the ML community).\n- The experiments offer a nice perspective on the advantages of the proposed model\n\nCONs:\n- I find inappropriate the use of the term \"reservoir\" in this work. In my understanding, that is essentially a fixed (untrained) recurrent layer. The aspect of the dynamics is fundamental in this respect. In the paper, instead, the use of \"reservoir\" stands mainly for \"randomized\". All in all, I don't see reservoir layers in the proposed model, and I suggest to take this into account (perhaps changing the name and the title)\n- The experiments show that using untrained layers gives a positive trade-off between accuracy and compute times. In the experimental comparison, I think it would - however - important to see how this trade-off behaves/scales wrt to the number of trainable weights (when avoiding training in some of the layers the number of trainable weights is reduced, and the overall complexity of the system is reduced too, having a sort of regularization effect).\n- The experiments show that it is possible to avoid training in some of the layers of a Transformer. However, in randomized neural networks, a pivotal role is played by the scaling parameters of the involved weight matrices. Orthogonal matrices are used in the experiments, which is good, but I wonder how (and if) the scaling of such matrices affect the performance. Ideally such scaling should be hyper-parameters and be chosen on a validation set.\n\na few minor points:\n- please, if possible, clarify more explicitly on the concept of \"converging faster\"\n- please, clarify on the data splitting (in tr/vl/ts) for the used datasets\n- please, introduce the datasets before section 3.1 (where they are already mentioned without intro).\n\n-- EDIT:\nI would like to thank the authors for the nice work during the review process. I am pretty satisfied with that and I feel serene to increase my rating to acceptance.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper399/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper399/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Reservoir Transformers", "authorids": ["sheng.s@berkeley.edu", "~Alexei_Baevski1", "~Ari_S._Morcos1", "~Kurt_Keutzer1", "~Michael_Auli1", "~Douwe_Kiela1"], "authors": ["Sheng Shen", "Alexei Baevski", "Ari S. Morcos", "Kurt Keutzer", "Michael Auli", "Douwe Kiela"], "keywords": [], "abstract": "We demonstrate that transformers obtain impressive performance even when some of the layers are randomly initialized and never updated. Inspired by old and well-established ideas in machine learning, we explore a variety of non-linear reservoir layers interspersed with regular transformer layers, and show improvements in wall-clock compute time until convergence, as well as overall performance, on various machine translation and (masked) language modelling tasks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shen|reservoir_transformers", "pdf": "/pdf/03dfd303778baf8efa60c572dea9de9dd9edaf41.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=7LipKY3ocg", "_bibtex": "@misc{\nshen2021reservoir,\ntitle={Reservoir Transformers},\nauthor={Sheng Shen and Alexei Baevski and Ari S. Morcos and Kurt Keutzer and Michael Auli and Douwe Kiela},\nyear={2021},\nurl={https://openreview.net/forum?id=5FRJWsiLRmA}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "5FRJWsiLRmA", "replyto": "5FRJWsiLRmA", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper399/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538144017, "tmdate": 1606915781185, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper399/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper399/-/Official_Review"}}}, {"id": "js7i-QUGGlR", "original": null, "number": 3, "cdate": 1604448453770, "ddate": null, "tcdate": 1604448453770, "tmdate": 1606511239886, "tddate": null, "forum": "5FRJWsiLRmA", "replyto": "5FRJWsiLRmA", "invitation": "ICLR.cc/2021/Conference/Paper399/-/Official_Review", "content": {"title": "Review #5", "review": "The authors demonstrate that transformers obtain impressive performance even when some of the layers are randomly initialized and never updated. The authors have experiments on four types of reservoir: Transformer Reservoir , FFN (feed-forward layer) Reservoir, BiGRU Reservoir and CNN Reservoir. And the results show that the Reservoir can achieve competitive/better performance or than normal Transformer on machine translation, language modeling, and MLM pre-training. As pointed out by the authors,  deep reservoir computing networks (Scardapane & Wang, 2017; Gallicchio & Micheli, 2017) have been explored before. The main novelty of this paper is only the Reservoir exploration of Transformer structure. Although the method is interesting and the experiments are well-done, the work \"REDUCING TRANSFORMER DEPTH ON DEMAND WITH STRUCTURED DROPOUT\" seems more straightforward and effective. The authors should have some comparisons with this method. As there's still residual connections, how about simply adding some noise to the previous layer? Overall, although the experiments are interesting, it doesn't provide novel theory on it and misses some comparison to previous works. It's hard to evaluate the significance of this work.\n\nPro:\n1. The reservoir on Transformer is interesting and has not been explored before.\n2. The authors prove the idea by many different tasks.\n\nCons:\n1. The reservoir operation was explored on other structures.\n2. No strong baseline provided, such as \"REDUCING TRANSFORMER DEPTH ON DEMAND WITH STRUCTURED DROPOUT\" or some other structure pruning based methods.\n3. No novel theory to explain the method.\n\n\n######update\n\nI like the experiment added in the revision. However, it is only tested on a IWSLT which is a smaller dataset and can be influence by many hyper-parameters. It's not clear what frozen layers mean for LayerDrop and it need to be clarified with more details. I didn't find clear comparison with stronger baselines on WMT in the revision.\n\nAs pointed our by the authors, I think the theory mainly come from previous works. \n\nI have also read other reviews. Overall, I would like to keep my rating.", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper399/AnonReviewer5"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper399/AnonReviewer5"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Reservoir Transformers", "authorids": ["sheng.s@berkeley.edu", "~Alexei_Baevski1", "~Ari_S._Morcos1", "~Kurt_Keutzer1", "~Michael_Auli1", "~Douwe_Kiela1"], "authors": ["Sheng Shen", "Alexei Baevski", "Ari S. Morcos", "Kurt Keutzer", "Michael Auli", "Douwe Kiela"], "keywords": [], "abstract": "We demonstrate that transformers obtain impressive performance even when some of the layers are randomly initialized and never updated. Inspired by old and well-established ideas in machine learning, we explore a variety of non-linear reservoir layers interspersed with regular transformer layers, and show improvements in wall-clock compute time until convergence, as well as overall performance, on various machine translation and (masked) language modelling tasks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shen|reservoir_transformers", "pdf": "/pdf/03dfd303778baf8efa60c572dea9de9dd9edaf41.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=7LipKY3ocg", "_bibtex": "@misc{\nshen2021reservoir,\ntitle={Reservoir Transformers},\nauthor={Sheng Shen and Alexei Baevski and Ari S. Morcos and Kurt Keutzer and Michael Auli and Douwe Kiela},\nyear={2021},\nurl={https://openreview.net/forum?id=5FRJWsiLRmA}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "5FRJWsiLRmA", "replyto": "5FRJWsiLRmA", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper399/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538144017, "tmdate": 1606915781185, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper399/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper399/-/Official_Review"}}}, {"id": "DRWr9xHmnI", "original": null, "number": 5, "cdate": 1606157188424, "ddate": null, "tcdate": 1606157188424, "tmdate": 1606157295639, "tddate": null, "forum": "5FRJWsiLRmA", "replyto": "meerRhluas6", "invitation": "ICLR.cc/2021/Conference/Paper399/-/Official_Comment", "content": {"title": "Paper updated with your feedback, thanks!", "comment": "Thank you very much for your constructive comments! We have worked very hard to address your concerns and to incorporate your feedback in the new revision. We hope you will find the new version more to your liking, and hope you can revise your scores accordingly. In detail:\n\nThanks for the constructive comments on the AUCC metric. As you say, measuring the efficiency-accuracy trade-off is not a trivial problem. As we say in the paper (Section 3), \u201cOne potential downside of this approach is that the AUCC metric could lead to higher scores for a model that converges quickly but to ultimately worse performance, if measured in a small window. We account for this by making sure that $\\hat{T}$ is set sufficiently high [4h for IWSLT, 20h for WMT, 40h for enwik8 and 60h for RoBERTa].\u201d For the same reason, we included the raw validation curves in the appendix and also report test set generalization in each experiment, so that anyone can check for themselves that our efficiency-v-performance claims hold up. From the training plots in the appendix, one can see that each model has converged long before the point where we set T.\n\nWe do want to make sure that we\u2019ve carefully addressed your concerns and agree that AUCC is not perfect: hence, we have added additional metrics, as per your suggestions, to demonstrate the differences in model efficiency and performance. Specifically, we added your suggested \u201cTime to X% of best score\u201d metric to Tables 1, 2 and 3; report the number of parameters; added wall-clock times per your request, and included standard deviations everywhere to ensure random oscillations cannot cause discrepancies. This also makes it easier to see how much faster the reservoir transformers are for a given number of layers. The speedup gain as measured by \u201ctime to best\u201d is up to 27% by using a FFN Reservoir. We also added time to 95% and 99% results in Tables 4 and 5 in the appendix. We'd be happy to provide multiple different percentages in the final version, if you like.\n\nWe provide the detailed training curves in the Appendix. We added the pretraining wall-clock time to the RoBERTa finetuning performance plots in the Appendix as well. Compared to the vanilla 12 layer Transformer model, the FFN Reservoir consistently outperforms the vanilla model in each time-step. By matching the best performance with the vanilla model, the FFN Reservoir can save up to 25% pretraining wall-clock time for MNLI-m and 10% pretraining wall-clock time for SST2.\n\nWe added the wall-clock time per epoch for each model and added a strong LayerDrop baseline per R5\u2019s request. It can be clearly seen that even though LayerDrop performs similarly in terms of efficiency with T Reservoir at an average 3% to 7% saving on wall-clock time per epoch (and as networks grow deeper, this difference grows bigger), FFN Reservoir consistently outperforms LayerDrop at average 5%~15% saving on wall-clock time per epoch.\n\nWe also explicitly state how many \"frozen\" layers are used for the reservoir transformers for each task now. For the number of frozen layers, we provide the \u201cun-shifted\u201d version of the AUCC plots in the appendix, which shows the total number of layers in each setting for easier comparison. We have clarified this in the new revision.\n\nYour suggestions led us to think about what sorts of things individual layers, whether frozen or not, actually learn. In Appendix G we report results for probing individual RoBERTa layers in the Transformer and T Reservoir case, shedding interesting new light on the question of what individual layers learn from a computational linguistics perspective. Very surprisingly, it appears that some reservoir layers, without \u201clearning\u201d, do much better on some of these probes than their learned counterparts.\n\nWe really appreciate your constructive feedback - it has helped us make the paper much better, and we really hope we\u2019ve sufficiently addressed your concerns for you to update your score to accept."}, "signatures": ["ICLR.cc/2021/Conference/Paper399/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper399/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Reservoir Transformers", "authorids": ["sheng.s@berkeley.edu", "~Alexei_Baevski1", "~Ari_S._Morcos1", "~Kurt_Keutzer1", "~Michael_Auli1", "~Douwe_Kiela1"], "authors": ["Sheng Shen", "Alexei Baevski", "Ari S. Morcos", "Kurt Keutzer", "Michael Auli", "Douwe Kiela"], "keywords": [], "abstract": "We demonstrate that transformers obtain impressive performance even when some of the layers are randomly initialized and never updated. Inspired by old and well-established ideas in machine learning, we explore a variety of non-linear reservoir layers interspersed with regular transformer layers, and show improvements in wall-clock compute time until convergence, as well as overall performance, on various machine translation and (masked) language modelling tasks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shen|reservoir_transformers", "pdf": "/pdf/03dfd303778baf8efa60c572dea9de9dd9edaf41.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=7LipKY3ocg", "_bibtex": "@misc{\nshen2021reservoir,\ntitle={Reservoir Transformers},\nauthor={Sheng Shen and Alexei Baevski and Ari S. Morcos and Kurt Keutzer and Michael Auli and Douwe Kiela},\nyear={2021},\nurl={https://openreview.net/forum?id=5FRJWsiLRmA}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "5FRJWsiLRmA", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper399/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper399/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper399/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper399/Authors|ICLR.cc/2021/Conference/Paper399/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper399/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923871397, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper399/-/Official_Comment"}}}, {"id": "D4mY1_dg7r4", "original": null, "number": 4, "cdate": 1606157056560, "ddate": null, "tcdate": 1606157056560, "tmdate": 1606157084929, "tddate": null, "forum": "5FRJWsiLRmA", "replyto": "7Ic7sYcpjmC", "invitation": "ICLR.cc/2021/Conference/Paper399/-/Official_Comment", "content": {"title": "Paper updated with your feedback, thanks!", "comment": "Thanks for your super valuable feedback! We\u2019re very glad that you enjoyed reading our paper and that you think the results are novel and interesting. We have revised the paper, incorporating your feedback as best as we could in this short timeframe. In particular, we really liked your suggestion to show the number of trainable and non-trainable parameters, and have added this to Tables 1, 2 and 3. We also did preliminary experiments to examine the impact of scaling on initializations, and added clarifications for what we mean by \u201cconverging faster\u201d and what datasets (including exact splits) we use in the work. We hope that you can revise your rating accordingly given this new information. In more detail:\n\n1. Regarding the meaning of \u201creservoir\u201d: You make a valid point that the term reservoir is often employed in the context of RNNs. In fact, we had initially titled our work \u201cEcho State Transformers\u201d but decided against this because the approach does not necessarily abide by the echo state property. Since Lukosevicius & Jaeger in their review refer to a \u201creservoir\u201d as something somewhat more general, which is \u201crandomly created and remains unchanged during training\u201d, we take this to be compatible with our highly non-linear random transformations in transformers, especially given the close connections between transformers and RNNs (just to name one relevant paper: \u201cTransformers are RNNs\u201d, Katharopoulos et al., ICML 2020). That said, if you think that this is not appropriate we would be more than happy to change the name.\n\n2. Number of parameters: you make an excellent point, we added this important information to the paper in the result tables, showing both the number of total parameters as well as the number of trainable parameters. In Table 1, 2 and 3 respectively, it can be seen that our results generalize from 39.5M (IWSLT), 75.6M (WMT) to 153.0M (RoBERTa).\n\n3. Thanks for the suggestion to explore the impact of the scaling parameters. We did some preliminary experiments, where we either a) scale by 1 as normal, b) scale by the global norm of that layer from a pre-trained same-sized network, c) scale by the local norm of that module from a pre-trained same-sized network on the validation set. What find that 1 performs the best on IWSLT with an average 34.6 Validation BLEU across different depths of the transformer. Using the global / local norm will significantly hurt the performance to around an average 25.8 Validation BLEU across different depths of the transformer. We will conduct more thorough experiments in the final version as an ablation study of the sensitivity of this scaling factor.\n\n4. Clarification about \u201cconverging faster\u201d: Per R2\u2019s request, we\u2019ve also added the time to (% of) the best validation score for the results tables. Converging faster can be interpreted there as achieving better/similar validation performance in a shorter amount of training time. For the FFN Reservoir, we achieve up to 27% convergence gain for a 24 layer Transformer model, which is significantly better than vanilla Transformer or the newly added LayerDrop baseline results.\n\n5. We also provide additional RoBERTa pretraining results when fine-tuning on downstream tasks in the Appendix in Figure 9. Each model has the same number of updatable layers (12) there. From the figure, we can clearly see that for the MLM-m task, both FFN Reservoir and T Reservoir constantly achieve better performance in each wall-clock time-stamp. Moreover, to match the best performed accuracy of the vanilla Transformer, we can save up to 25% of the pretraining time by using the FFN Reservoir. \n\n6. Data splitting for the used datasets: For WMT, we follow the pre-processing steps the same as Vaswani et al. (2017). The train/val/test split is 4.5M/16.5k/3k sentences. For IWSLT, we follow the pre-processing steps in Edunov et al. (2018). The train/val/test split is 129k/10k/6.8k sentences. For enwik8, we follow the pre-processing steps in Dai et al. (2019). The train/val/test split is 1M/54k/56k sentences. For RoBERTa pretraining, we follow the pre-processing steps in Liu et al. (2019). We added these details to Section 3.1, thanks for the great suggestion."}, "signatures": ["ICLR.cc/2021/Conference/Paper399/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper399/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Reservoir Transformers", "authorids": ["sheng.s@berkeley.edu", "~Alexei_Baevski1", "~Ari_S._Morcos1", "~Kurt_Keutzer1", "~Michael_Auli1", "~Douwe_Kiela1"], "authors": ["Sheng Shen", "Alexei Baevski", "Ari S. Morcos", "Kurt Keutzer", "Michael Auli", "Douwe Kiela"], "keywords": [], "abstract": "We demonstrate that transformers obtain impressive performance even when some of the layers are randomly initialized and never updated. Inspired by old and well-established ideas in machine learning, we explore a variety of non-linear reservoir layers interspersed with regular transformer layers, and show improvements in wall-clock compute time until convergence, as well as overall performance, on various machine translation and (masked) language modelling tasks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shen|reservoir_transformers", "pdf": "/pdf/03dfd303778baf8efa60c572dea9de9dd9edaf41.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=7LipKY3ocg", "_bibtex": "@misc{\nshen2021reservoir,\ntitle={Reservoir Transformers},\nauthor={Sheng Shen and Alexei Baevski and Ari S. Morcos and Kurt Keutzer and Michael Auli and Douwe Kiela},\nyear={2021},\nurl={https://openreview.net/forum?id=5FRJWsiLRmA}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "5FRJWsiLRmA", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper399/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper399/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper399/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper399/Authors|ICLR.cc/2021/Conference/Paper399/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper399/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923871397, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper399/-/Official_Comment"}}}, {"id": "yGGLsa2OW8D", "original": null, "number": 3, "cdate": 1606156927600, "ddate": null, "tcdate": 1606156927600, "tmdate": 1606156927600, "tddate": null, "forum": "5FRJWsiLRmA", "replyto": "js7i-QUGGlR", "invitation": "ICLR.cc/2021/Conference/Paper399/-/Official_Comment", "content": {"title": "Paper updated with your feedback, thanks!", "comment": "We thank you for your valuable comments! We have updated the paper, incorporating your feedback as best we could. In particular, we have added results for your requested baseline and show that our method performs similarly or better in the T reservoir case, and outperforms it in the FFN case. We really appreciate your suggestions, and we hope that you can revise your rating accordingly given this new information.\n\nAt your request, we performed a series of additional experiments with LayerDrop (Fan et al., \u201cReducing Transformer Depth on Demand with Structured Dropout\u201d) as a baseline. We have updated Tables 1, 2, 3, 4 and 5 with this new information, ensuring that the number of updatable layers is the same and results are comparable. We run all experiments with three random seeds to obtain error bounds. As shown, we can see T Reservoir achieves similar performance as LayerDrop on IWSLT where LayerDrop in terms of wall-clock per epoch and wall-clock time to the best performance. On WMT, where the network needs to be deeper, LayerDrop turns out to achieve worse efficiency gains compared to the T Reservoir. Moreover, on both tasks, FFN Reservoir turns out to perform much better than LayerDrop in terms of efficiency per epoch and achieves better/similar performance in less amount of time in each case. We would be happy to also do this experiment for the language modelling and pre-training settings in the final version of the paper, if you so desire.\n\nYou make a good point about the theoretical underpinnings of this work. We agree that there is lots of room for further exploration of this phenomenon, and it has interesting relationships with e.g. the lottery ticket hypothesis. However, as we argue in the paper (Section 2.1), this result is actually reasonably well-established from a theory perspective in the ML literature, with Rahimi & Recht (2008) being the most recent theoretical treatment of this phenomenon. We definitely hope that our empirical findings will inspire more thorough theoretical treatments of this phenomenon."}, "signatures": ["ICLR.cc/2021/Conference/Paper399/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper399/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Reservoir Transformers", "authorids": ["sheng.s@berkeley.edu", "~Alexei_Baevski1", "~Ari_S._Morcos1", "~Kurt_Keutzer1", "~Michael_Auli1", "~Douwe_Kiela1"], "authors": ["Sheng Shen", "Alexei Baevski", "Ari S. Morcos", "Kurt Keutzer", "Michael Auli", "Douwe Kiela"], "keywords": [], "abstract": "We demonstrate that transformers obtain impressive performance even when some of the layers are randomly initialized and never updated. Inspired by old and well-established ideas in machine learning, we explore a variety of non-linear reservoir layers interspersed with regular transformer layers, and show improvements in wall-clock compute time until convergence, as well as overall performance, on various machine translation and (masked) language modelling tasks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shen|reservoir_transformers", "pdf": "/pdf/03dfd303778baf8efa60c572dea9de9dd9edaf41.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=7LipKY3ocg", "_bibtex": "@misc{\nshen2021reservoir,\ntitle={Reservoir Transformers},\nauthor={Sheng Shen and Alexei Baevski and Ari S. Morcos and Kurt Keutzer and Michael Auli and Douwe Kiela},\nyear={2021},\nurl={https://openreview.net/forum?id=5FRJWsiLRmA}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "5FRJWsiLRmA", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper399/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper399/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper399/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper399/Authors|ICLR.cc/2021/Conference/Paper399/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper399/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923871397, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper399/-/Official_Comment"}}}], "count": 8}