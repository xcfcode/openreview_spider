{"notes": [{"id": "B1gF56VYPH", "original": "SyxDCkCvvH", "number": 714, "cdate": 1569439120743, "ddate": null, "tcdate": 1569439120743, "tmdate": 1587460195877, "tddate": null, "forum": "B1gF56VYPH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Deep 3D Pan via local adaptive \"t-shaped\" convolutions with global and local adaptive dilations", "authors": ["Juan Luis Gonzalez Bello", "Munchurl Kim"], "authorids": ["juanluisgb@kaist.ac.kr", "mkimee@kaist.ac.kr"], "keywords": ["Deep learning", "Stereoscopic view synthesis", "Monocular depth", "Deep 3D Pan"], "TL;DR": "Novel architecture for stereoscopic view synthesis at arbitrary camera shifts utilizing adaptive t-shaped kernels with adaptive dilations.", "abstract": "Recent advances in deep learning have shown promising results in many low-level vision tasks. However, solving the single-image-based view synthesis is still an open problem. In particular, the generation of new images at parallel camera views given a single input image is of great interest, as it enables 3D visualization of the 2D input scenery. We propose a novel network architecture to perform stereoscopic view synthesis at arbitrary camera positions along the X-axis, or \u201cDeep 3D Pan\u201d, with \u201ct-shaped\u201d adaptive kernels equipped with globally and locally adaptive dilations. Our proposed network architecture, the monster-net, is devised with a novel t-shaped adaptive kernel with globally and locally adaptive dilation, which can efficiently incorporate global camera shift into and handle local 3D geometries of the target image\u2019s pixels for the synthesis of naturally looking 3D panned views when a 2-D input image is given. Extensive experiments were performed on the KITTI, CityScapes, and our VICLAB_STEREO indoors dataset to prove the efficacy of our method. Our monster-net significantly outperforms the state-of-the-art method (SOTA) by a large margin in all metrics of RMSE, PSNR, and SSIM. Our proposed monster-net is capable of reconstructing more reliable image structures in synthesized images with coherent geometry. Moreover, the disparity information that can be extracted from the \u201ct-shaped\u201d kernel is much more reliable than that of the SOTA for the unsupervised monocular depth estimation task, confirming the effectiveness of our method.", "pdf": "/pdf/22ea94ce6d68bfcff55d49ecb4d5df171bf908a2.pdf", "paperhash": "bello|deep_3d_pan_via_local_adaptive_tshaped_convolutions_with_global_and_local_adaptive_dilations", "_bibtex": "@inproceedings{\nBello2020Deep,\ntitle={Deep 3D Pan via local adaptive \"t-shaped\" convolutions with global and local adaptive dilations},\nauthor={Juan Luis Gonzalez Bello and Munchurl Kim},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=B1gF56VYPH}\n}", "original_pdf": "/attachment/a0c343551ef295e37a0296d2f0b0d40e4b3e769c.pdf"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "VlxKilPd5g", "original": null, "number": 1, "cdate": 1576798704021, "ddate": null, "tcdate": 1576798704021, "tmdate": 1576800932040, "tddate": null, "forum": "B1gF56VYPH", "replyto": "B1gF56VYPH", "invitation": "ICLR.cc/2020/Conference/Paper714/-/Decision", "content": {"decision": "Accept (Poster)", "comment": "Two reviewers recommend acceptance while one is negative. The authors propose t-shaped kernels for view synthesis, focusing on stereo images. AC finds the problem and method interesting and the results to be sufficiently convincing to warrant acceptance.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep 3D Pan via local adaptive \"t-shaped\" convolutions with global and local adaptive dilations", "authors": ["Juan Luis Gonzalez Bello", "Munchurl Kim"], "authorids": ["juanluisgb@kaist.ac.kr", "mkimee@kaist.ac.kr"], "keywords": ["Deep learning", "Stereoscopic view synthesis", "Monocular depth", "Deep 3D Pan"], "TL;DR": "Novel architecture for stereoscopic view synthesis at arbitrary camera shifts utilizing adaptive t-shaped kernels with adaptive dilations.", "abstract": "Recent advances in deep learning have shown promising results in many low-level vision tasks. However, solving the single-image-based view synthesis is still an open problem. In particular, the generation of new images at parallel camera views given a single input image is of great interest, as it enables 3D visualization of the 2D input scenery. We propose a novel network architecture to perform stereoscopic view synthesis at arbitrary camera positions along the X-axis, or \u201cDeep 3D Pan\u201d, with \u201ct-shaped\u201d adaptive kernels equipped with globally and locally adaptive dilations. Our proposed network architecture, the monster-net, is devised with a novel t-shaped adaptive kernel with globally and locally adaptive dilation, which can efficiently incorporate global camera shift into and handle local 3D geometries of the target image\u2019s pixels for the synthesis of naturally looking 3D panned views when a 2-D input image is given. Extensive experiments were performed on the KITTI, CityScapes, and our VICLAB_STEREO indoors dataset to prove the efficacy of our method. Our monster-net significantly outperforms the state-of-the-art method (SOTA) by a large margin in all metrics of RMSE, PSNR, and SSIM. Our proposed monster-net is capable of reconstructing more reliable image structures in synthesized images with coherent geometry. Moreover, the disparity information that can be extracted from the \u201ct-shaped\u201d kernel is much more reliable than that of the SOTA for the unsupervised monocular depth estimation task, confirming the effectiveness of our method.", "pdf": "/pdf/22ea94ce6d68bfcff55d49ecb4d5df171bf908a2.pdf", "paperhash": "bello|deep_3d_pan_via_local_adaptive_tshaped_convolutions_with_global_and_local_adaptive_dilations", "_bibtex": "@inproceedings{\nBello2020Deep,\ntitle={Deep 3D Pan via local adaptive \"t-shaped\" convolutions with global and local adaptive dilations},\nauthor={Juan Luis Gonzalez Bello and Munchurl Kim},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=B1gF56VYPH}\n}", "original_pdf": "/attachment/a0c343551ef295e37a0296d2f0b0d40e4b3e769c.pdf"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "B1gF56VYPH", "replyto": "B1gF56VYPH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795720361, "tmdate": 1576800271176, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper714/-/Decision"}}}, {"id": "rkxUbQk2jH", "original": null, "number": 3, "cdate": 1573806845586, "ddate": null, "tcdate": 1573806845586, "tmdate": 1573806845586, "tddate": null, "forum": "B1gF56VYPH", "replyto": "B1gF56VYPH", "invitation": "ICLR.cc/2020/Conference/Paper714/-/Official_Comment", "content": {"title": "Revision uploaded on the 12th", "comment": "Please take a quick look at the revision uploaded on the 12th where we included the comments from: \n* Reviewer 3 on extending our t-shaped kernel to any rigid motion and increased citation frequency (we are a little bit limited here as we run out of space due to the ICLR citation style).\n* Reviewer 2 on minor writing errors.\n\n Additionally, we added more visualizations of the predicted depth maps in Figure 17. "}, "signatures": ["ICLR.cc/2020/Conference/Paper714/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper714/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep 3D Pan via local adaptive \"t-shaped\" convolutions with global and local adaptive dilations", "authors": ["Juan Luis Gonzalez Bello", "Munchurl Kim"], "authorids": ["juanluisgb@kaist.ac.kr", "mkimee@kaist.ac.kr"], "keywords": ["Deep learning", "Stereoscopic view synthesis", "Monocular depth", "Deep 3D Pan"], "TL;DR": "Novel architecture for stereoscopic view synthesis at arbitrary camera shifts utilizing adaptive t-shaped kernels with adaptive dilations.", "abstract": "Recent advances in deep learning have shown promising results in many low-level vision tasks. However, solving the single-image-based view synthesis is still an open problem. In particular, the generation of new images at parallel camera views given a single input image is of great interest, as it enables 3D visualization of the 2D input scenery. We propose a novel network architecture to perform stereoscopic view synthesis at arbitrary camera positions along the X-axis, or \u201cDeep 3D Pan\u201d, with \u201ct-shaped\u201d adaptive kernels equipped with globally and locally adaptive dilations. Our proposed network architecture, the monster-net, is devised with a novel t-shaped adaptive kernel with globally and locally adaptive dilation, which can efficiently incorporate global camera shift into and handle local 3D geometries of the target image\u2019s pixels for the synthesis of naturally looking 3D panned views when a 2-D input image is given. Extensive experiments were performed on the KITTI, CityScapes, and our VICLAB_STEREO indoors dataset to prove the efficacy of our method. Our monster-net significantly outperforms the state-of-the-art method (SOTA) by a large margin in all metrics of RMSE, PSNR, and SSIM. Our proposed monster-net is capable of reconstructing more reliable image structures in synthesized images with coherent geometry. Moreover, the disparity information that can be extracted from the \u201ct-shaped\u201d kernel is much more reliable than that of the SOTA for the unsupervised monocular depth estimation task, confirming the effectiveness of our method.", "pdf": "/pdf/22ea94ce6d68bfcff55d49ecb4d5df171bf908a2.pdf", "paperhash": "bello|deep_3d_pan_via_local_adaptive_tshaped_convolutions_with_global_and_local_adaptive_dilations", "_bibtex": "@inproceedings{\nBello2020Deep,\ntitle={Deep 3D Pan via local adaptive \"t-shaped\" convolutions with global and local adaptive dilations},\nauthor={Juan Luis Gonzalez Bello and Munchurl Kim},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=B1gF56VYPH}\n}", "original_pdf": "/attachment/a0c343551ef295e37a0296d2f0b0d40e4b3e769c.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1gF56VYPH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper714/Authors", "ICLR.cc/2020/Conference/Paper714/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper714/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper714/Reviewers", "ICLR.cc/2020/Conference/Paper714/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper714/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper714/Authors|ICLR.cc/2020/Conference/Paper714/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504167335, "tmdate": 1576860542045, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper714/Authors", "ICLR.cc/2020/Conference/Paper714/Reviewers", "ICLR.cc/2020/Conference/Paper714/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper714/-/Official_Comment"}}}, {"id": "SJlEThBVoH", "original": null, "number": 2, "cdate": 1573309627710, "ddate": null, "tcdate": 1573309627710, "tmdate": 1573309627710, "tddate": null, "forum": "B1gF56VYPH", "replyto": "ryeKDII6YB", "invitation": "ICLR.cc/2020/Conference/Paper714/-/Official_Comment", "content": {"title": "Reply to reviewer #2", "comment": "Thank you for your comments.\nIt seems that there is a misunderstanding regarding the values of Table 2. The method of Wang et al 2019, is a supervised method, that is, they train with the depth ground truth. Our depth estimation is unsupervised and belongs to a different category as signalized by the division lines in the table. Our method is compared to Tosi et al. 2019 as theirs is also unsupervised. It is worth to mention that Wang et al 2019 not only trained with depth ground truth but also with 9 consecutive views for additional supervision, thus exploiting temporal consistencies. Our method, with no ground truth and only 2 views during training, performs remarkably well.\n\nRegarding the comment in section A.9, our novel post-processing step remarkably improves over the naive post-processing used in previous works as visualized in Figure 12, by removing most depth \u201cshadows\u201d. However, due to the sparsity of the evaluation ground truth, the metric numbers seem to only slightly improve, as you noted.\n\nRegarding the minor details, we will use the correct wording, instead of \u201cprove\u201d we will change to \u201cdemonstrate\u201d. The same goes for \u201cis known to be a much more complex problem\u201d, \u201cplanned\u201d, and \u201chas a size of 153x153\u201d.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper714/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper714/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep 3D Pan via local adaptive \"t-shaped\" convolutions with global and local adaptive dilations", "authors": ["Juan Luis Gonzalez Bello", "Munchurl Kim"], "authorids": ["juanluisgb@kaist.ac.kr", "mkimee@kaist.ac.kr"], "keywords": ["Deep learning", "Stereoscopic view synthesis", "Monocular depth", "Deep 3D Pan"], "TL;DR": "Novel architecture for stereoscopic view synthesis at arbitrary camera shifts utilizing adaptive t-shaped kernels with adaptive dilations.", "abstract": "Recent advances in deep learning have shown promising results in many low-level vision tasks. However, solving the single-image-based view synthesis is still an open problem. In particular, the generation of new images at parallel camera views given a single input image is of great interest, as it enables 3D visualization of the 2D input scenery. We propose a novel network architecture to perform stereoscopic view synthesis at arbitrary camera positions along the X-axis, or \u201cDeep 3D Pan\u201d, with \u201ct-shaped\u201d adaptive kernels equipped with globally and locally adaptive dilations. Our proposed network architecture, the monster-net, is devised with a novel t-shaped adaptive kernel with globally and locally adaptive dilation, which can efficiently incorporate global camera shift into and handle local 3D geometries of the target image\u2019s pixels for the synthesis of naturally looking 3D panned views when a 2-D input image is given. Extensive experiments were performed on the KITTI, CityScapes, and our VICLAB_STEREO indoors dataset to prove the efficacy of our method. Our monster-net significantly outperforms the state-of-the-art method (SOTA) by a large margin in all metrics of RMSE, PSNR, and SSIM. Our proposed monster-net is capable of reconstructing more reliable image structures in synthesized images with coherent geometry. Moreover, the disparity information that can be extracted from the \u201ct-shaped\u201d kernel is much more reliable than that of the SOTA for the unsupervised monocular depth estimation task, confirming the effectiveness of our method.", "pdf": "/pdf/22ea94ce6d68bfcff55d49ecb4d5df171bf908a2.pdf", "paperhash": "bello|deep_3d_pan_via_local_adaptive_tshaped_convolutions_with_global_and_local_adaptive_dilations", "_bibtex": "@inproceedings{\nBello2020Deep,\ntitle={Deep 3D Pan via local adaptive \"t-shaped\" convolutions with global and local adaptive dilations},\nauthor={Juan Luis Gonzalez Bello and Munchurl Kim},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=B1gF56VYPH}\n}", "original_pdf": "/attachment/a0c343551ef295e37a0296d2f0b0d40e4b3e769c.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1gF56VYPH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper714/Authors", "ICLR.cc/2020/Conference/Paper714/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper714/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper714/Reviewers", "ICLR.cc/2020/Conference/Paper714/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper714/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper714/Authors|ICLR.cc/2020/Conference/Paper714/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504167335, "tmdate": 1576860542045, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper714/Authors", "ICLR.cc/2020/Conference/Paper714/Reviewers", "ICLR.cc/2020/Conference/Paper714/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper714/-/Official_Comment"}}}, {"id": "BkgmL3S4jB", "original": null, "number": 1, "cdate": 1573309515054, "ddate": null, "tcdate": 1573309515054, "tmdate": 1573309515054, "tddate": null, "forum": "B1gF56VYPH", "replyto": "rJem4jsAtB", "invitation": "ICLR.cc/2020/Conference/Paper714/-/Official_Comment", "content": {"title": "Reply to reviewer #3", "comment": "Thank you for your kind review. \nOur \u201ct-shaped\u201d adaptive kernel equipped with adaptive dilations can be trivially generalized to any camera translation (Y and Z axis) by simply allowing the kernel to rotate. E.g. when moving the camera in the Y-axis the long wing of the t-shaped kernel would point towards the Y-axis, and when moving the camera in the Z-axis the t-shaped kernels would point to the center of the image. In this work, we concentrated on horizontal stereoscopic view synthesis as it finds more direct applications in the real world.\n\nThe impact of our work goes beyond image synthesis, as we also present better than state-of-the-art results for monocular depth estimation when no ground truth depth is available (unsupervised approach) and present a way to learn view synthesis and depth from many datasets with different camera baselines simultaneously. \n\nTo put our paper in perspective with the most recent works, Choi et al. proposed \u201cExtreme view synthesis\u201d where, given two images with a narrow baseline as input (1.4cm), they generate a 30x extrapolated view, that is a new view with a baseline of 42cm. Our work, from a single image, generates a novel view with a baseline of 54cm (KITTI baseline), and even at 30% beyond the baseline (~70cm) our method still generates a decent image. Unfortunately, as [1] was made public after our ICLR submission, it is not yet included in our related works section..\n\nFinally, regarding your comment about the citations, we will increase the citation frequency in the final version of our paper.\n\n[1] Choi, Inchang, et al. \"Extreme view synthesis.\" Proceedings of the IEEE International Conference on Computer Vision. 2019.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper714/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper714/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep 3D Pan via local adaptive \"t-shaped\" convolutions with global and local adaptive dilations", "authors": ["Juan Luis Gonzalez Bello", "Munchurl Kim"], "authorids": ["juanluisgb@kaist.ac.kr", "mkimee@kaist.ac.kr"], "keywords": ["Deep learning", "Stereoscopic view synthesis", "Monocular depth", "Deep 3D Pan"], "TL;DR": "Novel architecture for stereoscopic view synthesis at arbitrary camera shifts utilizing adaptive t-shaped kernels with adaptive dilations.", "abstract": "Recent advances in deep learning have shown promising results in many low-level vision tasks. However, solving the single-image-based view synthesis is still an open problem. In particular, the generation of new images at parallel camera views given a single input image is of great interest, as it enables 3D visualization of the 2D input scenery. We propose a novel network architecture to perform stereoscopic view synthesis at arbitrary camera positions along the X-axis, or \u201cDeep 3D Pan\u201d, with \u201ct-shaped\u201d adaptive kernels equipped with globally and locally adaptive dilations. Our proposed network architecture, the monster-net, is devised with a novel t-shaped adaptive kernel with globally and locally adaptive dilation, which can efficiently incorporate global camera shift into and handle local 3D geometries of the target image\u2019s pixels for the synthesis of naturally looking 3D panned views when a 2-D input image is given. Extensive experiments were performed on the KITTI, CityScapes, and our VICLAB_STEREO indoors dataset to prove the efficacy of our method. Our monster-net significantly outperforms the state-of-the-art method (SOTA) by a large margin in all metrics of RMSE, PSNR, and SSIM. Our proposed monster-net is capable of reconstructing more reliable image structures in synthesized images with coherent geometry. Moreover, the disparity information that can be extracted from the \u201ct-shaped\u201d kernel is much more reliable than that of the SOTA for the unsupervised monocular depth estimation task, confirming the effectiveness of our method.", "pdf": "/pdf/22ea94ce6d68bfcff55d49ecb4d5df171bf908a2.pdf", "paperhash": "bello|deep_3d_pan_via_local_adaptive_tshaped_convolutions_with_global_and_local_adaptive_dilations", "_bibtex": "@inproceedings{\nBello2020Deep,\ntitle={Deep 3D Pan via local adaptive \"t-shaped\" convolutions with global and local adaptive dilations},\nauthor={Juan Luis Gonzalez Bello and Munchurl Kim},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=B1gF56VYPH}\n}", "original_pdf": "/attachment/a0c343551ef295e37a0296d2f0b0d40e4b3e769c.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1gF56VYPH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper714/Authors", "ICLR.cc/2020/Conference/Paper714/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper714/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper714/Reviewers", "ICLR.cc/2020/Conference/Paper714/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper714/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper714/Authors|ICLR.cc/2020/Conference/Paper714/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504167335, "tmdate": 1576860542045, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper714/Authors", "ICLR.cc/2020/Conference/Paper714/Reviewers", "ICLR.cc/2020/Conference/Paper714/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper714/-/Official_Comment"}}}, {"id": "B1euNLVnFH", "original": null, "number": 1, "cdate": 1571730992034, "ddate": null, "tcdate": 1571730992034, "tmdate": 1572972561560, "tddate": null, "forum": "B1gF56VYPH", "replyto": "B1gF56VYPH", "invitation": "ICLR.cc/2020/Conference/Paper714/-/Official_Review", "content": {"rating": "6: Weak Accept", "experience_assessment": "I do not know much about this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "N/A", "review": "The paper considers the problem of performing stereoscopic view synthesis (i.e., generating a new view seen from a different camera position) at an arbitrary position along the X-axis from a single input image only. This is an important problem as it enables 3D visualization of a 2D input scene. The paper focuses on the particular problem of generating a stereoscopic view from a single image (i.e., a right and left view from a center image). \nFor this purpose, the paper proposes a t-net architecture which is an autoencoder or U-net like architecture that estimates the values for the t-convolutions proposed in the paper. The network (called monster-net) takes a center image and a pan amount as input, and from those synthesizes the image with the respective view.\n\nThe paper demonstrates that their idea of t-convolutions outperforms recent competing approaches such as deep 3D on available datasets as well as on an in-house collected dataset. The figures provided demonstrate that the views generated by the proposed Monster-net visibly look slightly better than those generated by the competing approaches DeepD and SepConv. In addition, the paper is well written and easy to follows. I therefore recommend acceptance of this paper. I would like to emphasize that while I work in deep learning, I don't work on view synthesis and therefore it is difficult for me to evaluate the novelty of the proposed approach as well as the difficulty of the problem."}, "signatures": ["ICLR.cc/2020/Conference/Paper714/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper714/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep 3D Pan via local adaptive \"t-shaped\" convolutions with global and local adaptive dilations", "authors": ["Juan Luis Gonzalez Bello", "Munchurl Kim"], "authorids": ["juanluisgb@kaist.ac.kr", "mkimee@kaist.ac.kr"], "keywords": ["Deep learning", "Stereoscopic view synthesis", "Monocular depth", "Deep 3D Pan"], "TL;DR": "Novel architecture for stereoscopic view synthesis at arbitrary camera shifts utilizing adaptive t-shaped kernels with adaptive dilations.", "abstract": "Recent advances in deep learning have shown promising results in many low-level vision tasks. However, solving the single-image-based view synthesis is still an open problem. In particular, the generation of new images at parallel camera views given a single input image is of great interest, as it enables 3D visualization of the 2D input scenery. We propose a novel network architecture to perform stereoscopic view synthesis at arbitrary camera positions along the X-axis, or \u201cDeep 3D Pan\u201d, with \u201ct-shaped\u201d adaptive kernels equipped with globally and locally adaptive dilations. Our proposed network architecture, the monster-net, is devised with a novel t-shaped adaptive kernel with globally and locally adaptive dilation, which can efficiently incorporate global camera shift into and handle local 3D geometries of the target image\u2019s pixels for the synthesis of naturally looking 3D panned views when a 2-D input image is given. Extensive experiments were performed on the KITTI, CityScapes, and our VICLAB_STEREO indoors dataset to prove the efficacy of our method. Our monster-net significantly outperforms the state-of-the-art method (SOTA) by a large margin in all metrics of RMSE, PSNR, and SSIM. Our proposed monster-net is capable of reconstructing more reliable image structures in synthesized images with coherent geometry. Moreover, the disparity information that can be extracted from the \u201ct-shaped\u201d kernel is much more reliable than that of the SOTA for the unsupervised monocular depth estimation task, confirming the effectiveness of our method.", "pdf": "/pdf/22ea94ce6d68bfcff55d49ecb4d5df171bf908a2.pdf", "paperhash": "bello|deep_3d_pan_via_local_adaptive_tshaped_convolutions_with_global_and_local_adaptive_dilations", "_bibtex": "@inproceedings{\nBello2020Deep,\ntitle={Deep 3D Pan via local adaptive \"t-shaped\" convolutions with global and local adaptive dilations},\nauthor={Juan Luis Gonzalez Bello and Munchurl Kim},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=B1gF56VYPH}\n}", "original_pdf": "/attachment/a0c343551ef295e37a0296d2f0b0d40e4b3e769c.pdf"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "B1gF56VYPH", "replyto": "B1gF56VYPH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper714/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper714/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576478284735, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper714/Reviewers"], "noninvitees": [], "tcdate": 1570237748155, "tmdate": 1576478284748, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper714/-/Official_Review"}}}, {"id": "ryeKDII6YB", "original": null, "number": 2, "cdate": 1571804769332, "ddate": null, "tcdate": 1571804769332, "tmdate": 1572972561521, "tddate": null, "forum": "B1gF56VYPH", "replyto": "B1gF56VYPH", "invitation": "ICLR.cc/2020/Conference/Paper714/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The submission proposes a method to perform stereoscopic view synthesis. The method consists of a neural network model that estimates a novel viewpoint either to the left or to the right of a given image. The two key insights of the proposed method is 1) to learn the weights of a t-shaped kernel when performing novel view synthesis, and 2) to estimate and use adaptive dilations on those kernels.\n\nThe proposed approach is sound and the evaluation methodology used is adequate. The technique proposed is somewhat related to the recent interest in the community to apply CNNs to non-regular grids [1,2,3].\n\nSec. 4.2 states that the proposed method outperforms other existing methods on monocular depth estimation, while the table seems to indicate that other methods (e.g., Wang et al. 2019) obtain a better a1 measure.\n\nI would refrain from calling a 0.004 increase in a1 \u201ca remarkable improvement\u201d (sec. A.9).\n\nMinor details\n- I would refrain from using the word \u201cprove\u201d (abstract, sec. 4), since no proof is provided. \u201cdemonstrate\u201d\n- p. 2 \u201cis open known to be a much more complex problem\u201d: I think the authors meant either \u201cis known to be a much more complex problem\u201d or \u201cis still an open problem\u201d?\n- p. 9 \u201cplaned\u201d should be written \u201cplanned\u201d.\n- p. 9 \u201cthe receptive field [...] is of the 153x153 size\u201d: should be \u201chas a size of 153x153\u201d.\n\n\n[1] Su, Yu-Chuan, and Kristen Grauman. \"Learning spherical convolution for fast features from 360 imagery.\" Advances in Neural Information Processing Systems. 2017.\n[2] Coors, Benjamin, Alexandru Paul Condurache, and Andreas Geiger. \"Spherenet: Learning spherical representations for detection and classification in omnidirectional images.\" Proceedings of the European Conference on Computer Vision (ECCV). 2018.\n[3] Zhao, Qiang, et al. \"Distortion-aware CNNs for Spherical Images.\" IJCAI. 2018.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper714/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper714/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep 3D Pan via local adaptive \"t-shaped\" convolutions with global and local adaptive dilations", "authors": ["Juan Luis Gonzalez Bello", "Munchurl Kim"], "authorids": ["juanluisgb@kaist.ac.kr", "mkimee@kaist.ac.kr"], "keywords": ["Deep learning", "Stereoscopic view synthesis", "Monocular depth", "Deep 3D Pan"], "TL;DR": "Novel architecture for stereoscopic view synthesis at arbitrary camera shifts utilizing adaptive t-shaped kernels with adaptive dilations.", "abstract": "Recent advances in deep learning have shown promising results in many low-level vision tasks. However, solving the single-image-based view synthesis is still an open problem. In particular, the generation of new images at parallel camera views given a single input image is of great interest, as it enables 3D visualization of the 2D input scenery. We propose a novel network architecture to perform stereoscopic view synthesis at arbitrary camera positions along the X-axis, or \u201cDeep 3D Pan\u201d, with \u201ct-shaped\u201d adaptive kernels equipped with globally and locally adaptive dilations. Our proposed network architecture, the monster-net, is devised with a novel t-shaped adaptive kernel with globally and locally adaptive dilation, which can efficiently incorporate global camera shift into and handle local 3D geometries of the target image\u2019s pixels for the synthesis of naturally looking 3D panned views when a 2-D input image is given. Extensive experiments were performed on the KITTI, CityScapes, and our VICLAB_STEREO indoors dataset to prove the efficacy of our method. Our monster-net significantly outperforms the state-of-the-art method (SOTA) by a large margin in all metrics of RMSE, PSNR, and SSIM. Our proposed monster-net is capable of reconstructing more reliable image structures in synthesized images with coherent geometry. Moreover, the disparity information that can be extracted from the \u201ct-shaped\u201d kernel is much more reliable than that of the SOTA for the unsupervised monocular depth estimation task, confirming the effectiveness of our method.", "pdf": "/pdf/22ea94ce6d68bfcff55d49ecb4d5df171bf908a2.pdf", "paperhash": "bello|deep_3d_pan_via_local_adaptive_tshaped_convolutions_with_global_and_local_adaptive_dilations", "_bibtex": "@inproceedings{\nBello2020Deep,\ntitle={Deep 3D Pan via local adaptive \"t-shaped\" convolutions with global and local adaptive dilations},\nauthor={Juan Luis Gonzalez Bello and Munchurl Kim},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=B1gF56VYPH}\n}", "original_pdf": "/attachment/a0c343551ef295e37a0296d2f0b0d40e4b3e769c.pdf"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "B1gF56VYPH", "replyto": "B1gF56VYPH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper714/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper714/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576478284735, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper714/Reviewers"], "noninvitees": [], "tcdate": 1570237748155, "tmdate": 1576478284748, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper714/-/Official_Review"}}}, {"id": "rJem4jsAtB", "original": null, "number": 3, "cdate": 1571892011322, "ddate": null, "tcdate": 1571892011322, "tmdate": 1572972561477, "tddate": null, "forum": "B1gF56VYPH", "replyto": "B1gF56VYPH", "invitation": "ICLR.cc/2020/Conference/Paper714/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary:\nThis paper proposes a deep learning method to produce \"pans\" of an input image. That is, simulated images of the scene from translated viewpoints. Unlike some previous work that considers only a fixed baseline (such as the 2nd view of a stereo camera), this approach allows generation of a range of views. A specially crafted convolutional architecture is shown to be well-suited to this problem. Results demonstrate visually pleasing image generation and low metric errors on several datasets.\n\nStrengths:\n- The justifications for the design choices in this paper, in particular the convolution structure and connection to image geometry, was quite strong compared to recent papers (although, many of the presented ideas are known in more classical, non-learning, techniques). \n- All presented empirical results are impressive, and show the method is likely to \"really work\" and be reproducible, judging from the number of experiments where the method has consistently outperformed. \n- The method is clear and straightforward to implement either on its own, or as a module/architecture within a larger pipeline.\n\nAreas for Improvement and Detailed Suggestions:\n- The problem of panned view generation is a bit more narrow than some authors are lately considering (generate any viewpoint including off-axis rotations). \n- The t-shaped network architecture here is largely presented as only appropriate to handing image panning. Could a more general network be created, perhaps parameterized by the type of rigid motion occurring? Better, could more flexible networks be proposed with sparsity constraints that allow sub-patterns like the t-network to be learned in a data-driven fashion? \n- Please try to cite referenced work more consistently. For several methods, such as Deep 3D Zoom Net, you have begun to discuss the method using name only for large stretches. It would be helpful to keep using the citation at least once per paragraph to remind the reader of the source of these ideas. \n\nDecision and Justification: \nWeak reject due to the lack of generality in the approach. I suspect the impact of this work will be a bit less than the very competitive bar for ICLR this year. However, I must note that I am the least expert in this area, out of any paper in my stack. \n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper714/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper714/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep 3D Pan via local adaptive \"t-shaped\" convolutions with global and local adaptive dilations", "authors": ["Juan Luis Gonzalez Bello", "Munchurl Kim"], "authorids": ["juanluisgb@kaist.ac.kr", "mkimee@kaist.ac.kr"], "keywords": ["Deep learning", "Stereoscopic view synthesis", "Monocular depth", "Deep 3D Pan"], "TL;DR": "Novel architecture for stereoscopic view synthesis at arbitrary camera shifts utilizing adaptive t-shaped kernels with adaptive dilations.", "abstract": "Recent advances in deep learning have shown promising results in many low-level vision tasks. However, solving the single-image-based view synthesis is still an open problem. In particular, the generation of new images at parallel camera views given a single input image is of great interest, as it enables 3D visualization of the 2D input scenery. We propose a novel network architecture to perform stereoscopic view synthesis at arbitrary camera positions along the X-axis, or \u201cDeep 3D Pan\u201d, with \u201ct-shaped\u201d adaptive kernels equipped with globally and locally adaptive dilations. Our proposed network architecture, the monster-net, is devised with a novel t-shaped adaptive kernel with globally and locally adaptive dilation, which can efficiently incorporate global camera shift into and handle local 3D geometries of the target image\u2019s pixels for the synthesis of naturally looking 3D panned views when a 2-D input image is given. Extensive experiments were performed on the KITTI, CityScapes, and our VICLAB_STEREO indoors dataset to prove the efficacy of our method. Our monster-net significantly outperforms the state-of-the-art method (SOTA) by a large margin in all metrics of RMSE, PSNR, and SSIM. Our proposed monster-net is capable of reconstructing more reliable image structures in synthesized images with coherent geometry. Moreover, the disparity information that can be extracted from the \u201ct-shaped\u201d kernel is much more reliable than that of the SOTA for the unsupervised monocular depth estimation task, confirming the effectiveness of our method.", "pdf": "/pdf/22ea94ce6d68bfcff55d49ecb4d5df171bf908a2.pdf", "paperhash": "bello|deep_3d_pan_via_local_adaptive_tshaped_convolutions_with_global_and_local_adaptive_dilations", "_bibtex": "@inproceedings{\nBello2020Deep,\ntitle={Deep 3D Pan via local adaptive \"t-shaped\" convolutions with global and local adaptive dilations},\nauthor={Juan Luis Gonzalez Bello and Munchurl Kim},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=B1gF56VYPH}\n}", "original_pdf": "/attachment/a0c343551ef295e37a0296d2f0b0d40e4b3e769c.pdf"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "B1gF56VYPH", "replyto": "B1gF56VYPH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper714/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper714/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576478284735, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper714/Reviewers"], "noninvitees": [], "tcdate": 1570237748155, "tmdate": 1576478284748, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper714/-/Official_Review"}}}], "count": 8}