{"notes": [{"id": "rJg8yhAqKm", "original": "r1lw89YqKX", "number": 989, "cdate": 1538087902183, "ddate": null, "tcdate": 1538087902183, "tmdate": 1550863656461, "tddate": null, "forum": "rJg8yhAqKm", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "InfoBot: Transfer and Exploration via the Information Bottleneck", "abstract": "A central challenge in reinforcement learning is discovering effective policies for tasks where rewards are sparsely distributed. We postulate that in the absence of useful reward signals, an effective exploration strategy should seek out {\\it decision states}. These states lie at critical junctions in the state space from where the agent can transition to new, potentially unexplored regions. We propose to learn about decision states from prior experience. By training a goal-conditioned model with an information bottleneck, we can identify decision states by examining where the model accesses the goal state through the bottleneck. We find that this simple mechanism effectively identifies decision states, even in partially observed settings. In effect, the model learns the sensory cues that correlate with potential subgoals. In new environments, this model can then identify novel subgoals for further exploration, guiding the agent through a sequence of potential decision  states and through new regions of the state space.", "keywords": ["Information bottleneck", "policy transfer", "policy generalization", "exploration"], "authorids": ["anirudhgoyal9119@gmail.com", "riashat.islam@mail.mcgill.ca", "danieljstrouse@gmail.com", "zafarali.ahmed@mail.mcgill.ca", "hugolarochelle@google.com", "botvinick@google.com", "svlevine@eecs.berkeley.edu", "yoshua.bengio@mila.quebec"], "authors": ["Anirudh Goyal", "Riashat Islam", "DJ Strouse", "Zafarali Ahmed", "Hugo Larochelle", "Matthew Botvinick", "Yoshua Bengio", "Sergey Levine"], "TL;DR": "Training agents with goal-policy information bottlenecks promotes transfer and yields a powerful exploration bonus", "pdf": "/pdf/9bfd1fe1d796c1ebab86fec8129f320412daf1b4.pdf", "paperhash": "goyal|infobot_transfer_and_exploration_via_the_information_bottleneck", "_bibtex": "@inproceedings{\ngoyal2018transfer,\ntitle={Transfer and Exploration via the Information Bottleneck},\nauthor={Anirudh Goyal and Riashat Islam and DJ Strouse and Zafarali Ahmed and Hugo Larochelle and Matthew Botvinick and Sergey Levine and Yoshua Bengio},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJg8yhAqKm},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 33, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "r1xm0DSzl4", "original": null, "number": 1, "cdate": 1544865738532, "ddate": null, "tcdate": 1544865738532, "tmdate": 1545354533272, "tddate": null, "forum": "rJg8yhAqKm", "replyto": "rJg8yhAqKm", "invitation": "ICLR.cc/2019/Conference/-/Paper989/Meta_Review", "content": {"metareview": "The paper presents the use of information bottlenecks as a way to identify key \"decision states\" in exploration, in a goal-conditioned model. The concept of \"decision states\" is actually common in RL, states where exploring can lead to very diverse/new states. The implementation of the \"information bottleneck\" is done by adding a regularizing term, the conditional mutual information I(A;G|S).\n\nThe main weaknesses of the paper were its lack of clarity and the experimental section. It seems to me that the rebuttals, and the additional experiments and details, made the paper worthy of publication. The authors cleared enough of the gray areas and showcased the relative merits of the methods.", "confidence": "3: The area chair is somewhat confident", "recommendation": "Accept (Poster)", "title": "An interesting link between generalization and exploration"}, "signatures": ["ICLR.cc/2019/Conference/Paper989/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper989/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "InfoBot: Transfer and Exploration via the Information Bottleneck", "abstract": "A central challenge in reinforcement learning is discovering effective policies for tasks where rewards are sparsely distributed. We postulate that in the absence of useful reward signals, an effective exploration strategy should seek out {\\it decision states}. These states lie at critical junctions in the state space from where the agent can transition to new, potentially unexplored regions. We propose to learn about decision states from prior experience. By training a goal-conditioned model with an information bottleneck, we can identify decision states by examining where the model accesses the goal state through the bottleneck. We find that this simple mechanism effectively identifies decision states, even in partially observed settings. In effect, the model learns the sensory cues that correlate with potential subgoals. In new environments, this model can then identify novel subgoals for further exploration, guiding the agent through a sequence of potential decision  states and through new regions of the state space.", "keywords": ["Information bottleneck", "policy transfer", "policy generalization", "exploration"], "authorids": ["anirudhgoyal9119@gmail.com", "riashat.islam@mail.mcgill.ca", "danieljstrouse@gmail.com", "zafarali.ahmed@mail.mcgill.ca", "hugolarochelle@google.com", "botvinick@google.com", "svlevine@eecs.berkeley.edu", "yoshua.bengio@mila.quebec"], "authors": ["Anirudh Goyal", "Riashat Islam", "DJ Strouse", "Zafarali Ahmed", "Hugo Larochelle", "Matthew Botvinick", "Yoshua Bengio", "Sergey Levine"], "TL;DR": "Training agents with goal-policy information bottlenecks promotes transfer and yields a powerful exploration bonus", "pdf": "/pdf/9bfd1fe1d796c1ebab86fec8129f320412daf1b4.pdf", "paperhash": "goyal|infobot_transfer_and_exploration_via_the_information_bottleneck", "_bibtex": "@inproceedings{\ngoyal2018transfer,\ntitle={Transfer and Exploration via the Information Bottleneck},\nauthor={Anirudh Goyal and Riashat Islam and DJ Strouse and Zafarali Ahmed and Hugo Larochelle and Matthew Botvinick and Sergey Levine and Yoshua Bengio},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJg8yhAqKm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper989/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353009876, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJg8yhAqKm", "replyto": "rJg8yhAqKm", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper989/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper989/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper989/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353009876}}}, {"id": "ryeX1rXrlV", "original": null, "number": 42, "cdate": 1545053402634, "ddate": null, "tcdate": 1545053402634, "tmdate": 1545072551047, "tddate": null, "forum": "rJg8yhAqKm", "replyto": "SJxWVtZ5pX", "invitation": "ICLR.cc/2019/Conference/-/Paper989/Official_Comment", "content": {"title": "Thanks! ", "comment": "The authors thank the reviewer for reading the rebuttal, and increasing their score.\n\nThanks for your time! "}, "signatures": ["ICLR.cc/2019/Conference/Paper989/Authors"], "readers": ["ICLR.cc/2019/Conference/Paper989/Authors", "everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper989/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper989/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "InfoBot: Transfer and Exploration via the Information Bottleneck", "abstract": "A central challenge in reinforcement learning is discovering effective policies for tasks where rewards are sparsely distributed. We postulate that in the absence of useful reward signals, an effective exploration strategy should seek out {\\it decision states}. These states lie at critical junctions in the state space from where the agent can transition to new, potentially unexplored regions. We propose to learn about decision states from prior experience. By training a goal-conditioned model with an information bottleneck, we can identify decision states by examining where the model accesses the goal state through the bottleneck. We find that this simple mechanism effectively identifies decision states, even in partially observed settings. In effect, the model learns the sensory cues that correlate with potential subgoals. In new environments, this model can then identify novel subgoals for further exploration, guiding the agent through a sequence of potential decision  states and through new regions of the state space.", "keywords": ["Information bottleneck", "policy transfer", "policy generalization", "exploration"], "authorids": ["anirudhgoyal9119@gmail.com", "riashat.islam@mail.mcgill.ca", "danieljstrouse@gmail.com", "zafarali.ahmed@mail.mcgill.ca", "hugolarochelle@google.com", "botvinick@google.com", "svlevine@eecs.berkeley.edu", "yoshua.bengio@mila.quebec"], "authors": ["Anirudh Goyal", "Riashat Islam", "DJ Strouse", "Zafarali Ahmed", "Hugo Larochelle", "Matthew Botvinick", "Yoshua Bengio", "Sergey Levine"], "TL;DR": "Training agents with goal-policy information bottlenecks promotes transfer and yields a powerful exploration bonus", "pdf": "/pdf/9bfd1fe1d796c1ebab86fec8129f320412daf1b4.pdf", "paperhash": "goyal|infobot_transfer_and_exploration_via_the_information_bottleneck", "_bibtex": "@inproceedings{\ngoyal2018transfer,\ntitle={Transfer and Exploration via the Information Bottleneck},\nauthor={Anirudh Goyal and Riashat Islam and DJ Strouse and Zafarali Ahmed and Hugo Larochelle and Matthew Botvinick and Sergey Levine and Yoshua Bengio},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJg8yhAqKm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper989/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621617664, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJg8yhAqKm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper989/Authors", "ICLR.cc/2019/Conference/Paper989/Reviewers", "ICLR.cc/2019/Conference/Paper989/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper989/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper989/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper989/Authors|ICLR.cc/2019/Conference/Paper989/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper989/Reviewers", "ICLR.cc/2019/Conference/Paper989/Authors", "ICLR.cc/2019/Conference/Paper989/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621617664}}}, {"id": "SJxWVtZ5pX", "original": null, "number": 3, "cdate": 1542228265181, "ddate": null, "tcdate": 1542228265181, "tmdate": 1544940765651, "tddate": null, "forum": "rJg8yhAqKm", "replyto": "rJg8yhAqKm", "invitation": "ICLR.cc/2019/Conference/-/Paper989/Official_Review", "content": {"title": "Review Number 3 (So sorry for the delay!)", "review": "The authors propose a new regularizer for policy search in a multi-goal RL setting. The objective promotes a more efficient exploration strategy by encouraging the agent to learn policies that depend as little as possible on the target goal. This is achieved by regularizing standard RL losses with the negative conditional mutual information I(A;G|S). Although this regularizer cannot be optimize, the authors propose a tractable bound. The net effect of this regularizer is to promote more effective exploration by encouraging the agent to visit decision states, in which goal-depend decisions play a more important role. The idea of using this particular regularizer is inspired by an existing line of work on the information bottleneck.\n\nI find the idea proposed by the authors to be interesting. However, I have the following concerns, and overall I think this paper is borderline.\n\n1. The quality of the experimental validation provided by the authors is in my opinion borderline acceptable. Although the method performs better on toy settings, it seems barely better on more challenging ones. Experiments in section 4.5 lack detail and context.\n2. The clarity of the presentation is also not great.\n    2.1. The two-stage nature of the method was confusing to me. I didn\u2019t understand the role of the second stage. Most focus is on the first stage, and only very little on the second stage. For example, I was confused about why the sign of the regularizer was flipped.\n    2.2. I was confused by how exactly the bounds (3) and (4) we applied and in what order.\n    2.3. I think the intuition of the method could be better explained and better validated by experiments.\n\nI also have the following additional comments:\n* How is the regularizer applied with other policy search algorithms besides Reinforce? Was it done in the paper? I can\u2019t say for sure. Specifically, when comparing to PPO, was the algorithm compared to a version of PPO augmented with this regularizer? Why yes or why no?\n* More generally, experiments where more modern policy search algorithms are combined with the regularizer would be helpful. In particular, does it matter which policy search algorithm we use with this method?\n* Experimental plots in section 4.4 are missing error bars, and I can\u2019t tell if the results are significant without them.\n* I thought the motivation for choosing this regularizer was lacking. The authors cite the information bottleneck literature, but we shouldn\u2019t need to read all these papers, the main ideas should be summarized here.\n* The argument for how the regularizer improves exploration seemed to me very hand-wavy and not well substantiated by experiments.\n* I would love to see a better discussion of how the method is useful when he RL setting is not truly multi-goal.\n* The second part of the algorithm needs to be explained much more clearly.\n* What is the effect of the approximation on Q?\n\n---\n\nI have read the response of the authors, and they have addressed a significant numbers of concerns that I had. I am upgrading my rating to a 7.", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper989/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "InfoBot: Transfer and Exploration via the Information Bottleneck", "abstract": "A central challenge in reinforcement learning is discovering effective policies for tasks where rewards are sparsely distributed. We postulate that in the absence of useful reward signals, an effective exploration strategy should seek out {\\it decision states}. These states lie at critical junctions in the state space from where the agent can transition to new, potentially unexplored regions. We propose to learn about decision states from prior experience. By training a goal-conditioned model with an information bottleneck, we can identify decision states by examining where the model accesses the goal state through the bottleneck. We find that this simple mechanism effectively identifies decision states, even in partially observed settings. In effect, the model learns the sensory cues that correlate with potential subgoals. In new environments, this model can then identify novel subgoals for further exploration, guiding the agent through a sequence of potential decision  states and through new regions of the state space.", "keywords": ["Information bottleneck", "policy transfer", "policy generalization", "exploration"], "authorids": ["anirudhgoyal9119@gmail.com", "riashat.islam@mail.mcgill.ca", "danieljstrouse@gmail.com", "zafarali.ahmed@mail.mcgill.ca", "hugolarochelle@google.com", "botvinick@google.com", "svlevine@eecs.berkeley.edu", "yoshua.bengio@mila.quebec"], "authors": ["Anirudh Goyal", "Riashat Islam", "DJ Strouse", "Zafarali Ahmed", "Hugo Larochelle", "Matthew Botvinick", "Yoshua Bengio", "Sergey Levine"], "TL;DR": "Training agents with goal-policy information bottlenecks promotes transfer and yields a powerful exploration bonus", "pdf": "/pdf/9bfd1fe1d796c1ebab86fec8129f320412daf1b4.pdf", "paperhash": "goyal|infobot_transfer_and_exploration_via_the_information_bottleneck", "_bibtex": "@inproceedings{\ngoyal2018transfer,\ntitle={Transfer and Exploration via the Information Bottleneck},\nauthor={Anirudh Goyal and Riashat Islam and DJ Strouse and Zafarali Ahmed and Hugo Larochelle and Matthew Botvinick and Sergey Levine and Yoshua Bengio},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJg8yhAqKm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper989/Official_Review", "cdate": 1542234331552, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "rJg8yhAqKm", "replyto": "rJg8yhAqKm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper989/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335848093, "tmdate": 1552335848093, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper989/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "B1ls9HHb1V", "original": null, "number": 34, "cdate": 1543751059004, "ddate": null, "tcdate": 1543751059004, "tmdate": 1543751059004, "tddate": null, "forum": "rJg8yhAqKm", "replyto": "rJg8yhAqKm", "invitation": "ICLR.cc/2019/Conference/-/Paper989/Official_Comment", "content": {"title": "Final Rebuttal Summary", "comment": "Since today is the last day for the discussion period,  we want to summarize our rebuttal.\n\nFirst, We want to thank all reviewers for their critical feedback and suggestions, which has already helped us improve the paper\u2019s clarity and presentation. Both the reviewers (R1 and R2) agree that the paper tackles an interesting  problem. The main concerns were about the clarity of the writing (R1 and R3) , making it hard to clearly assess the underlying contributions, and about the diversity of the experimental results. (all the reviewers).\n\nWe aim to propose an algorithm whereby we incentive agents to learn task structure by training policies that perform well under a variety of goals, while not overfitting to any individual goal. We achieve this by training agents that, in addition to maximizing reward, minimize the policy dependence on the individual goal, quantified by the conditional mutual information  I(A; G | S).  Thus, we claim that adding this mutual information acts as a regularizer which promotes generalization across tasks, and it helps in achieve better exploration.\n\nWe show that the proposed method generalizes better by conducting experiments on MANY different problems during the rebuttal time (Maze based navigation, better regularization for multi-agent communication, better policy for instruction following, middle ground b/w model free and model based methods, comparisons to state of the art off policy methods), as well as we show that adding this specific term  helps the model to identify novel subgoals for further exploration, guiding the agent through a sequence of potential decision  states and through new regions of the state space. Thus, we show experimental results to back up our claims in our submission. \n\n============================\n\n-  The key point in R3 and R2 review was lack of experimental results. We believe that we did conducted many more experiments to show that adding I(A; G|S) helps in better generalization as well as exploration. And we are also thankful to R2 as they increased their score. \n\n- The key point in R1's review was lack of clarity (before the revision period) which we believe we have tried our best to address. And then after the revision period, the reviewer asked comparison to DISTRAL and clarity about Atari results, which we again believe we have addressed.\n\nWe urge R1 and R3  to read our rebuttal as we have carefully addressed and incorporated all critical feedback and suggestions. \n\nThanks very much for your time, and feedback which has improved the presentation of our paper! :-) "}, "signatures": ["ICLR.cc/2019/Conference/Paper989/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper989/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper989/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "InfoBot: Transfer and Exploration via the Information Bottleneck", "abstract": "A central challenge in reinforcement learning is discovering effective policies for tasks where rewards are sparsely distributed. We postulate that in the absence of useful reward signals, an effective exploration strategy should seek out {\\it decision states}. These states lie at critical junctions in the state space from where the agent can transition to new, potentially unexplored regions. We propose to learn about decision states from prior experience. By training a goal-conditioned model with an information bottleneck, we can identify decision states by examining where the model accesses the goal state through the bottleneck. We find that this simple mechanism effectively identifies decision states, even in partially observed settings. In effect, the model learns the sensory cues that correlate with potential subgoals. In new environments, this model can then identify novel subgoals for further exploration, guiding the agent through a sequence of potential decision  states and through new regions of the state space.", "keywords": ["Information bottleneck", "policy transfer", "policy generalization", "exploration"], "authorids": ["anirudhgoyal9119@gmail.com", "riashat.islam@mail.mcgill.ca", "danieljstrouse@gmail.com", "zafarali.ahmed@mail.mcgill.ca", "hugolarochelle@google.com", "botvinick@google.com", "svlevine@eecs.berkeley.edu", "yoshua.bengio@mila.quebec"], "authors": ["Anirudh Goyal", "Riashat Islam", "DJ Strouse", "Zafarali Ahmed", "Hugo Larochelle", "Matthew Botvinick", "Yoshua Bengio", "Sergey Levine"], "TL;DR": "Training agents with goal-policy information bottlenecks promotes transfer and yields a powerful exploration bonus", "pdf": "/pdf/9bfd1fe1d796c1ebab86fec8129f320412daf1b4.pdf", "paperhash": "goyal|infobot_transfer_and_exploration_via_the_information_bottleneck", "_bibtex": "@inproceedings{\ngoyal2018transfer,\ntitle={Transfer and Exploration via the Information Bottleneck},\nauthor={Anirudh Goyal and Riashat Islam and DJ Strouse and Zafarali Ahmed and Hugo Larochelle and Matthew Botvinick and Sergey Levine and Yoshua Bengio},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJg8yhAqKm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper989/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621617664, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJg8yhAqKm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper989/Authors", "ICLR.cc/2019/Conference/Paper989/Reviewers", "ICLR.cc/2019/Conference/Paper989/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper989/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper989/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper989/Authors|ICLR.cc/2019/Conference/Paper989/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper989/Reviewers", "ICLR.cc/2019/Conference/Paper989/Authors", "ICLR.cc/2019/Conference/Paper989/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621617664}}}, {"id": "SJxUd48kk4", "original": null, "number": 32, "cdate": 1543623789732, "ddate": null, "tcdate": 1543623789732, "tmdate": 1543624312694, "tddate": null, "forum": "rJg8yhAqKm", "replyto": "BJxrL1eLoQ", "invitation": "ICLR.cc/2019/Conference/-/Paper989/Official_Comment", "content": {"title": "Kind request to respond for Reviewer 1", "comment": "Dear Reviewer 1,\n\nWe thank you again for your informative review that you wrote before the revision period. In our response  we tried our best to address your concerns.  Your feedback has already been very helpful in improving the paper.\n\nThe problem statement is quite simple: we aim to propose an algorithm whereby we incentive agents to learn task structure by training policies that perform well under a variety of goals, while not overfitting to any individual goal. We achieve this by training agents that, in addition to maximizing reward, minimize the policy dependence on the individual goal, quantified by the conditional mutual information  I(A; G | S). We show that the proposed method generalizes better by conducting experiments on MANY different problems (Maze based navigation, better regularization for multi-agent communication, better policy for instruction following, middle ground b/w model free and model based methods), as well as we show that adding this specific term  helps the model to identify novel subgoals for further exploration, guiding the agent through a sequence of potential decision  states and through new regions of the state space.\n\nWe would highly appreciate to get some feedback from you regarding the changes that we have made and the extra experiments we conducted during the revision period.  In particular, we changed the introduction as you mentioned, and removed references to \"useful habits\". Then after the revision period you asked the difference b/w the proposed method and DISTRAL (see heading \"Difference with Distral (1/2)\"), which we believe that we have clarified. And we also clarified the Atari results (see heading, \"Clarification regarding Atari Results (2/2)\") , and acknowledge that due to the way we have presented these results,  it might have made results a bit difficult to interpret. \n\nSince, the discussion period is coming to end, and the reviewer might also get busy due to NeurIPS, we would highly appreciate a response  and suggestions on how it could be improved. If you still think that paper is uninteresting or not well executed, could you then suggest what specifically it is lacking? Or what result you are looking for essentially ?\n\nWe are sincerely hoping to hear from you. We really want to do our best to make sure, we can agree regarding the novelty and clarity of the proposed approach. Thanks very much for your time! :)\n\nThe Authors\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper989/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper989/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper989/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "InfoBot: Transfer and Exploration via the Information Bottleneck", "abstract": "A central challenge in reinforcement learning is discovering effective policies for tasks where rewards are sparsely distributed. We postulate that in the absence of useful reward signals, an effective exploration strategy should seek out {\\it decision states}. These states lie at critical junctions in the state space from where the agent can transition to new, potentially unexplored regions. We propose to learn about decision states from prior experience. By training a goal-conditioned model with an information bottleneck, we can identify decision states by examining where the model accesses the goal state through the bottleneck. We find that this simple mechanism effectively identifies decision states, even in partially observed settings. In effect, the model learns the sensory cues that correlate with potential subgoals. In new environments, this model can then identify novel subgoals for further exploration, guiding the agent through a sequence of potential decision  states and through new regions of the state space.", "keywords": ["Information bottleneck", "policy transfer", "policy generalization", "exploration"], "authorids": ["anirudhgoyal9119@gmail.com", "riashat.islam@mail.mcgill.ca", "danieljstrouse@gmail.com", "zafarali.ahmed@mail.mcgill.ca", "hugolarochelle@google.com", "botvinick@google.com", "svlevine@eecs.berkeley.edu", "yoshua.bengio@mila.quebec"], "authors": ["Anirudh Goyal", "Riashat Islam", "DJ Strouse", "Zafarali Ahmed", "Hugo Larochelle", "Matthew Botvinick", "Yoshua Bengio", "Sergey Levine"], "TL;DR": "Training agents with goal-policy information bottlenecks promotes transfer and yields a powerful exploration bonus", "pdf": "/pdf/9bfd1fe1d796c1ebab86fec8129f320412daf1b4.pdf", "paperhash": "goyal|infobot_transfer_and_exploration_via_the_information_bottleneck", "_bibtex": "@inproceedings{\ngoyal2018transfer,\ntitle={Transfer and Exploration via the Information Bottleneck},\nauthor={Anirudh Goyal and Riashat Islam and DJ Strouse and Zafarali Ahmed and Hugo Larochelle and Matthew Botvinick and Sergey Levine and Yoshua Bengio},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJg8yhAqKm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper989/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621617664, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJg8yhAqKm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper989/Authors", "ICLR.cc/2019/Conference/Paper989/Reviewers", "ICLR.cc/2019/Conference/Paper989/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper989/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper989/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper989/Authors|ICLR.cc/2019/Conference/Paper989/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper989/Reviewers", "ICLR.cc/2019/Conference/Paper989/Authors", "ICLR.cc/2019/Conference/Paper989/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621617664}}}, {"id": "SJlJ48UJJE", "original": null, "number": 33, "cdate": 1543624231137, "ddate": null, "tcdate": 1543624231137, "tmdate": 1543624231137, "tddate": null, "forum": "rJg8yhAqKm", "replyto": "SJxWVtZ5pX", "invitation": "ICLR.cc/2019/Conference/-/Paper989/Official_Comment", "content": {"title": "Kind request to respond for Reviewer 3 ", "comment": "Dear Reviewer 3,\n\nWe thank you again for your informative review that you wrote before the revision period. In our response  we tried our best to address your concerns.  Your feedback has already been very helpful in improving the paper.\n\nThe problem statement is quite simple: we aim to propose an algorithm whereby we incentive agents to learn task structure by training policies that perform well under a variety of goals, while not overfitting to any individual goal. We achieve this by training agents that, in addition to maximizing reward, minimize the policy dependence on the individual goal, quantified by the conditional mutual information  I(A; G | S). We show that the proposed method generalizes better by conducting experiments on MANY different problems (Maze based navigation, better regularization for multi-agent communication, better policy for instruction following, middle ground b/w model free and model based methods), as well as we show that adding this specific term  helps the model to identify novel subgoals for further exploration, guiding the agent through a sequence of potential decision  states and through new regions of the state space.\n\nSince, the discussion period is coming to end, and the reviewer might also get busy due to NeurIPS, we would highly appreciate a response  and suggestions on how it could be improved. If you still think that paper is not well executed, could you then suggest what specifically it is lacking? Or what result you are looking for essentially ?\n\nWe are sincerely hoping to hear from you.  Thanks very much for your time! :)\n\nThe Authors"}, "signatures": ["ICLR.cc/2019/Conference/Paper989/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper989/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper989/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "InfoBot: Transfer and Exploration via the Information Bottleneck", "abstract": "A central challenge in reinforcement learning is discovering effective policies for tasks where rewards are sparsely distributed. We postulate that in the absence of useful reward signals, an effective exploration strategy should seek out {\\it decision states}. These states lie at critical junctions in the state space from where the agent can transition to new, potentially unexplored regions. We propose to learn about decision states from prior experience. By training a goal-conditioned model with an information bottleneck, we can identify decision states by examining where the model accesses the goal state through the bottleneck. We find that this simple mechanism effectively identifies decision states, even in partially observed settings. In effect, the model learns the sensory cues that correlate with potential subgoals. In new environments, this model can then identify novel subgoals for further exploration, guiding the agent through a sequence of potential decision  states and through new regions of the state space.", "keywords": ["Information bottleneck", "policy transfer", "policy generalization", "exploration"], "authorids": ["anirudhgoyal9119@gmail.com", "riashat.islam@mail.mcgill.ca", "danieljstrouse@gmail.com", "zafarali.ahmed@mail.mcgill.ca", "hugolarochelle@google.com", "botvinick@google.com", "svlevine@eecs.berkeley.edu", "yoshua.bengio@mila.quebec"], "authors": ["Anirudh Goyal", "Riashat Islam", "DJ Strouse", "Zafarali Ahmed", "Hugo Larochelle", "Matthew Botvinick", "Yoshua Bengio", "Sergey Levine"], "TL;DR": "Training agents with goal-policy information bottlenecks promotes transfer and yields a powerful exploration bonus", "pdf": "/pdf/9bfd1fe1d796c1ebab86fec8129f320412daf1b4.pdf", "paperhash": "goyal|infobot_transfer_and_exploration_via_the_information_bottleneck", "_bibtex": "@inproceedings{\ngoyal2018transfer,\ntitle={Transfer and Exploration via the Information Bottleneck},\nauthor={Anirudh Goyal and Riashat Islam and DJ Strouse and Zafarali Ahmed and Hugo Larochelle and Matthew Botvinick and Sergey Levine and Yoshua Bengio},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJg8yhAqKm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper989/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621617664, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJg8yhAqKm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper989/Authors", "ICLR.cc/2019/Conference/Paper989/Reviewers", "ICLR.cc/2019/Conference/Paper989/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper989/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper989/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper989/Authors|ICLR.cc/2019/Conference/Paper989/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper989/Reviewers", "ICLR.cc/2019/Conference/Paper989/Authors", "ICLR.cc/2019/Conference/Paper989/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621617664}}}, {"id": "SklrGw820m", "original": null, "number": 30, "cdate": 1543427853175, "ddate": null, "tcdate": 1543427853175, "tmdate": 1543428269704, "tddate": null, "forum": "rJg8yhAqKm", "replyto": "HJlTHXViAX", "invitation": "ICLR.cc/2019/Conference/-/Paper989/Official_Comment", "content": {"title": "Feedback by reviewer very helpful! :)", "comment": "We appreciate the reviewer's responsiveness in helping us improve our paper. We would like to know if our response adequately addressed your concerns.  Are there any other aspects of the paper that you think could be improved?\n\n==================================================\n\nWe have shown that the proposed method generalizes better \n- By testing it on gym-minigrid as well as on goal based minipacman, and comparing it with exploration baselines, hierarchical baselines.\n- By showing better regularization for the scenario, when the goal is represented by language.\n- Using the proposed regularizer for multi-agent communication. \n\nWe show that the proposed method could be useful for exploration\n- Showing that using exploration bonus helps to train a policy from scratch as on gym-minigrid framework.\n- By showing that the policy trained with exploration bonus visits more number of states.\n- Some evidence that it could also be used for atari games.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper989/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper989/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper989/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "InfoBot: Transfer and Exploration via the Information Bottleneck", "abstract": "A central challenge in reinforcement learning is discovering effective policies for tasks where rewards are sparsely distributed. We postulate that in the absence of useful reward signals, an effective exploration strategy should seek out {\\it decision states}. These states lie at critical junctions in the state space from where the agent can transition to new, potentially unexplored regions. We propose to learn about decision states from prior experience. By training a goal-conditioned model with an information bottleneck, we can identify decision states by examining where the model accesses the goal state through the bottleneck. We find that this simple mechanism effectively identifies decision states, even in partially observed settings. In effect, the model learns the sensory cues that correlate with potential subgoals. In new environments, this model can then identify novel subgoals for further exploration, guiding the agent through a sequence of potential decision  states and through new regions of the state space.", "keywords": ["Information bottleneck", "policy transfer", "policy generalization", "exploration"], "authorids": ["anirudhgoyal9119@gmail.com", "riashat.islam@mail.mcgill.ca", "danieljstrouse@gmail.com", "zafarali.ahmed@mail.mcgill.ca", "hugolarochelle@google.com", "botvinick@google.com", "svlevine@eecs.berkeley.edu", "yoshua.bengio@mila.quebec"], "authors": ["Anirudh Goyal", "Riashat Islam", "DJ Strouse", "Zafarali Ahmed", "Hugo Larochelle", "Matthew Botvinick", "Yoshua Bengio", "Sergey Levine"], "TL;DR": "Training agents with goal-policy information bottlenecks promotes transfer and yields a powerful exploration bonus", "pdf": "/pdf/9bfd1fe1d796c1ebab86fec8129f320412daf1b4.pdf", "paperhash": "goyal|infobot_transfer_and_exploration_via_the_information_bottleneck", "_bibtex": "@inproceedings{\ngoyal2018transfer,\ntitle={Transfer and Exploration via the Information Bottleneck},\nauthor={Anirudh Goyal and Riashat Islam and DJ Strouse and Zafarali Ahmed and Hugo Larochelle and Matthew Botvinick and Sergey Levine and Yoshua Bengio},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJg8yhAqKm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper989/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621617664, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJg8yhAqKm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper989/Authors", "ICLR.cc/2019/Conference/Paper989/Reviewers", "ICLR.cc/2019/Conference/Paper989/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper989/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper989/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper989/Authors|ICLR.cc/2019/Conference/Paper989/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper989/Reviewers", "ICLR.cc/2019/Conference/Paper989/Authors", "ICLR.cc/2019/Conference/Paper989/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621617664}}}, {"id": "HJlTHXViAX", "original": null, "number": 29, "cdate": 1543353156550, "ddate": null, "tcdate": 1543353156550, "tmdate": 1543405492889, "tddate": null, "forum": "rJg8yhAqKm", "replyto": "HJeBJbNjRm", "invitation": "ICLR.cc/2019/Conference/-/Paper989/Official_Comment", "content": {"title": "Clarification regarding Atari Results (2/2)", "comment": "We note that our goal here is not to achieve state of the art results. We want to just say that by using the proposed method, it can help to achieve better results as compared to the baseline (a2c baseline), and hence learning a better exploratory policy.\n\nWe want to acknowledge that for atari results, we were plotting normalized score (returned by the gym framework) and hence because of this presentation error, it might have made the result difficult to interpret. We are again, mentioning the results here so that it becomes easier for the reviewer to interpret. We use the same code (a bit old version) of the open source version of  this code base. https://github.com/ikostrikov/pytorch-a2c-ppo-acktr And hence we can say with confidence that our A2C baseline is more or less the same as a \"good\" A2C baseline.\n\nMethod                               Pong             Qbert               Seaquest          Breakout      Freeway    MsPacMan  \n\nA2c (Ours)                           ~5                  ~3000               ~1400                  ~250                ~0               1700                       \nA2c(openai baselines)      ~10                ~3000                ~1500                  ~100                ~0                 ---\nProposed Method             ~17                 ~4000               ~1790                  ~400                ~21             2100                                                \n\nWe do note that for Pong our baseline is bit worse but besides having a bad baseline we can outperform the right baseline by the openai blog which reviewer mentioned. All these results are approximately after 5M frames. And seaquest, MsPacman is most definitely a game that requires exploration (Ideally, all RL tasks \"require\" exploration technically) (as said by [1]). We would update the results.   As we can see, that by using the proposed method in this way, it helps to train policy from scratch faster as compared to a2c baseline. \n\nWe also did more experiments for exploration where we achieve better results as compared to the Curiosity baseline, as well as VIME, both of which we believe to be strong baseline for gym-minigrid framework. \n\nWe also compared the proposed method on sparse reward mujoco tasks, where we compared to state of the art off policy method (Soft actor Critic).\n\n\n[1]  Deep exploration by Bootstrapped DQN. (https://arxiv.org/abs/1602.04621) \n\nFuthermore, Atari results were to demonstrate that the proposed method would also be applicable where there is no \"general\" notion of goals. We show that by using the exploration bonus we can successfully train a policy from scratch even in partial observable case, which was not possible otherwise. Here, we compare to number of strong baselines like curiosity driven as well as VIME. \nWe also show (in the appendix, by using the heat map of visited states) that by using the exploration bonus, agent visits more diverse states as compared to without the exploration bonus. Hence, by training a goal-conditioned model with an information bottleneck, we can identify decision states by examining where the model accesses the goal state through the bottleneck, and  in the new environments, we can then identify novel subgoals for further exploration, guiding the agent through a sequence of potential decision  states and through new regions of the state space.\n\n\nWe also show that the proposed method shows better direct transfer of policies as compared to the state of the art hierarchical methods. \n"}, "signatures": ["ICLR.cc/2019/Conference/Paper989/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper989/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper989/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "InfoBot: Transfer and Exploration via the Information Bottleneck", "abstract": "A central challenge in reinforcement learning is discovering effective policies for tasks where rewards are sparsely distributed. We postulate that in the absence of useful reward signals, an effective exploration strategy should seek out {\\it decision states}. These states lie at critical junctions in the state space from where the agent can transition to new, potentially unexplored regions. We propose to learn about decision states from prior experience. By training a goal-conditioned model with an information bottleneck, we can identify decision states by examining where the model accesses the goal state through the bottleneck. We find that this simple mechanism effectively identifies decision states, even in partially observed settings. In effect, the model learns the sensory cues that correlate with potential subgoals. In new environments, this model can then identify novel subgoals for further exploration, guiding the agent through a sequence of potential decision  states and through new regions of the state space.", "keywords": ["Information bottleneck", "policy transfer", "policy generalization", "exploration"], "authorids": ["anirudhgoyal9119@gmail.com", "riashat.islam@mail.mcgill.ca", "danieljstrouse@gmail.com", "zafarali.ahmed@mail.mcgill.ca", "hugolarochelle@google.com", "botvinick@google.com", "svlevine@eecs.berkeley.edu", "yoshua.bengio@mila.quebec"], "authors": ["Anirudh Goyal", "Riashat Islam", "DJ Strouse", "Zafarali Ahmed", "Hugo Larochelle", "Matthew Botvinick", "Yoshua Bengio", "Sergey Levine"], "TL;DR": "Training agents with goal-policy information bottlenecks promotes transfer and yields a powerful exploration bonus", "pdf": "/pdf/9bfd1fe1d796c1ebab86fec8129f320412daf1b4.pdf", "paperhash": "goyal|infobot_transfer_and_exploration_via_the_information_bottleneck", "_bibtex": "@inproceedings{\ngoyal2018transfer,\ntitle={Transfer and Exploration via the Information Bottleneck},\nauthor={Anirudh Goyal and Riashat Islam and DJ Strouse and Zafarali Ahmed and Hugo Larochelle and Matthew Botvinick and Sergey Levine and Yoshua Bengio},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJg8yhAqKm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper989/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621617664, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJg8yhAqKm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper989/Authors", "ICLR.cc/2019/Conference/Paper989/Reviewers", "ICLR.cc/2019/Conference/Paper989/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper989/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper989/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper989/Authors|ICLR.cc/2019/Conference/Paper989/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper989/Reviewers", "ICLR.cc/2019/Conference/Paper989/Authors", "ICLR.cc/2019/Conference/Paper989/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621617664}}}, {"id": "HJeBJbNjRm", "original": null, "number": 28, "cdate": 1543352540957, "ddate": null, "tcdate": 1543352540957, "tmdate": 1543356713116, "tddate": null, "forum": "rJg8yhAqKm", "replyto": "B1xKMzXsRQ", "invitation": "ICLR.cc/2019/Conference/-/Paper989/Official_Comment", "content": {"title": "Difference with  Distral (1/2)", "comment": "We appreciate the time the reviewer took in reading our paper, and also its encouraging to know that the reviewer find that by including the changes which the reviewer made has improved the clarity of the paper.\n\n\n\"Difference with Distral\"\n\nTo clarify -- the similarity with Whye Teh et al. is only in that both papers have a KL divergence term in the objective. But this term is used completely differently: Whye Teh et al. use a regularizer on the KL-divergence between action distributions of different policies to improve distillation, does not have any notion of goals, and is not concerned with exploration or with learning exploration strategies and transferring them to new domain. We use the variational information bottleneck, which has a KL divergence penalty on the difference between the posterior latent variable distribution and the prior (somewhat analogous to a VAE). We are not distilling multiple policies. Indeed, the two papers are almost completely unrelated. The phrasing in our related work section was clumsy in this regard, and will be revised, but we believe that even a cursory reading of our paper and the Whye Teh et al. paper will reveal that the two methods are very clearly distinct.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper989/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper989/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper989/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "InfoBot: Transfer and Exploration via the Information Bottleneck", "abstract": "A central challenge in reinforcement learning is discovering effective policies for tasks where rewards are sparsely distributed. We postulate that in the absence of useful reward signals, an effective exploration strategy should seek out {\\it decision states}. These states lie at critical junctions in the state space from where the agent can transition to new, potentially unexplored regions. We propose to learn about decision states from prior experience. By training a goal-conditioned model with an information bottleneck, we can identify decision states by examining where the model accesses the goal state through the bottleneck. We find that this simple mechanism effectively identifies decision states, even in partially observed settings. In effect, the model learns the sensory cues that correlate with potential subgoals. In new environments, this model can then identify novel subgoals for further exploration, guiding the agent through a sequence of potential decision  states and through new regions of the state space.", "keywords": ["Information bottleneck", "policy transfer", "policy generalization", "exploration"], "authorids": ["anirudhgoyal9119@gmail.com", "riashat.islam@mail.mcgill.ca", "danieljstrouse@gmail.com", "zafarali.ahmed@mail.mcgill.ca", "hugolarochelle@google.com", "botvinick@google.com", "svlevine@eecs.berkeley.edu", "yoshua.bengio@mila.quebec"], "authors": ["Anirudh Goyal", "Riashat Islam", "DJ Strouse", "Zafarali Ahmed", "Hugo Larochelle", "Matthew Botvinick", "Yoshua Bengio", "Sergey Levine"], "TL;DR": "Training agents with goal-policy information bottlenecks promotes transfer and yields a powerful exploration bonus", "pdf": "/pdf/9bfd1fe1d796c1ebab86fec8129f320412daf1b4.pdf", "paperhash": "goyal|infobot_transfer_and_exploration_via_the_information_bottleneck", "_bibtex": "@inproceedings{\ngoyal2018transfer,\ntitle={Transfer and Exploration via the Information Bottleneck},\nauthor={Anirudh Goyal and Riashat Islam and DJ Strouse and Zafarali Ahmed and Hugo Larochelle and Matthew Botvinick and Sergey Levine and Yoshua Bengio},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJg8yhAqKm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper989/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621617664, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJg8yhAqKm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper989/Authors", "ICLR.cc/2019/Conference/Paper989/Reviewers", "ICLR.cc/2019/Conference/Paper989/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper989/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper989/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper989/Authors|ICLR.cc/2019/Conference/Paper989/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper989/Reviewers", "ICLR.cc/2019/Conference/Paper989/Authors", "ICLR.cc/2019/Conference/Paper989/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621617664}}}, {"id": "B1xKMzXsRQ", "original": null, "number": 27, "cdate": 1543348752915, "ddate": null, "tcdate": 1543348752915, "tmdate": 1543348752915, "tddate": null, "forum": "rJg8yhAqKm", "replyto": "S1gQJLD907", "invitation": "ICLR.cc/2019/Conference/-/Paper989/Official_Comment", "content": {"title": "Read the updated paper, it's simply bad", "comment": "I read the updated paper and firmly stand by my recommendation to reject it. The new version is much clearer but unfortunately better writing reveals that it doesn't contain a good contribution.\n\nThe authors claim two contributions:\n1. A new regularizer that promotes generalization accross tasks.\n2. Better exploration.\n\nIn the related work section the authors state the following:\n\"Whye Teh et al. (2017) trained a policy with the same KL divergence term as in Eqn 1 for the\npurposes of encouraging transfer across tasks. They did not, however, note the connection to\nvariational information minimization and the information bottleneck, nor did they leverage the\nlearned task structure for exploration.\"\n\nSo the authors themselves admit they use the same method as Teh et al. who also target goal 1 above. So where is the novelty? The connection to variational information minimization is obvious and I have little doubt that Teh et al. were aware of it. In either case the authors don't derive any interesting insights from that connection. \n\nImprovements to exploration were not satisfactorily demonstrated in the experiments, which are generally quite bad. Just to pick one example, figure 6 claims to evalutate \u201ctransferable exploration strategies on Pong, Qbert, Seaquest and Breakout\u201d. However, none of these games require exploration. A purely greedy policy solves Pong faster than the proposed method. The baseline used also performs very well and it's not clear to me why. Comparing with existing results (http://htmlpreview.github.io/?https://github.com/openai/baselines/blob/master/benchmarks_atari10M.htm) the performance reported in the paper is simply tragic.\n\nIn conclusion, the authors contradict themselves on what the actual contribution is and they don't demonstrate the only potential novelty, namely improvements in exploration, either theoretically or experimentally. I recommend rejection of this paper.\n\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper989/AnonReviewer1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper989/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper989/AnonReviewer1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "InfoBot: Transfer and Exploration via the Information Bottleneck", "abstract": "A central challenge in reinforcement learning is discovering effective policies for tasks where rewards are sparsely distributed. We postulate that in the absence of useful reward signals, an effective exploration strategy should seek out {\\it decision states}. These states lie at critical junctions in the state space from where the agent can transition to new, potentially unexplored regions. We propose to learn about decision states from prior experience. By training a goal-conditioned model with an information bottleneck, we can identify decision states by examining where the model accesses the goal state through the bottleneck. We find that this simple mechanism effectively identifies decision states, even in partially observed settings. In effect, the model learns the sensory cues that correlate with potential subgoals. In new environments, this model can then identify novel subgoals for further exploration, guiding the agent through a sequence of potential decision  states and through new regions of the state space.", "keywords": ["Information bottleneck", "policy transfer", "policy generalization", "exploration"], "authorids": ["anirudhgoyal9119@gmail.com", "riashat.islam@mail.mcgill.ca", "danieljstrouse@gmail.com", "zafarali.ahmed@mail.mcgill.ca", "hugolarochelle@google.com", "botvinick@google.com", "svlevine@eecs.berkeley.edu", "yoshua.bengio@mila.quebec"], "authors": ["Anirudh Goyal", "Riashat Islam", "DJ Strouse", "Zafarali Ahmed", "Hugo Larochelle", "Matthew Botvinick", "Yoshua Bengio", "Sergey Levine"], "TL;DR": "Training agents with goal-policy information bottlenecks promotes transfer and yields a powerful exploration bonus", "pdf": "/pdf/9bfd1fe1d796c1ebab86fec8129f320412daf1b4.pdf", "paperhash": "goyal|infobot_transfer_and_exploration_via_the_information_bottleneck", "_bibtex": "@inproceedings{\ngoyal2018transfer,\ntitle={Transfer and Exploration via the Information Bottleneck},\nauthor={Anirudh Goyal and Riashat Islam and DJ Strouse and Zafarali Ahmed and Hugo Larochelle and Matthew Botvinick and Sergey Levine and Yoshua Bengio},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJg8yhAqKm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper989/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621617664, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJg8yhAqKm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper989/Authors", "ICLR.cc/2019/Conference/Paper989/Reviewers", "ICLR.cc/2019/Conference/Paper989/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper989/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper989/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper989/Authors|ICLR.cc/2019/Conference/Paper989/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper989/Reviewers", "ICLR.cc/2019/Conference/Paper989/Authors", "ICLR.cc/2019/Conference/Paper989/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621617664}}}, {"id": "rJg476FORQ", "original": null, "number": 25, "cdate": 1543179548388, "ddate": null, "tcdate": 1543179548388, "tmdate": 1543179548388, "tddate": null, "forum": "rJg8yhAqKm", "replyto": "BJxrL1eLoQ", "invitation": "ICLR.cc/2019/Conference/-/Paper989/Official_Comment", "content": {"title": "Request for feedback. ", "comment": "Your feedback has already been very helpful in improving the paper. Are there any other aspects of the paper that you think could be improved? "}, "signatures": ["ICLR.cc/2019/Conference/Paper989/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper989/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper989/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "InfoBot: Transfer and Exploration via the Information Bottleneck", "abstract": "A central challenge in reinforcement learning is discovering effective policies for tasks where rewards are sparsely distributed. We postulate that in the absence of useful reward signals, an effective exploration strategy should seek out {\\it decision states}. These states lie at critical junctions in the state space from where the agent can transition to new, potentially unexplored regions. We propose to learn about decision states from prior experience. By training a goal-conditioned model with an information bottleneck, we can identify decision states by examining where the model accesses the goal state through the bottleneck. We find that this simple mechanism effectively identifies decision states, even in partially observed settings. In effect, the model learns the sensory cues that correlate with potential subgoals. In new environments, this model can then identify novel subgoals for further exploration, guiding the agent through a sequence of potential decision  states and through new regions of the state space.", "keywords": ["Information bottleneck", "policy transfer", "policy generalization", "exploration"], "authorids": ["anirudhgoyal9119@gmail.com", "riashat.islam@mail.mcgill.ca", "danieljstrouse@gmail.com", "zafarali.ahmed@mail.mcgill.ca", "hugolarochelle@google.com", "botvinick@google.com", "svlevine@eecs.berkeley.edu", "yoshua.bengio@mila.quebec"], "authors": ["Anirudh Goyal", "Riashat Islam", "DJ Strouse", "Zafarali Ahmed", "Hugo Larochelle", "Matthew Botvinick", "Yoshua Bengio", "Sergey Levine"], "TL;DR": "Training agents with goal-policy information bottlenecks promotes transfer and yields a powerful exploration bonus", "pdf": "/pdf/9bfd1fe1d796c1ebab86fec8129f320412daf1b4.pdf", "paperhash": "goyal|infobot_transfer_and_exploration_via_the_information_bottleneck", "_bibtex": "@inproceedings{\ngoyal2018transfer,\ntitle={Transfer and Exploration via the Information Bottleneck},\nauthor={Anirudh Goyal and Riashat Islam and DJ Strouse and Zafarali Ahmed and Hugo Larochelle and Matthew Botvinick and Sergey Levine and Yoshua Bengio},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJg8yhAqKm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper989/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621617664, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJg8yhAqKm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper989/Authors", "ICLR.cc/2019/Conference/Paper989/Reviewers", "ICLR.cc/2019/Conference/Paper989/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper989/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper989/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper989/Authors|ICLR.cc/2019/Conference/Paper989/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper989/Reviewers", "ICLR.cc/2019/Conference/Paper989/Authors", "ICLR.cc/2019/Conference/Paper989/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621617664}}}, {"id": "SJlbab6rCm", "original": null, "number": 23, "cdate": 1542996408870, "ddate": null, "tcdate": 1542996408870, "tmdate": 1543082034955, "tddate": null, "forum": "rJg8yhAqKm", "replyto": "SJxWVtZ5pX", "invitation": "ICLR.cc/2019/Conference/-/Paper989/Official_Comment", "content": {"title": "Feedback by reviewer. Thanks for your time! :)", "comment": "Dear Reviewer, \n\nWe appreciate the reviewer's feedback. We would appreciate it if the reviewer could take another look at our changes and additional results, and let us know if the reviewer would like to request additional changes that would alleviate reviewers concerns. We hope that our updates to the manuscript address the reviewer's concerns about clarity, and we hope that the discussion above addresses the reviewer's concerns about empirical significance. We once again thank the reviewer for the feedback of our work.\n\n===============================================================================================\n\nWe have made following changes to the manuscript:\n\nWe have updated the paper with the following changes to address reviewer comments:\n- Added mathematical description of the proposed method in Appendix (Section A) to answer.\n- Added comparisons to exploration methods (ICM curiosity driven learning) and hierarchical methods (Feudal RL) on a more complicated navigation tasks, which includes branching as well as dead ends. \n- Added comparisons to the state of the art Off policy methods (SAC). \n- Added more experiments showing that the proposed method is more general by using for following scenarios. \n        (1) Using the proposed information regularizer for multi-agent communication, and improving against the strong baseline. \n        (2) Using the proposed method for instruction following where the goal is given by language instruction. \n        (3) Preliminary results showing that the method can be used to provide middle ground b/w model free RL and model based RL. "}, "signatures": ["ICLR.cc/2019/Conference/Paper989/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper989/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper989/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "InfoBot: Transfer and Exploration via the Information Bottleneck", "abstract": "A central challenge in reinforcement learning is discovering effective policies for tasks where rewards are sparsely distributed. We postulate that in the absence of useful reward signals, an effective exploration strategy should seek out {\\it decision states}. These states lie at critical junctions in the state space from where the agent can transition to new, potentially unexplored regions. We propose to learn about decision states from prior experience. By training a goal-conditioned model with an information bottleneck, we can identify decision states by examining where the model accesses the goal state through the bottleneck. We find that this simple mechanism effectively identifies decision states, even in partially observed settings. In effect, the model learns the sensory cues that correlate with potential subgoals. In new environments, this model can then identify novel subgoals for further exploration, guiding the agent through a sequence of potential decision  states and through new regions of the state space.", "keywords": ["Information bottleneck", "policy transfer", "policy generalization", "exploration"], "authorids": ["anirudhgoyal9119@gmail.com", "riashat.islam@mail.mcgill.ca", "danieljstrouse@gmail.com", "zafarali.ahmed@mail.mcgill.ca", "hugolarochelle@google.com", "botvinick@google.com", "svlevine@eecs.berkeley.edu", "yoshua.bengio@mila.quebec"], "authors": ["Anirudh Goyal", "Riashat Islam", "DJ Strouse", "Zafarali Ahmed", "Hugo Larochelle", "Matthew Botvinick", "Yoshua Bengio", "Sergey Levine"], "TL;DR": "Training agents with goal-policy information bottlenecks promotes transfer and yields a powerful exploration bonus", "pdf": "/pdf/9bfd1fe1d796c1ebab86fec8129f320412daf1b4.pdf", "paperhash": "goyal|infobot_transfer_and_exploration_via_the_information_bottleneck", "_bibtex": "@inproceedings{\ngoyal2018transfer,\ntitle={Transfer and Exploration via the Information Bottleneck},\nauthor={Anirudh Goyal and Riashat Islam and DJ Strouse and Zafarali Ahmed and Hugo Larochelle and Matthew Botvinick and Sergey Levine and Yoshua Bengio},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJg8yhAqKm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper989/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621617664, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJg8yhAqKm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper989/Authors", "ICLR.cc/2019/Conference/Paper989/Reviewers", "ICLR.cc/2019/Conference/Paper989/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper989/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper989/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper989/Authors|ICLR.cc/2019/Conference/Paper989/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper989/Reviewers", "ICLR.cc/2019/Conference/Paper989/Authors", "ICLR.cc/2019/Conference/Paper989/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621617664}}}, {"id": "Hkg7_bprAm", "original": null, "number": 22, "cdate": 1542996331491, "ddate": null, "tcdate": 1542996331491, "tmdate": 1543082017812, "tddate": null, "forum": "rJg8yhAqKm", "replyto": "BJxrL1eLoQ", "invitation": "ICLR.cc/2019/Conference/-/Paper989/Official_Comment", "content": {"title": "Feedback by the reviewer. Thanks for your time! :)", "comment": "Dear Reviewer, \n\nWe appreciate the reviewer's feedback. We would appreciate it if the reviewer could take another look at our changes and additional results, and let us know if the reviewer would like to request additional changes that would alleviate reviewers concerns. We hope that our updates to the manuscript address the reviewer's concerns about clarity, and we hope that the discussion above addresses the reviewer's concerns about empirical significance. We once again thank the reviewer for the feedback of our work.\n\n=====================================================\n\nWe have made following changes to the manuscript:\n\nWe have updated the paper with the following changes to address reviewer comments:\n- Remove references to \"useful habits\"  \n- Added mathematical description of the proposed method in Appendix (Section A) to answer.\n- Added comparisons to exploration methods (ICM curiosity driven learning) and hierarchical methods (Feudal RL) on a more complicated navigation tasks, which includes branching as well as dead ends. \n- Added comparisons to the state of the art Off policy methods (SAC). \n- Added more experiments showing that the proposed method is more general by using for following scenarios. \n        (1) Using the proposed information regularizer for multi-agent communication, and improving against the strong baseline. \n        (2) Using the proposed method for instruction following where the goal is given by language instruction. \n        (3) Preliminary results showing that the method can be used to provide middle ground b/w model free RL and model based RL. \n\n\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper989/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper989/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper989/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "InfoBot: Transfer and Exploration via the Information Bottleneck", "abstract": "A central challenge in reinforcement learning is discovering effective policies for tasks where rewards are sparsely distributed. We postulate that in the absence of useful reward signals, an effective exploration strategy should seek out {\\it decision states}. These states lie at critical junctions in the state space from where the agent can transition to new, potentially unexplored regions. We propose to learn about decision states from prior experience. By training a goal-conditioned model with an information bottleneck, we can identify decision states by examining where the model accesses the goal state through the bottleneck. We find that this simple mechanism effectively identifies decision states, even in partially observed settings. In effect, the model learns the sensory cues that correlate with potential subgoals. In new environments, this model can then identify novel subgoals for further exploration, guiding the agent through a sequence of potential decision  states and through new regions of the state space.", "keywords": ["Information bottleneck", "policy transfer", "policy generalization", "exploration"], "authorids": ["anirudhgoyal9119@gmail.com", "riashat.islam@mail.mcgill.ca", "danieljstrouse@gmail.com", "zafarali.ahmed@mail.mcgill.ca", "hugolarochelle@google.com", "botvinick@google.com", "svlevine@eecs.berkeley.edu", "yoshua.bengio@mila.quebec"], "authors": ["Anirudh Goyal", "Riashat Islam", "DJ Strouse", "Zafarali Ahmed", "Hugo Larochelle", "Matthew Botvinick", "Yoshua Bengio", "Sergey Levine"], "TL;DR": "Training agents with goal-policy information bottlenecks promotes transfer and yields a powerful exploration bonus", "pdf": "/pdf/9bfd1fe1d796c1ebab86fec8129f320412daf1b4.pdf", "paperhash": "goyal|infobot_transfer_and_exploration_via_the_information_bottleneck", "_bibtex": "@inproceedings{\ngoyal2018transfer,\ntitle={Transfer and Exploration via the Information Bottleneck},\nauthor={Anirudh Goyal and Riashat Islam and DJ Strouse and Zafarali Ahmed and Hugo Larochelle and Matthew Botvinick and Sergey Levine and Yoshua Bengio},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJg8yhAqKm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper989/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621617664, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJg8yhAqKm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper989/Authors", "ICLR.cc/2019/Conference/Paper989/Reviewers", "ICLR.cc/2019/Conference/Paper989/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper989/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper989/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper989/Authors|ICLR.cc/2019/Conference/Paper989/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper989/Reviewers", "ICLR.cc/2019/Conference/Paper989/Authors", "ICLR.cc/2019/Conference/Paper989/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621617664}}}, {"id": "HJlAV5ZPR7", "original": null, "number": 24, "cdate": 1543080501799, "ddate": null, "tcdate": 1543080501799, "tmdate": 1543082004838, "tddate": null, "forum": "rJg8yhAqKm", "replyto": "SJlWokadnQ", "invitation": "ICLR.cc/2019/Conference/-/Paper989/Official_Comment", "content": {"title": "Feedback by reviewer. Thanks for your time! :)", "comment": "Dear Reviewer, \n\nWe appreciate the reviewer's feedback. We would appreciate it if the reviewer could take another look at our changes and additional results, and let us know if the reviewer would like to request additional changes that would alleviate reviewers concerns. We hope that our updates to the manuscript address the reviewer's concerns about clarity, and we hope that the discussion above addresses the reviewer's concerns about empirical significance. We once again thank the reviewer for the feedback of our work.\n\n===============================================================================================\n\nWe have made following changes to the manuscript:\n\nWe have updated the paper with the following changes to address reviewer comments:\n- Added mathematical description of the proposed method in Appendix (Section A) to answer.\n- Added comparisons to exploration methods (ICM curiosity driven learning) and hierarchical methods (Feudal RL) on a more complicated navigation tasks, which includes branching as well as dead ends. \n- Added comparisons to the state of the art Off policy methods (SAC). \n- Added more experiments showing that the proposed method is more general by using for following scenarios. \n        (1) Using the proposed information regularizer for multi-agent communication, and improving against the strong baseline. \n        (2) Using the proposed method for instruction following where the goal is given by language instruction. \n        (3) Preliminary results showing that the method can be used to provide middle ground b/w model free RL and model based RL. \n"}, "signatures": ["ICLR.cc/2019/Conference/Paper989/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper989/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper989/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "InfoBot: Transfer and Exploration via the Information Bottleneck", "abstract": "A central challenge in reinforcement learning is discovering effective policies for tasks where rewards are sparsely distributed. We postulate that in the absence of useful reward signals, an effective exploration strategy should seek out {\\it decision states}. These states lie at critical junctions in the state space from where the agent can transition to new, potentially unexplored regions. We propose to learn about decision states from prior experience. By training a goal-conditioned model with an information bottleneck, we can identify decision states by examining where the model accesses the goal state through the bottleneck. We find that this simple mechanism effectively identifies decision states, even in partially observed settings. In effect, the model learns the sensory cues that correlate with potential subgoals. In new environments, this model can then identify novel subgoals for further exploration, guiding the agent through a sequence of potential decision  states and through new regions of the state space.", "keywords": ["Information bottleneck", "policy transfer", "policy generalization", "exploration"], "authorids": ["anirudhgoyal9119@gmail.com", "riashat.islam@mail.mcgill.ca", "danieljstrouse@gmail.com", "zafarali.ahmed@mail.mcgill.ca", "hugolarochelle@google.com", "botvinick@google.com", "svlevine@eecs.berkeley.edu", "yoshua.bengio@mila.quebec"], "authors": ["Anirudh Goyal", "Riashat Islam", "DJ Strouse", "Zafarali Ahmed", "Hugo Larochelle", "Matthew Botvinick", "Yoshua Bengio", "Sergey Levine"], "TL;DR": "Training agents with goal-policy information bottlenecks promotes transfer and yields a powerful exploration bonus", "pdf": "/pdf/9bfd1fe1d796c1ebab86fec8129f320412daf1b4.pdf", "paperhash": "goyal|infobot_transfer_and_exploration_via_the_information_bottleneck", "_bibtex": "@inproceedings{\ngoyal2018transfer,\ntitle={Transfer and Exploration via the Information Bottleneck},\nauthor={Anirudh Goyal and Riashat Islam and DJ Strouse and Zafarali Ahmed and Hugo Larochelle and Matthew Botvinick and Sergey Levine and Yoshua Bengio},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJg8yhAqKm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper989/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621617664, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJg8yhAqKm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper989/Authors", "ICLR.cc/2019/Conference/Paper989/Reviewers", "ICLR.cc/2019/Conference/Paper989/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper989/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper989/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper989/Authors|ICLR.cc/2019/Conference/Paper989/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper989/Reviewers", "ICLR.cc/2019/Conference/Paper989/Authors", "ICLR.cc/2019/Conference/Paper989/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621617664}}}, {"id": "ryxhXE9eAQ", "original": null, "number": 13, "cdate": 1542657060277, "ddate": null, "tcdate": 1542657060277, "tmdate": 1542926145715, "tddate": null, "forum": "rJg8yhAqKm", "replyto": "Bke-kjhq6m", "invitation": "ICLR.cc/2019/Conference/-/Paper989/Official_Comment", "content": {"title": "InfoBot for Instruction Following - New Result (4/4)", "comment": "We have added another result where we use our method for instruction following i.e the agent has to navigate to a particular goal where the goal is given by the language instruction. We ask the reviewer to refer to heading \"InfoBot for Instruction Following\" or refer to appendix (section G). \n\nWe would appreciate it if the reviewer could take another look at our changes and additional results, and let us know if the reviewer would like to request additional changes that would alleviate reviewers concerns. We hope that our updates to the manuscript address the reviewer's concerns about clarity, and we hope that the discussion above addresses the reviewer's concerns about empirical significance. We once again thank the reviewer for the feedback of our work.\n\n\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper989/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper989/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper989/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "InfoBot: Transfer and Exploration via the Information Bottleneck", "abstract": "A central challenge in reinforcement learning is discovering effective policies for tasks where rewards are sparsely distributed. We postulate that in the absence of useful reward signals, an effective exploration strategy should seek out {\\it decision states}. These states lie at critical junctions in the state space from where the agent can transition to new, potentially unexplored regions. We propose to learn about decision states from prior experience. By training a goal-conditioned model with an information bottleneck, we can identify decision states by examining where the model accesses the goal state through the bottleneck. We find that this simple mechanism effectively identifies decision states, even in partially observed settings. In effect, the model learns the sensory cues that correlate with potential subgoals. In new environments, this model can then identify novel subgoals for further exploration, guiding the agent through a sequence of potential decision  states and through new regions of the state space.", "keywords": ["Information bottleneck", "policy transfer", "policy generalization", "exploration"], "authorids": ["anirudhgoyal9119@gmail.com", "riashat.islam@mail.mcgill.ca", "danieljstrouse@gmail.com", "zafarali.ahmed@mail.mcgill.ca", "hugolarochelle@google.com", "botvinick@google.com", "svlevine@eecs.berkeley.edu", "yoshua.bengio@mila.quebec"], "authors": ["Anirudh Goyal", "Riashat Islam", "DJ Strouse", "Zafarali Ahmed", "Hugo Larochelle", "Matthew Botvinick", "Yoshua Bengio", "Sergey Levine"], "TL;DR": "Training agents with goal-policy information bottlenecks promotes transfer and yields a powerful exploration bonus", "pdf": "/pdf/9bfd1fe1d796c1ebab86fec8129f320412daf1b4.pdf", "paperhash": "goyal|infobot_transfer_and_exploration_via_the_information_bottleneck", "_bibtex": "@inproceedings{\ngoyal2018transfer,\ntitle={Transfer and Exploration via the Information Bottleneck},\nauthor={Anirudh Goyal and Riashat Islam and DJ Strouse and Zafarali Ahmed and Hugo Larochelle and Matthew Botvinick and Sergey Levine and Yoshua Bengio},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJg8yhAqKm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper989/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621617664, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJg8yhAqKm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper989/Authors", "ICLR.cc/2019/Conference/Paper989/Reviewers", "ICLR.cc/2019/Conference/Paper989/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper989/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper989/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper989/Authors|ICLR.cc/2019/Conference/Paper989/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper989/Reviewers", "ICLR.cc/2019/Conference/Paper989/Authors", "ICLR.cc/2019/Conference/Paper989/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621617664}}}, {"id": "rJxSNHQMRQ", "original": null, "number": 20, "cdate": 1542759725383, "ddate": null, "tcdate": 1542759725383, "tmdate": 1542759725383, "tddate": null, "forum": "rJg8yhAqKm", "replyto": "H1xMDas56m", "invitation": "ICLR.cc/2019/Conference/-/Paper989/Official_Comment", "content": {"title": "InfoBot for Instruction Following - New Result ", "comment": "In order to be empirically thorough, we have added another result where we use our method for instruction following i.e the agent has to navigate to a particular goal where the goal is given by the language instruction. We ask the reviewer to refer to heading \"InfoBot for Instruction Following\" or refer to appendix (section G). \n\nWe would appreciate it if the reviewer could take another look at our changes and additional results, and let us know if the reviewer would like to request additional changes that would alleviate reviewers concerns. We hope that our updates to the manuscript address the reviewer's concerns about clarity, and we hope that the discussion above addresses the reviewer's concerns about empirical significance. We once again thank the reviewer for the feedback of our work."}, "signatures": ["ICLR.cc/2019/Conference/Paper989/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper989/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper989/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "InfoBot: Transfer and Exploration via the Information Bottleneck", "abstract": "A central challenge in reinforcement learning is discovering effective policies for tasks where rewards are sparsely distributed. We postulate that in the absence of useful reward signals, an effective exploration strategy should seek out {\\it decision states}. These states lie at critical junctions in the state space from where the agent can transition to new, potentially unexplored regions. We propose to learn about decision states from prior experience. By training a goal-conditioned model with an information bottleneck, we can identify decision states by examining where the model accesses the goal state through the bottleneck. We find that this simple mechanism effectively identifies decision states, even in partially observed settings. In effect, the model learns the sensory cues that correlate with potential subgoals. In new environments, this model can then identify novel subgoals for further exploration, guiding the agent through a sequence of potential decision  states and through new regions of the state space.", "keywords": ["Information bottleneck", "policy transfer", "policy generalization", "exploration"], "authorids": ["anirudhgoyal9119@gmail.com", "riashat.islam@mail.mcgill.ca", "danieljstrouse@gmail.com", "zafarali.ahmed@mail.mcgill.ca", "hugolarochelle@google.com", "botvinick@google.com", "svlevine@eecs.berkeley.edu", "yoshua.bengio@mila.quebec"], "authors": ["Anirudh Goyal", "Riashat Islam", "DJ Strouse", "Zafarali Ahmed", "Hugo Larochelle", "Matthew Botvinick", "Yoshua Bengio", "Sergey Levine"], "TL;DR": "Training agents with goal-policy information bottlenecks promotes transfer and yields a powerful exploration bonus", "pdf": "/pdf/9bfd1fe1d796c1ebab86fec8129f320412daf1b4.pdf", "paperhash": "goyal|infobot_transfer_and_exploration_via_the_information_bottleneck", "_bibtex": "@inproceedings{\ngoyal2018transfer,\ntitle={Transfer and Exploration via the Information Bottleneck},\nauthor={Anirudh Goyal and Riashat Islam and DJ Strouse and Zafarali Ahmed and Hugo Larochelle and Matthew Botvinick and Sergey Levine and Yoshua Bengio},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJg8yhAqKm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper989/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621617664, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJg8yhAqKm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper989/Authors", "ICLR.cc/2019/Conference/Paper989/Reviewers", "ICLR.cc/2019/Conference/Paper989/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper989/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper989/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper989/Authors|ICLR.cc/2019/Conference/Paper989/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper989/Reviewers", "ICLR.cc/2019/Conference/Paper989/Authors", "ICLR.cc/2019/Conference/Paper989/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621617664}}}, {"id": "Hyg3Bds96Q", "original": null, "number": 2, "cdate": 1542268996260, "ddate": null, "tcdate": 1542268996260, "tmdate": 1542741723106, "tddate": null, "forum": "rJg8yhAqKm", "replyto": "rJg8yhAqKm", "invitation": "ICLR.cc/2019/Conference/-/Paper989/Official_Comment", "content": {"title": "More challenging navigation Environment - To ALL reviewers", "comment": "We thank all the reviewers for their valuable time and feedback. \n\nIn order to answer the  questions raised by all the reviewers, we setup another challenging task (Fig 7 main paper). Here, the goal of the agent is to navigate to a goal position. We use a partially observed formulation of the task, where the agent only observes a small number of squares ahead of it and the agent only gets a reward of \u201c1\u201d when it reaches the goal.  For standard RL algorithms, these tasks are difficult to solve due to the partial observability of the environment, sparse reward (as the agent receives a reward only after reaching the goal), and low probability of reaching the goal via random walks (precisely because these  junction states are crucial states where the right action must be taken and several junctions need to be crossed). This task also has dead ends as well as more complex branching factor which was not present in the task which we tried for the submitted paper.  We demonstrate the results on this env where we compare generalization performance to model free methods (a2c, ppo), and goal oriented baselines (UVFA) setup, as well as strong exploration baselines (ICM, count based exploration) and goal based hierarchical baselines (like Option Critic and Feudal Networks). We would be happy to add comparisons to other baselines which reviewers have in mind. \n\nWe first show direct policy transfer in which we first train a particular algorithm on 6 x 6 maze, and evaluate the direct policy transfer on 11 x 11 maze. In order to be exhaustive, we also compare to hierarchical baselines like Option Critic Architecture and Feudal Networks. We tested on 11 X 11 mazes by running a policy trained with different algorithms. The success rate is the number of times the agent solves a larger task (11 x 11) while it is being trained on the smaller task (6 x 6).  All these were run for 100M steps, and averaged over 3 random seeds.\n\nAlgorithm (Train on 6 x 6)                                                    (Evaluate on 11 x 11) (variance)\nActor Critic                                                                                               5% (1%)\nPPO (Proximal Policy Optimization)                                                    8% (1%)\nActor Critic + CT based exploration                                                     7% (1%)\nGoal Based (UVFA)                                                                                15% (4%)\nCuriosity Driven Learning using inverse models.                            47% (5%)\nOption Critic                                                                                            17% (4%)\nFeudal RL                                                                                                37% (4%)\nProposed Method                                                                                 64% (2%)\n\nWhen generalizing to larger mazes, the agent learns to solve the task  64\\% of the times, whereas other hierarchical agents solve <50\\% of mazes (Table 3).\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper989/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper989/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper989/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "InfoBot: Transfer and Exploration via the Information Bottleneck", "abstract": "A central challenge in reinforcement learning is discovering effective policies for tasks where rewards are sparsely distributed. We postulate that in the absence of useful reward signals, an effective exploration strategy should seek out {\\it decision states}. These states lie at critical junctions in the state space from where the agent can transition to new, potentially unexplored regions. We propose to learn about decision states from prior experience. By training a goal-conditioned model with an information bottleneck, we can identify decision states by examining where the model accesses the goal state through the bottleneck. We find that this simple mechanism effectively identifies decision states, even in partially observed settings. In effect, the model learns the sensory cues that correlate with potential subgoals. In new environments, this model can then identify novel subgoals for further exploration, guiding the agent through a sequence of potential decision  states and through new regions of the state space.", "keywords": ["Information bottleneck", "policy transfer", "policy generalization", "exploration"], "authorids": ["anirudhgoyal9119@gmail.com", "riashat.islam@mail.mcgill.ca", "danieljstrouse@gmail.com", "zafarali.ahmed@mail.mcgill.ca", "hugolarochelle@google.com", "botvinick@google.com", "svlevine@eecs.berkeley.edu", "yoshua.bengio@mila.quebec"], "authors": ["Anirudh Goyal", "Riashat Islam", "DJ Strouse", "Zafarali Ahmed", "Hugo Larochelle", "Matthew Botvinick", "Yoshua Bengio", "Sergey Levine"], "TL;DR": "Training agents with goal-policy information bottlenecks promotes transfer and yields a powerful exploration bonus", "pdf": "/pdf/9bfd1fe1d796c1ebab86fec8129f320412daf1b4.pdf", "paperhash": "goyal|infobot_transfer_and_exploration_via_the_information_bottleneck", "_bibtex": "@inproceedings{\ngoyal2018transfer,\ntitle={Transfer and Exploration via the Information Bottleneck},\nauthor={Anirudh Goyal and Riashat Islam and DJ Strouse and Zafarali Ahmed and Hugo Larochelle and Matthew Botvinick and Sergey Levine and Yoshua Bengio},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJg8yhAqKm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper989/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621617664, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJg8yhAqKm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper989/Authors", "ICLR.cc/2019/Conference/Paper989/Reviewers", "ICLR.cc/2019/Conference/Paper989/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper989/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper989/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper989/Authors|ICLR.cc/2019/Conference/Paper989/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper989/Reviewers", "ICLR.cc/2019/Conference/Paper989/Authors", "ICLR.cc/2019/Conference/Paper989/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621617664}}}, {"id": "BJlVbP2ZCQ", "original": null, "number": 19, "cdate": 1542731515654, "ddate": null, "tcdate": 1542731515654, "tmdate": 1542731552308, "tddate": null, "forum": "rJg8yhAqKm", "replyto": "rJxR90hl0Q", "invitation": "ICLR.cc/2019/Conference/-/Paper989/Official_Comment", "content": {"title": "Thanks!", "comment": "We thank the reviewer for taking time to read our lengthy rebuttal, and increasing their score. If the reviewer want us to include any more ablation/experiment, please let us know. Thanks again! :)"}, "signatures": ["ICLR.cc/2019/Conference/Paper989/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper989/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper989/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "InfoBot: Transfer and Exploration via the Information Bottleneck", "abstract": "A central challenge in reinforcement learning is discovering effective policies for tasks where rewards are sparsely distributed. We postulate that in the absence of useful reward signals, an effective exploration strategy should seek out {\\it decision states}. These states lie at critical junctions in the state space from where the agent can transition to new, potentially unexplored regions. We propose to learn about decision states from prior experience. By training a goal-conditioned model with an information bottleneck, we can identify decision states by examining where the model accesses the goal state through the bottleneck. We find that this simple mechanism effectively identifies decision states, even in partially observed settings. In effect, the model learns the sensory cues that correlate with potential subgoals. In new environments, this model can then identify novel subgoals for further exploration, guiding the agent through a sequence of potential decision  states and through new regions of the state space.", "keywords": ["Information bottleneck", "policy transfer", "policy generalization", "exploration"], "authorids": ["anirudhgoyal9119@gmail.com", "riashat.islam@mail.mcgill.ca", "danieljstrouse@gmail.com", "zafarali.ahmed@mail.mcgill.ca", "hugolarochelle@google.com", "botvinick@google.com", "svlevine@eecs.berkeley.edu", "yoshua.bengio@mila.quebec"], "authors": ["Anirudh Goyal", "Riashat Islam", "DJ Strouse", "Zafarali Ahmed", "Hugo Larochelle", "Matthew Botvinick", "Yoshua Bengio", "Sergey Levine"], "TL;DR": "Training agents with goal-policy information bottlenecks promotes transfer and yields a powerful exploration bonus", "pdf": "/pdf/9bfd1fe1d796c1ebab86fec8129f320412daf1b4.pdf", "paperhash": "goyal|infobot_transfer_and_exploration_via_the_information_bottleneck", "_bibtex": "@inproceedings{\ngoyal2018transfer,\ntitle={Transfer and Exploration via the Information Bottleneck},\nauthor={Anirudh Goyal and Riashat Islam and DJ Strouse and Zafarali Ahmed and Hugo Larochelle and Matthew Botvinick and Sergey Levine and Yoshua Bengio},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJg8yhAqKm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper989/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621617664, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJg8yhAqKm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper989/Authors", "ICLR.cc/2019/Conference/Paper989/Reviewers", "ICLR.cc/2019/Conference/Paper989/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper989/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper989/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper989/Authors|ICLR.cc/2019/Conference/Paper989/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper989/Reviewers", "ICLR.cc/2019/Conference/Paper989/Authors", "ICLR.cc/2019/Conference/Paper989/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621617664}}}, {"id": "SkgQ1PcgCm", "original": null, "number": 16, "cdate": 1542657754565, "ddate": null, "tcdate": 1542657754565, "tmdate": 1542669473842, "tddate": null, "forum": "rJg8yhAqKm", "replyto": "rJg8yhAqKm", "invitation": "ICLR.cc/2019/Conference/-/Paper989/Official_Comment", "content": {"title": "Paper Updated to address reviewer feedback.", "comment": "We have updated the paper with the following changes to address reviewer comments:\n- Remove references to \"useful habits\"  (Reviewer 1)\n- Added mathematical description of the proposed method in Appendix (Section A) to answer (Reviewer 1, and Reviewer 3)\n- Added comparisons to exploration methods (ICM curiosity driven learning) and hierarchical methods (Feudal RL) on a more complicated navigation tasks, which includes branching as well as dead ends. (All reviewers)\n- Added comparisons to the state of the art Off policy methods (SAC).  (All reviewers)\n- Added more experiments showing that the proposed method is more general by using for following scenarios. (All Reviewers) \n        (1) Using the proposed information regularizer for multi-agent communication, and improving against the strong baseline. \n        (2) Using the proposed method for instruction following where the goal is given by language instruction. \n        (3) Preliminary results showing that the method can be used to provide middle ground b/w model free RL and model based RL. \n\nThank you for your time! The authors appreciate the time reviewers have taken for providing feedback. which resulted in improving the presentation of our paper. Hence,  we would appreciate it if the reviewers could take a look at our changes and additional results, and let us know if they would like to either revise their rating of the paper, or request additional changes that would alleviate their concerns. \n"}, "signatures": ["ICLR.cc/2019/Conference/Paper989/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper989/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper989/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "InfoBot: Transfer and Exploration via the Information Bottleneck", "abstract": "A central challenge in reinforcement learning is discovering effective policies for tasks where rewards are sparsely distributed. We postulate that in the absence of useful reward signals, an effective exploration strategy should seek out {\\it decision states}. These states lie at critical junctions in the state space from where the agent can transition to new, potentially unexplored regions. We propose to learn about decision states from prior experience. By training a goal-conditioned model with an information bottleneck, we can identify decision states by examining where the model accesses the goal state through the bottleneck. We find that this simple mechanism effectively identifies decision states, even in partially observed settings. In effect, the model learns the sensory cues that correlate with potential subgoals. In new environments, this model can then identify novel subgoals for further exploration, guiding the agent through a sequence of potential decision  states and through new regions of the state space.", "keywords": ["Information bottleneck", "policy transfer", "policy generalization", "exploration"], "authorids": ["anirudhgoyal9119@gmail.com", "riashat.islam@mail.mcgill.ca", "danieljstrouse@gmail.com", "zafarali.ahmed@mail.mcgill.ca", "hugolarochelle@google.com", "botvinick@google.com", "svlevine@eecs.berkeley.edu", "yoshua.bengio@mila.quebec"], "authors": ["Anirudh Goyal", "Riashat Islam", "DJ Strouse", "Zafarali Ahmed", "Hugo Larochelle", "Matthew Botvinick", "Yoshua Bengio", "Sergey Levine"], "TL;DR": "Training agents with goal-policy information bottlenecks promotes transfer and yields a powerful exploration bonus", "pdf": "/pdf/9bfd1fe1d796c1ebab86fec8129f320412daf1b4.pdf", "paperhash": "goyal|infobot_transfer_and_exploration_via_the_information_bottleneck", "_bibtex": "@inproceedings{\ngoyal2018transfer,\ntitle={Transfer and Exploration via the Information Bottleneck},\nauthor={Anirudh Goyal and Riashat Islam and DJ Strouse and Zafarali Ahmed and Hugo Larochelle and Matthew Botvinick and Sergey Levine and Yoshua Bengio},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJg8yhAqKm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper989/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621617664, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJg8yhAqKm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper989/Authors", "ICLR.cc/2019/Conference/Paper989/Reviewers", "ICLR.cc/2019/Conference/Paper989/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper989/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper989/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper989/Authors|ICLR.cc/2019/Conference/Paper989/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper989/Reviewers", "ICLR.cc/2019/Conference/Paper989/Authors", "ICLR.cc/2019/Conference/Paper989/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621617664}}}, {"id": "SJlWokadnQ", "original": null, "number": 2, "cdate": 1541095320917, "ddate": null, "tcdate": 1541095320917, "tmdate": 1542667944560, "tddate": null, "forum": "rJg8yhAqKm", "replyto": "rJg8yhAqKm", "invitation": "ICLR.cc/2019/Conference/-/Paper989/Official_Review", "content": {"title": "Interesting idea. Not sure the significance of its experimental results", "review": "This paper proposes the concept of decision state, which is the state where decision is made \u201cmore\u201d dependent to a particular goal. The authors propose a KL divergence regularization to learn the structure of the tasks, and then use this information to encourage the policy to visit the decision states. The method is tested on several different experiment setups.\n\nIn general the paper is well-written and easy to follow. Learning a more general policy is not new (as also discussed in the paper), but using the learned structure to further guide the exploration of the policy is novel and interesting.\n\nI have a couple questions about the experimental part though, mostly about the baselines.\n1. What is the reasoning behind the selection of the baselines, e.g. A2C as the baseline for the miniGrid experiments? \n2. What are the performances of the methods in Table 2, in direct policy generalization? Or is there any reason not reporting them here?\n3.  What is the reasoning of picking \u201cCount-base baseline\u201d for Figure 4, rather than the method of curiosity-based exploration?\n4. For the Mujoco tasks, there are couple ones outperforming PPO, e.g. TD3, SAC etc.. [1,2] The authors should include their results too. \n5. As an ablation study, it would be interesting to see how the bonus reward of visiting decision states can help the exploration on the training tasks, compared to the policy learned from equation (1), and the policies learned without information of other tasks.\n6. Lastly, the idea of decision states can also be used in other RL algorithms. It would be also interesting to see if this idea can further improve their performances.\n\nOther comments:\n1. Equation (3) should be \\le.\n2. Why would Equation (5) hold? \n3. Right before section 2.2, incomplete sentence.\n\n\nDisclaimer: The reviewer is not familiar with multitask reinforcement learning, and the miniGrid environment in the paper. Other reviewers should have better judgement on the significance of the experimental results.\n\n[1] Haarnoja, Tuomas, et al. \"Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor.\"\u00a0arXiv preprint arXiv:1801.01290\u00a0(2018).\n[2] Fujimoto, Scott, Herke van Hoof, and Dave Meger. \"Addressing Function Approximation Error in Actor-Critic Methods.\"\u00a0arXiv preprint arXiv:1802.09477\u00a0(2018).", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper989/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "InfoBot: Transfer and Exploration via the Information Bottleneck", "abstract": "A central challenge in reinforcement learning is discovering effective policies for tasks where rewards are sparsely distributed. We postulate that in the absence of useful reward signals, an effective exploration strategy should seek out {\\it decision states}. These states lie at critical junctions in the state space from where the agent can transition to new, potentially unexplored regions. We propose to learn about decision states from prior experience. By training a goal-conditioned model with an information bottleneck, we can identify decision states by examining where the model accesses the goal state through the bottleneck. We find that this simple mechanism effectively identifies decision states, even in partially observed settings. In effect, the model learns the sensory cues that correlate with potential subgoals. In new environments, this model can then identify novel subgoals for further exploration, guiding the agent through a sequence of potential decision  states and through new regions of the state space.", "keywords": ["Information bottleneck", "policy transfer", "policy generalization", "exploration"], "authorids": ["anirudhgoyal9119@gmail.com", "riashat.islam@mail.mcgill.ca", "danieljstrouse@gmail.com", "zafarali.ahmed@mail.mcgill.ca", "hugolarochelle@google.com", "botvinick@google.com", "svlevine@eecs.berkeley.edu", "yoshua.bengio@mila.quebec"], "authors": ["Anirudh Goyal", "Riashat Islam", "DJ Strouse", "Zafarali Ahmed", "Hugo Larochelle", "Matthew Botvinick", "Yoshua Bengio", "Sergey Levine"], "TL;DR": "Training agents with goal-policy information bottlenecks promotes transfer and yields a powerful exploration bonus", "pdf": "/pdf/9bfd1fe1d796c1ebab86fec8129f320412daf1b4.pdf", "paperhash": "goyal|infobot_transfer_and_exploration_via_the_information_bottleneck", "_bibtex": "@inproceedings{\ngoyal2018transfer,\ntitle={Transfer and Exploration via the Information Bottleneck},\nauthor={Anirudh Goyal and Riashat Islam and DJ Strouse and Zafarali Ahmed and Hugo Larochelle and Matthew Botvinick and Sergey Levine and Yoshua Bengio},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJg8yhAqKm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper989/Official_Review", "cdate": 1542234331552, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "rJg8yhAqKm", "replyto": "rJg8yhAqKm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper989/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335848093, "tmdate": 1552335848093, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper989/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "rJxR90hl0Q", "original": null, "number": 18, "cdate": 1542667926210, "ddate": null, "tcdate": 1542667926210, "tmdate": 1542667926210, "tddate": null, "forum": "rJg8yhAqKm", "replyto": "SkeROX29am", "invitation": "ICLR.cc/2019/Conference/-/Paper989/Official_Comment", "content": {"title": "Thanks for the rebuttal", "comment": "I have read other reviews and I think the rebuttal has addressed my concerns on the significance of the experimental results, thus I increase the score from 6 to 7. \n"}, "signatures": ["ICLR.cc/2019/Conference/Paper989/AnonReviewer2"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper989/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper989/AnonReviewer2", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "InfoBot: Transfer and Exploration via the Information Bottleneck", "abstract": "A central challenge in reinforcement learning is discovering effective policies for tasks where rewards are sparsely distributed. We postulate that in the absence of useful reward signals, an effective exploration strategy should seek out {\\it decision states}. These states lie at critical junctions in the state space from where the agent can transition to new, potentially unexplored regions. We propose to learn about decision states from prior experience. By training a goal-conditioned model with an information bottleneck, we can identify decision states by examining where the model accesses the goal state through the bottleneck. We find that this simple mechanism effectively identifies decision states, even in partially observed settings. In effect, the model learns the sensory cues that correlate with potential subgoals. In new environments, this model can then identify novel subgoals for further exploration, guiding the agent through a sequence of potential decision  states and through new regions of the state space.", "keywords": ["Information bottleneck", "policy transfer", "policy generalization", "exploration"], "authorids": ["anirudhgoyal9119@gmail.com", "riashat.islam@mail.mcgill.ca", "danieljstrouse@gmail.com", "zafarali.ahmed@mail.mcgill.ca", "hugolarochelle@google.com", "botvinick@google.com", "svlevine@eecs.berkeley.edu", "yoshua.bengio@mila.quebec"], "authors": ["Anirudh Goyal", "Riashat Islam", "DJ Strouse", "Zafarali Ahmed", "Hugo Larochelle", "Matthew Botvinick", "Yoshua Bengio", "Sergey Levine"], "TL;DR": "Training agents with goal-policy information bottlenecks promotes transfer and yields a powerful exploration bonus", "pdf": "/pdf/9bfd1fe1d796c1ebab86fec8129f320412daf1b4.pdf", "paperhash": "goyal|infobot_transfer_and_exploration_via_the_information_bottleneck", "_bibtex": "@inproceedings{\ngoyal2018transfer,\ntitle={Transfer and Exploration via the Information Bottleneck},\nauthor={Anirudh Goyal and Riashat Islam and DJ Strouse and Zafarali Ahmed and Hugo Larochelle and Matthew Botvinick and Sergey Levine and Yoshua Bengio},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJg8yhAqKm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper989/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621617664, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJg8yhAqKm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper989/Authors", "ICLR.cc/2019/Conference/Paper989/Reviewers", "ICLR.cc/2019/Conference/Paper989/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper989/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper989/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper989/Authors|ICLR.cc/2019/Conference/Paper989/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper989/Reviewers", "ICLR.cc/2019/Conference/Paper989/Authors", "ICLR.cc/2019/Conference/Paper989/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621617664}}}, {"id": "rkluo4qgCQ", "original": null, "number": 14, "cdate": 1542657183850, "ddate": null, "tcdate": 1542657183850, "tmdate": 1542657183850, "tddate": null, "forum": "rJg8yhAqKm", "replyto": "SJlWokadnQ", "invitation": "ICLR.cc/2019/Conference/-/Paper989/Official_Comment", "content": {"title": "InfoBot for Instruction Following - New Result", "comment": "We have added another result where we use our method for instruction following i.e the agent has to navigate to a particular goal where the goal is given by the language instruction. We ask the reviewer to refer to heading \"InfoBot for Instruction Following\" or refer to appendix (section G). \n\nWe would appreciate it if the reviewer could take another look at our changes and additional results, and let us know if the reviewer would like to request additional changes that would alleviate reviewers concerns. We hope that our updates to the manuscript address the reviewer's concerns about clarity, and we hope that the discussion above addresses the reviewer's concerns about empirical significance. We once again thank the reviewer for the thorough feedback of our work."}, "signatures": ["ICLR.cc/2019/Conference/Paper989/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper989/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper989/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "InfoBot: Transfer and Exploration via the Information Bottleneck", "abstract": "A central challenge in reinforcement learning is discovering effective policies for tasks where rewards are sparsely distributed. We postulate that in the absence of useful reward signals, an effective exploration strategy should seek out {\\it decision states}. These states lie at critical junctions in the state space from where the agent can transition to new, potentially unexplored regions. We propose to learn about decision states from prior experience. By training a goal-conditioned model with an information bottleneck, we can identify decision states by examining where the model accesses the goal state through the bottleneck. We find that this simple mechanism effectively identifies decision states, even in partially observed settings. In effect, the model learns the sensory cues that correlate with potential subgoals. In new environments, this model can then identify novel subgoals for further exploration, guiding the agent through a sequence of potential decision  states and through new regions of the state space.", "keywords": ["Information bottleneck", "policy transfer", "policy generalization", "exploration"], "authorids": ["anirudhgoyal9119@gmail.com", "riashat.islam@mail.mcgill.ca", "danieljstrouse@gmail.com", "zafarali.ahmed@mail.mcgill.ca", "hugolarochelle@google.com", "botvinick@google.com", "svlevine@eecs.berkeley.edu", "yoshua.bengio@mila.quebec"], "authors": ["Anirudh Goyal", "Riashat Islam", "DJ Strouse", "Zafarali Ahmed", "Hugo Larochelle", "Matthew Botvinick", "Yoshua Bengio", "Sergey Levine"], "TL;DR": "Training agents with goal-policy information bottlenecks promotes transfer and yields a powerful exploration bonus", "pdf": "/pdf/9bfd1fe1d796c1ebab86fec8129f320412daf1b4.pdf", "paperhash": "goyal|infobot_transfer_and_exploration_via_the_information_bottleneck", "_bibtex": "@inproceedings{\ngoyal2018transfer,\ntitle={Transfer and Exploration via the Information Bottleneck},\nauthor={Anirudh Goyal and Riashat Islam and DJ Strouse and Zafarali Ahmed and Hugo Larochelle and Matthew Botvinick and Sergey Levine and Yoshua Bengio},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJg8yhAqKm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper989/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621617664, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJg8yhAqKm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper989/Authors", "ICLR.cc/2019/Conference/Paper989/Reviewers", "ICLR.cc/2019/Conference/Paper989/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper989/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper989/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper989/Authors|ICLR.cc/2019/Conference/Paper989/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper989/Reviewers", "ICLR.cc/2019/Conference/Paper989/Authors", "ICLR.cc/2019/Conference/Paper989/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621617664}}}, {"id": "rkgF37cxCQ", "original": null, "number": 12, "cdate": 1542656945255, "ddate": null, "tcdate": 1542656945255, "tmdate": 1542657129903, "tddate": null, "forum": "rJg8yhAqKm", "replyto": "HyeBMFj9p7", "invitation": "ICLR.cc/2019/Conference/-/Paper989/Official_Comment", "content": {"title": "InfoBot for Instruction Following - To ALL REVIEWERS", "comment": "Here we want to show that the proposed method can also be in the context of interactive worlds for spatial reasoning where the goal is given by language instruction, and hence the proposed method is very general. The agent is placed in an interactive world, and agent can take actions to reach the goal specified by language instruction. For ex. Reach the north-most house, the problem could be challenging because the language instruction is is highly context dependent. Therefore, for better generalization to unseen worlds, the model must jointly reason over the instruction text and environment configuration. Here, the text instructions can have both local and global references to objects. Local references require an understanding of spatial prepositional phrases such as \u2018above\u2019, \u2018next to\u2019 in order to reach the goal. This is invariant to the global position of the object references, on the other hand, global references contains superlatives such as \u2018easternmost\u2019 and \u2018topmost\u2019, which require reasoning over the entire map. Here we compare the proposed method to UVFA. We use the exact same experimental setup as in [1].\n\nMethod                                          Local (Policy Quality)                         Global (Policy Quality)\nUVFA                                                         0.57                                                     0.59\nInfoBot                                                     0.89                                                     0.81\n\n[1] \"Representation Learning for Grounded Spatial Reasoning\" https://arxiv.org/abs/1707.03938\n\nThe details for all these experiments have also been mentioned in Appendix (Section G). \n\nWe would appreciate it if the reviewer(s) could take another look at our changes and additional results, and let us know if the reviewer would like to request additional changes that would alleviate reviewers concerns. "}, "signatures": ["ICLR.cc/2019/Conference/Paper989/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper989/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper989/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "InfoBot: Transfer and Exploration via the Information Bottleneck", "abstract": "A central challenge in reinforcement learning is discovering effective policies for tasks where rewards are sparsely distributed. We postulate that in the absence of useful reward signals, an effective exploration strategy should seek out {\\it decision states}. These states lie at critical junctions in the state space from where the agent can transition to new, potentially unexplored regions. We propose to learn about decision states from prior experience. By training a goal-conditioned model with an information bottleneck, we can identify decision states by examining where the model accesses the goal state through the bottleneck. We find that this simple mechanism effectively identifies decision states, even in partially observed settings. In effect, the model learns the sensory cues that correlate with potential subgoals. In new environments, this model can then identify novel subgoals for further exploration, guiding the agent through a sequence of potential decision  states and through new regions of the state space.", "keywords": ["Information bottleneck", "policy transfer", "policy generalization", "exploration"], "authorids": ["anirudhgoyal9119@gmail.com", "riashat.islam@mail.mcgill.ca", "danieljstrouse@gmail.com", "zafarali.ahmed@mail.mcgill.ca", "hugolarochelle@google.com", "botvinick@google.com", "svlevine@eecs.berkeley.edu", "yoshua.bengio@mila.quebec"], "authors": ["Anirudh Goyal", "Riashat Islam", "DJ Strouse", "Zafarali Ahmed", "Hugo Larochelle", "Matthew Botvinick", "Yoshua Bengio", "Sergey Levine"], "TL;DR": "Training agents with goal-policy information bottlenecks promotes transfer and yields a powerful exploration bonus", "pdf": "/pdf/9bfd1fe1d796c1ebab86fec8129f320412daf1b4.pdf", "paperhash": "goyal|infobot_transfer_and_exploration_via_the_information_bottleneck", "_bibtex": "@inproceedings{\ngoyal2018transfer,\ntitle={Transfer and Exploration via the Information Bottleneck},\nauthor={Anirudh Goyal and Riashat Islam and DJ Strouse and Zafarali Ahmed and Hugo Larochelle and Matthew Botvinick and Sergey Levine and Yoshua Bengio},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJg8yhAqKm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper989/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621617664, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJg8yhAqKm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper989/Authors", "ICLR.cc/2019/Conference/Paper989/Reviewers", "ICLR.cc/2019/Conference/Paper989/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper989/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper989/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper989/Authors|ICLR.cc/2019/Conference/Paper989/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper989/Reviewers", "ICLR.cc/2019/Conference/Paper989/Authors", "ICLR.cc/2019/Conference/Paper989/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621617664}}}, {"id": "Bke-kjhq6m", "original": null, "number": 11, "cdate": 1542273753460, "ddate": null, "tcdate": 1542273753460, "tmdate": 1542657095518, "tddate": null, "forum": "rJg8yhAqKm", "replyto": "Skevj_35pX", "invitation": "ICLR.cc/2019/Conference/-/Paper989/Official_Comment", "content": {"title": "Proposed Method is general and can be combined with a2c/ppo/TD3/SAC. (3/4)", "comment": "As shown in the experiments, our method outperforms the baseline in the Atari case, as well as compared to PPO for continuous control tasks (Section 4.4), as well as Soft Actor Critic on Sparse reward Continuous control problems.\n\n\"More generally, experiments where more modern policy search algorithms are combined with the regularizer would be helpful. In particular, does it matter which policy search algorithm we use with this method?\"\n\nWe combined the proposed method with state of the art off policy algorithm (Soft actor Critic), PPO and A2C. In general, the proposed method is agnostic to the chosen policy search algorithm. We would be happy to add more comparisons, which the reviewer has in mind.\n\n\"Experimental plots in section 4.4 are missing error bars, and I can\u2019t tell if the results are significant without them.\"\n\nExperimental results in section 4.4 are averaged over 5 random seeds. We found the baseline (PPO) as well as proposed method to have less variance across different runs with different random seeds. Since, we were plotting many baselines (InfoBot with Low value states, InfoBot with High Value States, InfoBot with zero KL cost), and hence for clarity, we did not include the error bars. But if the reviewer wants, we would be happy to include them. \n\nWe would appreciate it if the reviewer could take another look at our changes and additional results, and let us know if the reviewer would like to request additional changes that would alleviate reviewers concerns. We hope that our updates to the manuscript address the reviewer's concerns about clarity, and we hope that the discussion above addresses the reviewer's concerns about empirical significance. We once again thank the reviewer for the thorough feedback of our work.\n\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper989/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper989/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper989/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "InfoBot: Transfer and Exploration via the Information Bottleneck", "abstract": "A central challenge in reinforcement learning is discovering effective policies for tasks where rewards are sparsely distributed. We postulate that in the absence of useful reward signals, an effective exploration strategy should seek out {\\it decision states}. These states lie at critical junctions in the state space from where the agent can transition to new, potentially unexplored regions. We propose to learn about decision states from prior experience. By training a goal-conditioned model with an information bottleneck, we can identify decision states by examining where the model accesses the goal state through the bottleneck. We find that this simple mechanism effectively identifies decision states, even in partially observed settings. In effect, the model learns the sensory cues that correlate with potential subgoals. In new environments, this model can then identify novel subgoals for further exploration, guiding the agent through a sequence of potential decision  states and through new regions of the state space.", "keywords": ["Information bottleneck", "policy transfer", "policy generalization", "exploration"], "authorids": ["anirudhgoyal9119@gmail.com", "riashat.islam@mail.mcgill.ca", "danieljstrouse@gmail.com", "zafarali.ahmed@mail.mcgill.ca", "hugolarochelle@google.com", "botvinick@google.com", "svlevine@eecs.berkeley.edu", "yoshua.bengio@mila.quebec"], "authors": ["Anirudh Goyal", "Riashat Islam", "DJ Strouse", "Zafarali Ahmed", "Hugo Larochelle", "Matthew Botvinick", "Yoshua Bengio", "Sergey Levine"], "TL;DR": "Training agents with goal-policy information bottlenecks promotes transfer and yields a powerful exploration bonus", "pdf": "/pdf/9bfd1fe1d796c1ebab86fec8129f320412daf1b4.pdf", "paperhash": "goyal|infobot_transfer_and_exploration_via_the_information_bottleneck", "_bibtex": "@inproceedings{\ngoyal2018transfer,\ntitle={Transfer and Exploration via the Information Bottleneck},\nauthor={Anirudh Goyal and Riashat Islam and DJ Strouse and Zafarali Ahmed and Hugo Larochelle and Matthew Botvinick and Sergey Levine and Yoshua Bengio},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJg8yhAqKm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper989/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621617664, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJg8yhAqKm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper989/Authors", "ICLR.cc/2019/Conference/Paper989/Reviewers", "ICLR.cc/2019/Conference/Paper989/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper989/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper989/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper989/Authors|ICLR.cc/2019/Conference/Paper989/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper989/Reviewers", "ICLR.cc/2019/Conference/Paper989/Authors", "ICLR.cc/2019/Conference/Paper989/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621617664}}}, {"id": "Skevj_35pX", "original": null, "number": 10, "cdate": 1542273182988, "ddate": null, "tcdate": 1542273182988, "tmdate": 1542657084607, "tddate": null, "forum": "rJg8yhAqKm", "replyto": "r1e5rrncT7", "invitation": "ICLR.cc/2019/Conference/-/Paper989/Official_Comment", "content": {"title": "More Intuition (2/4)", "comment": "\"I thought the motivation for choosing this regularizer was lacking. The authors cite the information bottleneck literature, but we shouldn\u2019t need to read all these papers, the main ideas should be summarized here.\"\n\nWe focus on multi-goal environments and goal-conditioned policies. The problem statement is quite simple: we aim to propose an algorithm whereby we incentive agents to learn task structure by training policies that perform well under a variety of goals, while not overfitting to any individual goal. We achieve this by training agents that, in addition to maximizing reward, minimize the policy dependence on the individual goal, quantified by the conditional mutual information  I(A; G | S). In order to minimize this quantity, we formulate it using ideas from variational information bottleneck. To make the paper self-explanatory, we added the mathematical description of the proposed method in the appendix (Section A).\n\n\"The argument for how the regularizer improves exploration seemed to me very hand-wavy and not well substantiated by experiments.\" \n\n\nWe evaluate in Table 2 how the regularizer improves exploration, the basic idea is how can we transfer the knowledge in form of decision states. Basically the intuition is, what we would like from any unsupervised/supervised transferrable exploration technique is to build a policy that is somehow good for adapting to or solving new problems. Here we try to generalize  to new mazes the knowledge acquired by the encoder in the form of the KL estimator. Basically, the intuition is that high KL = interesting state, even before the agent has discovered a single path to the goal. So if we can use egocentric observations and generalize effectively, we can predict which points have high KL before we have even learned to traverse the maze, and then we can use these high-KL regions as rewards without the need to have solved that particular maze in advance, using knowledge transferred from other mazes.\n\nTo do this, we first train agents with a goal bottleneck on one set of environments (MultiRoomN2S6) where they learn the sensory cues that correspond to decision states. Then, we use this  knowledge to guide exploration on another set of environments (MultiRoomN3S4, MultiRoomN4S4, and MultiRoomN5S4). And hence in this new environments, we are training another policy from scratch,  And using the KL from D_KL(p(z|s, g) | N(0,1)) as an exploration bonus to guide exploration. As shown in Table 2, We outperform strong exploration methods like Curiosity Driven Learning (ICM) and count based exploration. We would be happy to add other comparisons which reviewer has in mind.\n\nWe did another experiment in which we  show how the bonus reward of visiting decision states can help the exploration on the training tasks as compared to the policy learned from equation (1), and the policies learned without information of other tasks. The intuition is that  once the agent has been trained using goal bottleneck, then in the transfer state, the proposed  model can identify novel subgoals for further exploration using the learned knowledge of decision states in part 1, thus guiding the agent through a sequence of potential decision  states and through new regions of the state space.  We show that the agent trained with InfoBot visits more diverse states as compared to the baseline agent. We ask the reviewer to refer to Section E in appendix.\n\n\"How is the regularizer applied with other policy search algorithms besides Reinforce? Was it done in the paper? I can\u2019t say for sure. Specifically, when comparing to PPO, was the algorithm compared to a version of PPO augmented with this regularizer? Why yes or why no?\"\n\nYes, the proposed method can be combined with any policy based method (such that we have a parameterized form for the policy). In our experiments, we combine this with a2c for Minigrid Env as well as Atari setup, PPO for mujoco based envs, as well as state of the art off policy algorithms like soft actor critic. (SAC). Since, our method relies on goals, in order to minimize the I(A;G|S), in Atari as well as mujoco domains, we use high value states as a proxy to the goal state following [1] . In order to implement this, we maintain a buffer of 20000 high value states, and choose the state with the highest value under the current value function, as a proxy to the goal.  \n\n[1]  Recall Traces, https://arxiv.org/abs/1804.00379\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper989/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper989/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper989/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "InfoBot: Transfer and Exploration via the Information Bottleneck", "abstract": "A central challenge in reinforcement learning is discovering effective policies for tasks where rewards are sparsely distributed. We postulate that in the absence of useful reward signals, an effective exploration strategy should seek out {\\it decision states}. These states lie at critical junctions in the state space from where the agent can transition to new, potentially unexplored regions. We propose to learn about decision states from prior experience. By training a goal-conditioned model with an information bottleneck, we can identify decision states by examining where the model accesses the goal state through the bottleneck. We find that this simple mechanism effectively identifies decision states, even in partially observed settings. In effect, the model learns the sensory cues that correlate with potential subgoals. In new environments, this model can then identify novel subgoals for further exploration, guiding the agent through a sequence of potential decision  states and through new regions of the state space.", "keywords": ["Information bottleneck", "policy transfer", "policy generalization", "exploration"], "authorids": ["anirudhgoyal9119@gmail.com", "riashat.islam@mail.mcgill.ca", "danieljstrouse@gmail.com", "zafarali.ahmed@mail.mcgill.ca", "hugolarochelle@google.com", "botvinick@google.com", "svlevine@eecs.berkeley.edu", "yoshua.bengio@mila.quebec"], "authors": ["Anirudh Goyal", "Riashat Islam", "DJ Strouse", "Zafarali Ahmed", "Hugo Larochelle", "Matthew Botvinick", "Yoshua Bengio", "Sergey Levine"], "TL;DR": "Training agents with goal-policy information bottlenecks promotes transfer and yields a powerful exploration bonus", "pdf": "/pdf/9bfd1fe1d796c1ebab86fec8129f320412daf1b4.pdf", "paperhash": "goyal|infobot_transfer_and_exploration_via_the_information_bottleneck", "_bibtex": "@inproceedings{\ngoyal2018transfer,\ntitle={Transfer and Exploration via the Information Bottleneck},\nauthor={Anirudh Goyal and Riashat Islam and DJ Strouse and Zafarali Ahmed and Hugo Larochelle and Matthew Botvinick and Sergey Levine and Yoshua Bengio},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJg8yhAqKm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper989/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621617664, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJg8yhAqKm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper989/Authors", "ICLR.cc/2019/Conference/Paper989/Reviewers", "ICLR.cc/2019/Conference/Paper989/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper989/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper989/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper989/Authors|ICLR.cc/2019/Conference/Paper989/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper989/Reviewers", "ICLR.cc/2019/Conference/Paper989/Authors", "ICLR.cc/2019/Conference/Paper989/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621617664}}}, {"id": "r1e5rrncT7", "original": null, "number": 9, "cdate": 1542272322215, "ddate": null, "tcdate": 1542272322215, "tmdate": 1542657069872, "tddate": null, "forum": "rJg8yhAqKm", "replyto": "SJxWVtZ5pX", "invitation": "ICLR.cc/2019/Conference/-/Paper989/Official_Comment", "content": {"title": "Thanks for your feedback!  (1/4)", "comment": "We thank the reviewer for the positive and constructive feedback. We appreciate that the reviewer finds that our method interesting. \n\n\"The quality of the experimental validation provided by the authors is in my opinion borderline acceptable. Although the method performs better on toy settings, it seems barely better on more challenging ones. Experiments in section 4.5 lack detail and context.\" \n\nIn order to make our experiments more rigorous, we conducted more experiments to answer reviewers concern. \n\n1) More challenging Navigation setup.  - We ask the reviewer to refer to heading \"More challenging navigation Environment\". Also for more details refer to Section 4.6 in the main paper.  We compared to several strong baselines like hierarchical RL methods, strong exploration methods as well as goal based methods. We would be happy to add more comparisons which reviewer has in mind.\n\n2) Comparison to State of the art Off policy Methods (SAC) in sparse  rewards. We ask the reviewer to refer to heading \"InfoBot Comparison to State of the art off policy methods (Soft Actor Critic)\". Also, for more details refer to  Section F in the appendix.\n \n3) Application of the proposed method in multi-agent communication, such that the goal in the proposed method corresponds to the information obtained because of communication with another agent in multi-agent communication channel. Here,  we  want  to  show  that  by  training  agents  to  develop  \u201cdefault  behaviours\u201d  as  well  as  the knowledge of when to break those behaviours, using an information bottleneck can also help in other\nscenarios like multi-agent communication. Consider multiagent communication, where in order to\nsolve a task, agents require communicating with another agents. Ideally, an agent would would like\nto communicate with other agent, only when its essential to communicate, i.e the agents would like\nto minimize the communication with another agents. Here we show that selectively deciding when to\ncommunicate with another agent can result in faster learning. We follow the same  experimental setup as in the paper (Mordatch and Abbeel, 2018). \n\nMethod                                                 Train Reward                   TestReward\nNo Communication                               -0.919                                  -0.920\nCommunication                                      -0.36                                   -0.472\nCommunication (with KL cost)            -0.293                                  -0.38 \n\n(Lower is better). More details about this experimental setup can be found in Section D (Appendix). \n\nI. Mordatch and P. Abbeel.  Emergence of grounded compositional language in multi-agent pop-\nulations.\n\nWe acknowledge that the paper was certainly lacking polish and accept that this may have made the paper difficult to read in places. We have uploaded a revised version in which we have revised the problem statement and writing as per the reviewer's suggestions. "}, "signatures": ["ICLR.cc/2019/Conference/Paper989/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper989/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper989/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "InfoBot: Transfer and Exploration via the Information Bottleneck", "abstract": "A central challenge in reinforcement learning is discovering effective policies for tasks where rewards are sparsely distributed. We postulate that in the absence of useful reward signals, an effective exploration strategy should seek out {\\it decision states}. These states lie at critical junctions in the state space from where the agent can transition to new, potentially unexplored regions. We propose to learn about decision states from prior experience. By training a goal-conditioned model with an information bottleneck, we can identify decision states by examining where the model accesses the goal state through the bottleneck. We find that this simple mechanism effectively identifies decision states, even in partially observed settings. In effect, the model learns the sensory cues that correlate with potential subgoals. In new environments, this model can then identify novel subgoals for further exploration, guiding the agent through a sequence of potential decision  states and through new regions of the state space.", "keywords": ["Information bottleneck", "policy transfer", "policy generalization", "exploration"], "authorids": ["anirudhgoyal9119@gmail.com", "riashat.islam@mail.mcgill.ca", "danieljstrouse@gmail.com", "zafarali.ahmed@mail.mcgill.ca", "hugolarochelle@google.com", "botvinick@google.com", "svlevine@eecs.berkeley.edu", "yoshua.bengio@mila.quebec"], "authors": ["Anirudh Goyal", "Riashat Islam", "DJ Strouse", "Zafarali Ahmed", "Hugo Larochelle", "Matthew Botvinick", "Yoshua Bengio", "Sergey Levine"], "TL;DR": "Training agents with goal-policy information bottlenecks promotes transfer and yields a powerful exploration bonus", "pdf": "/pdf/9bfd1fe1d796c1ebab86fec8129f320412daf1b4.pdf", "paperhash": "goyal|infobot_transfer_and_exploration_via_the_information_bottleneck", "_bibtex": "@inproceedings{\ngoyal2018transfer,\ntitle={Transfer and Exploration via the Information Bottleneck},\nauthor={Anirudh Goyal and Riashat Islam and DJ Strouse and Zafarali Ahmed and Hugo Larochelle and Matthew Botvinick and Sergey Levine and Yoshua Bengio},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJg8yhAqKm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper989/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621617664, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJg8yhAqKm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper989/Authors", "ICLR.cc/2019/Conference/Paper989/Reviewers", "ICLR.cc/2019/Conference/Paper989/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper989/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper989/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper989/Authors|ICLR.cc/2019/Conference/Paper989/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper989/Reviewers", "ICLR.cc/2019/Conference/Paper989/Authors", "ICLR.cc/2019/Conference/Paper989/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621617664}}}, {"id": "HyeBMFj9p7", "original": null, "number": 3, "cdate": 1542269196601, "ddate": null, "tcdate": 1542269196601, "tmdate": 1542656580603, "tddate": null, "forum": "rJg8yhAqKm", "replyto": "Hyg3Bds96Q", "invitation": "ICLR.cc/2019/Conference/-/Paper989/Official_Comment", "content": {"title": "InfoBot Comparison to State of the art off policy methods (Soft Actor Critic) - To ALL reviewers", "comment": "We also compared the  proposed method  to the state of the art off-policy methods like SAC. For this domain, we again use high value states as an approximation to the goal state following [1] . In order to implement this, we maintain a buffer of 20000 states, and choose the state with the highest value under the current value function, as a proxy to the goal.  \n\nWe compare the proposed method with SAC in  sparse reward scenarios. We evaluate the proposed algorithm  on 4 MuJoCo tasks in OpenAI Gym. \n\nThe result in Figure. 11 (Section F appendix) shows the performance of the proposed method showing improvement over the baseline on the sparse reward HalfCheetah-v2, Walker2d-v2, Swimmer-v2, Hopper-v2 tasks. We did not observe much benefit over SAC in dense reward scenario. Unlike Minigrid, and maze goal based navigation task, the reward structure in this benchmark is  dense in that the agent always receives a reasonable amount of reward according to its continuous progress. Hence, in the dense reward scenario, the learning problem for the policy regularized with goal bottleneck is not simpler as compared to that of the regular policy, and hence the  agent may not be forced to generalize across different contexts (by learning ``useful'' habits).\n\nTo verify our conjecture, we conducted experiments by making the reward which agent gets from the environment sparse. More specifically, the modified tasks give an reward once in 50 steps. We see that in this scenario, the agent trained with goal bottleneck regularization performs much better as compared to the SAC baseline. \n\n\n[1]  Recall Traces, https://arxiv.org/abs/1804.00379\n[2]  SAC, Soft Actor Critic https://arxiv.org/abs/1801.01290\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper989/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper989/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper989/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "InfoBot: Transfer and Exploration via the Information Bottleneck", "abstract": "A central challenge in reinforcement learning is discovering effective policies for tasks where rewards are sparsely distributed. We postulate that in the absence of useful reward signals, an effective exploration strategy should seek out {\\it decision states}. These states lie at critical junctions in the state space from where the agent can transition to new, potentially unexplored regions. We propose to learn about decision states from prior experience. By training a goal-conditioned model with an information bottleneck, we can identify decision states by examining where the model accesses the goal state through the bottleneck. We find that this simple mechanism effectively identifies decision states, even in partially observed settings. In effect, the model learns the sensory cues that correlate with potential subgoals. In new environments, this model can then identify novel subgoals for further exploration, guiding the agent through a sequence of potential decision  states and through new regions of the state space.", "keywords": ["Information bottleneck", "policy transfer", "policy generalization", "exploration"], "authorids": ["anirudhgoyal9119@gmail.com", "riashat.islam@mail.mcgill.ca", "danieljstrouse@gmail.com", "zafarali.ahmed@mail.mcgill.ca", "hugolarochelle@google.com", "botvinick@google.com", "svlevine@eecs.berkeley.edu", "yoshua.bengio@mila.quebec"], "authors": ["Anirudh Goyal", "Riashat Islam", "DJ Strouse", "Zafarali Ahmed", "Hugo Larochelle", "Matthew Botvinick", "Yoshua Bengio", "Sergey Levine"], "TL;DR": "Training agents with goal-policy information bottlenecks promotes transfer and yields a powerful exploration bonus", "pdf": "/pdf/9bfd1fe1d796c1ebab86fec8129f320412daf1b4.pdf", "paperhash": "goyal|infobot_transfer_and_exploration_via_the_information_bottleneck", "_bibtex": "@inproceedings{\ngoyal2018transfer,\ntitle={Transfer and Exploration via the Information Bottleneck},\nauthor={Anirudh Goyal and Riashat Islam and DJ Strouse and Zafarali Ahmed and Hugo Larochelle and Matthew Botvinick and Sergey Levine and Yoshua Bengio},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJg8yhAqKm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper989/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621617664, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJg8yhAqKm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper989/Authors", "ICLR.cc/2019/Conference/Paper989/Reviewers", "ICLR.cc/2019/Conference/Paper989/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper989/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper989/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper989/Authors|ICLR.cc/2019/Conference/Paper989/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper989/Reviewers", "ICLR.cc/2019/Conference/Paper989/Authors", "ICLR.cc/2019/Conference/Paper989/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621617664}}}, {"id": "SkeROX29am", "original": null, "number": 8, "cdate": 1542271861886, "ddate": null, "tcdate": 1542271861886, "tmdate": 1542341656191, "tddate": null, "forum": "rJg8yhAqKm", "replyto": "SkgXc1n5pQ", "invitation": "ICLR.cc/2019/Conference/-/Paper989/Official_Comment", "content": {"title": "Decision States (Middle ground b/w Model based and Model free RL) / Comparison to Soft Actor Critic/ Multiagent communication (3/3)", "comment": "The idea of decision states can also be used in other RL algorithms. It would be also interesting to see if this idea can further improve their performances.\n\nThe reviewer is right. The notion of decision points can be used at other places too like planning, combination of model based and model free RL.  Identifying useful decision states can provide a\ncomfortable middle ground between model-free reasoning and model-based planning. For example,\nimagine planning over individual decision states, while using model-free knowledge to navigate\nbetween bottlenecks: aspects of the environment that are physically complex but vary little between\nproblem instances are handled in a model-free way (the navigation between decision points), while\nthe particular decision points that are relevant to the task can be handled by explicitly reasoning\nabout causality, in a more abstract representation. We demonstrate this using a similar setup as in\nimagination augmented agents  (Weber et al., 2017). In imagination augmented agents, model free\nagents are augmented with imagination, such that the imagination model can be queried to make\npredictions about the future. We use the dynamics models to simulate imagined trajectories, which\nare then summarized by a neural network and this summary is provided as additional context to\na policy network.  Here, we use the output of the imagination module as a \u201cgoal\u201d and we want to\nshow that only near the decision points (i.e potential subgoals) the agent wants to make use of the\ninformation which is a result of running imagination module).  Here, we want to see, at which points in the state space the policy wants to access the information provided by running the imagination module.  Ideally, only at the decision states (i.e potential sub-goals) policy should access the output of the imagination moduleFor more details, we ask the reviewer to refer to section 4.7 in the main paper.\n\n(Weber et. al, 2017) - Imagination Augmented Agents  https://arxiv.org/abs/1707.06203\n\n======================================================================\n\n2) Application of the proposed method in multi-agent communication, such that the goal in the proposed method corresponds to the information obtained because of communication with another agent in multi-agent communication channel. Here,  we  want  to  show  that  by  training  agents  to  develop  \u201cdefault  behaviours\u201d  as  well  as  the knowledge of when to break those behaviours, using an information bottleneck can also help in other scenarios like multi-agent communication. Consider multiagent communication, where in order to solve a task, agents require communicating with another agents. Ideally, an agent would would like to communicate with other agent, only when its essential to communicate, i.e the agents would like to minimize the communication with another agents. Here we show that selectively deciding when to\ncommunicate with another agent can result in faster learning. We follow the same  experimental setup as in the paper (Mordatch and Abbeel, 2018). \n\nMethod                                                 Train Reward                   TestReward\nNo Communication                               -0.919                                  -0.920\nCommunication                                      -0.36                                   -0.472\nCommunication (with KL cost)            -0.293                                  -0.38 \n\n(Lower is better). More details about this experimental setup can be found in Section D (Appendix). \n\nI. Mordatch and P. Abbeel.  Emergence of grounded compositional language in multi-agent pop-\nulations.\n\n===================================================================================\n\n3. Comparison to Soft Actor Critic - We compared the  proposed method  to the state of the art off-policy methods like SAC (Soft Actor Critic). For this domain, we again use high value states as an approximation to the goal state following [1] . In order to implement this, we maintain a buffer of 20000 states, and choose the state with the highest value under the current value function, as a proxy to the goal. \n\n[1]  Recall Traces, https://arxiv.org/abs/1804.00379\n[2]  SAC, Soft Actor Critic https://arxiv.org/abs/1801.01290\n\nWe ask the reviewer to refer to heading \"InfoBot Comparison to State of the art off policy methods (Soft Actor Critic)\". Also, for more details refer to  Section F in the appendix. \n\nClosing:\nThank you for your time. We hope you find that our revision addresses your concerns.\n We hope that our updates to the manuscript address the reviewer's concerns about clarity, and we hope that the discussion above addresses the reviewer's concerns about empirical significance. We once again thank the reviewer for the thorough feedback of our manuscript. Please let us know if anything is unclear here, if you\u2019re uncertain about part of the argument, or if there is any other comparison that would be helpful in clarifying things more.\n\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper989/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper989/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper989/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "InfoBot: Transfer and Exploration via the Information Bottleneck", "abstract": "A central challenge in reinforcement learning is discovering effective policies for tasks where rewards are sparsely distributed. We postulate that in the absence of useful reward signals, an effective exploration strategy should seek out {\\it decision states}. These states lie at critical junctions in the state space from where the agent can transition to new, potentially unexplored regions. We propose to learn about decision states from prior experience. By training a goal-conditioned model with an information bottleneck, we can identify decision states by examining where the model accesses the goal state through the bottleneck. We find that this simple mechanism effectively identifies decision states, even in partially observed settings. In effect, the model learns the sensory cues that correlate with potential subgoals. In new environments, this model can then identify novel subgoals for further exploration, guiding the agent through a sequence of potential decision  states and through new regions of the state space.", "keywords": ["Information bottleneck", "policy transfer", "policy generalization", "exploration"], "authorids": ["anirudhgoyal9119@gmail.com", "riashat.islam@mail.mcgill.ca", "danieljstrouse@gmail.com", "zafarali.ahmed@mail.mcgill.ca", "hugolarochelle@google.com", "botvinick@google.com", "svlevine@eecs.berkeley.edu", "yoshua.bengio@mila.quebec"], "authors": ["Anirudh Goyal", "Riashat Islam", "DJ Strouse", "Zafarali Ahmed", "Hugo Larochelle", "Matthew Botvinick", "Yoshua Bengio", "Sergey Levine"], "TL;DR": "Training agents with goal-policy information bottlenecks promotes transfer and yields a powerful exploration bonus", "pdf": "/pdf/9bfd1fe1d796c1ebab86fec8129f320412daf1b4.pdf", "paperhash": "goyal|infobot_transfer_and_exploration_via_the_information_bottleneck", "_bibtex": "@inproceedings{\ngoyal2018transfer,\ntitle={Transfer and Exploration via the Information Bottleneck},\nauthor={Anirudh Goyal and Riashat Islam and DJ Strouse and Zafarali Ahmed and Hugo Larochelle and Matthew Botvinick and Sergey Levine and Yoshua Bengio},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJg8yhAqKm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper989/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621617664, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJg8yhAqKm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper989/Authors", "ICLR.cc/2019/Conference/Paper989/Reviewers", "ICLR.cc/2019/Conference/Paper989/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper989/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper989/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper989/Authors|ICLR.cc/2019/Conference/Paper989/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper989/Reviewers", "ICLR.cc/2019/Conference/Paper989/Authors", "ICLR.cc/2019/Conference/Paper989/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621617664}}}, {"id": "HJgBWjs9am", "original": null, "number": 4, "cdate": 1542269693364, "ddate": null, "tcdate": 1542269693364, "tmdate": 1542307935494, "tddate": null, "forum": "rJg8yhAqKm", "replyto": "BJxrL1eLoQ", "invitation": "ICLR.cc/2019/Conference/-/Paper989/Official_Comment", "content": {"title": "Thanks for your feedback!  (1/2)", "comment": "Thanks for the feedback. We have conducted additional experiments to address the concerns raised about the evaluation, and we clarify specific points below. We believe that these additions address all of your concerns about the work, though we would appreciate any additional comments or feedback that you might have.\n\n\"While this is potentially useful, I found the motivation for the approach  It is not clear what problems the authors have in mind and why exactly they propose their specific method.\"\n\nWe acknowledge that the paper was certainly lacking polish and accept that this may have made the paper difficult to read in places. We have uploaded a revised version in which we have revised the problem statement and writing as per the reviewer's suggestions. We focus on multi-goal environments and goal-conditioned policies. The problem statement is quite simple: we aim to propose an algorithm whereby we incentive agents to learn task structure by training policies that perform well under a variety of goals, while not overfitting to any individual goal. We achieve this by training agents that, in addition to maximizing reward, minimize the policy dependence on the individual goal, quantified by the conditional mutual information  I(A; G | S). In order to minimize this quantity, we formulate it using ideas from variational information bottleneck. To make the paper self-explanatory, we added the mathematical description of the proposed method in the appendix (Section A).\n\n\"The presentation of the method itself is not self-contained and often relies on references to other papers to the point where it is difficult to understand just by reading the paper. \"\n\nWe again acknowledge that the paper was missing certain parts which made the paper difficult to read. We have added another section in the appendix which gives a more mathematical description of the proposed approach. We realized because of the way we have explained things there could be some fundamental misunderstanding about the proposed method. Thus, we would like to clarify this misunderstanding, not only with the intent of convincing you of the idea behind the proposed method but also with the intent of making amends to  the method description where necessary so that readers may not arrive at the same conclusions as you. We added the mathematical description of the proposed framework in the appendix (Section A). \n"}, "signatures": ["ICLR.cc/2019/Conference/Paper989/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper989/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper989/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "InfoBot: Transfer and Exploration via the Information Bottleneck", "abstract": "A central challenge in reinforcement learning is discovering effective policies for tasks where rewards are sparsely distributed. We postulate that in the absence of useful reward signals, an effective exploration strategy should seek out {\\it decision states}. These states lie at critical junctions in the state space from where the agent can transition to new, potentially unexplored regions. We propose to learn about decision states from prior experience. By training a goal-conditioned model with an information bottleneck, we can identify decision states by examining where the model accesses the goal state through the bottleneck. We find that this simple mechanism effectively identifies decision states, even in partially observed settings. In effect, the model learns the sensory cues that correlate with potential subgoals. In new environments, this model can then identify novel subgoals for further exploration, guiding the agent through a sequence of potential decision  states and through new regions of the state space.", "keywords": ["Information bottleneck", "policy transfer", "policy generalization", "exploration"], "authorids": ["anirudhgoyal9119@gmail.com", "riashat.islam@mail.mcgill.ca", "danieljstrouse@gmail.com", "zafarali.ahmed@mail.mcgill.ca", "hugolarochelle@google.com", "botvinick@google.com", "svlevine@eecs.berkeley.edu", "yoshua.bengio@mila.quebec"], "authors": ["Anirudh Goyal", "Riashat Islam", "DJ Strouse", "Zafarali Ahmed", "Hugo Larochelle", "Matthew Botvinick", "Yoshua Bengio", "Sergey Levine"], "TL;DR": "Training agents with goal-policy information bottlenecks promotes transfer and yields a powerful exploration bonus", "pdf": "/pdf/9bfd1fe1d796c1ebab86fec8129f320412daf1b4.pdf", "paperhash": "goyal|infobot_transfer_and_exploration_via_the_information_bottleneck", "_bibtex": "@inproceedings{\ngoyal2018transfer,\ntitle={Transfer and Exploration via the Information Bottleneck},\nauthor={Anirudh Goyal and Riashat Islam and DJ Strouse and Zafarali Ahmed and Hugo Larochelle and Matthew Botvinick and Sergey Levine and Yoshua Bengio},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJg8yhAqKm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper989/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621617664, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJg8yhAqKm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper989/Authors", "ICLR.cc/2019/Conference/Paper989/Reviewers", "ICLR.cc/2019/Conference/Paper989/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper989/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper989/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper989/Authors|ICLR.cc/2019/Conference/Paper989/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper989/Reviewers", "ICLR.cc/2019/Conference/Paper989/Authors", "ICLR.cc/2019/Conference/Paper989/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621617664}}}, {"id": "SkgXc1n5pQ", "original": null, "number": 7, "cdate": 1542270859096, "ddate": null, "tcdate": 1542270859096, "tmdate": 1542271889535, "tddate": null, "forum": "rJg8yhAqKm", "replyto": "S1eLE0i5am", "invitation": "ICLR.cc/2019/Conference/-/Paper989/Official_Comment", "content": {"title": "Visitation Count  (2/3)", "comment": "\"What is the reasoning of picking \u201cCount-base baseline\u201d for Figure 4, rather than the method of curiosity-based exploration?\"\n\nThe variance across different runs for curiosity based baseline was high, and thats why we reported only count based exploration. Though in table 2, we report the \u201cMAX\u201d performance we got using curiosity driven exploration (over 5 runs), while for InfoBot we average over 5 random seeds. \n\n\"would be interesting to see how the bonus reward of visiting decision states can help the exploration on the training tasks, compared to the policy learned from equation (1), and the policies learned without information of other tasks.\"\n\nWe thank the reviewer for the suggestion. This is indeed an insightful experiment which can help understand the exploration mechanism can effectively identifies decision states, and thus the  model can then identify novel subgoals for further exploration, guiding the agent through a sequence of potential decision  states and through new regions of the state space. Section E in appendix shows that the agent trained with InfoBot visits more diverse states as compared to the baseline agent. "}, "signatures": ["ICLR.cc/2019/Conference/Paper989/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper989/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper989/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "InfoBot: Transfer and Exploration via the Information Bottleneck", "abstract": "A central challenge in reinforcement learning is discovering effective policies for tasks where rewards are sparsely distributed. We postulate that in the absence of useful reward signals, an effective exploration strategy should seek out {\\it decision states}. These states lie at critical junctions in the state space from where the agent can transition to new, potentially unexplored regions. We propose to learn about decision states from prior experience. By training a goal-conditioned model with an information bottleneck, we can identify decision states by examining where the model accesses the goal state through the bottleneck. We find that this simple mechanism effectively identifies decision states, even in partially observed settings. In effect, the model learns the sensory cues that correlate with potential subgoals. In new environments, this model can then identify novel subgoals for further exploration, guiding the agent through a sequence of potential decision  states and through new regions of the state space.", "keywords": ["Information bottleneck", "policy transfer", "policy generalization", "exploration"], "authorids": ["anirudhgoyal9119@gmail.com", "riashat.islam@mail.mcgill.ca", "danieljstrouse@gmail.com", "zafarali.ahmed@mail.mcgill.ca", "hugolarochelle@google.com", "botvinick@google.com", "svlevine@eecs.berkeley.edu", "yoshua.bengio@mila.quebec"], "authors": ["Anirudh Goyal", "Riashat Islam", "DJ Strouse", "Zafarali Ahmed", "Hugo Larochelle", "Matthew Botvinick", "Yoshua Bengio", "Sergey Levine"], "TL;DR": "Training agents with goal-policy information bottlenecks promotes transfer and yields a powerful exploration bonus", "pdf": "/pdf/9bfd1fe1d796c1ebab86fec8129f320412daf1b4.pdf", "paperhash": "goyal|infobot_transfer_and_exploration_via_the_information_bottleneck", "_bibtex": "@inproceedings{\ngoyal2018transfer,\ntitle={Transfer and Exploration via the Information Bottleneck},\nauthor={Anirudh Goyal and Riashat Islam and DJ Strouse and Zafarali Ahmed and Hugo Larochelle and Matthew Botvinick and Sergey Levine and Yoshua Bengio},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJg8yhAqKm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper989/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621617664, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJg8yhAqKm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper989/Authors", "ICLR.cc/2019/Conference/Paper989/Reviewers", "ICLR.cc/2019/Conference/Paper989/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper989/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper989/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper989/Authors|ICLR.cc/2019/Conference/Paper989/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper989/Reviewers", "ICLR.cc/2019/Conference/Paper989/Authors", "ICLR.cc/2019/Conference/Paper989/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621617664}}}, {"id": "S1eLE0i5am", "original": null, "number": 6, "cdate": 1542270509760, "ddate": null, "tcdate": 1542270509760, "tmdate": 1542271876902, "tddate": null, "forum": "rJg8yhAqKm", "replyto": "SJlWokadnQ", "invitation": "ICLR.cc/2019/Conference/-/Paper989/Official_Comment", "content": {"title": "Thanks for your feedback!  (1/3)", "comment": "We thank the reviewer for their time and feedback. We  hope to address concerns the reviewer has here.\n\n\"What is the reasoning behind the selection of the baselines, e.g. A2C as the baseline for the miniGrid experiments? \"\n\nWe build from A2C with using goal-conditioned policies and the KL regularization. Hence, The A2C with no kl-regularization is the immediate baseline to consider.  Since for maze experiments, the env is of the  nature of mini-grid POMDP environments with sparse rewards, as well as discrete action, a2c worked out of the box and hence was the most straightforward baseline for comparison.  We would be happy to add other comparisons which reviewer has in mind. \n\n\n>> What are the performances of the methods in Table 2, in direct policy generalization? Or is there any reason not reporting them here?\n\nThe setup in direct policy generalization is different as to what we evaluate in Table 2. Direct policy generalization refers to first training an agent with a goal bottleneck on one set of environments (MultiRoomN2S6), and then evaluate the trained agent (without fine tuning on new set of environment ((MultiRoomN3S4, MultiRoomN4S4, and MultiRoomN5S4)).\n\nWhat we evaluate in Table 2, is how can we transfer the knowledge in form of decision states. \nBasically the intuition is, what we would like from any unsupervised/supervised transferrable exploration technique is to build a policy that is somehow good for adapting to or solving new problems. Here we try to generalize  to new mazes the knowledge acquired by the encoder in the form of the KL estimator. Basically, the intuition is that high KL = interesting state, even before the agent has discovered a single path to the goal. So if we can use egocentric observations and generalize effectively, we can predict which points have high KL before we have even learned to traverse the maze, and then we can use these high-KL regions as rewards without the need to have solved that particular maze in advance, using knowledge transferred from other mazes.\n\nTo do this, we first train agents with a goal bottleneck on one set of environments (MultiRoomN2S6) where they learn the sensory cues that correspond to decision states. Then, we use this  knowledge to guide exploration on another set of environments (MultiRoomN3S4, MultiRoomN4S4, and MultiRoomN5S4). And hence in this new environments, we are training another policy from scratch,  And using the KL from D_KL(p(z|s, g) | N(0,1)) as an exploration bonus to guide exploration. \n"}, "signatures": ["ICLR.cc/2019/Conference/Paper989/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper989/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper989/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "InfoBot: Transfer and Exploration via the Information Bottleneck", "abstract": "A central challenge in reinforcement learning is discovering effective policies for tasks where rewards are sparsely distributed. We postulate that in the absence of useful reward signals, an effective exploration strategy should seek out {\\it decision states}. These states lie at critical junctions in the state space from where the agent can transition to new, potentially unexplored regions. We propose to learn about decision states from prior experience. By training a goal-conditioned model with an information bottleneck, we can identify decision states by examining where the model accesses the goal state through the bottleneck. We find that this simple mechanism effectively identifies decision states, even in partially observed settings. In effect, the model learns the sensory cues that correlate with potential subgoals. In new environments, this model can then identify novel subgoals for further exploration, guiding the agent through a sequence of potential decision  states and through new regions of the state space.", "keywords": ["Information bottleneck", "policy transfer", "policy generalization", "exploration"], "authorids": ["anirudhgoyal9119@gmail.com", "riashat.islam@mail.mcgill.ca", "danieljstrouse@gmail.com", "zafarali.ahmed@mail.mcgill.ca", "hugolarochelle@google.com", "botvinick@google.com", "svlevine@eecs.berkeley.edu", "yoshua.bengio@mila.quebec"], "authors": ["Anirudh Goyal", "Riashat Islam", "DJ Strouse", "Zafarali Ahmed", "Hugo Larochelle", "Matthew Botvinick", "Yoshua Bengio", "Sergey Levine"], "TL;DR": "Training agents with goal-policy information bottlenecks promotes transfer and yields a powerful exploration bonus", "pdf": "/pdf/9bfd1fe1d796c1ebab86fec8129f320412daf1b4.pdf", "paperhash": "goyal|infobot_transfer_and_exploration_via_the_information_bottleneck", "_bibtex": "@inproceedings{\ngoyal2018transfer,\ntitle={Transfer and Exploration via the Information Bottleneck},\nauthor={Anirudh Goyal and Riashat Islam and DJ Strouse and Zafarali Ahmed and Hugo Larochelle and Matthew Botvinick and Sergey Levine and Yoshua Bengio},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJg8yhAqKm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper989/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621617664, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJg8yhAqKm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper989/Authors", "ICLR.cc/2019/Conference/Paper989/Reviewers", "ICLR.cc/2019/Conference/Paper989/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper989/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper989/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper989/Authors|ICLR.cc/2019/Conference/Paper989/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper989/Reviewers", "ICLR.cc/2019/Conference/Paper989/Authors", "ICLR.cc/2019/Conference/Paper989/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621617664}}}, {"id": "H1xMDas56m", "original": null, "number": 5, "cdate": 1542270298047, "ddate": null, "tcdate": 1542270298047, "tmdate": 1542271440699, "tddate": null, "forum": "rJg8yhAqKm", "replyto": "HJgBWjs9am", "invitation": "ICLR.cc/2019/Conference/-/Paper989/Official_Comment", "content": {"title": "More experimental results (Navigation/Comparison to Soft Actor Critic / Multiagent communication)  (2/2)", "comment": ">> The experiments are rather weak, they are missing comparison to strong exploration baselines and goal-oriented baselines.\n\nIn order to address reviewer\u2019s concern, we did several  more experiments.\n\nWe added these experiments to the main paper\n\n1) More challenging Navigation setup.  - We ask the reviewer to refer to heading \"More challenging navigation Environment\". Also for more details refer to Section 4.6 in the main paper. \n\n2) Comparison to State of the art Off policy Methods (SAC) in sparse  rewards. We ask the reviewer to refer to heading \"InfoBot Comparison to State of the art off policy methods (Soft Actor Critic)\". Also, for more details refer to  Section F in the appendix.\n \n3) Application of the proposed method in multi-agent communication, such that the goal in the proposed method corresponds to the information obtained because of communication with another agent in multi-agent communication channel. Here,  we  want  to  show  that  by  training  agents  to  develop  \u201cdefault  behaviours\u201d  as  well  as  the knowledge of when to break those behaviours, using an information bottleneck can also help in other\nscenarios like multi-agent communication. Consider multiagent communication, where in order to\nsolve a task, agents require communicating with another agents. Ideally, an agent would would like\nto communicate with other agent, only when its essential to communicate, i.e the agents would like\nto minimize the communication with another agents. Here we show that selectively deciding when to\ncommunicate with another agent can result in faster learning. We follow the same  experimental setup as in the paper (Mordatch and Abbeel, 2018). \n\nMethod                                                 Train Reward                   TestReward\nNo Communication                               -0.919                                  -0.920\nCommunication                                      -0.36                                   -0.472\nCommunication (with KL cost)            -0.293                                  -0.38 \n\n(Lower is better). More details about this experimental setup can be found in Section D (Appendix). \n\nI. Mordatch and P. Abbeel.  Emergence of grounded compositional language in multi-agent pop-\nulations.\n\n\nWe would appreciate it if the reviewer could take another look at our changes and additional results, and let us know if the reviewer has request for additional changes that would alleviate the reviewer's concerns.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper989/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper989/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper989/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "InfoBot: Transfer and Exploration via the Information Bottleneck", "abstract": "A central challenge in reinforcement learning is discovering effective policies for tasks where rewards are sparsely distributed. We postulate that in the absence of useful reward signals, an effective exploration strategy should seek out {\\it decision states}. These states lie at critical junctions in the state space from where the agent can transition to new, potentially unexplored regions. We propose to learn about decision states from prior experience. By training a goal-conditioned model with an information bottleneck, we can identify decision states by examining where the model accesses the goal state through the bottleneck. We find that this simple mechanism effectively identifies decision states, even in partially observed settings. In effect, the model learns the sensory cues that correlate with potential subgoals. In new environments, this model can then identify novel subgoals for further exploration, guiding the agent through a sequence of potential decision  states and through new regions of the state space.", "keywords": ["Information bottleneck", "policy transfer", "policy generalization", "exploration"], "authorids": ["anirudhgoyal9119@gmail.com", "riashat.islam@mail.mcgill.ca", "danieljstrouse@gmail.com", "zafarali.ahmed@mail.mcgill.ca", "hugolarochelle@google.com", "botvinick@google.com", "svlevine@eecs.berkeley.edu", "yoshua.bengio@mila.quebec"], "authors": ["Anirudh Goyal", "Riashat Islam", "DJ Strouse", "Zafarali Ahmed", "Hugo Larochelle", "Matthew Botvinick", "Yoshua Bengio", "Sergey Levine"], "TL;DR": "Training agents with goal-policy information bottlenecks promotes transfer and yields a powerful exploration bonus", "pdf": "/pdf/9bfd1fe1d796c1ebab86fec8129f320412daf1b4.pdf", "paperhash": "goyal|infobot_transfer_and_exploration_via_the_information_bottleneck", "_bibtex": "@inproceedings{\ngoyal2018transfer,\ntitle={Transfer and Exploration via the Information Bottleneck},\nauthor={Anirudh Goyal and Riashat Islam and DJ Strouse and Zafarali Ahmed and Hugo Larochelle and Matthew Botvinick and Sergey Levine and Yoshua Bengio},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJg8yhAqKm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper989/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621617664, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJg8yhAqKm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper989/Authors", "ICLR.cc/2019/Conference/Paper989/Reviewers", "ICLR.cc/2019/Conference/Paper989/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper989/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper989/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper989/Authors|ICLR.cc/2019/Conference/Paper989/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper989/Reviewers", "ICLR.cc/2019/Conference/Paper989/Authors", "ICLR.cc/2019/Conference/Paper989/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621617664}}}, {"id": "BJxrL1eLoQ", "original": null, "number": 1, "cdate": 1539862349005, "ddate": null, "tcdate": 1539862349005, "tmdate": 1541533517316, "tddate": null, "forum": "rJg8yhAqKm", "replyto": "rJg8yhAqKm", "invitation": "ICLR.cc/2019/Conference/-/Paper989/Official_Review", "content": {"title": "Potentially useful but poorly motivated and evaluated", "review": "The paper proposes a method of regularising goal-conditioned policies with a mutual information term. While this is potentially useful, I found the motivation for the approach and the experimental results insufficient. On top of that the presentation could also use some improvements. I do not recommend acceptance at this time.\n\nThe introduction is vague and involves undefined terms such as \"useful habits\". It is not clear what problems the authors have in mind and why exactly they propose their specific method. The presentation of the method itself is not self-contained and often relies on references to other papers to the point where it is difficult to understand just by reading the paper. Some symbols are not defined, for example what is Z and why is it discrete?\n\nThe experiments are rather weak, they are missing comparison to strong exploration baselines and goal-oriented baselines.", "rating": "3: Clear rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper989/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "InfoBot: Transfer and Exploration via the Information Bottleneck", "abstract": "A central challenge in reinforcement learning is discovering effective policies for tasks where rewards are sparsely distributed. We postulate that in the absence of useful reward signals, an effective exploration strategy should seek out {\\it decision states}. These states lie at critical junctions in the state space from where the agent can transition to new, potentially unexplored regions. We propose to learn about decision states from prior experience. By training a goal-conditioned model with an information bottleneck, we can identify decision states by examining where the model accesses the goal state through the bottleneck. We find that this simple mechanism effectively identifies decision states, even in partially observed settings. In effect, the model learns the sensory cues that correlate with potential subgoals. In new environments, this model can then identify novel subgoals for further exploration, guiding the agent through a sequence of potential decision  states and through new regions of the state space.", "keywords": ["Information bottleneck", "policy transfer", "policy generalization", "exploration"], "authorids": ["anirudhgoyal9119@gmail.com", "riashat.islam@mail.mcgill.ca", "danieljstrouse@gmail.com", "zafarali.ahmed@mail.mcgill.ca", "hugolarochelle@google.com", "botvinick@google.com", "svlevine@eecs.berkeley.edu", "yoshua.bengio@mila.quebec"], "authors": ["Anirudh Goyal", "Riashat Islam", "DJ Strouse", "Zafarali Ahmed", "Hugo Larochelle", "Matthew Botvinick", "Yoshua Bengio", "Sergey Levine"], "TL;DR": "Training agents with goal-policy information bottlenecks promotes transfer and yields a powerful exploration bonus", "pdf": "/pdf/9bfd1fe1d796c1ebab86fec8129f320412daf1b4.pdf", "paperhash": "goyal|infobot_transfer_and_exploration_via_the_information_bottleneck", "_bibtex": "@inproceedings{\ngoyal2018transfer,\ntitle={Transfer and Exploration via the Information Bottleneck},\nauthor={Anirudh Goyal and Riashat Islam and DJ Strouse and Zafarali Ahmed and Hugo Larochelle and Matthew Botvinick and Sergey Levine and Yoshua Bengio},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJg8yhAqKm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper989/Official_Review", "cdate": 1542234331552, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "rJg8yhAqKm", "replyto": "rJg8yhAqKm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper989/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335848093, "tmdate": 1552335848093, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper989/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 34}